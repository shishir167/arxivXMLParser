<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:06:47Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|97001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405060</id><created>2004-05-17</created><authors><author><keyname>Duchamp</keyname><forenames>Gerard</forenames></author><author><keyname>Kacem</keyname><forenames>Hatem Hadj</forenames></author><author><keyname>Laugerotte</keyname><forenames>Eric</forenames></author></authors><title>An unexpected application of minimization theory to module decompositions</title><categories>cs.SC</categories><comments>15-02-2004</comments><proxy>ccsd ccsd-00001573</proxy><acm-class>H.2</acm-class><abstract>  The aim of this work is to show how we can decompose a module (if
decomposable) into an indecomposable module with the help of the minimization
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405061</id><created>2004-05-17</created><authors><author><keyname>Vasudevan</keyname><forenames>Rangarajan</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Agrawal</keyname><forenames>Dharma P.</forenames></author></authors><title>Jigsaw-based Security in Data Transfer in Computer Networks</title><categories>cs.CR</categories><acm-class>E.3</acm-class><journal-ref>IEEE International Conference on Information Technology: Coding
  and Computing (ITCC'04), USA, IEEE Computer Society, Volume 1, pp. 2-6, 2004</journal-ref><abstract>  In this paper, we present a novel encryption-less algorithm to enhance
security in transmission of data in networks. The algorithm uses an intuitively
simple idea of a 'jigsaw puzzle' to break the transformed data into multiple
parts where these parts form the pieces of the puzzle. Then these parts are
packaged into packets and sent to the receiver. A secure and efficient
mechanism is provided to convey the information that is necessary for obtaining
the original data at the receiver-end from its parts in the packets, that is,
for solving the 'jigsaw puzzle'. The algorithm is designed to provide
information-theoretic (that is, unconditional) security by the use of a
one-time pad like scheme so that no intermediate or unintended node can obtain
the entire data. An authentication code is also used to ensure authenticity of
every packet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405062</id><created>2004-05-18</created><authors><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author></authors><title>Efficiency Enhancement of Probabilistic Model Building Genetic
  Algorithms</title><categories>cs.NE</categories><comments>Optimization by Building and Using Probabilistic Models. Workshop at
  the 2004 Genetic and Evolutionary Computation Conference</comments><report-no>IlliGAL Report No. 2004020</report-no><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  This paper presents two different efficiency-enhancement techniques for
probabilistic model building genetic algorithms. The first technique proposes
the use of a mutation operator which performs local search in the sub-solution
neighborhood identified through the probabilistic model. The second technique
proposes building and using an internal probabilistic model of the fitness
along with the probabilistic model of variable interactions. The fitness values
of some offspring are estimated using the probabilistic model, thereby avoiding
computationally expensive function evaluations. The scalability of the
aforementioned techniques are analyzed using facetwise models for convergence
time and population sizing. The speed-up obtained by each of the methods is
predicted and verified with empirical results. The results show that for
additively separable problems the competent mutation operator requires O(k 0.5
logm)--where k is the building-block size, and m is the number of building
blocks--less function evaluations than its selectorecombinative counterpart.
The results also show that the use of an internal probabilistic fitness model
reduces the required number of function evaluations to as low as 1-10% and
yields a speed-up of 2--50.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405063</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405063</id><created>2004-05-18</created><authors><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author></authors><title>Let's Get Ready to Rumble: Crossover Versus Mutation Head to Head</title><categories>cs.NE</categories><comments>Genetic and Evolutionary Computation Conference (GECCO-2004)</comments><report-no>IlliGAL Report No. 2004005</report-no><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  This paper analyzes the relative advantages between crossover and mutation on
a class of deterministic and stochastic additively separable problems. This
study assumes that the recombination and mutation operators have the knowledge
of the building blocks (BBs) and effectively exchange or search among competing
BBs. Facetwise models of convergence time and population sizing have been used
to determine the scalability of each algorithm. The analysis shows that for
additively separable deterministic problems, the BB-wise mutation is more
efficient than crossover, while the crossover outperforms the mutation on
additively separable problems perturbed with additive Gaussian noise. The
results show that the speed-up of using BB-wise mutation on deterministic
problems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs.
Likewise, the speed-up of using crossover on stochastic problems with fixed
noise variance is O(mk^{0.5}log m).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405064</id><created>2004-05-18</created><authors><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author></authors><title>Designing Competent Mutation Operators via Probabilistic Model Building
  of Neighborhoods</title><categories>cs.NE</categories><comments>Genetic and Evolutionary Computation Conference (GECCO-2004)</comments><report-no>IlliGAL Report No. 2004006</report-no><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  This paper presents a competent selectomutative genetic algorithm (GA), that
adapts linkage and solves hard problems quickly, reliably, and accurately. A
probabilistic model building process is used to automatically identify key
building blocks (BBs) of the search problem. The mutation operator uses the
probabilistic model of linkage groups to find the best among competing building
blocks. The competent selectomutative GA successfully solves additively
separable problems of bounded difficulty, requiring only subquadratic number of
function evaluations. The results show that for additively separable problems
the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and
requires O(k^{0.5}logm) less function evaluations than its selectorecombinative
counterpart, confirming theoretical results reported elsewhere (Sastry &amp;
Goldberg, 2004).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405065</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405065</id><created>2004-05-18</created><authors><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author></authors><title>Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise
  Fitness Estimation</title><categories>cs.NE</categories><comments>IEEE International Conference on Evolutionary Computation (CEC-2004)</comments><report-no>IlliGAL Report No. 2004010</report-no><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  This paper studies fitness inheritance as an efficiency enhancement technique
for a class of competent genetic algorithms called estimation distribution
algorithms. Probabilistic models of important sub-solutions are developed to
estimate the fitness of a proportion of individuals in the population, thereby
avoiding computationally expensive function evaluations. The effect of fitness
inheritance on the convergence time and population sizing are modeled and the
speed-up obtained through inheritance is predicted. The results show that a
fitness-inheritance mechanism which utilizes information on building-block
fitnesses provides significant efficiency enhancement. For additively separable
problems, fitness inheritance reduces the number of function evaluations to
about half and yields a speed-up of about 1.75--2.25.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405066</id><created>2004-05-18</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author><author><keyname>Weissman</keyname><forenames>Vicky</forenames></author></authors><title>A Logic for Reasoning about Digital Rights</title><categories>cs.CR cs.LO</categories><comments>21 pages. Appeared in the Proceedings of the 15th IEEE Computer
  Security Foundations Workshop, pp. 282-294, 2002</comments><acm-class>D.4.6; K.6.5; F.4.1</acm-class><abstract>  We present a logic for reasoning about licenses, which are ``terms of use''
for digital resources. The logic provides a language for writing both
properties of licenses and specifications that govern a client's actions. We
discuss the complexity of checking properties and specifications written in our
logic and propose a technique for verification. A key feature of our approach
is that it is essentially parameterized by the language in which the licenses
are written, provided that this language can be given a trace-based semantics.
We consider two license languages to illustrate this flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405067</id><created>2004-05-18</created><authors><author><keyname>Brightwell</keyname><forenames>Graham R.</forenames></author><author><keyname>Winkler</keyname><forenames>Peter</forenames></author></authors><title>Note on Counting Eulerian Circuits</title><categories>cs.CC cs.DM</categories><comments>2 figures</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  We show that the problem of counting the number of Eulerian circuits in an
undirected graph is complete for the class #P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405068</id><created>2004-05-20</created><updated>2005-10-17</updated><authors><author><keyname>Cao</keyname><forenames>Yongzhi</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Observability and Decentralized Control of Fuzzy Discrete Event Systems</title><categories>cs.DM cs.DC</categories><comments>14 pages, 1 figure. to be published in the IEEE Transactions on Fuzzy
  Systems</comments><acm-class>G.3; I.6.8</acm-class><journal-ref>IEEE Transactions on Fuzzy Systems, 14(2), pp. 202-216, April 2006</journal-ref><abstract>  Fuzzy discrete event systems as a generalization of (crisp) discrete event
systems have been introduced in order that it is possible to effectively
represent uncertainty, imprecision, and vagueness arising from the dynamic of
systems. A fuzzy discrete event system has been modelled by a fuzzy automaton;
its behavior is described in terms of the fuzzy language generated by the
automaton. In this paper, we are concerned with the supervisory control problem
for fuzzy discrete event systems with partial observation. Observability,
normality, and co-observability of crisp languages are extended to fuzzy
languages. It is shown that the observability, together with controllability,
of the desired fuzzy language is a necessary and sufficient condition for the
existence of a partially observable fuzzy supervisor. When a decentralized
solution is desired, it is proved that there exist local fuzzy supervisors if
and only if the fuzzy language to be synthesized is controllable and
co-observable. Moreover, the infimal controllable and observable fuzzy
superlanguage, and the supremal controllable and normal fuzzy sublanguage are
also discussed. Simple examples are provided to illustrate the theoretical
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405069</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405069</id><created>2004-05-20</created><authors><author><keyname>Grahne</keyname><forenames>G&#xf6;sta</forenames></author><author><keyname>Zhu</keyname><forenames>Jianfei</forenames></author></authors><title>Mining Frequent Itemsets from Secondary Memory</title><categories>cs.DB cs.IR</categories><acm-class>H.2.8</acm-class><abstract>  Mining frequent itemsets is at the core of mining association rules, and is
by now quite well understood algorithmically. However, most algorithms for
mining frequent itemsets assume that the main memory is large enough for the
data structures used in the mining, and very few efficient algorithms deal with
the case when the database is very large or the minimum support is very low.
Mining frequent itemsets from a very large database poses new challenges, as
astronomical amounts of raw data is ubiquitously being recorded in commerce,
science and government. In this paper, we discuss approaches to mining frequent
itemsets when data structures are too large to fit in main memory. Several
divide-and-conquer algorithms are given for mining from disks. Many novel
techniques are introduced. Experimental results show that the techniques reduce
the required disk accesses by orders of magnitude, and enable truly scalable
data mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405070</id><created>2004-05-20</created><updated>2004-05-24</updated><authors><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Traffic-driven model of the World Wide Web graph</title><categories>cs.NI cond-mat.stat-mech</categories><journal-ref>LNCS 3243, 56 (2004)</journal-ref><abstract>  We propose a model for the World Wide Web graph that couples the topological
growth with the traffic's dynamical evolution. The model is based on a simple
traffic-driven dynamics and generates weighted directed graphs exhibiting the
statistical properties observed in the Web. In particular, the model yields a
non-trivial time evolution of vertices and heavy-tail distributions for the
topological and traffic properties. The generated graphs exhibit a complex
architecture with a hierarchy of cohesiveness levels similar to those observed
in the analysis of real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405071</id><created>2004-05-21</created><authors><author><keyname>Tuan</keyname><forenames>Le-Chi</forenames></author><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Son</keyname><forenames>Tran Cao</forenames></author></authors><title>Regression with respect to sensing actions and partial states</title><categories>cs.AI</categories><comments>38 pages</comments><acm-class>I.2.4; I.2.8</acm-class><abstract>  In this paper, we present a state-based regression function for planning
domains where an agent does not have complete information and may have sensing
actions. We consider binary domains and employ the 0-approximation [Son &amp; Baral
2001] to define the regression function. In binary domains, the use of
0-approximation means using 3-valued states. Although planning using this
approach is incomplete with respect to the full semantics, we adopt it to have
a lower complexity. We prove the soundness and completeness of our regression
formulation with respect to the definition of progression. More specifically,
we show that (i) a plan obtained through regression for a planning problem is
indeed a progression solution of that planning problem, and that (ii) for each
plan found through progression, using regression one obtains that plan or an
equivalent one. We then develop a conditional planner that utilizes our
regression function. We prove the soundness and completeness of our planning
algorithm and present experimental results with respect to several well known
planning problems in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405072</id><created>2004-05-21</created><authors><author><keyname>Amendolia</keyname><forenames>S. R.</forenames></author><author><keyname>Estrella</keyname><forenames>F.</forenames></author><author><keyname>Hauer</keyname><forenames>T.</forenames></author><author><keyname>Manset</keyname><forenames>D.</forenames></author><author><keyname>McClatchey</keyname><forenames>R.</forenames></author><author><keyname>Odeh</keyname><forenames>M.</forenames></author><author><keyname>Reading</keyname><forenames>T.</forenames></author><author><keyname>Rogulin</keyname><forenames>D.</forenames></author><author><keyname>Schottlander</keyname><forenames>D.</forenames></author><author><keyname>Solomonides</keyname><forenames>T.</forenames></author></authors><title>Grid Databases for Shared Image Analysis in the MammoGrid Project</title><categories>cs.DB cs.DC</categories><comments>10 pages, 5 figures</comments><acm-class>H2.4;J.3</acm-class><journal-ref>Proceedings of the 2004 International Database Engineering and
  Applications Symposium (IDEAS'04). Coimbra Portugal. IEEE Press</journal-ref><abstract>  The MammoGrid project aims to prove that Grid infrastructures can be used for
collaborative clinical analysis of database-resident but geographically
distributed medical images. This requires: a) the provision of a
clinician-facing front-end workstation and b) the ability to service real-world
clinician queries across a distributed and federated database. The MammoGrid
project will prove the viability of the Grid by harnessing its power to enable
radiologists from geographically dispersed hospitals to share standardized
mammograms, to compare diagnoses (with and without computer aided detection of
tumours) and to perform sophisticated epidemiological studies across national
boundaries. This paper outlines the approach taken in MammoGrid to seamlessly
connect radiologist workstations across a Grid using an &quot;information
infrastructure&quot; and a DICOM-compliant object model residing in multiple
distributed data stores in Italy and the UK
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405073</id><created>2004-05-21</created><authors><author><keyname>Gay</keyname><forenames>Olivier</forenames></author></authors><title>Advanced exploitation of buffer overflow</title><categories>cs.CR</categories><comments>92 pages, in french</comments><abstract>  This article describes in depth several ways of exploiting buffer overflows
in the UNIX operating systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405074</id><created>2004-05-21</created><authors><author><keyname>Amendolia</keyname><forenames>S R</forenames></author><author><keyname>Estrella</keyname><forenames>F</forenames></author><author><keyname>Hassan</keyname><forenames>W</forenames></author><author><keyname>Hauer</keyname><forenames>T</forenames></author><author><keyname>Manset</keyname><forenames>D</forenames></author><author><keyname>McClatchey</keyname><forenames>R</forenames></author><author><keyname>Rogulin</keyname><forenames>D</forenames></author><author><keyname>Solomonides</keyname><forenames>T</forenames></author></authors><title>MammoGrid: A Service Oriented Architecture based Medical Grid
  Application</title><categories>cs.DC cs.DB</categories><comments>12 pages, 9 figures</comments><acm-class>H2.4;J.3;C2.4</acm-class><journal-ref>Proceedings of the 3rd International Conference on Grid and
  Cooperative Computing. Wuhan. China 2004</journal-ref><abstract>  The MammoGrid project has recently delivered its first proof-of-concept
prototype using a Service-Oriented Architecture (SOA)-based Grid application to
enable distributed computing spanning national borders. The underlying AliEn
Grid infrastructure has been selected because of its practicality and because
of its emergence as a potential open source standards-based solution for
managing and coordinating distributed resources. The resultant prototype is
expected to harness the use of huge amounts of medical image data to perform
epidemiological studies, advanced image processing, radiographic education and
ultimately, tele-diagnosis over communities of medical virtual organisations.
The MammoGrid prototype comprises a high-quality clinician visualization
workstation used for data acquisition and inspection, a DICOM-compliant
interface to a set of medical services (annotation, security, image analysis,
data storage and querying services) residing on a so-called Grid-box and secure
access to a network of other Grid-boxes connected through Grid middleware. This
paper outlines the MammoGrid approach in managing a federation of
Grid-connected mammography databases in the context of the recently delivered
prototype and will also describe the next phase of prototyping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405075</id><created>2004-05-22</created><authors><author><keyname>Qi</keyname><forenames>Xiaochu</forenames></author></authors><title>Reduction Strategies in Lambda Term Normalization and their Effects on
  Heap Usage</title><categories>cs.PL</categories><comments>This is a modified version of the master's thesis of Xiaochu Qi</comments><abstract>  Higher-order representations of objects such as programs, proofs, formulas
and types have become important to many symbolic computation tasks. Systems
that support such representations usually depend on the implementation of an
intensional view of the terms of some variant of the typed lambda-calculus.
Various notations have been proposed for lambda-terms to explicitly treat
substitutions as basis for realizing such implementations. There are, however,
several choices in the actual reduction strategies. The most common strategy
utilizes such notations only implicitly via an incremental use of environments.
This approach does not allow the smaller substitution steps to be intermingled
with other operations of interest on lambda-terms. However, a naive strategy
explicitly using such notations can also be costly: each use of the
substitution propagation rules causes the creation of a new structure on the
heap that is often discarded in the immediately following step. There is thus a
tradeoff between these two approaches. This thesis describes the actual
realization of the two approaches, discusses their tradeoffs based on this and,
finally, offers an amalgamated approach that utilizes recursion in rewrite rule
application but also suspends substitution operations where necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405076</id><created>2004-05-22</created><authors><author><keyname>Sakama</keyname><forenames>Chiaki</forenames></author><author><keyname>Inoue</keyname><forenames>Katsumi</forenames></author></authors><title>An Abductive Framework For Computing Knowledge Base Updates</title><categories>cs.DB</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 3, no. 6,
  2003</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 3, no. 6, 2003</journal-ref><abstract>  This paper introduces an abductive framework for updating knowledge bases
represented by extended disjunctive programs. We first provide a simple
transformation from abductive programs to update programs which are logic
programs specifying changes on abductive hypotheses. Then, extended abduction,
which was introduced by the same authors as a generalization of traditional
abduction, is computed by the answer sets of update programs. Next, different
types of updates, view updates and theory updates are characterized by
abductive programs and computed by update programs. The task of consistency
restoration is also realized as special cases of these updates. Each update
problem is comparatively assessed from the computational complexity viewpoint.
The result of this paper provides a uniform framework for different types of
knowledge base updates, and each update is computed using existing procedures
of logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405077</id><created>2004-05-22</created><updated>2006-07-28</updated><authors><author><keyname>Lubachevsky</keyname><forenames>Boris D.</forenames></author></authors><title>Fast Simulation of Multicomponent Dynamic Systems</title><categories>cs.DS cond-mat.mtrl-sci cs.DC</categories><comments>38 pages, 9 figures</comments><acm-class>I.6.8; G.3; G.4; J.2; J.4</acm-class><journal-ref>Bell Labs Technical Journal, Vol.5, No.2, April-June 2000,
  pp.134-156</journal-ref><abstract>  A computer simulation has to be fast to be helpful, if it is employed to
study the behavior of a multicomponent dynamic system. This paper discusses
modeling concepts and algorithmic techniques useful for creating such fast
simulations. Concrete examples of simulations that range from econometric
modeling to communications to material science are used to illustrate these
techniques and concepts. The algorithmic and modeling methods discussed include
event-driven processing, ``anticipating'' data structures, and ``lazy''
evaluation, Poisson dispenser, parallel processing by cautious advancements and
by synchronous relaxations. The paper gives examples of how these techniques
and models are employed in assessing efficiency of capacity management methods
in wireless and wired networks, in studies of magnetization, crystalline
structure, and sediment formation in material science, in studies of
competition in economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405078</id><created>2004-05-23</created><authors><author><keyname>Schlee</keyname><forenames>Max</forenames></author><author><keyname>Vanderdonckt</keyname><forenames>Jean</forenames></author></authors><title>Generative Programming of Graphical User Interfaces</title><categories>cs.HC</categories><comments>4 pages, 12 figures, ACM Conference on Visual Interfaces AVI'2004</comments><acm-class>D.2.1, D.2.2, H.2.4, I.3.6</acm-class><abstract>  Generative Programming (GP) is a computing paradigm allowing automatic
creation of entire software families utilizing the configuration of elementary
and reusable components. GP can be projected on different technologies, e.g.
C++-templates, Java-Beans, Aspect-Oriented Programming (AOP), or Frame
technology. This paper focuses on Frame Technology, which aids the possible
implementation and completion of software components. The purpose of this paper
is to introduce the GP paradigm in the area of GUI application generation. It
demonstrates how automatically customized executable applications with GUI
parts can be generated from an abstract specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405079</id><created>2004-05-23</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Higher-Order Concurrent Win32 Programming</title><categories>cs.PL</categories><comments>10 pages; appeared in Proceedings of the 3rd Usenix Windows NT
  Symposium, Seattle, pp. 113-122, 1999</comments><acm-class>D.1.1;D.3.3;H.5.2</acm-class><abstract>  We present a concurrent framework for Win32 programming based on Concurrent
ML, a concurrent language with higher-order functions, static typing,
lightweight threads and synchronous communication channels. The key points of
the framework are the move from an event loop model to a threaded model for the
processing of window messages, and the decoupling of controls notifications
from the system messages. This last point allows us to derive a general way of
writing controls that leads to easy composition, and can accommodate ActiveX
Controls in a transparent way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405080</id><created>2004-05-23</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Reactive Programming in Standard ML</title><categories>cs.PL</categories><comments>11 pages; appeared in Proceedings of the IEEE International
  Conference on Computer Languages (ICCL'98), pp. 48-57, 1998</comments><acm-class>D.1.1;D.3.3</acm-class><abstract>  Reactive systems are systems that maintain an ongoing interaction with their
environment, activated by receiving input events from the environment and
producing output events in response. Modern programming languages designed to
program such systems use a paradigm based on the notions of instants and
activations. We describe a library for Standard ML that provides basic
primitives for programming reactive systems. The library is a low-level system
upon which more sophisticated reactive behaviors can be built, which provides a
convenient framework for prototyping extensions to existing reactive languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405081</id><created>2004-05-23</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>An Analysis of Lambek's Production Machines</title><categories>cs.LO</categories><comments>13 pages, 1 figure</comments><acm-class>F.1.1;F.4.2</acm-class><journal-ref>RAIRO Informatique Theorique et Applications, 31 (5), pp. 483-497,
  1997</journal-ref><abstract>  Lambek's production machines may be used to generate and recognize sentences
in a subset of the language described by a production grammar. We determine in
this paper the subset of the language of a grammar generated and recognized by
such machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405082</id><created>2004-05-23</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author><author><keyname>Meijer</keyname><forenames>Erik</forenames></author><author><keyname>Oliva</keyname><forenames>Dino</forenames></author></authors><title>Aspects de la Programmation d'Applications Win32 avec un Langage
  Fonctionnel</title><categories>cs.PL</categories><comments>In french, 25 pages, 3 figures. Appeared in the Proceedings of the
  &quot;Journees Francophones des Langages Applicatifs&quot; (JFLA'99), 1999</comments><acm-class>D.1.1;D.3.3;H.5.2</acm-class><abstract>  A useful programming language needs to support writing programs that take
advantage of services and communication mechanisms supplied by the operating
system. We examine the problem of programming native Win32 applications under
Windows with Standard ML. We introduce an framework based on the IDL interface
language et a minimal foreign-functions interface to explore the Win32 API et
COM in the context of Standard ML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405083</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405083</id><created>2004-05-23</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>The Design of a COM-Oriented Module System</title><categories>cs.PL</categories><comments>15 pages; appeared in Proceedings of the Joint Modular Languages
  Conference (JMLC'00). LNCS 1897, pp. 104-118, 2000</comments><acm-class>D.2.2;D.3.3</acm-class><abstract>  We present in this paper the preliminary design of a module system based on a
notion of components such as they are found in COM. This module system is
inspired from that of Standard ML, and features first-class instances of
components, first-class interfaces, and interface-polymorphic functions, as
well as allowing components to be both imported from the environment and
exported to the environment using simple mechanisms. The module system
automates the memory management of interfaces and hides the IUnknown interface
and QueryInterface mechanisms from the programmer, favoring instead a
higher-level approach to handling interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405084</id><created>2004-05-23</created><authors><author><keyname>Fisher</keyname><forenames>Kathleen</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author><author><keyname>Reppy</keyname><forenames>John</forenames></author></authors><title>A Framework for Interoperability</title><categories>cs.PL</categories><comments>16 pages; 1 figure; appeared in Proceedings of the 1st International
  Workshop on Multi-Language Infrastructure and Interoperability (BABEL'01),
  ENTCS 59(1), 2001</comments><acm-class>D.2.12;D.3.4</acm-class><abstract>  Practical implementations of high-level languages must provide access to
libraries and system services that have APIs specified in a low-level language
(usually C). An important characteristic of such mechanisms is the
foreign-interface policy that defines how to bridge the semantic gap between
the high-level language and C. For example, IDL-based tools generate code to
marshal data into and out of the high-level representation according to user
annotations. The design space of foreign-interface policies is large and there
are pros and cons to each approach. Rather than commit to a particular policy,
we choose to focus on the problem of supporting a gamut of interoperability
policies. In this paper, we describe a framework for language interoperability
that is expressive enough to support very efficient implementations of a wide
range of different foreign-interface policies. We describe two tools that
implement substantially different policies on top of our framework and present
benchmarks that demonstrate their efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405085</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405085</id><created>2004-05-24</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author><author><keyname>Panangaden</keyname><forenames>Prakash</forenames></author></authors><title>On the Expressive Power of First-Order Boolean Functions in PCF</title><categories>cs.PL</categories><comments>23 pages</comments><acm-class>F.3.2</acm-class><journal-ref>Theoretical Computer Science 266(1-2), pp. 543-567, 2001</journal-ref><abstract>  Recent results of Bucciarelli show that the semilattice of degrees of
parallelism of first-order boolean functions in PCF has both infinite chains
and infinite antichains. By considering a simple subclass of Sieber's
sequentiality relations, we identify levels in the semilattice and derive
inexpressibility results concerning functions on different levels. This allows
us to further explore the structure of the semilattice of degrees of
parallelism: we identify semilattices characterized by simple level properties,
and show the existence of new infinite hierarchies which are in a certain sense
natural with respect to the levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405086</id><created>2004-05-24</created><authors><author><keyname>Zhakhovskii</keyname><forenames>Vasilii</forenames></author><author><keyname>Nishihara</keyname><forenames>Katsunobu</forenames></author><author><keyname>Fukuda</keyname><forenames>Yuko</forenames></author><author><keyname>Shimojo</keyname><forenames>Shinji</forenames></author></authors><title>A New Dynamical Domain Decomposition Method for Parallel Molecular
  Dynamics Simulation on Grid</title><categories>cs.DC</categories><report-no>Annual Progress Report 2003, Institute of Laser Engineering,Osaka
  University (2004)</report-no><abstract>  We develop a new Lagrangian material particle -- dynamical domain
decomposition method (MPD^3) for large scale parallel molecular dynamics (MD)
simulation of nonstationary heterogeneous systems on a heterogeneous computing
net. MPD^3 is based on Voronoi decomposition of simulated matter. The map of
Voronoi polygons is known as the Dirichlet tessellation and used for grid
generation in computational fluid dynamics. From the hydrodynamics point of
view the moving Voronoi polygon looks as a material particle (MP). MPs can
exchange particles and information. To balance heterogeneous computing
conditions the MP centers should be dependent on timing data. We propose a
simple and efficient iterative algorithm which based on definition of the
timing-dependent balancing displacement of MP center for next simulation step.
  The MPD^3 program was tested in various computing environments and physical
problems. We have demonstrated that MPD^3 is a high-adaptive decomposition
algorithm for MD simulation. It was shown that the well-balanced decomposition
can result from dynamical Voronoi polygon tessellation. One would expect the
similar approach can be successfully applied for other particle methods like
Monte Carlo, particle-in-cell, and smooth-particle-hydrodynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405087</id><created>2004-05-24</created><authors><author><keyname>Rogulin</keyname><forenames>D</forenames></author><author><keyname>Estrella</keyname><forenames>F</forenames></author><author><keyname>Hauer</keyname><forenames>T</forenames></author><author><keyname>McClatchey</keyname><forenames>R</forenames></author><author><keyname>Solomonides</keyname><forenames>T</forenames></author></authors><title>A Grid Information Infrastructure for Medical Image Analysis</title><categories>cs.DB cs.DC</categories><comments>9 pages, 5 figures</comments><acm-class>H2.4;J.3</acm-class><abstract>  The storage and manipulation of digital images and the analysis of the
information held in those images are essential requirements for next-generation
medical information systems. The medical community has been exploring
collaborative approaches for managing image data and exchanging knowledge and
Grid technology [1] is a promising approach to enabling distributed analysis
across medical institutions and for developing new collaborative and
cooperative approaches for image analysis without the necessity for clinicians
to co-locate. The EU-funded MammoGrid project [2] is one example of this and it
aims to develop a Europe-wide database of mammograms to support effective
co-working between healthcare professionals across the EU. The MammoGrid
prototype comprises a high-quality clinician visualization workstation (for
data acquisition and inspection), a DICOM-compliant interface to a set of
medical services (annotation, security, image analysis, data storage and
querying services) residing on a so-called Grid-box and secure access to a
network of other Grid-boxes connected through Grid middleware. One of the main
deliverables of the project is a Grid-enabled infrastructure that manages
federated mammogram databases across Europe. This paper outlines the MammoGrid
Information Infrastructure (MII) for meta-data analysis and knowledge discovery
in the medical imaging domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405088</id><created>2004-05-24</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author><author><keyname>Dahl</keyname><forenames>Veronica</forenames></author></authors><title>High-Level Networking With Mobile Code And First Order AND-Continuations</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 3,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 3, 2001</journal-ref><abstract>  We describe a scheme for moving living code between a set of distributed
processes coordinated with unification based Linda operations, and its
application to building a comprehensive Logic programming based Internet
programming framework. Mobile threads are implemented by capturing first order
continuations in a compact data structure sent over the network. Code is
fetched lazily from its original base turned into a server as the continuation
executes at the remote site. Our code migration techniques, in combination with
a dynamic recompilation scheme, ensure that heavily used code moves up smoothly
on a speed hierarchy while volatile dynamic code is kept in a quickly updatable
form. Among the examples, we describe how to build programmable client and
server components (Web servers, in particular) and mobile agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405089</id><created>2004-05-24</created><authors><author><keyname>Simon</keyname><forenames>Axel</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author></authors><title>Convex Hull of Planar H-Polyhedra</title><categories>cs.CG</categories><acm-class>I.3.5; I.3.6; F.3.1</acm-class><journal-ref>International Journal of Computer Mathematics, 81(4):259-271, 2004</journal-ref><abstract>  Suppose $&lt;A_i, \vec{c}_i&gt;$ are planar (convex) H-polyhedra, that is, $A_i \in
\mathbb{R}^{n_i \times 2}$ and $\vec{c}_i \in \mathbb{R}^{n_i}$. Let $P_i =
\{\vec{x} \in \mathbb{R}^2 \mid A_i\vec{x} \leq \vec{c}_i \}$ and $n = n_1 +
n_2$. We present an $O(n \log n)$ algorithm for calculating an H-polyhedron
$&lt;A, \vec{c}&gt;$ with the smallest $P = \{\vec{x} \in \mathbb{R}^2 \mid A\vec{x}
\leq \vec{c} \}$ such that $P_1 \cup P_2 \subseteq P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405090</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405090</id><created>2004-05-24</created><authors><author><keyname>Maher</keyname><forenames>Michael J.</forenames></author></authors><title>Propositional Defeasible Logic has Linear Complexity</title><categories>cs.AI</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 6,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 6, 2001</journal-ref><abstract>  Defeasible logic is a rule-based nonmonotonic logic, with both strict and
defeasible rules, and a priority relation on rules. We show that inference in
the propositional form of the logic can be performed in linear time. This
contrasts markedly with most other propositional nonmonotonic logics, in which
inference is intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405091</id><created>2004-05-24</created><authors><author><keyname>Caseau</keyname><forenames>Yves</forenames></author><author><keyname>Josset</keyname><forenames>Francois-Xavier</forenames></author><author><keyname>Laburthe</keyname><forenames>Francois</forenames></author></authors><title>CLAIRE: Combining Sets, Search And Rules To Better Express Algorithms</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 2, no. 6,
  2002</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 2, no. 6, 2002</journal-ref><abstract>  This paper presents a programming language which includes paradigms that are
usually associated with declarative languages, such as sets, rules and search,
into an imperative (functional) language. Although these paradigms are
separately well known and are available under various programming environments,
the originality of the CLAIRE language comes from the tight integration, which
yields interesting run-time performances, and from the richness of this
combination, which yields new ways in which to express complex algorithmic
patterns with few elegant lines. To achieve the opposite goals of a high
abstraction level (conciseness and readability) and run-time performance
(CLAIRE is used as a C++ preprocessor), we have developed two kinds of
compiler: first, a pattern pre-processor handles iterations over both concrete
and abstract sets (data types and program fragments), in a completely
user-extensible manner; secondly, an inference compiler transforms a set of
logical rules into a set of functions (demons that are used through procedural
attachment).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405092</id><created>2004-05-24</created><authors><author><keyname>Caseau</keyname><forenames>Yves</forenames></author><author><keyname>Silverstein</keyname><forenames>Glenn</forenames></author><author><keyname>Laburthe</keyname><forenames>Francois</forenames></author></authors><title>Learning Hybrid Algorithms for Vehicle Routing Problems</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 6,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 6, 2001</journal-ref><abstract>  This paper presents a generic technique for improving hybrid algorithms
through the discovery of and tuning of meta-heuristics. The idea is to
represent a family of push/pull heuristics that are based upon inserting and
removing tasks in a current solution, with an algebra. We then let a learning
algorithm search for the best possible algebraic term, which represents a
hybrid algorithm for a given set of problems and an optimization criterion. In
a previous paper, we described this algebra in detail and provided a set of
preliminary results demonstrating the utility of this approach, using vehicle
routing with time windows (VRPTW) as a domain example. In this paper we expand
upon our results providing a more robust experimental framework and learning
algorithms, and report on some new results using the standard Solomon
benchmarks. In particular, we show that our learning algorithm is able to
achieve results similar to the best-published algorithms using only a fraction
of the CPU time. We also show that the automatic tuning of the best hybrid
combination of such techniques yields a better solution than hand tuning, with
considerably less effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405093</id><created>2004-05-25</created><updated>2004-10-20</updated><authors><author><keyname>Perlibakas</keyname><forenames>Vytautas</forenames></author></authors><title>Computerized Face Detection and Recognition</title><categories>cs.CV</categories><comments>PhD dissertation summary. 35 pages, 12 figures, 7 tables</comments><acm-class>I.4.8; I.5</acm-class><abstract>  This publication presents methods for face detection, analysis and
recognition: fast normalized cross-correlation (fast correlation coefficient)
between multiple templates based face pre-detection method, method for
detection of exact face contour based on snakes and Generalized Gradient Vector
Flow field, method for combining recognition algorithms based on Cumulative
Match Characteristics in order to increase recognition speed and accuracy, and
face recognition method based on Principal Component Analysis of the Wavelet
Packet Decomposition allowing to use PCA - based recognition method with large
number of training images. For all the methods are presented experimental
results and comparisons of speed and accuracy with large face databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405094</id><created>2004-05-25</created><authors><author><keyname>Mielik&#xe4;inen</keyname><forenames>Taneli</forenames></author><author><keyname>Ukkonen</keyname><forenames>Esko</forenames></author></authors><title>The Complexity of Maximum Matroid-Greedoid Intersection and Weighted
  Greedoid Maximization</title><categories>cs.DS</categories><report-no>Report C-2004-2, Department of Computer Science, University of
  Helsinki</report-no><acm-class>F.2.2</acm-class><abstract>  The maximum intersection problem for a matroid and a greedoid, given by
polynomial-time oracles, is shown $NP$-hard by expressing the satisfiability of
boolean formulas in 3-conjunctive normal form as such an intersection. The
corresponding approximation problems are shown $NP$-hard for certain
approximation performance bounds. Moreover, some natural parameterized variants
of the problem are shown $W[P]$-hard. The results are in contrast with the
maximum matroid-matroid intersection which is solvable in polynomial time by an
old result of Edmonds. We also prove that it is $NP$-hard to approximate the
weighted greedoid maximization within $2^{n^{O(1)}}$ where $n$ is the size of
the domain of the greedoid.
  A preliminary version ``The Complexity of Maximum Matroid-Greedoid
Intersection'' appeared in Proc. FCT 2001, LNCS 2138, pp. 535--539,
Springer-Verlag 2001.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405095</id><created>2004-05-25</created><authors><author><keyname>Ma</keyname><forenames>Lili</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Moore</keyname><forenames>Kevin L.</forenames></author></authors><title>Blind Detection and Compensation of Camera Lens Geometric Distortions</title><categories>cs.CV</categories><comments>6 pages, 4 figures, 2 tables</comments><acm-class>I 4.1</acm-class><journal-ref>SIAM Imaging Science, 2004</journal-ref><abstract>  This paper presents a blind detection and compensation technique for camera
lens geometric distortions. The lens distortion introduces higher-order
correlations in the frequency domain and in turn it can be detected using
higher-order spectral analysis tools without assuming any specific calibration
target. The existing blind lens distortion removal method only considered a
single-coefficient radial distortion model. In this paper, two coefficients are
considered to model approximately the geometric distortion. All the models
considered have analytical closed-form inverse formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405096</id><created>2004-05-26</created><authors><author><keyname>Chashkov</keyname><forenames>Yuriy A.</forenames></author></authors><title>Developing Intellectual Network Management Facilities by Means of
  Pattern Recognition Theory</title><categories>cs.NI</categories><comments>7 pages, 3 figures</comments><report-no>MIEM-02-06</report-no><acm-class>C.2.3</acm-class><abstract>  In this paper considered question of using pattern recognition methods in
network equipment state identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405097</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405097</id><created>2004-05-26</created><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>A Coalgebraic Approach to Kleene Algebra with Tests</title><categories>cs.LO cs.PL</categories><comments>21 pages, 1 figure; preliminary version appeared in Proc. Workshop on
  Coalgebraic Methods in Computer Science (CMCS'03)</comments><acm-class>F.3.1;I.1.1;I.1.3</acm-class><journal-ref>Theoretical Computer Science, 327 (1-2), 23-44 (2004)</journal-ref><abstract>  Kleene algebra with tests is an extension of Kleene algebra, the algebra of
regular expressions, which can be used to reason about programs. We develop a
coalgebraic theory of Kleene algebra with tests, along the lines of the
coalgebraic theory of regular expressions based on deterministic automata.
Since the known automata-theoretic presentation of Kleene algebra with tests
does not lend itself to a coalgebraic theory, we define a new interpretation of
Kleene algebra with tests expressions and a corresponding automata-theoretic
presentation. One outcome of the theory is a coinductive proof principle, that
can be used to establish equivalence of our Kleene algebra with tests
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405098</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405098</id><created>2004-05-26</created><updated>2006-08-03</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>A Logic for Reasoning about Evidence</title><categories>cs.AI cs.LO</categories><comments>34 pages. A preliminary version appeared in Proc. 19th Conference on
  Uncertainty in Artificial Intelligence (UAI'03)</comments><acm-class>I.2.4; F.2.1</acm-class><journal-ref>Journal of Artificial Intelligence Research 26, pp. 1-34, 2006</journal-ref><abstract>  We introduce a logic for reasoning about evidence that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405099</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405099</id><created>2004-05-26</created><authors><author><keyname>Liang</keyname><forenames>Wang</forenames></author><author><keyname>YiPing</keyname><forenames>Guo</forenames></author><author><keyname>Ming</keyname><forenames>Fang</forenames></author></authors><title>Web search engine based on DNS</title><categories>cs.NI cs.IR</categories><comments>9 pages,2 figures</comments><acm-class>H.3.3;H.3.7;C.2.2</acm-class><abstract>  Now no web search engine can cover more than 60 percent of all the pages on
Internet. The update interval of most pages database is almost one month. This
condition hasn't changed for many years. Converge and recency problems have
become the bottleneck problem of current web search engine. To solve these
problems, a new system, search engine based on DNS is proposed in this paper.
This system adopts the hierarchical distributed architecture like DNS, which is
different from any current commercial search engine. In theory, this system can
cover all the web pages on Internet. Its update interval could even be one day.
The original idea, detailed content and implementation of this system all are
introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405100</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405100</id><created>2004-05-26</created><authors><author><keyname>Fages</keyname><forenames>Francois</forenames></author><author><keyname>Coquery</keyname><forenames>Emmanuel</forenames></author></authors><title>Typing constraint logic programs</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 6,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 6, 2001</journal-ref><abstract>  We present a prescriptive type system with parametric polymorphism and
subtyping for constraint logic programs. The aim of this type system is to
detect programming errors statically. It introduces a type discipline for
constraint logic programs and modules, while maintaining the capabilities of
performing the usual coercions between constraint domains, and of typing
meta-programming predicates, thanks to the flexibility of subtyping. The
property of subject reduction expresses the consistency of a prescriptive type
system w.r.t. the execution model: if a program is &quot;well-typed&quot;, then all
derivations starting from a &quot;well-typed&quot; goal are again &quot;well-typed&quot;. That
property is proved w.r.t. the abstract execution model of constraint
programming which proceeds by accumulation of constraints only, and w.r.t. an
enriched execution model with type constraints for substitutions. We describe
our implementation of the system for type checking and type inference. We
report our experimental results on type checking ISO-Prolog, the (constraint)
libraries of Sicstus Prolog and other Prolog programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405101</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405101</id><created>2004-05-26</created><authors><author><keyname>Genaim</keyname><forenames>Samir</forenames></author><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Howe</keyname><forenames>Jacob M.</forenames></author></authors><title>Worst-Case Groundness Analysis Using Definite Boolean Functions</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 5,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 5, 2001</journal-ref><abstract>  This note illustrates theoretical worst-case scenarios for groundness
analyses obtained through abstract interpretation over the abstract domains of
definite (Def) and positive (Pos) Boolean functions. For Def, an example is
given for which any Def-based abstract interpretation for groundness analysis
follows a chain which is exponential in the number of argument positions as
well as in the number of clauses but sub-exponential in the size of the
program. For Pos, we strengthen a previous result by illustrating an example
for which any Pos-based abstract interpretation for groundness analysis follows
a chain which is exponential in the size of the program. It remains an open
problem to determine if the worst case for Def is really as bad as that for
Pos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405102</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405102</id><created>2004-05-26</created><authors><author><keyname>Lopez-Fraguas</keyname><forenames>Francisco Javier</forenames></author><author><keyname>Sanchez-Hernandez</keyname><forenames>Jaime</forenames></author></authors><title>A Proof Theoretic Approach to Failure in Functional Logic Programming</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 4, no.
  1&amp;2, 2004</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 4, no. 1&amp;2, 2004</journal-ref><abstract>  How to extract negative information from programs is an important issue in
logic programming. Here we address the problem for functional logic programs,
from a proof-theoretic perspective. The starting point of our work is CRWL
(Constructor based ReWriting Logic), a well established theoretical framework
for functional logic programming, whose fundamental notion is that of
non-strict non-deterministic function. We present a proof calculus, CRWLF,
which is able to deduce negative information from CRWL-programs. In particular,
CRWLF is able to prove finite failure of reduction within CRWL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405103</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405103</id><created>2004-05-26</created><authors><author><keyname>Lazic</keyname><forenames>R. S.</forenames></author><author><keyname>Newcomb</keyname><forenames>T. C.</forenames></author><author><keyname>Roscoe</keyname><forenames>A. W.</forenames></author></authors><title>On model checking data-independent systems with arrays without reset</title><categories>cs.LO</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 4, no.
  5&amp;6, 2004</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 4, no. 5&amp;6, 2004</journal-ref><abstract>  A system is data-independent with respect to a data type X iff the operations
it can perform on values of type X are restricted to just equality testing. The
system may also store, input and output values of type X. We study model
checking of systems which are data-independent with respect to two distinct
type variables X and Y, and may in addition use arrays with indices from X and
values from Y . Our main interest is the following parameterised model-checking
problem: whether a given program satisfies a given temporal-logic formula for
all non-empty nite instances of X and Y . Initially, we consider instead the
abstraction where X and Y are infinite and where partial functions with finite
domains are used to model arrays. Using a translation to data-independent
systems without arrays, we show that the u-calculus model-checking problem is
decidable for these systems. From this result, we can deduce properties of all
systems with finite instances of X and Y . We show that there is a procedure
for the above parameterised model-checking problem of the universal fragment of
the u-calculus, such that it always terminates but may give false negatives. We
also deduce that the parameterised model-checking problem of the universal
disjunction-free fragment of the u-calculus is decidable. Practical motivations
for model checking data-independent systems with arrays include verification of
memory and cache systems, where X is the type of memory addresses, and Y the
type of storable values. As an example we verify a fault-tolerant memory
interface over a set of unreliable memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405104</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405104</id><created>2004-05-27</created><authors><author><keyname>He</keyname><forenames>Yuguo</forenames></author></authors><title>Knowledge Reduction and Discovery based on Demarcation Information</title><categories>cs.LG cs.DB cs.IT math.IT</categories><comments>93 pages with 46 figures</comments><acm-class>H.2.8;I.2.6; H.1.1;I.5.2</acm-class><abstract>  Knowledge reduction, includes attribute reduction and value reduction, is an
important topic in rough set literature. It is also closely relevant to other
fields, such as machine learning and data mining. In this paper, an algorithm
called TWI-SQUEEZE is proposed. It can find a reduct, or an irreducible
attribute subset after two scans. Its soundness and computational complexity
are given, which show that it is the fastest algorithm at present. A measure of
variety is brought forward, of which algorithm TWI-SQUEEZE can be regarded as
an application. The author also argues the rightness of this measure as a
measure of information, which can make it a unified measure for
&quot;differentiation, a concept appeared in cognitive psychology literature. Value
reduction is another important aspect of knowledge reduction. It is interesting
that using the same algorithm we can execute a complete value reduction
efficiently. The complete knowledge reduction, which results in an irreducible
table, can therefore be accomplished after four scans of table. The byproducts
of reduction are two classifiers of different styles. In this paper, various
cases and models will be discussed to prove the efficiency and effectiveness of
the algorithm. Some topics, such as how to integrate user preference to find a
local optimal attribute subset will also be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405105</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405105</id><created>2004-05-27</created><authors><author><keyname>Nadeem</keyname><forenames>Muhammad</forenames><affiliation>Szabist, Karachi</affiliation></author><author><keyname>Laghari</keyname><forenames>Javaid R.</forenames><affiliation>Szabist, Karachi</affiliation></author></authors><title>Study of Pakistan Election System as Intelligent e-Election</title><categories>cs.CY</categories><comments>6 pages</comments><acm-class>H.2.8</acm-class><abstract>  The proposed election system lies in ensuring that it is transparent and
impartial.Thus while the electoral system may vary from country to country, It
has to take into account the peculiarities of every society while at the same
time incorporating remedies to problems prevailing in the system.
  The Electoral process expressed serious concerns regarding the independence
of the Election Commission of Pakistan, the restrictions on political parties
and their candidates, the misuse of state resources, some unbalanced coverage
in the state media, deficiencies in the compilation of the voting register and
significant problems relating to the provision of ID cards.
  The holding of a general election does not in itself guarantee the
restoration of democracy. The unjustified interference with electoral
arrangements, as detailed above, irrespective of the alleged motivation,
resulted in serious flaws being inflicted on the electoral process.
Additionally, questions remain as to whether or not there will be a full
transfer of power from a military to civilian administration.
  The Independent study research has following modules:
  Login/Subscription Module Candidate Subscription Module Vote casting Module
Administration Module Intelligent decision data analysis Module
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405106</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405106</id><created>2004-05-27</created><authors><author><keyname>Ches&#xf1;evar</keyname><forenames>Carlos Iv&#xe1;n</forenames></author><author><keyname>Simari</keyname><forenames>Guillermo Ricardo</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Alejandro Javier</forenames></author></authors><title>Pruning Search Space in Defeasible Argumentation</title><categories>cs.AI</categories><comments>11 pages</comments><journal-ref>Proc. of the Workshop on Advances and Trends in Search in
  Artificial Intelligence, pp.40-47. International Conf. of the Chilean Society
  in Computer Science, Santiago, Chile, 2000</journal-ref><abstract>  Defeasible argumentation has experienced a considerable growth in AI in the
last decade. Theoretical results have been combined with development of
practical applications in AI &amp; Law, Case-Based Reasoning and various
knowledge-based systems. However, the dialectical process associated with
inference is computationally expensive. This paper focuses on speeding up this
inference process by pruning the involved search space. Our approach is
twofold. On one hand, we identify distinguished literals for computing defeat.
On the other hand, we restrict ourselves to a subset of all possible
conflicting arguments by introducing dialectical constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405107</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405107</id><created>2004-05-27</created><authors><author><keyname>Ches&#xf1;evar</keyname><forenames>Carlos Iv&#xe1;n</forenames></author><author><keyname>Simari</keyname><forenames>Guillermo Ricardo</forenames></author></authors><title>A Framework for Combining Defeasible Argumentation with Labeled
  Deduction</title><categories>cs.AI cs.SC</categories><comments>15 pages, presented at CMSRA Workshop 2003. Buenos Aires, Argentina</comments><journal-ref>In &quot;Computer Modeling of Scientific Reasoning&quot; (C.Delrieux,
  J.Legris, Eds.). Pp. 43-56, Ed. Ediuns, Argentina, 2003. ISBN 987-89281-89-6</journal-ref><abstract>  In the last years, there has been an increasing demand of a variety of
logical systems, prompted mostly by applications of logic in AI and other
related areas. Labeled Deductive Systems (LDS) were developed as a flexible
methodology to formalize such a kind of complex logical systems. Defeasible
argumentation has proven to be a successful approach to formalizing commonsense
reasoning, encompassing many other alternative formalisms for defeasible
reasoning. Argument-based frameworks share some common notions (such as the
concept of argument, defeater, etc.) along with a number of particular features
which make it difficult to compare them with each other from a logical
viewpoint. This paper introduces LDSar, a LDS for defeasible argumentation in
which many important issues concerning defeasible argumentation are captured
within a unified logical framework. We also discuss some logical properties and
extensions that emerge from the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405108</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405108</id><created>2004-05-27</created><authors><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author></authors><title>&quot;User Interfaces&quot; and the Social Negotiation of Availability</title><categories>cs.HC</categories><comments>3 pages</comments><acm-class>H.5.2; H.4.3; H.1.2</acm-class><journal-ref>Workshop on Forecasting Presence and Availability, ACM SIGCHI
  Conf. on Human Factors in Computing Systems (CHI 2004), Vienna, Austria, Apr.
  2004.</journal-ref><abstract>  In current presence or availability systems, the method of presenting a
user's state often supposes an instantaneous notion of that state - for
example, a visualization is rendered or an inference is made about the
potential actions that might be consistent with a user's state. Drawing on
observational research on the use of existing communication technology, we
argue (as have others in the past) that determination of availability is often
a joint process, and often one that takes the form of a negotiation (whether
implicit or explicit). We briefly describe our current research on applying
machine learning to infer degrees of conversational engagement from observed
conversational behavior. Such inferences can be applied to facilitate the
implicit negotiation of conversational engagement - in effect, helping users to
weave together the act of contact with the act of determining availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405109</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405109</id><created>2004-05-27</created><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author></authors><title>Conversation Analysis and the User Experience</title><categories>cs.HC</categories><comments>4 pages</comments><acm-class>H.5.2; H.1.2</acm-class><journal-ref>Workshop on Exploring Experience Methods Across Disciplines, ACM
  SIGCHI Conf. on Human Factors in Computing Systems (CHI 2004), Vienna,
  Austria, Apr. 2004.</journal-ref><abstract>  We provide two case studies in the application of ideas drawn from
conversation analysis to the design of technologies that enhance the experience
of human conversation. We first present a case study of the design of an
electronic guidebook, focusing on how conversation analytic principles played a
role in the design process. We then discuss how the guidebook project has
inspired our continuing work in social, mobile audio spaces. In particular, we
describe some as yet unrealized concepts for adaptive audio spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405110</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405110</id><created>2004-05-28</created><authors><author><keyname>Ananthraman</keyname><forenames>Gopal</forenames></author></authors><title>An analysis of a bounded resource search puzzle</title><categories>cs.DS cs.DM</categories><comments>4 Pages</comments><acm-class>F.2.2</acm-class><abstract>  Consider the commonly known puzzle, given $k$ glass balls, find an optimal
algorithm to determine the lowest floor of a building of $n$ floors from which
a thrown glass ball will break. This puzzle was originally posed in its
original form in \cite{focs1980}and was later cited in the book \cite{algthc}.
There are several internet sites that presents this puzzle and its solution to
the special case of $k=2$ balls. This is the first such analysis of the puzzle
in its general form. Several variations of this puzzle have been studied with
applications in Network Loading \cite{cgstctl} which analyzes a case similar to
a scenario where an adversary is changing the lowest floor with time. Although
the algorithm specified in \cite{algthc} solves the problem, it is not an
efficient algorithm. In this paper another algorithm for the same problem is
analyzed. It is shown that if $m$ is the minimum number of attempts required
then for $k \geq m$ we have $m = \log (n+1)$ and for $k &lt; m$ we have, $1 +
\sum_{i=1}^{k}{{m-1}\choose{i}} &lt; n \leq \sum_{i=1}^{k}{{m}\choose{i}}$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405111</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405111</id><created>2004-05-28</created><updated>2004-11-27</updated><authors><author><keyname>Giuli</keyname><forenames>T. J.</forenames></author><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>Attrition Defenses for a Peer-to-Peer Digital Preservation System</title><categories>cs.CR</categories><comments>14 pages, 8 figures. version 2: Reworked the paper according to
  reviews. Expanded the evaluation section with experiments with more AUs</comments><acm-class>C.2.4; D.4.6</acm-class><abstract>  In peer-to-peer systems, attrition attacks include both traditional,
network-level denial of service attacks as well as application-level attacks in
which malign peers conspire to waste loyal peers' resources. We describe
several defenses for LOCKSS, a peer-to-peer digital preservation system, that
help ensure that application-level attacks even from powerful adversaries are
less effective than simple network-level attacks, and that network-level
attacks must be intense, wide-spread, and prolonged to impair the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405112</identifier>
 <datestamp>2008-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405112</id><created>2004-05-31</created><updated>2006-06-19</updated><authors><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Suderman</keyname><forenames>Matthew</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Really Straight Graph Drawings</title><categories>cs.DM cs.CG</categories><comments>This paper has been withdrawn by the authors. It has been superseeded
  by the papers: &quot;Drawings of Planar Graphs with Few Slopes and Segments&quot;
  (math/0606450) and &quot;Graph Drawings with Few Slopes&quot; (math/0606446). A
  preliminary version appeared in the Graph Drawing 2004 conference</comments><abstract>  This paper has been withdrawn by the authors. It has been replaced by the
papers: &quot;Drawings of Planar Graphs with Few Slopes and Segments&quot; (math/0606450)
and &quot;Graph Drawings with Few Slopes&quot; (math/0606446).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0405113</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0405113</id><created>2004-05-31</created><updated>2004-06-01</updated><authors><author><keyname>Severe</keyname><forenames>Andrea</forenames></author></authors><title>A proposal to design expert system for the calculations in the domain of
  QFT</title><categories>cs.AI</categories><abstract>  Main purposes of the paper are followings: 1) To show examples of the
calculations in domain of QFT via ``derivative rules'' of an expert system; 2)
To consider advantages and disadvantage that technology of the calculations; 3)
To reflect about how one would develop new physical theories, what knowledge
would be useful in their investigations and how this problem can be connected
with designing an expert system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406001</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406001</id><created>2004-06-01</created><authors><author><keyname>Nguyen</keyname><forenames>Kim-Chi</forenames></author><author><keyname>Van Assche</keyname><forenames>Gilles</forenames></author><author><keyname>Cerf</keyname><forenames>Nicolas J.</forenames></author></authors><title>Side-Information Coding with Turbo Codes and its Application to Quantum
  Key Distribution</title><categories>cs.IT cs.CR math.IT quant-ph</categories><comments>3 pages, submitted to ISITA 2004</comments><acm-class>E.3</acm-class><abstract>  Turbo coding is a powerful class of forward error correcting codes, which can
achieve performances close to the Shannon limit. The turbo principle can be
applied to the problem of side-information source coding, and we investigate
here its application to the reconciliation problem occurring in a
continuous-variable quantum key distribution protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406002</id><created>2004-06-02</created><authors><author><keyname>Fischbacher</keyname><forenames>Thomas</forenames></author></authors><title>A novel approach to symbolic algebra</title><categories>cs.SC</categories><comments>15 pages</comments><report-no>AEI-2004-043</report-no><acm-class>G.4; I.1.3</acm-class><abstract>  A prototype for an extensible interactive graphical term manipulation system
is presented that combines pattern matching and nondeterministic evaluation to
provide a convenient framework for doing tedious algebraic manipulations that
so far had to be done manually in a semi-automatic fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406003</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406003</id><created>2004-06-02</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames><affiliation>Xerox Research Centre Europe, France</affiliation></author><author><keyname>Guingne</keyname><forenames>Franck</forenames><affiliation>Xerox Research Centre Europe, France</affiliation><affiliation>Rouen University, France</affiliation></author><author><keyname>Nicart</keyname><forenames>Florent</forenames><affiliation>Xerox Research Centre Europe, France</affiliation><affiliation>Rouen University, France</affiliation></author></authors><title>Algorithms for weighted multi-tape automata</title><categories>cs.CL cs.DS</categories><comments>28 pages, 7 figures, LaTeX (+ .eps)</comments><report-no>XRCE Research Report 2004/031</report-no><acm-class>F.1.1; I.2.7</acm-class><abstract>  This report defines various operations for weighted multi-tape automata
(WMTAs) and describes algorithms that have been implemented for those
operations in the WFSC toolkit. Some algorithms are new, others are known or
similar to known algorithms. The latter will be recalled to make this report
more complete and self-standing. We present a new approach to multi-tape
intersection, meaning the intersection of a number of tapes of one WMTA with
the same number of tapes of another WMTA. In our approach, multi-tape
intersection is not considered as an atomic operation but rather as a sequence
of more elementary ones, which facilitates its implementation. We show an
example of multi-tape intersection, actually transducer intersection, that can
be compiled with our approach but not with several other methods that we
analysed. To show the practical relavance of our work, we include an example of
application: the preservation of intermediate results in transduction cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406004</id><created>2004-06-02</created><authors><author><keyname>Nadeem</keyname><forenames>Muhammad</forenames><affiliation>Szabist</affiliation></author><author><keyname>Jaffri</keyname><forenames>Syed Ata Hussain</forenames><affiliation>Szabist</affiliation></author></authors><title>Application of Business Intelligence In Banks (Pakistan)</title><categories>cs.DB</categories><comments>6 Pages</comments><acm-class>H.2.7</acm-class><abstract>  The financial services industry is rapidly changing. Factors such as
globalization, deregulation, mergers and acquisitions, competition from
non-financial institutions, and technological innovation, have forced companies
to re-think their business.Many large companies have been using Business
Intelligence (BI) computer software for some years to help them gain
competitive advantage. With the introduction of cheaper and more generalized
products to the market place BI is now in the reach of smaller and medium sized
companies. Business Intelligence is also known as knowledge management,
management information systems (MIS), Executive information systems (EIS) and
On-line analytical Processing (OLAP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406005</id><created>2004-06-02</created><updated>2004-10-12</updated><authors><author><keyname>Candea</keyname><forenames>George</forenames></author><author><keyname>Kawamoto</keyname><forenames>Shinichi</forenames></author><author><keyname>Fujiki</keyname><forenames>Yuichi</forenames></author><author><keyname>Friedman</keyname><forenames>Greg</forenames></author><author><keyname>Fox</keyname><forenames>Armando</forenames></author></authors><title>Microreboot -- A Technique for Cheap Recovery</title><categories>cs.OS cs.DC</categories><comments>14 pages</comments><acm-class>D.4.0</acm-class><journal-ref>Proc. 6th Symposium on Operating Systems Design and Implementation
  (OSDI), San Francisco, CA, Dec 2004</journal-ref><abstract>  A significant fraction of software failures in large-scale Internet systems
are cured by rebooting, even when the exact failure causes are unknown.
However, rebooting can be expensive, causing nontrivial service disruption or
downtime even when clusters and failover are employed. In this work we separate
process recovery from data recovery to enable microrebooting -- a fine-grain
technique for surgically recovering faulty application components, without
disturbing the rest of the application.
  We evaluate microrebooting in an Internet auction system running on an
application server. Microreboots recover most of the same failures as full
reboots, but do so an order of magnitude faster and result in an order of
magnitude savings in lost work. This cheap form of recovery engenders a new
approach to high availability: microreboots can be employed at the slightest
hint of failure, prior to node failover in multi-node clusters, even when
mistakes in failure detection are likely; failure and recovery can be masked
from end users through transparent call-level retries; and systems can be
rejuvenated by parts, without ever being shut down.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406006</id><created>2004-06-02</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author></authors><title>Dichotomy Theorems for Alternation-Bounded Quantified Boolean Formulas</title><categories>cs.CC cs.LO</categories><abstract>  In 1978, Schaefer proved his famous dichotomy theorem for generalized
satisfiability problems. He defined an infinite number of propositional
satisfiability problems, showed that all these problems are either in P or
NP-complete, and gave a simple criterion to determine which of the two cases
holds. This result is surprising in light of Ladner's theorem, which implies
that there are an infinite number of complexity classes between P and
NP-complete (under the assumption that P is not equal to NP).
  Schaefer also stated a dichotomy theorem for quantified generalized Boolean
formulas, but this theorem was only recently proven by Creignou, Khanna, and
Sudan, and independently by Dalmau: Determining truth of quantified Boolean
formulas is either PSPACE-complete or in P.
  This paper looks at alternation-bounded quantified generalized Boolean
formulas. In their unrestricted forms, these problems are the canonical
problems complete for the levels of the polynomial hierarchy. In this paper, we
prove dichotomy theorems for alternation-bounded quantified generalized Boolean
formulas, by showing that these problems are either $\Sigma_i^p$-complete or in
P, and we give a simple criterion to determine which of the two cases holds.
This is the first result that obtains dichotomy for an infinite number of
classes at once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406007</id><created>2004-06-03</created><authors><author><keyname>Ocenasek</keyname><forenames>Jiri</forenames></author><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author></authors><title>Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis</title><categories>cs.NE cs.DC</categories><comments>Optimization by Building and Using Probabilistic Models OBUPM-2004</comments><acm-class>G.1.6; G.3; I.2.6; I.2.8</acm-class><abstract>  Estimation of Distribution Algorithms have been proposed as a new paradigm
for evolutionary optimization. This paper focuses on the parallelization of
Estimation of Distribution Algorithms. More specifically, the paper discusses
how to predict performance of parallel Mixed Bayesian Optimization Algorithm
(MBOA) that is based on parallel construction of Bayesian networks with
decision trees. We determine the time complexity of parallel Mixed Bayesian
Optimization Algorithm and compare this complexity with experimental results
obtained by solving the spin glass optimization problem. The empirical results
fit well the theoretical time complexity, so the scalability and efficiency of
parallel Mixed Bayesian Optimization Algorithm for unknown instances of spin
glass benchmarks can be predicted. Furthermore, we derive the guidelines that
can be used to design effective parallel Estimation of Distribution Algorithms
with the speedup proportional to the number of variables in the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406008</id><created>2004-06-04</created><authors><author><keyname>Zavadsky</keyname><forenames>Vyacheslav</forenames></author></authors><title>Image compression by rectangular wavelet transform</title><categories>cs.CV</categories><abstract>  We study image compression by a separable wavelet basis
$\big\{\psi(2^{k_1}x-i)\psi(2^{k_2}y-j),$ $\phi(x-i)\psi(2^{k_2}y-j),$
$\psi(2^{k_1}(x-i)\phi(y-j),$ $\phi(x-i)\phi(y-i)\big\},$ where $k_1, k_2 \in
\mathbb{Z}_+$; $i,j\in\mathbb{Z}$; and $\phi,\psi$ are elements of a standard
biorthogonal wavelet basis in $L_2(\mathbb{R})$. Because $k_1\ne k_2$, the
supports of the basis elements are rectangles, and the corresponding transform
is known as the {\em rectangular wavelet transform}. We prove that if
one-dimensional wavelet basis has $M$ dual vanishing moments then the rate of
approximation by $N$ coefficients of rectangular wavelet transform is
$\mathcal{O}(N^{-M}\log^C N)$ for functions with mixed derivative of order $M$
in each direction.
  The square wavelet transform yields the approximation rate is
$\mathcal{O}(N^{-M/2})$ for functions with all derivatives of the total order
$M$. Thus, the rectangular wavelet transform can outperform the square one if
an image has a mixed derivative. We provide experimental comparison of image
compression which shows that rectangular wavelet transform outperform the
square one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406009</id><created>2004-06-04</created><authors><author><keyname>Rennard</keyname><forenames>J. -P.</forenames></author></authors><title>Implementation of Logical Functions in the Game of Life</title><categories>cs.CC</categories><comments>17 pages, 28 figures</comments><acm-class>F.1.1</acm-class><journal-ref>Rennard, J.-P. (2002). Implementation of Logical Functions in the
  Game of Life. In A. Adamatzky (Ed.), Collision-Based Computing (pp. 491-512).
  London: Springer</journal-ref><abstract>  The Game of Life cellular automaton is a classical example of a massively
parallel collision-based computing device. The automaton exhibits mobile
patterns, gliders, and generators of the mobile patterns, glider guns, in its
evolution. We show how to construct the basic logical operations, AND, OR, NOT
in space-time configurations of the cellular automaton. Also decomposition of
complicated Boolean functions is discussed. Advantages of our technique are
demonstrated on an example of binary adder, realized via collision of glider
streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406010</id><created>2004-06-06</created><authors><author><keyname>Sun</keyname><forenames>Yidong</forenames></author></authors><title>Another Proof of an Extension of a Curious Identity</title><categories>cs.DM</categories><comments>3 pages</comments><acm-class>G.2.1</acm-class><abstract>  Based on Jensen formulae and the second kind of Chebyshev polynomials,
another proof is presented for an extension of a curious binomial identity due
to Z. W. Sun and K. J. Wu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406011</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406011</id><created>2004-06-06</created><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Shalizi</keyname><forenames>Kristina Lisa</forenames></author></authors><title>Blind Construction of Optimal Nonlinear Recursive Predictors for
  Discrete Sequences</title><categories>cs.LG math.ST nlin.CD physics.data-an stat.TH</categories><comments>8 pages, 4 figures</comments><acm-class>I.2.6</acm-class><journal-ref>pp. 504--511 in Max Chickering and Joseph Halpern (eds.),
  _Uncertainty in Artificial Intelligence: Proceedings of the Twentieth
  Conference_ (2004)</journal-ref><abstract>  We present a new method for nonlinear prediction of discrete random sequences
under minimal structural assumptions. We give a mathematical construction for
optimal predictors of such processes, in the form of hidden Markov models. We
then describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which
approximates the ideal predictor from data. We discuss the reliability of CSSR,
its data requirements, and its performance in simulations. Finally, we compare
our approach to existing methods using variable-length Markov models and
cross-validated hidden Markov models, and show theoretically and experimentally
that our method delivers results superior to the former and at least comparable
to the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406012</id><created>2004-06-07</created><authors><author><keyname>Loke</keyname><forenames>Seng Wai</forenames></author><author><keyname>Davison</keyname><forenames>Andrew</forenames></author></authors><title>Secure Prolog-Based Mobile Code</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 3,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 3, 2001</journal-ref><abstract>  LogicWeb mobile code consists of Prolog-like rules embedded in Web pages,
thereby adding logic programming behaviour to those pages. Since LogicWeb
programs are downloaded from foreign hosts and executed locally, there is a
need to protect the client from buggy or malicious code. A security model is
crucial for making LogicWeb mobile code safe to execute. This paper presents
such a model, which supports programs of varying trust levels by using
different resource access policies. The implementation of the model derives
from an extended operational semantics for the LogicWeb language, which
provides a precise meaning of safety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406013</id><created>2004-06-07</created><authors><author><keyname>Greco</keyname><forenames>G.</forenames></author><author><keyname>Greco</keyname><forenames>S.</forenames></author><author><keyname>Trubtsyna</keyname><forenames>I.</forenames></author><author><keyname>Zumpano</keyname><forenames>E.</forenames></author></authors><title>Optimization of Bound Disjunctive Queries with Constraints</title><categories>cs.LO</categories><comments>35 pages</comments><abstract>  &quot;To Appear in Theory and Practice of Logic Programming (TPLP)&quot; This paper
presents a technique for the optimization of bound queries over disjunctive
deductive databases with constraints. The proposed approach is an extension of
the well-known Magic-Set technique and is well-suited for being integrated in
current bottom-up (stable) model inference engines. More specifically, it is
based on the exploitation of binding propagation techniques which reduce the
size of the data relevant to answer the query and, consequently, reduces both
the complexity of computing a single model and the number of models to be
considered. The motivation of this work stems from the observation that
traditional binding propagation optimization techniques for bottom-up model
generator systems, simulating the goal driven evaluation of top-down engines,
are only suitable for positive (disjunctive) queries, while hard problems are
expressed using unstratified negation. The main contribution of the paper
consists in the extension of a previous technique, defined for positive
disjunctive queries, to queries containing both disjunctive heads and
constraints (a simple and expressive form of unstratified negation). As the
usual way of expressing declaratively hard problems is based on the
guess-and-check technique, where the guess part is expressed by means of
disjunctive rules and the check part is expressed by means of constraints, the
technique proposed here is highly relevant for the optimization of queries
expressing hard problems. The value of the technique has been proved by several
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406014</id><created>2004-06-07</created><authors><author><keyname>O'Keefe</keyname><forenames>Richard A.</forenames></author></authors><title>O(1) Reversible Tree Navigation Without Cycles</title><categories>cs.PL</categories><comments>Appeared in Theory and Practice of Logic Programming, vol. 1, no. 5,
  2001</comments><acm-class>D.1.6; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, vol. 1, no. 5, 2001</journal-ref><abstract>  Imperative programmers often use cyclically linked trees in order to achieve
O(1) navigation time to neighbours. Some logic programmers believe that cyclic
terms are necessary to achieve the same in logic-based languages. An old but
little-known technique provides O(1) time and space navigation without cyclic
links, in the form of reversible predicates. A small modification provides O(1)
amortised time and space editing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406015</id><created>2004-06-07</created><authors><author><keyname>Zanette</keyname><forenames>Damian H.</forenames></author></authors><title>Zipf's law and the creation of musical context</title><categories>cs.CL cond-mat.stat-mech</categories><abstract>  This article discusses the extension of the notion of context from
linguistics to the domain of music. In language, the statistical regularity
known as Zipf's law -which concerns the frequency of usage of different words-
has been quantitatively related to the process of text generation. This
connection is established by Simon's model, on the basis of a few assumptions
regarding the accompanying creation of context. Here, it is shown that the
statistics of note usage in musical compositions are compatible with the
predictions of Simon's model. This result, which gives objective support to the
conceptual likeness of context in language and music, is obtained through
automatic analysis of the digital versions of several compositions. As a
by-product, a quantitative measure of context definiteness is introduced and
used to compare tonal and atonal works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406016</id><created>2004-06-07</created><authors><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Scherzinger</keyname><forenames>Stefanie</forenames></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author><author><keyname>Stegmaier</keyname><forenames>Bernhard</forenames></author></authors><title>Schema-based Scheduling of Event Processors and Buffer Minimization for
  Queries on Structured Data Streams</title><categories>cs.DB</categories><comments>14 pages, 4 figures, to appear in Proc. 30th VLDB 2004, Toronto,
  Canada. Extended version</comments><acm-class>H.2.3, H.2.4</acm-class><abstract>  We introduce an extension of the XQuery language, FluX, that supports
event-based query processing and the conscious handling of main memory buffers.
Purely event-based queries of this language can be executed on streaming XML
data in a very direct way. We then develop an algorithm that allows to
efficiently rewrite XQueries into the event-based FluX language. This algorithm
uses order constraints from a DTD to schedule event handlers and to thus
minimize the amount of buffering required for evaluating a query. We discuss
the various technical aspects of query optimization and query evaluation within
our framework. This is complemented with an experimental evaluation of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406017</id><created>2004-06-08</created><updated>2004-08-22</updated><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Using Self-Organising Mappings to Learn the Structure of Data Manifolds</title><categories>cs.NE cs.CV</categories><comments>26 pages, 19 figures, version 1 translated into LaTeX</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  In this paper it is shown how to map a data manifold into a simpler form by
progressively discarding small degrees of freedom. This is the key to
self-organising data fusion, where the raw data is embedded in a very
high-dimensional space (e.g. the pixel values of one or more images), and the
requirement is to isolate the important degrees of freedom which lie on a
low-dimensional manifold. A useful advantage of the approach used in this paper
is that the computations are arranged as a feed-forward processing chain, where
all the details of the processing in each stage of the chain are learnt by
self-organisation. This approach is demonstrated using hierarchically
correlated data, which causes the processing chain to split the data into
separate processing channels, and then to progressively merge these channels
wherever they are correlated with each other. This is the key to
self-organising data fusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406018</id><created>2004-06-11</created><authors><author><keyname>Eremin</keyname><forenames>A. A.</forenames></author></authors><title>Effects of wireless computing technology</title><categories>cs.OH</categories><comments>HTML, also available at http://verystrangeplace.com/</comments><acm-class>C.2.1</acm-class><abstract>  Wireless technology can provide many benefits to computing including faster
response to queries, reduced time spent on paperwork, increased online time for
users, just-in-time and real time control, tighter communications between
clients and hosts. Wireless Computing is governed by two general forces:
Technology, which provides a set of basic building blocks and User
Applications, which determine a set of operations that must be carried out
efficiently on demand. This paper summarizes technological changes that are
underway and describes their impact on wireless computing development and
implementation. It also describes the applications that influence the
development and implementation of wireless computing and shows what current
systems offer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406019</id><created>2004-06-11</created><authors><author><keyname>Firoiu</keyname><forenames>Victor</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohui</forenames></author><author><keyname>Gunduzhan</keyname><forenames>Emre</forenames></author><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author></authors><title>Providing Service Guarantees in High-Speed Switching Systems with
  Feedback Output Queuing</title><categories>cs.NI</categories><comments>30 pages, 9 figures. Shorter preliminary version appeared in
  Proceedings of Hot Interconnects X, Stanford CA, August 2002</comments><acm-class>C.2.1; C.2.6</acm-class><abstract>  We consider the problem of providing service guarantees in a high-speed
packet switch. As basic requirements, the switch should be scalable to high
speeds per port, a large number of ports and a large number of traffic flows
with independent guarantees. Existing scalable solutions are based on Virtual
Output Queuing, which is computationally complex when required to provide
service guarantees for a large number of flows.
  We present a novel architecture for packet switching that provides support
for such service guarantees. A cost-effective fabric with small external
speedup is combined with a feedback mechanism that enables the fabric to be
virtually lossless, thus avoiding packet drops indiscriminate of flows. Through
analysis and simulation, we show that this architecture provides accurate
support for service guarantees, has low computational complexity and is
scalable to very high port speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406020</id><created>2004-06-15</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Algorithms for Drawing Media</title><categories>cs.DS cs.CG</categories><comments>13 pages, 11 figures</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  We describe algorithms for drawing media, systems of states, tokens and
actions that have state transition graphs in the form of partial cubes. Our
algorithms are based on two principles: embedding the state transition graph in
a low-dimensional integer lattice and projecting the lattice onto the plane, or
drawing the medium as a planar graph with centrally symmetric faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406021</id><created>2004-06-15</created><updated>2006-05-20</updated><authors><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author><author><keyname>Ghaoui</keyname><forenames>Laurent El</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert R. G.</forenames></author></authors><title>A direct formulation for sparse PCA using semidefinite programming</title><categories>cs.CE</categories><comments>Final version, to appear in SIAM review</comments><report-no>Working paper: UCB//CSD-04-1330</report-no><abstract>  We examine the problem of approximating, in the Frobenius-norm sense, a
positive, semidefinite symmetric matrix by a rank-one matrix, with an upper
bound on the cardinality of its eigenvector. The problem arises in the
decomposition of a covariance matrix into sparse factors, and has wide
applications ranging from biology to finance. We use a modification of the
classical variational representation of the largest eigenvalue of a symmetric
matrix, where cardinality is constrained, and derive a semidefinite programming
based relaxation for our problem. We also discuss Nesterov's smooth
minimization technique applied to the SDP arising in the direct sparse PCA
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406022</id><created>2004-06-16</created><authors><author><keyname>King</keyname><forenames>David</forenames></author><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Uncovering the epistemological and ontological assumptions of software
  designers</title><categories>cs.SE cs.GL</categories><comments>9 pages</comments><acm-class>K.6.3, I.0</acm-class><journal-ref>Proceedings 9e colloque de l'AIM, Evry, France, May 2004</journal-ref><abstract>  The ontological and epistemological positions adopted by information systems
design methods are incommensur-able when pushed to their extremes. Information
systems research has therefore tended to focus on the similarities between
different positions, usually in search of a single, unifying position. However,
by focusing on the similari-ties, the clarity of argument provided by any one
philoso-phical position is necessarily diminished. Consequently, researchers
often treat the philosophical foundations of design methods as being of only
minor importance. In this paper, we have deliberately chosen to focus on the
differences between various philosophical positions. From this focus, we
believe we can offer a clearer under-standing of the empirical behaviour of
software as viewed from particular philosophical positions. Since the
em-pirical evidence does not favour any single position, we conclude by arguing
for the validity of ad hoc approaches to software design which we believe
provides a stronger and more theoretically grounded approach to software
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406023</id><created>2004-06-16</created><authors><author><keyname>King</keyname><forenames>David</forenames></author><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Notions of Equivalence in Software Design</title><categories>cs.SE cs.GL</categories><comments>8 pages</comments><acm-class>K.6.3; I.0</acm-class><journal-ref>Proceedings 9e colloque de l'AIM, Evry, France, May 2004</journal-ref><abstract>  Design methods in information systems frequently create software descriptions
using formal languages. Nonetheless, most software designers prefer to describe
software using natural languages. This distinction is not simply a matter of
convenience. Natural languages are not the same as formal languages; in
particular, natural languages do not follow the notions of equivalence used by
formal languages. In this paper, we show both the existence and coexistence of
different notions of equivalence by extending the no-tion of oracles used in
formal languages. This allows distinctions to be made between the trustworthy
oracles assumed by formal languages and the untrust-worthy oracles used by
natural languages. By examin-ing the notion of equivalence, we hope to
encourage designers of software to rethink the place of ambiguity in software
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406024</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406024</id><created>2004-06-16</created><authors><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Layout of Graphs with Bounded Tree-Width</title><categories>cs.DM cs.CG</categories><comments>This is a revised version of a journal paper submitted in October
  2002. This paper incorporates the following conference papers: (1) Dujmovic',
  Morin &amp; Wood. Path-width and three-dimensional straight-line grid drawings of
  graphs (GD'02), LNCS 2528:42-53, Springer, 2002. (2) Wood. Queue layouts,
  tree-width, and three-dimensional graph drawing (FSTTCS'02), LNCS
  2556:348--359, Springer, 2002. (3) Dujmovic' &amp; Wood. Tree-partitions of
  $k$-trees with applications in graph layout (WG '03), LNCS 2880:205-217, 2003</comments><journal-ref>SIAM J. Computing 34.3:553-579, 2005</journal-ref><doi>10.1137/S0097539702416141</doi><abstract>  A \emph{queue layout} of a graph consists of a total order of the vertices,
and a partition of the edges into \emph{queues}, such that no two edges in the
same queue are nested. The minimum number of queues in a queue layout of a
graph is its \emph{queue-number}. A \emph{three-dimensional (straight-line
grid) drawing} of a graph represents the vertices by points in $\mathbb{Z}^3$
and the edges by non-crossing line-segments. This paper contributes three main
results:
  (1) It is proved that the minimum volume of a certain type of
three-dimensional drawing of a graph $G$ is closely related to the queue-number
of $G$. In particular, if $G$ is an $n$-vertex member of a proper minor-closed
family of graphs (such as a planar graph), then $G$ has a $O(1)\times
O(1)\times O(n)$ drawing if and only if $G$ has O(1) queue-number.
  (2) It is proved that queue-number is bounded by tree-width, thus resolving
an open problem due to Ganley and Heath (2001), and disproving a conjecture of
Pemmaraju (1992). This result provides renewed hope for the positive resolution
of a number of open problems in the theory of queue layouts.
  (3) It is proved that graphs of bounded tree-width have three-dimensional
drawings with O(n) volume. This is the most general family of graphs known to
admit three-dimensional drawings with O(n) volume.
  The proofs depend upon our results regarding \emph{track layouts} and
\emph{tree-partitions} of graphs, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406025</id><created>2004-06-16</created><authors><author><keyname>Goualard</keyname><forenames>Frederic</forenames></author><author><keyname>Granvilliers</keyname><forenames>Laurent</forenames></author></authors><title>Directional Consistency for Continuous Numerical Constraints</title><categories>cs.AI cs.MS</categories><comments>16 pages, 3 figures</comments><abstract>  Bounds consistency is usually enforced on continuous constraints by first
decomposing them into binary and ternary primitives. This decomposition has
long been shown to drastically slow down the computation of solutions. To
tackle this, Benhamou et al. have introduced an algorithm that avoids formally
decomposing constraints. Its better efficiency compared to the former method
has already been experimentally demonstrated. It is shown here that their
algorithm implements a strategy to enforce on a continuous constraint a
consistency akin to Directional Bounds Consistency as introduced by Dechter and
Pearl for discrete problems. The algorithm is analyzed in this framework, and
compared with algorithms that enforce bounds consistency. These theoretical
results are eventually contrasted with new experimental results on standard
benchmarks from the interval constraint community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406026</id><created>2004-06-16</created><authors><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>Improving Prolog Programs: Refactoring for Prolog</title><categories>cs.SE cs.PL</categories><comments>To appear in ICLP 2004</comments><acm-class>D.2.7; D.1.6</acm-class><abstract>  Refactoring is an established technique from the OO-community to restructure
code: it aims at improving software readability, maintainability and
extensibility. Although refactoring is not tied to the OO-paradigm in
particular, its ideas have not been applied to Logic Programming until now.
  This paper applies the ideas of refactoring to Prolog programs. A catalogue
is presented listing refactorings classified according to scope. Some of the
refactorings have been adapted from the OO-paradigm, while others have been
specifically designed for Prolog. Also the discrepancy between intended and
operational semantics in Prolog is addressed by some of the refactorings.
  In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and
the experience with applying \vipress to a large Prolog legacy system is
reported. Our main conclusion is that refactoring is not only a viable
technique in Prolog but also a rather desirable one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406027</id><created>2004-06-16</created><authors><author><keyname>Fahrenholtz</keyname><forenames>Dietrich</forenames></author><author><keyname>Turau</keyname><forenames>Volker</forenames></author></authors><title>Fluctuation in Peer-to-Peer Networks: Mitigating Its Effect on DHT
  Performance</title><categories>cs.NI cs.DC</categories><comments>5 pages</comments><abstract>  Due to the transient nature of peers, any Peer-to-Peer network is in peril to
falling apart if peers do not receive routing table updates periodically. To
this end, maintenance, which affects every peer, ensures connectedness and
sustained data operation performance. However, a high rate of change in peer
population usually incurs lots of network maintenance messages and can severely
degrade overall performance. We discuss three methods how to tackle and
mitigate the effect of peer fluctuation on a tree-based distributed hash table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406028</id><created>2004-06-16</created><authors><author><keyname>Bartal</keyname><forenames>Yair</forenames></author><author><keyname>Bollobas</keyname><forenames>Bela</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author></authors><title>Ramsey-type theorems for metric spaces with applications to online
  problems</title><categories>cs.DS</categories><comments>Fix an error in the metadata. 31 pages, 0 figures. Preliminary
  version in FOCS '01. To be published in J. Comput. System Sci</comments><journal-ref>J. Comput. System Sci. 72(5):890-921, 2006</journal-ref><doi>10.1016/j.jcss.2005.05.008</doi><abstract>  A nearly logarithmic lower bound on the randomized competitive ratio for the
metrical task systems problem is presented. This implies a similar lower bound
for the extensively studied k-server problem. The proof is based on Ramsey-type
theorems for metric spaces, that state that every metric space contains a large
subspace which is approximately a hierarchically well-separated tree (and in
particular an ultrametric). These Ramsey-type theorems may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406029</id><created>2004-06-17</created><authors><author><keyname>Valluri</keyname><forenames>Satyanarayana R</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Subset Queries in Relational Databases</title><categories>cs.DB</categories><comments>15 pages</comments><acm-class>H2.3, H2.4</acm-class><abstract>  In this paper, we motivated the need for relational database systems to
support subset query processing. We defined new operators in relational
algebra, and new constructs in SQL for expressing subset queries. We also
illustrated the applicability of subset queries through different examples
expressed using extended SQL statements and relational algebra expressions. Our
aim is to show the utility of subset queries for next generation applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406030</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406030</id><created>2004-06-17</created><updated>2006-09-13</updated><authors><author><keyname>Bonacina</keyname><forenames>Maria Paola</forenames></author><author><keyname>Dershowitz</keyname><forenames>Nachum</forenames></author></authors><title>Abstract Canonical Inference</title><categories>cs.LO cs.SC</categories><comments>28 pages, no figures, to appear in ACM Trans. on Computational Logic</comments><report-no>RR 18/2004</report-no><journal-ref>ACM Transactions on Computational Logic, 8(1):180-208, January
  2007</journal-ref><doi>10.1145/1182613.1182619</doi><abstract>  An abstract framework of canonical inference is used to explore how different
proof orderings induce different variants of saturation and completeness.
Notions like completion, paramodulation, saturation, redundancy elimination,
and rewrite-system reduction are connected to proof orderings. Fairness of
deductive mechanisms is defined in terms of proof orderings, distinguishing
between (ordinary) &quot;fairness,&quot; which yields completeness, and &quot;uniform
fairness,&quot; which yields saturation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406031</id><created>2004-06-17</created><authors><author><keyname>Qiu</keyname><forenames>Long</forenames></author><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author><author><keyname>Chua</keyname><forenames>Tat-Seng</forenames></author></authors><title>A Public Reference Implementation of the RAP Anaphora Resolution
  Algorithm</title><categories>cs.CL</categories><comments>4-page LREC 2004 paper</comments><abstract>  This paper describes a standalone, publicly-available implementation of the
Resolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The
RAP algorithm resolves third person pronouns, lexical anaphors, and identifies
pleonastic pronouns. Our implementation, JavaRAP, fills a current need in
anaphora resolution research by providing a reference implementation that can
be benchmarked against current algorithms. The implementation uses the
standard, publicly available Charniak (2000) parser as input, and generates a
list of anaphora-antecedent pairs as output. Alternately, an in-place
annotation or substitution of the anaphors with their antecedents can be
produced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an
accuracy of 57.9%, similar to the performance given previously in the
literature (e.g., Preiss 2002).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406032</id><created>2004-06-17</created><authors><author><keyname>Borges</keyname><forenames>Jos&#xe9;</forenames><affiliation>School of Engineering, University of Porto, Portuga</affiliation></author><author><keyname>Levene</keyname><forenames>Mark</forenames><affiliation>Birkbeck, University of London, U.K</affiliation></author></authors><title>A Dynamic Clustering-Based Markov Model for Web Usage Mining</title><categories>cs.IR cs.AI</categories><abstract>  Markov models have been widely utilized for modelling user web navigation
behaviour. In this work we propose a dynamic clustering-based method to
increase a Markov model's accuracy in representing a collection of user web
navigation sessions. The method makes use of the state cloning concept to
duplicate states in a way that separates in-links whose corresponding
second-order probabilities diverge. In addition, the new method incorporates a
clustering technique which determines an effcient way to assign in-links with
similar second-order probabilities to the same clone. We report on experiments
conducted with both real and random data and we provide a comparison with the
N-gram Markov concept. The results show that the number of additional states
induced by the dynamic clustering method can be controlled through a threshold
parameter, and suggest that the method's performance is linear time in the size
of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406033</identifier>
 <datestamp>2007-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406033</id><created>2004-06-17</created><updated>2007-09-28</updated><authors><author><keyname>Mendel</keyname><forenames>Manor</forenames></author></authors><title>Randomized k-server algorithms for growth-rate bounded graphs</title><categories>cs.DS</categories><comments>The paper is withdrawn</comments><journal-ref>J. Algorithms, 55(2): 192-202, 2005</journal-ref><doi>10.1016/j.jalgor.2004.06.002</doi><abstract>  The paper referred to in the title is withdrawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406034</id><created>2004-06-17</created><authors><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author></authors><title>Better algorithms for unfair metrical task systems and applications</title><categories>cs.DS</categories><comments>20 pages, 1 figure</comments><journal-ref>SIAM Journal on Computing 32(6), pp. 1403-1422, 2003</journal-ref><doi>10.1137/S0097539700376159</doi><abstract>  Unfair metrical task systems are a generalization of online metrical task
systems. In this paper we introduce new techniques to combine algorithms for
unfair metrical task systems and apply these techniques to obtain improved
randomized online algorithms for metrical task systems on arbitrary metric
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406035</id><created>2004-06-18</created><updated>2005-09-28</updated><authors><author><keyname>Ahmadinia</keyname><forenames>Ali</forenames></author><author><keyname>Bobda</keyname><forenames>Christophe</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor</forenames></author><author><keyname>Teich</keyname><forenames>Juergen</forenames></author><author><keyname>van der Veen</keyname><forenames>Jan</forenames></author></authors><title>Optimal Free-Space Management and Routing-Conscious Dynamic Placement
  for Reconfigurable Devices</title><categories>cs.DS cs.CG</categories><comments>18 pages, 8 figures, 1, table; previous 5-page extended abstract
  appears in &quot;International Conference on Field-Programmable Logic and
  Applications&quot;, 2004. New version is final journal version, to appear in IEEE
  Transactions on Computers</comments><acm-class>C.1.3; F.2.2</acm-class><abstract>  We describe algorithmic results for two crucial aspects of allocating
resources on computational hardware devices with partial reconfigurability. By
using methods from the field of computational geometry, we derive a method that
allows correct maintainance of free and occupied space of a set of n
rectangular modules in optimal time Theta(n log n); previous approaches needed
a time of O(n^2) for correct results and O(n) for heuristic results. We also
show that finding an optimal feasible communication-conscious placement (which
minimizes the total weighted Manhattan distance between the new module and
existing demand points) can be computed in Theta(n log n). Both resulting
algorithms are practically easy to implement and show convincing experimental
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406036</id><created>2004-06-18</created><authors><author><keyname>Mendel</keyname><forenames>Manor</forenames></author><author><keyname>Seiden</keyname><forenames>Steven S.</forenames></author></authors><title>Online Companion Caching</title><categories>cs.DS</categories><comments>17 pages, 1 figure. Preliminary version in ESA '02. To be published
  in Theoretical Computer Science A</comments><journal-ref>Theoret. Comput. Sci. 324(2-3): 183-200, 2004</journal-ref><doi>10.1016/j.tcs.2004.05.015</doi><abstract>  This paper is concerned with online caching algorithms for the
(n,k)-companion cache, defined by Brehob et. al. In this model the cache is
composed of two components: a k-way set-associative cache and a companion
fully-associative cache of size n. We show that the deterministic competitive
ratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio is
O(\log n \log k) and \Omega(\log n +\log k).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406037</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406037</id><created>2004-06-18</created><updated>2004-06-20</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Propositional Computability Logic II</title><categories>cs.LO cs.GT math.LO</categories><comments>25 pages</comments><acm-class>F.4.1; F.1.2</acm-class><journal-ref>ACM Transactions on Computational Logic 7 (2006), pp. 331-362</journal-ref><doi>10.1145/1131313.1131319</doi><abstract>  Computability logic is a formal theory of computational tasks and resources.
Its formulas represent interactive computational problems, logical operators
stand for operations on computational problems, and validity of a formula is
understood as being a scheme of problems that always have algorithmic
solutions. A comprehensive online source on the subject is available at
http://www.cis.upenn.edu/~giorgi/cl.html . The earlier article &quot;Propositional
computability logic I&quot; proved soundness and completeness for the (in a sense)
minimal nontrivial fragment CL1 of computability logic. The present paper
extends that result to the significantly more expressive propositional system
CL2. What makes CL2 more expressive than CL1 is the presence of two sorts of
atoms in its language: elementary atoms, representing elementary computational
problems (i.e. predicates), and general atoms, representing arbitrary
computational problems. CL2 conservatively extends CL1, with the latter being
nothing but the general-atom-free fragment of the former.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406038</id><created>2004-06-21</created><authors><author><keyname>Vuckovic</keyname><forenames>Vladan</forenames></author><author><keyname>Vidanovic</keyname><forenames>Djordje</forenames></author></authors><title>A New Approach to Draw Detection by Move Repetition in Computer Chess
  Programming</title><categories>cs.AI</categories><comments>15 pages, 4 figures</comments><acm-class>I.2.1</acm-class><abstract>  We will try to tackle both the theoretical and practical aspects of a very
important problem in chess programming as stated in the title of this article -
the issue of draw detection by move repetition. The standard approach that has
so far been employed in most chess programs is based on utilising positional
matrices in original and compressed format as well as on the implementation of
the so-called bitboard format.
  The new approach that we will be trying to introduce is based on using
variant strings generated by the search algorithm (searcher) during the tree
expansion in decision making. We hope to prove that this approach is more
efficient than the standard treatment of the issue, especially in positions
with few pieces (endgames). To illustrate what we have in mind a machine
language routine that implements our theoretical assumptions is attached. The
routine is part of the Axon chess program, developed by the authors. Axon, in
its current incarnation, plays chess at master strength (ca. 2400-2450 Elo,
based on both Axon vs computer programs and Axon vs human masters in over 3000
games altogether).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406039</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406039</id><created>2004-06-21</created><updated>2004-06-23</updated><authors><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author><author><keyname>Dumer</keyname><forenames>Ilya</forenames></author></authors><title>Long Nonbinary Codes Exceeding the Gilbert - Varshamov Bound for any
  Fixed Distance</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Info. Theory</comments><abstract>  Let A(q,n,d) denote the maximum size of a q-ary code of length n and distance
d. We study the minimum asymptotic redundancy \rho(q,n,d)=n-log_q A(q,n,d) as n
grows while q and d are fixed. For any d and q&lt;=d-1, long algebraic codes are
designed that improve on the BCH codes and have the lowest asymptotic
redundancy \rho(q,n,d) &lt;= ((d-3)+1/(d-2)) log_q n known to date. Prior to this
work, codes of fixed distance that asymptotically surpass BCH codes and the
Gilbert-Varshamov bound were designed only for distances 4,5 and 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406040</id><created>2004-06-21</created><updated>2004-06-26</updated><authors><author><keyname>Donat</keyname><forenames>Bonifac</forenames></author></authors><title>Alchemistry of the P versus NP question</title><categories>cs.CC cs.LO</categories><abstract>  Are P and NP provably inseparable ? Take a look at some unorthodox, guilty
mentioned folklore and related unpublished results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406041</id><created>2004-06-22</created><authors><author><keyname>Payet</keyname><forenames>Etienne</forenames></author><author><keyname>Mesnard</keyname><forenames>Fred</forenames></author></authors><title>Non-Termination Inference of Logic Programs</title><categories>cs.PL cs.LO</categories><comments>Long version (algorithms and proofs included) of a paper submitted to
  TOPLAS</comments><abstract>  We present a static analysis technique for non-termination inference of logic
programs. Our framework relies on an extension of the subsumption test, where
some specific argument positions can be instantiated while others are
generalized. We give syntactic criteria to statically identify such argument
positions from the text of a program. Atomic left looping queries are generated
bottom-up from selected subsets of the binary unfoldings of the program of
interest. We propose a set of correct algorithms for automating the approach.
Then, non-termination inference is tailored to attempt proofs of optimality of
left termination conditions computed by a termination inference tool. An
experimental evaluation is reported. When termination and non-termination
analysis produce complementary results for a logic procedure, then with respect
to the leftmost selection rule and the language used to describe sets of atomic
queries, each analysis is optimal and together, they induce a characterization
of the operational behavior of the logic procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406042</id><created>2004-06-22</created><authors><author><keyname>Vitolins</keyname><forenames>Valdis</forenames></author></authors><title>Business Process Measures</title><categories>cs.CE cs.PF</categories><comments>12 pages, 7 figures, 2 tables, Proceedings of International
  Conference &quot;Baltic DB&amp;IS 2004&quot;, Riga, Latvia (June 6-9, 2004)</comments><acm-class>J.1; J.7; C.4; I.6.5</acm-class><journal-ref>Vitolins Valdis, Business Process Measures. Computer Science and
  Information Technologies, Databases and Information Systems Doctoral
  Consortium, Scientific Papers University of Latvia Vol. 673, University of
  Latvia, 2004, pp. 186.-197</journal-ref><abstract>  The paper proposes a new methodology for defining business process measures
and their computation. The approach is based on metamodeling according to MOF.
Especially, a metamodel providing precise definitions of typical process
measures for UML activity diagram-like notation is proposed, including precise
definitions how measures should be aggregated for composite process elements.
The proposed approach allows defining values in a natural way, and measurement
of data, which are of interest to business, without deep investigation into
specific technical solutions. This provides new possibilities for business
process measurement, decreasing the gap between technical solutions and asset
management methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406043</id><created>2004-06-23</created><updated>2004-06-28</updated><authors><author><keyname>Mielik&#xe4;inen</keyname><forenames>Taneli</forenames></author><author><keyname>Ravantti</keyname><forenames>Janne</forenames></author><author><keyname>Ukkonen</keyname><forenames>Esko</forenames></author></authors><title>The Computational Complexity of Orientation Search Problems in
  Cryo-Electron Microscopy</title><categories>cs.DS cs.CG cs.CV</categories><report-no>C-2004-3, Department of Computer Science, University of Helsinki</report-no><acm-class>F.2.2; I.4.5; J.3</acm-class><abstract>  In this report we study the problem of determining three-dimensional
orientations for noisy projections of randomly oriented identical particles.
The problem is of central importance in the tomographic reconstruction of the
density map of macromolecular complexes from electron microscope images and it
has been studied intensively for more than 30 years.
  We analyze the computational complexity of the orientation problem and show
that while several variants of the problem are $NP$-hard, inapproximable and
fixed-parameter intractable, some restrictions are polynomial-time approximable
within a constant factor or even solvable in logarithmic space. The orientation
search problem is formalized as a constrained line arrangement problem that is
of independent interest. The negative complexity results give a partial
justification for the heuristic methods used in orientation search, and the
positive complexity results on the orientation search have some positive
implications also to the problem of finding functionally analogous genes.
  A preliminary version ``The Computational Complexity of Orientation Search in
Cryo-Electron Microscopy'' appeared in Proc. ICCS 2004, LNCS 3036, pp.
231--238. Springer-Verlag 2004.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406044</id><created>2004-06-23</created><updated>2005-08-30</updated><authors><author><keyname>Harary</keyname><forenames>Frank</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>On the Computational Complexity of the Forcing Chromatic Number</title><categories>cs.CC</categories><comments>24 pages; This is the final version</comments><abstract>  We consider vertex colorings of graphs in which adjacent vertices have
distinct colors. A graph is $s$-chromatic if it is colorable in $s$ colors and
any coloring of it uses at least $s$ colors. The forcing chromatic number
$F(G)$ of an $s$-chromatic graph $G$ is the smallest number of vertices which
must be colored so that, with the restriction that $s$ colors are used, every
remaining vertex has its color determined uniquely. We estimate the
computational complexity of $F(G)$ relating it to the complexity class US
introduced by Blass and Gurevich. We prove that recognizing if $F(G)\le 2$ is
US-hard with respect to polynomial-time many-one reductions. Moreover, this
problem is coNP-hard even under the promises that $F(G)\le 3$ and $G$ is
3-chromatic. On the other hand, recognizing if $F(G)\le k$, for each constant
$k$, is reducible to a problem in US via disjunctive truth-table reduction.
  Similar results are obtained also for forcing variants of the clique and the
domination numbers of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406045</id><created>2004-06-23</created><updated>2005-03-04</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Gal</keyname><forenames>Shmuel</forenames></author></authors><title>Online Searching with Turn Cost</title><categories>cs.DS</categories><comments>15 pages, 2 figures, 1 table; to appear in Theoretical Computer
  Science. Did some minor editorial changes, fixed some typos, etc</comments><acm-class>F.2.2</acm-class><abstract>  We consider the problem of searching for an object on a line at an unknown
distance OPT from the original position of the searcher, in the presence of a
cost of d for each time the searcher changes direction. This is a
generalization of the well-studied linear-search problem. We describe a
strategy that is guaranteed to find the object at a cost of at most 9*OPT + 2d,
which has the optimal competitive ratio 9 with respect to OPT plus the minimum
corresponding additive term. Our argument for upper and lower bound uses an
infinite linear program, which we solve by experimental solution of an infinite
series of approximating finite linear programs, estimating the limits, and
solving the resulting recurrences. We feel that this technique is interesting
in its own right and should help solve other searching problems. In particular,
we consider the star search or cow-path problem with turn cost, where the
hidden object is placed on one of m rays emanating from the original position
of the searcher. For this problem we give a tight bound of
(1+(2(m^m)/((m-1)^(m-1))) OPT + m ((m/(m-1))^(m-1) - 1) d. We also discuss
tradeoff between the corresponding coefficients, and briefly consider
randomized strategies on the line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406046</id><created>2004-06-23</created><authors><author><keyname>Huang</keyname><forenames>Andrew C.</forenames></author><author><keyname>Fox</keyname><forenames>Armando</forenames></author></authors><title>Cheap Recovery: A Key to Self-Managing State</title><categories>cs.NI cs.DC</categories><comments>Submitted to the Sixth Symposium on Operating Systems Design and
  Implementation (OSDI '04)</comments><abstract>  Cluster hash tables (CHTs) are a key persistent-storage component of many
large-scale Internet services due to their high performance and scalability. We
show that a correctly-designed CHT can also be as easy to manage as a farm of
stateless servers. Specifically, we trade away some consistency to obtain
reboot-based recovery that is simple, maintains full data availability, and
only has modest impact on performance. This simplifies management in two ways.
First, it simplifies failure detection by lowering the cost of acting on false
positives, allowing us to use simple but aggressive statistical techniques to
quickly detect potential failures and node degradations; even when a false
alarm is raised or when rebooting will not fix the problem, attempting recovery
by rebooting is relatively non-intrusive to system availability and
performance. Second, it allows us to re-cast online repartitioning as failure
plus recovery, simplifying dynamic scaling and capacity planning. These
properties make it possible for the system to be continuously self-adjusting, a
key property of self-managing, autonomic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406047</id><created>2004-06-24</created><authors><author><keyname>Ososkov</keyname><forenames>G. A.</forenames></author><author><keyname>Dmitrievskiy</keyname><forenames>S. G.</forenames></author><author><keyname>Stadnik</keyname><forenames>A. V.</forenames></author></authors><title>Self-organizing neural networks in classification and image recognition</title><categories>cs.CV cs.AI</categories><abstract>  Self-organizing neural networks are used for brick finding in OPERA
experiment. Self-organizing neural networks and wavelet analysis used for
recognition and extraction of car numbers from images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406048</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406048</id><created>2004-06-25</created><authors><author><keyname>Lal</keyname><forenames>H. L. Janwa A. K.</forenames></author></authors><title>On Expanders Graphs: Parameters and Applications</title><categories>cs.IT math.IT</categories><comments>Submitted to SIAM J. DAM on Feb. 1, 2001</comments><abstract>  We give a new lower bound on the expansion coefficient of an edge-vertex
graph of a $d$-regular graph. As a consequence, we obtain an improvement on the
lower bound on relative minimum distance of the expander codes constructed by
Sipser and Spielman. We also derive some improved results on the vertex
expansion of graphs that help us in improving the parameters of the expander
codes of Alon, Bruck, Naor, Naor, and Roth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406049</id><created>2004-06-25</created><authors><author><keyname>Mendenhall</keyname><forenames>Marcus H.</forenames></author></authors><title>A Fast, Vectorizable Algorithm for Producing Single-Precision
  Sine-Cosine Pairs</title><categories>cs.MS</categories><comments>4 pages, 1 block sample code</comments><abstract>  This paper presents an algorithm for computing Sine-Cosine pairs to modest
accuracy, but in a manner which contains no conditional tests or branching,
making it highly amenable to vectorization. An exemplary implementation for
PowerPC AltiVec processors is included, but the algorithm should be easily
portable to other achitectures, such as Intel SSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406050</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406050</id><created>2004-06-26</created><authors><author><keyname>Amraoui</keyname><forenames>Abdelaziz</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Urbanke</keyname><forenames>Ruediger</forenames></author></authors><title>Finite-Length Scaling for Iteratively Decoded LDPC Ensembles</title><categories>cs.IT cond-mat.dis-nn cs.DM math.IT</categories><comments>45 pages, 14 figures</comments><abstract>  In this paper we investigate the behavior of iteratively decoded low-density
parity-check codes over the binary erasure channel in the so-called ``waterfall
region.&quot; We show that the performance curves in this region follow a very basic
scaling law. We conjecture that essentially the same scaling behavior applies
in a much more general setting and we provide some empirical evidence to
support this conjecture. The scaling law, together with the error floor
expressions developed previously, can be used for fast finite-length
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406051</id><created>2004-06-27</created><authors><author><keyname>Lahiri</keyname><forenames>Somdeb</forenames></author></authors><title>Stable Outcomes for Two-Sided Contract Choice Problems</title><categories>cs.GT</categories><comments>13 pages; source file: MS Word</comments><abstract>  We show, that a simple generalization of the Deferred Acceptance Procedure
with firms proposing due to Gale and Shapley(1962), yeild outcomes for a
two-sided contract choice problem, which necessarily belong to the core and are
Weakly Pareto Optimal for firms. Under additional assumptions: (a) given any
two distinct workers, the set of yields acheivable by a firm with the first
worker is disjoint from the set of yields acheivable by it with the second, and
(b) the contract choice problem is pair-wise efficient, we prove that there is
no stable outcome at which a firm can get more than what it gets at the unique
outcome of our procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406052</id><created>2004-06-28</created><authors><author><keyname>Dornseif</keyname><forenames>Maximillian</forenames></author><author><keyname>Holz</keyname><forenames>Thorsten</forenames></author><author><keyname>Klein</keyname><forenames>Christian N.</forenames></author></authors><title>NoSEBrEaK - Attacking Honeynets</title><categories>cs.CR cs.CY</categories><acm-class>K.6.5; K.5.m</acm-class><journal-ref>Proceedings from the fifth IEEE Systems, Man and Cybernetics
  Information Assurance Workshop, Westpoint, 2004; Pages 123-129</journal-ref><abstract>  It is usually assumed that Honeynets are hard to detect and that attempts to
detect or disable them can be unconditionally monitored. We scrutinize this
assumption and demonstrate a method how a host in a honeynet can be completely
controlled by an attacker without any substantial logging taking place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406053</id><created>2004-06-28</created><updated>2004-07-27</updated><authors><author><keyname>Konwar</keyname><forenames>K.</forenames></author><author><keyname>Mandoiu</keyname><forenames>I.</forenames></author><author><keyname>Russell</keyname><forenames>A.</forenames></author><author><keyname>Shvartsman</keyname><forenames>A.</forenames></author></authors><title>Approximation Algorithms for Minimum PCR Primer Set Selection with
  Amplification Length and Uniqueness Constraints</title><categories>cs.DS cs.DM q-bio.QM</categories><acm-class>F.2.2; G.1.6</acm-class><abstract>  A critical problem in the emerging high-throughput genotyping protocols is to
minimize the number of polymerase chain reaction (PCR) primers required to
amplify the single nucleotide polymorphism loci of interest. In this paper we
study PCR primer set selection with amplification length and uniqueness
constraints from both theoretical and practical perspectives. We give a greedy
algorithm that achieves a logarithmic approximation factor for the problem of
minimizing the number of primers subject to a given upperbound on the length of
PCR amplification products. We also give, using randomized rounding, the first
non-trivial approximation algorithm for a version of the problem that requires
unique amplification of each amplification target. Empirical results on
randomly generated testcases as well as testcases extracted from the from the
National Center for Biotechnology Information's genomic databases show that our
algorithms are highly scalable and produce better results compared to previous
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406054</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406054</id><created>2004-06-28</created><authors><author><keyname>Paijmans</keyname><forenames>J. J.</forenames></author></authors><title>Building a linguistic corpus from bee dance data</title><categories>cs.CL</categories><journal-ref>Proceedings of the first international congres of bioinformatics,
  Havana (Cuba), 2004</journal-ref><abstract>  This paper discusses the problems and possibility of collecting bee dance
data in a linguistic \textit{corpus} and use linguistic instruments such as
Zipf's law and entropy statistics to decide on the question whether the dance
carries information of any kind. We describe this against the historical
background of attempts to analyse non-human communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406055</id><created>2004-06-28</created><authors><author><keyname>Ferrara</keyname><forenames>Andrea</forenames></author></authors><title>Web Services: A Process Algebra Approach</title><categories>cs.AI cs.DB</categories><abstract>  It is now well-admitted that formal methods are helpful for many issues
raised in the Web service area. In this paper we present a framework for the
design and verification of WSs using process algebras and their tools. We
define a two-way mapping between abstract specifications written using these
calculi and executable Web services written in BPEL4WS. Several choices are
available: design and correct errors in BPEL4WS, using process algebra
verification tools, or design and correct in process algebra and automatically
obtaining the corresponding BPEL4WS code. The approaches can be combined.
Process algebra are not useful only for temporal logic verification: we remark
the use of simulation/bisimulation both for verification and for the
hierarchical refinement design method. It is worth noting that our approach
allows the use of any process algebra depending on the needs of the user at
different levels (expressiveness, existence of reasoning tools, user
expertise).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406056</id><created>2004-06-28</created><authors><author><keyname>Bringsjord</keyname><forenames>Selmer</forenames></author><author><keyname>Taylor</keyname><forenames>Joshua</forenames></author></authors><title>P=NP</title><categories>cs.CC cs.AI</categories><abstract>  We claim to resolve the P=?NP problem via a formal argument for P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406057</id><created>2004-06-28</created><authors><author><keyname>Dornseif</keyname><forenames>Maximillian</forenames></author><author><keyname>May</keyname><forenames>Sascha</forenames></author></authors><title>Modelling the costs and benefits of Honeynets</title><categories>cs.CR cs.CY</categories><comments>was presented at the &quot;Third Annual Workshop on Economics and
  Information Security&quot; 2004 (WEIS04)</comments><acm-class>K.4.1; K.6.0; K.6.5</acm-class><abstract>  For many IT-security measures exact costs and benefits are not known. This
makes it difficult to allocate resources optimally to different security
measures. We present a model for costs and benefits of so called Honeynets.
This can foster informed reasoning about the deployment of honeynet technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406058</id><created>2004-06-29</created><updated>2004-07-14</updated><authors><author><keyname>Bauer</keyname><forenames>Matthias</forenames></author></authors><title>Proofs of Zero Knowledge</title><categories>cs.CR cs.DB</categories><comments>16 pages</comments><acm-class>E.2; H.2.0</acm-class><abstract>  We present a protocol for verification of ``no such entry'' replies from
databases. We introduce a new cryptographic primitive as the underlying
structure, the keyed hash tree, which is an extension of Merkle's hash tree. We
compare our scheme to Buldas et al.'s Undeniable Attesters and Micali et al.'s
Zero Knowledge Sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406059</id><created>2004-06-29</created><authors><author><keyname>Dornseif</keyname><forenames>Maximillian</forenames></author><author><keyname>Gaertner</keyname><forenames>Felix C.</forenames></author><author><keyname>Holz</keyname><forenames>Thorsten</forenames></author></authors><title>Ermittlung von Verwundbarkeiten mit elektronischen Koedern</title><categories>cs.CR</categories><comments>Presented at Detection of Intrusions and Malware &amp; Vulnerability
  Assessment (DIMVA 2004), in German</comments><acm-class>K.4.0; K.4.1; K.5.9; K.7.4</acm-class><abstract>  Electronic bait (honeypots) are network resources whose value consists of
being attacked and compromised. These are often computers which do not have a
task in the network, but are otherwise indestinguishable from regular
computers. Such bait systems could be interconnected (honeynets). These
honeynets are equipped with special software, facilitating forensic anylisis of
incidents. Taking average of the wide variety of recorded data it is possible
to learn considerable more about the behaviour of attackers in networks than
with traditional methods. This article is an introduction into electronic bait
and a description of the setup and first experiences of such a network deployed
at RWTH Aachen University.
  -----
  Als elektronische Koeder (honeypots) bezeichnet man Netzwerkressourcen, deren
Wert darin besteht, angegriffen und kompromittiert zu werden. Oft sind dies
Computer, die keine spezielle Aufgabe im Netzwerk haben, aber ansonsten nicht
von regulaeren Rechnern zu unterscheiden sind. Koeder koennen zu
Koeder-Netzwerken (honeynets) zusammengeschlossen werden. Sie sind mit
spezieller Software ausgestattet, die die Forensik einer eingetretenen
Schutzzielverletzung erleichtert. Durch die Vielfalt an mitgeschnittenen Daten
kann man deutlich mehr ueber das Verhalten von Angreifern in Netzwerken lernen
als mit herkoemmlichen forensischen Methoden. Dieser Beitrag stellt die
Philosophie der Koeder-Netzwerke vor und beschreibt die ersten Erfahrungen, die
mit einem solchen Netzwerk an der RWTH Aachen gemacht wurden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406060</id><created>2004-06-29</created><authors><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Van Gucht</keyname><forenames>Dirk</forenames></author><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author></authors><title>Well-Definedness and Semantic Type-Checking in the Nested Relational
  Calculus and XQuery</title><categories>cs.DB cs.PL</categories><abstract>  Two natural decision problems regarding the XML query language XQuery are
well-definedness and semantic type-checking. We study these problems in the
setting of a relational fragment of XQuery. We show that well-definedness and
semantic type-checking are undecidable, even in the positive-existential case.
Nevertheless, for a ``pure'' variant of XQuery, in which no identification is
made between an item and the singleton containing that item, the problems
become decidable. We also consider the analogous problems in the setting of the
nested relational calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0406061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0406061</id><created>2004-06-30</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>The Complexity of Agreement</title><categories>cs.CC cs.GT</categories><comments>27 pages, 4 figures</comments><abstract>  A celebrated 1976 theorem of Aumann asserts that honest, rational Bayesian
agents with common priors will never &quot;agree to disagree&quot;: if their opinions
about any topic are common knowledge, then those opinions must be equal.
Economists have written numerous papers examining the assumptions behind this
theorem. But two key questions went unaddressed: first, can the agents reach
agreement after a conversation of reasonable length? Second, can the
computations needed for that conversation be performed efficiently? This paper
answers both questions in the affirmative, thereby strengthening Aumann's
original conclusion.
  We first show that, for two agents with a common prior to agree within
epsilon about the expectation of a [0,1] variable with high probability over
their prior, it suffices for them to exchange order 1/epsilon^2 bits. This
bound is completely independent of the number of bits n of relevant knowledge
that the agents have. We then extend the bound to three or more agents; and we
give an example where the economists' &quot;standard protocol&quot; (which consists of
repeatedly announcing one's current expectation) nearly saturates the bound,
while a new &quot;attenuated protocol&quot; does better. Finally, we give a protocol that
would cause two Bayesians to agree within epsilon after exchanging order
1/epsilon^2 messages, and that can be simulated by agents with limited
computational resources. By this we mean that, after examining the agents'
knowledge and a transcript of their conversation, no one would be able to
distinguish the agents from perfect Bayesians. The time used by the simulation
procedure is exponential in 1/epsilon^6 but not in n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407001</id><created>2004-07-01</created><authors><author><keyname>Asadzadeh</keyname><forenames>Parvin</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Kei</keyname><forenames>Chun Ling</forenames></author><author><keyname>Nayar</keyname><forenames>Deepa</forenames></author><author><keyname>Venugopal</keyname><forenames>Srikumar</forenames></author></authors><title>Global Grids and Software Toolkits: A Study of Four Grid Middleware
  Technologies</title><categories>cs.DC</categories><comments>19 pages, 10 figures</comments><report-no>* Technical Report, GRIDS-TR-2004-4, Grid Computing and Distributed
  Systems Laboratory, University of Melbourne, Australia, July 1, 2004</report-no><acm-class>C.1.4</acm-class><abstract>  Grid is an infrastructure that involves the integrated and collaborative use
of computers, networks, databases and scientific instruments owned and managed
by multiple organizations. Grid applications often involve large amounts of
data and/or computing resources that require secure resource sharing across
organizational boundaries. This makes Grid application management and
deployment a complex undertaking. Grid middlewares provide users with seamless
computing ability and uniform access to resources in the heterogeneous Grid
environment. Several software toolkits and systems have been developed, most of
which are results of academic research projects, all over the world. This
chapter will focus on four of these middlewares--UNICORE, Globus, Legion and
Gridbus. It also presents our implementation of a resource broker for UNICORE
as this functionality was not supported in it. A comparison of these systems on
the basis of the architecture, implementation model and several other features
is included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407002</id><created>2004-07-01</created><authors><author><keyname>Cyrus</keyname><forenames>Lea</forenames></author><author><keyname>Feddes</keyname><forenames>Hendrik</forenames></author><author><keyname>Schumacher</keyname><forenames>Frank</forenames></author></authors><title>Annotating Predicate-Argument Structure for a Parallel Treebank</title><categories>cs.CL</categories><comments>8 pages, 5 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the LREC 2004 Workshop on Building Lexical
  Resources from Semantically Annotated Corpora, Lisbon, May 30, 2004, pp.
  39-46</journal-ref><abstract>  We report on a recently initiated project which aims at building a
multi-layered parallel treebank of English and German. Particular attention is
devoted to a dedicated predicate-argument layer which is used for aligning
translationally equivalent sentences of the two languages. We describe both our
conceptual decisions and aspects of their technical realisation. We discuss
some selected problems and conclude with a few remarks on how this project
relates to similar projects in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407003</id><created>2004-07-01</created><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel</forenames></author></authors><title>Insertion Sort is O(n log n)</title><categories>cs.DS</categories><comments>6 pages, Latex. In Proceedings of the Third International Conference
  on Fun With Algorithms, FUN 2004</comments><acm-class>E.5; F.2.2</acm-class><abstract>  Traditional Insertion Sort runs in O(n^2) time because each insertion takes
O(n) time. When people run Insertion Sort in the physical world, they leave
gaps between items to accelerate insertions. Gaps help in computers as well.
This paper shows that Gapped Insertion Sort has insertion times of O(log n)
with high probability, yielding a total running time of O(n log n) with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407004</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407004</id><created>2004-07-01</created><authors><author><keyname>Wullschleger</keyname><forenames>J&#xfc;rg</forenames></author></authors><title>Zero-error communication over networks</title><categories>cs.IT cs.CR math.IT</categories><comments>10 pages, full version of the paper presented at the 2004 IEEE
  International Symposium on Information Theory</comments><abstract>  Zero-Error communication investigates communication without any error. By
defining channels without probabilities, results from Elias can be used to
completely characterize which channel can simulate which other channels. We
introduce the ambiguity of a channel, which completely characterizes the
possibility in principle of a channel to simulate any other channel. In the
second part we will look at networks of players connected by channels, while
some players may be corrupted. We will show how the ambiguity of a virtual
channel connecting two arbitrary players can be calculated. This means that we
can exactly specify what kind of zero-error communication is possible between
two players in any network of players connected by channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407005</id><created>2004-07-01</created><updated>2005-11-23</updated><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Statistical Machine Translation by Generalized Parsing</title><categories>cs.CL</categories><comments>45 pages, with fixes for generating correct PDF format</comments><acm-class>I.2.7</acm-class><abstract>  Designers of statistical machine translation (SMT) systems have begun to
employ tree-structured translation models. Systems involving tree-structured
translation models tend to be complex. This article aims to reduce the
conceptual complexity of such systems, in order to make them easier to design,
implement, debug, use, study, understand, explain, modify, and improve. In
service of this goal, the article extends the theory of semiring parsing to
arrive at a novel abstract parsing algorithm with five functional parameters: a
logic, a grammar, a semiring, a search strategy, and a termination condition.
The article then shows that all the common algorithms that revolve around
tree-structured translation models, including hierarchical alignment, inference
for parameter estimation, translation, and structured evaluation, can be
derived by generalizing two of these parameters -- the grammar and the logic.
The article culminates with a recipe for using such generalized parsers to
train, apply, and evaluate an SMT system that is driven by tree-structured
translation models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407006</id><created>2004-07-02</created><authors><author><keyname>Lahiri</keyname><forenames>Shuvendu K.</forenames></author><author><keyname>Bryant</keyname><forenames>Randal E.</forenames></author></authors><title>Predicate Abstraction with Indexed Predicates</title><categories>cs.LO</categories><comments>27 pages, 4 figures, 1 table, short version appeared in International
  Conference on Verification, Model Checking and Abstract Interpretation
  (VMCAI'04), LNCS 2937, pages = 267--281</comments><acm-class>F.3.1</acm-class><abstract>  Predicate abstraction provides a powerful tool for verifying properties of
infinite-state systems using a combination of a decision procedure for a subset
of first-order logic and symbolic methods originally developed for finite-state
model checking. We consider models containing first-order state variables,
where the system state includes mutable functions and predicates. Such a model
can describe systems containing arbitrarily large memories, buffers, and arrays
of identical processes. We describe a form of predicate abstraction that
constructs a formula over a set of universally quantified variables to describe
invariant properties of the first-order state variables. We provide a formal
justification of the soundness of our approach and describe how it has been
used to verify several hardware and software designs, including a
directory-based cache coherence protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407007</id><created>2004-07-02</created><authors><author><keyname>Leinders</keyname><forenames>Dirk</forenames><affiliation>Limburgs Universitair Centrum, Diepenbeek, Belgium</affiliation></author><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames><affiliation>Institute of Informatics, Warsaw University, Warsaw, Poland</affiliation></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames><affiliation>Limburgs Universitair Centrum, Diepenbeek, Belgium</affiliation></author></authors><title>The semijoin algebra and the guarded fragment</title><categories>cs.DB cs.LO</categories><comments>11 pages, 2 figures</comments><acm-class>H.2.3;F.4.1</acm-class><abstract>  The semijoin algebra is the variant of the relational algebra obtained by
replacing the join operator by the semijoin operator. We discuss some
interesting connections between the semijoin algebra and the guarded fragment
of first-order logic. We also provide an Ehrenfeucht-Fraisse game,
characterizing the discerning power of the semijoin algebra. This game gives a
method for showing that certain queries are not expressible in the semijoin
algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407008</id><created>2004-07-02</created><authors><author><keyname>Ravichandran</keyname><forenames>S.</forenames></author><author><keyname>Karthik</keyname><forenames>M. N.</forenames></author></authors><title>Autogenic Training With Natural Language Processing Modules: A Recent
  Tool For Certain Neuro Cognitive Studies</title><categories>cs.AI</categories><comments>2 Pages. Proceedings of 11th International Congress on Biological &amp;
  Medical Engineering, Singapore (IEEE-EMBS &amp; IFMBE endorsed)</comments><acm-class>I.2.1; I.2.6; I.2.7</acm-class><abstract>  Learning to respond to voice-text input involves the subject's ability in
understanding the phonetic and text based contents and his/her ability to
communicate based on his/her experience. The neuro-cognitive facility of the
subject has to support two important domains in order to make the learning
process complete. In many cases, though the understanding is complete, the
response is partial. This is one valid reason why we need to support the
information from the subject with scalable techniques such as Natural Language
Processing (NLP) for abstraction of the contents from the output. This paper
explores the feasibility of using NLP modules interlaced with Neural Networks
to perform the required task in autogenic training related to medical
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407009</id><created>2004-07-02</created><authors><author><keyname>Karthik</keyname><forenames>M. N.</forenames></author><author><keyname>Davis</keyname><forenames>Moshe</forenames></author></authors><title>Search Using N-gram Technique Based Statistical Analysis for Knowledge
  Extraction in Case Based Reasoning Systems</title><categories>cs.AI cs.IR</categories><comments>4 Pages. National Level Mathematics &amp; CS Symposium, Chennai
  Mathematical Institute ICS '02, March</comments><acm-class>H.3.3; I.2.1</acm-class><abstract>  Searching techniques for Case Based Reasoning systems involve extensive
methods of elimination. In this paper, we look at a new method of arriving at
the right solution by performing a series of transformations upon the data.
These involve N-gram based comparison and deduction of the input data with the
case data, using Morphemes and Phonemes as the deciding parameters. A similar
technique for eliminating possible errors using a noise removal function is
performed. The error tracking and elimination is performed through a
statistical analysis of obtained data, where the entire data set is analyzed as
sub-categories of various etymological derivatives. A probability analysis for
the closest match is then performed, which yields the final expression. This
final expression is referred to the Case Base. The output is redirected through
an Expert System based on best possible match. The threshold for the match is
customizable, and could be set by the Knowledge-Architect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407010</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407010</id><created>2004-07-04</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Improved error bounds for the erasure/list scheme: the binary and
  spherical cases</title><categories>cs.IT math.IT</categories><comments>18 pages, 3 figures. Submitted to IEEE Transactions on Informatin
  Theory in May 2001, will appear in Oct. 2004 (tentative)</comments><journal-ref>IEEE Transactions on Informatin Theory, vol. 50, no. 10, 2004, pp.
  2503-2511</journal-ref><abstract>  We derive improved bounds on the error and erasure rate for spherical codes
and for binary linear codes under Forney's erasure/list decoding scheme and
prove some related results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407011</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407011</id><created>2004-07-04</created><updated>2005-07-29</updated><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames></author></authors><title>Distance distribution of binary codes and the error probability of
  decoding</title><categories>cs.IT math.IT</categories><comments>16 pages, 3 figures. Submitted to IEEE Transactions on Information
  Theory. The revision was done for a final journal version (it may still be
  different from the published version)</comments><journal-ref>IEEE Transactions on Information Theory vol. 51, no. 12, pp.
  4237-4246 (2005).</journal-ref><abstract>  We address the problem of bounding below the probability of error under
maximum likelihood decoding of a binary code with a known distance distribution
used on a binary symmetric channel. An improved upper bound is given for the
maximum attainable exponent of this probability (the reliability function of
the channel). In particular, we prove that the ``random coding exponent'' is
the true value of the channel reliability for code rate $R$ in some interval
immediately below the critical rate of the channel. An analogous result is
obtained for the Gaussian channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407012</id><created>2004-07-05</created><authors><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Mehmood</keyname><forenames>Atif</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author></authors><title>A Taxonomy and Survey of Grid Resource Planning and Reservation Systems
  for Grid Enabled Analysis Environment</title><categories>cs.DC</categories><comments>8 pages, 2 figures. Proceedings of the 2004 International Symposium
  on Distributed Computing and Applications to Business Engineering and Science</comments><acm-class>H2.4;J.3</acm-class><abstract>  The concept of coupling geographically distributed resources for solving
large scale problems is becoming increasingly popular forming what is popularly
called grid computing. Management of resources in the Grid environment becomes
complex as the resources are geographically distributed, heterogeneous in
nature and owned by different individuals and organizations each having their
own resource management policies and different access and cost models. There
have been many projects that have designed and implemented the resource
management systems with a variety of architectures and services. In this paper
we have presented the general requirements that a Resource Management system
should satisfy. The taxonomy has also been defined based on which survey of
resource management systems in different existing Grid projects has been
conducted to identify the key areas where these systems lack the desired
functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407013</id><created>2004-07-05</created><authors><author><keyname>Ahmad</keyname><forenames>Naveed</forenames></author><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Hassan</keyname><forenames>Ali</forenames></author><author><keyname>Ikram</keyname><forenames>Ahsan</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author></authors><title>Distributed Analysis and Load Balancing System for Grid Enabled Analysis
  on Hand-held devices using Multi-Agents Systems</title><categories>cs.DC</categories><comments>4 pages, 3 figures. Proceedings of the 3rd International Conference
  on Grid and Cooperative Computing (GCC 2004)</comments><acm-class>H2.4;J.3</acm-class><abstract>  Handheld devices, while growing rapidly, are inherently constrained and lack
the capability of executing resource hungry applications. This paper presents
the design and implementation of distributed analysis and load-balancing system
for hand-held devices using multi-agents system. This system enables low
resource mobile handheld devices to act as potential clients for Grid enabled
applications and analysis environments. We propose a system, in which mobile
agents will transport, schedule, execute and return results for heavy
computational jobs submitted by handheld devices. Moreover, in this way, our
system provides high throughput computing environment for hand-held devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407014</id><created>2004-07-05</created><updated>2004-09-30</updated><authors><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Ikram</keyname><forenames>Ahsan</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author></authors><title>A Grid-enabled Interface to Condor for Interactive Analysis on Handheld
  and Resource-limited Devices</title><categories>cs.DC</categories><comments>This paper has been withdrawn</comments><acm-class>H2.4;J.3</acm-class><abstract>  This paper was withdrawn by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407015</identifier>
 <datestamp>2010-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407015</id><created>2004-07-05</created><updated>2005-06-28</updated><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Suzuki</keyname><forenames>Toshio</forenames></author></authors><title>Resource Bounded Immunity and Simplicity</title><categories>cs.CC cs.DM</categories><comments>This is a complete version of the conference paper that appeared in
  the Proceedings of the 3rd IFIP International Conference on Theoretical
  Computer Science, Kluwer Academic Publishers, pp.81-95, Toulouse, France,
  August 23-26, 2004</comments><journal-ref>(journal version) Theoretical Computer Sceince, Vol.347,
  pp.90-129, 2005</journal-ref><abstract>  Revisiting the thirty years-old notions of resource-bounded immunity and
simplicity, we investigate the structural characteristics of various immunity
notions: strong immunity, almost immunity, and hyperimmunity as well as their
corresponding simplicity notions. We also study limited immunity and
simplicity, called k-immunity and feasible k-immunity, and their simplicity
notions. Finally, we propose the k-immune hypothesis as a working hypothesis
that guarantees the existence of simple sets in NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407016</id><created>2004-07-06</created><authors><author><keyname>Bulitko</keyname><forenames>Vadim</forenames></author></authors><title>Learning for Adaptive Real-time Search</title><categories>cs.AI cs.LG</categories><abstract>  Real-time heuristic search is a popular model of acting and learning in
intelligent autonomous agents. Learning real-time search agents improve their
performance over time by acquiring and refining a value function guiding the
application of their actions. As computing the perfect value function is
typically intractable, a heuristic approximation is acquired instead. Most
studies of learning in real-time search (and reinforcement learning) assume
that a simple value-function-greedy policy is used to select actions. This is
in contrast to practice, where high-performance is usually attained by
interleaving planning and acting via a lookahead search of a non-trivial depth.
In this paper, we take a step toward bridging this gap and propose a novel
algorithm that (i) learns a heuristic function to be used specifically with a
lookahead-based policy, (ii) selects the lookahead depth adaptively in each
state, (iii) gives the user control over the trade-off between exploration and
exploitation. We extensively evaluate the algorithm in the sliding tile puzzle
testbed comparing it to the classical LRTA* and the more recent weighted LRTA*,
bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speed
are observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407017</id><created>2004-07-07</created><authors><author><keyname>Cantino</keyname><forenames>Andrew</forenames><affiliation>Haverford College</affiliation></author><author><keyname>Crawford</keyname><forenames>Fronefield</forenames><affiliation>Haverford College</affiliation></author><author><keyname>Dhital</keyname><forenames>Saurav</forenames><affiliation>Haverford College</affiliation></author><author><keyname>Dougherty</keyname><forenames>John P.</forenames><affiliation>Haverford College</affiliation></author><author><keyname>Sherman</keyname><forenames>Reid</forenames><affiliation>Haverford College</affiliation></author></authors><title>A Low Cost Distributed Computing Approach to Pulsar Searches at a Small
  College</title><categories>cs.DC</categories><comments>Adapted from a talk given at the Eleventh SIAM Conference on Parallel
  Computing, February 25-27, 2004 in San Francisco, CA. 18 pages, including 3
  figures and 2 tables</comments><acm-class>J.2</acm-class><abstract>  We describe a distributed processing cluster of inexpensive Linux machines
developed jointly by the Astronomy and Computer Science departments at
Haverford College which has been successfully used to search a large volume of
data from a recent radio pulsar survey. Analysis of radio pulsar surveys
requires significant computational resources to handle the demanding data
storage and processing needs. One goal of this project was to explore issues
encountered when processing a large amount of pulsar survey data with limited
computational resources. This cluster, which was developed and activated in
only a few weeks by supervised undergraduate summer research students, used
existing decommissioned computers, the campus network, and a script-based,
client-oriented, self-scheduled data distribution approach to process the data.
This setup provided simplicity, efficiency, and &quot;on-the-fly&quot; scalability at low
cost. The entire 570 GB data set from the pulsar survey was processed at
Haverford over the course of a ten-week summer period using this cluster. We
conclude that this cluster can serve as a useful computational model in cases
where data processing must be carried out on a limited budget. We have also
constructed a DVD archive of the raw survey data in order to investigate the
feasibility of using DVD as an inexpensive and easily accessible raw data
storage format for pulsar surveys. DVD-based storage has not been widely
explored in the pulsar community, but it has several advantages. The DVD
archive we have constructed is reliable, portable, inexpensive, and can be
easily read by any standard modern machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407018</id><created>2004-07-07</created><updated>2004-12-18</updated><authors><author><keyname>Ganguly</keyname><forenames>Pritam</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen A.</forenames></author><author><keyname>Papoulia</keyname><forenames>Katerina D.</forenames></author></authors><title>An algorithm for two-dimensional mesh generation based on the pinwheel
  tiling</title><categories>cs.CG cs.NA</categories><comments>Short version appears in Proceedings of 2004 International Meshing
  Roundtable at http://www.imr.sandia.gov</comments><acm-class>F.2.2;G.1.8;J.2</acm-class><abstract>  We propose a new two-dimensional meshing algorithm called PINW able to
generate meshes that accurately approximate the distance between any two domain
points by paths composed only of cell edges. This technique is based on an
extension of pinwheel tilings proposed by Radin and Conway. We prove that the
algorithm produces triangles of bounded aspect ratio. This kind of mesh would
be useful in cohesive interface finite element modeling when the crack
propagation pathis an outcome of a simulation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407019</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407019</id><created>2004-07-08</created><updated>2012-04-12</updated><authors><author><keyname>Jurkovic</keyname><forenames>Franc</forenames></author></authors><title>Stochastic fuzzy controller</title><categories>cs.AR</categories><comments>Withdrawn by the author</comments><acm-class>B.0</acm-class><abstract>  A standard approach to building a fuzzy controller based on stochastic logic
uses binary random signals with an average (expected value of a random
variable) in the range [0, 1]. A different approach is presented, founded on a
representation of the membership functions with the probability density
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407020</id><created>2004-07-08</created><authors><author><keyname>Panigrahy</keyname><forenames>Rina</forenames></author></authors><title>Minimum Enclosing Polytope in High Dimensions</title><categories>cs.CG</categories><comments>Submitted to SODA05</comments><acm-class>F.2.2</acm-class><abstract>  We study the problem of covering a given set of $n$ points in a high,
$d$-dimensional space by the minimum enclosing polytope of a given arbitrary
shape. We present algorithms that work for a large family of shapes, provided
either only translations and no rotations are allowed, or only rotation about a
fixed point is allowed; that is, one is allowed to only scale and translate a
given shape, or scale and rotate the shape around a fixed point. Our algorithms
start with a polytope guessed to be of optimal size and iteratively moves it
based on a greedy principle: simply move the current polytope directly towards
any outside point till it touches the surface. For computing the minimum
enclosing ball, this gives a simple greedy algorithm with running time
$O(nd/\eps)$ producing a ball of radius $1+\eps$ times the optimal. This simple
principle generalizes to arbitrary convex shape when only translations are
allowed, requiring at most $O(1/\eps^2)$ iterations. Our algorithm implies that
{\em core-sets} of size $O(1/\eps^2)$ exist not only for minimum enclosing ball
but also for any convex shape with a fixed orientation. A {\em Core-Set} is a
small subset of $poly(1/\eps)$ points whose minimum enclosing polytope is
almost as large as that of the original points. Although we are unable to
combine our techniques for translations and rotations for general shapes, for
the min-cylinder problem, we give an algorithm similar to the one in
\cite{HV03}, but with an improved running time of $2^{O(\frac{1}{\eps^2}\log
\frac{1}{\eps})} nd$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407021</id><created>2004-07-08</created><updated>2004-07-14</updated><authors><author><keyname>Li</keyname><forenames>Sanjiang</forenames></author><author><keyname>Wang</keyname><forenames>Huaiqing</forenames></author></authors><title>Multi-agent coordination using nearest neighbor rules: revisiting the
  Vicsek model</title><categories>cs.MA cs.AI</categories><comments>11 pages, linguistic mistakes corrected, title modified</comments><abstract>  Recently, Jadbabaie, Lin, and Morse (IEEE TAC, 48(6)2003:988-1001) offered a
mathematical analysis of the discrete time model of groups of mobile autonomous
agents raised by Vicsek et al. in 1995. In their paper, Jadbabaie et al. showed
that all agents shall move in the same heading, provided that these agents are
periodically linked together. This paper sharpens this result by showing that
coordination will be reached under a very weak condition that requires all
agents are finally linked together. This condition is also strictly weaker than
the one Jadbabaie et al. desired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407022</identifier>
 <datestamp>2008-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407022</id><created>2004-07-09</created><updated>2008-04-04</updated><authors><author><keyname>Boman</keyname><forenames>Erik</forenames></author><author><keyname>Hendrickson</keyname><forenames>Bruce</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen</forenames></author></authors><title>Solving Elliptic Finite Element Systems in Near-Linear Time with Support
  Preconditioners</title><categories>cs.NA</categories><comments>Added ref to Chen &amp; Toledo in Section 9 Added description of how to
  form RHS; added ref to Wang and Sarin</comments><acm-class>F.2.1</acm-class><abstract>  We consider linear systems arising from the use of the finite element method
for solving scalar linear elliptic problems. Our main result is that these
linear systems, which are symmetric and positive semidefinite, are well
approximated by symmetric diagonally dominant matrices. Our framework for
defining matrix approximation is support theory. Significant graph theoretic
work has already been developed in the support framework for preconditioners in
the diagonally dominant case, and in particular it is known that such systems
can be solved with iterative methods in nearly linear time. Thus, our
approximation result implies that these graph theoretic techniques can also
solve a class of finite element problems in nearly linear time. We show that
the support number bounds, which control the number of iterations in the
preconditioned iterative solver, depend on mesh quality measures but not on the
problem size or shape of the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407023</id><created>2004-07-09</created><authors><author><keyname>Panigrahy</keyname><forenames>Rina</forenames></author></authors><title>Efficient Hashing with Lookups in two Memory Accesses</title><categories>cs.DS</categories><comments>Submitted to SODA05</comments><acm-class>E.2</acm-class><abstract>  The study of hashing is closely related to the analysis of balls and bins. It
is well-known that instead of using a single hash function if we randomly hash
a ball into two bins and place it in the smaller of the two, then this
dramatically lowers the maximum load on bins. This leads to the concept of
two-way hashing where the largest bucket contains $O(\log\log n)$ balls with
high probability. The hash look up will now search in both the buckets an item
hashes to. Since an item may be placed in one of two buckets, we could
potentially move an item after it has been initially placed to reduce maximum
load. with a maximum load of We show that by performing moves during inserts, a
maximum load of 2 can be maintained on-line, with high probability, while
supporting hash update operations. In fact, with $n$ buckets, even if the space
for two items are pre-allocated per bucket, as may be desirable in hardware
implementations, more than $n$ items can be stored giving a high memory
utilization. We also analyze the trade-off between the number of moves
performed during inserts and the maximum load on a bucket. By performing at
most $h$ moves, we can maintain a maximum load of $O(\frac{\log \log n}{h
\log(\log\log n/h)})$. So, even by performing one move, we achieve a better
bound than by performing no moves at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407024</id><created>2004-07-10</created><authors><author><keyname>Athanasiadis</keyname><forenames>Ioannis N</forenames></author><author><keyname>Mitkas</keyname><forenames>Pericles A</forenames></author></authors><title>An agent-based intelligent environmental monitoring system</title><categories>cs.MA cs.CE</categories><comments>Multi-Agent Systems, Intelligent Applications, Data Mining, Inductive
  Agents, Air-Quality Monitoring</comments><journal-ref>Management of Environmental Quality, 15(3):238-249, May 2004</journal-ref><doi>10.1108/14777830410531216</doi><abstract>  Fairly rapid environmental changes call for continuous surveillance and
on-line decision making. There are two main areas where IT technologies can be
valuable. In this paper we present a multi-agent system for monitoring and
assessing air-quality attributes, which uses data coming from a meteorological
station. A community of software agents is assigned to monitor and validate
measurements coming from several sensors, to assess air-quality, and, finally,
to fire alarms to appropriate recipients, when needed. Data mining techniques
have been used for adding data-driven, customized intelligence into agents. The
architecture of the developed system, its domain ontology, and typical agent
interactions are presented. Finally, the deployment of a real-world test case
is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407025</id><created>2004-07-10</created><authors><author><keyname>Mitkas</keyname><forenames>P.</forenames></author><author><keyname>Symeonidis</keyname><forenames>A.</forenames></author><author><keyname>Kechagias</keyname><forenames>D.</forenames></author><author><keyname>Athanasiadis</keyname><forenames>I. N.</forenames></author><author><keyname>Laleci</keyname><forenames>G.</forenames></author><author><keyname>Kurt</keyname><forenames>G.</forenames></author><author><keyname>Kabak</keyname><forenames>Y.</forenames></author><author><keyname>Acar</keyname><forenames>A.</forenames></author><author><keyname>Dogac</keyname><forenames>A.</forenames></author></authors><title>An agent framework for dynamic agent retraining: Agent academy</title><categories>cs.MA</categories><journal-ref>In B. Stanford-Smith, E. Chiozza, and M. Edin, editors, Challenges
  and Achievements in e-business and e-work, pages 757-764, Prague, Czech
  Republic, October 2002. IOS Press</journal-ref><abstract>  Agent Academy (AA) aims to develop a multi-agent society that can train new
agents for specific or general tasks, while constantly retraining existing
agents in a recursive mode. The system is based on collecting information both
from the environment and the behaviors of the acting agents and their related
successes/failures to generate a body of data, stored in the Agent Use
Repository, which is mined by the Data Miner module, in order to generate
useful knowledge about the application domain. Knowledge extracted by the Data
Miner is used by the Agent Training Module as to train new agents or to enhance
the behavior of agents already running. In this paper the Agent Academy
framework is introduced, and its overall architecture and functionality are
presented. Training issues as well as agent ontologies are discussed. Finally,
a scenario, which aims to provide environmental alerts to both individuals and
public authorities, is described an AA-based use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407026</id><created>2004-07-10</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Summarizing Encyclopedic Term Descriptions on the Web</title><categories>cs.CL</categories><comments>7 pages, Proceedings of the 20th International Conference on
  Computational Linguistics (to appear)</comments><acm-class>I.2.7; H.3.3; H.3.5</acm-class><journal-ref>Proceedings of the 20th International Conference on Computational
  Linguistics (COLING 2004), pp.645-651, Aug. 2004</journal-ref><abstract>  We are developing an automatic method to compile an encyclopedic corpus from
the Web. In our previous work, paragraph-style descriptions for a term are
extracted from Web pages and organized based on domains. However, these
descriptions are independent and do not comprise a condensed text as in
hand-crafted encyclopedias. To resolve this problem, we propose a summarization
method, which produces a single text from multiple descriptions. The resultant
summary concisely describes a term from different viewpoints. We also show the
effectiveness of our method by means of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407027</id><created>2004-07-10</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author><author><keyname>Akiba</keyname><forenames>Tomoyosi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Unsupervised Topic Adaptation for Lecture Speech Retrieval</title><categories>cs.CL</categories><comments>4 pages, Proceedings of the 8th International Conference on Spoken
  Language Processing (to appear)</comments><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>Proceedings of the 8th International Conference on Spoken Language
  Processing (ICSLP 2004), pp.2957-2960, Oct. 2004</journal-ref><abstract>  We are developing a cross-media information retrieval system, in which users
can view specific segments of lecture videos by submitting text queries. To
produce a text index, the audio track is extracted from a lecture video and a
transcription is generated by automatic speech recognition. In this paper, to
improve the quality of our retrieval system, we extensively investigate the
effects of adapting acoustic and language models on speech recognition. We
perform an MLLR-based method to adapt an acoustic model. To obtain a corpus for
language model adaptation, we use the textbook for a target lecture to search a
Web collection for the pages associated with the lecture topic. We show the
effectiveness of our method by means of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407028</id><created>2004-07-10</created><authors><author><keyname>Akiba</keyname><forenames>Tomoyosi</forenames></author><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author></authors><title>Effects of Language Modeling on Speech-driven Question Answering</title><categories>cs.CL</categories><comments>4 pages, Proceedings of the 8th International Conference on Spoken
  Language Processing (to appear)</comments><acm-class>I.2.7; H.3.3; H.3.4; H.5.1</acm-class><journal-ref>Proceedings of the 8th International Conference on Spoken Language
  Processing (ICSLP 2004), pp.1053-1056, Oct. 2004</journal-ref><abstract>  We integrate automatic speech recognition (ASR) and question answering (QA)
to realize a speech-driven QA system, and evaluate its performance. We adapt an
N-gram language model to natural language questions, so that the input of our
system can be recognized with a high accuracy. We target WH-questions which
consist of the topic part and fixed phrase used to ask about something. We
first produce a general N-gram model intended to recognize the topic and
emphasize the counts of the N-grams that correspond to the fixed phrases. Given
a transcription by the ASR engine, the QA engine extracts the answer candidates
from target documents. We propose a passage retrieval method robust against
recognition errors in the transcription. We use the QA test collection produced
in NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness
of our method by means of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407029</id><created>2004-07-10</created><authors><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author></authors><title>Static versus Dynamic Arbitrage Bounds on Multivariate Option Prices</title><categories>cs.CE</categories><comments>Submitted to IMA series</comments><abstract>  We compare static arbitrage price bounds on basket calls, i.e. bounds that
only involve buy-and-hold trading strategies, with the price range obtained
within a multi-variate generalization of the Black-Scholes model. While there
is no gap between these two sets of prices in the univariate case, we observe
here that contrary to our intuition about model risk for at-the-money calls,
there is a somewhat large gap between model prices and static arbitrage prices,
hence a similarly large set of prices on which a multivariate Black-Scholes
model cannot be calibrated but where no conclusion can be drawn on the presence
or not of a static arbitrage opportunity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407030</id><created>2004-07-12</created><authors><author><keyname>Eiden</keyname><forenames>Wolfgang Anthony</forenames></author></authors><title>Scheduling with Fuzzy Methods</title><categories>cs.OH</categories><report-no>20040628</report-no><abstract>  Nowadays, manufacturing industries -- driven by fierce competition and rising
customer requirements -- are forced to produce a broader range of individual
products of rising quality at the same (or preferably lower) cost. Meeting
these demands implies an even more complex production process and thus also an
appropriately increasing request to its scheduling. Aggravatingly, vagueness of
scheduling parameters -- such as times and conditions -- are often inherent in
the production process. In addition, the search for an optimal schedule
normally leads to very difficult problems (NP-hard problems in the complexity
theoretical sense), which cannot be solved effciently. With the intent to
minimize these problems, the introduced heuristic method combines standard
scheduling methods with fuzzy methods to get a nearly optimal schedule within
an appropriate time considering vagueness adequately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407031</id><created>2004-07-12</created><authors><author><keyname>Naumov</keyname><forenames>Pavel</forenames></author></authors><title>On Modal Logics of Partial Recursive Functions</title><categories>cs.LO</categories><abstract>  The classical propositional logic is known to be sound and complete with
respect to the set semantics that interprets connectives as set operations. The
paper extends propositional language by a new binary modality that corresponds
to partial recursive function type constructor under the above interpretation.
The cases of deterministic and non-deterministic functions are considered and
for both of them semantically complete modal logics are described and
decidability of these logics is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407032</id><created>2004-07-13</created><authors><author><keyname>Nathan</keyname><forenames>Darran</forenames></author></authors><title>Exposing Software Defined Radio Functionality To Native Operating System
  Applications via Virtual Devices</title><categories>cs.AR</categories><comments>4 pages, 9 figures</comments><acm-class>D.2.11</acm-class><abstract>  Many reconfigurable platforms require that applications be written
specifically to take advantage of the reconfigurable hardware. In a PC-based
environment, this presents an undesirable constraint in that the many already
available applications cannot leverage on such hardware. Greatest benefit can
only be derived from reconfigurable devices if even native OS applications can
transparently utilize reconfigurable devices as they would normal full-fledged
hardware devices. This paper presents how Proteus Virtual Devices are used to
expose reconfigurable hardware in a transparent manner for use by typical
native OS applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407033</identifier>
 <datestamp>2011-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407033</id><created>2004-07-13</created><authors><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Por</keyname><forenames>Attila</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Track Layouts of Graphs</title><categories>cs.DM cs.CG</categories><comments>The paper is submitted for publication. Preliminary draft appeared as
  Technical Report TR-2003-07, School of Computer Science, Carleton University,
  Ottawa, Canada</comments><journal-ref>Discrete Maths. &amp; Theoretical Computer Science 6.2:497-522, 2004</journal-ref><abstract>  A \emph{$(k,t)$-track layout} of a graph $G$ consists of a (proper) vertex
$t$-colouring of $G$, a total order of each vertex colour class, and a
(non-proper) edge $k$-colouring such that between each pair of colour classes
no two monochromatic edges cross. This structure has recently arisen in the
study of three-dimensional graph drawings. This paper presents the beginnings
of a theory of track layouts. First we determine the maximum number of edges in
a $(k,t)$-track layout, and show how to colour the edges given fixed linear
orderings of the vertex colour classes. We then describe methods for the
manipulation of track layouts. For example, we show how to decrease the number
of edge colours in a track layout at the expense of increasing the number of
tracks, and vice versa. We then study the relationship between track layouts
and other models of graph layout, namely stack and queue layouts, and geometric
thickness. One of our principle results is that the queue-number and
track-number of a graph are tied, in the sense that one is bounded by a
function of the other. As corollaries we prove that acyclic chromatic number is
bounded by both queue-number and stack-number. Finally we consider track
layouts of planar graphs. While it is an open problem whether planar graphs
have bounded track-number, we prove bounds on the track-number of outerplanar
graphs, and give the best known lower bound on the track-number of planar
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407034</id><created>2004-07-15</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>On the Complexity of Case-Based Planning</title><categories>cs.AI cs.CC</categories><acm-class>I.2.8</acm-class><abstract>  We analyze the computational complexity of problems related to case-based
planning: planning when a plan for a similar instance is known, and planning
from a library of plans. We prove that planning from a single case has the same
complexity than generative planning (i.e., planning &quot;from scratch&quot;); using an
extended definition of cases, complexity is reduced if the domain stored in the
case is similar to the one to search plans for. Planning from a library of
cases is shown to have the same complexity. In both cases, the complexity of
planning remains, in the worst case, PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407035</id><created>2004-07-15</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Haritsa</keyname><forenames>Jayant R.</forenames></author></authors><title>A Framework for High-Accuracy Privacy-Preserving Mining</title><categories>cs.DB cs.IR</categories><report-no>TR-2004-02, DSL/SERC, Indian Institute of Science</report-no><abstract>  To preserve client privacy in the data mining process, a variety of
techniques based on random perturbation of data records have been proposed
recently. In this paper, we present a generalized matrix-theoretic model of
random perturbation, which facilitates a systematic approach to the design of
perturbation mechanisms for privacy-preserving mining. Specifically, we
demonstrate that (a) the prior techniques differ only in their settings for the
model parameters, and (b) through appropriate choice of parameter settings, we
can derive new perturbation techniques that provide highly accurate mining
results even under strict privacy guarantees. We also propose a novel
perturbation mechanism wherein the model parameters are themselves
characterized as random variables, and demonstrate that this feature provides
significant improvements in privacy at a very marginal cost in accuracy.
  While our model is valid for random-perturbation-based privacy-preserving
mining in general, we specifically evaluate its utility here with regard to
frequent-itemset mining on a variety of real datasets. The experimental results
indicate that our mechanisms incur substantially lower identity and support
errors as compared to the prior techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407036</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407036</id><created>2004-07-15</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>All Maximal Independent Sets and Dynamic Dominance for Sparse Graphs</title><categories>cs.DS</categories><comments>10 pages</comments><acm-class>F.2.2</acm-class><journal-ref>ACM Trans. Algorithms 5(4):A38, 2009</journal-ref><doi>10.1145/1597036.1597042</doi><abstract>  We describe algorithms, based on Avis and Fukuda's reverse search paradigm,
for listing all maximal independent sets in a sparse graph in polynomial time
and delay per output. For bounded degree graphs, our algorithms take constant
time per set generated; for minor-closed graph families, the time is O(n) per
set, and for more general sparse graph families we achieve subquadratic time
per set. We also describe new data structures for maintaining a dynamic vertex
set S in a sparse or minor-closed graph family, and querying the number of
vertices not dominated by S; for minor-closed graph families the time per
update is constant, while it is sublinear for any sparse graph family. We can
also maintain a dynamic vertex set in an arbitrary m-edge graph and test the
independence of the maintained set in time O(sqrt m) per update. We use the
domination data structures as part of our enumeration algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407037</id><created>2004-07-16</created><authors><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Murty</keyname><forenames>M. Narasimha</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Generalized Evolutionary Algorithm based on Tsallis Statistics</title><categories>cs.AI</categories><comments>Submitted to Physical Review E, 5 pages, 6 figures</comments><abstract>  Generalized evolutionary algorithm based on Tsallis canonical distribution is
proposed. The algorithm uses Tsallis generalized canonical distribution to
weigh the configurations for `selection' instead of Gibbs-Boltzmann
distribution. Our simulation results show that for an appropriate choice of
non-extensive index that is offered by Tsallis statistics, evolutionary
algorithms based on this generalization outperform algorithms based on
Gibbs-Boltzmann distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407038</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407038</id><created>2004-07-16</created><authors><author><keyname>Bhaduri</keyname><forenames>Purandar</forenames><affiliation>TRDDC, Pune, India</affiliation></author><author><keyname>Ramesh</keyname><forenames>S.</forenames><affiliation>IIT Bombay, India</affiliation></author></authors><title>Model Checking of Statechart Models: Survey and Research Directions</title><categories>cs.SE</categories><acm-class>D.2.4 Software/Program Verification</acm-class><abstract>  We survey existing approaches to the formal verification of statecharts using
model checking. Although the semantics and subset of statecharts used in each
approach varies considerably, along with the model checkers and their
specification languages, most approaches rely on translating the hierarchical
structure into the flat representation of the input language of the model
checker. This makes model checking difficult to scale to industrial models, as
the state space grows exponentially with flattening. We look at current
approaches to model checking hierarchical structures and find that their
semantics is significantly different from statecharts. We propose to address
the problem of state space explosion using a combination of techniques, which
are proposed as directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407039</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407039</id><created>2004-07-16</created><authors><author><keyname>Poland</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>On the Convergence Speed of MDL Predictions for Bernoulli Sequences</title><categories>cs.LG cs.AI cs.IT math.IT math.PR</categories><comments>17 pages</comments><report-no>IDSIA-13-04</report-no><acm-class>I.2.6; E.4; G.3</acm-class><journal-ref>Proc. 15th International Conf. on Algorithmic Learning Theory
  (ALT-2004), pages 294-308</journal-ref><abstract>  We consider the Minimum Description Length principle for online sequence
prediction. If the underlying model class is discrete, then the total expected
square loss is a particularly interesting performance measure: (a) this
quantity is bounded, implying convergence with probability one, and (b) it
additionally specifies a `rate of convergence'. Generally, for MDL only
exponential loss bounds hold, as opposed to the linear bounds for a Bayes
mixture. We show that this is even the case if the model class contains only
Bernoulli distributions. We derive a new upper bound on the prediction error
for countable Bernoulli classes. This implies a small bound (comparable to the
one for Bayes mixtures) for certain important model classes. The results apply
to many Machine Learning tasks including classification and hypothesis testing.
We provide arguments that our theorems generalize to countable classes of
i.i.d. models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407040</id><created>2004-07-16</created><authors><author><keyname>van Hoeve</keyname><forenames>W. J.</forenames></author><author><keyname>Milano</keyname><forenames>M.</forenames></author></authors><title>Decomposition Based Search - A theoretical and experimental evaluation</title><categories>cs.AI</categories><comments>16 pages, 8 figures. LIA Technical Report LIA00203, University of
  Bologna, 2003</comments><acm-class>I.2.8; D.3.3</acm-class><abstract>  In this paper we present and evaluate a search strategy called Decomposition
Based Search (DBS) which is based on two steps: subproblem generation and
subproblem solution. The generation of subproblems is done through value
ranking and domain splitting. Subdomains are explored so as to generate,
according to the heuristic chosen, promising subproblems first.
  We show that two well known search strategies, Limited Discrepancy Search
(LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First
we present a tuning of DBS that visits the same search nodes as IB, but avoids
restarts. Then we compare both theoretically and computationally DBS and LDS
using the same heuristic. We prove that DBS has a higher probability of being
successful than LDS on a comparable number of nodes, under realistic
assumptions. Experiments on a constraint satisfaction problem and an
optimization problem show that DBS is indeed very effective if compared to LDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407041</id><created>2004-07-16</created><authors><author><keyname>van Hoeve</keyname><forenames>Willem Jan</forenames></author></authors><title>Exploiting Semidefinite Relaxations in Constraint Programming</title><categories>cs.DM cs.PL</categories><comments>18 pages, 4 figures. Submitted preprint</comments><acm-class>G.1.6; G.2.2; D.3.3</acm-class><abstract>  Constraint programming uses enumeration and search tree pruning to solve
combinatorial optimization problems. In order to speed up this solution
process, we investigate the use of semidefinite relaxations within constraint
programming. In principle, we use the solution of a semidefinite relaxation to
guide the traversal of the search tree, using a limited discrepancy search
strategy. Furthermore, a semidefinite relaxation produces a bound for the
solution value, which is used to prune parts of the search tree. Experimental
results on stable set and maximum clique problem instances show that constraint
programming can indeed greatly benefit from semidefinite relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407042</id><created>2004-07-16</created><authors><author><keyname>van Hoeve</keyname><forenames>Willem Jan</forenames></author><author><keyname>Milano</keyname><forenames>Michela</forenames></author></authors><title>Postponing Branching Decisions</title><categories>cs.AI</categories><comments>11 pages, 3 figures</comments><acm-class>I.2.8; D.3.3</acm-class><abstract>  Solution techniques for Constraint Satisfaction and Optimisation Problems
often make use of backtrack search methods, exploiting variable and value
ordering heuristics. In this paper, we propose and analyse a very simple method
to apply in case the value ordering heuristic produces ties: postponing the
branching decision. To this end, we group together values in a tie, branch on
this sub-domain, and defer the decision among them to lower levels of the
search tree. We show theoretically and experimentally that this simple
modification can dramatically improve the efficiency of the search strategy.
Although in practise similar methods may have been applied already, to our
knowledge, no empirical or theoretical study has been proposed in the
literature to identify when and to what extent this strategy should be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407043</id><created>2004-07-16</created><authors><author><keyname>van Hoeve</keyname><forenames>Willem Jan</forenames></author></authors><title>A Hyper-Arc Consistency Algorithm for the Soft Alldifferent Constraint</title><categories>cs.PL</categories><comments>11 pages, 1 figure</comments><acm-class>D.3.2; D.3.3; G.2.2</acm-class><abstract>  This paper presents an algorithm that achieves hyper-arc consistency for the
soft alldifferent constraint. To this end, we prove and exploit the equivalence
with a minimum-cost flow problem. Consistency of the constraint can be checked
in O(nm) time, and hyper-arc consistency is achieved in O(m) time, where n is
the number of variables involved and m is the sum of the cardinalities of the
domains. It improves a previous method that did not ensure hyper-arc
consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407044</id><created>2004-07-16</created><authors><author><keyname>Milano</keyname><forenames>M.</forenames></author><author><keyname>van Hoeve</keyname><forenames>W. J.</forenames></author></authors><title>Reduced cost-based ranking for generating promising subproblems</title><categories>cs.AI</categories><comments>15 pages, 1 figure. Accepted at CP 2002</comments><acm-class>I.2.8; G.1.6; D.3.3</acm-class><abstract>  In this paper, we propose an effective search procedure that interleaves two
steps: subproblem generation and subproblem solution. We mainly focus on the
first part. It consists of a variable domain value ranking based on reduced
costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search
tree, the most promising subproblems first. An interesting result is that
reduced costs provide a very precise ranking that allows to almost always find
the optimal solution in the first generated subproblem, even if its dimension
is significantly smaller than that of the original problem. Concerning the
proof of optimality, we exploit a way to increase the lower bound for
subproblems at higher discrepancies. We show experimental results on the TSP
and its time constrained variant to show the effectiveness of the proposed
approach, but the technique could be generalized for other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407045</id><created>2004-07-17</created><updated>2004-10-03</updated><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>The First-Order Theory of Sets with Cardinality Constraints is Decidable</title><categories>cs.LO cs.PL</categories><comments>18 pages</comments><report-no>MIT CSAIL 958</report-no><abstract>  We show that the decidability of the first-order theory of the language that
combines Boolean algebras of sets of uninterpreted elements with Presburger
arithmetic operations. We thereby disprove a recent conjecture that this theory
is undecidable. Our language allows relating the cardinalities of sets to the
values of integer variables, and can distinguish finite and infinite sets. We
use quantifier elimination to show the decidability and obtain an elementary
upper bound on the complexity.
  Precise program analyses can use our decidability result to verify
representation invariants of data structures that use an integer field to
represent the number of stored elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407046</id><created>2004-07-19</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames></author><author><keyname>Ulrich</keyname><forenames>Stefan</forenames></author><author><keyname>Hammervold</keyname><forenames>Kathrine</forenames></author></authors><title>A Bimachine Compiler for Ranked Tagging Rules</title><categories>cs.CL</categories><comments>7 pages, 3 figures Proceedings of COLING 2004 (to appear)</comments><acm-class>I.2.7;F.4.2;F.4.3</acm-class><abstract>  This paper describes a novel method of compiling ranked tagging rules into a
deterministic finite-state device called a bimachine. The rules are formulated
in the framework of regular rewrite operations and allow unrestricted regular
expressions in both left and right rule contexts. The compiler is illustrated
by an application within a speech synthesis system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407047</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407047</id><created>2004-07-19</created><updated>2005-09-26</updated><authors><author><keyname>Levin</keyname><forenames>David N.</forenames><affiliation>U.of Chicago</affiliation></author></authors><title>Channel-Independent and Sensor-Independent Stimulus Representations</title><categories>cs.CV cs.AI</categories><comments>The results of a numerically simulated experiment, which illustrates
  the proposed method, have been added to the version submitted on October 27,
  2004. This paper has been accepted for publication in the Journal of Applied
  Physics. For related papers, see http://www.geocities.com/dlevin2001/</comments><acm-class>I.5; I.5.4.m; I.5.2.b; I.2.10.f; I.2.4.j; I.2.7.g; I.5.4.b; I.2;
  I.2.0.b</acm-class><doi>10.1063/1.2128687</doi><abstract>  This paper shows how a machine, which observes stimuli through an
uncharacterized, uncalibrated channel and sensor, can glean machine-independent
information (i.e., channel- and sensor-independent information) about the
stimuli. First, we demonstrate that a machine defines a specific coordinate
system on the stimulus state space, with the nature of that coordinate system
depending on the device's channel and sensor. Thus, machines with different
channels and sensors &quot;see&quot; the same stimulus trajectory through state space,
but in different machine-specific coordinate systems. For a large variety of
physical stimuli, statistical properties of that trajectory endow the stimulus
configuration space with differential geometric structure (a metric and
parallel transfer procedure), which can then be used to represent relative
stimulus configurations in a coordinate-system-independent manner (and,
therefore, in a channel- and sensor-independent manner). The resulting
description is an &quot;inner&quot; property of the stimulus time series in the sense
that it does not depend on extrinsic factors like the observer's choice of a
coordinate system in which the stimulus is viewed (i.e., the observer's choice
of channel and sensor). This methodology is illustrated with analytic examples
and with a numerically simulated experiment. In an intelligent sensory device,
this kind of representation &quot;engine&quot; could function as a &quot;front-end&quot; that
passes channel/sensor-independent stimulus representations to a pattern
recognition module. After a pattern recognizer has been trained in one of these
devices, it could be used without change in other devices having different
channels and sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407048</id><created>2004-07-19</created><authors><author><keyname>Balthrop</keyname><forenames>Justin</forenames></author><author><keyname>Forrest</keyname><forenames>Stephanie</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author><author><keyname>Williamson</keyname><forenames>Matthew M.</forenames></author></authors><title>Technological networks and the spread of computer viruses</title><categories>cs.NI cs.CY</categories><comments>9 pages, 1 figure</comments><journal-ref>Science 304, 527-529 (2004)</journal-ref><abstract>  Computer infections such as viruses and worms spread over networks of
contacts between computers, with different types of networks being exploited by
different types of infections. Here we analyze the structures of several of
these networks, exploring their implications for modes of spread and the
control of infection. We argue that vaccination strategies that focus on a
limited number of network nodes, whether targeted or randomly chosen, are in
many cases unlikely to be effective. An alternative dynamic mechanism for the
control of contagion, called throttling, is introduced and argued to be
effective under a range of conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407049</id><created>2004-07-19</created><authors><author><keyname>Van Nieuwenborgh</keyname><forenames>Davy</forenames></author><author><keyname>Vermeir</keyname><forenames>Dirk</forenames></author></authors><title>Preferred Answer Sets for Ordered Logic Programs</title><categories>cs.LO cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><abstract>  We extend answer set semantics to deal with inconsistent programs (containing
classical negation), by finding a ``best'' answer set. Within the context of
inconsistent programs, it is natural to have a partial order on rules,
representing a preference for satisfying certain rules, possibly at the cost of
violating less important ones. We show that such a rule order induces a natural
order on extended answer sets, the minimal elements of which we call preferred
answer sets. We characterize the expressiveness of the resulting semantics and
show that it can simulate negation as failure, disjunction and some other
formalisms such as logic programs with ordered disjunction. The approach is
shown to be useful in several application areas, e.g. repairing database, where
minimal repairs correspond to preferred answer sets.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407050</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407050</id><created>2004-07-20</created><authors><author><keyname>Aichernig</keyname><forenames>Bernhard K.</forenames></author><author><keyname>Kainhofer</keyname><forenames>Reinhold</forenames></author></authors><title>Modeling and Validating Hybrid Systems Using VDM and Mathematica</title><categories>cs.SE</categories><comments>Presented at LfM2000, published in the proceedings</comments><acm-class>D.2.4</acm-class><journal-ref>In C.Michael Holloway, editor, Lfm2000, Fifth NASA Langley Formal
  Methods Workshop, Williamsburg, Virginia, June 2000, number CP-2000-210100,
  pages 35-46. NASA, June 2000</journal-ref><abstract>  Hybrid systems are characterized by the hybrid evolution of their state: A
part of the state changes discretely, the other part changes continuously over
time. Typically, modern control applications belong to this class of systems,
where a digital controller interacts with a physical environment. In this
article we illustrate how a combination of the formal method VDM and the
computer algebra system Mathematica can be used to model and simulate both
aspects: the control logic and the physics involved. A new Mathematica package
emulating VDM-SL has been developed that allows the integration of differential
equation systems into formal specifications. The SAFER example from Kelly
(1997) serves to demonstrate the new simulation capabilities Mathematica adds:
After the thruster selection process, the astronaut's actual position and
velocity is calculated by numerically solving Euler's and Newton's equations
for rotation and translation. Furthermore, interactive validation is supported
by a graphical user interface and data animation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407051</id><created>2004-07-20</created><updated>2004-07-29</updated><authors><author><keyname>Worley</keyname><forenames>G Gordon</forenames><suffix>III</suffix></author></authors><title>Bug shallowness in open-source, Macintosh software</title><categories>cs.SE</categories><comments>added description of bug shallowness; corrected abstract (5 pages,
  PDF only (no LaTeX))</comments><journal-ref>Worley III, G Gordon. &quot;Bug shallowness in open-source, Macintosh
  software&quot;. Advanced Developers Hands On Conference 19. 2004</journal-ref><abstract>  Central to the power of open-source software is bug shallowness, the relative
ease of finding and fixing bugs. The open-source movement began with Unix
software, so many users were also programmers capable of finding and fixing
bugs given the source code. But as the open-source movement reaches the
Macintosh platform, bugs may not be shallow because few Macintosh users are
programmers. Based on reports from open-source developers, I, however, conclude
that that bugs are as shallow in open-source, Macintosh software as in any
other open-source software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407052</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407052</id><created>2004-07-20</created><authors><author><keyname>Kainhofer</keyname><forenames>Reinhold</forenames></author><author><keyname>Simonovits</keyname><forenames>Reinhard V.</forenames></author></authors><title>M@th Desktop and MD Tools - Mathematics and Mathematica Made Easy for
  Students</title><categories>cs.MS</categories><comments>13 pages, 16 figures. Paper presented at the PrimMath[2003]
  conference in Zagreb, Croatia, on September 26, 2003. Published in the
  Proceedings of PrimMath[2003]</comments><acm-class>K.3.1</acm-class><abstract>  We present two add-ons for Mathematica for teaching mathematics to
undergraduate and high school students. These two applications, M@th Desktop
(MD) and M@th Desktop Tools (MDTools), include several palettes and notebooks
covering almost every field. The underlying didactic concept is so-called
&quot;blended learning&quot;, in which these tools are meant to be used as a complement
to the professor or teacher rather than as a replacement, which other
e-learning applications do. They enable students to avoid the usual problem of
computer-based learning, namely that too large an amount of time is wasted
struggling with computer and program errors instead of actually learning the
mathematical concepts.
  M@th Desktop Tools is palette-based and provides easily accessible and
user-friendly templates for the most important functions in the fields of
Analysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, is
a modern, interactive teaching and learning software package for mathematics
classes. It is comprised of modules for Differentiation, Integration, and
Statistics, and each module presents its topic with a combination of
interactive notebooks and palettes.
  Both packages can be obtained from Deltasoft's homepage at
http://www.deltasoft.at/ .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407053</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407053</id><created>2004-07-21</created><authors><author><keyname>Orlando</keyname><forenames>Salvatore</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Venezia - Mestre, Italy</affiliation></author><author><keyname>Perego</keyname><forenames>Raffaele</forenames><affiliation>Istituto di Scienza e Tecnologia per l'Informazione</affiliation></author><author><keyname>Silvestri</keyname><forenames>Fabrizio</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Venezia - Mestre, Italy</affiliation><affiliation>Dipartimento di Informatica, Universit&#xe0; di Pisa, Italy</affiliation></author></authors><title>Design of a Parallel and Distributed Web Search Engine</title><categories>cs.IR cs.DC</categories><comments>8 pages. In Proceedings of the 2001 Parallel Computing Conference
  (ParCo 2001), 4-7 September 2001, Naples, Italy, Imperial College Press, pp.
  197-204</comments><abstract>  This paper describes the architecture of MOSE (My Own Search Engine), a
scalable parallel and distributed engine for searching the web. MOSE was
specifically designed to efficiently exploit affordable parallel architectures,
such as clusters of workstations. Its modular and scalable architecture can
easily be tuned to fulfill the bandwidth requirements of the application at
hand. Both task-parallel and data-parallel approaches are exploited within MOSE
in order to increase the throughput and efficiently use communication, storing
and computational resources. We used a collection of html documents as a
benchmark, and conducted preliminary experiments on a cluster of three SMP
Linux PCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407054</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407054</id><created>2004-07-20</created><updated>2005-07-19</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>From truth to computability I</title><categories>cs.LO cs.AI cs.GT math.LO</categories><comments>To appear in Theoretical Computer Science</comments><acm-class>F.1.1; F.1.2</acm-class><journal-ref>Theoretical Computer Science 357 (2006), pp. 100-135</journal-ref><doi>10.1016/j.tcs.2006.03.014</doi><abstract>  The recently initiated approach called computability logic is a formal theory
of interactive computation. See a comprehensive online source on the subject at
http://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a
soundness and completeness proof for the deductive system CL3 which axiomatizes
the most basic first-order fragment of computability logic called the
finite-depth, elementary-base fragment. Among the potential application areas
for this result are the theory of interactive computation, constructive applied
theories, knowledgebase systems, systems for resource-bound planning and
action. This paper is self-contained as it reintroduces all relevant
definitions as well as main motivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407055</id><created>2004-07-22</created><updated>2004-07-23</updated><authors><author><keyname>Pedicini</keyname><forenames>M.</forenames></author><author><keyname>Quaglia</keyname><forenames>F.</forenames></author></authors><title>PELCR: Parallel Environment for Optimal Lambda-Calculus Reduction</title><categories>cs.LO cs.DC</categories><acm-class>F.4.1</acm-class><abstract>  In this article we present the implementation of an environment supporting
L\'evy's \emph{optimal reduction} for the $\lambda$-calculus \cite{Lev78} on
parallel (or distributed) computing systems. In a similar approach to Lamping's
one in \cite{Lamping90}, we base our work on a graph reduction technique known
as \emph{directed virtual reduction} \cite{DPR97} which is actually a
restriction of Danos-Regnier virtual reduction \cite{DanosRegnier93}.
  The environment, which we refer to as PELCR (Parallel Environment for optimal
Lambda-Calculus Reduction) relies on a strategy for directed virtual reduction,
namely {\em half combustion}, which we introduce in this article. While
developing PELCR we have adopted both a message aggregation technique, allowing
a reduction of the communication overhead, and a fair policy for distributing
dynamically originated load among processors.
  We also present an experimental study demonstrating the ability of PELCR to
definitely exploit parallelism intrinsic to $\lambda$-terms while performing
the reduction. By the results we show how PELCR allows achieving up to 70/80%
of the ideal speedup on last generation multiprocessor computing systems. As a
last note, the software modules have been developed with the {\tt C} language
and using a standard interface for message passing, i.e. MPI, thus making PELCR
itself a highly portable software package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407056</id><created>2004-07-22</created><authors><author><keyname>Rosgen</keyname><forenames>Bill</forenames></author><author><keyname>Watrous</keyname><forenames>John</forenames></author></authors><title>On the hardness of distinguishing mixed-state quantum computations</title><categories>cs.CC quant-ph</categories><comments>17 pages</comments><acm-class>F.1.2; F.1.3</acm-class><abstract>  This paper considers the following problem. Two mixed-state quantum circuits
Q and R are given, and the goal is to determine which of two possibilities
holds: (i) Q and R act nearly identically on all possible quantum state inputs,
or (ii) there exists some input state that Q and R transform into almost
perfectly distinguishable outputs. This problem may be viewed as an abstraction
of the following problem: given two physical processes described by sequences
of local interactions, are the processes effectively the same or are they
different? We prove that this problem is a complete promise problem for the
class QIP of problems having quantum interactive proof systems, and is
therefore PSPACE-hard. This is in sharp contrast to the fact that the analogous
problem for classical (probabilistic) circuits is in AM, and for unitary
quantum circuits is in QMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407057</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407057</id><created>2004-07-23</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Muchnik</keyname><forenames>Andrej</forenames></author></authors><title>Universal Convergence of Semimeasures on Individual Random Sequences</title><categories>cs.LG cs.AI cs.CC cs.IT math.IT math.PR</categories><comments>16 pages</comments><report-no>IDSIA-14-04</report-no><acm-class>I.2.6; E.4; G.3; F.1.3</acm-class><journal-ref>Proc. 15th International Conf. on Algorithmic Learning Theory
  (ALT-2004), pages 234-248</journal-ref><abstract>  Solomonoff's central result on induction is that the posterior of a universal
semimeasure M converges rapidly and with probability 1 to the true sequence
generating posterior mu, if the latter is computable. Hence, M is eligible as a
universal sequence predictor in case of unknown mu. Despite some nearby results
and proofs in the literature, the stronger result of convergence for all
(Martin-Loef) random sequences remained open. Such a convergence result would
be particularly interesting and natural, since randomness can be defined in
terms of M itself. We show that there are universal semimeasures M which do not
converge for all random sequences, i.e. we give a partial negative answer to
the open problem. We also provide a positive answer for some non-universal
semimeasures. We define the incomputable measure D as a mixture over all
computable measures and the enumerable semimeasure W as a mixture over all
enumerable nearly-measures. We show that W converges to D and D to mu on all
random sequences. The Hellinger distance measuring closeness of two
distributions plays a central role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407058</id><created>2004-07-24</created><updated>2005-12-06</updated><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Bunde</keyname><forenames>David P.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Leung</keyname><forenames>Vitus J.</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author><author><keyname>Phillips</keyname><forenames>Cynthia A.</forenames></author></authors><title>Communication-Aware Processor Allocation for Supercomputers</title><categories>cs.DS cs.DC</categories><comments>19 pages, 7 figures, 1 table, Latex, submitted for journal
  publication. Previous version is extended abstract (14 pages), appeared in
  Proceedings WADS, Springer LNCS 3608, pp. 169-181</comments><acm-class>F.2.2; C.1.4</acm-class><abstract>  This paper gives processor-allocation algorithms for minimizing the average
number of communication hops between the assigned processors for grid
architectures, in the presence of occupied cells. The simpler problem of
assigning processors on a free grid has been studied by Karp, McKellar, and
Wong who show that the solutions have nontrivial structure; they left open the
complexity of the problem.
  The associated clustering problem is as follows: Given n points in Re^d, find
k points that minimize their average pairwise L1 distance. We present a natural
approximation algorithm and show that it is a 7/4-approximation for 2D grids.
For d-dimensional space, the approximation guarantee is 2-(1/2d), which is
tight. We also give a polynomial-time approximation scheme (PTAS) for constant
dimension d, and report on experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407059</id><created>2004-07-24</created><authors><author><keyname>Tsarev</keyname><forenames>Sergey P.</forenames></author></authors><title>On rational definite summation</title><categories>cs.SC cs.DM</categories><comments>LaTeX 2.09, 7 pages, submitted to &quot;Programming &amp; Computer Software&quot;</comments><acm-class>I.1.2</acm-class><abstract>  We present a partial proof of van Hoeij-Abramov conjecture about the
algorithmic possibility of computation of finite sums of rational functions.
The theoretical results proved in this paper provide an algorithm for
computation of a large class of sums $ S(n) = \sum_{k=0}^{n-1}R(k,n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407060</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407060</id><created>2004-07-25</created><updated>2005-05-27</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Tight bounds for LDPC and LDGM codes under MAP decoding</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>28 pages, 9 eps figures; Second version contains a generalization of
  the previous result</comments><journal-ref>IEEE Trans. on Inf. Theory, vol.51, pp. 3221-3246 (2005)</journal-ref><abstract>  A new method for analyzing low density parity check (LDPC) codes and low
density generator matrix (LDGM) codes under bit maximum a posteriori
probability (MAP) decoding is introduced. The method is based on a rigorous
approach to spin glasses developed by Francesco Guerra. It allows to construct
lower bounds on the entropy of the transmitted message conditional to the
received one. Based on heuristic statistical mechanics calculations, we
conjecture such bounds to be tight. The result holds for standard irregular
ensembles when used over binary input output symmetric channels. The method is
first developed for Tanner graph ensembles with Poisson left degree
distribution. It is then generalized to `multi-Poisson' graphs, and, by a
completion procedure, to arbitrary degree distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407061</id><created>2004-07-28</created><authors><author><keyname>Blondel</keyname><forenames>Vincent</forenames></author><author><keyname>Gajardo</keyname><forenames>Anahi</forenames></author><author><keyname>Heymans</keyname><forenames>Maureen</forenames></author><author><keyname>Senellart</keyname><forenames>Pierre</forenames></author><author><keyname>Van Dooren</keyname><forenames>Paul</forenames></author></authors><title>A measure of similarity between graph vertices</title><categories>cs.IR cond-mat.dis-nn cs.DM physics.data-an</categories><abstract>  We introduce a concept of similarity between vertices of directed graphs. Let
G_A and G_B be two directed graphs. We define a similarity matrix whose (i,
j)-th real entry expresses how similar vertex j (in G_A) is to vertex i (in
G_B. The similarity matrix can be obtained as the limit of the normalized even
iterates of a linear transformation. In the special case where G_A=G_B=G, the
matrix is square and the (i, j)-th entry is the similarity score between the
vertices i and j of G. We point out that Kleinberg's &quot;hub and authority&quot; method
to identify web-pages relevant to a given query can be viewed as a special case
of our definition in the case where one of the graphs has two vertices and a
unique directed edge between them. In analogy to Kleinberg, we show that our
similarity scores are given by the components of a dominant eigenvector of a
non-negative matrix. Potential applications of our similarity concept are
numerous. We illustrate an application for the automatic extraction of synonyms
in a monolingual dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407062</id><created>2004-07-28</created><authors><author><keyname>Zhang</keyname><forenames>Xuehai</forenames></author><author><keyname>Schopf</keyname><forenames>Jennifer M.</forenames></author></authors><title>Performance Analysis of the Globus Toolkit Monitoring and Discovery
  Service, MDS2</title><categories>cs.DC cs.PF</categories><report-no>Preprint ANL/MCS-P1115-0104</report-no><acm-class>H.3.4; H.5.3</acm-class><abstract>  Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the monitoring system,
and help evaluate future development work. To this end, we examined the
performance of the Globus Toolkit(reg. trdmrk) Monitoring and Discovery Service
(MDS2) by instrumenting its main services using NetLogger. Our study shows a
strong advantage to caching or prefetching the data, as well as the need to
have primary components at well-connected sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407063</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407063</id><created>2004-07-28</created><updated>2015-03-11</updated><authors><author><keyname>Benbernou</keyname><forenames>Nadia</forenames></author><author><keyname>Cahn</keyname><forenames>Patricia</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Unfolding Smooth Prismatoids</title><categories>cs.CG cs.DM</categories><comments>19 pages, 15 figures, 1st draft Revised version corrects an error in
  the proof of Lemma 3.3. The statement of the lemma remains unchanged. Second
  revised version: corrected a typo in the title</comments><report-no>Smith College Computer Science Technical Report 078</report-no><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a notion for unfolding smooth, ruled surfaces, and prove that every
smooth prismatoid (the convex hull of two smooth curves lying in parallel
planes), has a nonoverlapping &quot;volcano unfolding.&quot; These unfoldings keep the
base intact, unfold the sides outward, splayed around the base, and attach the
top to the tip of some side rib. Our result answers a question for smooth
prismatoids whose analog for polyhedral prismatoids remains unsolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407064</id><created>2004-07-29</created><authors><author><keyname>Olivetti</keyname><forenames>Nicola</forenames></author><author><keyname>Pozzato</keyname><forenames>Gian Luca</forenames></author><author><keyname>Schwind</keyname><forenames>Camilla</forenames></author></authors><title>A Sequent Calculus and a Theorem Prover for Standard Conditional Logics</title><categories>cs.LO cs.AI</categories><comments>45 pages with 2 figures, uses prooftree.sty</comments><acm-class>D.1.6; F.4.1; I.2.3</acm-class><abstract>  In this paper we present a cut-free sequent calculus, called SeqS, for some
standard conditional logics, namely CK, CK+ID, CK+MP and CK+MP+ID. The calculus
uses labels and transition formulas and can be used to prove decidability and
space complexity bounds for the respective logics. We also present CondLean, a
theorem prover for these logics implementing SeqS calculi written in SICStus
Prolog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407065</id><created>2004-07-29</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Word Sense Disambiguation by Web Mining for Word Co-occurrence
  Probabilities</title><categories>cs.CL cs.IR cs.LG</categories><comments>related work available at http://purl.org/peter.turney/</comments><acm-class>H.3.1; H.3.3; I.2.6; I.2.7; J.5</acm-class><journal-ref>Proceedings of the Third International Workshop on the Evaluation
  of Systems for the Semantic Analysis of Text (SENSEVAL-3), (2004), Barcelona,
  Spain, 239-242</journal-ref><abstract>  This paper describes the National Research Council (NRC) Word Sense
Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS)
task in Senseval-3. The NRC system approaches WSD as a classical supervised
machine learning problem, using familiar tools such as the Weka machine
learning software and Brill's rule-based part-of-speech tagger. Head words are
represented as feature vectors with several hundred features. Approximately
half of the features are syntactic and the other half are semantic. The main
novelty in the system is the method for generating the semantic features, based
on word \hbox{co-occurrence} probabilities. The probabilities are estimated
using the Waterloo MultiText System with a corpus of about one terabyte of
unlabeled text, collected by a web crawler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0407066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0407066</id><created>2004-07-30</created><authors><author><keyname>Tentyukov</keyname><forenames>M.</forenames></author><author><keyname>Fliegner</keyname><forenames>D.</forenames></author><author><keyname>Frank</keyname><forenames>M.</forenames></author><author><keyname>Onischenko</keyname><forenames>A.</forenames></author><author><keyname>Retey</keyname><forenames>A.</forenames></author><author><keyname>Staudenmaier</keyname><forenames>H. M.</forenames></author><author><keyname>Vermaseren</keyname><forenames>J. A. M.</forenames></author></authors><title>ParFORM: Parallel Version of the Symbolic Manipulation Program FORM</title><categories>cs.SC cs.DC hep-ph</categories><comments>5 pages, 4 Encapsulated postscript figures, LaTeX2e uses casc.cls
  (included). Presented at CASC'04 http://wwwmayr.in.tum.de/CASC2004/</comments><report-no>TTP04-15</report-no><acm-class>I.1; I.1.2; I.1.4</acm-class><abstract>  After an introduction to the sequential version of FORM and the mechanisms
behind, we report on the status of our project of parallelization. We have now
a parallel version of FORM running on Cluster- and SMP-architectures. This
version can be used to run arbitrary FORM programs in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408001</id><created>2004-07-31</created><authors><author><keyname>Engelhardt</keyname><forenames>Michael</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>Semantic Linking - a Context-Based Approach to Interactivity in
  Hypermedia</title><categories>cs.IR cs.LG</categories><acm-class>H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1</acm-class><journal-ref>R. Tolksdorf, R. Eckstein: Proc. of Berliner XML Tage.
  Humboldt-Universitaet zu Berlin; pp. 55-66; ISBN 3-88579-116-1; Berlin; 2003</journal-ref><abstract>  The semantic Web initiates new, high level access schemes to online content
and applications. One area of superior need for a redefined content exploration
is given by on-line educational applications and their concepts of
interactivity in the framework of open hypermedia systems. In the present paper
we discuss aspects and opportunities of gaining interactivity schemes from
semantic notions of components. A transition from standard educational
annotation to semantic statements of hyperlinks is discussed. Further on we
introduce the concept of semantic link contexts as an approach to manage a
coherent rhetoric of linking. A practical implementation is introduced, as
well. Our semantic hyperlink implementation is based on the more general
Multimedia Information Repository MIR, an open hypermedia system supporting the
standards XML, Corba and JNDI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408002</id><created>2004-07-31</created><authors><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Roaming Real-Time Applications - Mobility Services in IPv6 Networks</title><categories>cs.NI cs.PF</categories><comments>15 pages, 5 figures</comments><acm-class>C.2.1; C.2.2; C.2.6; J.7</acm-class><journal-ref>Proceedings TERENA Networking Conference Zagreb, 2003,
  http://www.terena.nl/conferences/tnc2003/programme/final-programme.html</journal-ref><abstract>  Emerging mobility standards within the next generation Internet Protocol,
IPv6, promise to continuously operate devices roaming between IP networks.
Associated with the paradigm of ubiquitous computing and communication, network
technology is on the spot to deliver voice and videoconferencing as a standard
internet solution. However, current roaming procedures are too slow, to remain
seamless for real-time applications. Multicast mobility still waits for a
convincing design. This paper investigates the temporal behaviour of mobile
IPv6 with dedicated focus on topological impacts. Extending the hierarchical
mobile IPv6 approach we suggest protocol improvements for a continuous
handover, which may serve bidirectional multicast communication, as well. Along
this line a multicast mobility concept is introduced as a service for clients
and sources, as they are of dedicated importance in multipoint conferencing
applications. The mechanisms introduced do not rely on assumptions of any
specific multicast routing protocol in use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408003</id><created>2004-08-02</created><authors><author><keyname>Bartal</keyname><forenames>Yair</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author></authors><title>Multi-Embedding of Metric Spaces</title><categories>cs.DS</categories><journal-ref>SIAM J. Comput. 34(1): 248-259, 2004</journal-ref><doi>10.1137/S0097539703433122</doi><abstract>  Metric embedding has become a common technique in the design of algorithms.
Its applicability is often dependent on how high the embedding's distortion is.
For example, embedding finite metric space into trees may require linear
distortion as a function of its size. Using probabilistic metric embeddings,
the bound on the distortion reduces to logarithmic in the size.
  We make a step in the direction of bypassing the lower bound on the
distortion in terms of the size of the metric. We define &quot;multi-embeddings&quot; of
metric spaces in which a point is mapped onto a set of points, while keeping
the target metric of polynomial size and preserving the distortion of paths.
The distortion obtained with such multi-embeddings into ultrametrics is at most
O(log Delta loglog Delta) where Delta is the aspect ratio of the metric. In
particular, for expander graphs, we are able to obtain constant distortion
embeddings into trees in contrast with the Omega(log n) lower bound for all
previous notions of embeddings.
  We demonstrate the algorithmic application of the new embeddings for two
optimization problems: group Steiner tree and metrical task systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408004</id><created>2004-07-31</created><authors><author><keyname>Engelhardt</keyname><forenames>Michael</forenames></author><author><keyname>K&#xe1;rp&#xe1;ti</keyname><forenames>Andreas</forenames></author><author><keyname>Rack</keyname><forenames>Torsten</forenames></author><author><keyname>Schmidt</keyname><forenames>Ivette</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>Hypermedia Learning Objects System - On the Way to a Semantic
  Educational Web</title><categories>cs.IR cs.LG</categories><comments>11 pages, 7 figures</comments><acm-class>H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1</acm-class><journal-ref>Proceedings of the International Workshop {&quot;}Interactive Computer
  aided Learning{&quot;} ICL 2003. Learning Objects and Reusability of Content,
  Kassel University Press 2003, ISBN 3-89958-029-X</journal-ref><abstract>  While eLearning systems become more and more popular in daily education,
available applications lack opportunities to structure, annotate and manage
their contents in a high-level fashion. General efforts to improve these
deficits are taken by initiatives to define rich meta data sets and a
semanticWeb layer. In the present paper we introduce Hylos, an online learning
system. Hylos is based on a cellular eLearning Object (ELO) information model
encapsulating meta data conforming to the LOM standard. Content management is
provisioned on this semantic meta data level and allows for variable,
dynamically adaptable access structures. Context aware multifunctional links
permit a systematic navigation depending on the learners and didactic needs,
thereby exploring the capabilities of the semantic web. Hylos is built upon the
more general Multimedia Information Repository (MIR) and the MIR adaptive
context linking environment (MIRaCLE), its linking extension. MIR is an open
system supporting the standards XML, Corba and JNDI. Hylos benefits from
manageable information structures, sophisticated access logic and high-level
authoring tools like the ELO editor responsible for the semi-manual creation of
meta data and WYSIWYG like content editing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408005</id><created>2004-08-01</created><authors><author><keyname>Engelhardt</keyname><forenames>Michael</forenames></author><author><keyname>Hildebrand</keyname><forenames>Arne</forenames></author><author><keyname>K&#xe1;rp&#xe1;ti</keyname><forenames>Andreas</forenames></author><author><keyname>Rack</keyname><forenames>Torsten</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>Educational Content Management - A Cellular Approach</title><categories>cs.CY cs.IR</categories><comments>9 pages, 6 figures</comments><acm-class>K.3.1; H.3.4; H.3.5; H.5.1; H.5.4; J.7</acm-class><journal-ref>Proceedings of the International Workshop &quot;Interactive Computer
  aided Learning&quot; ICL 2002. Blended Learning. Kassel University Press 2002,
  ISBN 3-933146-83-6</journal-ref><abstract>  In recent times online educational applications more and more are requested
to provide self-consistent learning offers for students at the university
level. Consequently they need to cope with the wide range of complexity and
interrelations university course teaching brings along. An urgent need to
overcome simplistically linked HTMLc ontent pages becomes apparent. In the
present paper we discuss a schematic concept of educational content
construction from information cells and introduce its implementation on the
storage and runtime layer. Starting from cells content is annotated according
to didactic needs, structured for dynamic arrangement, dynamically decorated
with hyperlinks and, as all works are based on XML, open to any presentation
layer. Data can be variably accessed through URIs built on semantic path-names
and edited via an adaptive authoring toolbox. Our content management approach
is based on the more general Multimedia Information Repository MIR. and allows
for personalisation, as well. MIR is an open system supporting the standards
XML, Corba and JNDI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408006</id><created>2004-08-01</created><authors><author><keyname>Geodakian</keyname><forenames>Vigen A.</forenames></author></authors><title>Why Two Sexes?</title><categories>cs.NE cs.GL q-bio.PE</categories><comments>12 pages, 3 figures</comments><acm-class>F.1.1; J.3</acm-class><journal-ref>Nauka i zhizn (Science and Life), 1965</journal-ref><abstract>  Evolutionary role of the separation into two sexes from a cyberneticist's
point of view. [I translated this 1965 article from Russian &quot;Nauka i Zhizn&quot;
(Science and Life) in 1988. In a popular form, the article puts forward several
useful ideas not all of which even today are necessarily well known or widely
accepted. Boris Lubachevsky, bdl@bell-labs.com ]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408007</id><created>2004-08-02</created><authors><author><keyname>Flaxman</keyname><forenames>Abraham D.</forenames></author><author><keyname>Kalai</keyname><forenames>Adam Tauman</forenames></author><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author></authors><title>Online convex optimization in the bandit setting: gradient descent
  without a gradient</title><categories>cs.LG cs.CC</categories><comments>12 pages</comments><abstract>  We consider a the general online convex optimization framework introduced by
Zinkevich. In this setting, there is a sequence of convex functions. Each
period, we must choose a signle point (from some feasible set) and pay a cost
equal to the value of the next function on our chosen point. Zinkevich shows
that, if the each function is revealed after the choice is made, then one can
achieve vanishingly small regret relative the best single decision chosen in
hindsight.
  We extend this to the bandit setting where we do not find out the entire
functions but rather just their value at our chosen point. We show how to get
vanishingly small regret in this setting.
  Our approach uses a simple approximation of the gradient that is computed
from evaluating a function at a single (random) point. We show that this
estimate is sufficient to mimic Zinkevich's gradient descent online analysis,
with access to the gradient (only being able to evaluate the function at a
single point).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408008</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408008</id><created>2004-08-02</created><authors><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Yedidia</keyname><forenames>Jonathan S.</forenames></author></authors><title>Iterative Quantization Using Codes On Graphs</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><journal-ref>Proceedings of the 41st Annual Allerton Conference on
  Communication, Control, and Computing; Monticello, IL; 2004</journal-ref><abstract>  We study codes on graphs combined with an iterative message passing algorithm
for quantization. Specifically, we consider the binary erasure quantization
(BEQ) problem which is the dual of the binary erasure channel (BEC) coding
problem. We show that duals of capacity achieving codes for the BEC yield codes
which approach the minimum possible rate for the BEQ. In contrast, low density
parity check codes cannot achieve the minimum rate unless their density grows
at least logarithmically with block length. Furthermore, we show that duals of
efficient iterative decoding algorithms for the BEC yield efficient encoding
algorithms for the BEQ. Hence our results suggest that graphical models may
yield near optimal codes in source coding as well as in channel coding and that
duality plays a key role in such constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408009</id><created>2004-08-03</created><authors><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Performance Analysis of Multicast Mobility in a Hierarchical Mobile IP
  Proxy Environment</title><categories>cs.NI cs.PF</categories><comments>11 pages, 7 figures</comments><acm-class>C.2.1; C.2.2; C.2.6; J.7</acm-class><journal-ref>Selected Papers from TERENA Networking Conference Rhodes, 2004,
  http://www.terena.nl/library/tnc2004-proceedings/papers/schmidt.pdf</journal-ref><abstract>  Mobility support in IPv6 networks is ready for release as an RFC, stimulating
major discussions on improvements to meet real-time communication requirements.
Sprawling hot spots of IP-only wireless networks at the same time await voice
and videoconferencing as standard mobile Internet services, thereby adding the
request for multicast support to real-time mobility. This paper briefly
introduces current approaches for seamless multicast extensions to Mobile IPv6.
Key issues of multicast mobility are discussed. Both analytically and in
simulations comparisons are drawn between handover performance characteristics,
dedicating special focus on the M-HMIPv6 approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408010</id><created>2004-08-03</created><updated>2004-09-19</updated><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Dezert</keyname><forenames>Jean</forenames></author></authors><title>A Simple Proportional Conflict Redistribution Rule</title><categories>cs.AI</categories><comments>21 pages</comments><acm-class>I.2.3</acm-class><journal-ref>International Journal of Applied Mathematics and Statistics, Vol.
  3, No. J05, 1-36, 2005.</journal-ref><abstract>  One proposes a first alternative rule of combination to WAO (Weighted Average
Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called
Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are
particular cases of WO (the Weighted Operator) because the conflicting mass is
redistributed with respect to some weighting factors. In this first PCR rule,
the proportionalization is done for each non-empty set with respect to the
non-zero sum of its corresponding mass matrix - instead of its mass column
average as in WAO, but the results are the same as Ph. Smets has pointed out.
Also, we extend WAO (which herein gives no solution) for the degenerate case
when all column sums of all non-empty sets are zero, and then the conflicting
mass is transferred to the non-empty disjunctive form of all non-empty sets
together; but if this disjunctive form happens to be empty, then one considers
an open world (i.e. the frame of discernment might contain new hypotheses) and
thus all conflicting mass is transferred to the empty set. In addition to WAO,
we propose a general formula for PCR1 (WAO for non-degenerate cases).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408011</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408011</id><created>2004-08-04</created><authors><author><keyname>Wild</keyname><forenames>Marcel</forenames></author></authors><title>The asymptotic number of binary codes and binary matroids</title><categories>cs.IT cs.DM math.IT</categories><comments>12 pages</comments><journal-ref>SIAM Journal of Discrete Mathematics 19 (2005) 691-699</journal-ref><abstract>  The asyptotic number of nonequivalent binary n-codes is determined. This is
also the asymptotic number of nonisomorphic binary n-matroids. The connection
to a result of Lefmann, Roedl, Phelps is explored. The latter states that
almost all binary n-codes have a trivial automorphism group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408012</id><created>2004-08-04</created><authors><author><keyname>Kaminski</keyname><forenames>J. Y.</forenames></author><author><keyname>Teicher</keyname><forenames>M.</forenames></author><author><keyname>Knaan</keyname><forenames>D.</forenames></author><author><keyname>Shavit</keyname><forenames>A.</forenames></author></authors><title>Three-Dimensional Face Orientation and Gaze Detection from a Single Image</title><categories>cs.CV cs.HC</categories><abstract>  Gaze detection and head orientation are an important part of many advanced
human-machine interaction applications. Many systems have been proposed for
gaze detection. Typically, they require some form of user cooperation and
calibration. Additionally, they may require multiple cameras and/or restricted
head positions. We present a new approach for inference of both face
orientation and gaze direction from a single image with no restrictions on the
head position. Our algorithm is based on a face and eye model, deduced from
anthropometric data. This approach allows us to use a single camera and
requires no cooperation from the user. Using a single image avoids the
complexities associated with of a multi-camera system. Evaluation tests show
that our system is accurate, fast and can be used in a variety of applications,
including ones where the user is unaware of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408013</id><created>2004-08-04</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Lam</keyname><forenames>Patrick</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>Roles Are Really Great!</title><categories>cs.PL cs.SE</categories><comments>29 pages. A version appeared in POPL 2002</comments><report-no>MIT CSAIL 822</report-no><acm-class>D.2.4; D.3.1; D.3.3; F.3.1; F.3.2</acm-class><abstract>  We present a new role system for specifying changing referencing
relationships of heap objects. The role of an object depends, in large part, on
its aliasing relationships with other objects, with the role of each object
changing as its aliasing relationships change. Roles therefore capture
important object and data structure properties and provide useful information
about how the actions of the program interact with these properties. Our role
system enables the programmer to specify the legal aliasing relationships that
define the set of roles that objects may play, the roles of procedure
parameters and object fields, and the role changes that procedures perform
while manipulating objects. We present an interprocedural, compositional, and
context-sensitive role analysis algorithm that verifies that a program respects
the role constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408014</id><created>2004-08-04</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>Typestate Checking and Regular Graph Constraints</title><categories>cs.PL cs.LO</categories><comments>21 page. A version appeared in SAS 2003</comments><report-no>MIT CSAIL 863</report-no><acm-class>D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1</acm-class><abstract>  We introduce regular graph constraints and explore their decidability
properties. The motivation for regular graph constraints is 1) type checking of
changing types of objects in the presence of linked data structures, 2) shape
analysis techniques, and 3) generalization of similar constraints over trees
and grids. We define a subclass of graphs called heaps as an abstraction of the
data structures that a program constructs during its execution. We prove that
determining the validity of implication for regular graph constraints over the
class of heaps is undecidable. We show undecidability by exhibiting a
characterization of certain &quot;corresponder graphs&quot; in terms of presence and
absence of homomorphisms to a finite number of fixed graphs. The undecidability
of implication of regular graph constraints implies that there is no algorithm
that will verify that procedure preconditions are met or that the invariants
are maintained when these properties are expressed in any specification
language at least as expressive as regular graph constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408015</id><created>2004-08-05</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>On the Theory of Structural Subtyping</title><categories>cs.LO cs.PL cs.SE</categories><comments>51 page. A version appeared in LICS 2003</comments><report-no>MIT CSAIL 879</report-no><acm-class>D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1</acm-class><abstract>  We show that the first-order theory of structural subtyping of non-recursive
types is decidable. Let $\Sigma$ be a language consisting of function symbols
(representing type constructors) and $C$ a decidable structure in the
relational language $L$ containing a binary relation $\leq$. $C$ represents
primitive types; $\leq$ represents a subtype ordering. We introduce the notion
of $\Sigma$-term-power of $C$, which generalizes the structure arising in
structural subtyping. The domain of the $\Sigma$-term-power of $C$ is the set
of $\Sigma$-terms over the set of elements of $C$. We show that the
decidability of the first-order theory of $C$ implies the decidability of the
first-order theory of the $\Sigma$-term-power of $C$. Our decision procedure
makes use of quantifier elimination for term algebras and Feferman-Vaught
theorem. Our result implies the decidability of the first-order theory of
structural subtyping of non-recursive types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408016</id><created>2004-08-05</created><authors><author><keyname>Sundell</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Lock-Free and Practical Deques using Single-Word Compare-And-Swap</title><categories>cs.DC cs.DS</categories><report-no>2004-02</report-no><acm-class>E.1</acm-class><abstract>  We present an efficient and practical lock-free implementation of a
concurrent deque that is disjoint-parallel accessible and uses atomic
primitives which are available in modern computer systems. Previously known
lock-free algorithms of deques are either based on non-available atomic
synchronization primitives, only implement a subset of the functionality, or
are not designed for disjoint accesses. Our algorithm is based on a doubly
linked list, and only requires single-word compare-and-swap atomic primitives,
even for dynamic memory sizes. We have performed an empirical study using full
implementations of the most efficient algorithms of lock-free deques known. For
systems with low concurrency, the algorithm by Michael shows the best
performance. However, as our algorithm is designed for disjoint accesses, it
performs significantly better on systems with high concurrency and non-uniform
memory architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408017</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408017</id><created>2004-08-05</created><authors><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author></authors><title>Improved Upper Bound for the Redundancy of Fix-Free Codes</title><categories>cs.IT math.IT</categories><abstract>  A variable-length code is a fix-free code if no codeword is a prefix or a
suffix of any other codeword. In a fix-free code any finite sequence of
codewords can be decoded in both directions, which can improve the robustness
to channel noise and speed up the decoding process. In this paper we prove a
new sufficient condition of the existence of fix-free codes and improve the
upper bound on the redundancy of optimal fix-free codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408018</id><created>2004-08-05</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>On Role Logic</title><categories>cs.PL cs.LO</categories><comments>20 pages. Our later SAS 2004 result builds on this work</comments><report-no>MIT CSAIL 925</report-no><acm-class>D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1</acm-class><abstract>  We present role logic, a notation for describing properties of relational
structures in shape analysis, databases, and knowledge bases. We construct role
logic using the ideas of de Bruijn's notation for lambda calculus, an encoding
of first-order logic in lambda calculus, and a simple rule for implicit
arguments of unary and binary predicates. The unrestricted version of role
logic has the expressive power of first-order logic with transitive closure.
Using a syntactic restriction on role logic formulas, we identify a natural
fragment RL^2 of role logic. We show that the RL^2 fragment has the same
expressive power as two-variable logic with counting C^2 and is therefore
decidable. We present a translation of an imperative language into the
decidable fragment RL^2, which allows compositional verification of programs
that manipulate relational structures. In addition, we show how RL^2 encodes
boolean shape analysis constraints and an expressive description logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408019</id><created>2004-08-05</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>On Generalized Records and Spatial Conjunction in Role Logic</title><categories>cs.PL cs.LO</categories><comments>30 pages. A version appears in SAS 2004</comments><report-no>MIT CSAIL 942</report-no><acm-class>D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1</acm-class><abstract>  We have previously introduced role logic as a notation for describing
properties of relational structures in shape analysis, databases and knowledge
bases. A natural fragment of role logic corresponds to two-variable logic with
counting and is therefore decidable. We show how to use role logic to describe
open and closed records, as well the dual of records, inverse records. We
observe that the spatial conjunction operation of separation logic naturally
models record concatenation. Moreover, we show how to eliminate the spatial
conjunction of formulas of quantifier depth one in first-order logic with
counting. As a result, allowing spatial conjunction of formulas of quantifier
depth one preserves the decidability of two-variable logic with counting. This
result applies to two-variable role logic fragment as well. The resulting logic
smoothly integrates type system and predicate calculus notation and can be
viewed as a natural generalization of the notation for constraints arising in
role analysis and similar shape analysis approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408020</id><created>2004-08-06</created><authors><author><keyname>Tilak</keyname><forenames>Sameer</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael</forenames></author><author><keyname>Heinzelman</keyname><forenames>Wendi</forenames></author></authors><title>Collaborative Storage Management In Sensor Networks</title><categories>cs.NI cs.AR</categories><comments>13 pages, 7 figures</comments><report-no>CS-TR-04-NA01</report-no><acm-class>C.2.1 Wireless communication;C.2.2 Network Protocols;D.4.2 Storage
  Management</acm-class><abstract>  In this paper, we consider a class of sensor networks where the data is not
required in real-time by an observer; for example, a sensor network monitoring
a scientific phenomenon for later play back and analysis. In such networks, the
data must be stored in the network. Thus, in addition to battery power, storage
is a primary resource: the useful lifetime of the network is constrained by its
ability to store the generated data samples. We explore the use of
collaborative storage technique to efficiently manage data in storage
constrained sensor networks. The proposed collaborative storage technique takes
advantage of spatial correlation among the data collected by nearby sensors to
significantly reduce the size of the data near the data sources. We show that
the proposed approach provides significant savings in the size of the stored
data vs. local buffering, allowing the network to run for a longer time without
running out of storage space and reducing the amount of data that will
eventually be relayed to the observer. In addition, collaborative storage
performs load balancing of the available storage space if data generation rates
are not uniform across sensors (as would be the case in an event driven sensor
network), or if the available storage varies across the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408021</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408021</id><created>2004-08-08</created><updated>2004-08-14</updated><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Dezert</keyname><forenames>Jean</forenames></author></authors><title>An Algorithm for Quasi-Associative and Quasi-Markovian Rules of
  Combination in Information Fusion</title><categories>cs.AI</categories><comments>9 pages</comments><acm-class>I.2.4</acm-class><journal-ref>International Journal of Applied Mathematics &amp; Statistics, Vol.
  22, No. S11 (Special Issue on Soft Computing), 33-42, 2011</journal-ref><abstract>  In this paper one proposes a simple algorithm of combining the fusion rules,
those rules which first use the conjunctive rule and then the transfer of
conflicting mass to the non-empty sets, in such a way that they gain the
property of associativity and fulfill the Markovian requirement for dynamic
fusion. Also, a new rule, SDL-improved, is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408022</id><created>2004-08-09</created><authors><author><keyname>Chang</keyname><forenames>Guey-Yun</forenames></author><author><keyname>Chang</keyname><forenames>Gerard J.</forenames></author><author><keyname>Chen</keyname><forenames>Gen-Huey</forenames></author></authors><title>Diagnosabilities of regular networks</title><categories>cs.NI</categories><comments>26 pages</comments><report-no>NCTS/TPE-Math Technical Report 2004-013</report-no><abstract>  In this paper, we study diagnosabilities of multiprocessor systems under two
diagnosis models: the PMC model and the comparison model. In each model, we
further consider two different diagnosis strategies: the precise diagnosis
strategy proposed by Preparata et al. and the pessimistic diagnosis strategy
proposed by Friedman. The main result of this paper is to determine
diagnosabilities of regular networks with certain conditions, which include
several widely used multiprocessor systems such as variants of hypercubes and
many others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408023</id><created>2004-08-09</created><authors><author><keyname>van Hoeve</keyname><forenames>Willem Jan</forenames></author><author><keyname>Pesant</keyname><forenames>Gilles</forenames></author><author><keyname>Rousseau</keyname><forenames>Louis-Martin</forenames></author></authors><title>On Global Warming (Softening Global Constraints)</title><categories>cs.AI cs.PL</categories><comments>15 pages, 7 figures. Accepted at the 6th International Workshop on
  Preferences and Soft Constraints</comments><acm-class>D.3.2; D.3.3; G.2.2</acm-class><abstract>  We describe soft versions of the global cardinality constraint and the
regular constraint, with efficient filtering algorithms maintaining domain
consistency. For both constraints, the softening is achieved by augmenting the
underlying graph. The softened constraints can be used to extend the
meta-constraint framework for over-constrained problems proposed by Petit,
Regin and Bessiere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408024</id><created>2004-08-09</created><authors><author><keyname>Tilak</keyname><forenames>Sameer</forenames></author><author><keyname>Murphy</keyname><forenames>Amy</forenames></author><author><keyname>Heinzelman</keyname><forenames>Wendi</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael B.</forenames></author></authors><title>Non-uniform Information Dissemination for Sensor Networks</title><categories>cs.NI</categories><comments>16 pages, 7 Figures, An earlier version of this paper appeared in the
  International Conference on Networking Protocols (ICNP 2003). Significant
  changes in this paper include the addition of an analysis of the protocol in
  the presence of mobility, as well as an expanded discussion section. At
  present this paper has been submitted for a journal publication and it is
  currently under review</comments><report-no>CS-TR-04-NA03</report-no><acm-class>C.2.1 Wireless communication;C.2.2 Network Protocols</acm-class><abstract>  Future smart environments will be characterized by multiple nodes that sense,
collect, and disseminate information about environmental phenomena through a
wireless network. In this paper, we define a set of applications that require a
new form of distributed knowledge about the environment, referred to as
non-uniform information granularity. By non-uniform information granularity we
mean that the required accuracy or precision of information is proportional to
the distance between a source node (information producer) and current sink node
(information consumer). That is, as the distance between the source node and
sink node increases, loss in information precision is acceptable. Applications
that can benefit from this type of knowledge range from battlefield scenarios
to rescue operations. The main objectives of this paper are two-fold: first, we
will precisely define non-uniform information granularity, and second, we will
describe different protocols that achieve non-uniform information dissemination
and analyze these protocols based on complexity, energy consumption, and
accuracy of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408025</identifier>
 <datestamp>2010-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408025</id><created>2004-08-10</created><authors><author><keyname>Holzbaur</keyname><forenames>Christian</forenames></author><author><keyname>de la Banda</keyname><forenames>Maria Garcia</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author><author><keyname>Duck</keyname><forenames>Gregory J.</forenames></author></authors><title>Optimizing compilation of constraint handling rules in HAL</title><categories>cs.PL</categories><comments>29 pages 6 figures, 4 tables. To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>D.3.2 Constraint and logic langauges; D.3.4 Optimization</acm-class><journal-ref>Theory and Practice of Logic Programming: 5(4-5):503-532, 2005</journal-ref><abstract>  In this paper we discuss the optimizing compilation of Constraint Handling
Rules (CHRs). CHRs are a multi-headed committed choice constraint language,
commonly applied for writing incremental constraint solvers. CHRs are usually
implemented as a language extension that compiles to the underlying language.
In this paper we show how we can use different kinds of information in the
compilation of CHRs in order to obtain access efficiency, and a better
translation of the CHR rules into the underlying language, which in this case
is HAL. The kinds of information used include the types, modes, determinism,
functional dependencies and symmetries of the CHR constraints. We also show how
to analyze CHR programs to determine this information about functional
dependencies, symmetries and other kinds of information supporting
optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408026</id><created>2004-08-10</created><authors><author><keyname>Skut</keyname><forenames>Wojciech</forenames></author></authors><title>Incremental Construction of Minimal Acyclic Sequential Transducers from
  Unsorted Data</title><categories>cs.CL cs.DS</categories><comments>Proceedings of COLING 2004 (to appear), 7 pages, 5 figures</comments><acm-class>F.4.3</acm-class><abstract>  This paper presents an efficient algorithm for the incremental construction
of a minimal acyclic sequential transducer (ST) for a dictionary consisting of
a list of input and output strings. The algorithm generalises a known method of
constructing minimal finite-state automata (Daciuk et al. 2000). Unlike the
algorithm published by Mihov and Maurel (2001), it does not require the input
strings to be sorted. The new method is illustrated by an application to
pronunciation dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408027</id><created>2004-08-12</created><authors><author><keyname>Christiansen</keyname><forenames>Henning</forenames></author></authors><title>CHR Grammars</title><categories>cs.CL cs.PL</categories><comments>36 pp. To appear in TPLP, 2005</comments><abstract>  A grammar formalism based upon CHR is proposed analogously to the way
Definite Clause Grammars are defined and implemented on top of Prolog. These
grammars execute as robust bottom-up parsers with an inherent treatment of
ambiguity and a high flexibility to model various linguistic phenomena. The
formalism extends previous logic programming based grammars with a form of
context-sensitive rules and the possibility to include extra-grammatical
hypotheses in both head and body of grammar rules. Among the applications are
straightforward implementations of Assumption Grammars and abduction under
integrity constraints for language analysis. CHR grammars appear as a powerful
tool for specification and implementation of language processors and may be
proposed as a new standard for bottom-up grammars in logic programming.
  To appear in Theory and Practice of Logic Programming (TPLP), 2005
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408028</id><created>2004-08-12</created><authors><author><keyname>Friedman</keyname><forenames>Joel</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Calculus on Graphs</title><categories>cs.DM math.CO</categories><comments>63 pages, LaTeX</comments><acm-class>G.2.2</acm-class><abstract>  The purpose of this paper is to develop a &quot;calculus&quot; on graphs that allows
graph theory to have new connections to analysis. For example, our framework
gives rise to many new partial differential equations on graphs, most notably a
new (Laplacian based) wave equation; this wave equation gives rise to a partial
improvement on the Chung-Faber-Manteuffel diameter/eigenvalue bound in graph
theory, and the Chung-Grigoryan-Yau and (in a certain case) Bobkov-Ledoux
distance/eigenvalue bounds in analysis. Our framework also allows most
techniques for the non-linear p-Laplacian in analysis to be easily carried over
to graph theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408029</id><created>2004-08-13</created><authors><author><keyname>Cantarella</keyname><forenames>Jason</forenames></author><author><keyname>Piatek</keyname><forenames>Michael</forenames></author></authors><title>Tsnnls: A solver for large sparse least squares problems with
  non-negative variables</title><categories>cs.MS</categories><comments>7 pages, 2 figures</comments><acm-class>G. 1 3</acm-class><abstract>  The solution of large, sparse constrained least-squares problems is a staple
in scientific and engineering applications. However, currently available codes
for such problems are proprietary or based on MATLAB. We announce a freely
available C implementation of the fast block pivoting algorithm of Portugal,
Judice, and Vicente. Our version is several times faster than Matstoms' MATLAB
implementation of the same algorithm. Further, our code matches the accuracy of
MATLAB's built-in lsqnonneg function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408030</id><created>2004-08-13</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>The Revolution In Database System Architecture</title><categories>cs.DB</categories><comments>Original at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=735</comments><report-no>MSR-TR-2004-31</report-no><acm-class>C.4</acm-class><journal-ref>Proc ACM SIGMOD 2004, Paris, pp 1-4</journal-ref><abstract>  Database system architectures are undergoing revolutionary changes.
Algorithms and data are being unified by integrating programming languages with
the database system. This gives an extensible object-relational system where
non-procedural relational operators manipulate object sets. Coupled with this,
each DBMS is now a web service. This has huge implications for how we structure
applications. DBMSs are now object containers. Queues are the first objects to
be added. These queues are the basis for transaction processing and workflow
applica-tions. Future workflow systems are likely to be built on this core.
Data cubes and online analytic processing are now baked into most DBMSs. Beyond
that, DBMSs have a framework for data mining and machine learning algorithms.
Decision trees, Bayes nets, clustering, and time series analysis are built in;
new algorithms can be added. Text, temporal, and spatial data access methods,
along with their probabilistic reasoning have been added to database systems.
Allowing approximate and probabilistic answers is essential for many
applications. Many believe that XML and xQuery will be the main data structure
and access pattern. Database systems must accommodate that perspective.These
changes mandate a much more dynamic query optimization strategy. Intelligence
is moving to the periphery of the network. Each disk and each sensor will be a
competent database machine. Relational algebra is a convenient way to program
these systems. Database systems are now expected to be self-managing,
self-healing, and always-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408031</id><created>2004-08-13</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Thakar</keyname><forenames>Aniruddha R.</forenames></author><author><keyname>Fekete</keyname><forenames>Gyorgy</forenames></author><author><keyname>O'Mullane</keyname><forenames>William</forenames></author><author><keyname>Nieto-Santisteban</keyname><forenames>Maria A.</forenames></author><author><keyname>Heber</keyname><forenames>Gerd</forenames></author><author><keyname>Rots</keyname><forenames>Arnold H.</forenames></author></authors><title>There Goes the Neighborhood: Relational Algebra for Spatial Data Search</title><categories>cs.DB</categories><comments>Original at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=736</comments><report-no>MSR-TR-2004-32</report-no><acm-class>C.4</acm-class><abstract>  We explored ways of doing spatial search within a relational database: (1)
hierarchical triangular mesh (a tessellation of the sphere), (2) a zoned
bucketing system, and (3) representing areas as disjunctive-normal form
constraints. Each of these approaches has merits. They all allow efficient
point-in-region queries. A relational representation for regions allows Boolean
operations among them and allows quick tests for point-in-region,
regions-containing-point, and region-overlap. The speed of these algorithms is
much improved by a zone and multi-scale zone-pyramid scheme. The approach has
the virtue that the zone mechanism works well on B-Trees native to all SQL
systems and integrates naturally with current query optimizers - rather than
requiring a new spatial access method and concomitant query optimizer
extensions. Over the last 5 years, we have used these techniques extensively in
our work on SkyServer.sdss.org, and SkyQuery.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408032</id><created>2004-08-14</created><updated>2004-11-08</updated><authors><author><keyname>Barchet-Estefanel</keyname><forenames>Luiz Angelo</forenames><affiliation>ID - IMAG</affiliation></author><author><keyname>Mounie</keyname><forenames>Gregory</forenames><affiliation>ID - IMAG</affiliation></author></authors><title>Performance Characterisation of Intra-Cluster Collective Communications</title><categories>cs.DC</categories><proxy>ccsd ccsd-00002546</proxy><journal-ref>Proceedings of the SBAC-PAD 2004 16th Symposium on Computer
  Architecture and High Performance Computing (2004) 254-261</journal-ref><abstract>  Although recent works try to improve collective communication in grid systems
by separating intra and inter-cluster communication, the optimisation of
communications focus only on inter-cluster communications. We believe, instead,
that the overall performance of the application may be improved if
intra-cluster collective communications performance is known in advance. Hence,
it is important to have an accurate model of the intra-cluster collective
communications, which provides the necessary evidences to tune and to predict
their performance correctly. In this paper we present our experience on
modelling such communication strategies. We describe and compare different
implementation strategies with their communication models, evaluating the
models' accuracy and describing the practical challenges that can be found when
modelling collective communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408033</id><created>2004-08-14</created><updated>2004-12-02</updated><authors><author><keyname>Barchet-Estefanel</keyname><forenames>Luiz Angelo</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author><author><keyname>Mounie</keyname><forenames>Gregory</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author></authors><title>Identifying Logical Homogeneous Clusters for Efficient Wide-area
  Communications</title><categories>cs.DC</categories><comments>http://www.springerlink.com/index/TTJJL61R1EXDLCMC</comments><proxy>ccsd ccsd-00002547</proxy><journal-ref>Lecture Notes in Computer Sciences Proceedings of the EuroPVM/MPI
  2004 11th European PVM/MPI Users' Group Meeting (2004) 319-326</journal-ref><abstract>  Recently, many works focus on the implementation of collective communication
operations adapted to wide area computational systems, like computational Grids
or global-computing. Due to the inherently heterogeneity of such environments,
most works separate &quot;clusters&quot; in different hierarchy levels. to better model
the communication. However, in our opinion, such works do not give enough
attention to the delimitation of such clusters, as they normally use the
locality or the IP subnet from the machines to delimit a cluster without
verifying the &quot;homogeneity&quot; of such clusters. In this paper, we describe a
strategy to gather network information from different local-area networks and
to construct &quot;logical homogeneous clusters&quot;, better suited to the performance
modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408034</id><created>2004-08-14</created><updated>2004-12-02</updated><authors><author><keyname>Barchet-Estefanel</keyname><forenames>Luiz Angelo</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author><author><keyname>Mounie</keyname><forenames>Gregory</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author></authors><title>Fast Tuning of Intra-Cluster Collective Communications</title><categories>cs.DC</categories><comments>http://www.springerlink.com/index/0NWE43KUHCB15JCD</comments><proxy>ccsd ccsd-00002548</proxy><journal-ref>Lecture Notes in Computer Sciences - Proceedings of the
  EuroPVM/MPI 2004 11th European PVM/MPI Users' Group Meeting (2004) 28-35</journal-ref><abstract>  Recent works try to optimise collective communication in grid systems
focusing mostly on the optimisation of communications among different clusters.
We believe that intra-cluster collective communications should also be
optimised, as a way to improve the overall efficiency and to allow the
construction of multi-level collective operations. Indeed, inside homogeneous
clusters, a simple optimisation approach rely on the comparison from different
implementation strategies, through their communication models. In this paper we
evaluate this approach, comparing different implementation strategies with
their predicted performances. As a result, we are able to choose the
communication strategy that better adapts to each network environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408035</id><created>2004-08-14</created><authors><author><keyname>Oppenheimer</keyname><forenames>David</forenames></author><author><keyname>Vatkovskiy</keyname><forenames>Vitaliy</forenames></author><author><keyname>Weatherspoon</keyname><forenames>Hakim</forenames></author><author><keyname>Lee</keyname><forenames>Jason</forenames></author><author><keyname>Patterson</keyname><forenames>David A.</forenames></author><author><keyname>Kubiatowicz</keyname><forenames>John</forenames></author></authors><title>Monitoring, Analyzing, and Controlling Internet-scale Systems with ACME</title><categories>cs.DC cs.NI</categories><report-no>UCB//CSD-03-1276</report-no><abstract>  Analyzing and controlling large distributed services under a wide range of
conditions is difficult. Yet these capabilities are essential to a number of
important development and operational tasks such as benchmarking, testing, and
system management. To facilitate these tasks, we have built the Application
Control and Monitoring Environment (ACME), a scalable, flexible infrastructure
for monitoring, analyzing, and controlling Internet-scale systems. ACME
consists of two parts. ISING, the Internet Sensor In-Network agGregator,
queries sensors and aggregates the results as they are routed through an
overlay network. ENTRIE, the ENgine for TRiggering Internet Events, uses the
data streams supplied by ISING, in combination with a user's XML configuration
file, to trigger actuators such as killing processes during a robustness
benchmark or paging a system administrator when predefined anomalous conditions
are observed. In this paper we describe the design, implementation, and
evaluation of ACME and its constituent parts. We find that for a 512-node
system running atop an emulated Internet topology, ISING's use of in-network
aggregation can reduce end-to-end query-response latency by more than 50%
compared to using either direct network connections or the same overlay network
without aggregation. We also find that an untuned implementation of ACME can
invoke an actuator on one or all nodes in response to a discrete or aggregate
event in less than four seconds, and we illustrate ACME's applicability to
concrete benchmarking and monitoring scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408036</id><created>2004-08-14</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Lamport</keyname><forenames>Leslie</forenames></author></authors><title>Consensus on Transaction Commit</title><categories>cs.DC cs.DB</categories><comments>Original at
  http://research.microsoft.com/research/pubs/view.aspx?tr_id=701</comments><report-no>MSR-TR-2003-96</report-no><acm-class>C.4</acm-class><abstract>  The distributed transaction commit problem requires reaching agreement on
whether a transaction is committed or aborted. The classic Two-Phase Commit
protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms
also reach agreement, but do not block whenever any majority of the processes
are working. Running a Paxos consensus algorithm on the commit/abort decision
of each participant yields a transaction commit protocol that uses 2F +1
coordinators and makes progress if at least F +1 of them are working. In the
fault-free case, this algorithm requires one extra message delay but has the
same stable-storage write delay as Two-Phase Commit. The classic Two-Phase
Commit algorithm is obtained as the special F = 0 case of the general Paxos
Commit algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408037</id><created>2004-08-15</created><authors><author><keyname>Villadsen</keyname><forenames>J&#xf8;rgen</forenames></author></authors><title>Multi-dimensional Type Theory: Rules, Categories, and Combinators for
  Syntax and Semantics</title><categories>cs.CL cs.AI cs.LO</categories><comments>20 pages</comments><acm-class>I.2.7</acm-class><abstract>  We investigate the possibility of modelling the syntax and semantics of
natural language by constraints, or rules, imposed by the multi-dimensional
type theory Nabla. The only multiplicity we explicitly consider is two, namely
one dimension for the syntax and one dimension for the semantics, but the
general perspective is important. For example, issues of pragmatics could be
handled as additional dimensions.
  One of the main problems addressed is the rather complicated repertoire of
operations that exists besides the notion of categories in traditional Montague
grammar. For the syntax we use a categorial grammar along the lines of Lambek.
For the semantics we use so-called lexical and logical combinators inspired by
work in natural logic. Nabla provides a concise interpretation and a sequent
calculus as the basis for implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408038</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408038</id><created>2004-08-16</created><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr.</suffix></author><author><keyname>Trott</keyname><forenames>Mitchell D.</forenames></author></authors><title>The Dynamics of Group Codes: Dual Abelian Group Codes and Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 11 figures. To appear in IEEE Trans. Inform. Theory, 2004</comments><acm-class>E.4; H.1.1</acm-class><journal-ref>IEEE Trans. Inform. Theory, vol. 50, pp. 2935-2965, Dec. 2004.</journal-ref><doi>10.1109/TIT.2004.838340</doi><abstract>  Fundamental results concerning the dynamics of abelian group codes
(behaviors) and their duals are developed. Duals of sequence spaces over
locally compact abelian groups may be defined via Pontryagin duality; dual
group codes are orthogonal subgroups of dual sequence spaces. The dual of a
complete code or system is finite, and the dual of a Laurent code or system is
(anti-)Laurent. If C and C^\perp are dual codes, then the state spaces of C act
as the character groups of the state spaces of C^\perp. The controllability
properties of C are the observability properties of C^\perp. In particular, C
is (strongly) controllable if and only if C^\perp is (strongly) observable, and
the controller memory of C is the observer memory of C^\perp. The controller
granules of C act as the character groups of the observer granules of C^\perp.
Examples of minimal observer-form encoder and syndrome-former constructions are
given. Finally, every observer granule of C is an &quot;end-around&quot; controller
granule of C.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408039</id><created>2004-08-16</created><authors><author><keyname>Shrivastava</keyname><forenames>Nisheeth</forenames></author><author><keyname>Buragohain</keyname><forenames>Chiranjeeb</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author></authors><title>Medians and Beyond: New Aggregation Techniques for Sensor Networks</title><categories>cs.DC cs.DB cs.DS</categories><journal-ref>Proceedings of the Second ACM Conference on Embedded Networked
  Sensor Systems (SenSys 2004)</journal-ref><abstract>  Wireless sensor networks offer the potential to span and monitor large
geographical areas inexpensively. Sensors, however, have significant power
constraint (battery life), making communication very expensive. Another
important issue in the context of sensor-based information systems is that
individual sensor readings are inherently unreliable. In order to address these
two aspects, sensor database systems like TinyDB and Cougar enable in-network
data aggregation to reduce the communication cost and improve reliability. The
existing data aggregation techniques, however, are limited to relatively simple
types of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we propose
a data aggregation scheme that significantly extends the class of queries that
can be answered using sensor networks. These queries include (approximate)
quantiles, such as the median, the most frequent data values, such as the
consensus value, a histogram of the data distribution, as well as range
queries. In our scheme, each sensor aggregates the data it has received from
other sensors into a fixed (user specified) size message. We provide strict
theoretical guarantees on the approximation quality of the queries in terms of
the message size. We evaluate the performance of our aggregation scheme by
simulation and demonstrate its accuracy, scalability and low resource
utilization for highly variable input data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408040</id><created>2004-08-17</created><authors><author><keyname>Gilreath</keyname><forenames>William F.</forenames></author></authors><title>Hash sort: A linear time complexity multiple-dimensional sort algorithm</title><categories>cs.DS</categories><journal-ref>Proceedings of First Southern Symposium on Computing December 1998</journal-ref><abstract>  Sorting and hashing are two completely different concepts in computer
science, and appear mutually exclusive to one another. Hashing is a search
method using the data as a key to map to the location within memory, and is
used for rapid storage and retrieval. Sorting is a process of organizing data
from a random permutation into an ordered arrangement, and is a common activity
performed frequently in a variety of applications.
  Almost all conventional sorting algorithms work by comparison, and in doing
so have a linearithmic greatest lower bound on the algorithmic time complexity.
Any improvement in the theoretical time complexity of a sorting algorithm can
result in overall larger gains in implementation performance.. A gain in
algorithmic performance leads to much larger gains in speed for the application
that uses the sort algorithm. Such a sort algorithm needs to use an alternative
method for ordering the data than comparison, to exceed the linearithmic time
complexity boundary on algorithmic performance.
  The hash sort is a general purpose non-comparison based sorting algorithm by
hashing, which has some interesting features not found in conventional sorting
algorithms. The hash sort asymptotically outperforms the fastest traditional
sorting algorithm, the quick sort. The hash sort algorithm has a linear time
complexity factor -- even in the worst case. The hash sort opens an area for
further work and investigation into alternative means of sorting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408041</id><created>2004-08-17</created><authors><author><keyname>Eftekhari</keyname><forenames>Ali</forenames></author></authors><title>Fractal geometry of literature: first attempt to Shakespeare's works</title><categories>cs.CL cs.CC</categories><comments>26 pages, 7 figures, 3 tables</comments><abstract>  It was demonstrated that there is a geometrical order in the structure of
literature. Fractal geometry as a modern mathematical approach and a new
geometrical viewpoint on natural objects including both processes and
structures was employed for analysis of literature. As the first study, the
works of William Shakespeare were chosen as the most important items in western
literature. By counting the number of letters applied in a manuscript, it is
possible to study the whole manuscript statistically. A novel method based on
basic assumption of fractal geometry was proposed for the calculation of
fractal dimensions of the literature. The results were compared with Zipf's
law. Zipf's law was successfully used for letters instead of words. Two new
concepts namely Zipf's dimension and Zipf's order were also introduced. It was
found that changes of both fractal dimension and Zipf's dimension are similar
and dependent on the manuscript length. Interestingly, direct plotting the data
obtained in semi-logarithmic and logarithmic forms also led to a power-law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408042</id><created>2004-08-18</created><authors><author><keyname>Tilak</keyname><forenames>Sameer</forenames></author><author><keyname>Kolar</keyname><forenames>Vinay</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael B.</forenames></author><author><keyname>Kang</keyname><forenames>Kyoung-Don</forenames></author></authors><title>Dynamic Localization Protocols for Mobile Sensor Networks</title><categories>cs.NI</categories><comments>10 Pages</comments><report-no>CS-TR-04-NA02</report-no><acm-class>C.2.1 Wireless communication, C.2.2 Network Protocols</acm-class><abstract>  The ability of a sensor node to determine its physical location within a
network (Localization) is of fundamental importance in sensor networks.
Interpretating data from sensors will not be possible unless the context of the
data is known; this is most often accomplished by tracking its physical
location. Existing research has focused on localization in static sensor
networks where localization is a one-time (or low frequency) activity. In
contrast, this paper considers localization for mobile sensors: when sensors
are mobile, localization must be invoked periodically to enable the sensors to
track their location. The higher the frequency of localization, the lower the
error introduced because of mobility. However, localization is a costly
operation since it involves both communication and computation. In this paper,
we propose and investigate adaptive and predictive protocols that control the
frequency of localization based on sensor mobility behavior to reduce the
energy requirements for localization while bounding the localization error. We
show that such protocols can significantly reduce the localization energy
without sacrificing accuracy (in fact, improving accuracy for most situations).
Using simulation and analysis we explore the tradeoff between energy efficiency
and localization error due to mobility for several protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408043</id><created>2004-08-18</created><authors><author><keyname>Hitchcock</keyname><forenames>John M.</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Terwijn</keyname><forenames>Sebastiaan A.</forenames></author></authors><title>The Arithmetical Complexity of Dimension and Randomness</title><categories>cs.LO cs.CC</categories><comments>20 pages</comments><abstract>  Constructive dimension and constructive strong dimension are effectivizations
of the Hausdorff and packing dimensions, respectively. Each infinite binary
sequence A is assigned a dimension dim(A) in [0,1] and a strong dimension
Dim(A) in [0,1].
  Let DIM^alpha and DIMstr^alpha be the classes of all sequences of dimension
alpha and of strong dimension alpha, respectively. We show that DIM^0 is
properly Pi^0_2, and that for all Delta^0_2-computable alpha in (0,1],
DIM^alpha is properly Pi^0_3.
  To classify the strong dimension classes, we use a more powerful effective
Borel hierarchy where a co-enumerable predicate is used rather than a
enumerable predicate in the definition of the Sigma^0_1 level. For all
Delta^0_2-computable alpha in [0,1), we show that DIMstr^alpha is properly in
the Pi^0_3 level of this hierarchy. We show that DIMstr^1 is properly in the
Pi^0_2 level of this hierarchy.
  We also prove that the class of Schnorr random sequences and the class of
computably random sequences are properly Pi^0_3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408044</id><created>2004-08-19</created><authors><author><keyname>Thielscher</keyname><forenames>Michael</forenames></author></authors><title>FLUX: A Logic Programming Method for Reasoning Agents</title><categories>cs.AI</categories><abstract>  FLUX is a programming method for the design of agents that reason logically
about their actions and sensor information in the presence of incomplete
knowledge. The core of FLUX is a system of Constraint Handling Rules, which
enables agents to maintain an internal model of their environment by which they
control their own behavior. The general action representation formalism of the
fluent calculus provides the formal semantics for the constraint solver. FLUX
exhibits excellent computational behavior due to both a carefully restricted
expressiveness and the inference paradigm of progression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408045</id><created>2004-08-19</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Leino</keyname><forenames>K. Rustan M.</forenames></author></authors><title>On computing the fixpoint of a set of boolean equations</title><categories>cs.PL cs.LO cs.SE</categories><comments>15 pages</comments><report-no>MSR-TR-2003-08</report-no><acm-class>D.2.4; D.3.1; F.3.1; F.3.2; F.4.1</acm-class><abstract>  This paper presents a method for computing a least fixpoint of a system of
equations over booleans. The resulting computation can be significantly shorter
than the result of iteratively evaluating the entire system until a fixpoint is
reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408046</id><created>2004-08-20</created><updated>2004-11-11</updated><authors><author><keyname>Volkmer</keyname><forenames>Markus</forenames></author><author><keyname>Schaumburg</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Authenticated tree parity machine key exchange</title><categories>cs.CR cond-mat.dis-nn</categories><comments>This work directly relates to cond-mat/0202112 (see also
  http://arxiv.org/find/cond-mat/1/au:+Kinzel/0/1/0/all/0/1)</comments><abstract>  The synchronisation of Tree Parity Machines (TPMs), has proven to provide a
valuable alternative concept for secure symmetric key exchange. Yet, from a
cryptographer's point of view, authentication is at least as important as a
secure exchange of keys. Adding an authentication via hashing e.g. is
straightforward but with no relation to Neural Cryptography. We consequently
formulate an authenticated key exchange within this concept. Another
alternative, integrating a Zero-Knowledge protocol into the synchronisation, is
also presented. A Man-In-The-Middle attack and even all currently known
attacks, that are based on using identically structured TPMs and
synchronisation as well, can so be averted. This in turn has practical
consequences on using the trajectory in weight space. Both suggestions have the
advantage of not affecting the previously observed physics of this interacting
system at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408047</identifier>
 <datestamp>2009-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408047</id><created>2004-08-20</created><updated>2009-10-04</updated><authors><author><keyname>Heistracher</keyname><forenames>Thomas</forenames></author><author><keyname>Kurz</keyname><forenames>Thomas</forenames></author><author><keyname>Masuch</keyname><forenames>Claudius</forenames></author><author><keyname>Ferronato</keyname><forenames>Pierfranco</forenames></author><author><keyname>Vidal</keyname><forenames>Miguel</forenames></author><author><keyname>Corallo</keyname><forenames>Angelo</forenames></author><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author><author><keyname>Dini</keyname><forenames>Paolo</forenames></author></authors><title>Pervasive Service Architecture for a Digital Business Ecosystem</title><categories>cs.CE cs.NI</categories><comments>10 Pages 2 Figures Presented at WCAT04 Workshop (ECOOP 2004
  Conference), Olso, Norway, 14 June 2004</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present ideas and architectural principles upon which we are
basing the development of a distributed, open-source infrastructure that, in
turn, will support the expression of business models, the dynamic composition
of software services, and the optimisation of service chains through automatic
self-organising and evolutionary algorithms derived from biology. The target
users are small and medium-sized enterprises (SMEs). We call the collection of
the infrastructure, the software services, and the SMEs a Digital Business
Ecosystem (DBE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408048</id><created>2004-08-21</created><authors><author><keyname>Funge</keyname><forenames>John David</forenames></author></authors><title>Journal of New Democratic Methods: An Introduction</title><categories>cs.CY cs.LG</categories><comments>8 pages, 1 figure</comments><acm-class>I.2.6; J.1; K.4.3</acm-class><abstract>  This paper describes a new breed of academic journals that use statistical
machine learning techniques to make them more democratic. In particular, not
only can anyone submit an article, but anyone can also become a reviewer.
Machine learning is used to decide which reviewers accurately represent the
views of the journal's readers and thus deserve to have their opinions carry
more weight. The paper concentrates on describing a specific experimental
prototype of a democratic journal called the Journal of New Democratic Methods
(JNDM). The paper also mentions the wider implications that machine learning
and the techniques used in the JNDM may have for representative democracy in
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408049</id><created>2004-08-21</created><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Using Stochastic Encoders to Discover Structure in Data</title><categories>cs.NE cs.CV</categories><comments>18 pages, 9 figures. Full version of a short paper that was published
  in the Digest of the 5th IMA International Conference on Mathematics in
  Signal Processing, 18-20 December 2000, Warwick University, UK</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  In this paper a stochastic generalisation of the standard Linde-Buzo-Gray
(LBG) approach to vector quantiser (VQ) design is presented, in which the
encoder is implemented as the sampling of a vector of code indices from a
probability distribution derived from the input vector, and the decoder is
implemented as a superposition of reconstruction vectors. This stochastic VQ
(SVQ) is optimised using a minimum mean Euclidean reconstruction distortion
criterion, as in the LBG case. Numerical simulations are used to demonstrate
how this leads to self-organisation of the SVQ, where different stochastically
sampled code indices become associated with different input subspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408050</id><created>2004-08-21</created><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Invariant Stochastic Encoders</title><categories>cs.NE cs.CV</categories><comments>16 pages, 12 figures. Full version of a short paper that was
  published in the Digest of the 5th IMA International Conference on
  Mathematics in Signal Processing, 18-20 December 2000, Warwick University, UK</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  The theory of stochastic vector quantisers (SVQ) has been extended to allow
the quantiser to develop invariances, so that only &quot;large&quot; degrees of freedom
in the input vector are represented in the code. This has been applied to the
problem of encoding data vectors which are a superposition of a &quot;large&quot; jammer
and a &quot;small&quot; signal, so that only the jammer is represented in the code. This
allows the jammer to be subtracted from the total input vector (i.e. the jammer
is nulled), leaving a residual that contains only the underlying signal. The
main advantage of this approach to jammer nulling is that little prior
knowledge of the jammer is assumed, because these properties are automatically
discovered by the SVQ as it is trained on examples of input vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408051</id><created>2004-08-21</created><authors><author><keyname>Guo</keyname><forenames>Zhimao</forenames></author><author><keyname>Li</keyname><forenames>Min</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoling</forenames></author><author><keyname>Zhou</keyname><forenames>Aoying</forenames></author></authors><title>Scalable XSLT Evaluation</title><categories>cs.DB</categories><comments>It appeared on the international conference of APWeb 04. And it
  includes 10 pages</comments><journal-ref>In Proc. of APWeb, 2004</journal-ref><abstract>  XSLT is an increasingly popular language for processing XML data. It is
widely supported by application platform software. However, little optimization
effort has been made inside the current XSLT processing engines. Evaluating a
very simple XSLT program on a large XML document with a simple schema may
result in extensive usage of memory. In this paper, we present a novel notion
of \emph{Streaming Processing Model} (\emph{SPM}) to evaluate a subset of XSLT
programs on XML documents, especially large ones. With SPM, an XSLT processor
can transform an XML source document to other formats without extra memory
buffers required. Therefore, our approach can not only tackle large source
documents, but also produce large results. We demonstrate with a performance
study the advantages of the SPM approach. Experimental results clearly confirm
that SPM improves XSLT evaluation typically 2 to 10 times better than the
existing approaches. Moreover, the SPM approach also features high scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408052</id><created>2004-08-22</created><authors><author><keyname>Yacob</keyname><forenames>Daniel</forenames></author></authors><title>Application of the Double Metaphone Algorithm to Amharic Orthography</title><categories>cs.CL</categories><comments>International Conference of Ethiopian Studies XV, 13 pages</comments><abstract>  The Metaphone algorithm applies the phonetic encoding of orthographic
sequences to simplify words prior to comparison. While Metaphone has been
highly successful for the English language, for which it was designed, it may
not be applied directly to Ethiopian languages. The paper details how the
principles of Metaphone can be applied to Ethiopic script and uses Amharic as a
case study. Match results improve as specific considerations are made for
Amharic writing practices. Results are shown to improve further when common
errors from Amharic input methods are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408053</id><created>2004-08-23</created><authors><author><keyname>Yuste</keyname><forenames>Santos B.</forenames></author></authors><title>Weighted average finite difference methods for fractional diffusion
  equations</title><categories>cs.NA cond-mat.stat-mech physics.comp-ph</categories><comments>Communication presented at the FDA'04 Workshop (with some minor
  corrections and updates)</comments><acm-class>G.1.9, G.1.4, G.1.0</acm-class><abstract>  Weighted averaged finite difference methods for solving fractional diffusion
equations are discussed and different formulae of the discretization of the
Riemann-Liouville derivative are considered. The stability analysis of the
different numerical schemes is carried out by means of a procedure close to the
well-known von Neumann method of ordinary diffusion equations. The stability
bounds are easily found and checked in some representative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408054</id><created>2004-08-24</created><authors><author><keyname>Heuscher</keyname><forenames>Stephan</forenames><affiliation>Swiss Federal Archives</affiliation></author><author><keyname>Jaermann</keyname><forenames>Stephan</forenames><affiliation>Swiss Federal Archives</affiliation></author><author><keyname>Keller-Marxer</keyname><forenames>Peter</forenames><affiliation>Swiss Federal Archives</affiliation></author><author><keyname>Moehle</keyname><forenames>Frank</forenames><affiliation>Swiss Federal Archives</affiliation></author></authors><title>Providing Authentic Long-term Archival Access to Complex Relational Data</title><categories>cs.DL cs.DB</categories><comments>18 pages with 4 figures (color). Submitted to: European Space Agency
  Symposium &quot;Ensuring Long-Term Preservation and Adding Value to Scientific and
  Technical Data&quot;, 5 - 7 October 2004, Frascati, Italy</comments><report-no>ESA WPP-232, pp. 241 -261</report-no><journal-ref>Proceedings PV-2004: Ensuring the Long-Term Preservation and
  Adding Value to the Scientific and Technical Data, 5-7 October 2004,
  ESA/ESRIN, Frascati, Italy, (ESA WPP-232), Noordwijk: European Space Agency,
  2004, pp. 241\^A?&quot;261.</journal-ref><abstract>  We discuss long-term preservation of and access to relational databases. The
focus is on national archives and science data archives which have to ingest
and integrate data from a broad spectrum of vendor-specific relational database
management systems (RDBMS). Furthermore, we present our solution SIARD which
analyzes and extracts data and data logic from almost any RDBMS. It enables, to
a reasonable level of authenticity, complete detachment of databases from their
vendor-specific environment. The user can add archival descriptive metadata
according to a customizable schema. A SIARD database archive integrates data,
data logic, technical metadata, and archival descriptive information in one
archival information package, independent of any specific software and
hardware, based upon plain text files and the standardized languages SQL and
XML. For usage purposes, a SIARD archive can be reloaded into any current or
future RDBMS which supports standard SQL. In addition, SIARD contains a client
that enables 'on demand' reload of archives into a target RDBMS, and multi-user
remote access for querying and browsing the data together with its technical
and descriptive metadata in one graphical user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408055</id><created>2004-08-24</created><authors><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Murty</keyname><forenames>M. Narasimha</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection
  Scheme in Evolutionary Algorithms</title><categories>cs.AI</categories><journal-ref>Dukkipati, A., M. N. Murty, and S. Bhatnagar, 2004, in Proceedings
  of the Congress on Evolutionary Computation (CEC'2004), IEEE Press, pp. 55-62</journal-ref><abstract>  Boltzmann selection is an important selection mechanism in evolutionary
algorithms as it has theoretical properties which help in theoretical analysis.
However, Boltzmann selection is not used in practice because a good annealing
schedule for the `inverse temperature' parameter is lacking. In this paper we
propose a Cauchy annealing schedule for Boltzmann selection scheme based on a
hypothesis that selection-strength should increase as evolutionary process goes
on and distance between two selection strengths should decrease for the process
to converge. To formalize these aspects, we develop formalism for selection
mechanisms using fitness distributions and give an appropriate measure for
selection-strength. In this paper, we prove an important result, by which we
derive an annealing schedule called Cauchy annealing schedule. We demonstrate
the novelty of proposed annealing schedule using simulations in the framework
of genetic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408056</id><created>2004-08-24</created><authors><author><keyname>Alberti</keyname><forenames>Marco</forenames></author><author><keyname>Gavanelli</keyname><forenames>Marco</forenames></author><author><keyname>Lamma</keyname><forenames>Evelina</forenames></author><author><keyname>Mello</keyname><forenames>Paola</forenames></author><author><keyname>Milano</keyname><forenames>Michela</forenames></author></authors><title>A CHR-based Implementation of Known Arc-Consistency</title><categories>cs.LO cs.AI</categories><comments>22 pages, 2 figures, 1 table To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><abstract>  In classical CLP(FD) systems, domains of variables are completely known at
the beginning of the constraint propagation process. However, in systems
interacting with an external environment, acquiring the whole domains of
variables before the beginning of constraint propagation may cause waste of
computation time, or even obsolescence of the acquired data at the time of use.
  For such cases, the Interactive Constraint Satisfaction Problem (ICSP) model
has been proposed as an extension of the CSP model, to make it possible to
start constraint propagation even when domains are not fully known, performing
acquisition of domain elements only when necessary, and without the need for
restarting the propagation after every acquisition.
  In this paper, we show how a solver for the two sorted CLP language, defined
in previous work, to express ICSPs, has been implemented in the Constraint
Handling Rules (CHR) language, a declarative language particularly suitable for
high level implementation of constraint solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408057</id><created>2004-08-25</created><authors><author><keyname>Ballim</keyname><forenames>Afzal</forenames></author><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author></authors><title>The role of robust semantic analysis in spoken language dialogue systems</title><categories>cs.CL cs.AI cs.HC</categories><comments>6 pages</comments><acm-class>H.5.2;H.3.1;H.3.4</acm-class><journal-ref>Proceedings of the 3rd International Workshop on Human-Computer
  Conversation, July 3-5, 2000, Bellagio, Italy</journal-ref><abstract>  In this paper we summarized a framework for designing grammar-based procedure
for the automatic extraction of the semantic content from spoken queries.
Starting with a case study and following an approach which combines the notions
of fuzziness and robustness in sentence parsing, we showed we built practical
domain-dependent rules which can be applied whenever it is possible to
superimpose a sentence-level semantic structure to a text without relying on a
previous deep syntactical analysis. This kind of procedure can be also
profitably used as a pre-processing tool in order to cut out part of the
sentence which have been recognized to have no relevance in the understanding
process. In the case of particular dialogue applications where there is no need
to build a complex semantic structure (e.g. word spotting or excerpting) the
presented methodology may represent an efficient alternative solution to a
sequential composition of deep linguistic analysis modules. Even if the query
generation problem may not seem a critical application it should be held in
mind that the sentence processing must be done on-line. Having this kind of
constraints we cannot design our system without caring for efficiency and thus
provide an immediate response. Another critical issue is related to whole
robustness of the system. In our case study we tried to make experiences on how
it is possible to deal with an unreliable and noisy input without asking the
user for any repetition or clarification. This may correspond to a similar
problem one may have when processing text coming from informal writing such as
e-mails, news and in many cases Web pages where it is often the case to have
irrelevant surrounding information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408058</id><created>2004-08-25</created><authors><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author></authors><title>Non-negative matrix factorization with sparseness constraints</title><categories>cs.LG cs.NE</categories><abstract>  Non-negative matrix factorization (NMF) is a recently developed technique for
finding parts-based, linear representations of non-negative data. Although it
has successfully been applied in several applications, it does not always
result in parts-based representations. In this paper, we show how explicitly
incorporating the notion of `sparseness' improves the found decompositions.
Additionally, we provide complete MATLAB code both for standard NMF and for our
extension. Our hope is that this will further the application of these methods
to solving novel data-analysis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408059</id><created>2004-08-26</created><authors><author><keyname>Tsalidis</keyname><forenames>Ch.</forenames><affiliation>Neurosoft S.A</affiliation></author><author><keyname>Orphanos</keyname><forenames>G.</forenames><affiliation>Neurosoft S.A</affiliation></author><author><keyname>Iordanidou</keyname><forenames>A.</forenames><affiliation>Patra's University</affiliation></author><author><keyname>Vagelatos</keyname><forenames>A.</forenames><affiliation>RACTI</affiliation></author></authors><title>Proofing Tools Technology at Neurosoft S.A.</title><categories>cs.CL</categories><comments>Workshop on International Proofing Tools and Language Technologies
  July 1-2, 2004, Patras, Greece</comments><report-no>CTI T.R.: 2004.06.01</report-no><acm-class>I.2.7</acm-class><abstract>  The aim of this paper is to present the R&amp;D activities carried out at
Neurosoft S.A. regarding the development of proofing tools for Modern Greek.
Firstly, we focus on infrastructure issues that we faced during our initial
steps. Subsequently, we describe the most important insights of three proofing
tools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the
thesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss
some improvement ideas and give our future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408060</id><created>2004-08-26</created><authors><author><keyname>Bes</keyname><forenames>Gabriel G.</forenames><affiliation>GRIL</affiliation></author><author><keyname>Lamadon</keyname><forenames>Lionel</forenames><affiliation>GRIL</affiliation></author><author><keyname>Trouilleux</keyname><forenames>Francois</forenames><affiliation>GRIL</affiliation></author></authors><title>Verbal chunk extraction in French using limited resources</title><categories>cs.CL</categories><proxy>ccsd ccsd-00002699</proxy><abstract>  A way of extracting French verbal chunks, inflected and infinitive, is
explored and tested on effective corpus. Declarative morphological and local
grammar rules specifying chunks and some simple contextual structures are used,
relying on limited lexical information and some simple heuristic/statistic
properties obtained from restricted corpora. The specific goals, the
architecture and the formalism of the system, the linguistic information on
which it relies and the obtained results on effective corpus are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408061</id><created>2004-08-26</created><authors><author><keyname>Tsalidis</keyname><forenames>Ch.</forenames><affiliation>Neurosoft S.A</affiliation></author><author><keyname>Vagelatos</keyname><forenames>A.</forenames><affiliation>RACTI</affiliation></author><author><keyname>Orphanos</keyname><forenames>G.</forenames><affiliation>Neurosoft S.A</affiliation></author></authors><title>An electronic dictionary as a basis for NLP tools: The Greek case</title><categories>cs.CL</categories><comments>Traitement Automatique des Langues Naturelles (TALN) 2004, Fez,
  Morocco</comments><report-no>CTI T.R.: 2004.04.03</report-no><acm-class>I.2.7</acm-class><abstract>  The existence of a Dictionary in electronic form for Modern Greek (MG) is
mandatory if one is to process MG at the morphological and syntactic levels
since MG is a highly inflectional language with marked stress and a spelling
system with many characteristics carried over from Ancient Greek. Moreover,
such a tool becomes necessary if one is to create efficient and sophisticated
NLP applications with substantial linguistic backing and coverage. The present
paper will focus on the deployment of such an electronic dictionary for Modern
Greek, which was built in two phases: first it was constructed to be the basis
for a spelling correction schema and then it was reconstructed in order to
become the platform for the deployment of a wider spectrum of NLP tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408062</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408062</id><created>2004-08-27</created><authors><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Source Coding With Distortion Side Information At The Encoder</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures</comments><acm-class>E.4; H.1.1</acm-class><journal-ref>Proceedings of the 2004 Data Compression Conference in Snowbird,
  UT</journal-ref><abstract>  We consider lossy source coding when side information affecting the
distortion measure may be available at the encoder, decoder, both, or neither.
For example, such distortion side information can model reliabilities for noisy
measurements, sensor calibration information, or perceptual effects like
masking and sensitivity to context. When the distortion side information is
statistically independent of the source, we show that in many cases (e.g, for
additive or multiplicative distortion side information) there is no penalty for
knowing the side information only at the encoder, and there is no advantage to
knowing it at the decoder. Furthermore, for quadratic distortion measures
scaled by the distortion side information, we evaluate the penalty for lack of
encoder knowledge and show that it can be arbitrarily large. In this scenario,
we also sketch transform based quantizers constructions which efficiently
exploit encoder side information in the high-resolution limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408063</id><created>2004-08-27</created><authors><author><keyname>Haubold</keyname><forenames>Alexander</forenames></author><author><keyname>Kender</keyname><forenames>John R.</forenames></author></authors><title>Analysis and Visualization of Index Words from Audio Transcripts of
  Instructional Videos</title><categories>cs.IR cs.MM</categories><comments>2004 IEEE International Workshop on Multimedia Content-based Analysis
  and Retrieval; 20 pages, 8 figures, 7 tables</comments><acm-class>H.3.1;H.3.3;I.2.10</acm-class><abstract>  We introduce new techniques for extracting, analyzing, and visualizing
textual contents from instructional videos of low production quality. Using
Automatic Speech Recognition, approximate transcripts (H75% Word Error Rate)
are obtained from the originally highly compressed videos of university
courses, each comprising between 10 to 30 lectures. Text material in the form
of books or papers that accompany the course are then used to filter meaningful
phrases from the seemingly incoherent transcripts. The resulting index into the
transcripts is tied together and visualized in 3 experimental graphs that help
in understanding the overall course structure and provide a tool for localizing
certain topics for indexing. We specifically discuss a Transcript Index Map,
which graphically lays out key phrases for a course, a Textbook Chapter to
Transcript Match, and finally a Lecture Transcript Similarity graph, which
clusters semantically similar lectures. We test our methods and tools on 7 full
courses with 230 hours of video and 273 transcripts. We are able to extract up
to 98 unique key terms for a given transcript and up to 347 unique key terms
for an entire course. The accuracy of the Textbook Chapter to Transcript Match
exceeds 70% on average. The methods used can be applied to genres of video in
which there are recurrent thematic words (news, sports, meetings,...)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408064</id><created>2004-08-27</created><updated>2005-03-25</updated><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Dezert</keyname><forenames>Jean</forenames></author></authors><title>Proportional Conflict Redistribution Rules for Information Fusion</title><categories>cs.AI</categories><comments>41 pages</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of the 8th International Conference on Information
  Fusion, Philadelphia, 25-29 July, 2005; IEEE Catalog Number: 05EX1120C,
  ISBN: 0-7803-9287-6.</journal-ref><abstract>  In this paper we propose five versions of a Proportional Conflict
Redistribution rule (PCR) for information fusion together with several
examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of
the rules and also the exactitude of the redistribution of conflicting masses.
PCR1 restricted from the hyper-power set to the power set and without
degenerate cases gives the same result as the Weighted Average Operator (WAO)
proposed recently by J{\o}sang, Daniel and Vannoorenberghe but does not satisfy
the neutrality property of vacuous belief assignment. That's why improved PCR
rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's
rules. The PCR rules redistribute the conflicting mass, after the conjunctive
rule has been applied, proportionally with some functions depending on the
masses assigned to their corresponding columns in the mass matrix. There are
infinitely many ways these functions (weighting factors) can be chosen
depending on the complexity one wants to deal with in specific applications and
fusion systems. Any fusion combination rule is at some degree ad-hoc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408065</id><created>2004-08-28</created><updated>2004-09-11</updated><authors><author><keyname>Lahiri</keyname><forenames>Somdeb</forenames></author></authors><title>The Core of Directed Network Problems with Quotas</title><categories>cs.GT</categories><comments>6 pages, 0 figures, source file: MS Word; definitions of the feasible
  allocations have been strengthened; examples provided; network obtained by
  the procedure can be decentralized</comments><abstract>  This paper proves the existence of non-empty cores for directed network
problems with quotas and for those combinatorial allocation problems which
permit only exclusive allocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408066</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408066</id><created>2004-08-30</created><authors><author><keyname>Ben-Sasson</keyname><forenames>Eli</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Robust Locally Testable Codes and Products of Codes</title><categories>cs.IT cs.CC math.IT</categories><acm-class>E.4</acm-class><abstract>  We continue the investigation of locally testable codes, i.e.,
error-correcting codes for whom membership of a given word in the code can be
tested probabilistically by examining it in very few locations. We give two
general results on local testability: First, motivated by the recently proposed
notion of {\em robust} probabilistically checkable proofs, we introduce the
notion of {\em robust} local testability of codes. We relate this notion to a
product of codes introduced by Tanner, and show a very simple composition lemma
for this notion. Next, we show that codes built by tensor products can be
tested robustly and somewhat locally, by applying a variant of a test and proof
technique introduced by Raz and Safra in the context of testing low-degree
multivariate polynomials (which are a special case of tensor codes).
  Combining these two results gives us a generic construction of codes of
inverse polynomial rate, that are testable with poly-logarithmically many
queries. We note these locally testable tensor codes can be obtained from {\em
any} linear error correcting code with good distance. Previous results on local
testability, albeit much stronger quantitatively, rely heavily on algebraic
properties of the underlying codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408067</id><created>2004-08-31</created><authors><author><keyname>Hansen</keyname><forenames>Jennie C.</forenames></author><author><keyname>Schmutz</keyname><forenames>Eric</forenames></author></authors><title>The Expected Size of the Rule k Dominating Set</title><categories>cs.DM</categories><abstract>  Rule k is a localized approximation algorithm that finds a small connected
dominating set in a graph. We estimate the expected size of the Rule k
dominating set for the model of random unit disk graphs constructed from n
random points in an s_n by s_n square region of the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408068</id><created>2004-08-31</created><authors><author><keyname>Hansen</keyname><forenames>Jennie C.</forenames></author><author><keyname>Schmutz</keyname><forenames>Eric</forenames></author><author><keyname>Sheng</keyname><forenames>Li</forenames></author></authors><title>Probabilistic Analysis of Rule 2</title><categories>cs.DM</categories><abstract>  Li and Wu proposed Rule 2, a localized approximation algorithm that attempts
to find a small connected dominating set in a graph. Here we study the
asymptotic performance of Rule 2 on random unit disk graphs formed from n
random points in an s_n by s_n square region of the plane. If s_n is below the
threshold for connectivity, then Rule 2 produces a dominating set whose
expected size is O(n/(loglog n)^{3/2}). We conjecture that this bound is not
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0408069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0408069</id><created>2004-08-31</created><authors><author><keyname>Bader</keyname><forenames>Sebastian</forenames></author><author><keyname>Hitzler</keyname><forenames>Pascal</forenames></author><author><keyname>Hoelldobler</keyname><forenames>Steffen</forenames></author></authors><title>The Integration of Connectionism and First-Order Knowledge
  Representation and Reasoning as a Challenge for Artificial Intelligence</title><categories>cs.AI cs.LO cs.NE</categories><comments>In Proceedings of INFORMATION'2004, Tokyo, Japan, to appear. 12 pages</comments><acm-class>I.2.3,I.2.6</acm-class><abstract>  Intelligent systems based on first-order logic on the one hand, and on
artificial neural networks (also called connectionist systems) on the other,
differ substantially. It would be very desirable to combine the robust neural
networking machinery with symbolic knowledge representation and reasoning
paradigms like logic programming in such a way that the strengths of either
paradigm will be retained. Current state-of-the-art research, however, fails by
far to achieve this ultimate goal. As one of the main obstacles to be overcome
we perceive the question how symbolic knowledge can be encoded by means of
connectionist systems: Satisfactory answers to this will naturally lead the way
to knowledge extraction algorithms and to integrated neural-symbolic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409001</identifier>
 <datestamp>2007-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409001</id><created>2004-09-01</created><authors><author><keyname>Stauffer</keyname><forenames>A. O.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>Probabilistic heuristics for disseminating information in networks</title><categories>cs.NI cs.DC</categories><report-no>ES-660/04</report-no><acm-class>C.2.2; F.2.2</acm-class><journal-ref>IEEE/ACM Transactions on Networking 15 (2007), 425-435</journal-ref><doi>10.1109/TNET.2007.892877</doi><abstract>  We study the problem of disseminating a piece of information through all the
nodes of a network, given that it is known originally only to a single node. In
the absence of any structural knowledge on the network other than the nodes'
neighborhoods, this problem is traditionally solved by flooding all the
network's edges. We analyze a recently introduced probabilistic algorithm for
flooding and give an alternative probabilistic heuristic that can lead to some
cost-effective improvements, like better trade-offs between the message and
time complexities involved. We analyze the two algorithms both mathematically
and by means of simulations, always within a random-graph framework and
considering relevant node-degree distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409002</id><created>2004-09-01</created><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames></author></authors><title>Default reasoning over domains and concept hierarchies</title><categories>cs.AI cs.LO</categories><comments>Short version appeared in Proceedings of the 27th German conference
  on Artificial Intelligence, KI'2004, Ulm, Germany, September 2004, Lecture
  Notes in Artificial Intelligence</comments><acm-class>I.2.3; I.2.4; D.1.6</acm-class><abstract>  W.C. Rounds and G.-Q. Zhang (2001) have proposed to study a form of
disjunctive logic programming generalized to algebraic domains. This system
allows reasoning with information which is hierarchically structured and forms
a (suitable) domain. We extend this framework to include reasoning with default
negation, giving rise to a new nonmonotonic reasoning framework on hierarchical
knowledge which encompasses answer set programming with extended disjunctive
logic programs. We also show that the hierarchically structured knowledge on
which programming in this paradigm can be done, arises very naturally from
formal concept analysis. Together, we obtain a default reasoning paradigm for
conceptual knowledge which is in accordance with mainstream developments in
nonmonotonic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409003</id><created>2004-09-02</created><authors><author><keyname>Bhawalkar</keyname><forenames>Parth</forenames></author><author><keyname>Bigio</keyname><forenames>Victor</forenames></author><author><keyname>Davis</keyname><forenames>Adam</forenames></author><author><keyname>Narayanaswami</keyname><forenames>Karthik</forenames></author><author><keyname>Olumoko</keyname><forenames>Femi</forenames></author></authors><title>ScheduleNanny: Using GPS to Learn the User's Significant Locations,
  Travel Times and Schedule</title><categories>cs.AI cs.CV cs.HC</categories><comments>7 pages, 10 figures. Adaptive &amp; Ubiquitous Computing</comments><acm-class>F.2.2; I.5.3; H.5.3; H.5.m</acm-class><abstract>  As computing technology becomes more pervasive, personal devices such as the
PDA, cell-phone, and notebook should use context to determine how to act.
Location is one form of context that can be used in many ways. We present a
multiple-device system that collects and clusters GPS data into significant
locations. These locations are then used to determine travel times and a
probabilistic model of the user's schedule, which is used to intelligently
alert the user. We evaluate our system and suggest how it should be integrated
with a variety of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409004</id><created>2004-09-03</created><updated>2004-09-06</updated><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>The Password Change Phase is Still Insecure</title><categories>cs.CR</categories><comments>10 Pages, This work is related to the remote user authentication
  scheme with smart cards</comments><acm-class>C.3, Cryptography and network security</acm-class><abstract>  In 2004, W. C. Ku and S. M. Chen proposed an efficient remote user
authentication scheme using smart cards to solve the security problems of Chien
et al.'s scheme. Recently, Hsu and Yoon et al. pointed out the security
weaknesses of the Ku and Chen's scheme Furthermore, Yoon et al. also proposed a
new efficient remote user authentication scheme using smart cards. Yoon et al.
also modified the password change phase of Ku and Chen's scheme. This paper
analyzes that password change phase of Yoon et al's modified scheme is still
insecure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409005</id><created>2004-09-03</created><authors><author><keyname>Slagell</keyname><forenames>Adam J.</forenames></author><author><keyname>Yurcik</keyname><forenames>William</forenames></author></authors><title>Sharing Computer Network Logs for Security and Privacy: A Motivation for
  New Methodologies of Anonymization</title><categories>cs.CR</categories><comments>17 pages, 1 figure</comments><acm-class>K.6.5</acm-class><abstract>  Logs are one of the most fundamental resources to any security professional.
It is widely recognized by the government and industry that it is both
beneficial and desirable to share logs for the purpose of security research.
However, the sharing is not happening or not to the degree or magnitude that is
desired. Organizations are reluctant to share logs because of the risk of
exposing sensitive information to potential attackers. We believe this
reluctance remains high because current anonymization techniques are weak and
one-size-fits-all--or better put, one size tries to fit all. We must develop
standards and make anonymization available at varying levels, striking a
balance between privacy and utility. Organizations have different needs and
trust other organizations to different degrees. They must be able to map
multiple anonymization levels with defined risks to the trust levels they share
with (would-be) receivers. It is not until there are industry standards for
multiple levels of anonymization that we will be able to move forward and
achieve the goal of widespread sharing of logs for security researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409006</id><created>2004-09-04</created><authors><author><keyname>Vulcanov</keyname><forenames>Dumitru N.</forenames><affiliation>The West University of Timisoara, Romania</affiliation></author><author><keyname>Vulcanov</keyname><forenames>Valentina D.</forenames><affiliation>The West University of Timisoara, Romania</affiliation></author></authors><title>Maple+GrTensorII libraries for cosmology</title><categories>cs.SC gr-qc</categories><comments>LaTeX LLNCS style, 8 pages, accepted for SYNASC 2004 - 6th
  International Symposium on Symbolic and Numeric Algorithms for Scientific
  Computing, Timisoara, Romania, September 26-30 2004</comments><acm-class>I.1; J.2</acm-class><abstract>  The article mainly presents some results in using MAPLE platform for computer
algebra and GrTensorII package in doing calculations for theoretical and
numerical cosmology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409007</id><created>2004-09-06</created><authors><author><keyname>Dezert</keyname><forenames>Jean</forenames></author><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Daniel</keyname><forenames>Milan</forenames></author></authors><title>The Generalized Pignistic Transformation</title><categories>cs.AI</categories><comments>8 pages, 3 graphs, many tables. The Seventh International Conference
  on Information Fusion, Stockholm, Sweden, 28 June - 1 July 2004</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of the Seventh International Conference on Information
  Fusion, International Society for Information Fusion, Stockholm, Sweden,
  384-391, 2004</journal-ref><abstract>  This paper presents in detail the generalized pignistic transformation (GPT)
succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a
tool for decision process. The GPT allows to provide a subjective probability
measure from any generalized basic belief assignment given by any corpus of
evidence. We mainly focus our presentation on the 3D case and provide the
complete result obtained by the GPT and its validation drawn from the
probability theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409008</id><created>2004-09-07</created><authors><author><keyname>Cyrus</keyname><forenames>Lea</forenames></author><author><keyname>Feddes</keyname><forenames>Hendrik</forenames></author></authors><title>A Model for Fine-Grained Alignment of Multilingual Texts</title><categories>cs.CL</categories><comments>8 pages, 4 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proc. COLING 2004 Workshop on Multilingual Linguistic Resources
  (MLR2004), Geneva, August 28, 2004, pp. 15-22</journal-ref><abstract>  While alignment of texts on the sentential level is often seen as being too
coarse, and word alignment as being too fine-grained, bi- or multilingual texts
which are aligned on a level in-between are a useful resource for many
purposes. Starting from a number of examples of non-literal translations, which
tend to make alignment difficult, we describe an alignment model which copes
with these cases by explicitly coding them. The model is based on
predicate-argument structures and thus covers the middle ground between
sentence and word alignment. The model is currently used in a recently
initiated project of a parallel English-German treebank (FuSe), which can in
principle be extended with additional languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409009</id><created>2004-09-07</created><authors><author><keyname>Beyer</keyname><forenames>Dirk</forenames><affiliation>University of California, Berkeley</affiliation></author><author><keyname>Noack</keyname><forenames>Andreas</forenames><affiliation>Brandenburg University of Technology</affiliation></author></authors><title>CrocoPat 2.1 Introduction and Reference Manual</title><categories>cs.PL cs.DM cs.DS cs.SE</categories><comments>19 pages + cover, 2 eps figures, uses llncs.cls and
  cs_techrpt_cover.sty, for downloading the source code, binaries, and RML
  examples, see http://www.software-systemtechnik.de/CrocoPat/</comments><report-no>UCB//CSD-04-1338</report-no><acm-class>D.1.6; G.2.2.a; E.1.d; D.2.7m</acm-class><abstract>  CrocoPat is an efficient, powerful and easy-to-use tool for manipulating
relations of arbitrary arity, including directed graphs. This manual provides
an introduction to and a reference for CrocoPat and its programming language
RML. It includes several application examples, in particular from the analysis
of structural models of software systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409010</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409010</id><created>2004-09-07</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Zemor</keyname><forenames>Gilles</forenames></author></authors><title>Distance properties of expander codes</title><categories>cs.IT cs.DM math.IT</categories><comments>19 pages, 7 figures</comments><abstract>  We study the minimum distance of codes defined on bipartite graphs. Weight
spectrum and the minimum distance of a random ensemble of such codes are
computed. It is shown that if the vertex codes have minimum distance $\ge 3$,
the overall code is asymptotically good, and sometimes meets the
Gilbert-Varshamov bound.
  Constructive families of expander codes are presented whose minimum distance
asymptotically exceeds the product bound for all code rates between 0 and 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409011</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409011</id><created>2004-09-07</created><updated>2004-09-26</updated><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>Shannon meets Wiener II: On MMSE estimation in successive decoding
  schemes</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures. To appear in Proc. 2004 Allerton Conf.
  (Monticello, IL), Sept. 2004; final version</comments><acm-class>E.4</acm-class><abstract>  We continue to discuss why MMSE estimation arises in coding schemes that
approach the capacity of linear Gaussian channels. Here we consider schemes
that involve successive decoding, such as decision-feedback equalization or
successive cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409012</id><created>2004-09-07</created><updated>2005-10-30</updated><authors><author><keyname>Maneva</keyname><forenames>Eliza N.</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>A New Look at Survey Propagation and its Generalizations</title><categories>cs.CC</categories><comments>v2:typoes and reference corrections; v3: expanded exposition</comments><abstract>  This paper provides a new conceptual perspective on survey propagation, which
is an iterative algorithm recently introduced by the statistical physics
community that is very effective in solving random k-SAT problems even with
densities close to the satisfiability threshold. We first describe how any SAT
formula can be associated with a novel family of Markov random fields (MRFs),
parameterized by a real number \rho \in [0,1]. We then show that applying
belief propagation--a well-known ``message-passing'' technique for estimating
marginal probabilities--to this family of MRFs recovers a known family of
algorithms, ranging from pure survey propagation at one extreme (\rho = 1) to
standard belief propagation on the uniform distribution over SAT assignments at
the other extreme (\rho = 0). Configurations in these MRFs have a natural
interpretation as partial satisfiability assignments, on which a partial order
can be defined. We isolate cores as minimal elements in this partial ordering,
which are also fixed points of survey propagation and the only assignments with
positive probability in the MRF for \rho=1. Our experimental results for k=3
suggest that solutions of random formulas typically do not possess non-trivial
cores. This makes it necessary to study the structure of the space of partial
assignments for \rho&lt;1 and investigate the role of assignments that are very
close to being cores. To that end, we investigate the associated lattice
structure, and prove a weight-preserving identity that shows how any MRF with
\rho&gt;0 can be viewed as a ``smoothed'' version of the uniform distribution over
satisfying assignments (\rho=0). Finally, we isolate properties of Gibbs
sampling and message-passing algorithms that are typical for an ensemble of
k-SAT problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409013</id><created>2004-09-08</created><authors><author><keyname>Lin</keyname><forenames>Ching-Chi</forenames></author><author><keyname>Chang</keyname><forenames>Gerard J.</forenames></author><author><keyname>Chen</keyname><forenames>Gen-Huey</forenames></author></authors><title>Locally connected spanning trees on graphs</title><categories>cs.DS cs.DM</categories><comments>14 pages, 3 figures</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  A locally connected spanning tree of a graph $G$ is a spanning tree $T$ of
$G$ such that the set of all neighbors of $v$ in $T$ induces a connected
subgraph of $G$ for every $v\in V(G)$. The purpose of this paper is to give
linear-time algorithms for finding locally connected spanning trees on strongly
chordal graphs and proper circular-arc graphs, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409014</id><created>2004-09-08</created><updated>2004-11-02</updated><authors><author><keyname>lal</keyname><forenames>Sunder</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>A Digital Signature with Threshold Generation and Verification</title><categories>cs.CR</categories><comments>10 Pages</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  This paper proposes a signature scheme where the signatures are generated by
the cooperation of a number of people from a given group of senders and the
signatures are verified by a certain number of people from the group of
recipients. Shamir's threshold scheme and Schnorr's signature scheme are used
to realize the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409015</id><created>2004-09-08</created><authors><author><keyname>Cook</keyname><forenames>Stephen</forenames></author><author><keyname>Thapen</keyname><forenames>Neil</forenames></author></authors><title>The strength of replacement in weak arithmetic</title><categories>cs.LO cs.CC</categories><abstract>  The replacement (or collection or choice) axiom scheme asserts bounded
quantifier exchange. We prove the independence of this scheme from various weak
theories of arithmetic, sometimes under a complexity assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409016</id><created>2004-09-08</created><authors><author><keyname>Lugovsky</keyname><forenames>V. S.</forenames></author></authors><title>Using a hierarchy of Domain Specific Languages in complex software
  systems design</title><categories>cs.PL cs.DS cs.SE</categories><comments>8 pages, 1 figure</comments><acm-class>D.1.1;I.2.2;D.3.2;D.2.10</acm-class><abstract>  A new design methodology is introduced, with some examples on building Domain
Specific Languages hierarchy on top of Scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409017</id><created>2004-09-08</created><updated>2005-11-30</updated><authors><author><keyname>Zeng</keyname><forenames>Jianyang</forenames></author><author><keyname>Hsu</keyname><forenames>Wen-Jing</forenames></author><author><keyname>Wang</keyname><forenames>Jiangdian</forenames></author></authors><title>Near Optimal Routing for Small-World Networks with Augmented Local
  Awareness</title><categories>cs.DM cs.DC cs.DS</categories><comments>16 pages, 1 table and 3 figures. Experimental results are added</comments><abstract>  In order to investigate the routing aspects of small-world networks,
Kleinberg proposes a network model based on a $d$-dimensional lattice with
long-range links chosen at random according to the $d$-harmonic distribution.
Kleinberg shows that the greedy routing algorithm by using only local
information performs in $O(\log^2 n)$ expected number of hops, where $n$
denotes the number of nodes in the network. Martel and Nguyen have found that
the expected diameter of Kleinberg's small-world networks is $\Theta(\log n)$.
Thus a question arises naturally: Can we improve the routing algorithms to
match the diameter of the networks while keeping the amount of information
stored on each node as small as possible? We extend Kleinberg's model and add
three augmented local links for each node: two of which are connected to nodes
chosen randomly and uniformly within $\log^2 n$ Mahattan distance, and the
third one is connected to a node chosen randomly and uniformly within $\log n$
Mahattan distance. We show that if each node is aware of $O(\log n)$ number of
neighbors via the augmented local links, there exist both non-oblivious and
oblivious algorithms that can route messages between any pair of nodes in
$O(\log n \log \log n)$ expected number of hops, which is a near optimal
routing complexity and outperforms the other related results for routing in
Kleinberg's small-world networks. Our schemes keep only $O(\log^2 n)$ bits of
routing information on each node, thus they are scalable with the network size.
Besides adding new light to the studies of social networks, our results may
also find applications in the design of large-scale distributed networks, such
as peer-to-peer systems, in the same spirit of Symphony.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409018</id><created>2004-09-09</created><updated>2005-11-09</updated><authors><author><keyname>Slagell</keyname><forenames>Adam J</forenames></author><author><keyname>Bonilla</keyname><forenames>Rafael</forenames></author></authors><title>PKI Scalability Issues</title><categories>cs.CR</categories><comments>23 pages, 2 figures</comments><acm-class>E.3</acm-class><abstract>  This report surveys different PKI technologies such as PKIX and SPKI and the
issues of PKI that affect scalability. Much focus is spent on certificate
revocation methodologies and status verification systems such as CRLs,
Delta-CRLs, CRS, Certificate Revocation Trees, Windowed Certificate Revocation,
OCSP, SCVP and DVCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409019</id><created>2004-09-09</created><updated>2005-10-13</updated><authors><author><keyname>Angiulli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>Outlier Detection by Logic Programming</title><categories>cs.AI cs.LO</categories><abstract>  The development of effective knowledge discovery techniques has become in the
recent few years a very active research area due to the important impact it has
in several relevant application areas. One interesting task thereof is that of
singling out anomalous individuals from a given population, e.g., to detect
rare events in time-series analysis settings, or to identify objects whose
behavior is deviant w.r.t. a codified standard set of &quot;social&quot; rules. Such
exceptional individuals are usually referred to as outliers in the literature.
  Recently, outlier detection has also emerged as a relevant KR&amp;R problem. In
this paper, we formally state the concept of outliers by generalizing in
several respects an approach recently proposed in the context of default logic,
for instance, by having outliers not being restricted to single individuals
but, rather, in the more general case, to correspond to entire (sub)theories.
We do that within the context of logic programming and, mainly through
examples, we discuss its potential practical impact in applications. The
formalization we propose is a novel one and helps in shedding some light on the
real nature of outliers. Moreover, as a major contribution of this work, we
illustrate the exploitation of minimality criteria in outlier detection. The
computational complexity of outlier detection problems arising in this novel
setting is thoroughly investigated and accounted for in the paper as well.
Finally, we also propose a rewriting algorithm that transforms any outlier
detection problem into an equivalent inference problem under the stable model
semantics, thereby making outlier computation effective and realizable on top
of any stable model solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409020</id><created>2004-09-11</created><authors><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>He</keyname><forenames>Yuanchun</forenames></author><author><keyname>Sunderraman</keyname><forenames>Rajshekhar</forenames></author></authors><title>A Generalized Disjunctive Paraconsistent Data Model for Negative and
  Disjunctive Information</title><categories>cs.DB</categories><comments>12 pages</comments><abstract>  This paper presents a generalization of the disjunctive paraconsistent
relational data model in which disjunctive positive and negative information
can be represented explicitly and manipulated. There are situations where the
closed world assumption to infer negative facts is not valid or undesirable and
there is a need to represent and reason with negation explicitly. We consider
explicit disjunctive negation in the context of disjunctive databases as there
is an interesting interplay between these two types of information. Generalized
disjunctive paraconsistent relation is introduced as the main structure in this
model. The relational algebra is appropriately generalized to work on
generalized disjunctive paraconsistent relations and their correctness is
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409021</id><created>2004-09-11</created><authors><author><keyname>Hinze-Hoare</keyname><forenames>Vita</forenames></author></authors><title>Should Cyberspace Chat Rooms be closed to protect Children?</title><categories>cs.CY</categories><abstract>  The explosion of people networking in cyberspace, disseminating terabytes of
information, is being promoted through the use of broadband, bluetooth
technology, and wireless mobile computing facilities. New communities within
such venues as virtual chat rooms discussion groups, newsgroups etc are being
created daily and even hourly. This is raising issues of cyberethics concerning
privacy,security, crime, human needs, e-business, e-healthcare, e-government
and intellectual property among others that need to be evaluated and reflected
upon. With this new freedom come new moral and ethical responsibilities, which
raise questions as to whether anything can be published or whether there should
be restrictions. This paper addresses one specific area, that has come into the
public eye, the closure by Microsoft of all of its free chat rooms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409022</id><created>2004-09-13</created><authors><author><keyname>Frommer</keyname><forenames>Ian</forenames></author><author><keyname>Harder</keyname><forenames>Eric</forenames></author><author><keyname>Hunt</keyname><forenames>Brian</forenames></author><author><keyname>Lance</keyname><forenames>Ryan</forenames></author><author><keyname>Ott</keyname><forenames>Edward</forenames></author><author><keyname>Yorke</keyname><forenames>James</forenames></author></authors><title>Two Models for the Study of Congested Internet Connections</title><categories>cs.NI</categories><comments>10 pages, 9 figures, extended version of CCN '04 Paper</comments><abstract>  In this paper, we introduce two deterministic models aimed at capturing the
dynamics of congested Internet connections. The first model is a
continuous-time model that combines a system of differential equations with a
sudden change in one of the state variables. The second model is a
discrete-time model with a time step that arises naturally from the system.
Results from these models show good agreement with the well-known ns network
simulator, better than the results of a previous, similar model. This is due in
large part to the use of the sudden change to reflect the impact of lost data
packets. We also discuss the potential use of this model in network traffic
state estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409023</id><created>2004-09-12</created><updated>2004-10-25</updated><authors><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Proximity Inversion Functions on the Non-Negative Integers</title><categories>cs.DM</categories><comments>20 pages, Latex; v2: Introduction expanded, Proof of main theorem
  rewritten for clarity, formatting changed</comments><acm-class>G.2</acm-class><abstract>  We consider functions mapping non-negative integers to non-negative real
numbers such that a and a+n are mapped to values at least 1/n apart. In this
paper we use a novel method to construct such a function. We conjecture that
the supremum of the generated function is optimal and pose some unsolved
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409024</id><created>2004-09-13</created><updated>2005-08-29</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Aperiodic Tilings: Breaking Translational Symmetry</title><categories>cs.DM cs.DC</categories><comments>4 pages, 2 figures, minor changes</comments><acm-class>F.1.1; G.2.1</acm-class><abstract>  Classical results on aperiodic tilings are rather complicated and not widely
understood. Below, an alternative approach is discussed in hope to provide
additional intuition not apparent in classical works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409025</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409025</id><created>2004-09-13</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Topics in asynchronous systems</title><categories>cs.AR</categories><comments>40 pages</comments><acm-class>B.0</acm-class><journal-ref>Analele Universitatii din Oradea, Fascicola Matematica, TOM X,
  2003</journal-ref><abstract>  In the paper we define and characterize the asynchronous systems from the
point of view of their autonomy, determinism, order, non-anticipation, time
invariance, symmetry, stability and other important properties. The study is
inspired by the models of the asynchronous circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409026</identifier>
 <datestamp>2009-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409026</id><created>2004-09-13</created><authors><author><keyname>Pfister</keyname><forenames>H.</forenames></author><author><keyname>Sason</keyname><forenames>I.</forenames></author><author><keyname>Urbanke</keyname><forenames>R.</forenames></author></authors><title>Capacity-achieving ensembles for the binary erasure channel with bounded
  complexity</title><categories>cs.IT math.IT</categories><comments>47 pages, 9 figures. Submitted to IEEE Transactions on Information
  Theory</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 51 (7), pp.
  2352-2379, July 2005</journal-ref><doi>10.1109/TIT.2005.850079</doi><abstract>  We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel and is refined for the BEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409027</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409027</id><created>2004-09-14</created><authors><author><keyname>Pfister</keyname><forenames>H.</forenames></author><author><keyname>Sason</keyname><forenames>I.</forenames></author><author><keyname>Urbanke</keyname><forenames>R.</forenames></author></authors><title>Bounds on the decoding complexity of punctured codes on graphs</title><categories>cs.IT math.IT</categories><comments>11 pages, 3 figures. To appear in the Proc. Forty-Second Annual
  Allerton Conference on Communication, Control and Computing,
  Urbana-Champaign, IL, USA, September 29-October 1, 2004</comments><abstract>  We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel, and is refined for the BEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409028</id><created>2004-09-14</created><updated>2005-06-17</updated><authors><author><keyname>Schmidt</keyname><forenames>Andreas U.</forenames></author></authors><title>Incentive Systems in Multi-Level Markets for Virtual Goods</title><categories>cs.GT cs.CY</categories><comments>18 pages, 5 figures; graphics with reduced resolution. Full
  resolution available on author's homepage. Accepted contribution to the
  Workshop 'Virtual Goods' at the Conference AXMEDIS 2005, 30. November - 2.
  December, Florence, Italy</comments><acm-class>K.4.4</acm-class><abstract>  As an alternative to rigid DRM measures, ways of marketing virtual goods
through multi-level or networked marketing have raised some interest. This
report is a first approach to multi-level markets for virtual goods from the
viewpoint of theoretical economy. A generic, kinematic model for the monetary
flow in multi-level markets, which quantitatively describes the incentives that
buyers receive through resales revenues, is devised. Building on it, the
competition of goods is examined in a dynamical, utility-theoretic model
enabling, in particular, a treatment of the free-rider problem. The most
important implications for the design of multi-level market mechanisms for
virtual goods, or multi-level incentive management systems, are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409029</identifier>
 <datestamp>2008-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409029</id><created>2004-09-14</created><updated>2008-12-09</updated><authors><author><keyname>Dubrois</keyname><forenames>Jacques</forenames><affiliation>Axalto</affiliation></author><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author></authors><title>Efficient polynomial time algorithms computing industrial-strength
  primitive roots</title><categories>cs.SC math.NT</categories><proxy>ccsd ccsd-00002828</proxy><journal-ref>Information Processing Letters 97, 2 (2006) 41-45</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E. Bach, following an idea of T. Itoh, has shown how to build a small set of
numbers modulo a prime p such that at least one element of this set is a
generator of $\pF{p}$\cite{Bach:1997:sppr,Itoh:2001:PPR}. E. Bach suggests also
that at least half of his set should be generators. We show here that a slight
variant of this set can indeed be made to contain a ratio of primitive roots as
close to 1 as necessary. We thus derive several algorithms computing primitive
roots correct with very high probability in polynomial time. In particular we
present an asymptotically $O^{\sim}(\sqrt{\frac{1}{\epsilon}}log^1.5(p) +
\log^2(p))$ algorithm providing primitive roots of $p$ with probability of
correctness greater than $1-\epsilon$ and several $O(log^\alpha(p))$, $\alpha
\leq 5.23$ algorithms computing &quot;Industrial-strength&quot; primitive roots with
probabilities e.g. greater than the probability of &quot;hardware malfunctions&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409030</id><created>2004-09-14</created><authors><author><keyname>Abdennadher</keyname><forenames>Slim</forenames></author><author><keyname>Rigotti</keyname><forenames>Christophe</forenames></author></authors><title>Automatic Generation of CHR Constraint Solvers</title><categories>cs.LO cs.PL</categories><comments>to be published in Theory and Practice of Logic Programming, 16
  pages, 2 figures</comments><acm-class>D.3.2; I.2.2</acm-class><abstract>  In this paper, we present a framework for automatic generation of CHR solvers
given the logical specification of the constraints. This approach takes
advantage of the power of tabled resolution for constraint logic programming,
in order to check the validity of the rules. Compared to previous works where
different methods for automatic generation of constraint solvers have been
proposed, our approach enables the generation of more expressive rules (even
recursive and splitting rules) that can be used directly as CHR solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409031</id><created>2004-09-15</created><authors><author><keyname>McGuire</keyname><forenames>Patrick C.</forenames></author><author><keyname>Gomez-Elvira</keyname><forenames>Javier</forenames></author><author><keyname>Rodriguez-Manfredi</keyname><forenames>Jose Antonio</forenames></author><author><keyname>Sebastian-Martinez</keyname><forenames>Eduardo</forenames></author><author><keyname>Ormo</keyname><forenames>Jens</forenames></author><author><keyname>Diaz-Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Ritter</keyname><forenames>Helge</forenames></author><author><keyname>Oesker</keyname><forenames>Markus</forenames></author><author><keyname>Haschke</keyname><forenames>Robert</forenames></author><author><keyname>Ontrup</keyname><forenames>Joerg</forenames></author></authors><title>Field Geology with a Wearable Computer: First Results of the Cyborg
  Astrobiologist System</title><categories>cs.CV astro-ph cs.RO</categories><comments>7 pages, 6 figures. Submitted to ICRA'2005 (Int'l Conf. on Robotics &amp;
  Automation, IEEE), April 18-22, 2005, Barcelona, Spain</comments><acm-class>I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9</acm-class><abstract>  We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist. The Cyborg
Astrobiologist platform has thus far been used for testing and development of
these algorithms and systems: robotic acquisition of quasi-mosaics of images,
real-time image segmentation, and real-time determination of interesting points
in the image mosaics. The hardware and software systems function reliably, and
the computer-vision algorithms are adequate for the first field tests. In
addition to the proof-of-concept aspect of these field tests, the main result
of these field tests is the enumeration of those issues that we can improve in
the future, including: dealing with structural shadow and microtexture, and
also, controlling the camera's zoom lens in an intelligent manner. Nonetheless,
despite these and other technical inadequacies, this Cyborg Astrobiologist
system, consisting of a camera-equipped wearable-computer and its
computer-vision algorithms, has demonstrated its ability of finding genuinely
interesting points in real-time in the geological scenery, and then gathering
more information about these interest points in an automated manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409032</id><created>2004-09-16</created><authors><author><keyname>Kolakowska</keyname><forenames>A.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author></authors><title>Desynchronization and Speedup in an Asynchronous Conservative Parallel
  Update Protocol</title><categories>cs.DC cond-mat.mtrl-sci physics.comp-ph</categories><comments>Invited chapter in &quot;Progress in Computer Science Research&quot;, Nova
  Science Publishers; 34 pages, 12 figures, 32 references</comments><acm-class>D.1.3; F.2.0</acm-class><journal-ref>Ch.6 in &quot;Artificial Inteligence and Computer Science&quot; ed by S.
  Shannon, pp.151-176 (2005 Nova Science Piblishers, Inc., New York) ISBN
  1-59454-411-5</journal-ref><abstract>  In a state-update protocol for a system of $L$ asynchronous parallel
processes that communicate only with nearest neighbors, global
desynchronization in operation times can be deduced from kinetic roughening of
the corresponding virtual-time horizon (VTH). The utilization of the parallel
processing environment can be deduced by analyzing the microscopic structure of
the VTH. We give an overview of how the methods of non-equilibrium surface
growth (physics of complex systems) can be applied to uncover some properties
of state update algorithms used in distributed parallel discrete-event
simulations (PDES). In particular, we focus on the asynchronous conservative
PDES algorithm in a ring communication topology. The time evolution of its VTH
is simulated numerically as asynchronous cellular automaton whose update rule
corresponds to the update rule followed by this algorithm. We give theoretical
estimates of the performance as a function of $L$ and the load per processor,
i.e., approximate formulas for the mean speedup and for the desynchronization.
It is established that, for a given simulation size, there is a theoretical
upper bound for the desynchronization and a theoretical non-zero lower bound
for the utilization. The new approach to performance studies, outlined in this
chapter, is particularly useful in the search for the design of a
new-generation of algorithms that would efficiently carry out an autonomous or
tunable synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409033</identifier>
 <datestamp>2009-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409033</id><created>2004-09-17</created><updated>2009-07-29</updated><authors><author><keyname>Suslo</keyname><forenames>Tomasz</forenames></author></authors><title>Mean and Variance Estimation by Kriging</title><categories>cs.NA cs.MS</categories><comments>3 pages, 1 figure, source code (combo.pas) and input file (inp.dat)
  attached</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to derive the numerical least-squares estimator for
mean and variance of random variable. In order to do so the following questions
have to be answered: (i) what is the statistical model for the estimation
procedure? (ii) what are the properties of the estimator, like optimality (in
which class) or asymptotic properties? (iii) how does the estimator work in
practice, how compared to competing estimators?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409034</id><created>2004-09-17</created><authors><author><keyname>Stanton</keyname><forenames>Paul</forenames></author></authors><title>Securing Data in Storage: A Review of Current Research</title><categories>cs.OS cs.CR</categories><comments>22 pages, 4 figures, 3 tables</comments><acm-class>D.4.3;D.4.6;K.6.5</acm-class><abstract>  Protecting data from malicious computer users continues to grow in
importance. Whether preventing unauthorized access to personal photographs,
ensuring compliance with federal regulations, or ensuring the integrity of
corporate secrets, all applications require increased security to protect data
from talented intruders. Specifically, as more and more files are preserved on
disk the requirement to provide secure storage has increased in importance.
This paper presents a survey of techniques for securely storing data, including
theoretical approaches, prototype systems, and existing systems currently
available. Due to the wide variety of potential solutions available and the
variety of techniques to arrive at a particular solution, it is important to
review the entire field prior to selecting an implementation that satisfies
particular requirements. This paper provides an overview of the prominent
characteristics of several systems to provide a foundation for making an
informed decision. Initially, the paper establishes a set of criteria for
evaluating a storage solution based on confidentiality, integrity,
availability, and performance. Then, using these criteria, the paper explains
the relevant characteristics of select storage systems and provides a
comparison of the major differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409035</id><created>2004-09-18</created><authors><author><keyname>Lu</keyname><forenames>Ning</forenames></author><author><keyname>Taylor</keyname><forenames>Z. Todd</forenames></author><author><keyname>Chassin</keyname><forenames>David P.</forenames></author><author><keyname>Guttromson</keyname><forenames>Ross T.</forenames></author><author><keyname>Studham</keyname><forenames>R. Scott</forenames></author></authors><title>Parallel Computing Environments and Methods for Power Distribution
  System Simulation</title><categories>cs.DC cs.CE cs.MA cs.PF</categories><comments>7 pages, 4 figures, 6 tables, submitted to HICSS-38</comments><abstract>  The development of cost-effective highperformance parallel computing on
multi-processor supercomputers makes it attractive to port excessively time
consuming simulation software from personal computers (PC) to super computes.
The power distribution system simulator (PDSS) takes a bottom-up approach and
simulates load at the appliance level, where detailed thermal models for
appliances are used. This approach works well for a small power distribution
system consisting of a few thousand appliances. When the number of appliances
increases, the simulation uses up the PC memory and its runtime increases to a
point where the approach is no longer feasible to model a practical large power
distribution system. This paper presents an effort made to port a PC-based
power distribution system simulator to a 128-processor shared-memory
supercomputer. The paper offers an overview of the parallel computing
environment and a description of the modification made to the PDSS model. The
performance of the PDSS running on a standalone PC and on the supercomputer is
compared. Future research direction of utilizing parallel computing in the
power distribution system simulation is also addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409036</id><created>2004-09-18</created><authors><author><keyname>Lal</keyname><forenames>Sunder</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>A Directed Signature Scheme and its Applications</title><categories>cs.CR</categories><comments>9 Pages, No figures. Presented in the National Conference on
  Information Security, New Delhi- India, Jan-2003</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  This paper presents a directed signature scheme with the property that the
signature can be verified only with the help of signer or signature receiver.
We also propose its applications to share verification of signatures and to
threshold cryptosystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409037</id><created>2004-09-19</created><authors><author><keyname>Dumay</keyname><forenames>Mark</forenames></author></authors><title>Business Processes: The Theoretical Impact of Process Thinking on
  Information Systems Development</title><categories>cs.OH</categories><comments>22 pages</comments><abstract>  This paper investigates two aspects of process thinking that affect the
success rate of IT projects. These two aspects are the changes in the structure
of organizations and the epistemology of Information Systems Development.
Firstly, the conception of business processes within the management of
organizations increases the structural complexity of Information Systems,
because existing systems have to be integrated into a coherent cross-functional
architecture. Secondly, process thinking leads to a particular view of
organizations that ultimately has a negative effect on the support of
Information Systems. As an illustration of process thinking, the Business
Process Reengineering movement adheres to a technocratic management perspective
of organizations. Particularly this conception of organization views people as
mechanisms to realize certain organizational goals. As a result of this view
stakeholders are confronted with the implemented systems, rather than consulted
about the scope and functionality of those systems. Therefore, both aspects of
process thinking have a negative impact on the success of IT projects. The
problem of structural complexity is an area that is addressed by Enterprise
Application Integration, and mainly requires technical solutions. However, the
problems associated with the conception of organization require a different,
markedly non-technical, perspective. Several directions are discussed to
overcome some limitations of process thinking, but these directions are merely
small pointers. If truly effective and useful Information Systems are to be
acquired, IT practitioners and scientists require a completely different
mindset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409038</identifier>
 <datestamp>2010-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409038</id><created>2004-09-21</created><authors><author><keyname>de la Banda</keyname><forenames>Maria Garcia</forenames></author><author><keyname>Harvey</keyname><forenames>Warwick</forenames></author><author><keyname>Marriott</keyname><forenames>Kim</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author><author><keyname>Demoen</keyname><forenames>Bart</forenames></author></authors><title>Checking modes of HAL programs</title><categories>cs.PL</categories><comments>46 pages, 3 figures To appear in Theory and Practice of Logic
  Programming</comments><acm-class>D.3.2; F.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming: 5(6):623-668, 2005</journal-ref><abstract>  Recent constraint logic programming (CLP) languages, such as HAL and Mercury,
require type, mode and determinism declarations for predicates. This
information allows the generation of efficient target code and the detection of
many errors at compile-time. Unfortunately, mode checking in such languages is
difficult. One of the main reasons is that, for each predicate mode
declaration, the compiler is required to appropriately re-order literals in the
predicate's definition. The task is further complicated by the need to handle
complex instantiations (which interact with type declarations and higher-order
predicates) and automatic initialization of solver variables. Here we define
mode checking for strongly typed CLP languages which require reordering of
clause body literals. In addition, we show how to handle a simple case of
polymorphic modes by using the corresponding polymorphic types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409039</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409039</id><created>2004-09-21</created><updated>2014-12-10</updated><authors><author><keyname>Ionescu</keyname><forenames>Marius Constantin</forenames></author></authors><title>On Certain Modular Equations</title><categories>cs.CC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the MEoP problem that decides the existence of solutions to
certain modular equations over prime numbers and show how this separates the
complexity class NP from its subclass P
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409040</id><created>2004-09-22</created><updated>2004-10-29</updated><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Unification of Fusion Theories</title><categories>cs.AI</categories><comments>14 pages</comments><acm-class>I.2.3</acm-class><journal-ref>Presented at NATO Advanced Study Institute, Albena, Bulgaria,
  16-27 May 2005. International Journal of Applied Mathematics &amp; Statistics,
  Vol. 2, 1-14, 2004.</journal-ref><abstract>  Since no fusion theory neither rule fully satisfy all needed applications,
the author proposes a Unification of Fusion Theories and a combination of
fusion rules in solving problems/applications. For each particular application,
one selects the most appropriate model, rule(s), and algorithm of
implementation. We are working in the unification of the fusion theories and
rules, which looks like a cooking recipe, better we'd say like a logical chart
for a computer programmer, but we don't see another method to comprise/unify
all things. The unification scenario presented herein, which is now in an
incipient form, should periodically be updated incorporating new discoveries
from the fusion and engineering research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409041</id><created>2004-09-23</created><authors><author><keyname>Hinze-Hoare</keyname><forenames>Vita</forenames></author></authors><title>Four Principles Fundamental to Design Practice for Human Centred Systems</title><categories>cs.HC</categories><abstract>  A Survey of the principal literature on Human Centred Design reveals the four
most referenced principles. These are discussed with reference to the
application of a particular website, and a user survey is constructed based
upon the four principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409042</id><created>2004-09-23</created><authors><author><keyname>Fiti&#xe9;</keyname><forenames>Harry</forenames></author></authors><title>A new architecture for making highly scalable applications</title><categories>cs.HC cs.CL</categories><comments>13 pages, 7 figures, 5 tables</comments><abstract>  An application is a logical image of the world on a computer. A scalable
application is an application that allows one to update that logical image at
run time. To put it in operational terms: an application is scalable if a
client can change between time T1 and time T2 - the logic of the application as
expressed by language L;
  - the structure and volume of the stored knowledge;
  - the user interface of the application; while clients working with the
application at time T1 will work with the changed application at time T2
without performing any special action between T1 and T2. In order to realize
such a scalable application a new architecture has been developed that fully
orbits around language. In order to verify the soundness of that architecture a
program has been build. Both architecture and program are called CommunSENS.
The main purpose of this paper is: - to list the relevant elements of the
architecture; - to give a visual presentation of how the program and its image
of the world look like; - to give a visual presentation of how the image can be
updated. Some relevant philosophical and practical backgrounds are included in
the appendixes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409043</id><created>2004-09-23</created><authors><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Inapproximability of Combinatorial Optimization Problems</title><categories>cs.CC</categories><abstract>  We survey results on the hardness of approximating combinatorial optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409044</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409044</id><created>2004-09-23</created><authors><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Some Applications of Coding Theory in Computational Complexity</title><categories>cs.CC cs.IT math.IT</categories><abstract>  Error-correcting codes and related combinatorial constructs play an important
role in several recent (and old) results in computational complexity theory. In
this paper we survey results on locally-testable and locally-decodable
error-correcting codes, and their applications to complexity theory and to
cryptography.
  Locally decodable codes are error-correcting codes with sub-linear time
error-correcting algorithms. They are related to private information retrieval
(a type of cryptographic protocol), and they are used in average-case
complexity and to construct ``hard-core predicates'' for one-way permutations.
  Locally testable codes are error-correcting codes with sub-linear time
error-detection algorithms, and they are the combinatorial core of
probabilistically checkable proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409045</id><created>2004-09-24</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>Augmenting ALC(D) (atemporal) roles and (aspatial) concrete domain with
  temporal roles and a spatial concrete domain -first results</title><categories>cs.AI cs.LO</categories><comments>in Proceedings of the ECAI Workshop on Spatial and Temporal
  Reasoning, pp. 123-127, Valencia, Spain, 2004</comments><acm-class>I.2 (I.2.3; I.2.4; I.2.m)</acm-class><abstract>  We consider the well-known family ALC(D) of description logics with a
concrete domain, and provide first results on a framework obtained by
augmenting ALC(D) atemporal roles and aspatial concrete domain with temporal
roles and a spatial concrete domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409046</id><created>2004-09-24</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>A TCSP-like decidable constraint language generalising existing cardinal
  direction relations</title><categories>cs.AI cs.LO</categories><comments>in Proceedings of the ECAI Workshop on Spatial and Temporal
  Reasoning, pp. 135-139, Valencia, Spain, 2004</comments><acm-class>I.2 (I.2.3; I.2.4; I.2.m)</acm-class><abstract>  We define a quantitative constraint language subsuming two calculi well-known
in QSR (Qualitative Spatial Reasoning): Frank's cone-shaped and
projection-based calculi of cardinal direction relations. We show how to solve
a CSP (Constraint Satisfaction Problem) expressed in the language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409047</id><created>2004-09-24</created><authors><author><keyname>Isli</keyname><forenames>Amar</forenames></author></authors><title>An ALC(D)-based combination of temporal constraints and spatial
  constraints suitable for continuous (spatial) change</title><categories>cs.AI cs.LO</categories><comments>in Proceedings of the ECAI Workshop on Spatial and Temporal
  Reasoning, pp. 129-133, Valencia, Spain, 2004</comments><acm-class>I.2 (I.2.3; I.2.4; I.2.m)</acm-class><abstract>  We present a family of spatio-temporal theories suitable for continuous
spatial change in general, and for continuous motion of spatial scenes in
particular. The family is obtained by spatio-temporalising the well-known
ALC(D) family of Description Logics (DLs) with a concrete domain D, as follows,
where TCSPs denotes &quot;Temporal Constraint Satisfaction Problems&quot;, a well-known
constraint-based framework:
 (1) temporalisation of the roles, so that they consist of TCSP constraints
(specifically, of an adaptation of TCSP constraints to interval variables); and
 (2) spatialisation of the concrete domain D: the concrete domain is now $D_x$,
and is generated by a spatial Relation Algebra (RA) $x$, in the style of the
Region-Connection Calculus RCC8.
  We assume durative truth (i.e., holding during a durative interval). We also
assume the homogeneity property (if a truth holds during a given interval, it
holds during all of its subintervals). Among other things, these assumptions
raise the &quot;conflicting&quot; problem of overlapping truths, which the work solves
with the use of a specific partition of the 13 atomic relations of Allen's
interval algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409048</identifier>
 <datestamp>2007-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409048</id><created>2004-09-27</created><authors><author><keyname>Tung</keyname><forenames>Michael M.</forenames><affiliation>Universidad Politecnica de Valencia</affiliation></author></authors><title>FORM Matters: Fast Symbolic Computation under UNIX</title><categories>cs.SC</categories><comments>10 pages, PDF document (PDFLaTeX source available upon request) with
  2 JPG figures; submitted to Computers &amp; Mathematics with Applications</comments><acm-class>I.1; H.5.2</acm-class><journal-ref>Comp. Math. Appl. 49 (2005) 1127-1137</journal-ref><abstract>  We give a brief introduction to FORM, a symbolic programming language for
massive batch operations, designed by J.A.M. Vermaseren. In particular, we
stress various methods to efficiently use FORM under the UNIX operating system.
Several scripts and examples are given, and suggestions on how to use the vim
editor as development platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409049</id><created>2004-09-25</created><updated>2004-11-02</updated><authors><author><keyname>lal</keyname><forenames>Sunder</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>A Directed -Threshold Multi-Signature Scheme</title><categories>cs.CR</categories><comments>12 pages, no figures</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  In this paper, we propose a Directed Threshold Multi-Signature Scheme. In
this threshold signature scheme, any malicious set of signers cannot
impersonate any other set of signers to forge the signatures. In case of
forgery, it is possible to trace the signing set. This threshold signature
scheme is applicable when the message is sensitive to the signature receiver;
and the signatures are generated by the cooperation of a number of people from
a given group of senders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409050</id><created>2004-09-26</created><authors><author><keyname>Lal</keyname><forenames>Sunder</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Some Applications of Directed Signature Scheme</title><categories>cs.CR</categories><comments>12 pages, no figures, this paper provides some application of
  directed signature scheme</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  Directed signature scheme is applicable when the signed message contains
information sensitive to the receiver, because only receiver can directly
verify the signature and that he/she can prove its validity to any third party,
whenever necessary. This paper presents two applications of directed signature
scheme. (i) Directed &amp;#8211;Delegated Signature Scheme. This scheme combines
the idea of proxy signatures with directed signature scheme. (ii) Allocation of
registration number. This scheme proposes a registration scheme in which the
registration number cannot be forged and misused.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409051</id><created>2004-09-26</created><authors><author><keyname>Tusarova</keyname><forenames>Tereza</forenames></author></authors><title>Quantum Complexity Classes</title><categories>cs.CC quant-ph</categories><comments>Master thesis from 2003. Comparing to the original version, here I
  corrected some typos and formal errors. I am now writing a short extract from
  this thesis</comments><report-no>IR-TI-001</report-no><abstract>  In our thesis, we try to shed more light onto the complexity of quantum
complexity classes by refining the related part of the hierarchy. First, we
review the basic concepts of quantum computing in general. Then, inspired by
BQP, we define new complexity classes. They are placed between BPP and PSPACE.
We show that they incorporate the current important quantum algorithms.
Furthermore, the importance of the unitarity constraint given by quantum
mechanics is revealed. Without this requirement, we naturally arrive at the
class AWPP, which was up to now thought to be just an artificially defined
class. We hope that some of our newly defined classes could find their use in
proving results about BQP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409052</id><created>2004-09-26</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames></author><author><keyname>Nylen</keyname><forenames>Aletta</forenames></author></authors><title>Better Quasi-Ordered Transition Systems</title><categories>cs.LO</categories><comments>30 pages, 6 figures</comments><abstract>  Many existing algorithms for model checking of infinite-state systems operate
on constraints which are used to represent (potentially infinite) sets of
states. A general powerful technique which can be employed for proving
termination of these algorithms is that of well quasi-orderings. Several
methodologies have been proposed for derivation of new well quasi-ordered
constraint systems. However, many of these constraint systems suffer from a
&quot;constraint explosion problem&quot;, as the number of the generated constraints
grows exponentially with the size of the problem. In this paper, we demonstrate
that a refinement of the theory of well quasi-orderings, called the theory of
better quasi-orderings, is more appropriate for symbolic model checking, since
it allows inventing constraint systems which are both well quasi-ordered and
compact. As a main application, we introduce existential zones, a constraint
system for verification of systems with unboundedly many clocks and use our
methodology to prove that existential zones are better quasi-ordered. We show
how to use existential zones in verification of timed Petri nets and present
some experimental results. Also, we apply our methodology to derive new
constraint systems for verification of broadcast protocols, lossy channel
systems, and integral relational automata. The new constraint systems are
exponentially more succinct than existing ones, and their well quasi-ordering
cannot be shown by previous methods in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409053</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409053</id><created>2004-09-26</created><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>On the role of MMSE estimation in approaching the information-theoretic
  limits of linear Gaussian channels: Shannon meets Wiener</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures</comments><acm-class>E.4</acm-class><journal-ref>Proc. 2003 Allerton Conf. (Monticello, IL), pp. 430-439, Oct. 2003</journal-ref><abstract>  We discuss why MMSE estimation arises in lattice-based schemes for
approaching the capacity of linear Gaussian channels, and comment on its
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409054</id><created>2004-09-26</created><authors><author><keyname>Roch</keyname><forenames>S.</forenames></author><author><keyname>Marcotte</keyname><forenames>P.</forenames></author><author><keyname>Savard</keyname><forenames>G.</forenames></author></authors><title>An Approximation Algorithm for Stackelberg Network Pricing</title><categories>cs.GT</categories><comments>38 pages</comments><abstract>  We consider the problem of maximizing the revenue raised from tolls set on
the arcs of a transportation network, under the constraint that users are
assigned to toll-compatible shortest paths. We first prove that this problem is
strongly NP-hard. We then provide a polynomial time algorithm with a worst-case
precision guarantee of ${1/2}\log_2 m_T+1$, where $m_T$ denotes the number of
toll arcs. Finally we show that the approximation is tight with respect to a
natural relaxation by constructing a family of instances for which the
relaxation gap is reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409055</id><created>2004-09-28</created><authors><author><keyname>Beler</keyname><forenames>Alpay</forenames></author><author><keyname>Borda</keyname><forenames>Ann</forenames></author><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author><author><keyname>Filippini-Fantoni</keyname><forenames>Silvia</forenames></author></authors><title>The Building of Online Communities: An approach for learning
  organizations, with a particular focus on the museum sector</title><categories>cs.CY cs.DL</categories><comments>(15 pages, 1 figure)</comments><acm-class>H3.5; H3.7; H4.3; H5.2; H5.3; K3.1; K.4.0</acm-class><journal-ref>In James Hemsley, Vito Cappellini and Gerd Stanke (eds.), EVA 2004
  London Conference Proceedings, University College London, The Institute of
  Archaeology, UK, 26-30 July 2004, pages 2.1-2.15</journal-ref><abstract>  This paper considers the move toward and potential of building online
communities, with a particular focus on the museum sector. For instance, the
increase in the use of `personalized' toolkits that are becoming an integral
part of the online presence for learning organizations, like museums, can
provide a basis for creating and sustaining communities. A set of case studies
further illustrates working examples of the ways in which personalization and
specific tools are developing collaborative spaces, community channels and
group interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409056</id><created>2004-09-29</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Using sparse matrices and splines-based interpolation in computational
  fluid dynamics simulations</title><categories>cs.NA cs.CE physics.comp-ph</categories><comments>Talk at SIMAI (Societa' Italiana di Matematica Applicata e
  Industriale) 2004 Congress, Venice, San Servolo Island, September 20-24, 2004</comments><acm-class>C.1.4; D.1.3; G.1.0</acm-class><abstract>  In this relation I present a technique of construction and fast evaluation of
a family of cubic polynomials for analytic smoothing and graphical rendering of
particles trajectories for flows in a generic geometry. The principal result of
the work was implementation and test of a method for interpolating 3D points by
regular parametric curves and their fast and efficient evaluation for a good
resolution of rendering. For the purpose a parallel environment using a
multiprocessor cluster architecture has been used. This work has been developed
for the Research and Development Department of my company for planning advanced
customized models of industrial burners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409057</id><created>2004-09-29</created><updated>2005-08-22</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author></authors><title>Fast Construction of Nets in Low Dimensional Metrics, and Their
  Applications</title><categories>cs.DS cs.CG</categories><comments>41 pages. Extensive clean-up of minor English errors</comments><journal-ref>SIAM J. Comput. 35(5):1148-1184, 2006</journal-ref><doi>10.1137/S0097539704446281</doi><abstract>  We present a near linear time algorithm for constructing hierarchical nets in
finite metric spaces with constant doubling dimension. This data-structure is
then applied to obtain improved algorithms for the following problems:
Approximate nearest neighbor search, well-separated pair decomposition, compact
representation scheme, doubling measure, and computation of the (approximate)
Lipschitz constant of a function. In all cases, the running (preprocessing)
time is near-linear and the space being used is linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409058</id><created>2004-09-29</created><authors><author><keyname>Pang</keyname><forenames>Bo</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>A Sentimental Education: Sentiment Analysis Using Subjectivity
  Summarization Based on Minimum Cuts</title><categories>cs.CL</categories><comments>Data available at
  http://www.cs.cornell.edu/people/pabo/movie-review-data/</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 42nd ACL, pp. 271--278, 2004</journal-ref><abstract>  Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;
an example application is classifying a movie review as &quot;thumbs up&quot; or &quot;thumbs
down&quot;. To determine this sentiment polarity, we propose a novel
machine-learning method that applies text-categorization techniques to just the
subjective portions of the document. Extracting these portions can be
implemented using efficient techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence contextual constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0409059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0409059</id><created>2004-09-30</created><authors><author><keyname>Hinze-Hoare</keyname><forenames>Vita</forenames></author></authors><title>From Digital Television to Internet?</title><categories>cs.MM cs.CY</categories><abstract>  This paper provides a general technical overview of the Multimedia Home
Platform (MHP) specifications. MHP is a generic interface between digital
applications and user machines, whether they happen to be set top boxes,
digital TV sets or Multimedia PC's. MHP extends the DVB open standards.
Addressed are MHP architexture, System core and MHP Profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410001</id><created>2004-10-01</created><updated>2004-10-02</updated><authors><author><keyname>Jensen</keyname><forenames>C. S.</forenames></author><author><keyname>Lahrmann</keyname><forenames>H.</forenames></author><author><keyname>Pakalnis</keyname><forenames>S.</forenames></author><author><keyname>Runge</keyname><forenames>J.</forenames></author></authors><title>The Infati Data</title><categories>cs.DB</categories><report-no>TR-79</report-no><acm-class>H.2.8</acm-class><abstract>  The ability to perform meaningful empirical studies is of essence in research
in spatio-temporal query processing. Such studies are often necessary to gain
detailed insight into the functional and performance characteristics of
proposals for new query processing techniques.
  We present a collection of spatio-temporal data, collected during an
intelligent speed adaptation project, termed INFATI, in which some two dozen
cars equipped with GPS receivers and logging equipment took part. We describe
how the data was collected and how it was &quot;modified&quot; to afford the drivers some
degree of anonymity.
  We also present the road network in which the cars were moving during data
collection.
  The GPS data is publicly available for non-commercial purposes. It is our
hope that this resource will help the spatio-temporal research community in its
efforts to develop new and better query processing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410002</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410002</id><created>2004-10-01</created><authors><author><keyname>Grunwald</keyname><forenames>Peter</forenames><affiliation>CWI</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Shannon Information and Kolmogorov Complexity</title><categories>cs.IT math.IT</categories><comments>Survey, LaTeX 54 pages, 3 figures, Submitted to IEEE Trans
  Information Theory</comments><acm-class>E.4, H.1.1</acm-class><abstract>  We compare the elementary theories of Shannon information and Kolmogorov
complexity, the extent to which they have a common purpose, and where they are
fundamentally different. We discuss and relate the basic notions of both
theories: Shannon entropy versus Kolmogorov complexity, the relation of both to
universal coding, Shannon mutual information versus Kolmogorov (`algorithmic')
mutual information, probabilistic sufficient statistic versus algorithmic
sufficient statistic (related to lossy compression in the Shannon theory versus
meaningful information in the Kolmogorov theory), and rate distortion theory
versus Kolmogorov's structure function. Part of the material has appeared in
print before, scattered through various publications, but this is the first
comprehensive systematic comparison. The last mentioned relations are new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410003</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410003</id><created>2004-10-01</created><updated>2006-12-28</updated><authors><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author><author><keyname>Wang</keyname><forenames>Ying</forenames></author></authors><title>Capacity and Random-Coding Exponents for Channel Coding with Side
  Information</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Information Theory, without
  Appendices G and H</comments><abstract>  Capacity formulas and random-coding exponents are derived for a generalized
family of Gel'fand-Pinsker coding problems. These exponents yield asymptotic
upper bounds on the achievable log probability of error. In our model,
information is to be reliably transmitted through a noisy channel with finite
input and output alphabets and random state sequence, and the channel is
selected by a hypothetical adversary. Partial information about the state
sequence is available to the encoder, adversary, and decoder. The design of the
transmitter is subject to a cost constraint. Two families of channels are
considered: 1) compound discrete memoryless channels (CDMC), and 2) channels
with arbitrary memory, subject to an additive cost constraint, or more
generally to a hard constraint on the conditional type of the channel output
given the input. Both problems are closely connected. The random-coding
exponent is achieved using a stacked binning scheme and a maximum penalized
mutual information decoder, which may be thought of as an empirical generalized
Maximum a Posteriori decoder. For channels with arbitrary memory, the
random-coding exponents are larger than their CDMC counterparts. Applications
of this study include watermarking, data hiding, communication in presence of
partially known interferers, and problems such as broadcast channels, all of
which involve the fundamental idea of binning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410004</id><created>2004-10-02</created><authors><author><keyname>Szita</keyname><forenames>I.</forenames></author><author><keyname>Lorincz</keyname><forenames>A.</forenames></author></authors><title>Applying Policy Iteration for Training Recurrent Neural Networks</title><categories>cs.AI cs.LG cs.NE</categories><comments>Supplementary material. 17 papes, 1 figure</comments><abstract>  Recurrent neural networks are often used for learning time-series data. Based
on a few assumptions we model this learning task as a minimization problem of a
nonlinear least-squares cost function. The special structure of the cost
function allows us to build a connection to reinforcement learning. We exploit
this connection and derive a convergent, policy iteration-based algorithm.
Furthermore, we argue that RNN training can be fit naturally into the
reinforcement learning framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410005</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410005</id><created>2004-10-02</created><authors><author><keyname>Harder</keyname><forenames>Uli</forenames></author><author><keyname>Harrison</keyname><forenames>Peter</forenames></author><author><keyname>Paczuski</keyname><forenames>Maya</forenames></author><author><keyname>Shah</keyname><forenames>Tejas</forenames></author></authors><title>A dynamical model of a GRID market</title><categories>cs.MA cond-mat.other cs.CE</categories><comments>4 pages, 3 figures</comments><abstract>  We discuss potential market mechanisms for the GRID. A complete dynamical
model of a GRID market is defined with three types of agents. Providers,
middlemen and users exchange universal GRID computing units (GCUs) at varying
prices. Providers and middlemen have strategies aimed at maximizing profit
while users are 'satisficing' agents, and only change their behavior if the
service they receive is sufficiently poor or overpriced. Preliminary results
from a multi-agent numerical simulation of the market model shows that the
distribution of price changes has a power law tail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410006</id><created>2004-10-03</created><authors><author><keyname>Dumay</keyname><forenames>Mark</forenames></author></authors><title>Demo or Practice: Critical Analysis of the Language/Action Perspective</title><categories>cs.OH</categories><comments>23 pages</comments><abstract>  Despite offering several promising concepts, the Language/Action Perspective
(LAP) is still not in the mainstream of Information Systems Development (ISD).
Since at present there is only a limited understanding of LAP theory and
practice, it remains unclear whether the lack of LAP's impact is due to
shortcomings in LAP theory itself. One classic problem within ISD is the
dichotomy between social perspectives and technical perspectives. LAP claims it
offers a solution to this problem. This paper investigates this claim as a
means to review LAP theory. To provide a structure to a critical analysis of
DEMO - an example methodology that belongs to the LAP research community - this
paper utilizes a paradigmatic framework. This framework is augmented by the
opinion of several DEMO practitioners by means of an expert discussion. With
use of a comparative evaluation of LAP theory and DEMO theory, the implication
of DEMO's reflection upon LAP is determined. The paper concludes by outlining
an agenda for further research if LAP is to improve its footprint in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410007</id><created>2004-10-04</created><updated>2005-01-09</updated><authors><author><keyname>Souvatzis</keyname><forenames>Ignatios</forenames></author></authors><title>A Shared Write-protected Root Filesystem for a Group of Networked
  Clients</title><categories>cs.OS cs.DC</categories><comments>Presented at the 2nd European BSD Conference, 2002, Amsterdam, The
  Netherlands; v2: reformatted to help citation browser</comments><acm-class>C.2.4; D.4.7</acm-class><journal-ref>Proceedings of the 2nd European BSD Conference, 2002, Amsterdam,
  The Netherlands</journal-ref><abstract>  A method to boot a cluster of diskless network clients from a single
write-protected NFS root file system is shown. The problems encountered when
first implementing the setup and their solution are discussed. Finally, the
setup is briefly compared to using a kernel-embedded root file system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410008</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410008</id><created>2004-10-04</created><authors><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author></authors><title>Source Coding with Fixed Lag Side Information</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures</comments><journal-ref>Allerton Conference on communication, control, and computing;
  October 2004</journal-ref><abstract>  We consider source coding with fixed lag side information at the decoder. We
focus on the special case of perfect side information with unit lag
corresponding to source coding with feedforward (the dual of channel coding
with feedback) introduced by Pradhan. We use this duality to develop a linear
complexity algorithm which achieves the rate-distortion bound for any
memoryless finite alphabet source and distortion measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410009</id><created>2004-10-05</created><authors><author><keyname>Douglas</keyname><forenames>Scott</forenames></author><author><keyname>Harwood</keyname><forenames>Aaron</forenames></author></authors><title>Diffusive Load Balancing of Loosely-Synchronous Parallel Programs over
  Peer-to-Peer Networks</title><categories>cs.DC</categories><comments>14 pages with 10 figures</comments><abstract>  The use of under-utilized Internet resources is widely recognized as a viable
form of high performance computing. Sustained processing power of roughly 40T
FLOPS using 4 million volunteered Internet hosts has been reported for
embarrassingly parallel problems. At the same time, peer-to-peer (P2P) file
sharing networks, with more than 50 million participants, have demonstrated the
capacity for scale in distributed systems. This paper contributes a study of
load balancing techniques for a general class of loosely-synchronous parallel
algorithms when executed over a P2P network. We show that decentralized,
diffusive load balancing can be effective at balancing load and is facilitated
by the dynamic properties of P2P. While a moderate degree of dynamicity can
benefit load balancing, significant dynamicity hinders the parallel program
performance due to the need for increased load migration. To the best of our
knowledge this study provides new insight into the performance of
loosely-synchronous parallel programs over the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410010</id><created>2004-10-05</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K.</forenames></author><author><keyname>Lal</keyname><forenames>Sunder</forenames></author></authors><title>A New Proxy Ring Signature Scheme</title><categories>cs.CR math.RA</categories><comments>RMS 2004, Agra, INDIA</comments><journal-ref>Proceeding of RMS 2004, Page 29</journal-ref><abstract>  The concept of ring signature was introduced by Rivest, Shamir and Tauman.
This signature is considered to be a simplified group signature from which
identity of signer is ambiguous although verifier knows the group to which
signer belong. In this paper we introduce a new proxy ring signature scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410011</id><created>2004-10-05</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K</forenames></author></authors><title>Comment on A dynamic ID-based Remote User Authentication Scheme</title><categories>cs.CR</categories><comments>3 pages. Available at : http://gfcr.org</comments><journal-ref>Transaction on Cryptology, Vol. 01, Issue 02, Page 15-17, Sep 2004</journal-ref><abstract>  Since 1981, when Lamport introduced the remote user authentication scheme
using table, a plenty of schemes had been proposed with tables or without table
using. Recently Das et al. proposed a dynamic id-based remote user
authentication scheme. They claimed that their scheme is secure against
ID-theft, and can resist the reply attacks, forgery attacks, insider attacks an
so on. In this paper we show that Das et al's scheme is completly insecure and
using of this scheme is like an open server access without password.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410012</id><created>2004-10-05</created><authors><author><keyname>Dumitrescu</keyname><forenames>Catalin</forenames></author><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>DiPerF: an automated DIstributed PERformance testing Framework</title><categories>cs.PF cs.DC</categories><comments>8 pages, 8 figures, will appear in IEEE/ACM Grid2004, November 2004</comments><abstract>  We present DiPerF, a distributed performance testing framework, aimed at
simplifying and automating service performance evaluation. DiPerF coordinates a
pool of machines that test a target service, collects and aggregates
performance metrics, and generates performance statistics. The aggregate data
collected provide information on service throughput, on service &quot;fairness&quot; when
serving multiple clients concurrently, and on the impact of network latency on
service performance. Furthermore, using this data, it is possible to build
predictive models that estimate a service performance given the service load.
We have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab,
and explored the performance of job submission services (pre WS GRAM and WS
GRAM) included with Globus Toolkit 3.2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410013</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410013</id><created>2004-10-06</created><updated>2005-10-08</updated><authors><author><keyname>Vinokur</keyname><forenames>Alex</forenames></author></authors><title>Fibonacci connection between Huffman codes and Wythoff array</title><categories>cs.DM cs.DS math.CO math.NT</categories><comments>12 pages, 9 tables</comments><acm-class>E.1; E.4; G.2.2; H.1.1</acm-class><abstract>  Fibonacci connection between non-decreasing sequences of positive integers
producing maximum height Huffman trees and the Wythoff array has been proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410014</id><created>2004-10-06</created><authors><author><keyname>Costantini</keyname><forenames>Stefania</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Normal forms for Answer Sets Programming</title><categories>cs.AI</categories><comments>15 pages, To appear in Theory and Practice of Logic Programming (TPLP)</comments><acm-class>I.2.4</acm-class><abstract>  Normal forms for logic programs under stable/answer set semantics are
introduced. We argue that these forms can simplify the study of program
properties, mainly consistency. The first normal form, called the {\em kernel}
of the program, is useful for studying existence and number of answer sets. A
kernel program is composed of the atoms which are undefined in the Well-founded
semantics, which are those that directly affect the existence of answer sets.
The body of rules is composed of negative literals only. Thus, the kernel form
tends to be significantly more compact than other formulations. Also, it is
possible to check consistency of kernel programs in terms of colorings of the
Extended Dependency Graph program representation which we previously developed.
The second normal form is called {\em 3-kernel.} A 3-kernel program is composed
of the atoms which are undefined in the Well-founded semantics. Rules in
3-kernel programs have at most two conditions, and each rule either belongs to
a cycle, or defines a connection between cycles. 3-kernel programs may have
positive conditions. The 3-kernel normal form is very useful for the static
analysis of program consistency, i.e., the syntactic characterization of
existence of answer sets. This result can be obtained thanks to a novel
graph-like representation of programs, called Cycle Graph which presented in
the companion article \cite{Cos04b}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410015</id><created>2004-10-07</created><authors><author><keyname>Szabo</keyname><forenames>Z.</forenames></author><author><keyname>Lorincz</keyname><forenames>A.</forenames></author></authors><title>L1 regularization is better than L2 for learning and predicting chaotic
  systems</title><categories>cs.LG cs.AI</categories><comments>13 pages, 4 figures</comments><abstract>  Emergent behaviors are in the focus of recent research interest. It is then
of considerable importance to investigate what optimizations suit the learning
and prediction of chaotic systems, the putative candidates for emergence. We
have compared L1 and L2 regularizations on predicting chaotic time series using
linear recurrent neural networks. The internal representation and the weights
of the networks were optimized in a unifying framework. Computational tests on
different problems indicate considerable advantages for the L1 regularization:
It had considerably better learning time and better interpolating capabilities.
We shall argue that optimization viewed as a maximum likelihood estimation
justifies our results, because L1 regularization fits heavy-tailed
distributions -- an apparently general feature of emergent systems -- better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410016</id><created>2004-10-07</created><authors><author><keyname>Amorim</keyname><forenames>Antonio</forenames></author><author><keyname>Villate</keyname><forenames>Jaime</forenames></author><author><keyname>Andrade</keyname><forenames>Pedro</forenames></author></authors><title>HEP@Home - A distributed computing system based on BOINC</title><categories>cs.DC</categories><comments>4 pages, 4 Postscript figures, uses CHEP2004.cls, submitted to
  CHEP2004</comments><abstract>  Project SETI@HOME has proven to be one of the biggest successes of
distributed computing during the last years. With a quite simple approach SETI
manages to process large volumes of data using a vast amount of distributed
computer power.
  To extend the generic usage of this kind of distributed computing tools,
BOINC is being developed. In this paper we propose HEP@HOME, a BOINC version
tailored to the specific requirements of the High Energy Physics (HEP)
community.
  The HEP@HOME will be able to process large amounts of data using virtually
unlimited computing power, as BOINC does, and it should be able to work
according to HEP specifications. In HEP the amounts of data to be analyzed or
reconstructed are of central importance. Therefore, one of the design
principles of this tool is to avoid data transfer. This will allow scientists
to run their analysis applications and taking advantage of a large number of
CPUs. This tool also satisfies other important requirements in HEP, namely,
security, fault-tolerance and monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410017</id><created>2004-10-07</created><authors><author><keyname>McTague</keyname><forenames>Carl S.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Automated Pattern Detection--An Algorithm for Constructing Optimally
  Synchronizing Multi-Regular Language Filters</title><categories>cs.CV cond-mat.stat-mech cs.CL cs.DS cs.IR cs.LG nlin.AO nlin.CG nlin.PS physics.comp-ph q-bio.GN</categories><comments>18 pages, 12 figures, 2 appendices; http://www.santafe.edu/~cmg</comments><report-no>Santa Fe Institute 04-09-027</report-no><abstract>  In the computational-mechanics structural analysis of one-dimensional
cellular automata the following automata-theoretic analogue of the
\emph{change-point problem} from time series analysis arises: \emph{Given a
string $\sigma$ and a collection $\{\mc{D}_i\}$ of finite automata, identify
the regions of $\sigma$ that belong to each $\mc{D}_i$ and, in particular, the
boundaries separating them.} We present two methods for solving this
\emph{multi-regular language filtering problem}. The first, although providing
the ideal solution, requires a stack, has a worst-case compute time that grows
quadratically in $\sigma$'s length and conditions its output at any point on
arbitrarily long windows of future input. The second method is to
algorithmically construct a transducer that approximates the first algorithm.
In contrast to the stack-based algorithm, however, the transducer requires only
a finite amount of memory, runs in linear time, and gives immediate output for
each letter read; it is, moreover, the best possible finite-state approximation
with these three features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410018</id><created>2004-10-11</created><updated>2005-03-15</updated><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul</forenames></author><author><keyname>Martin</keyname><forenames>Russell</forenames></author></authors><title>Utilitarian resource assignment</title><categories>cs.GT math.GM</categories><comments>19 pages</comments><abstract>  This paper studies a resource allocation problem introduced by Koutsoupias
and Papadimitriou. The scenario is modelled as a multiple-player game in which
each player selects one of a finite number of known resources. The cost to the
player is the total weight of all players who choose that resource, multiplied
by the ``delay'' of that resource. Recent papers have studied the Nash
equilibria and social optima of this game in terms of the $L_\infty$ cost
metric, in which the social cost is taken to be the maximum cost to any player.
We study the $L_1$ variant of this game, in which the social cost is taken to
be the sum of the costs to the individual players, rather than the maximum of
these costs. We give bounds on the size of the coordination ratio, which is the
ratio between the social cost incurred by selfish behavior and the optimal
social cost; we also study the algorithmic problem of finding optimal
(lowest-cost) assignments and Nash Equilibria. Additionally, we obtain bounds
on the ratio between alternative Nash equilibria for some special cases of the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410019</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410019</id><created>2004-10-10</created><authors><author><keyname>Amraoui</keyname><forenames>Abdelaziz</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Finite-Length Scaling and Finite-Length Shift for Low-Density
  Parity-Check Codes</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>42nd Allerton Conference on Communication, Control and Computing
  (invited paper)</comments><abstract>  Consider communication over the binary erasure channel BEC using random
low-density parity-check codes with finite-blocklength n from `standard'
ensembles. We show that large error events is conveniently described within a
scaling theory, and explain how to estimate heuristically their effect. Among
other quantities, we consider the finite length threshold e(n), defined by
requiring a block error probability P_B = 1/2. For ensembles with minimum
variable degree larger than two, the following expression is argued to hold
e(n) = e -e_1 n^{-2/3} +\Theta(n^{-1}) with a calculable shift} parameter
e_1&gt;0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410020</id><created>2004-10-10</created><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Adaptive Cluster Expansion (ACE): A Hierarchical Bayesian Network</title><categories>cs.NE cs.CV</categories><comments>35 pages, 20 figures</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  Using the maximum entropy method, we derive the &quot;adaptive cluster expansion&quot;
(ACE), which can be trained to estimate probability density functions in high
dimensional spaces. The main advantage of ACE over other Bayesian networks is
its ability to capture high order statistics after short training times, which
it achieves by making use of a hierarchical vector quantisation of the input
data. We derive a scheme for representing the state of an ACE network as a
&quot;probability image&quot;, which allows us to identify statistically anomalous
regions in an otherwise statistically homogeneous image, for instance. Finally,
we present some probability images that we obtained after training ACE on some
Brodatz texture images - these demonstrate the ability of ACE to detect subtle
textural anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410021</id><created>2004-10-10</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanislaw P.</forenames></author><author><keyname>Tripathi</keyname><forenames>Rahul</forenames></author></authors><title>Complexity Results in Graph Reconstruction</title><categories>cs.CC cs.DM</categories><report-no>URCS-TR-2004-852</report-no><acm-class>F.1.3; F.2.2</acm-class><abstract>  We investigate the relative complexity of the graph isomorphism problem (GI)
and problems related to the reconstruction of a graph from its vertex-deleted
or edge-deleted subgraphs (in particular, deck checking (DC) and legitimate
deck (LD) problems). We show that these problems are closely related for all
amounts $c \geq 1$ of deletion:
  1) $GI \equiv^{l}_{iso} VDC_{c}$, $GI \equiv^{l}_{iso} EDC_{c}$, $GI
\leq^{l}_{m} LVD_c$, and $GI \equiv^{p}_{iso} LED_c$.
  2) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-VDC_c$ and $GI \equiv^{p}_{iso}
k-EDC_c$.
  3) For all $k \geq 2$, $GI \leq^{l}_{m} k-LVD_c$.
  4)$GI \equiv^{p}_{iso} 2-LVC_c$.
  5) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-LED_c$.
  For many of these results, even the $c = 1$ case was not previously known.
  Similar to the definition of reconstruction numbers $vrn_{\exists}(G)$ [HP85]
and $ern_{\exists}(G)$ (see page 120 of [LS03]), we introduce two new graph
parameters, $vrn_{\forall}(G)$ and $ern_{\forall}(G)$, and give an example of a
family $\{G_n\}_{n \geq 4}$ of graphs on $n$ vertices for which
$vrn_{\exists}(G_n) &lt; vrn_{\forall}(G_n)$. For every $k \geq 2$ and $n \geq 1$,
we show that there exists a collection of $k$ graphs on $(2^{k-1}+1)n+k$
vertices with $2^{n}$ 1-vertex-preimages, i.e., one has families of graph
collections whose number of 1-vertex-preimages is huge relative to the size of
the graphs involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410022</id><created>2004-10-11</created><authors><author><keyname>Piwek</keyname><forenames>P.</forenames></author><author><keyname>Krenn</keyname><forenames>B.</forenames></author><author><keyname>Schroeder</keyname><forenames>M.</forenames></author><author><keyname>Grice</keyname><forenames>M.</forenames></author><author><keyname>Baumann</keyname><forenames>S.</forenames></author><author><keyname>Pirker</keyname><forenames>H.</forenames></author></authors><title>RRL: A Rich Representation Language for the Description of Agent
  Behaviour in NECA</title><categories>cs.MM cs.MA</categories><comments>7 pages, 4 figures</comments><acm-class>H5.1, H5.2</acm-class><journal-ref>In Proceedings of the AAMAS-02 Workshop ``Embodied conversational
  agents - let's specify and evaluate them!'', July 16 2002, Bologna, Italy.</journal-ref><abstract>  In this paper, we describe the Rich Representation Language (RRL) which is
used in the NECA system. The NECA system generates interactions between two or
more animated characters. The RRL is an XML compliant framework for
representing the information that is exchanged at the interfaces between the
various NECA system modules. The full XML Schemas for the RRL are available at
http://www.ai.univie.ac.at/NECA/RRL
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410023</id><created>2004-10-11</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Hempel</keyname><forenames>Harald</forenames></author></authors><title>All Superlinear Inverse Schemes are coNP-Hard</title><categories>cs.CC cs.CR</categories><comments>Conf. version was in MFCS 2004</comments><report-no>URCS-TR-2004-841</report-no><acm-class>F.1.3</acm-class><abstract>  How hard is it to invert NP-problems? We show that all superlinearly
certified inverses of NP problems are coNP-hard. To do so, we develop a novel
proof technique that builds diagonalizations against certificates directly into
a circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410024</id><created>2004-10-12</created><authors><author><keyname>Wiesmaier</keyname><forenames>A.</forenames><affiliation>TU Darmstadt</affiliation></author><author><keyname>Lippert</keyname><forenames>M.</forenames><affiliation>TU Darmstadt</affiliation></author><author><keyname>Karatsiolis</keyname><forenames>V.</forenames><affiliation>TU Darmstadt</affiliation></author></authors><title>The Key Authority - Secure Key Management in Hierarchical Public Key
  Infrastructures</title><categories>cs.CR</categories><comments>5 pages, 2 figures</comments><journal-ref>in Proceedings of the International Conference on Security and
  Management. CSREA Press, June 2004, pp. 89-93</journal-ref><abstract>  We model a private key`s life cycle as a finite state machine. The states are
the key`s phases of life and the transition functions describe tasks to be done
with the key. Based on this we define and describe the key authority, a trust
center module, which potentiates the easy enforcement of secure management of
private keys in hierarchical public key infrastructures. This is done by
assembling all trust center tasks concerning the crucial handling of private
keys within one centralized module. As this module resides under full control
of the trust center`s carrier it can easily be protected by well-known
organizational and technical measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410025</id><created>2004-10-12</created><updated>2005-05-26</updated><authors><author><keyname>Wiesmaier</keyname><forenames>A.</forenames></author><author><keyname>Fischer</keyname><forenames>M.</forenames></author><author><keyname>Lippert</keyname><forenames>M.</forenames></author><author><keyname>Buchmann</keyname><forenames>J.</forenames></author></authors><title>Outflanking and securely using the PIN/TAN-System</title><categories>cs.CR</categories><comments>7 pages; 2 figures; IEEE style; final version</comments><journal-ref>Proceedings of the 2005 International Conference on Security and
  Management (SAM'05); June 2005</journal-ref><abstract>  The PIN/TAN-system is an authentication and authorization scheme used in
e-business. Like other similar schemes it is successfully attacked by
criminals. After shortly classifying the various kinds of attacks we accomplish
malicious code attacks on real World Wide Web transaction systems. In doing so
we find that it is really easy to outflank these systems. This is even
supported by the users' behavior. We give a few simple behavior rules to
improve this situation. But their impact is limited. Also the providers support
the attacks by having implementation flaws in their installations. Finally we
show that the PIN/TAN-system is not suitable for usage in highly secure
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410026</id><created>2004-10-12</created><authors><author><keyname>Neilsen</keyname><forenames>Eric H.</forenames><suffix>Jr</suffix></author><author><keyname>Simone</keyname><forenames>James</forenames></author></authors><title>Lattice QCD Data and Metadata Archives at Fermilab and the International
  Lattice Data Grid</title><categories>cs.DC hep-lat</categories><comments>Proceedings for CHEP 2004 presentation</comments><abstract>  The lattice gauge theory community produces large volumes of data. Because
the data produced by completed computations form the basis for future work, the
maintenance of archives of existing data and metadata describing the
provenance, generation parameters, and derived characteristics of that data is
essential not only as a reference, but also as a basis for future work.
Development of these archives according to uniform standards both in the data
and metadata formats provided and in the software interfaces to the component
services could greatly simplify collaborations between institutions and enable
the dissemination of meaningful results.
  This paper describes the progress made in the development of a set of such
archives at the Fermilab lattice QCD facility. We are coordinating the
development of the interfaces to these facilities and the formats of the data
and metadata they provide with the efforts of the international lattice data
grid (ILDG) metadata and middleware working groups, whose goals are to develop
standard formats for lattice QCD data and metadata and a uniform interface to
archive facilities that store them. Services under development include those
commonly associate with data grids: a service registry, a metadata database, a
replica catalog, and an interface to a mass storage system. All services
provide GSI authenticated web service interfaces following modern standards,
including WSDL and SOAP, and accept and provide data and metadata following
recent XML based formats proposed by the ILDG metadata working group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410027</id><created>2004-10-12</created><authors><author><keyname>Yu</keyname><forenames>Chen</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author></authors><title>Detecting User Engagement in Everyday Conversations</title><categories>cs.SD cs.CL cs.HC</categories><comments>4 pages (A4), 1 figure (EPS)</comments><acm-class>I.5.4; I.2.7; H.5.2; H.4.3</acm-class><journal-ref>Proc. 8th Int'l Conf. on Spoken Language Processing (ICSLP) (Vol.
  2), Jeju Island, Republic of Korea, Oct. 2004, 1329-1332. ISCA.</journal-ref><abstract>  This paper presents a novel application of speech emotion recognition:
estimation of the level of conversational engagement between users of a voice
communication system. We begin by using machine learning techniques, such as
the support vector machine (SVM), to classify users' emotions as expressed in
individual utterances. However, this alone fails to model the temporal and
interactive aspects of conversational engagement. We therefore propose the use
of a multilevel structure based on coupled hidden Markov models (HMM) to
estimate engagement levels in continuous natural speech. The first level is
comprised of SVM-based classifiers that recognize emotional states, which could
be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM
then uses these emotional states as input, estimating users' engagement in
conversation by decoding the internal states of the HMM. We report experimental
results obtained by applying our algorithms to the LDC Emotional Prosody and
CallFriend speech corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410028</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410028</id><created>2004-10-13</created><updated>2004-11-14</updated><authors><author><keyname>Measson</keyname><forenames>Cyril</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Richardson</keyname><forenames>Tom</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Life Above Threshold: From List Decoding to Area Theorem and MSE</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>2004 IEEE Information Theory Workshop, San Antonio, October 24-29
  (invited paper)</comments><abstract>  We consider communication over memoryless channels using low-density
parity-check code ensembles above the iterative (belief propagation) threshold.
What is the computational complexity of decoding (i.e., of reconstructing all
the typical input codewords for a given channel output) in this regime? We
define an algorithm accomplishing this task and analyze its typical
performance. The behavior of the new algorithm can be expressed in purely
information-theoretical terms. Its analysis provides an alternative proof of
the area theorem for the binary erasure channel. Finally, we explain how the
area theorem is generalized to arbitrary memoryless channels. We note that the
recently discovered relation between mutual information and minimal square
error is an instance of the area theorem in the setting of Gaussian channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410029</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410029</id><created>2004-10-14</created><authors><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author></authors><title>Nondeterministic Linear Logic</title><categories>cs.LO</categories><journal-ref>IPSJ SIGNotes PROgramming No.12, 1996</journal-ref><abstract>  In this paper, we introduce Linear Logic with a nondeterministic facility,
which has a self-dual additive connective. In the system the proof net
technology is available in a natural way. The important point is that
nondeterminism in the system is expressed by the process of normalization, not
by proof search. Moreover we can incorporate the system into Light Linear Logic
and Elementary Linear Logic developed by J.-Y.Girard recently: Nondeterministic
Light Linear Logic and Nondeterministic Elementary Linear Logic are defined in
a very natural way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410030</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410030</id><created>2004-10-14</created><updated>2005-04-16</updated><authors><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author></authors><title>Weak Typed Boehm Theorem on IMLL</title><categories>cs.LO</categories><comments>a few minor corrections</comments><doi>10.1016/j.apal.2006.06.001</doi><abstract>  In the Boehm theorem workshop on Crete island, Zoran Petric called Statman's
``Typical Ambiguity theorem'' typed Boehm theorem. Moreover, he gave a new
proof of the theorem based on set-theoretical models of the simply typed lambda
calculus. In this paper, we study the linear version of the typed Boehm theorem
on a fragment of Intuitionistic Linear Logic. We show that in the
multiplicative fragment of intuitionistic linear logic without the
multiplicative unit 1 (for short IMLL) weak typed Boehm theorem holds. The
system IMLL exactly corresponds to the linear lambda calculus without
exponentials, additives and logical constants. The system IMLL also exactly
corresponds to the free symmetric monoidal closed category without the unit
object. As far as we know, our separation result is the first one with regard
to these systems in a purely syntactical manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410031</id><created>2004-10-14</created><updated>2005-03-31</updated><authors><author><keyname>Cao</keyname><forenames>Yongzhi</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author></authors><title>Similarity-Based Supervisory Control of Discrete Event Systems</title><categories>cs.DM</categories><comments>22 pages, 5 figures</comments><acm-class>B.1; I.6.8</acm-class><journal-ref>A short version has been published in the IEEE Transactions on
  Automatic Control, 51(2), pp. 325-330, February 2006.</journal-ref><abstract>  Due to the appearance of uncontrollable events in discrete event systems, one
may wish to replace the behavior leading to the uncontrollability of
pre-specified language by some quite similar one. To capture this similarity,
we introduce metric to traditional supervisory control theory and generalize
the concept of original controllability to $\ld$-controllability, where $\ld$
indicates the similarity degree of two languages. A necessary and sufficient
condition for a language to be $\ld$-controllable is provided. We then examine
some properties of $\ld$-controllable languages and present an approach to
optimizing a realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410032</identifier>
 <datestamp>2009-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410032</id><created>2004-10-14</created><updated>2005-05-12</updated><authors><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author></authors><title>The state complexity of L^2 and L^k</title><categories>cs.CC cs.FL</categories><comments>5 pages, 1 figure; some errors corrected</comments><acm-class>F.1.1</acm-class><abstract>  We show that if M is a DFA with n states over an arbitrary alphabet and L =
L(M), then the worst-case state complexity of L^2 is n*2^n - 2^{n-1}. If,
however, M is a DFA over a unary alphabet, then the worst-case state complexity
of L^k is kn-k+1 for all k &gt;= 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410033</identifier>
 <datestamp>2009-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410033</id><created>2004-10-14</created><updated>2004-10-27</updated><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>An In-Depth Look at Information Fusion Rules &amp; the Unification of Fusion
  Theories</title><categories>cs.AI</categories><comments>27 pages. To be presented at NASA Langley Research Center (Hampton,
  Virginia), on November 5th, 2004</comments><acm-class>I.2.3</acm-class><journal-ref>Partially published in Review of the Air Force Academy (The
  Scientific Informative Review), Brasov, No. 2, pp. 31-40, 2006.</journal-ref><abstract>  This paper may look like a glossary of the fusion rules and we also introduce
new ones presenting their formulas and examples: Conjunctive, Disjunctive,
Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule,
Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache
classical and hybrid rules, Murphy's average rule,
Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as
particular cases: Iganaki's parameterized rule, Weighting Average Operator,
minC (M. Daniel), and newly Proportional Conflict Redistribution rules
(Smarandache-Dezert) among which PCR5 is the most exact way of redistribution
of the conflicting mass to non-empty sets following the path of the conjunctive
rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus
Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and
three new T-norm &amp; T-conorm rules adjusted from fuzzy and neutrosophic sets to
information fusion (Tchamova-Smarandache). Introducing the degree of union and
degree of inclusion with respect to the cardinal of sets not with the fuzzy set
point of view, besides that of intersection, many fusion rules can be improved.
There are corner cases where each rule might have difficulties working or may
not get an expected result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410034</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410034</id><created>2004-10-15</created><updated>2004-10-18</updated><authors><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author></authors><title>P-time Completeness of Light Linear Logic and its Nondeterministic
  Extension</title><categories>cs.LO</categories><abstract>  In CSL'99 Roversi pointed out that the Turing machine encoding of Girard's
seminal paper &quot;Light Linear Logic&quot; has a flaw. Moreover he presented a working
version of the encoding in Light Affine Logic, but not in Light Linear Logic.
In this paper we present a working version of the encoding in Light Linear
Logic. The idea of the encoding is based on a remark of Girard's tutorial paper
on Linear Logic. The encoding is also an example which shows usefulness of
additive connectives. Moreover we also consider a nondeterministic extension of
Light Linear Logic. We show that the extended system is NP-complete in the same
meaning as P-completeness of Light Linear Logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410035</id><created>2004-10-15</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Mukherji</keyname><forenames>Proshanto</forenames></author><author><keyname>Tantau</keyname><forenames>Till</forenames></author></authors><title>Overhead-Free Computation, DCFLs, and CFLs</title><categories>cs.CC</categories><report-no>URCS-TR-2004-844</report-no><acm-class>F.4.3; F.1.1</acm-class><abstract>  We study Turing machines that are allowed absolutely no space overhead. The
only work space the machines have, beyond the fixed amount of memory implicit
in their finite-state control, is that which they can create by cannibalizing
the input bits' own space. This model more closely reflects the fixed-sized
memory of real computers than does the standard complexity-theoretic model of
linear space.
  Though some context-sensitive languages cannot be accepted by such machines,
we show that all context-free languages can be accepted nondeterministically in
polynomial time with absolutely no space overhead, and that all deterministic
context-free languages can be accepted deterministically in polynomial time
with absolutely no space overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410036</id><created>2004-10-15</created><updated>2005-09-09</updated><authors><author><keyname>Luttrell</keyname><forenames>Stephen</forenames></author></authors><title>Self-Organised Factorial Encoding of a Toroidal Manifold</title><categories>cs.LG cs.CV</categories><comments>46 pages, 11 figures, corrected equation 39</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  It is shown analytically how a neural network can be used optimally to encode
input data that is derived from a toroidal manifold. The case of a 2-layer
network is considered, where the output is assumed to be a set of discrete
neural firing events. The network objective function measures the average
Euclidean error that occurs when the network attempts to reconstruct its input
from its output. This optimisation problem is solved analytically for a
toroidal input manifold, and two types of solution are obtained: a joint
encoder in which the network acts as a soft vector quantiser, and a factorial
encoder in which the network acts as a pair of soft vector quantisers (one for
each of the circular subspaces of the torus). The factorial encoder is favoured
for small network sizes when the number of observed firing events is large.
Such self-organised factorial encoding may be used to restrict the size of
network that is required to perform a given encoding task, and will decompose
an input manifold into its constituent submanifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410037</id><created>2004-10-15</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author></authors><title>Hardware-Oriented Group Solutions for Hard Problems</title><categories>cs.CC</categories><acm-class>F.2.0; F.1.3; F.2.2</acm-class><abstract>  Group and individual solutions are considered for hard problems such as
satisfiability problem. Time-space trade-off in a structured active memory
provides means to achieve lower time complexity for solutions of these
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410038</id><created>2004-10-16</created><authors><author><keyname>Geerts</keyname><forenames>Floris</forenames></author></authors><title>Frequent Knot Discovery</title><categories>cs.DB</categories><comments>7 pages, 2 figures, recreational data mining</comments><abstract>  We explore the possibility of applying the framework of frequent pattern
mining to a class of continuous objects appearing in nature, namely knots. We
introduce the frequent knot mining problem and present a solution. The key
observation is that a database consisting of knots can be transformed into a
transactional database. This observation is based on the Prime Decomposition
Theorem of knots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410039</id><created>2004-10-17</created><authors><author><keyname>Cohen</keyname><forenames>Sara</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author></authors><title>Generating All Maximal Induced Subgraphs for Hereditary,
  Connected-Hereditary and Rooted-Hereditary Properties</title><categories>cs.DS cs.DM</categories><acm-class>G.2.2; F.2; F.1.3</acm-class><abstract>  The problem of computing all maximal induced subgraphs of a graph G that have
a graph property P, also called the maximal P-subgraphs problem, is considered.
This problem is studied for hereditary, connected-hereditary and
rooted-hereditary graph properties. The maximal P-subgraphs problem is reduced
to restricted versions of this problem by providing algorithms that solve the
general problem, assuming that an algorithm for a restricted version is given.
The complexity of the algorithms are analyzed in terms of total polynomial
time, incremental polynomial time and the complexity class P-enumerable. The
general results presented allow simple proofs that the maximal P-subgraphs
problem can be solved efficiently (in terms of the input and output) for many
different properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410040</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410040</id><created>2004-10-17</created><authors><author><keyname>Fukatani</keyname><forenames>Takayuki</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Uyematsu</keyname><forenames>Tomohoko</forenames></author></authors><title>Two Methods for Decreasing the Computational Complexity of the MIMO ML
  Decoder</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, using a LaTeX style file ieice.cls</comments><journal-ref>IEICE Trans. Fundamentals, vol. E87-A, no. 10, pp. 2571-2576, Oct.
  2004</journal-ref><abstract>  We propose use of QR factorization with sort and Dijkstra's algorithm for
decreasing the computational complexity of the sphere decoder that is used for
ML detection of signals on the multi-antenna fading channel. QR factorization
with sort decreases the complexity of searching part of the decoder with small
increase in the complexity required for preprocessing part of the decoder.
Dijkstra's algorithm decreases the complexity of searching part of the decoder
with increase in the storage complexity. The computer simulation demonstrates
that the complexity of the decoder is reduced by the proposed methods
significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410041</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410041</id><created>2004-10-18</created><authors><author><keyname>Tanaka</keyname><forenames>Kenji</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Uyematsu</keyname><forenames>Tomohiko</forenames></author></authors><title>Maximum Mutual Information of Space-Time Block Codes with Symbolwise
  Decodability</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, using isita2004.sty, appeared in Proc. ISITA
  2004, pp. 1025-1030, Parma, Italy, Oct. 10-13, 2004</comments><abstract>  In this paper, we analyze the performance of space-time block codes which
enable symbolwise maximum likelihood decoding. We derive an upper bound of
maximum mutual information (MMI) on space-time block codes that enable
symbolwise maximum likelihood decoding for a frequency non-selective
quasi-static fading channel. MMI is an upper bound on how much one can send
information with vanishing error probability by using the target code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410042</id><created>2004-10-18</created><authors><author><keyname>Ritter</keyname><forenames>H.</forenames></author><author><keyname>Steil</keyname><forenames>J. J.</forenames></author><author><keyname>Noelker</keyname><forenames>C.</forenames></author><author><keyname>Roethling</keyname><forenames>F.</forenames></author><author><keyname>McGuire</keyname><forenames>P. C.</forenames></author></authors><title>Neural Architectures for Robot Intelligence</title><categories>cs.RO cs.CV cs.HC cs.LG cs.NE q-bio.NC</categories><comments>37 pages, 17 figures</comments><acm-class>I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4</acm-class><journal-ref>Reviews in the Neurosciences, vol. 14, no. 1-2, pp. 121-143 (2003)</journal-ref><abstract>  We argue that the direct experimental approaches to elucidate the
architecture of higher brains may benefit from insights gained from exploring
the possibilities and limits of artificial control architectures for robot
systems. We present some of our recent work that has been motivated by that
view and that is centered around the study of various aspects of hand actions
since these are intimately linked with many higher cognitive abilities. As
examples, we report on the development of a modular system for the recognition
of continuous hand postures based on neural nets, the use of vision and tactile
sensing for guiding prehensile movements of a multifingered hand, and the
recognition and use of hand gestures for robot teaching.
  Regarding the issue of learning, we propose to view real-world learning from
the perspective of data mining and to focus more strongly on the imitation of
observed actions instead of purely reinforcement-based exploration. As a
concrete example of such an effort we report on the status of an ongoing
project in our lab in which a robot equipped with an attention system with a
neurally inspired architecture is taught actions by using hand gestures in
conjunction with speech commands. We point out some of the lessons learnt from
this system, and discuss how systems of this kind can contribute to the study
of issues at the junction between natural and artificial cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410043</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410043</id><created>2004-10-18</created><authors><author><keyname>Peczarski</keyname><forenames>Marcin</forenames></author></authors><title>Strategy in Ulam's Game and Tree Code Give Error-Resistant Protocols</title><categories>cs.DC cs.IT math.IT</categories><comments>10 pages, 2 figures</comments><abstract>  We present a new approach to construction of protocols which are proof
against communication errors. The construction is based on a generalization of
the well known Ulam's game. We show equivalence between winning strategies in
this game and robust protocols for multi-party computation. We do not give any
complete theory. We want rather to describe a new fresh idea. We use a tree
code defined by Schulman. The tree code is the most important part of the
interactive version of Shannon's Coding Theorem proved by Schulman. He uses
probabilistic argument for the existence of a tree code without giving any
effective construction. We show another proof yielding a randomized
construction which in contrary to his proof almost surely gives a good code.
Moreover our construction uses much smaller alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410044</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410044</id><created>2004-10-18</created><updated>2005-12-09</updated><authors><author><keyname>Kisil</keyname><forenames>Vladimir V.</forenames></author></authors><title>An Example of Clifford Algebras Calculations with GiNaC</title><categories>cs.MS cs.CG cs.GR cs.SC</categories><comments>20 pages, LaTeX2e, 12 PS graphics in one figure; v3 code
  improvements; v4 small code correction for new libraries; v5 comments are
  redesined to be more readable</comments><report-no>LEEDS-MATH-PURE-2004-18</report-no><journal-ref>Advances in Applied Clifford Algebras, 15(2005), no. 2, pp.
  239-269</journal-ref><abstract>  This example of Clifford algebras calculations uses GiNaC
(http://www.ginac.de/) library, which includes a support for generic Clifford
algebra starting from version~1.3.0. Both symbolic and numeric calculation are
possible and can be blended with other functions of GiNaC. This calculations
was made for the paper math.CV/0410399.
 Described features of GiNaC are already available at PyGiNaC
(http://sourceforge.net/projects/pyginac/) and due to course should propagate
into other software like GNU Octave (http://www.octave.org/), gTybalt
(http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as
their back-end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410045</identifier>
 <datestamp>2011-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410045</id><created>2004-10-18</created><updated>2011-05-22</updated><authors><author><keyname>Shontz</keyname><forenames>Suzanne M.</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen A.</forenames></author></authors><title>Analysis of and workarounds for element reversal for a finite
  element-based algorithm for warping triangular and tetrahedral meshes</title><categories>cs.NA</categories><comments>Revision of earlier version of paper. Submitted for publication in
  BIT Numerical Mathematics on 27 April 2010. Accepted for publication on 7
  September 2010. Published online on 9 October 2010. The final publication is
  available at http://www.springerlink.com</comments><acm-class>G.1.8; G.1.0; G.4</acm-class><journal-ref>BIT, Numerical Mathematics, Vol. 50, Issue 4, 2010, p. 863-884</journal-ref><doi>10.1007/s10543-010-0283-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an algorithm called FEMWARP for warping triangular and
tetrahedral finite element meshes that computes the warping using the finite
element method itself. The algorithm takes as input a two- or three-dimensional
domain defined by a boundary mesh (segments in one dimension or triangles in
two dimensions) that has a volume mesh (triangles in two dimensions or
tetrahedra in three dimensions) in its interior. It also takes as input a
prescribed movement of the boundary mesh. It computes as output updated
positions of the vertices of the volume mesh. The first step of the algorithm
is to determine from the initial mesh a set of local weights for each interior
vertex that describes each interior vertex in terms of the positions of its
neighbors. These weights are computed using a finite element stiffness matrix.
After a boundary transformation is applied, a linear system of equations based
upon the weights is solved to determine the final positions of the interior
vertices. The FEMWARP algorithm has been considered in the previous literature
(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing
deformed meshes for certain applications. However, sometimes FEMWARP reverses
elements; this is our main concern in this paper. We analyze the causes for
this undesirable behavior and propose several techniques to make the method
more robust against reversals. The most successful of the proposed methods
includes combining FEMWARP with an optimization-based untangler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410046</id><created>2004-10-18</created><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Jawor</keyname><forenames>Wojciech</forenames></author><author><keyname>Kowalik</keyname><forenames>Lukasz</forenames></author><author><keyname>Kurowski</keyname><forenames>Maciej</forenames></author></authors><title>A Note on Scheduling Equal-Length Jobs to Maximize Throughput</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  We study the problem of scheduling equal-length jobs with release times and
deadlines, where the objective is to maximize the number of completed jobs.
Preemptions are not allowed. In Graham's notation, the problem is described as
1|r_j;p_j=p|\sum U_j. We give the following results: (1) We show that the often
cited algorithm by Carlier from 1981 is not correct. (2) We give an algorithm
for this problem with running time O(n^5).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410047</id><created>2004-10-19</created><authors><author><keyname>Hoepman</keyname><forenames>Jaap-Henk</forenames></author></authors><title>Simple Distributed Weighted Matchings</title><categories>cs.DC cs.DM</categories><abstract>  Wattenhofer [WW04] derive a complicated distributed algorithm to compute a
weighted matching of an arbitrary weighted graph, that is at most a factor 5
away from the maximum weighted matching of that graph. We show that a variant
of the obvious sequential greedy algorithm [Pre99], that computes a weighted
matching at most a factor 2 away from the maximum, is easily distributed. This
yields the best known distributed approximation algorithm for this problem so
far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410048</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410048</id><created>2004-10-19</created><updated>2013-11-27</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>Worst-Case Optimal Tree Layout in External Memory</title><categories>cs.DS</categories><comments>10 pages, 1 figure. To appear in Algorithmica</comments><acm-class>F.2.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider laying out a fixed-topology tree of N nodes into external memory
with block size B so as to minimize the worst-case number of block memory
transfers required to traverse a path from the root to a node of depth D. We
prove that the optimal number of memory transfers is $$ \cases{
  \displaystyle
  \Theta\left( {D \over \lg (1{+}B)} \right)
  &amp; when $D = O(\lg N)$, \cr
  \displaystyle
  \Theta\left( {\lg N \over \lg \left(1{+}{B \lg N \over D}\right)} \right)
  &amp; when $D = \Omega(\lg N)$ and $D = O(B \lg N)$, \cr
  \displaystyle
  \Theta\left( {D \over B} \right)
  &amp; when $D = \Omega(B \lg N)$.
  } $$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410049</id><created>2004-10-19</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Intransitivity and Vagueness</title><categories>cs.AI</categories><comments>A preliminary version of this paper appears in Principles of
  Knowledge Representation and Reasoning: Proceedings of the Ninth
  International Conference (KR 2004)</comments><abstract>  There are many examples in the literature that suggest that
indistinguishability is intransitive, despite the fact that the
indistinguishability relation is typically taken to be an equivalence relation
(and thus transitive). It is shown that if the uncertainty perception and the
question of when an agent reports that two things are indistinguishable are
both carefully modeled, the problems disappear, and indistinguishability can
indeed be taken to be an equivalence relation. Moreover, this model also
suggests a logic of vagueness that seems to solve many of the problems related
to vagueness discussed in the philosophical literature. In particular, it is
shown here how the logic can handle the sorites paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410050</id><created>2004-10-19</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Sleeping Beauty Reconsidered: Conditioning and Reflection in
  Asynchronous Systems</title><categories>cs.AI</categories><comments>A preliminary version of this paper appears in Principles of
  Knowledge Representation and Reasoning: Proceedings of the Ninth
  International Conference (KR 2004). This version will appear in Oxford
  Studies in Epistemology</comments><abstract>  A careful analysis of conditioning in the Sleeping Beauty problem is done,
using the formal model for reasoning about knowledge and probability developed
by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as
revealing problems with conditioning in the presence of imperfect recall, the
analysis done here reveals that the problems are not so much due to imperfect
recall as to asynchrony. The implications of this analysis for van Fraassen's
Reflection Principle and Savage's Sure-Thing Principle are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410051</id><created>2004-10-20</created><authors><author><keyname>Vinokur</keyname><forenames>Alex</forenames></author></authors><title>Turing Machine with Faults, Failures and Recovery</title><categories>cs.LO</categories><comments>8 pages; C++ Simulator has been developed</comments><acm-class>F.1.1; F.4.1</acm-class><abstract>  A Turing machine with faults, failures and recovery (TMF) is described. TMF
is (weakly) non-deterministic Turing machine consisting of five semi-infinite
tapes (Master Tape, Synchro Tape, Backup Tape, Backup Synchro Tape, User Tape)
and four controlling components (Program, Daemon, Apparatus, User).
Computational process consists of three phases (Program Phase, Failure Phase,
Repair Phase). C++ Simulator of a Turing machine with faults, failures and
recovery has been developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410052</id><created>2004-10-19</created><authors><author><keyname>Glass</keyname><forenames>Julie</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author><author><keyname>Snoeyink</keyname><forenames>Jack</forenames></author><author><keyname>Zhong</keyname><forenames>Jianyuan K.</forenames></author></authors><title>A 2-chain can interlock with a k-chain</title><categories>cs.CG cs.DM</categories><comments>10 pages, 6 figures</comments><acm-class>F.2.2</acm-class><abstract>  One of the open problems posed in [3] is: what is the minimal number k such
that an open, flexible k-chain can interlock with a flexible 2-chain? In this
paper, we establish the assumption behind this problem, that there is indeed
some k that achieves interlocking. We prove that a flexible 2-chain can
interlock with a flexible, open 16-chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410053</id><created>2004-10-20</created><authors><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>Tian</keyname><forenames>Hao</forenames></author><author><keyname>Sunderraman</keyname><forenames>Rajshekhar</forenames></author></authors><title>An Extended Generalized Disjunctive Paraconsistent Data Model for
  Disjunctive Information</title><categories>cs.DB</categories><comments>10 pages</comments><abstract>  This paper presents an extension of generalized disjunctive paraconsistent
relational data model in which pure disjunctive positive and negative
information as well as mixed disjunctive positive and negative information can
be represented explicitly and manipulated. We consider explicit mixed
disjunctive information in the context of disjunctive databases as there is an
interesting interplay between these two types of information. Extended
generalized disjunctive paraconsistent relation is introduced as the main
structure in this model. The relational algebra is appropriately generalized to
work on extended generalized disjunctive paraconsistent relations and their
correctness is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410054</id><created>2004-10-20</created><authors><author><keyname>Sunderraman</keyname><forenames>Rajshekhar</forenames></author><author><keyname>Wang</keyname><forenames>Haibin</forenames></author></authors><title>Paraconsistent Intuitionistic Fuzzy Relational Data Model</title><categories>cs.DB</categories><comments>19 pages, 1 figure</comments><abstract>  In this paper, we present a generalization of the relational data model based
on paraconsistent intuitionistic fuzzy sets. Our data model is capable of
manipulating incomplete as well as inconsistent information. Fuzzy relation or
intuitionistic fuzzy relation can only handle incomplete information.
Associated with each relation are two membership functions one is called
truth-membership function $T$ which keeps track of the extent to which we
believe the tuple is in the relation, another is called false-membership
function which keeps track of the extent to which we believe that it is not in
the relation. A paraconsistent intuitionistic fuzzy relation is inconsistent if
there exists one tuple $a$ such that $T(a) + F(a) &gt; 1$. In order to handle
inconsistent situation, we propose an operator called split to transform
inconsistent paraconsistent intuitionistic fuzzy relations into
pseudo-consistent paraconsistent intuitionistic fuzzy relations and do the
set-theoretic and relation-theoretic operations on them and finally use another
operator called combine to transform the result back to paraconsistent
intuitionistic fuzzy relation. For this model, we define algebraic operators
that are generalisations of the usual operators such as union, selection, join
on fuzzy relations. Our data model can underlie any database and knowledge-base
management system that deals with incomplete and inconsistent information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410055</id><created>2004-10-21</created><authors><author><keyname>Hazewinkel</keyname><forenames>Michiel</forenames></author></authors><title>Mathematical knowledge management is needed</title><categories>cs.IR</categories><comments>Keynote speech at the November, 2003 MKM meeting ar Herriott-Watt,
  Edinburg, UK. Nine pages, one figure</comments><abstract>  In this lecture I discuss some aspects of MKM, Mathematical Knowledge
Management, with particuar emphasis on information storage and information
retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410056</id><created>2004-10-21</created><updated>2004-11-03</updated><authors><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Zhang</keyname><forenames>Yanqing</forenames></author><author><keyname>Sunderraman</keyname><forenames>Rajshekhar</forenames></author></authors><title>Interval Neutrosophic Logics: Theory and Applications</title><categories>cs.LO</categories><comments>18 pages, 4 figures</comments><abstract>  In this paper, we present the interval neutrosophic logics which generalizes
the fuzzy logic, paraconsistent logic, intuitionistic fuzzy logic and many
other non-classical and non-standard logics. We will give the formal definition
of interval neutrosophic propositional calculus and interval neutrosophic
predicate calculus. Then we give one application of interval neutrosophic
logics to do approximate reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410057</id><created>2004-10-25</created><updated>2006-06-27</updated><authors><author><keyname>Rao</keyname><forenames>M. V. Panduranga</forenames></author></authors><title>Generalized Counters and Reversal Complexity</title><categories>cs.CC</categories><comments>9 pages, no figures; Layout and content changed to some extent;
  Conference version</comments><acm-class>F.1.1</acm-class><journal-ref>pp.318-326, Proceedings of TAMC 2006, Beijing, China, Springer
  LNCS, 3959</journal-ref><abstract>  We generalize the definition of a counter and counter reversal complexity and
investigate the power of generalized deterministic counter automata in terms of
language recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410058</id><created>2004-10-22</created><authors><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Ballim</keyname><forenames>Afzal</forenames></author></authors><title>Robust Dialogue Understanding in HERALD</title><categories>cs.CL cs.AI cs.HC cs.MA cs.SE</categories><comments>6 pages</comments><acm-class>H.5.2, I.2.7, I.2.11</acm-class><journal-ref>Proceedings of RANLP 2001 - EuroConference on Recent Advances in
  Natural Language Processing, September 5-7, 2001, Tzigov-Chark, Bulgaria</journal-ref><abstract>  We tackle the problem of robust dialogue processing from the perspective of
language engineering. We propose an agent-oriented architecture that allows us
a flexible way of composing robust processors. Our approach is based on
Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP
agent model can be enriched with special features and components that allow us
to deal with classical problems of dialogue understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410059</id><created>2004-10-22</created><authors><author><keyname>Ballim</keyname><forenames>Afzal</forenames></author><author><keyname>Fatemi</keyname><forenames>Nastaran</forenames></author><author><keyname>Ghorbel</keyname><forenames>Hatem</forenames></author><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author></authors><title>A knowledge-based approach to semi-automatic annotation of multimedia
  documents via user adaptation</title><categories>cs.DL cs.CL cs.IR</categories><comments>4 pages</comments><acm-class>I.7.2, H.3.7</acm-class><journal-ref>First EAGLES/ISLE Workshop on Meta-Descriptions and Annotation
  Schemas for Multimodal/Multimedia Language Resources (LREC 2000
  Pre-Conference Workshop), 29-30 May 2000, Athens, Greece</journal-ref><abstract>  Current approaches to the annotation process focus on annotation schemas,
languages for annotation, or are very application driven. In this paper it is
proposed that a more flexible architecture for annotation requires a knowledge
component to allow for flexible search and navigation of the annotated
material. In particular, it is claimed that a general approach must take into
account the needs, competencies, and goals of the producers, annotators, and
consumers of the annotated material. We propose that a user-model based
approach is, therefore, necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410060</id><created>2004-10-22</created><authors><author><keyname>Ballim</keyname><forenames>Afzal</forenames></author><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author></authors><title>Semantic filtering by inference on domain knowledge in spoken dialogue
  systems</title><categories>cs.CL cs.AI cs.HC cs.IR</categories><comments>6 pages</comments><acm-class>H.5.2;H.3.1;H.3.4</acm-class><journal-ref>Proceedings of the LREC 2000 Workshop &quot;From spoken dialogue to
  full natural interactive dialogue. Theory, empirical analysis and
  evaluation&quot;, May 29th, 2000 Athen, Greece</journal-ref><abstract>  General natural dialogue processing requires large amounts of domain
knowledge as well as linguistic knowledge in order to ensure acceptable
coverage and understanding. There are several ways of integrating lexical
resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at
different levels of dialogue processing. We concentrate in this paper on how to
exploit domain knowledge for filtering interpretation hypotheses generated by a
robust semantic parser. We use domain knowledge to semantically constrain the
hypothesis space. Moreover, adding an inference mechanism allows us to complete
the interpretation when information is not explicitly available. Further, we
discuss briefly how this can be generalized towards a predictive natural
interactive system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410061</id><created>2004-10-24</created><authors><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Ghorbel</keyname><forenames>Hatem</forenames></author><author><keyname>Ruch</keyname><forenames>Patrick</forenames></author><author><keyname>Coray</keyname><forenames>Giovanni</forenames></author></authors><title>An argumentative annotation schema for meeting discussions</title><categories>cs.CL cs.DL cs.IR</categories><comments>4 pages</comments><acm-class>H.3.1;I.7.2;H.5.1</acm-class><journal-ref>Procedings of the LREC 2004 international conference, 26-28 May
  2004, Lisbon, Portugal. Pages 1003-1006</journal-ref><abstract>  In this article, we are interested in the annotation of transcriptions of
human-human dialogue taken from meeting records. We first propose a meeting
content model where conversational acts are interpreted with respect to their
argumentative force and their role in building the argumentative structure of
the meeting discussion. Argumentation in dialogue describes the way
participants take part in the discussion and argue their standpoints. Then, we
propose an annotation scheme based on such an argumentative dialogue model as
well as the evaluation of its adequacy. The obtained higher-level semantic
annotations are exploited in the conceptual indexing of the information
contained in meeting discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410062</id><created>2004-10-24</created><authors><author><keyname>van der Plas</keyname><forenames>Lonneke</forenames></author><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Rajman</keyname><forenames>Martin</forenames></author><author><keyname>Ghorbel</keyname><forenames>Hatem</forenames></author></authors><title>Automatic Keyword Extraction from Spoken Text. A Comparison of two
  Lexical Resources: the EDR and WordNet</title><categories>cs.CL cs.DL cs.IR</categories><comments>4 pages</comments><acm-class>H.3.1;H.3.3;I.5.3;I.7.3</acm-class><journal-ref>Procedings of the LREC 2004 international conference, 26-28 May
  2004, Lisbon, Portugal. Pages 2205-2208</journal-ref><abstract>  Lexical resources such as WordNet and the EDR electronic dictionary have been
used in several NLP tasks. Probably, partly due to the fact that the EDR is not
freely available, WordNet has been used far more often than the EDR. We have
used both resources on the same task in order to make a comparison possible.
The task is automatic assignment of keywords to multi-party dialogue episodes
(i.e. thematically coherent stretches of spoken text). We show that the use of
lexical resources in such a task results in slightly higher performances than
the use of a purely statistically based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410063</id><created>2004-10-24</created><authors><author><keyname>Moeller</keyname><forenames>Sebastian</forenames></author><author><keyname>Krebber</keyname><forenames>Jan</forenames></author><author><keyname>Raake</keyname><forenames>Alexander</forenames></author><author><keyname>Smeele</keyname><forenames>Paula</forenames></author><author><keyname>Rajman</keyname><forenames>Martin</forenames></author><author><keyname>Melichar</keyname><forenames>Mirek</forenames></author><author><keyname>Pallotta</keyname><forenames>Vincenzo</forenames></author><author><keyname>Tsakou</keyname><forenames>Gianna</forenames></author><author><keyname>Kladis</keyname><forenames>Basilis</forenames></author><author><keyname>Vovos</keyname><forenames>Anestis</forenames></author><author><keyname>Hoonhout</keyname><forenames>Jettie</forenames></author><author><keyname>Schuchardt</keyname><forenames>Dietmar</forenames></author><author><keyname>Fakotakis</keyname><forenames>Nikos</forenames></author><author><keyname>Ganchev</keyname><forenames>Todor</forenames></author><author><keyname>Potamitis</keyname><forenames>Ilyas</forenames></author></authors><title>INSPIRE: Evaluation of a Smart-Home System for Infotainment Management
  and Device Control</title><categories>cs.HC cs.CL</categories><comments>4 pages</comments><acm-class>H.5.2;I.2.7;H.1.2</acm-class><journal-ref>Procedings of the LREC 2004 international conference, 26-28 May
  2004, Lisbon, Portugal. Pages 1603-1606</journal-ref><abstract>  This paper gives an overview of the assessment and evaluation methods which
have been used to determine the quality of the INSPIRE smart home system. The
system allows different home appliances to be controlled via speech, and
consists of speech and speaker recognition, speech understanding, dialogue
management, and speech output components. The performance of these components
is first assessed individually, and then the entire system is evaluated in an
interaction experiment with test users. Initial results of the assessment and
evaluation are given, in particular with respect to the transmission channel
impact on speech and speaker recognition, and the assessment of speech output
for different system metaphors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410064</id><created>2004-10-25</created><authors><author><keyname>Balic</keyname><forenames>J.</forenames></author></authors><title>Intelligent Computer Numerical Control unit for machine tools</title><categories>cs.CE</categories><comments>7 pages, 4 figures, 17 references</comments><acm-class>J.6.2; J.7.3</acm-class><journal-ref>Neural-Network-Based Numerical Control for Milling Machine,
  Journal of Intelligent and Robotic System, Volume 40, Issue 4, Aug 2004;
  Pages: 343-358 (extended version)</journal-ref><abstract>  The paper describes a new CNC control unit for machining centres with
learning ability and automatic intelligent generating of NC programs on the
bases of a neural network, which is built-in into a CNC unit as special device.
The device performs intelligent and completely automatically the NC part
programs only on the bases of 2D, 2,5D or 3D computer model of prismatic part.
Intervention of the operator is not needed. The neural network for milling,
drilling, reaming, threading and operations alike has learned to generate NC
programs in the learning module, which is a part of intelligent CAD/CAM system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410065</identifier>
 <datestamp>2012-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410065</id><created>2004-10-25</created><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames></author><author><keyname>Kr&#xf6;tzsch</keyname><forenames>Markus</forenames></author><author><keyname>Zhang</keyname><forenames>Guo-Qiang</forenames></author></authors><title>A Categorical View on Algebraic Lattices in Formal Concept Analysis</title><categories>cs.LO</categories><comments>36 pages</comments><acm-class>F.3.2;I.2.4</acm-class><journal-ref>Fundam. Inform. 74:2-3 (2006) 301-328</journal-ref><abstract>  Formal concept analysis has grown from a new branch of the mathematical field
of lattice theory to a widely recognized tool in Computer Science and
elsewhere. In order to fully benefit from this theory, we believe that it can
be enriched with notions such as approximation by computation or
representability. The latter are commonly studied in denotational semantics and
domain theory and captured most prominently by the notion of algebraicity, e.g.
of lattices. In this paper, we explore the notion of algebraicity in formal
concept analysis from a category-theoretical perspective. To this end, we build
on the the notion of approximable concept with a suitable category and show
that the latter is equivalent to the category of algebraic lattices. At the
same time, the paper provides a relatively comprehensive account of the
representation theory of algebraic lattices in the framework of Stone duality,
relating well-known structures such as Scott information systems with further
formalisms from logic, topology, domains and lattice theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410066</id><created>2004-10-25</created><updated>2005-10-11</updated><authors><author><keyname>Ma</keyname><forenames>Xiaoqin</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author></authors><title>Fast Query Processing by Distributing an Index over CPU Caches</title><categories>cs.DC cs.PF</categories><comments>New version published at IEEE Cluster Computing 2005</comments><acm-class>C.4</acm-class><abstract>  Data intensive applications on clusters often require requests quickly be
sent to the node managing the desired data. In many applications, one must look
through a sorted tree structure to determine the responsible node for accessing
or storing the data.
  Examples include object tracking in sensor networks, packet routing over the
internet, request processing in publish-subscribe middleware, and query
processing in database systems. When the tree structure is larger than the CPU
cache, the standard implementation potentially incurs many cache misses for
each lookup; one cache miss at each successive level of the tree. As the
CPU-RAM gap grows, this performance degradation will only become worse in the
future.
  We propose a solution that takes advantage of the growing speed of local area
networks for clusters. We split the sorted tree structure among the nodes of
the cluster. We assume that the structure will fit inside the aggregation of
the CPU caches of the entire cluster. We then send a word over the network (as
part of a larger packet containing other words) in order to examine the tree
structure in another node's CPU cache. We show that this is often faster than
the standard solution, which locally incurs multiple cache misses while
accessing each successive level of the tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410067</id><created>2004-10-26</created><authors><author><keyname>Troy</keyname><forenames>Richard M.</forenames><suffix>III</suffix></author></authors><title>Computational Unification: a Vision for Connecting Researchers</title><categories>cs.DC cs.CY</categories><comments>Three page white-paper which was written for the December 2002 AGU
  meeting in San Francisco, CA. Also note that the author is Chief Scientist of
  Science Tools and developed a working system capable of implementing the
  ideas expressed in this abstract in 1997</comments><acm-class>C.2.4; D.2.12; D.2.13; E.1; F.1.2; H.1.1; H.2.4; H.2.5; H.3.4;
  H.3.7; H.5.3; K.6.1</acm-class><abstract>  The extent to which the benefits of science can be fully realized depends
critically upon the quality of the connection between researchers themselves
and between researchers and members of the public. We believe that it is now
possible to improve these connections on a community-wide and even world-wide
basis through the use of an appropriate information management system. In this
paper we explore the concepts and challenges, and propose an architecture for
the implementation of such a system.
  &quot;One of the greatest visions for science is a computational unification in
which every researcher can interact with all other researchers through use of
their own research system.&quot;
  &quot;These features not only enable research collaboration on a scale never
previously envisaged, they also enable sharing and dissemination of scientific
knowledge to the public at large with a sophistication unparalleled in
history.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410068</id><created>2004-10-25</created><authors><author><keyname>Li</keyname><forenames>Zhuowei</forenames></author><author><keyname>Das</keyname><forenames>Amitabha</forenames></author></authors><title>Analyzing and Improving Performance of a Class of Anomaly-based
  Intrusion Detectors</title><categories>cs.CR cs.AI</categories><comments>Submit to journal for publication</comments><report-no>cais-tr-2004-001</report-no><abstract>  Anomaly-based intrusion detection (AID) techniques are useful for detecting
novel intrusions into computing resources. One of the most successful AID
detectors proposed to date is stide, which is based on analysis of system call
sequences. In this paper, we present a detailed formal framework to analyze,
understand and improve the performance of stide and similar AID techniques.
Several important properties of stide-like detectors are established through
formal proofs, and validated by carefully conducted experiments using test
datasets. Finally, the framework is utilized to design two applications to
improve the cost and performance of stide-like detectors which are based on
sequence analysis. The first application reduces the cost of developing AID
detectors by identifying the critical sections in the training dataset, and the
second application identifies the intrusion context in the intrusive dataset,
that helps to fine-tune the detectors. Such fine-tuning in turn helps to
improve detection rate and reduce false alarm rate, thereby increasing the
effectiveness and efficiency of the intrusion detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410069</id><created>2004-10-26</created><authors><author><keyname>Corbo</keyname><forenames>Jacomo</forenames></author><author><keyname>Petermann</keyname><forenames>Thomas</forenames></author></authors><title>Selfish peering and routing in the Internet</title><categories>cs.GT cond-mat.stat-mech cs.NI</categories><comments>Contribution to the Proceedings of the Complex Systems Summer School
  2004, organized by the Santa Fe Institute (6 pages comprising 4 figures)</comments><acm-class>C.2.1</acm-class><abstract>  The Internet is a loose amalgamation of independent service providers acting
in their own self-interest. We examine the implications of this economic
reality on peering relationships. Specifically, we consider how the incentives
of the providers might determine where they choose to interconnect with each
other. We consider a game where two selfish network providers must establish
peering points between their respective network graphs, given knowledge of
traffic conditions and a nearest-exit routing policy for out-going traffic, as
well as costs based on congestion and peering connectivity. We focus on the
pairwise stability equilibrium concept and use a stochastic procedure to solve
for the stochastically pairwise stable configurations. Stochastically stable
networks are selected for their robustness to deviations in strategy and are
therefore posited as the more likely networks to emerge in a dynamic setting.
We note a paucity of stochastically stable peering configurations under
asymmetric conditions, particularly to unequal interdomain traffic flow, with
adverse effects on system-wide efficiency. Under bilateral flow conditions, we
find that as the cost associated with the establishment of peering links
approaches zero, the variance in the number of peering links of stochastically
pairwise stable equilibria increases dramatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410070</id><created>2004-10-26</created><authors><author><keyname>Gasparri</keyname><forenames>Giovanni</forenames></author></authors><title>Using image partitions in 4th Dimension</title><categories>cs.DB</categories><comments>4 pages, 3 figures</comments><acm-class>C.2.4</acm-class><abstract>  I have plotted an image by using mathematical functions in the Database &quot;4th
Dimension&quot;. I'm going to show an alternative method to: detect which sector has
been clicked; highlight it and combine it with other sectors already
highlighted; store the graph information in an efficient way; load and splat
image layers to reconstruct the stored graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410071</id><created>2004-10-27</created><authors><author><keyname>McGuire</keyname><forenames>Patrick C.</forenames></author><author><keyname>Ormo</keyname><forenames>Jens</forenames></author><author><keyname>Diaz-Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Rodriguez-Manfredi</keyname><forenames>Jose Antonio</forenames></author><author><keyname>Gomez-Elvira</keyname><forenames>Javier</forenames></author><author><keyname>Ritter</keyname><forenames>Helge</forenames></author><author><keyname>Oesker</keyname><forenames>Markus</forenames></author><author><keyname>Ontrup</keyname><forenames>Joerg</forenames></author></authors><title>The Cyborg Astrobiologist: First Field Experience</title><categories>cs.CV astro-ph cs.AI cs.CE cs.HC cs.RO cs.SE q-bio.NC</categories><comments>29 pages, 10 figures (in press)</comments><acm-class>I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9</acm-class><journal-ref>Int.J.Astrobiol. 3 (2004) 189-207</journal-ref><doi>10.1017/S147355040500220X</doi><abstract>  We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist and
field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used
for testing and development of these algorithms and systems: robotic
acquisition of quasi-mosaics of images, real-time image segmentation, and
real-time determination of interesting points in the image mosaics. The
hardware and software systems function reliably, and the computer-vision
algorithms are adequate for the first field tests. In addition to the
proof-of-concept aspect of these field tests, the main result of these field
tests is the enumeration of those issues that we can improve in the future,
including: first, detection and accounting for shadows caused by 3D jagged
edges in the outcrop; second, reincorporation of more sophisticated
texture-analysis algorithms into the system; third, creation of hardware and
software capabilities to control the camera's zoom lens in an intelligent
manner; and fourth, development of algorithms for interpretation of complex
geological scenery. Nonetheless, despite these technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410072</id><created>2004-10-27</created><authors><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames></author><author><keyname>Potapov</keyname><forenames>Igor</forenames></author></authors><title>Temporal logic with predicate abstraction</title><categories>cs.LO cs.CL</categories><comments>14 pages, 4 figures</comments><acm-class>F.1.1; F.3.1; F.4.1; F.4.3</acm-class><abstract>  A predicate linear temporal logic LTL_{\lambda,=} without quantifiers but
with predicate abstraction mechanism and equality is considered. The models of
LTL_{\lambda,=} can be naturally seen as the systems of pebbles (flexible
constants) moving over the elements of some (possibly infinite) domain. This
allows to use LTL_{\lambda,=} for the specification of dynamic systems using
some resources, such as processes using memory locations, mobile agents
occupying some sites, etc. On the other hand we show that LTL_{\lambda,=} is
not recursively axiomatizable and, therefore, fully automated verification of
LTL_{\lambda,=} specifications is not, in general, possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410073</id><created>2004-10-28</created><authors><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author><author><keyname>Rinard</keyname><forenames>Martin</forenames></author></authors><title>On Spatial Conjunction as Second-Order Logic</title><categories>cs.LO cs.PL cs.SE</categories><comments>16 pages</comments><report-no>MIT CSAIL 970</report-no><abstract>  Spatial conjunction is a powerful construct for reasoning about dynamically
allocated data structures, as well as concurrent, distributed and mobile
computation. While researchers have identified many uses of spatial
conjunction, its precise expressive power compared to traditional logical
constructs was not previously known. In this paper we establish the expressive
power of spatial conjunction. We construct an embedding from first-order logic
with spatial conjunction into second-order logic, and more surprisingly, an
embedding from full second order logic into first-order logic with spatial
conjunction. These embeddings show that the satisfiability of formulas in
first-order logic with spatial conjunction is equivalent to the satisfiability
of formulas in second-order logic. These results explain the great expressive
power of spatial conjunction and can be used to show that adding unrestricted
spatial conjunction to a decidable logic leads to an undecidable logic. As one
example, we show that adding unrestricted spatial conjunction to two-variable
logic leads to undecidability. On the side of decidability, the embedding into
second-order logic immediately implies the decidability of first-order logic
with a form of spatial conjunction over trees. The embedding into spatial
conjunction also has useful consequences: because a restricted form of spatial
conjunction in two-variable logic preserves decidability, we obtain that a
correspondingly restricted form of second-order quantification in two-variable
logic is decidable. The resulting language generalizes the first-order theory
of boolean algebra over sets and is useful in reasoning about the contents of
data structures in object-oriented languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410074</id><created>2004-10-28</created><updated>2004-11-01</updated><authors><author><keyname>Zeng</keyname><forenames>Jianyang</forenames></author><author><keyname>Hsu</keyname><forenames>Wen-Jing</forenames></author></authors><title>ReCord: A Distributed Hash Table with Recursive Structure</title><categories>cs.DC</categories><comments>16 pages, 7 figures</comments><abstract>  We propose a simple distributed hash table called ReCord, which is a
generalized version of Randomized-Chord and offers improved tradeoffs in
performance and topology maintenance over existing P2P systems. ReCord is
scalable and can be easily implemented as an overlay network, and offers a good
tradeoff between the node degree and query latency. For instance, an $n$-node
ReCord with $O(\log n)$ node degree has an expected latency of $\Theta(\log n)$
hops. Alternatively, it can also offer $\Theta(\frac{\log n}{\log \log n})$
hops latency at a higher cost of $O(\frac{\log^2 n}{\log
 \log n})$ node degree. Meanwhile, simulations of the dynamic behaviors of
ReCord are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0410075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0410075</id><created>2004-10-29</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Some first thoughts on the stability of the asynchronous systems</title><categories>cs.GL</categories><comments>12 pages, conference</comments><journal-ref>The 12-th Conference on Applied and Industrial Mathematics CAIM
  2004, University of Pitesti, October 15-17, 2004</journal-ref><abstract>  The (non-initialized, non-deterministic) asynchronous systems (in the
input-output sense) are multi-valued functions from m-dimensional signals to
sets of n-dimensional signals, the concept being inspired by the modeling of
the asynchronous circuits. Our purpose is to state the problem of the their
stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411001</id><created>2004-11-01</created><authors><author><keyname>Worytkiewicz</keyname><forenames>Krzysztof</forenames></author></authors><title>Synchronization from a Categorical Perspective</title><categories>cs.PL cs.DM</categories><abstract>  We introduce a notion of synchronization for higher-dimensional automata,
based on coskeletons of cubical sets. Categorification transports this notion
to the setting of categorical transition systems. We apply the results to study
the semantics of an imperative programming language with message-passing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411002</id><created>2004-11-01</created><authors><author><keyname>Vinokur</keyname><forenames>Alex</forenames></author></authors><title>Fibonacci-Like Polynomials Produced by m-ary Huffman Codes for
  Absolutely Ordered Sequences</title><categories>cs.DM math.NT</categories><comments>10 pages, 2 tables</comments><acm-class>E.1; E.4; F.2.0; G.2.2; I.4.2</acm-class><abstract>  Fibonacci-like polynomials produced by m-ary Huffman codes for absolutely
ordered sequences have been described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411003</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411003</id><created>2004-11-02</created><updated>2007-01-29</updated><authors><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Dihidar</keyname><forenames>Souvik</forenames></author><author><keyname>Calderbank</keyname><forenames>A. R.</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steven</forenames></author><author><keyname>Merolla</keyname><forenames>Jean-Marc</forenames></author></authors><title>Applications of LDPC Codes to the Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>30 pages, revised version</comments><abstract>  With the advent of quantum key distribution (QKD) systems, perfect (i.e.
information-theoretic) security can now be achieved for distribution of a
cryptographic key. QKD systems and similar protocols use classical
error-correcting codes for both error correction (for the honest parties to
correct errors) and privacy amplification (to make an eavesdropper fully
ignorant). From a coding perspective, a good model that corresponds to such a
setting is the wire tap channel introduced by Wyner in 1975. In this paper, we
study fundamental limits and coding methods for wire tap channels. We provide
an alternative view of the proof for secrecy capacity of wire tap channels and
show how capacity achieving codes can be used to achieve the secrecy capacity
for any wiretap channel. We also consider binary erasure channel and binary
symmetric channel special cases for the wiretap channel and propose specific
practical codes. In some cases our designs achieve the secrecy capacity and in
others the codes provide security at rates below secrecy capacity. For the
special case of a noiseless main channel and binary erasure channel, we
consider encoder and decoder design for codes achieving secrecy on the wiretap
channel; we show that it is possible to construct linear-time decodable secrecy
codes based on LDPC codes that achieve secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411004</id><created>2004-11-02</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Computational Aspects of a Numerical Model for Combustion Flow</title><categories>cs.NA physics.comp-ph</categories><comments>9 pages, 4 figures; Talk at Workshop 2004 on Science and Applications
  of Advanced Computing Paradigms, Centre of Excellence MIUR (prof. Gianfranco
  Bilardi), Universita' di Padova, Department of Information Engineering,
  October 28-29, 2004</comments><acm-class>G.1; J.2</acm-class><abstract>  A computational method for numeric resolution of a PDEs system, based on a
Finite Differences schema integrated by interpolations of partial results, and
an estimate of the error of its solution respect to the normal FD solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411005</id><created>2004-11-02</created><authors><author><keyname>Lal</keyname><forenames>Sunder</forenames></author><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>A Directed Threshold - Signature Scheme</title><categories>cs.CR</categories><comments>10 Pages, No figures</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  Directed signature is the solution of such problems when the signed message
contains information sensitive to the signature receiver. Generally, in many
application of directed signature, the signer is generally a single person. But
when the message is on behalf of an organization, a valid sensitive message may
require the approval of several people. Threshold signature schemes are used to
solve these problems. This paper presents a threshold directed signature
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411006</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411006</id><created>2004-11-03</created><authors><author><keyname>Sankarasubramaniam</keyname><forenames>Yogesh</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steven W.</forenames></author></authors><title>Capacity Achieving Code Constructions for Two Classes of (d,k)
  Constraints</title><categories>cs.IT math.IT</categories><comments>16 pages, submitted to the IEEE Transactions on Information Theory</comments><abstract>  In this paper, we present two low complexity algorithms that achieve capacity
for the noiseless (d,k) constrained channel when k=2d+1, or when k-d+1 is not
prime. The first algorithm, called symbol sliding, is a generalized version of
the bit flipping algorithm introduced by Aviran et al. [1]. In addition to
achieving capacity for (d,2d+1) constraints, it comes close to capacity in
other cases. The second algorithm is based on interleaving, and is a
generalized version of the bit stuffing algorithm introduced by Bender and Wolf
[2]. This method uses fewer than k-d biased bit streams to achieve capacity for
(d,k) constraints with k-d+1 not prime. In particular, the encoder for
(d,d+2^m-1) constraints, 1\le m&lt;\infty, requires only m biased bit streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411007</id><created>2004-11-04</created><authors><author><keyname>Cervelle</keyname><forenames>Julien</forenames><affiliation>IGM</affiliation></author><author><keyname>Formenti</keyname><forenames>Enrico</forenames><affiliation>I3S</affiliation></author><author><keyname>Masson</keyname><forenames>Benoit</forenames><affiliation>I3S</affiliation></author></authors><title>Basic properties for sand automata</title><categories>cs.CC</categories><comments>submitted to STACS 2005</comments><proxy>ccsd ccsd-00003208</proxy><abstract>  We prove several results about the relations between injectivity and
surjectivity for sand automata. Moreover, we begin the exploration of the
dynamical behavior of sand automata proving that the property of nilpotency is
undecidable. We believe that the proof technique used for this last result
might reveal useful for many other results in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411008</identifier>
 <datestamp>2010-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411008</id><created>2004-11-04</created><updated>2006-06-14</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Intuitionistic computability logic</title><categories>cs.LO cs.AI math.LO</categories><acm-class>F.4.1; F.1.2</acm-class><journal-ref>Acta Cybernetica 18 (2007), pp. 77-113</journal-ref><abstract>  Computability logic (CL) is a systematic formal theory of computational tasks
and resources, which, in a sense, can be seen as a semantics-based alternative
to (the syntactically introduced) linear logic. With its expressive and
flexible language, where formulas represent computational problems and &quot;truth&quot;
is understood as algorithmic solvability, CL potentially offers a comprehensive
logical basis for constructive applied theories and computing systems
inherently requiring constructive and computationally meaningful underlying
logics.
  Among the best known constructivistic logics is Heyting's intuitionistic
calculus INT, whose language can be seen as a special fragment of that of CL.
The constructivistic philosophy of INT, however, has never really found an
intuitively convincing and mathematically strict semantical justification. CL
has good claims to provide such a justification and hence a materialization of
Kolmogorov's known thesis &quot;INT = logic of problems&quot;. The present paper contains
a soundness proof for INT with respect to the CL semantics. A comprehensive
online source on CL is available at http://www.cis.upenn.edu/~giorgi/cl.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411009</id><created>2004-11-05</created><updated>2004-11-08</updated><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>The equations of the ideal latches</title><categories>cs.GL</categories><comments>16 pages, 18 figures, conference</comments><journal-ref>The 12-th Conference on Applied and Industrial Mathematics CAIM
  2004, University of Pitesti, October 15-17, 2004</journal-ref><abstract>  The latches are simple circuits with feedback from the digital electrical
engineering. We have included in our work the C element of Muller, the RS
latch, the clocked RS latch, the D latch and also circuits containing two
interconnected latches: the edge triggered RS flip-flop, the D flip-flop, the
JK flip-flop, the T flip-flop. The purpose of this study is to model with
equations the previous circuits, considered to be ideal, i.e. non-inertial. The
technique of analysis is the pseudoboolean differential calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411010</id><created>2004-11-05</created><updated>2004-11-30</updated><authors><author><keyname>Corin</keyname><forenames>Ricardo</forenames></author><author><keyname>Durante</keyname><forenames>Antonio</forenames></author><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Hartel</keyname><forenames>Pieter</forenames></author></authors><title>A Trace Logic for Local Security Properties</title><categories>cs.CR</categories><comments>New version</comments><abstract>  We propose a new simple \emph{trace} logic that can be used to specify
\emph{local security properties}, i.e. security properties that refer to a
single participant of the protocol specification. Our technique allows a
protocol designer to provide a formal specification of the desired security
properties, and integrate it naturally into the design process of cryptographic
protocols. Furthermore, the logic can be used for formal verification. We
illustrate the utility of our technique by exposing new attacks on the well
studied protocol TMN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411011</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411011</id><created>2004-11-06</created><authors><author><keyname>Fozunbal</keyname><forenames>Majid</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steven W.</forenames></author><author><keyname>Schafer</keyname><forenames>Ronald W.</forenames></author></authors><title>Capacity Analysis for Continuous Alphabet Channels with Side
  Information, Part I: A General Framework</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inform. Theory</comments><abstract>  Capacity analysis for channels with side information at the receiver has been
an active area of interest. This problem is well investigated for the case of
finite alphabet channels. However, the results are not easily generalizable to
the case of continuous alphabet channels due to analytic difficulties inherent
with continuous alphabets. In the first part of this two-part paper, we address
an analytical framework for capacity analysis of continuous alphabet channels
with side information at the receiver. For this purpose, we establish novel
necessary and sufficient conditions for weak* continuity and strict concavity
of the mutual information. These conditions are used in investigating the
existence and uniqueness of the capacity-achieving measures. Furthermore, we
derive necessary and sufficient conditions that characterize the capacity value
and the capacity-achieving measure for continuous alphabet channels with side
information at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411012</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411012</id><created>2004-11-06</created><authors><author><keyname>Fozunbal</keyname><forenames>Majid</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steven W.</forenames></author><author><keyname>Schafer</keyname><forenames>Ronald W.</forenames></author></authors><title>Capacity Analysis for Continuous Alphabet Channels with Side
  Information, Part II: MIMO Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to Trans. Inform. Theory</comments><abstract>  In this part, we consider the capacity analysis for wireless mobile systems
with multiple antenna architectures. We apply the results of the first part to
a commonly known baseband, discrete-time multiple antenna system where both the
transmitter and receiver know the channel's statistical law. We analyze the
capacity for additive white Gaussian noise (AWGN) channels, fading channels
with full channel state information (CSI) at the receiver, fading channels with
no CSI, and fading channels with partial CSI at the receiver. For each type of
channels, we study the capacity value as well as issues such as the existence,
uniqueness, and characterization of the capacity-achieving measures for
different types of moment constraints. The results are applicable to both
Rayleigh and Rician fading channels in the presence of arbitrary line-of-sight
and correlation profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411013</id><created>2004-11-07</created><authors><author><keyname>Donnet</keyname><forenames>Benoit</forenames></author><author><keyname>Raoult</keyname><forenames>Philippe</forenames></author><author><keyname>Friedman</keyname><forenames>Timur</forenames></author><author><keyname>Crovella</keyname><forenames>Mark</forenames></author></authors><title>Efficient Algorithms for Large-Scale Topology Discovery</title><categories>cs.NI</categories><comments>23 pages</comments><abstract>  There is a growing interest in discovery of internet topology at the
interface level. A new generation of highly distributed measurement systems is
currently being deployed. Unfortunately, the research community has not
examined the problem of how to perform such measurements efficiently and in a
network-friendly manner. In this paper we make two contributions toward that
end. First, we show that standard topology discovery methods (e.g., skitter)
are quite inefficient, repeatedly probing the same interfaces. This is a
concern, because when scaled up, such methods will generate so much traffic
that they will begin to resemble DDoS attacks. We measure two kinds of
redundancy in probing (intra- and inter-monitor) and show that both kinds are
important. We show that straightforward approaches to addressing these two
kinds of redundancy must take opposite tacks, and are thus fundamentally in
conflict. Our second contribution is to propose and evaluate Doubletree, an
algorithm that reduces both types of redundancy simultaneously on routers and
end systems. The key ideas are to exploit the tree-like structure of routes to
and from a single point in order to guide when to stop probing, and to probe
each path by starting near its midpoint. Our results show that Doubletree can
reduce both types of measurement load on the network dramatically, while
permitting discovery of nearly the same set of nodes and links. We then show
how to enable efficient communication between monitors through the use of Bloom
filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411014</identifier>
 <datestamp>2009-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411014</id><created>2004-11-06</created><updated>2009-11-26</updated><authors><author><keyname>Vereshchagin</keyname><forenames>Nikolai K.</forenames><affiliation>Moscow State Univ.</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Rate Distortion and Denoising of Individual Data Using Kolmogorov
  complexity</title><categories>cs.IT math.IT</categories><comments>LaTex, 31 pages, 2 figures. The new version is again completely
  rewritten, newly titled, and adds new results</comments><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the structure of families of distortion balls from the perspective
of Kolmogorov complexity. Special attention is paid to the canonical
rate-distortion function of a source word which returns the minimal Kolmogorov
complexity of all distortion balls containing that word subject to a bound on
their cardinality. This canonical rate-distortion function is related to the
more standard algorithmic rate-distortion function for the given distortion
measure. Examples are given of list distortion, Hamming distortion, and
Euclidean distortion. The algorithmic rate-distortion function can behave
differently from Shannon's rate-distortion function. To this end, we show that
the canonical rate-distortion function can and does assume a wide class of
shapes (unlike Shannon's); we relate low algorithmic mutual information to low
Kolmogorov complexity (and consequently suggest that certain aspects of the
mutual information formulation of Shannon's rate-distortion function behave
differently than would an analogous formulation using algorithmic mutual
information); we explore the notion that low Kolmogorov complexity distortion
balls containing a given word capture the interesting properties of that word
(which is hard to formalize in Shannon's theory) and this suggests an approach
to denoising; and, finally, we show that the different behavior of the
rate-distortion curves of individual source words to some extent disappears
after averaging over the source words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411015</id><created>2004-11-07</created><authors><author><keyname>Flikop</keyname><forenames>Ziny</forenames></author></authors><title>Bounded Input Bounded Predefined Control Bounded Output</title><categories>cs.AI</categories><comments>8 pages, 6 figures</comments><acm-class>I.2.8 I.2.9</acm-class><abstract>  The paper is an attempt to generalize a methodology, which is similar to the
bounded-input bounded-output method currently widely used for the system
stability studies. The presented earlier methodology allows decomposition of
input space into bounded subspaces and defining for each subspace its bounding
surface. It also defines a corresponding predefined control, which maps any
point of a bounded input into a desired bounded output subspace. This
methodology was improved by providing a mechanism for the fast defining a
bounded surface. This paper presents enhanced bounded-input
bounded-predefined-control bounded-output approach, which provides adaptability
feature to the control and allows transferring of a controlled system along a
suboptimal trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411016</id><created>2004-11-08</created><updated>2004-11-18</updated><authors><author><keyname>Wolf</keyname><forenames>Armin</forenames></author></authors><title>Intelligent search strategies based on adaptive Constraint Handling Rules</title><categories>cs.AI cs.PL</categories><comments>Number of pages: 27 Number of figures: 14 Number of Tables: 2</comments><acm-class>I.1.4</acm-class><abstract>  The most advanced implementation of adaptive constraint processing with
Constraint Handling Rules (CHR) allows the application of intelligent search
strategies to solve Constraint Satisfaction Problems (CSP). This presentation
compares an improved version of conflict-directed backjumping and two variants
of dynamic backtracking with respect to chronological backtracking on some of
the AIM instances which are a benchmark set of random 3-SAT problems. A CHR
implementation of a Boolean constraint solver combined with these different
search strategies in Java is thus being compared with a CHR implementation of
the same Boolean constraint solver combined with chronological backtracking in
SICStus Prolog. This comparison shows that the addition of ``intelligence'' to
the search process may reduce the number of search steps dramatically.
Furthermore, the runtime of their Java implementations is in most cases faster
than the implementations of chronological backtracking. More specifically,
conflict-directed backjumping is even faster than the SICStus Prolog
implementation of chronological backtracking, although our Java implementation
of CHR lacks the optimisations made in the SICStus Prolog system. To appear in
Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411017</id><created>2004-11-08</created><authors><author><keyname>Sharma</keyname><forenames>Srikant</forenames></author></authors><title>Analysis of 802.11b MAC: A QoS, Fairness, and Performance Perspective</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><abstract>  Wireless LANs have achieved a tremendous amount of growth in recent years.
Among various wireless LAN technologies, the IEEE 802.11b based wireless LAN
technology can be cited as the most prominent technology today. Despite being
widely deployed, 802.11b cannot be termed as a well matured technology.
Although 802.11b is adequate for basic connectivity and packet switching, It is
evident that there is ample scope for its improvement in areas like quality of
service, fairness, performance, security, etc. In this survey report, we
identify and argue that the Medium Access Controller for 802.11b networks is
the prime area for these improvements. To enunciate our claims we highlight
some of the quality of service, fairness, and performance issues related to
802.11b MAC. We also describe and analyze some of the current research aimed at
addressing these issues. We then propose a novel scheme called the Intelligent
Collision Avoidance, seeking to enhance the MAC to address some of the
performance issues in 802.11b and similar networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411018</id><created>2004-11-08</created><authors><author><keyname>Lima</keyname><forenames>Pedro U.</forenames></author><author><keyname>Custodio</keyname><forenames>Luis M. M.</forenames></author></authors><title>Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots</title><categories>cs.RO cs.AI</categories><journal-ref>International Journal of Advanced Robotic Systems, ISSN 1729-8806,
  Volume 1, Number 3 September 2004, pp.141-148</journal-ref><abstract>  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for &quot;Society of Robots&quot; and &quot;Soccer Robots&quot;, the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411019</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411019</id><created>2004-11-08</created><authors><author><keyname>Sharma</keyname><forenames>Srikant</forenames></author><author><keyname>Chiueh</keyname><forenames>Tzi-cker</forenames></author></authors><title>Programmable Ethernet Switches and Their Applications</title><categories>cs.NI cs.AR cs.PF</categories><comments>6 pages</comments><acm-class>C.2.5</acm-class><abstract>  Modern Ethernet switches support many advanced features beyond route learning
and packet forwarding such as VLAN tagging, IGMP snooping, rate limiting, and
status monitoring, which can be controlled through a programmatic interface.
Traditionally, these features are mostly used to statically configure a
network. This paper proposes to apply them as dynamic control mechanisms to
maximize physical network link resources, to minimize failure recovery time, to
enforce QoS requirements, and to support link-layer multicast without
broadcasting. With these advanced programmable control mechanisms, standard
Ethernet switches can be used as effective building blocks for
metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and
computation cluster interconnects. We demonstrate the usefulness of this new
level of control over Ethernet switches with a MEN architecture that features
multi-fold throughput gains and sub-second failure recovery time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411020</id><created>2004-11-08</created><authors><author><keyname>Albagul</keyname><forenames>A.</forenames></author><author><keyname>Wahyudi</keyname></author></authors><title>Dynamic Modelling and Adaptive Traction Control for Mobile Robots</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 3, September 2004, pp.149-154</journal-ref><abstract>  Mobile robots have received a great deal of research in recent years. A
significant amount of research has been published in many aspects related to
mobile robots. Most of the research is devoted to design and develop some
control techniques for robot motion and path planning. A large number of
researchers have used kinematic models to develop motion control strategy for
mobile robots. Their argument and assumption that these models are valid if the
robot has low speed, low acceleration and light load. However, dynamic
modelling of mobile robots is very important as they are designed to travel at
higher speed and perform heavy duty work. This paper presents and discusses a
new approach to develop a dynamic model and control strategy for wheeled mobile
robot which I modelled as a rigid body that roles on two wheels and a castor.
The motion control strategy consists of two levels. The first level is dealing
with the dynamic of the system and denoted as Low level controller. The second
level is developed to take care of path planning and trajectory generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411021</id><created>2004-11-08</created><authors><author><keyname>Ronghua</keyname><forenames>Luo</forenames></author><author><keyname>Bingrong</keyname><forenames>Hong</forenames></author></authors><title>Coevolution Based Adaptive Monte Carlo Localization (CEAMCL)</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 3, September 2004, pp. 183-190</journal-ref><abstract>  An adaptive Monte Carlo localization algorithm based on coevolution mechanism
of ecological species is proposed. Samples are clustered into species, each of
which represents a hypothesis of the robots pose. Since the coevolution between
the species ensures that the multiple distinct hypotheses can be tracked
stably, the problem of premature convergence when using MCL in highly symmetric
environments can be solved. And the sample size can be adjusted adaptively over
time according to the uncertainty of the robots pose by using the population
growth model. In addition, by using the crossover and mutation operators in
evolutionary computation, intra-species evolution can drive the samples move
towards the regions where the desired posterior density is large. So a small
size of samples can represent the desired density well enough to make precise
localization. The new algorithm is termed coevolution based adaptive Monte
Carlo localization (CEAMCL). Experiments have been carried out to prove the
efficiency of the new localization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411022</id><created>2004-11-08</created><authors><author><keyname>Szabo</keyname><forenames>Richard</forenames></author></authors><title>Topological Navigation of Simulated Robots using Occupancy Grid</title><categories>cs.RO cs.AI</categories><journal-ref>International Journal of Advanced Robotic Systems, ISSN 1729-8806,
  Volume 1, Number 3 (2004), pp.201-206</journal-ref><abstract>  Formerly I presented a metric navigation method in the Webots mobile robot
simulator. The navigating Khepera-like robot builds an occupancy grid of the
environment and explores the square-shaped room around with a value iteration
algorithm. Now I created a topological navigation procedure based on the
occupancy grid process. The extension by a skeletonization algorithm results a
graph of important places and the connecting routes among them. I also show the
significant time profit gained during the process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411023</id><created>2004-11-08</created><authors><author><keyname>Wang</keyname><forenames>Changda</forenames></author><author><keyname>Chen</keyname><forenames>Xianyi</forenames></author><author><keyname>Zhao</keyname><forenames>Xibin</forenames></author><author><keyname>Ju</keyname><forenames>Shiguang</forenames></author></authors><title>Design and Implementation of a General Decision-making Model in RoboCup
  Simulation</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, ISSN 1729-8806,
  Volume 1, Number 3 (2004), pp.207-112</journal-ref><abstract>  The study of the collaboration, coordination and negotiation among different
agents in a multi-agent system (MAS) has always been the most challenging yet
popular in the research of distributed artificial intelligence. In this paper,
we will suggest for RoboCup simulation, a typical MAS, a general
decision-making model, rather than define a different algorithm for each tactic
(e.g. ball handling, pass, shoot and interception, etc.) in soccer games as
most RoboCup simulation teams did. The general decision-making model is based
on two critical factors in soccer games: the vertical distance to the goal line
and the visual angle for the goalpost. We have used these two parameters to
formalize the defensive and offensive decisions in RoboCup simulation and the
results mentioned above had been applied in NOVAURO, original name is UJDB, a
RoboCup simulation team of Jiangsu University, whose decision-making model,
compared with that of Tsinghua University, the world champion team in 2001, is
a universal model and easier to be implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411024</id><created>2004-11-08</created><authors><author><keyname>Ellery</keyname><forenames>Alex</forenames></author></authors><title>Space Robotics Part 2: Space-based Manipulators</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, ISSN 1729-8806,
  Volume 1, Number 3 (2004), pp.213-216</journal-ref><abstract>  In this second of three short papers, I introduce some of the basic concepts
of space robotics with an emphasis on some specific challenging areas of
research that are peculiar to the application of robotics to space
infrastructure development. The style of these short papers is pedagogical and
the concepts in this paper are developed from fundamental manipulator robotics.
This second paper considers the application of space manipulators to on-orbit
servicing (OOS), an application which has considerable commercial application.
I provide some background to the notion of robotic on-orbit servicing and
explore how manipulator control algorithms may be modified to accommodate space
manipulators which operate in the micro-gravity of space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411025</id><created>2004-11-08</created><authors><author><keyname>Bar-Cohen</keyname><forenames>Yoseph</forenames></author></authors><title>Bionic Humans Using EAP as Artificial Muscles Reality and Challenges</title><categories>cs.RO cs.AI</categories><journal-ref>International Journal of Advanced Robotic Systems, ISSN 1729-8806,
  Volume 1, Number 3 (2004), pp.217-222</journal-ref><abstract>  For many years, the idea of a human with bionic muscles immediately conjures
up science fiction images of a TV series superhuman character that was
implanted with bionic muscles and portrayed with strength and speed far
superior to any normal human. As fantastic as this idea may seem, recent
developments in electroactive polymers (EAP) may one day make such bionics
possible. Polymers that exhibit large displacement in response to stimulation
that is other than electrical signal were known for many years. Initially, EAP
received relatively little attention due to their limited actuation capability.
However, in the recent years, the view of the EAP materials has changed due to
the introduction of effective new materials that significantly surpassed the
capability of the widely used piezoelectric polymer, PVDF. As this technology
continues to evolve, novel mechanisms that are biologically inspired are
expected to emerge. EAP materials can potentially provide actuation with
lifelike response and more flexible configurations. While further improvements
in performance and robustness are still needed, there already have been several
reported successes. In recognition of the need for cooperation in this
multidisciplinary field, the author initiated and organized a series of
international forums that are leading to a growing number of research and
development projects and to great advances in the field. In 1999, he challenged
the worldwide science and engineering community of EAP experts to develop a
robotic arm that is actuated by artificial muscles to win a wrestling match
against a human opponent. In this paper, the field of EAP as artificial muscles
will be reviewed covering the state of the art, the challenges and the vision
for the progress in future years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411026</id><created>2004-11-08</created><authors><author><keyname>Tylevich</keyname><forenames>Boris Mark</forenames><affiliation>Moscow Institute of Physics and Technology, Moscow, Russia</affiliation></author></authors><title>A Search Relevancy Tuning Method Using Expert Results Content Evaluation</title><categories>cs.IR</categories><comments>10 pages, 2 figures</comments><acm-class>H.3.3</acm-class><abstract>  The article presents an online relevancy tuning method using explicit user
feedback. The author developed and tested a method of words' weights
modification based on search result evaluation by user. User decides whether
the result is useful or not after inspecting the full result content. The
experiment proved that the constantly accumulated words weights base leads to
better search quality in a specified data domain. The author also suggested
future improvements of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411027</id><created>2004-11-10</created><authors><author><keyname>Ravelomanana</keyname><forenames>Vlady</forenames><affiliation>LIPN</affiliation></author></authors><title>Extremal Properties of Three Dimensional Sensor Networks with
  Applications</title><categories>cs.DS cs.DC cs.DM</categories><proxy>ccsd ccsd-00003248</proxy><acm-class>ACM classification: C.2.1 Network architecture and design; F.2.2
  Nonnumerical algorithms and problems; G.3 Probability and statistics</acm-class><journal-ref>IEEE Transactions on Mobile Computing Vol 3 (2004) pages 246--257</journal-ref><abstract>  In this paper, we analyze various critical transmitting/sensing ranges for
connectivity and coverage in three-dimensional sensor networks. As in other
large-scale complex systems, many global parameters of sensor networks undergo
phase transitions: For a given property of the network, there is a critical
threshold, corresponding to the minimum amount of the communication effort or
power expenditure by individual nodes, above (resp. below) which the property
exists with high (resp. a low) probability. For sensor networks, properties of
interest include simple and multiple degrees of connectivity/coverage. First,
we investigate the network topology according to the region of deployment, the
number of deployed sensors and their transmitting/sensing ranges. More
specifically, we consider the following problems: Assume that $n$ nodes, each
capable of sensing events within a radius of $r$, are randomly and uniformly
distributed in a 3-dimensional region $\mathcal{R}$ of volume $V$, how large
must the sensing range be to ensure a given degree of coverage of the region to
monitor? For a given transmission range, what is the minimum (resp. maximum)
degree of the network? What is then the typical hop-diameter of the underlying
network? Next, we show how these results affect algorithmic aspects of the
network by designing specific distributed protocols for sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411028</id><created>2004-11-10</created><authors><author><keyname>Souvatzis</keyname><forenames>Ignatios</forenames></author></authors><title>A machine-independent port of the SR language run-time system to the
  NetBSD operating system</title><categories>cs.DC cs.PL</categories><comments>presented at 3rd European BSD Conference, Karlsruhe, Germany, 2004
  Oct. 29-31; 4 pages</comments><acm-class>D.1.3; D.3.4</acm-class><journal-ref>Juergen Egeling (Ed.): Proceedings of the 3rd European BSD
  Conference, Karlsruhe, Germany 2004, p.181</journal-ref><abstract>  SR (synchronizing resources) is a PASCAL - style language enhanced with
constructs for concurrent programming developed at the University of Arizona in
the late 1980s. MPD (presented in Gregory Andrews' book about Foundations of
Multithreaded, Parallel, and Distributed Programming) is its successor,
providing the same language primitives with a different syntax. The run-time
system (in theory, identical) of both languages provides the illusion of a
multiprocessor machine on a single single- or multi- CPU Unix-like system or a
(local area) network of Unix-like machines. Chair V of the Computer Science
Department of the University of Bonn is operating a laboratory for a practical
course in parallel programming consisting of computing nodes running
NetBSD/arm, normally used via PVM, MPI etc. We are considering to offer SR and
MPD for this, too. As the original language distributions are only targeted at
a few commercial Unix systems, some porting effort is needed, outlined in the
SR porting guide. The integrated POSIX threads support of NetBSD-2.0 should
allow us to use library primitives provided for NetBSD's phtread system to
implement the primitives needed by the SR run-time system, thus implementing 13
target CPUs at once and automatically making use of SMP on VAX, Alpha, PowerPC,
Sparc, 32-bit Intel and 64 bit AMD CPUs.
  This paper describes work in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411029</id><created>2004-11-10</created><authors><author><keyname>Fouquere</keyname><forenames>Christophe</forenames></author><author><keyname>Mogbil</keyname><forenames>Virgile</forenames></author></authors><title>Modules and Logic Programming</title><categories>cs.LO</categories><abstract>  We study conditions for a concurrent construction of proof-nets in the
framework developed by Andreoli in recent papers. We define specific
correctness criteria for that purpose. We first study closed modules (i.e.
validity of the execution of a logic program), then extend the criterion to
open modules (i.e. validity during the execution) distinguishing criteria for
acyclicity and connectability in order to allow incremental verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411030</id><created>2004-11-10</created><authors><author><keyname>Bergamo</keyname><forenames>Pina</forenames></author><author><keyname>D'Arco</keyname><forenames>Paolo</forenames></author><author><keyname>De Santis</keyname><forenames>Alfredo</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Security of public key cryptosystems based on Chebyshev Polynomials</title><categories>cs.CR</categories><comments>Submitted for publication</comments><abstract>  Chebyshev polynomials have been recently proposed for designing public-key
systems. Indeed, they enjoy some nice chaotic properties, which seem to be
suitable for use in Cryptography. Moreover, they satisfy a semi-group property,
which makes possible implementing a trapdoor mechanism. In this paper we study
a public key cryptosystem based on such polynomials, which provides both
encryption and digital signature. The cryptosystem works on real numbers and is
quite efficient. Unfortunately, from our analysis it comes up that it is not
secure. We describe an attack which permits to recover the corresponding
plaintext from a given ciphertext. The same attack can be applied to produce
forgeries if the cryptosystem is used for signing messages. Then, we point out
that also other primitives, a Diffie-Hellman like key agreement scheme and an
authentication scheme, designed along the same lines of the cryptosystem, are
not secure due to the aforementioned attack. We close the paper by discussing
the issues and the possibilities of constructing public key cryptosystems on
real numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411031</id><created>2004-11-10</created><authors><author><keyname>Pratt-Hartmann</keyname><forenames>Ian</forenames></author></authors><title>Complexity of the Two-Variable Fragment with (Binary-Coded) Counting
  Quantifiers</title><categories>cs.LO</categories><comments>24 pages, 1 pstex_t figure</comments><acm-class>F.4.1</acm-class><abstract>  We show that the satisfiability and finite satisfiability problems for the
two-variable fragment of first-order logic with counting quantifiers are both
in NEXPTIME, even when counting quantifiers are coded succinctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411032</id><created>2004-11-11</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Logic Column 10: Specifying Confidentiality</title><categories>cs.LO</categories><comments>12 pages</comments><acm-class>F,4.1; D.4.6</acm-class><journal-ref>SIGACT News, 35(4), pp. 72-83, 2004</journal-ref><abstract>  This article illustrates the use of a logical specification language to
capture various forms of confidentiality properties used in the security
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411033</id><created>2004-11-11</created><authors><author><keyname>Moscu</keyname><forenames>Mircea Alexandru Popescu</forenames></author></authors><title>On Invariance and Convergence in Time Complexity theory</title><categories>cs.CC</categories><abstract>  This article introduces three invariance principles under which P is
different from NP. In the second part a theorem of convergence is proven. This
theorem states that for any language L there exists an infinite sequence of
languages from O(n) that converges to L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411034</identifier>
 <datestamp>2008-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411034</id><created>2004-11-11</created><updated>2008-08-04</updated><authors><author><keyname>Das</keyname><forenames>Balaram</forenames></author></authors><title>Generating Conditional Probabilities for Bayesian Networks: Easing the
  Knowledge Acquisition Problem</title><categories>cs.AI</categories><comments>24pages, 2figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of probability distributions required to populate a conditional
probability table (CPT) in a Bayesian network, grows exponentially with the
number of parent-nodes associated with that table. If the table is to be
populated through knowledge elicited from a domain expert then the sheer
magnitude of the task forms a considerable cognitive barrier. In this paper we
devise an algorithm to populate the CPT while easing the extent of knowledge
acquisition. The input to the algorithm consists of a set of weights that
quantify the relative strengths of the influences of the parent-nodes on the
child-node, and a set of probability distributions the number of which grows
only linearly with the number of associated parent-nodes. These are elicited
from the domain expert. The set of probabilities are obtained by taking into
consideration the heuristics that experts use while arriving at probabilistic
estimations. The algorithm is used to populate the CPT by computing appropriate
weighted sums of the elicited distributions. We invoke the methods of
information geometry to demonstrate how these weighted sums capture the
expert's judgemental strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411035</id><created>2004-11-12</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author></authors><title>A FP-Tree Based Approach for Mining All Strongly Correlated Pairs
  without Candidate Generation</title><categories>cs.DB cs.AI</categories><report-no>TR-04-06</report-no><abstract>  Given a user-specified minimum correlation threshold and a transaction
database, the problem of mining all-strong correlated pairs is to find all item
pairs with Pearson's correlation coefficients above the threshold . Despite the
use of upper bound based pruning technique in the Taper algorithm [1], when the
number of items and transactions are very large, candidate pair generation and
test is still costly. To avoid the costly test of a large number of candidate
pairs, in this paper, we propose an efficient algorithm, called Tcp, based on
the well-known FP-tree data structure, for mining the complete set of
all-strong correlated item pairs. Our experimental results on both synthetic
and real world datasets show that, Tcp's performance is significantly better
than that of the previously developed Taper algorithm over practical ranges of
correlation threshold specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411036</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411036</id><created>2004-11-12</created><updated>2005-09-02</updated><authors><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Feedback Capacity of the First-Order Moving Average Gaussian Channel</title><categories>cs.IT math.IT</categories><comments>Updated version, 36 pages, 4 figures, submitted to IEEE Trans.
  Inform. Theory</comments><acm-class>H.1.1; E.4</acm-class><abstract>  The feedback capacity of the stationary Gaussian additive noise channel has
been open, except for the case where the noise is white. Here we find the
feedback capacity of the stationary first-order moving average additive
Gaussian noise channel in closed form. Specifically, the channel is given by
$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\{X_i\}$ satisfies a power
constraint and the noise $\{Z_i\}$ is a first-order moving average Gaussian
process defined by $Z_i = \alpha U_{i-1} + U_i,$ $|\alpha| \le 1,$ with white
Gaussian innovations $U_i,$ $i = 0,1,....$
  We show that the feedback capacity of this channel is $-\log x_0,$ where
$x_0$ is the unique positive root of the equation $ \rho x^2 = (1-x^2) (1 -
|\alpha|x)^2,$ and $\rho$ is the ratio of the average input power per
transmission to the variance of the noise innovation $U_i$. The optimal coding
scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath
for the additive white Gaussian noise channel -- the transmitter sends a
real-valued information-bearing signal at the beginning of communication and
subsequently refines the receiver's error by processing the feedback noise
signal through a linear stationary first-order autoregressive filter. The
resulting error probability of the maximum likelihood decoding decays
doubly-exponentially in the duration of the communication. This feedback
capacity of the first-order moving average Gaussian channel is very similar in
form to the best known achievable rate for the first-order
\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and
Tiernan, although the optimality of the latter is yet to be established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411037</id><created>2004-11-12</created><updated>2004-11-12</updated><authors><author><keyname>Matsui</keyname><forenames>Tetsushi</forenames></author></authors><title>A Note on Bulk Quantum Turing Machine</title><categories>cs.CC</categories><comments>8 pages</comments><abstract>  Recently, among experiments for realization of quantum computers, NMR quantum
computers have achieved the most impressive succession. There is a model of the
NMR quantum computation,namely Atsumi and Nishino's bulk quantum Turing
Machine. It assumes, however, an unnatural assumption with quantum mechanics.
We, then, define a more natural and quantum mechanically realizable modified
bulk quantum Turing Machine, and show its computational ability by comparing
complexity classes with quantum Turing Machine's counter part.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411038</id><created>2004-11-15</created><authors><author><keyname>Shajeemohan</keyname><forenames>B. S.</forenames></author></authors><title>Impact of IT on Higher education Through Continuing Education</title><categories>cs.CY</categories><abstract>  Information Technology is emerging to be the technology of 21st century. The
paradigm shift from industrial society to information society had already
become a reality! It is indeed high time to think about integrating IT in all
facets of education -- may it be in secondary level, or be it in reskilling the
employed ones. This paper discusses various issues in incorporating IT in
various levels of education, and the need to think about a task force to
counter the so-called slow down and recession in IT industry. The opportunities
for aspiring IT professionals were also discussed. The importance of reskilling
as a continuing education programme to make the people aware of the changing
trends in IT was also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411039</id><created>2004-11-12</created><authors><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author><author><keyname>Richter</keyname><forenames>Owen</forenames></author><author><keyname>Schwiebert</keyname><forenames>Loren</forenames></author><author><keyname>Zeadally</keyname><forenames>Sherali</forenames></author></authors><title>Using Wireless Sensor Networks to Narrow the Gap between Low-Level
  Information and Context-Awareness</title><categories>cs.NI</categories><comments>Published at ISCA 17th International Conference on Computers and
  Their Applications, CATA 2002, April 2002, San Francisco, California, USA. 6
  pages</comments><acm-class>C.2.1</acm-class><abstract>  Wireless sensor networks are finally becoming a reality. In this paper, we
present a scalable architecture for using wireless sensor networks in
combination with wireless Ethernet networks to provide a complete end-to-end
solution to narrow the gap between the low-level information and context
awareness. We developed and implemented a complete proximity detector in order
to give a wearable computer, such as a PDA, location context. Since location is
only one element of contextawareness, we pursued utilizing photo sensors and
temperature sensors in learning as much as possible about the environment. We
used the TinyOS RF Motes as our test bed WSN (Wireless Sensor Network), 802.11
compatible hardware as our wireless Ethernet network, and conventional PCs and
wired 802.3 networks to build the upper levels of the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411040</id><created>2004-11-12</created><authors><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author></authors><title>Efficient Even Distribution of Power Consumption in Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>Published at ISCA 18th International Conference on Computers and
  Their Applications, CATA 2003, March 2003, Honolulu, Hawaii, USA. 4 pages</comments><acm-class>C.2.1</acm-class><abstract>  One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. We investigate a new routing algorithm that uses diffusion in
order to achieve relatively even power dissipation throughout a wireless sensor
network by making good local decisions. We leverage from concepts of
peer-to-peer networks in which the system acts completely decentralized and all
nodes in the network are equal peers. Our algorithm utilizes the node load,
power levels, and spatial information in order to make the optimal routing
decision. According to our preliminary experimental results, our proposed
algorithm performs well according to its goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411041</id><created>2004-11-15</created><updated>2006-06-19</updated><authors><author><keyname>Thampi</keyname><forenames>Sabu . M</forenames></author><author><keyname>Sekaran</keyname><forenames>K. Chandra</forenames></author></authors><title>Content Based Image Retrieval with Mobile Agents and Steganography</title><categories>cs.CR</categories><comments>6 pages, 10 figures</comments><abstract>  In this paper we present an image retrieval system based on Gabor texture
features, steganography, and mobile agents.. By employing the information
hiding technique, the image attributes can be hidden in an image without
degrading the image quality. Thus the image retrieval process becomes simple.
Java based mobile agents manage the query phase of the system. Based on the
simulation results, the proposed system not only shows the efficiency in hiding
the attributes but also provides other advantages such as: (1) fast
transmission of the retrieval image to the receiver, (2) searching made easy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411042</id><created>2004-11-12</created><authors><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author></authors><title>An Empirical Analysis of Internet Protocol Version 6 (IPv6)</title><categories>cs.NI cs.PF</categories><comments>Master Thesis, May 2002, Computer Science Department, Wayne State
  University, Detroit, Michigan, USA, 146 pages</comments><acm-class>C.2.2; C.2.5</acm-class><abstract>  Although the current Internet Protocol known as IPv4 has served its purpose
for over 20 years, its days are numbered. With IPv6 reaching a mature enough
level, there is a need to evaluate the performance benefits or drawbacks that
the new IPv6 protocol will have in comparison to the well established IPv4
protocol. Theoretically, the overhead between the two different protocols
should be directly proportional to the difference in the packet's header size,
however according to our findings, the empirical performance difference between
IPv4 and IPv6, especially when the transition mechanisms are taken into
consideration, is much larger than anticipated. We first examine the
performance of each protocol independently. We then examined two transition
mechanisms which perform the encapsulation at various points in the network:
host-to-host and router-to-router (tunneling). Our experiments were conducted
using two dual stack (IPv4/IPv6) routers using end nodes running both Windows
2000 and Solaris 8.0 in order to compare two different IPv6 implementations
side by side. Our tests were written in C++ and utilized metrics such as
latency, throughput, CPU utilization, socket creation time, socket connection
time, web server simulation, and a video client/server application for TCP/UDP
in IPv4/IPv6 under both Windows 2000 and Solaris 8.0. Our empirical evaluation
proved that IPv6 is not yet a mature enough technology and that it is still
years away from having consistent and good enough implementations, as the
performance of IPv6 in many cases proved to be significantly worse than IPv4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411043</id><created>2004-11-12</created><authors><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author><author><keyname>Schwiebert</keyname><forenames>Loren</forenames></author><author><keyname>Fowler</keyname><forenames>Scott</forenames></author><author><keyname>Gupta</keyname><forenames>Sandeep K. S.</forenames></author></authors><title>e3D: An Energy-Efficient Routing Algorithm for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>Accepted for publication at IEEE ISSNIP 2004 (The International
  Conference on Intelligent Sensors, Sensor Networks and Information
  Processing), Melbourne, Australia, December 2004. 6 pages</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. Any communication protocol that involves synchronization of peer
nodes incurs some overhead for setting up the communication. We introduce a new
algorithm, e3D (energy-efficient Distributed Dynamic Diffusion routing
algorithm), and compare it to two other algorithms, namely directed, and random
clustering communication. We take into account the setup costs and analyze the
energy-efficiency and the useful lifetime of the system. In order to better
understand the characteristics of each algorithm and how well e3D really
performs, we also compare e3D with its optimum counterpart and an optimum
clustering algorithm. The benefit of introducing these ideal algorithms is to
show the upper bound on performance at the cost of an astronomical prohibitive
synchronization costs. We compare the algorithms in terms of system lifetime,
power dissipation distribution, cost of synchronization, and simplicity of the
algorithm. Our simulation results show that e3D performs comparable to its
optimal counterpart while having significantly less overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411044</id><created>2004-11-12</created><authors><author><keyname>Raicu</keyname><forenames>Ioan</forenames></author></authors><title>Routing Algorithms for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>Poster at Grace Hopper Celebration of Women in Computing 2002,
  GHC2002, October 2002, British Columbia, Canada</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  Our contribution in this paper is e3D, a diffusion based routing protocol
that prolongs the system lifetime, evenly distributes the power dissipation
throughout the network, and incurs minimal overhead for synchronizing
communication. We compare e3D with other algorithms in terms of system
lifetime, power dissipation distribution, cost of synchronization, and
simplicity of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411045</id><created>2004-11-12</created><authors><author><keyname>Dumitrescu</keyname><forenames>Catalin</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>Usage Policy-based CPU Sharing in VOs</title><categories>cs.DC</categories><abstract>  Resource sharing within Grid collaborations usually implies specific sharing
mechanisms at participating sites. Challenging policy issues can arise within
virtual organizations (VOs) that integrate participants and resources spanning
multiple physical institutions. Resource owners may wish to grant to one or
more VOs the right to use certain resources subject to local policy and service
level agreements, and each VO may then wish to use those resources subject to
VO policy. Thus, we must address the question of what usage policies (UPs)
should be considered for resource sharing in VOs. As a first step in addressing
this question, we develop and evaluate different UP scenarios within a
specialized context that mimics scientific Grids within which the resources to
be shared are computers. We also present a UP architecture and define roles and
functions for scheduling resources in such grid environments while satisfying
resource owner policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411046</id><created>2004-11-15</created><updated>2006-01-16</updated><authors><author><keyname>Bridgewater</keyname><forenames>Jesse S. A.</forenames></author><author><keyname>Boykin</keyname><forenames>P. Oscar</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani P.</forenames></author></authors><title>Balanced Overlay Networks (BON): Decentralized Load Balancing via
  Self-Organized Random Networks</title><categories>cs.DC</categories><comments>13 pages, 12 figures. Draft submitted to IEEE TPDS</comments><acm-class>C.2.4</acm-class><abstract>  We present a novel framework, called balanced overlay networks (BON), that
provides scalable, decentralized load balancing for distributed computing using
large-scale pools of heterogeneous computers. Fundamentally, BON encodes the
information about each node's available computational resources in the
structure of the links connecting the nodes in the network. This distributed
encoding is self-organized, with each node managing its in-degree and local
connectivity via random-walk sampling. Assignment of incoming jobs to nodes
with the most free resources is also accomplished by sampling the nodes via
short random walks. Extensive simulations show that the resulting highly
dynamic and self-organized graph structure can efficiently balance
computational load throughout large-scale networks. These simulations cover a
wide spectrum of cases, including significant heterogeneity in available
computing resources and high burstiness in incoming load. We provide analytical
results that prove BON's scalability for truly large-scale networks: in
particular we show that under certain ideal conditions, the network structure
converges to Erdos-Renyi (ER) random graphs; our simulation results, however,
show that the algorithm does much better, and the structures seem to approach
the ideal case of d-regular random graphs. We also make a connection between
highly-loaded BONs and the well-known ball-bin randomized load balancing
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411047</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411047</id><created>2004-11-15</created><updated>2005-06-04</updated><authors><author><keyname>Erturk</keyname><forenames>E.</forenames></author><author><keyname>Corke</keyname><forenames>T. C.</forenames></author><author><keyname>Gokcol</keyname><forenames>C.</forenames></author></authors><title>Numerical Solutions of 2-D Steady Incompressible Driven Cavity Flow at
  High Reynolds Numbers</title><categories>cs.NA math.NA physics.comp-ph physics.flu-dyn</categories><journal-ref>International Journal for Numerical Methods in Fluids 2005, Vol
  48, pp 747-774</journal-ref><doi>10.1002/fld.953</doi><abstract>  Numerical calculations of the 2-D steady incompressible driven cavity flow
are presented. The Navier-Stokes equations in streamfunction and vorticity
formulation are solved numerically using a fine uniform grid mesh of 601x601.
The steady driven cavity solutions are computed for Re&lt;21,000 with a maximum
absolute residuals of the governing equations that were less than 10-10. A new
quaternary vortex at the bottom left corner and a new tertiary vortex at the
top left corner of the cavity are observed in the flow field as the Reynolds
number increases. Detailed results are presented and comparisons are made with
benchmark solutions found in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411048</identifier>
 <datestamp>2009-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411048</id><created>2004-11-15</created><updated>2009-06-17</updated><authors><author><keyname>Erturk</keyname><forenames>E.</forenames></author></authors><title>Discussions on Driven Cavity Flow</title><categories>cs.NA physics.comp-ph physics.flu-dyn</categories><journal-ref>International Journal for Numerical Methods in Fluids 2009, Vol
  60, pp 275-294</journal-ref><doi>10.1002/fld.1887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widely studied benchmark problem, 2-D driven cavity flow problem is
discussed in details in terms of physical and mathematical and also numerical
aspects. A very brief literature survey on studies on the driven cavity flow is
given. Based on the several numerical and experimental studies, the fact of the
matter is, above moderate Reynolds numbers physically the flow in a driven
cavity is not two-dimensional. However there exist numerical solutions for 2-D
driven cavity flow at high Reynolds numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411049</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411049</id><created>2004-11-15</created><updated>2005-12-22</updated><authors><author><keyname>Erturk</keyname><forenames>E.</forenames></author><author><keyname>Gokcol</keyname><forenames>C.</forenames></author></authors><title>Fourth Order Compact Formulation of Navier-Stokes Equations and Driven
  Cavity Flow at High Reynolds Numbers</title><categories>cs.NA math.NA physics.comp-ph physics.flu-dyn</categories><journal-ref>International Journal for Numerical Methods in Fluids 2006, Vol
  50, pp 421-436</journal-ref><doi>10.1002/fld.1061</doi><abstract>  A new fourth order compact formulation for the steady 2-D incompressible
Navier-Stokes equations is presented. The formulation is in the same form of
the Navier-Stokes equations such that any numerical method that solve the
Navier-Stokes equations can also be applied to this fourth order compact
formulation. In particular in this work the formulation is solved with an
efficient numerical method that requires the solution of tridiagonal systems
using a fine grid mesh of 601x601. Using this formulation, the steady 2-D
incompressible flow in a driven cavity is solved up to Reynolds number of
20,000 with fourth order spatial accuracy. Detailed solutions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411050</id><created>2004-11-15</created><authors><author><keyname>Nathan</keyname><forenames>Darran</forenames></author><author><keyname>Clemens</keyname><forenames>Ralf</forenames></author></authors><title>Utilizing Reconfigurable Hardware Processors via Grid Services</title><categories>cs.DC cs.AR</categories><comments>3 pages, 8 figures</comments><abstract>  Computational grids typically consist of nodes utilizing ordinary processors
such as the Intel Pentium. Field Programmable Gate Arrays (FPGAs) are able to
perform certain compute-intensive tasks very well due to their inherent
parallel architecture, often resulting in orders of magnitude speedups. This
paper explores how FPGAs can be transparently exposed for remote use via grid
services, by integrating the Proteus Software Platform with the Globus Toolkit
3.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411051</id><created>2004-11-15</created><authors><author><keyname>Leguay</keyname><forenames>Jeremie</forenames></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames></author><author><keyname>Friedman</keyname><forenames>Timur</forenames></author><author><keyname>Salamatian</keyname><forenames>Kave</forenames></author></authors><title>Describing and Simulating Internet Routes</title><categories>cs.NI</categories><comments>13 pages</comments><abstract>  This paper introduces relevant statistics for the description of routes in
the internet, seen as a graph at the interface level. Based on the observed
properties, we propose and evaluate methods for generating artificial routes
suitable for simulation purposes. The work in this paper is based upon a study
of over seven million route traces produced by CAIDA's skitter infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411052</id><created>2004-11-17</created><authors><author><keyname>Soula</keyname><forenames>H.</forenames></author><author><keyname>Beslon</keyname><forenames>G.</forenames></author><author><keyname>Mazet</keyname><forenames>O.</forenames></author></authors><title>Spontaneous Dynamics of Asymmetric Random Recurrent Spiking Neural
  Networks</title><categories>cs.NE math.PR</categories><comments>28 pages, 7 figures</comments><abstract>  We study in this paper the effect of an unique initial stimulation on random
recurrent networks of leaky integrate and fire neurons. Indeed given a
stochastic connectivity this so-called spontaneous mode exhibits various non
trivial dynamics. This study brings forward a mathematical formalism that
allows us to examine the variability of the afterward dynamics according to the
parameters of the weight distribution. Provided independence hypothesis (e.g.
in the case of very large networks) we are able to compute the average number
of neurons that fire at a given time -- the spiking activity. In accordance
with numerical simulations, we prove that this spiking activity reaches a
steady-state, we characterize this steady-state and explore the transients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411053</id><created>2004-11-17</created><authors><author><keyname>Flissi</keyname><forenames>Areski</forenames><affiliation>JACQUARD Ur-F Lifl</affiliation></author><author><keyname>Merle</keyname><forenames>Philippe</forenames><affiliation>JACQUARD Ur-F Lifl</affiliation></author></authors><title>Vers un environnement multi personnalites pour la configuration et le
  deploiement d'applications a base de composants logiciels</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003276</proxy><journal-ref>DECOR04 (2004) 3-14</journal-ref><abstract>  The multiplication of architecture description languages, component models
and platforms implies a serious dilemma for component based software
architects. On the one hand, they have to choose a language to describe
concrete configurations which will be automatically deployed on execution
platforms. On the other hand, they wish to capitalize their software
architectures independently of any description languages or platforms. To solve
this problem, we propose a multi personalities environment for the
configuration and the deployment of component based applications. This
environment is composed of a core capturing a canonical model of configuration
and deployment, and a set of personalities tailored to languages and platforms.
This paper details the architecture of such an environment and describes the
personalities for the CORBA and Fractal component models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411054</id><created>2004-11-17</created><authors><author><keyname>Exertier</keyname><forenames>Francois</forenames></author></authors><title>J2EE Deployment: The JOnAS Case Study</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003278</proxy><journal-ref>DECOR04 (2004) 27-36</journal-ref><abstract>  La specification J2EE (Java 2 platform Enterprise Edition) definit une
architecture de serveur d'application Java. Jusqu'a J2EE 1.3, seuls les aspects
de deploiement concernant le developpeur d'applications etaient adresses. Avec
J2EE 1.4, les interfaces et les etapes de deploiement ont ete plus precisement
specifiees dans la specification &quot;J2EE Deployment&quot;. JOnAS (Java Open
Application Server) est une plate-forme J2EE developpee au sein du consortium
ObjectWeb. Les aspects deploiement sont en cours de developpement. Cet article
decrit les concepts lies au deploiement dans J2EE, ainsi que les problematiques
levees lors de leur mise en oeuvre pour JOnAS. Il n'a pas pour but de presenter
un travail abouti, mais illustre le deploiement par un cas concret et ebauche
une liste de besoins non encore satisfaits dans le domaine.
  -----
  The J2EE (Java 2 platform Enterprise Edition) specification defines an
architecture for Java Application Servers. Until J2EE 1.3, the deployment
aspect was addressed from the developer point of view only. Since J2EE 1.4,
deployment APIs and steps have been more precisely specified within the &quot;J2EE
Deployment Specification&quot;. JOnAS (Java Open Application Server) is a J2EE
platform implementation by ObjectWeb. The deployment aspects are under
development. This article describes the J2EE Deployment concepts, and the
issues raised when implementing deployment features within JOnAS. It does not
provide a complete solution, but illustrates deployment through a concrete
example and initiates a list of non fulfilled requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411055</id><created>2004-11-17</created><authors><author><keyname>Charles</keyname><forenames>Laurent</forenames></author><author><keyname>Vacelet</keyname><forenames>Manuel</forenames></author><author><keyname>Chaari</keyname><forenames>Mohamed</forenames></author><author><keyname>Santana</keyname><forenames>Miguel</forenames></author></authors><title>SDS : Une infrastructure d'installation de logiciels libres pour des
  organisations multi-sites</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003280</proxy><journal-ref>DECOR04 (2004) 37-48</journal-ref><abstract>  Les developpements logiciels sur les systemes UNIX font de plus en plus appel
aux logiciels libres. Nous proposons une solution de deploiement et de controle
de ces logiciels libres au sein d'une grande organisation. Nous nous attachons
particulierement a resoudre les problemes lies au deploiement multi-sites ainsi
qu'a la gestion de configuration de ces deploiements. L'originalite de notre
approche repose sur sa capacite a etre mise en oeuvre et controlee par les
utilisateurs plutot que par les administrateurs, sans necessiter d'expertise
particuliere, et par les possibilites de deploiement dans des environnements
heterogenes.
  -----
  Free and open source software is more and more used for software developments
on UNIX systems. We are proposing a solution to control the deployment of free
software in the context of a large corporation, focusing on multi-site
deployment and configuration management. The originality of our approach rests
on its ability to be implemented and controlled by users rather than
administrators, without requiring any particular expertise, and on its facility
to be deployed in heterogeneous environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411056</id><created>2004-11-17</created><authors><author><keyname>Cremene</keyname><forenames>Marcel</forenames></author><author><keyname>Riveill</keyname><forenames>Michel</forenames></author><author><keyname>Martel</keyname><forenames>Christian</forenames></author><author><keyname>Loghin</keyname><forenames>Calin</forenames></author><author><keyname>Miron</keyname><forenames>Costin</forenames></author></authors><title>Adaptation dynamique de services</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003282</proxy><journal-ref>DECOR04 (2004) 53-64</journal-ref><abstract>  This paper proposes a software architecture for dynamical service adaptation.
The services are constituted by reusable software components. The adaptation's
goal is to optimize the service function of their execution context. For a
first step, the context will take into account just the user needs but other
elements will be added. A particular feature in our proposition is the profiles
that are used not only to describe the context's elements but also the
components itself. An Adapter analyzes the compatibility between all these
profiles and detects the points where the profiles are not compatibles. The
same Adapter search and apply the possible adaptation solutions: component
customization, insertion, extraction or replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411057</id><created>2004-11-17</created><authors><author><keyname>Matevska-Meyer</keyname><forenames>Jasminka</forenames></author><author><keyname>Olliges</keyname><forenames>Sascha</forenames></author><author><keyname>Hasselbring</keyname><forenames>Wilhelm</forenames></author></authors><title>Runtime Reconfiguration of J2EE Applications</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003285</proxy><journal-ref>DECOR04 (2004) 77-84</journal-ref><abstract>  Runtime reconfiguration considered as &quot;applying required changes to a running
system&quot; plays an important role for providing high availability not only of
safety- and mission-critical systems, but also for commercial web-applications
offering professional services. Hereby, the main concerns are maintaining the
consistency of the running system during reconfiguration and minimizing its
down-time caused by the reconfiguration. This paper focuses on the platform
independent subsystem that realises deployment and redeployment of J2EE modules
based on the new J2EE Deployment API as a part of the implementation of our
proposed system architecture enabling runtime reconfiguration of
component-based systems. Our &quot;controlled runtime redeployment&quot; comprises an
extension of hot deployment and dynamic reloading, complemented by allowing for
structural change
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411058</id><created>2004-11-17</created><authors><author><keyname>Ruiz</keyname><forenames>Jose L.</forenames></author><author><keyname>Duenas</keyname><forenames>Juan C.</forenames></author><author><keyname>Usero</keyname><forenames>Fernando</forenames></author><author><keyname>Diaz</keyname><forenames>Cristina</forenames></author></authors><title>Deployment in dynamic environments</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003286</proxy><journal-ref>DECOR04 (2004) 85-98</journal-ref><abstract>  Information and communication technologies are moving towards a new stage
where applications will be dynamically deployed, uninstalled, updated and
(re)configured. Several approaches have been followed with the goal of creating
a fully automated and context-aware deployment system. Ideally, this system
should be capable of handling the dynamics of this new situation, without
losing sight of other factors, such as performance, security, availability or
scalability. We will take some of the technologies that follow the principles
of Service Oriented Architectures, SOA, as a paradigm of dynamic environments.
SOA promote the breaking down of applications into sets of loosely coupled
elements, called services. Services can be dynamically bound, deployed,
reconfigured, uninstalled and updated. First of all, we will try to offer a
broad view on the specific deployment issues that arise in these environments.
Later on, we will present our approach to the problem. One of the essential
points that has to be tackled to develop an automated deployment engine will be
to have enough information to carry out tasks without human intervention. In
the article we will focus on the format and contents of deployment descriptors.
Additionally, we will go into the details of the deployment framework for OSGi
enabled gateways that has been developed by our research group. Finally we will
give some concluding remarks and some ideas for future work
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411059</id><created>2004-11-17</created><authors><author><keyname>Briclet</keyname><forenames>Frederic</forenames><affiliation>JACQUARD Ur-F Lifl</affiliation></author><author><keyname>Contreras</keyname><forenames>Christophe</forenames><affiliation>JACQUARD Ur-F Lifl</affiliation></author><author><keyname>Merle</keyname><forenames>Philippe</forenames><affiliation>JACQUARD Ur-F Lifl</affiliation></author></authors><title>OpenCCM : une infrastructure a composants pour le deploiement
  d'applications a base de composants CORBA</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003287</proxy><journal-ref>DECOR04 (2004) 101-112</journal-ref><abstract>  Deployment of software components for building distributed applications
consists of the coordination of a set of basic tasks like uploading component
binaries to the execution sites, loading them in memory, instantiating
components, interconnecting their ports, setting their business and technical
attributes. The automation of the deployment process then requires the presence
of a software infrastructure distributed itself on the different execution
sites. This paper presents the characteristics of such an infrastructure for
the deployment of CORBA component-based applications. This latter is designed
and implemented in the context of our OpenCCM platform, an open source
implementation of the CORBA Component Model. The main characteristic lays on
the fact that this infrastructure is itself designed as a set of CORBA
component assemblies. This allows its dynamic assembly during its deployment
over the execution sites
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411060</id><created>2004-11-17</created><authors><author><keyname>Frenot</keyname><forenames>Stephane</forenames><affiliation>ARES UR-RA</affiliation></author></authors><title>Gestion du deploiement de composants sur reseau P2P</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003288</proxy><journal-ref>DECOR04 (2004) 113-124</journal-ref><abstract>  The deployment of component-based applications relies on a centralized
directory to store the components. This paper describes an approach to
distribute software components to be deployed on a set of peers of a peer to
peer network in order to exploit some associated characteristics (load
balancing, fault-tolerance, self-organisation). The proposed architecture is
situated in the context of OSGI application deployment management. The software
components (bundles) are distributed among a set of nodes participating in the
execution of services. When a node wants to install a component which is not
deployed locally, the component is looked for and installed using a p2p
network.
  -----
  Le deploiement d'applications a composants repose sur une approche d'annuaire
centralise de stockage des composants. Cet article decrit une approche pour
distribuer les composants logiciels a deployer sur un ensemble de noeuds d'un
reseau pair-a-pair afin de pouvoir exploiter certaines caracteristiques
associees (equilibrage de charge, tolerance de panne, auto-organisation).
L'architecture proposee entre dans le cadre de la gestion du deploiement
d'applications sur le modele OSGi. Les composants logiciels (ou bundles) sont
repartis a travers un ensemble de noeuds participant a l'execution de services.
Lorsqu'un noeud veut installer un composant et si celui-ci n'est pas encore
deploye localement, il est recherche et installe en utilisant un reseau p2p
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411061</id><created>2004-11-17</created><authors><author><keyname>Merle</keyname><forenames>Noelle</forenames><affiliation>LSR - IMAG</affiliation></author></authors><title>Un meta-modele pour l'automatisation du deploiement d'applications
  logicielles</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003289</proxy><journal-ref>DECOR04 (2004) 125-132</journal-ref><abstract>  Le deploiement est maintenant considere comme une activite a part entiere du
cycle de vie du logiciel. Les grandes entreprises souhaitent pouvoir
automatiser cette etape tout en prenant en compte les caracteristiques de
chaque machine cible. Pour repondre a ces besoins, nous avons defini un
environnement de deploiement : ORYA (Open enviRonment to deploY Applications).
Cet environnement utilise un meta-modele de deploiement, decrit dans ce papier.
Notre approche utilise aussi les technologies des federations et des procedes,
fournissant un environnement flexible et extensible pour l'utilisateur.
  -----
  The deployment is now a full activity of the software lifecycle. Large
enterprises want to automate this step, taking into account characteristics of
each target machine. To satisfy these needs, we have defined an environment for
the deployment phase: ORYA (Open enviRonment to deploY Applications). This
environment uses a deployment metamodel, described in this paper. Our approach
is based also on federation and process federations, providing a flexible and
extensible environment to the user
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411062</id><created>2004-11-17</created><authors><author><keyname>Donsez</keyname><forenames>Didier</forenames><affiliation>LSR - IMAG</affiliation></author><author><keyname>Cervantes</keyname><forenames>Humberto</forenames><affiliation>LSR - IMAG</affiliation></author><author><keyname>Desertot</keyname><forenames>Mikael</forenames><affiliation>LSR - IMAG</affiliation></author></authors><title>FROGi : Deploiement de composants Fractal sur OSGi</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003291</proxy><journal-ref>DECOR04 (2004) 147-158</journal-ref><abstract>  Cet article presente FROGi, une proposition visant a introduire le modele a
composants Fractal a l'interieur de la plateforme de services OSGi. La
motivation derriere ce travail est double. D'un cote, FROGi offre aux
developpeurs de services OSGi un modele a composants extensibles qui facilite
le developpement des bundles ; ces derniers restent toutefois compatibles avec
les bundles &quot;patrimoniaux&quot;. D'un autre cote, FROGi beneficie de
l'infrastructure de deploiement que represente OSGi et qui facilite la
realisation du conditionnement et du deploiement de composants Fractal. Dans
FROGi, une application Fractal est conditionnee sous la forme d'un ou plusieurs
bundles et elle peut etre deployee de facon partielle et les activites de
deploiement peuvent avoir lieu de facon continue.
  -- This paper presents FROGi, a proposal to introduce the Fractal component
model into the OSGi services platform. There are two motivations for this work.
The first one is to offer a flexible component model to the OSGi developers to
simplify bundle development. Bundles developed with FROGi are nevertheless
compatible with standard bundles. The second motivation is to leverage OSGi's
deployment capabilities to package and deploy Fractal components. In FROGi, a
Fractal application is packaged and delivered as a set of OSGi bundles; such an
application supports partial deployment and additionally, deployment activities
can occur continuously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411063</id><created>2004-11-17</created><authors><author><keyname>Lechner</keyname><forenames>Christiane</forenames></author><author><keyname>Alic</keyname><forenames>Dana</forenames></author><author><keyname>Husa</keyname><forenames>Sascha</forenames></author></authors><title>From Tensor Equations to Numerical Code -- Computer Algebra Tools for
  Numerical Relativity</title><categories>cs.SC</categories><comments>LaTeX llncs style, 9 pages, 1 figure, to appaer in the proceedings of
  &quot;SYNASC 2004 - 6th International Symposium on Symbolic and Numeric Algorithms
  for Scientific Computing&quot;, Timisoara, Romania, September 26-30 2004</comments><report-no>AEI-2004-108</report-no><abstract>  In this paper we present our recent work in developing a computer-algebra
tool for systems of partial differential equations (PDEs), termed &quot;Kranc&quot;. Our
work is motivated by the problem of finding solutions of the Einstein equations
through numerical simulations. Kranc consists of Mathematica based
computer-algebra packages, that facilitate the task of dealing with symbolic
tensorial calculations and realize the conversion of systems of partial
differential evolution equations into parallelized C or Fortran code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411064</id><created>2004-11-17</created><updated>2005-05-13</updated><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Emek</keyname><forenames>Yuval</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Lower-Stretch Spanning Trees</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.2.2</acm-class><abstract>  We prove that every weighted graph contains a spanning tree subgraph of
average stretch O((log n log log n)^2). Moreover, we show how to construct such
a tree in time O(m log^2 n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411065</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411065</id><created>2004-11-18</created><updated>2005-05-26</updated><authors><author><keyname>Wiesmaier</keyname><forenames>A.</forenames></author><author><keyname>Lippert</keyname><forenames>M.</forenames></author><author><keyname>Karatsiolis</keyname><forenames>E.</forenames></author><author><keyname>Raptis</keyname><forenames>G.</forenames></author><author><keyname>Buchmann</keyname><forenames>J.</forenames></author></authors><title>An Evaluated Certification Services System for the German National Root
  CA - Legally Binding and Trustworthy Transactions in E-Business and
  E-Government</title><categories>cs.CR</categories><comments>6 pages; 1 figure; IEEE style; final version</comments><journal-ref>Proceedings of &quot;The 2005 International Conference on E-Business,
  Enterprise Information Systems, E-Government, and Outsourcing (EEE'05)&quot;; June
  2005</journal-ref><abstract>  National Root CAs enable legally binding E-Business and E-Government
transactions. This is a report about the development, the evaluation and the
certification of the new certification services system for the German National
Root CA. We illustrate why a new certification services system was necessary,
and which requirements to the new system existed. Then we derive the tasks to
be done from the mentioned requirements. After that we introduce the initial
situation at the beginning of the project. We report about the very process and
talk about some unfamiliar situations, special approaches and remarkable
experiences. Finally we present the ready IT system and its impact to
E-Business and E-Government.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411066</id><created>2004-11-18</created><authors><author><keyname>Karatsiolis</keyname><forenames>V.</forenames></author><author><keyname>Lippert</keyname><forenames>M.</forenames></author><author><keyname>Wiesmaier</keyname><forenames>A.</forenames></author></authors><title>Using LDAP Directories for Management of PKI Processes</title><categories>cs.CR</categories><comments>9 pages, 1 figure</comments><journal-ref>In Proceedings of Public Key Infrastructure: First European PKI
  Workshop: Research and Applications, EuroPKI 2004, volume 3093 of Lecture
  Notes in Computer Science, pages 126-134, June 2004</journal-ref><abstract>  We present a framework for extending the functionality of LDAP servers from
their typical use as a public directory in public key infrastructures. In this
framework the LDAP servers are used for administrating infrastructure
processes. One application of this framework is a method for providing
proof-of-possession, especially in the case of encryption keys. Another one is
the secure delivery of software personal security environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411067</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411067</id><created>2004-11-18</created><authors><author><keyname>Karatsiolis</keyname><forenames>V.</forenames></author><author><keyname>Lippert</keyname><forenames>M.</forenames></author><author><keyname>Wiesmaier</keyname><forenames>A.</forenames></author><author><keyname>Pitaev</keyname><forenames>A.</forenames></author><author><keyname>Ruppert</keyname><forenames>M.</forenames></author><author><keyname>Buchmann</keyname><forenames>J.</forenames></author></authors><title>Towards a Flexible Intra-Trustcenter Management Protocol</title><categories>cs.CR</categories><comments>12 pages, 0 figures; in The Third International Workshop for Applied
  PKI (IWAP2004)</comments><abstract>  This paper proposes the Intra Trustcenter Protocol (ITP), a flexible and
secure management protocol for communication between arbitrary trustcenter
components. Unlike other existing protocols (like PKCS#7, CMP or XKMS) ITP
focuses on the communication within a trustcenter. It is powerful enough for
transferring complex messages which are machine and human readable and easy to
understand. In addition it includes an extension mechanism to be prepared for
future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411068</id><created>2004-11-18</created><updated>2005-01-13</updated><authors><author><keyname>Karatsiolis</keyname><forenames>V.</forenames></author><author><keyname>Lippert</keyname><forenames>M.</forenames></author><author><keyname>Wiesmaier</keyname><forenames>A.</forenames></author></authors><title>Planning for Directory Services in Public Key Infrastructures</title><categories>cs.CR</categories><comments>12 pages; 1 figure; accepted at QSIG2005 (see
  http://www-sec.uni-regensburg.de/sicherheit2005/index.shtml); camera ready
  version</comments><journal-ref>Proceedings of &quot;Sicherheit 2005&quot;; April 2005</journal-ref><abstract>  In this paper we provide a guide for public key infrastructure designers and
administrators when planning for directory services. We concentrate on the LDAP
directories and how they can be used to successfully publish PKI information.
We analyse their available mechanisms and propose a best practice guide for use
in PKI. We then take a look into the German Signature Act and Ordinance and
discuss their part as far as directories concerning. Finally, we translate
those to the LDAP directories practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411069</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411069</id><created>2004-11-18</created><authors><author><keyname>Peng</keyname><forenames>Gang</forenames></author></authors><title>CDN: Content Distribution Network</title><categories>cs.NI cs.IR</categories><comments>26 pages</comments><acm-class>C.2.4 Distributed Systems;H.3.4 Systems and Software</acm-class><abstract>  Internet evolves and operates largely without a central coordination, the
lack of which was and is critically important to the rapid growth and evolution
of Internet. However, the lack of management in turn makes it very difficult to
guarantee proper performance and to deal systematically with performance
problems. Meanwhile, the available network bandwidth and server capacity
continue to be overwhelmed by the skyrocketing Internet utilization and the
accelerating growth of bandwidth intensive content. As a result, Internet
service quality perceived by customers is largely unpredictable and
unsatisfactory. Content Distribution Network (CDN) is an effective approach to
improve Internet service quality. CDN replicates the content from the place of
origin to the replica servers scattered over the Internet and serves a request
from a replica server close to where the request originates. In this paper, we
first give an overview about CDN. We then present the critical issues involved
in designing and implementing an effective CDN and survey the approaches
proposed in literature to address these problems. An example of CDN is
described to show how a real commercial CDN operates. After this, we present a
scheme that provides fast service location for peer-to-peer systems, a special
type of CDN with no infrastructure support. We conclude with a brief projection
about CDN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411070</id><created>2004-11-18</created><authors><author><keyname>De</keyname><forenames>Pradipta</forenames></author></authors><title>Data Path Processing in Fast Programmable Routers</title><categories>cs.NI</categories><comments>ECSL Technical Report #127, Stony Brook University</comments><acm-class>C.2.1</acm-class><abstract>  Internet is growing at a fast pace. The link speeds are surging toward 40
Gbps with the emergence of faster link technologies. New applications are
coming up which require intelligent processing at the intermediate routers.
Switches and routers are becoming the bottlenecks in fast communication. On one
hand faster links deliver more packets every second and on the other hand
intelligent processing consumes more CPU cycles at the router. The conflicting
goals of providing faster but computationally expensive processing call for new
approaches in designing routers.
  This survey takes a look at the core functionalities, like packet
classification, buffer memory management, switch scheduling and output link
scheduling performed by a router in its data path processing and discusses the
algorithms that aim to reduce the performance bound for these operations. An
important requirement for the routers is to provide Quality of Service
guarantees. We propose an algorithm to guarantee QoS in Input Queued Routers.
The hardware solution to speed up router operation was Application Specific
Integrated Circuits (ASICs). But the inherent inflexibility of the method is a
demerit as network standards and application requirements are constantly
evolving, which seek a faster turnaround time to keep up with the changes. The
promise of Network Processors (NP) is the flexibility of general-purpose
processors together with the speed of ASICs. We will study the architectural
choices for the design of Network Processors and focus on some of the
commercially available NPs. There is a plethora of NP vendors in the market.
The discussion on the NP benchmarks sets the normalizing platform to evaluate
these NPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411071</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411071</id><created>2004-11-19</created><authors><author><keyname>Sidenbladh</keyname><forenames>Hedvig</forenames></author><author><keyname>Svenson</keyname><forenames>Pontus</forenames></author><author><keyname>Schubert</keyname><forenames>Johan</forenames></author></authors><title>Comparing Multi-Target Trackers on Different Force Unit Levels</title><categories>cs.AI</categories><comments>9 pages</comments><acm-class>I.2.0</acm-class><journal-ref>Proc SPIE Vol 5429, p 306-314 (2004)</journal-ref><doi>10.1117/12.542024</doi><abstract>  Consider the problem of tracking a set of moving targets. Apart from the
tracking result, it is often important to know where the tracking fails, either
to steer sensors to that part of the state-space, or to inform a human operator
about the status and quality of the obtained information. An intuitive quality
measure is the correlation between two tracking results based on uncorrelated
observations. In the case of Bayesian trackers such a correlation measure could
be the Kullback-Leibler difference.
  We focus on a scenario with a large number of military units moving in some
terrain. The units are observed by several types of sensors and &quot;meta-sensors&quot;
with force aggregation capabilities. The sensors register units of different
size. Two separate multi-target probability hypothesis density (PHD) particle
filters are used to track some type of units (e.g., companies) and their
sub-units (e.g., platoons), respectively, based on observations of units of
those sizes. Each observation is used in one filter only.
  Although the state-space may well be the same in both filters, the posterior
PHD distributions are not directly comparable -- one unit might correspond to
three or four spatially distributed sub-units. Therefore, we introduce a
mapping function between distributions for different unit size, based on
doctrine knowledge of unit configuration.
  The mapped distributions can now be compared -- locally or globally -- using
some measure, which gives the correlation between two PHD distributions in a
bounded volume of the state-space. To locate areas where the tracking fails, a
discretized quality map of the state-space can be generated by applying the
measure locally to different parts of the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411072</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411072</id><created>2004-11-19</created><authors><author><keyname>Svenson</keyname><forenames>Pontus</forenames></author></authors><title>Extremal optimization for sensor report pre-processing</title><categories>cs.AI</categories><comments>10 pages</comments><acm-class>I.2.8</acm-class><journal-ref>Proc SPIE Vol 5429, p 162-171 (2004)</journal-ref><doi>10.1117/12.542027</doi><abstract>  We describe the recently introduced extremal optimization algorithm and apply
it to target detection and association problems arising in pre-processing for
multi-target tracking.
  Here we consider the problem of pre-processing for multiple target tracking
when the number of sensor reports received is very large and arrives in large
bursts. In this case, it is sometimes necessary to pre-process reports before
sending them to tracking modules in the fusion system. The pre-processing step
associates reports to known tracks (or initializes new tracks for reports on
objects that have not been seen before). It could also be used as a pre-process
step before clustering, e.g., in order to test how many clusters to use.
  The pre-processing is done by solving an approximate version of the original
problem. In this approximation, not all pair-wise conflicts are calculated. The
approximation relies on knowing how many such pair-wise conflicts that are
necessary to compute. To determine this, results on phase-transitions occurring
when coloring (or clustering) large random instances of a particular graph
ensemble are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411073</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411073</id><created>2004-11-19</created><authors><author><keyname>Subramanian</keyname><forenames>Sundar</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Geographic Routing with Limited Information in Sensor Networks</title><categories>cs.IT math.IT</categories><comments>11 pages,13 figures</comments><abstract>  Geographic routing with greedy relaying strategies have been widely studied
as a routing scheme in sensor networks. These schemes assume that the nodes
have perfect information about the location of the destination. When the
distance between the source and destination is normalized to unity, the
asymptotic routing delays in these schemes are $\Theta(\frac{1}{M(n)}),$ where
M(n) is the maximum distance traveled in a single hop (transmission range of a
radio). In this paper, we consider routing scenarios where nodes have location
errors (imprecise GPS), or where only coarse geographic information about the
destination is available, and only a fraction of the nodes have routing
information. We show that even with such imprecise or limited
destination-location information, the routing delays are
$\Theta(\frac{1}{M(n)})$. We also consider the throughput-capacity of networks
with progressive routing strategies that take packets closer to the destination
in every step, but not necessarily along a straight-line. We show that the
throughput-capacity with progressive routing is order-wise the same as the
maximum achievable throughput-capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411074</id><created>2004-11-19</created><authors><author><keyname>Gayo-Avello</keyname><forenames>Daniel</forenames></author></authors><title>Building Chinese Lexicons from Scratch by Unsupervised Short Document
  Self-Segmentation</title><categories>cs.CL cs.IR</categories><comments>9 pages 3 figures 2 tables</comments><abstract>  Chinese text segmentation is a well-known and difficult problem. On one side,
there is not a simple notion of &quot;word&quot; in Chinese language making really hard
to implement rule-based systems to segment written texts, thus lexicons and
statistical information are usually employed to achieve such a task. On the
other side, any piece of Chinese text usually includes segments present neither
in the lexicons nor in the training data. Even worse, such unseen sequences can
be segmented into a number of totally unrelated words making later processing
phases difficult. For instance, using a lexicon-based system the sequence
???(Baluozuo, Barroso, current president-designate of the European Commission)
can be segmented into ?(ba, to hope, to wish) and ??(luozuo, an undefined word)
changing completely the meaning of the sentence. A new and extremely simple
algorithm specially suited to work over short Chinese documents is introduced.
This new algorithm performs text &quot;self-segmentation&quot; producing results
comparable to those achieved by native speakers without using either lexicons
or any statistical information beyond the obtained from the input text.
Furthermore, it is really robust for finding new &quot;words&quot;, especially proper
nouns, and it is well suited to build lexicons from scratch. Some preliminary
results are provided in addition to examples of its employment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411075</id><created>2004-11-20</created><authors><author><keyname>Weisensee</keyname><forenames>Andreas</forenames></author><author><keyname>Nathan</keyname><forenames>Darran</forenames></author></authors><title>A Self-Reconfigurable Computing Platform Hardware Architecture</title><categories>cs.AR cs.DC</categories><comments>5 pages, 6 figures</comments><abstract>  Field Programmable Gate Arrays (FPGAs) have recently been increasingly used
for highly-parallel processing of compute intensive tasks. This paper
introduces an FPGA hardware platform architecture that is PC-based, allows for
fast reconfiguration over the PCI bus, and retains a simple physical hardware
design. The design considerations are first discussed, then the resulting
system architecture designed is illustrated. Finally, experimental results on
the FPGA resources utilized for this design are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411076</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411076</id><created>2004-11-20</created><updated>2015-01-12</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Gasarch</keyname><forenames>William</forenames></author><author><keyname>Srinavasan</keyname><forenames>Aravind</forenames></author><author><keyname>Utis</keyname><forenames>Andrey</forenames></author></authors><title>Lower bounds on the Deterministic and Quantum Communication Complexity
  of Hamming Distance</title><categories>cs.CC quant-ph</categories><comments>12 pages, this version to appear in ACM Transactions on Computation
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alice and Bob want to know if two strings of length n are almost equal. That
is, do they differ on \textit{at most} a bits? Let 0\leq a\leq n-1. We show
that any deterministic protocol, as well as any error-free quantum protocol (C*
version), for this problem requires at least n-2 bits of communication. We show
the same bounds for the problem of determining if two strings differ in exactly
a bits. We also prove a lower bound of n/2-1 for error-free Q* quantum
protocols. Our results are obtained by lower-bounding the ranks of the
appropriate matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411077</id><created>2004-11-21</created><authors><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author><author><keyname>Lipkis</keyname><forenames>Thomas</forenames></author><author><keyname>Robertson</keyname><forenames>Thomas</forenames></author><author><keyname>Morabito</keyname><forenames>Seth</forenames></author></authors><title>Transparent Format Migration of Preserved Web Content</title><categories>cs.DL</categories><comments>6 pages, 2 figures</comments><acm-class>H.5.4;H.3.7</acm-class><abstract>  The LOCKSS digital preservation system collects content by crawling the web
and preserves it in the format supplied by the publisher. Eventually, browsers
will no longer understand that format. A process called format migration
converts it to a newer format that the browsers do understand. The LOCKSS
program has designed and tested an initial implementation of format migration
for Web content that is transparent to readers, building on the content
negotiation capabilities of HTTP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411078</id><created>2004-11-21</created><authors><author><keyname>Rosenthal</keyname><forenames>David S. H.</forenames></author><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author><author><keyname>Giuli</keyname><forenames>T. J.</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Notes On The Design Of An Internet Adversary</title><categories>cs.DL</categories><acm-class>H.3.7</acm-class><journal-ref>Second Annual Adaptive and Resilient Computing Security Workshop,
  Santa Fe, 2003</journal-ref><abstract>  The design of the defenses Internet systems can deploy against attack,
especially adaptive and resilient defenses, must start from a realistic model
of the threat. This requires an assessment of the capabilities of the
adversary. The design typically evolves through a process of simulating both
the system and the adversary. This requires the design and implementation of a
simulated adversary based on the capability assessment. Consensus on the
capabilities of a suitable adversary is not evident. Part of the recent
redesign of the protocol used by peers in the LOCKSS digital preservation
system included a conservative assessment of the adversary's capabilities. We
present our assessment and the implications we drew from it as a step towards a
reusable adversary specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411079</id><created>2004-11-22</created><authors><author><keyname>Sharma</keyname><forenames>Srikant</forenames></author><author><keyname>Gopalan</keyname><forenames>Kartik</forenames></author><author><keyname>Zhu</keyname><forenames>Ningning</forenames></author><author><keyname>Peng</keyname><forenames>Gang</forenames></author><author><keyname>De</keyname><forenames>Pradipta</forenames></author><author><keyname>Chiueh</keyname><forenames>Tzi-cker</forenames></author></authors><title>Supporting Bandwidth Guarantee and Mobility for Real-Time Applications
  on Wireless LANs</title><categories>cs.NI</categories><comments>This paper integrates the QoS scheme published in MMCN 2002 with a
  low latency mobility scheme that appeared in IEEE JSAC May 2004. This paper
  deals with both the issues with a fresh perspective of new networking
  technologies and standards such as 802.11e</comments><acm-class>C.1.3; C.2.1</acm-class><abstract>  The proliferation of IEEE 802.11-based wireless LANs opens up avenues for
creation of several tetherless and mobility oriented services. Most of these
services, like voice over WLAN, media streaming etc., generate delay and
bandwidth sensitive traffic. These traffic flows require undisrupted network
connectivity with some QoS guarantees. Unfortunately, there is no adequate
support built into these wireless LANs towards QoS provisioning. Further, the
network layer handoff latency incurred by mobile nodes in these wireless LANs
is too high for real-time applications to function properly. In this paper, we
describe a QoS mechanism, called Rether, to effectively support bandwidth
guarantee on wireless LANs. Rether is designed to support the current wireless
LAN technologies like 802.11b and 802.11a with a specific capability of being
tailored for QoS oriented technology like 802.11e. We also describe a
low-latency handoff mechanism which expedites network level handoff to provide
real-time applications with an added advantage of seamless mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411080</id><created>2004-11-23</created><authors><author><keyname>Assis</keyname><forenames>C. A. G.</forenames></author><author><keyname>Fernandes</keyname><forenames>E. S. T.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>Modeling the input history of programs for improved instruction-memory
  performance</title><categories>cs.OS</categories><report-no>ES-662/04</report-no><acm-class>B.3.2</acm-class><journal-ref>Computer Journal 49 (2006), 744-761</journal-ref><doi>10.1093/comjnl/bxl044</doi><abstract>  When a program is loaded into memory for execution, the relative position of
its basic blocks is crucial, since loading basic blocks that are unlikely to be
executed first places them high in the instruction-memory hierarchy only to be
dislodged as the execution goes on. In this paper we study the use of Bayesian
networks as models of the input history of a program. The main point is the
creation of a probabilistic model that persists as the program is run on
different inputs and at each new input refines its own parameters in order to
reflect the program's input history more accurately. As the model is thus
tuned, it causes basic blocks to be reordered so that, upon arrival of the next
input for execution, loading the basic blocks into memory automatically takes
into account the input history of the program. We report on extensive
experiments, whose results demonstrate the efficacy of the overall approach in
progressively lowering the execution times of a program on identical inputs
placed randomly in a sequence of varied inputs. We provide results on selected
SPEC CINT2000 programs and also evaluate our approach as compared to the gcc
level-3 optimization and to Pettis-Hansen reordering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411081</id><created>2004-11-24</created><authors><author><keyname>Hachichi</keyname><forenames>Assia</forenames><affiliation>REGAL Ur-R Lip6</affiliation></author><author><keyname>Martin</keyname><forenames>Cyril</forenames><affiliation>REGAL Ur-R Lip6</affiliation></author><author><keyname>Thomas</keyname><forenames>Gael</forenames><affiliation>REGAL Ur-R Lip6</affiliation></author><author><keyname>Patarin</keyname><forenames>Simon</forenames><affiliation>REGAL Ur-R Lip6</affiliation></author><author><keyname>Folliot</keyname><forenames>Bertil</forenames><affiliation>REGAL Ur-R Lip6</affiliation></author></authors><title>Reconfigurations dynamiques de services dans un intergiciel a composants
  CORBA CCM</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003292</proxy><journal-ref>DECOR04 (2004) 159-170</journal-ref><abstract>  Today, component oriented middlewares are used to design, develop and deploy
easily distributed applications, by ensuring the heterogeneity,
interoperability, and reuse of the software modules, and the separation between
the business code encapsulated in the components and the system code managed by
the containers. Several standards answer this definition such as: CCM (CORBA
Component Model), EJB (Enterprise Java Beans) and .Net. However these standards
offer a limited and fixed number of system services, removing any possibility
to add system services or to reconfigure dynamically the middleware. Our works
propose mechanisms to add and to adapt dynamically the system services, based
on a reconfiguration language which is dynamically adaptable to the need of the
reconfiguration, and on a tool of dynamic reconfiguration, a prototype was
achieved for the OpenCCM platform, that is an implementation of the CCM
specification. This work was partially financed by the european project
IST-COACH (2001-34445).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411082</id><created>2004-11-24</created><authors><author><keyname>Kornas</keyname><forenames>Jakub</forenames><affiliation>SARDES Ur-Ra Imag</affiliation></author><author><keyname>Leclercq</keyname><forenames>Matthieu</forenames><affiliation>SARDES Ur-Ra Imag</affiliation></author><author><keyname>Quema</keyname><forenames>Vivien</forenames><affiliation>SARDES Ur-Ra Imag</affiliation></author><author><keyname>Stefani</keyname><forenames>Jean-Bernard</forenames><affiliation>SARDES Ur-Ra Imag</affiliation></author></authors><title>Support pour la reconfiguration d'implantation dans les applications a
  composants Java</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003293</proxy><journal-ref>DECOR04 (2004) 171-184</journal-ref><abstract>  Nowadays, numerous component models are used for various purposes: to build
applications, middleware or even operating systems. Those models commonly
support structure reconfiguration, that is modification of application's
architecture at runtime. On the other hand, very few allow implementation
reconfiguration, that is runtime modification of the code of components
building the application. In this article we present the work we performed on
JULIA, a Java-based implementation of the FRACTAL component model, in order for
it to support implementation reconfigurations. We show how we overcame the
limitations of Java class loading mechanism to allow runtime modifications of
components' implementation and interfaces. We also describe the integration of
our solution with the JULIA ADL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411083</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411083</id><created>2004-11-24</created><authors><author><keyname>Sommer</keyname><forenames>Nicolas Le</forenames><affiliation>VALORIA</affiliation></author></authors><title>Contractualisation des ressources pour le deploiement des composants
  logiciels</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003298</proxy><journal-ref>DECOR04 (2004) 211-222</journal-ref><abstract>  Software deployment can turn into a baffling problem when the components
being deployed exhibit non-functional requirements. If the platform on which
such components are deployed cannot satisfy their non-functional requirements,
then they may in turn fail to perform satisfactorily. In this paper, we present
a contract-based approach to take a specific category of non-functional
properties specified by components into account, that is those that pertain to
the resources that are necessary for their execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411084</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411084</id><created>2004-11-24</created><authors><author><keyname>Marin</keyname><forenames>Cristina</forenames><affiliation>LSR - IMAG</affiliation></author><author><keyname>Belkhatir</keyname><forenames>Noureddine</forenames><affiliation>LSR - IMAG</affiliation></author><author><keyname>Donsez</keyname><forenames>Didier</forenames><affiliation>LSR - IMAG</affiliation></author></authors><title>Gestion transactionnelle de la reprise sur erreurs dans le deploiement</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003296</proxy><journal-ref>DECOR04 (2004) 199-210</journal-ref><abstract>  With the development of the networks and the Internet, the problems of
automated deployment on broad scale became increasingly crucial. Software
deployment is a complex process covering several activities going from the
configuration to the retirement of a software product. During the execution of
a deployment process, exceptions can be met which put the site in an incoherent
state. To solve them, we propose an approach based on transactional concepts
which describes the actions to be undertaken when an exceptional situation is
met during the deployment process. The approach guaranties the respect of the
site's consistency by preserving part of the work already carried out by the
process. This article presents our approach and an experimentation made in an
academic deployment system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411085</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411085</id><created>2004-11-24</created><authors><author><keyname>Duchesne</keyname><forenames>Herve</forenames><affiliation>OBASCO Irisa</affiliation></author><author><keyname>Augier</keyname><forenames>Christophe</forenames><affiliation>OBASCO Irisa</affiliation></author><author><keyname>Urunuela</keyname><forenames>Richard</forenames><affiliation>OBASCO Irisa</affiliation></author></authors><title>Deploiement d'ordonnanceurs de processus specifiques dans un systeme
  d'exploitation generaliste</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003295</proxy><journal-ref>DECOR04 (2004) 193-198</journal-ref><abstract>  Bossa is a framework to develop new processes schedulers in commodity
operating systems. Although Bossa enables fine-grained management of the
processor through new scheduling policies, deploying an application with its
own scheduler raises some problems. In this paper we study the problems caused
when deploying an application and its scheduler and to adresse these, we
propose to establish Quality of Service contracts and mechanisms to reconfigure
the scheduler hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411086</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411086</id><created>2004-11-24</created><authors><author><keyname>Lacour</keyname><forenames>Sebastien</forenames><affiliation>PARIS Irisa</affiliation></author><author><keyname>Perez</keyname><forenames>Christian</forenames><affiliation>PARIS Irisa</affiliation></author><author><keyname>Priol</keyname><forenames>Thierry</forenames><affiliation>PARIS Irisa</affiliation></author></authors><title>A Software Architecture for Automatic Deployment of CORBA Components
  Using Grid Technologies</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003294</proxy><journal-ref>DECOR04 (2004) 187-192</journal-ref><abstract>  Software components turn out to be a convenient model to build complex
applications for scientific computing and to run them on a computational grid.
However, deploying complex, component-based applications in a grid environment
is particularly arduous. To prevent the user from directly dealing with a large
number of execution hosts and their heterogeneity within a grid, the
application deployment phase must be as automatic as possible. This paper
describes an architecture for automatic deployment of component-based
applications on computational grids. In the context of the CORBA Component
Model (CCM), this paper details all the steps to achieve an automatic
deployment of components as well as the entities involved: a grid access
middleware and its grid information service (like OGSI), a component deployment
model, as specified by CCM, an enriched application description and a
deployment planner in order to select resources and map components onto
computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411087</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411087</id><created>2004-11-24</created><authors><author><keyname>Patarin</keyname><forenames>Simon</forenames><affiliation>REGAL UR-R LIP6</affiliation></author><author><keyname>Makpangou</keyname><forenames>Mesaac</forenames><affiliation>REGAL UR-R LIP6</affiliation></author></authors><title>Pandora : une plate-forme efficace pour la construction d'applications
  autonomes</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003277</proxy><journal-ref>DECOR04 (2004) 15-26</journal-ref><abstract>  Autonomic computing has been proposed recently as a way to address the
difficult management of applications whose complexity is constantly increasing.
Autonomous applications will have to be especially flexible and be able to
monitor themselves permanently. This work presents a framework, Pandora, which
eases the construction of applications that satisfy this double goal. Pandora
relies on an original application programming pattern - based on stackable
layers and message passing - to obtain minimalist model and architecture that
allows to control the overhead imposed by the full reflexivity of the
framework. Besides, a prototype of the framework has been implemented in C++. A
detailed performance study, together with examples of use, complement this
presentation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411088</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411088</id><created>2004-11-24</created><authors><author><keyname>Loriant</keyname><forenames>Nicolas</forenames><affiliation>OBASCO IRISA</affiliation></author><author><keyname>Devillechaise</keyname><forenames>Marc Segura</forenames><affiliation>OBASCO IRISA</affiliation></author><author><keyname>Menaud</keyname><forenames>Jean-Marc</forenames><affiliation>OBASCO IRISA</affiliation></author></authors><title>Des correctifs de securite a la mise a jour</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003283</proxy><journal-ref>DECOR04 (2004) 65-76</journal-ref><abstract>  The ever growing software complexity suggests that they will never be bugfree
and therefore secure. Software compagnies regulary publish updates. But maybe
because of lack of time or care or maybe because stopping application is
annoying, such updates are rarely if ever deployed on users' machines. We
propose an integrated tool allowing system administrators to deploy critical
security updates on the fly on applications running remotly without end-user
intervention. Our approach is based on an aspect weaving system, Arachne, that
dynamicaly rewrites binary code. Hence updated applications are still running
while they are updated. Our second tool Minerve integrates Arachne within the
standart updating process: Minerve takes a patch produced by dif and eventually
builds a dynamic patch that can later be woven to update the application on the
fly. In addition, Minerve allows to consult patches translated in a dedicated
language and hence eases auditing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411089</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411089</id><created>2004-11-24</created><authors><author><keyname>Herault</keyname><forenames>Colombe</forenames><affiliation>LAMIH</affiliation></author><author><keyname>Lecomte</keyname><forenames>Sylvain</forenames><affiliation>LAMIH</affiliation></author></authors><title>Gestion Dynamique des Services Techniques pour Modele a Composants</title><categories>cs.NI</categories><proxy>ccsd ccsd-00003290</proxy><journal-ref>DECOR04 (2004) 135-146</journal-ref><abstract>  The new applications being intended for more and more heterogeneous
environments, it is necessary to propose solutions of development which answer
in best the necessities of adaptation of new services. Component-based
programming partially answers this aim, allowing easy replacement of software
blocks in order to provide the most adapted version of a component.
Nevertheless, most of the industrial component-based model implementations do
not allow to provide to components the most adapted technical services (naming,
trading, security, transaction, etc.). In this paper, we suggest defining
technical services themselves under the shape of components. We shall detail
our proposition, by basing it on the Fractal component model of Objectweb.
Then, we shall bring solutions for the use of these new component-based
technical services and shall propose a set of management components which allow
to administer in a dynamic and stand-alone way the obtained components. Finally
we present the prototype of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411090</id><created>2004-11-25</created><authors><author><keyname>Stauffer</keyname><forenames>A. O.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>Local heuristics and the emergence of spanning subgraphs in complex
  networks</title><categories>cs.NI</categories><report-no>ES-663/04</report-no><acm-class>C.2.2; F.2.2</acm-class><journal-ref>Theoretical Computer Science 355 (2006), 80-95</journal-ref><doi>10.1016/j.tcs.2005.12.007</doi><abstract>  We study the use of local heuristics to determine spanning subgraphs for use
in the dissemination of information in complex networks. We introduce two
different heuristics and analyze their behavior in giving rise to spanning
subgraphs that perform well in terms of allowing every node of the network to
be reached, of requiring relatively few messages and small node bandwidth for
information dissemination, and also of stretching paths with respect to the
underlying network only modestly. We contribute a detailed mathematical
analysis of one of the heuristics and provide extensive simulation results on
random graphs for both of them. These results indicate that, within certain
limits, spanning subgraphs are indeed expected to emerge that perform well in
respect to all requirements. We also discuss the spanning subgraphs' inherent
resilience to failures and adaptability to topological changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411091</id><created>2004-11-24</created><updated>2004-12-18</updated><authors><author><keyname>Gladney</keyname><forenames>H. M.</forenames></author></authors><title>Principles for Digital Preservation</title><categories>cs.DL</categories><comments>7 pages with 2b figures</comments><acm-class>H.1 Long term digital preservation</acm-class><abstract>  The immense investments in creating and disseminating digitally represented
information have not been accompanied by commensurate effort to ensure the
longevity of information of permanent interest. Asserted difficulties with
long-term digital preservation prove to be largely underestimation of what
technology can provide. We show how to clarify prominent misunderstandings and
sketch a 'Trustworthy Digital Object (TDO)' method that solves all the
published technical challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411092</id><created>2004-11-24</created><authors><author><keyname>Gladney</keyname><forenames>H. M.</forenames></author><author><keyname>Lorie</keyname><forenames>R. A.</forenames></author></authors><title>Trustworthy 100-Year Digital Objects: Durable Encoding for When It's Too
  Late to Ask</title><categories>cs.DL</categories><acm-class>H.1</acm-class><abstract>  How can an author store digital information so that it will be reliably
useful, even years later when he is no longer available to answer questions?
Methods that might work are not good enough; what is preserved today should be
reliably useful whenever someone wants it. Prior proposals fail because they
confound saved data with irrelevant details of today's information
technology--details that are difficult to define, extract, and save completely
and accurately.
  We use a virtual machine to represent and eventually to render any data
whatsoever. We focus on a case of intermediate difficulty--an executable
procedure--and identify a variant for every other data type. This solution
might be more elaborate than needed to render some text, image, audio, or video
data. Simple data can be preserved as representations using well-known
standards. We sketch practical methods for files ranging from simple structures
to those containing computer programs, treating simple cases here and deferring
complex cases for future work. Enough of the complete solution is known to
enable practical aggressive preservation programs today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411093</id><created>2004-11-25</created><authors><author><keyname>Ravelomanana</keyname><forenames>Vlady</forenames><affiliation>LIPN</affiliation></author><author><keyname>Thimonier</keyname><forenames>Loys</forenames><affiliation>LARIA</affiliation></author></authors><title>Forbidden Subgraphs in Connected Graphs</title><categories>cs.DS cs.DM math.CO</categories><proxy>ccsd ccsd-00003325</proxy><acm-class>ACM Classification: G.2.1 Combinatorics G.2.2 Graph Theory General
  Terms: Algorithms, Theory</acm-class><abstract>  Given a set $\xi=\{H_1,H_2,...\}$ of connected non acyclic graphs, a
$\xi$-free graph is one which does not contain any member of $% \xi$ as copy.
Define the excess of a graph as the difference between its number of edges and
its number of vertices. Let ${\gr{W}}_{k,\xi}$ be theexponential generating
function (EGF for brief) of connected $\xi$-free graphs of excess equal to $k$
($k \geq 1$). For each fixed $\xi$, a fundamental differential recurrence
satisfied by the EGFs ${\gr{W}}_{k,\xi}$ is derived. We give methods on how to
solve this nonlinear recurrence for the first few values of $k$ by means of
graph surgery. We also show that for any finite collection $\xi$ of non-acyclic
graphs, the EGFs ${\gr{W}}_{k,\xi}$ are always rational functions of the
generating function, $T$, of Cayley's rooted (non-planar) labelled trees. From
this, we prove that almost all connected graphs with $n$ nodes and $n+k$ edges
are $\xi$-free, whenever $k=o(n^{1/3})$ and $|\xi| &lt; \infty$ by means of
Wright's inequalities and saddle point method. Limiting distributions are
derived for sparse connected $\xi$-free components that are present when a
random graph on $n$ nodes has approximately $\frac{n}{2}$ edges. In particular,
the probability distribution that it consists of trees, unicyclic components,
$...$, $(q+1)$-cyclic components all $\xi$-free is derived. Similar results are
also obtained for multigraphs, which are graphs where self-loops and
multiple-edges are allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411094</id><created>2004-11-25</created><updated>2005-02-23</updated><authors><author><keyname>Srinivasan</keyname><forenames>Radhakrishnan</forenames></author><author><keyname>Raghunandan</keyname><forenames>H. P.</forenames></author></authors><title>On the existence of truly autonomic computing systems and the link with
  quantum computing</title><categories>cs.LO math.LO quant-ph</categories><comments>13 pages (two-column, single-spaced), 2 figures. Section 4 and
  Appendix B (with a figure in each) added. Improved explanations given for the
  NAFL model of (hyper-) computation (Sec. 6) and for the failure of
  relativistic locality in NAFL (Appendix B)</comments><acm-class>F.4.1; F.1.1</acm-class><abstract>  A theoretical model of truly autonomic computing systems (ACS), with
infinitely many constraints, is proposed. An argument similar to Turing's for
the unsolvability of the halting problem, which is permitted in classical
logic, shows that such systems cannot exist. Turing's argument fails in the
recently proposed non-Aristotelian finitary logic (NAFL), which permits the
existence of ACS. NAFL also justifies quantum superposition and entanglement,
which are essential ingredients of quantum algorithms, and resolves the
Einstein-Podolsky-Rosen (EPR) paradox in favour of quantum mechanics and
non-locality. NAFL requires that the autonomic manager (AM) must be
conceptually and architecturally distinct from the managed element, in order
for the ACS to exist as a non-self-referential entity. Such a scenario is
possible if the AM uses quantum algorithms and is protected from all problems
by (unbreakable) quantum encryption, while the managed element remains
classical. NAFL supports such a link between autonomic and quantum computing,
with the AM existing as a metamathematical entity. NAFL also allows quantum
algorithms to access truly random elements and thereby supports non-standard
models of quantum (hyper-) computation that permit infinite parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411095</id><created>2004-11-26</created><authors><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>Embeddings into the Pancake Interconnection Network</title><categories>cs.DC cs.DM cs.DS</categories><comments>Article paru en 2002 dans Parallel Processing Letters</comments><proxy>ccsd ccsd-00003362</proxy><journal-ref>Parallel Processing Letters 12, 3-4 (2002) 297-310</journal-ref><abstract>  Owing to its nice properties, the pancake is one of the Cayley graphs that
were proposed as alternatives to the hypercube for interconnecting processors
in parallel computers. In this paper, we present embeddings of rings, grids and
hypercubes into the pancake with constant dilation and congestion. We also
extend the results to similar efficient embeddings into the star graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411096</id><created>2004-11-28</created><authors><author><keyname>LaBelle</keyname><forenames>Nathan</forenames></author><author><keyname>Wallingford</keyname><forenames>Eugene</forenames></author></authors><title>Inter-Package Dependency Networks in Open-Source Software</title><categories>cs.SE</categories><comments>6 Pages, 1 Figure, Submitted to J. Theoretical Comp. Science</comments><abstract>  This research analyzes complex networks in open-source software at the
inter-package level, where package dependencies often span across projects and
between development groups. We review complex networks identified at ``lower''
levels of abstraction, and then formulate a description of interacting software
components at the package level, a relatively ``high'' level of abstraction. By
mining open-source software repositories from two sources, we empirically show
that the coupling of modules at this granularity creates a small-world and
scale-free network in both instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411097</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411097</id><created>2004-11-29</created><updated>2007-06-29</updated><authors><author><keyname>Dambreville</keyname><forenames>Frederic</forenames><affiliation>DGA/Cta/DT/Gip</affiliation></author></authors><title>Deterministic Bayesian Logic</title><categories>cs.LO math.LO math.PR</categories><comments>Fourth version. A sequent formalism is used</comments><abstract>  In this paper a conditional logic is defined and studied. This conditional
logic, Deterministic Bayesian Logic, is constructed as a deterministic
counterpart to the (probabilistic) Bayesian conditional. The logic is
unrestricted, so that any logical operations are allowed. This logic is shown
to be non-trivial and is not reduced to classical propositions. The Bayesian
conditional of DBL implies a definition of logical independence. Interesting
results are derived about the interactions between the logical independence and
the proofs. A model is constructed for the logic. Completeness results are
proved. It is shown that any unconditioned probability can be extended to the
whole logic DBL. The Bayesian conditional is then recovered from the
probabilistic DBL. At last, it is shown why DBL is compliant with Lewis
triviality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411098</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411098</id><created>2004-11-29</created><authors><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>On the High-SNR Capacity of Non-Coherent Networks</title><categories>cs.IT math.IT</categories><comments>This work was presented in part at the Third Joint Workshop on
  Communications and Coding (JWCC'04) in Donnini-Firenze, Italy, October 14-17,
  2004</comments><abstract>  We obtain the first term in the high signal-to-noise ratio (SNR) expansion of
the capacity of fading networks where the transmitters and receivers--while
fully cognizant of the fading \emph{law}--have no access to the fading
\emph{realization}. This term is an integer multiple of $\log \log
\textnormal{SNR}$ with the coefficient having a simple combinatorial
characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411099</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411099</id><created>2004-11-30</created><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author></authors><title>A Note on the PAC Bayesian Theorem</title><categories>cs.LG cs.AI</categories><comments>9 pages</comments><acm-class>I.5.1</acm-class><abstract>  We prove general exponential moment inequalities for averages of [0,1]-valued
iid random variables and use them to tighten the PAC Bayesian Theorem. The
logarithmic dependence on the sample count in the enumerator of the PAC
Bayesian bound is halved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0411100</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0411100</id><created>2004-11-30</created><updated>2006-03-29</updated><authors><author><keyname>Lanotte</keyname><forenames>Ruggero</forenames><affiliation>Dipartimento di Scienze della Cultura, Politiche e dell'Informazione</affiliation></author><author><keyname>Beauquier</keyname><forenames>Daniele</forenames><affiliation>LACL, Dept. of Informatics</affiliation></author></authors><title>A Decidable Probability Logic for Timed Probabilistic Systems</title><categories>cs.LO</categories><acm-class>C.3; D.2.4; F.3.1; F.4.1; G.3</acm-class><abstract>  In this paper we extend the predicate logic introduced in [Beauquier et al.
2002] in order to deal with Semi-Markov Processes. We prove that with respect
to qualitative probabilistic properties, model checking is decidable for this
logic applied to Semi-Markov Processes. Furthermore we apply our logic to
Probabilistic Timed Automata considering classical and urgent semantics, and
considering also predicates on clock. We prove that results on Semi Markov
Processes hold also for Probabilistic Timed Automata for both the two semantics
considered. Moreover, we prove that results for Markov Processes shown in
[Beauquier et al. 2002] are extensible to Probabilistic Timed Automata where
urgent semantics is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412001</id><created>2004-12-01</created><authors><author><keyname>Rouveyrol</keyname><forenames>Serge</forenames><affiliation>IMAG</affiliation></author><author><keyname>Chiaramella</keyname><forenames>Yves</forenames><affiliation>IMAG</affiliation></author><author><keyname>Leinardi</keyname><forenames>Francesca</forenames><affiliation>IMAG</affiliation></author><author><keyname>Janik</keyname><forenames>Joanna</forenames><affiliation>IMAG</affiliation></author><author><keyname>Marmol</keyname><forenames>Bruno</forenames><affiliation>INRIA-RA</affiliation></author><author><keyname>Silvy</keyname><forenames>Carole</forenames><affiliation>INRIA-RA</affiliation></author><author><keyname>Allauzun</keyname><forenames>Catherine</forenames><affiliation>INRIA-RA</affiliation></author></authors><title>EURYDICE : A platform for unified access to documents</title><categories>cs.DL</categories><comments>redige le 14 octobre 2001</comments><proxy>ccsd ccsd-00003436</proxy><abstract>  In this paper we present Eurydice, a platform dedicated to provide a unified
gateway to documents. Its basic functionalities about collecting documents have
been designed based on a long experience about the management of scientific
documentation among large and demanding academic communities such as IMAG and
INRIA. Besides the basic problem of accessing documents - which was of course
the original and main motivation of the project - a great effort has been
dedicated to the development of management functionalities which could help
institutions to control, analyse the current situation about the use of the
documentation, and finally to set a better ground for a documentation policy.
Finally a great emphasis - and corresponding technical investment - has been
put on the protection of property and reproduction rights both from the users'
intitution side and from the editors' side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412002</id><created>2004-12-01</created><updated>2004-12-09</updated><authors><author><keyname>Borges</keyname><forenames>Jose</forenames></author><author><keyname>Levene</keyname><forenames>Mark</forenames></author></authors><title>Ranking Pages by Topology and Popularity within Web Sites</title><categories>cs.AI cs.IR</categories><comments>15 pages, 6 figures</comments><abstract>  We compare two link analysis ranking methods of web pages in a site. The
first, called Site Rank, is an adaptation of PageRank to the granularity of a
web site and the second, called Popularity Rank, is based on the frequencies of
user clicks on the outlinks in a page that are captured by navigation sessions
of users through the web site. We ran experiments on artificially created web
sites of different sizes and on two real data sets, employing the relative
entropy to compare the distributions of the two ranking methods. For the real
data sets we also employ a nonparametric measure, called Spearman's footrule,
which we use to compare the top-ten web pages ranked by the two methods. Our
main result is that the distributions of the Popularity Rank and Site Rank are
surprisingly close to each other, implying that the topology of a web site is
very instrumental in guiding users through the site. Thus, in practice, the
Site Rank provides a reasonable first order approximation of the aggregate
behaviour of users within a web site given by the Popularity Rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412003</id><created>2004-12-01</created><authors><author><keyname>Duchene</keyname><forenames>Florence</forenames><affiliation>TIMC - IMAG</affiliation></author><author><keyname>Garbay</keyname><forenames>Catherine</forenames><affiliation>TIMC - IMAG</affiliation></author><author><keyname>Rialle</keyname><forenames>Vincent</forenames><affiliation>TIMC - IMAG, SIIM</affiliation></author></authors><title>Mining Heterogeneous Multivariate Time-Series for Learning Meaningful
  Patterns: Application to Home Health Telecare</title><categories>cs.LG</categories><proxy>ccsd</proxy><acm-class>G.3</acm-class><abstract>  For the last years, time-series mining has become a challenging issue for
researchers. An important application lies in most monitoring purposes, which
require analyzing large sets of time-series for learning usual patterns. Any
deviation from this learned profile is then considered as an unexpected
situation. Moreover, complex applications may involve the temporal study of
several heterogeneous parameters. In that paper, we propose a method for mining
heterogeneous multivariate time-series for learning meaningful patterns. The
proposed approach allows for mixed time-series -- containing both pattern and
non-pattern data -- such as for imprecise matches, outliers, stretching and
global translating of patterns instances in time. We present the early results
of our approach in the context of monitoring the health status of a person at
home. The purpose is to build a behavioral profile of a person by analyzing the
time variations of several quantitative or qualitative parameters recorded
through a provision of sensors installed in the home.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412004</id><created>2004-12-01</created><authors><author><keyname>Allison</keyname><forenames>L.</forenames></author></authors><title>Finding Approximate Palindromes in Strings Quickly and Simply</title><categories>cs.DS</categories><comments>4 pages, 3 figures, code of the simple algorithm will soon be placed
  at http://www.csse.monash.edu.au/~lloyd/tildeProgLang/Java2/Palindromes/</comments><report-no>2004/162</report-no><acm-class>F.2.2; I.2.8</acm-class><abstract>  Described are two algorithms to find long approximate palindromes in a
string, for example a DNA sequence. A simple algorithm requires O(n)-space and
almost always runs in $O(k.n)$-time where n is the length of the string and k
is the number of ``errors'' allowed in the palindrome. Its worst-case
time-complexity is $O(n^2)$ but this does not occur with real biological
sequences. A more complex algorithm guarantees $O(k.n)$ worst-case time
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412005</id><created>2004-12-02</created><authors><author><keyname>Parisse</keyname><forenames>Bernard</forenames><affiliation>IF</affiliation></author><author><keyname>Vaughan</keyname><forenames>Morgane</forenames><affiliation>IF</affiliation></author></authors><title>Jordan Normal and Rational Normal Form Algorithms</title><categories>cs.SC</categories><proxy>ccsd ccsd-00003444</proxy><acm-class>MSC2000 15A21 68W30</acm-class><abstract>  In this paper, we present a determinist Jordan normal form algorithms based
on the Fadeev formula: \[(\lambda \cdot I-A) \cdot B(\lambda)=P(\lambda) \cdot
I\] where $B(\lambda)$ is $(\lambda \cdot I-A)$'s comatrix and $P(\lambda)$ is
$A$'s characteristic polynomial. This rational Jordan normal form algorithm
differs from usual algorithms since it is not based on the Frobenius/Smith
normal form but rather on the idea already remarked in Gantmacher that the
non-zero column vectors of $B(\lambda_0)$ are eigenvectors of $A$ associated to
$\lambda_0$ for any root $\lambda_0$ of the characteristical polynomial. The
complexity of the algorithm is $O(n^4)$ field operations if we know the
factorization of the characteristic polynomial (or $O(n^5 \ln(n))$ operations
for a matrix of integers of fixed size). This algorithm has been implemented
using the Maple and Giac/Xcas computer algebra systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412006</id><created>2004-12-02</created><authors><author><keyname>Sedjelmaci</keyname><forenames>Sidi Mohamed</forenames><affiliation>LIPN</affiliation></author></authors><title>The Accelerated Euclidean Algorithm</title><categories>cs.DS</categories><proxy>ccsd ccsd-00003459</proxy><journal-ref>Proceedings of the EACA, (2004) 283-287</journal-ref><abstract>  We present a new GCD algorithm of two integers or polynomials. The algorithm
is iterative and its time complexity is still $O(n \\log^2 n ~ log \\log n)$
for $n$-bit inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412007</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412007</id><created>2004-12-02</created><authors><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Alvarez-Hamelin</keyname><forenames>Ignacio</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Vazquez</keyname><forenames>Alexei</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Exploring networks with traceroute-like probes: theory and simulations</title><categories>cs.NI cond-mat.other</categories><comments>This paper is related to cond-mat/0406404, with explorations of
  different networks and complementary discussions</comments><journal-ref>Theoretical Computer Science 355 (2006) 6</journal-ref><doi>10.1016/j.tcs.2005.12.009</doi><abstract>  Mapping the Internet generally consists in sampling the network from a
limited set of sources by using traceroute-like probes. This methodology, akin
to the merging of different spanning trees to a set of destination, has been
argued to introduce uncontrolled sampling biases that might produce statistical
properties of the sampled graph which sharply differ from the original ones. In
this paper we explore these biases and provide a statistical analysis of their
origin. We derive an analytical approximation for the probability of edge and
vertex detection that exploits the role of the number of sources and targets
and allows us to relate the global topological properties of the underlying
network with the statistical accuracy of the sampled graph. In particular, we
find that the edge and vertex detection probability depends on the betweenness
centrality of each element. This allows us to show that shortest path routed
sampling provides a better characterization of underlying graphs with broad
distributions of connectivity. We complement the analytical discussion with a
throughout numerical investigation of simulated mapping strategies in network
models with different topologies. We show that sampled graphs provide a fair
qualitative characterization of the statistical properties of the original
networks in a fair range of different strategies and exploration parameters.
Moreover, we characterize the level of redundancy and completeness of the
exploration process as a function of the topological properties of the network.
Finally, we study numerically how the fraction of vertices and edges discovered
in the sampled graph depends on the particular deployements of probing sources.
The results might hint the steps toward more efficient mapping strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412008</id><created>2004-12-02</created><updated>2005-08-18</updated><authors><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Mendel</keyname><forenames>Manor</forenames></author><author><keyname>Naor</keyname><forenames>Assaf</forenames></author></authors><title>Measured descent: A new embedding method for finite metrics</title><categories>cs.DS math.MG</categories><comments>17 pages. No figures. Appeared in FOCS '04. To appeaer in Geometric &amp;
  Functional Analysis. This version fixes a subtle error in Section 2.2</comments><journal-ref>Geom. Funct. Anal. 15(4):839-858, 2005</journal-ref><doi>10.1007/s00039-005-0527-6</doi><abstract>  We devise a new embedding technique, which we call measured descent, based on
decomposing a metric space locally, at varying speeds, according to the density
of some probability measure. This provides a refined and unified framework for
the two primary methods of constructing Frechet embeddings for finite metrics,
due to [Bourgain, 1985] and [Rao, 1999]. We prove that any n-point metric space
(X,d) embeds in Hilbert space with distortion O(sqrt{alpha_X log n}), where
alpha_X is a geometric estimate on the decomposability of X. As an immediate
corollary, we obtain an O(sqrt{(log lambda_X) \log n}) distortion embedding,
where \lambda_X is the doubling constant of X. Since \lambda_X\le n, this
result recovers Bourgain's theorem, but when the metric X is, in a sense,
``low-dimensional,'' improved bounds are achieved.
  Our embeddings are volume-respecting for subsets of arbitrary size. One
consequence is the existence of (k, O(log n)) volume-respecting embeddings for
all 1 \leq k \leq n, which is the best possible, and answers positively a
question posed by U. Feige. Our techniques are also used to answer positively a
question of Y. Rabinovich, showing that any weighted n-point planar graph
embeds in l_\infty^{O(log n)} with O(1) distortion. The O(log n) bound on the
dimension is optimal, and improves upon the previously known bound of O((log
n)^2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412009</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412009</id><created>2004-12-02</created><authors><author><keyname>Srijuntongsiri</keyname><forenames>Gun</forenames></author><author><keyname>Vavasis</keyname><forenames>Stephen A.</forenames></author></authors><title>A Fully Sparse Implementation of a Primal-Dual Interior-Point Potential
  Reduction Method for Semidefinite Programming</title><categories>cs.NA</categories><abstract>  In this paper, we show a way to exploit sparsity in the problem data in a
primal-dual potential reduction method for solving a class of semidefinite
programs. When the problem data is sparse, the dual variable is also sparse,
but the primal one is not. To avoid working with the dense primal variable, we
apply Fukuda et al.'s theory of partial matrix completion and work with partial
matrices instead. The other place in the algorithm where sparsity should be
exploited is in the computation of the search direction, where the gradient and
the Hessian-matrix product of the primal and dual barrier functions must be
computed in every iteration. By using an idea from automatic differentiation in
backward mode, both the gradient and the Hessian-matrix product can be computed
in time proportional to the time needed to compute the barrier functions of
sparse variables itself. Moreover, the high space complexity that is normally
associated with the use of automatic differentiation in backward mode can be
avoided in this case. In addition, we suggest a technique to efficiently
compute the determinant of the positive definite matrix completion that is
required to compute primal search directions. The method of obtaining one of
the primal search directions that minimizes the number of the evaluations of
the determinant of the positive definite completion is also proposed. We then
implement the algorithm and test it on the problem of finding the maximum cut
of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412010</id><created>2004-12-02</created><authors><author><keyname>Guiochet</keyname><forenames>Jeremie</forenames><affiliation>LAAS</affiliation></author><author><keyname>Motet</keyname><forenames>Gilles</forenames></author><author><keyname>Baron</keyname><forenames>Claude</forenames></author><author><keyname>Boy</keyname><forenames>Guy</forenames></author></authors><title>Toward a Human-Centered Uml for Risk Analysis</title><categories>cs.OH</categories><proxy>ccsd ccsd-00003461</proxy><acm-class>IFIP Conf</acm-class><journal-ref>Proc. of the 18th IFIP World Computer Congress (WCC), Human Error,
  Safety and Systems Development (HESSD04) (2004) 177-191</journal-ref><abstract>  Safety is now a major concern in many complex systems such as medical robots.
A way to control the complexity of such systems is to manage risk. The first
and important step of this activity is risk analysis. During risk analysis, two
main studies concerning human factors must be integrated: task analysis and
human error analysis. This multidisciplinary analysis often leads to a work
sharing between several stakeholders who use their own languages and
techniques. This often produces consistency errors and understanding
difficulties between them. Hence, this paper proposes to treat the risk
analysis on the common expression language UML (Unified Modeling Language) and
to handle human factors concepts for task analysis and human error analysis
based on the features of this language. The approach is applied to the
development of a medical robot for teleechography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412011</id><created>2004-12-02</created><authors><author><keyname>Avaliani</keyname><forenames>Archil</forenames></author></authors><title>Successful E-Business Systems - Paypal</title><categories>cs.OH</categories><comments>6 pages, Accepted at IADIS Conference, Lisbon. But because nobody was
  able to present this research paper, it didn't publish in proceedings.
  Extended version was written at International University under supervision of
  Prof.Keiichi Nakata. Keywords: PayPal, Payment Systems, E-business, Money
  Transactions, PayPal Analyses</comments><abstract>  PayPal is an account-based system that allows anyone with an email address to
send and receive online payment s. This service is easy to use for customers.
Members can instantaneously send money to anyone. Recipients are informed by
email that they have received a payment. PayPal is also available to people in
38 countries. This paper starts with introduction to the company and its
services. The information about the history and the current company situation
are covered. Later some interesting and different technical issues are
discussed. The Paper ends with analysis of the company and several future
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412012</id><created>2004-12-03</created><authors><author><keyname>Oriat</keyname><forenames>Catherine</forenames><affiliation>LSR - IMAG</affiliation></author></authors><title>Jartege: a Tool for Random Generation of Unit Tests for Java Classes</title><categories>cs.PL</categories><proxy>ccsd ccsd-00003466</proxy><report-no>RR-1069-I</report-no><acm-class>ACM: D.2 ; D.2.5</acm-class><abstract>  This report presents Jartege, a tool which allows random generation of unit
tests for Java classes specified in JML. JML (Java Modeling Language) is a
specification language for Java which allows one to write invariants for
classes, and pre- and postconditions for operations. As in the JML-JUnit tool,
we use JML specifications on the one hand to eliminate irrelevant test cases,
and on the other hand as a test oracle. Jartege randomly generates test cases,
which consist of a sequence of constructor and method calls for the classes
under test. The random aspect of the tool can be parameterized by associating
weights to classes and operations, and by controlling the number of instances
which are created for each class under test. The practical use of Jartege is
illustrated by a small case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412013</id><created>2004-12-03</created><authors><author><keyname>Dubacq</keyname><forenames>Jean-Christophe</forenames><affiliation>LIPN, GREYC</affiliation></author><author><keyname>Terrier</keyname><forenames>Veronique</forenames><affiliation>GREYC</affiliation></author></authors><title>Signals for Cellular Automata in dimension 2 or higher</title><categories>cs.CC cs.DC cs.DM math.CO</categories><proxy>ccsd ccsd-00003469</proxy><journal-ref>LATIN 2002: Theoretical Informatics (2002) 451-464</journal-ref><abstract>  We investigate how increasing the dimension of the array can help to draw
signals on cellular automata.We show the existence of a gap of constructible
signals in any dimension. We exhibit two cellular automata in dimension 2 to
show that increasing the dimension allows to reduce the number of states
required for some constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412014</id><created>2004-12-03</created><authors><author><keyname>Ravelomanana</keyname><forenames>Vlady</forenames><affiliation>LIPN</affiliation></author></authors><title>Randomized Initialization of a Wireless Multihop Network</title><categories>cs.DC cs.DM</categories><proxy>ccsd ccsd-00003470</proxy><acm-class>ACM Classification: C.2.1 Network architecture and design; F.2.2
  Nonnumerical algorithms and problems; G.3 Probability and statistics</acm-class><abstract>  Address autoconfiguration is an important mechanism required to set the IP
address of a node automatically in a wireless network. The address
autoconfiguration, also known as initialization or naming, consists to give a
unique identifier ranging from 1 to $n$ for a set of $n$ indistinguishable
nodes. We consider a wireless network where $n$ nodes (processors) are randomly
thrown in a square $X$, uniformly and independently. We assume that the network
is synchronous and two nodes are able to communicate if they are within
distance at most of $r$ of each other ($r$ is the transmitting/receiving
range). The model of this paper concerns nodes without the collision detection
ability: if two or more neighbors of a processor $u$ transmit concurrently at
the same time, then $u$ would not receive either messages. We suppose also that
nodes know neither the topology of the network nor the number of nodes in the
network. Moreover, they start indistinguishable, anonymous and unnamed. Under
this extremal scenario, we design and analyze a fully distributed protocol to
achieve the initialization task for a wireless multihop network of $n$ nodes
uniformly scattered in a square $X$. We show how the transmitting range of the
deployed stations can affect the typical characteristics such as the degrees
and the diameter of the network. By allowing the nodes to transmit at a range
$r= \sqrt{\frac{(1+\ell) \ln{n} \SIZE}{\pi n}}$ (slightly greater than the one
required to have a connected network), we show how to design a randomized
protocol running in expected time $O(n^{3/2} \log^2{n})$ in order to assign a
unique number ranging from 1 to $n$ to each of the $n$ participating nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412015</id><created>2004-12-03</created><updated>2005-03-11</updated><authors><author><keyname>Prescher</keyname><forenames>Detlef</forenames></author></authors><title>A Tutorial on the Expectation-Maximization Algorithm Including
  Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free
  Grammars</title><categories>cs.CL</categories><comments>Presented at the 15th European Summer School in Logic, Language and
  Information (ESSLLI 2003). Example 5 extended (and partially corrected)</comments><abstract>  The paper gives a brief review of the expectation-maximization algorithm
(Dempster 1977) in the comprehensible framework of discrete mathematics. In
Section 2, two prominent estimation methods, the relative-frequency estimation
and the maximum-likelihood estimation are presented. Section 3 is dedicated to
the expectation-maximization algorithm and a simpler variant, the generalized
expectation-maximization algorithm. In Section 4, two loaded dice are rolled. A
more interesting example is presented in Section 5: The estimation of
probabilistic context-free grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412016</id><created>2004-12-03</created><authors><author><keyname>Prescher</keyname><forenames>Detlef</forenames></author></authors><title>Inside-Outside Estimation Meets Dynamic EM</title><categories>cs.CL</categories><comments>4 pages, some typos corrected</comments><journal-ref>Proceedings of IWPT 2001</journal-ref><abstract>  We briefly review the inside-outside and EM algorithm for probabilistic
context-free grammars. As a result, we formally prove that inside-outside
estimation is a dynamic-programming variant of EM. This is interesting in its
own right, but even more when considered in a theoretical context since the
well-known convergence behavior of inside-outside estimation has been confirmed
by many experiments but apparently has never been formally proved. However,
being a version of EM, inside-outside estimation also inherits the good
convergence behavior of EM. Therefore, the as yet imperfect line of
argumentation can be transformed into a coherent proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412017</identifier>
 <datestamp>2008-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412017</id><created>2004-12-04</created><authors><author><keyname>Baset</keyname><forenames>Salman A.</forenames></author><author><keyname>Schulzrinne</keyname><forenames>Henning</forenames></author></authors><title>An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol</title><categories>cs.NI cs.MM</categories><report-no>CUCS-039-04</report-no><acm-class>C.2.2</acm-class><abstract>  Skype is a peer-to-peer VoIP client developed by KaZaa in 2003. Skype claims
that it can work almost seamlessly across NATs and firewalls and has better
voice quality than the MSN and Yahoo IM applications. It encrypts calls
end-to-end, and stores user information in a decentralized fashion. Skype also
supports instant messaging and conferencing. This report analyzes key Skype
functions such as login, NAT and firewall traversal, call establishment, media
transfer, codecs, and conferencing under three different network setups.
Analysis is performed by careful study of Skype network traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412018</id><created>2004-12-04</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author></authors><title>Modeling Complex Higher Order Patterns</title><categories>cs.DB cs.AI</categories><comments>11 pages</comments><report-no>Tr-04-11</report-no><abstract>  The goal of this paper is to show that generalizing the notion of frequent
patterns can be useful in extending association analysis to more complex higher
order patterns. To that end, we describe a general framework for modeling a
complex pattern based on evaluating the interestingness of its sub-patterns. A
key goal of any framework is to allow people to more easily express, explore,
and communicate ideas, and hence, we illustrate how our framework can be used
to describe a variety of commonly used patterns, such as frequent patterns,
frequent closed patterns, indirect association patterns, hub patterns and
authority patterns. To further illustrate the usefulness of the framework, we
also present two new kinds of patterns that derived from the framework: clique
pattern and bi-clique pattern and illustrate their practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412019</id><created>2004-12-04</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author></authors><title>A Link Clustering Based Approach for Clustering Categorical Data</title><categories>cs.DL cs.AI</categories><comments>10 pages</comments><journal-ref>A poster paper in Proc. of WAIM 2004</journal-ref><abstract>  Categorical data clustering (CDC) and link clustering (LC) have been
considered as separate research and application areas. The main focus of this
paper is to investigate the commonalities between these two problems and the
uses of these commonalities for the creation of new clustering algorithms for
categorical data based on cross-fertilization between the two disjoint research
fields. More precisely, we formally transform the CDC problem into an LC
problem, and apply LC approach for clustering categorical data. Experimental
results on real datasets show that LC based clustering method is competitive
with existing CDC algorithms with respect to clustering accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412020</id><created>2004-12-04</created><authors><author><keyname>Rogers</keyname><forenames>Paul</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael</forenames></author></authors><title>Towards Reliable Network Wide Broadcast in Mobile Ad Hoc Networks</title><categories>cs.NI</categories><comments>12 pages, 22 figures</comments><report-no>Tech Report: tr-cs-02-04</report-no><abstract>  Network-Wide Broadcast (NWB) is a common operation in Mobile Ad hoc Networks
(MANETs) used by routing protocols to discover routes and in group
communication operations. NWB is commonly performed via flooding, which has
been shown to be expensive in dense MANETs because of its high redundancy.
Several efforts have targeted reducing the redundancy of floods. In this work,
we target another problem that can substantially impact the success of NWBs:
since MAC level broadcasts are unreliable, it is possible for critical
rebroadcasts to be lost, leading to a significant drop in the node coverage.
This is especially true under heavy load and in sparse topologies. We show that
the techniques that target reducing the overhead of flooding, reduce its
inherent redundancy and harm its reliability. In addition, we show that static
approaches are more vulnerable to this problem. We then present a selective
rebroadcast approach to improve the robustness of NWBs. We show that our
approach leads to considerable improvement in NWB coverage relative to a
recently proposed solution to this problem, with a small increase in overhead.
The proposed approaches do not require proactive neighbor discovery and are
therefore resilient to mobility. Finally, the solution can be added to
virtually all NWB approaches to improve their reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412021</id><created>2004-12-06</created><authors><author><keyname>Choi</keyname><forenames>Chiu Wo</forenames></author><author><keyname>Harvey</keyname><forenames>Warwick</forenames></author><author><keyname>Lee</keyname><forenames>Jimmy Ho-Man</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Finite Domain Bounds Consistency Revisited</title><categories>cs.AI cs.LO</categories><comments>12 pages</comments><abstract>  A widely adopted approach to solving constraint satisfaction problems
combines systematic tree search with constraint propagation for pruning the
search space. Constraint propagation is performed by propagators implementing a
certain notion of consistency. Bounds consistency is the method of choice for
building propagators for arithmetic constraints and several global constraints
in the finite integer domain. However, there has been some confusion in the
definition of bounds consistency. In this paper we clarify the differences and
similarities among the three commonly used notions of bounds consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412022</id><created>2004-12-06</created><updated>2006-01-11</updated><authors><author><keyname>Potgieter</keyname><forenames>Petrus H.</forenames></author></authors><title>Zeno machines and hypercomputation</title><categories>cs.CC</categories><comments>11 pages. First submitted in December 2004, substantially revised in
  July and in November 2005. To appear in Theoretical Computer Science</comments><acm-class>F.1.1</acm-class><abstract>  This paper reviews the Church-Turing Thesis (or rather, theses) with
reference to their origin and application and considers some models of
&quot;hypercomputation&quot;, concentrating on perhaps the most straight-forward option:
Zeno machines (Turing machines with accelerating clock). The halting problem is
briefly discussed in a general context and the suggestion that it is an
inevitable companion of any reasonable computational model is emphasised. It is
hinted that claims to have &quot;broken the Turing barrier&quot; could be toned down and
that the important and well-founded role of Turing computability in the
mathematical sciences stands unchallenged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412023</id><created>2004-12-06</created><authors><author><keyname>Boinee</keyname><forenames>P.</forenames></author><author><keyname>Barbarino</keyname><forenames>F.</forenames></author><author><keyname>De Angelis</keyname><forenames>A.</forenames></author></authors><title>Multidimensional data classification with artificial neural networks</title><categories>cs.NE cs.AI</categories><comments>8 pages, 4 figures, Submitted to EURASIP Journal on Applied Signal
  Processing, 2004</comments><acm-class>F.1.1; K.3.2; I.2.6</acm-class><abstract>  Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412024</id><created>2004-12-06</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Human-Level Performance on Word Analogy Questions by Latent Relational
  Analysis</title><categories>cs.CL cs.IR cs.LG</categories><comments>32 pages, issued 2004, related work available at
  http://purl.org/peter.turney</comments><report-no>NRC-47422</report-no><acm-class>H.3.1; I.2.6; I.2.7</acm-class><abstract>  This paper introduces Latent Relational Analysis (LRA), a method for
measuring relational similarity. LRA has potential applications in many areas,
including information extraction, word sense disambiguation, machine
translation, and information retrieval. Relational similarity is correspondence
between relations, in contrast with attributional similarity, which is
correspondence between attributes. When two words have a high degree of
attributional similarity, we call them synonyms. When two pairs of words have a
high degree of relational similarity, we say that their relations are
analogous. For example, the word pair mason/stone is analogous to the pair
carpenter/wood. Past work on semantic similarity measures has mainly been
concerned with attributional similarity. Recently the Vector Space Model (VSM)
of information retrieval has been adapted to the task of measuring relational
similarity, achieving a score of 47% on a collection of 374 college-level
multiple-choice word analogy questions. In the VSM approach, the relation
between a pair of words is characterized by a vector of frequencies of
predefined patterns in a large corpus. LRA extends the VSM approach in three
ways: (1) the patterns are derived automatically from the corpus (they are not
predefined), (2) the Singular Value Decomposition (SVD) is used to smooth the
frequency data (it is also used this way in Latent Semantic Analysis), and (3)
automatically generated synonyms are used to explore reformulations of the word
pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent
to the average human score of 57%. On the related problem of classifying
noun-modifier relations, LRA achieves similar gains over the VSM, while using a
smaller corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412025</identifier>
 <datestamp>2007-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412025</id><created>2004-12-06</created><updated>2005-03-15</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Wortman</keyname><forenames>Kevin A.</forenames></author></authors><title>Minimum Dilation Stars</title><categories>cs.CG</categories><comments>6 pages, 3 figures</comments><acm-class>F.2.2; G.1.6</acm-class><journal-ref>Comp. Geom. Theory and Appl. 37(1):27-37, 2007</journal-ref><doi>10.1016/j.comgeo.2006.05.007</doi><abstract>  The dilation of a Euclidean graph is defined as the ratio of distance in the
graph divided by distance in R^d. In this paper we consider the problem of
positioning the root of a star such that the dilation of the resulting star is
minimal. We present a deterministic O(n log n)-time algorithm for evaluating
the dilation of a given star; a randomized O(n log n) expected-time algorithm
for finding an optimal center in R^d; and for the case d=2, a randomized O(n
2^(alpha(n)) log^2 n) expected-time algorithm for finding an optimal center
among the input points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412026</id><created>2004-12-07</created><authors><author><keyname>Choi</keyname><forenames>Chiu Wo</forenames></author><author><keyname>Lee</keyname><forenames>Jimmy Ho-Man</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Removing Propagation Redundant Constraints in Redundant Modeling</title><categories>cs.LO cs.AI</categories><comments>30 pages, submitted to ACM Transactions on Computational Logic (TOCL)</comments><acm-class>D.3.3; F.4.1</acm-class><abstract>  A widely adopted approach to solving constraint satisfaction problems
combines systematic tree search with various degrees of constraint propagation
for pruning the search space. One common technique to improve the execution
efficiency is to add redundant constraints, which are constraints logically
implied by others in the problem model. However, some redundant constraints are
propagation redundant and hence do not contribute additional propagation
information to the constraint solver. Redundant constraints arise naturally in
the process of redundant modeling where two models of the same problem are
connected and combined through channeling constraints. In this paper, we give
general theorems for proving propagation redundancy of one constraint with
respect to channeling constraints and constraints in the other model. We
illustrate, on problems from CSPlib (http://www.csplib.org/), how detecting and
removing propagation redundant constraints in redundant modeling can
significantly speed up constraint solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412027</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412027</id><created>2004-12-07</created><authors><author><keyname>Harder</keyname><forenames>Uli</forenames></author><author><keyname>Paczuski</keyname><forenames>Maya</forenames></author></authors><title>Correlated dynamics in human printing behavior</title><categories>cs.PF cond-mat.other</categories><comments>4 pages, 4 figures</comments><acm-class>D.4.8</acm-class><abstract>  Arrival times of requests to print in a student laboratory were analyzed.
Inter-arrival times between subsequent requests follow a universal scaling law
relating time intervals and the size of the request, indicating a scale
invariant dynamics with respect to the size. The cumulative distribution of
file sizes is well-described by a modified power law often seen in
non-equilibrium critical systems. For each user, waiting times between their
individual requests show long range dependence and are broadly distributed from
seconds to weeks. All results are incompatible with Poisson models, and may
provide evidence of critical dynamics associated with voluntary thought
processes in the brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412028</id><created>2004-12-08</created><authors><author><keyname>Baillot</keyname><forenames>Patrick</forenames></author><author><keyname>Terui</keyname><forenames>Kazushige</forenames></author></authors><title>A feasible algorithm for typing in Elementary Affine Logic</title><categories>cs.LO</categories><comments>20 pages</comments><abstract>  We give a new type inference algorithm for typing lambda-terms in Elementary
Affine Logic (EAL), which is motivated by applications to complexity and
optimal reduction. Following previous references on this topic, the variant of
EAL type system we consider (denoted EAL*) is a variant without sharing and
without polymorphism. Our algorithm improves over the ones already known in
that it offers a better complexity bound: if a simple type derivation for the
term t is given our algorithm performs EAL* type inference in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412029</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Kafiyatullov</keyname><forenames>Rustem R.</forenames></author><author><keyname>Safin</keyname><forenames>Ilsur T.</forenames></author></authors><title>The modular technology of development of the CAD expansions: profiles of
  outside networks of water supply and water drain</title><categories>cs.CE cs.DS</categories><comments>8 pages, 2 figures, in Russian</comments><acm-class>E.2; I.2.1; J.6</acm-class><abstract>  The modular technology of development of the problem-oriented CAD expansions
is applied to a task of designing of profiles of outside networks of water
supply and water drain with realization in program system TechnoCAD GlassX. The
unity of structure of this profiles is revealed, the system model of the
drawings of profiles of networks is developed including the structured
parametric representation (properties of objects and their interdependence,
general settings and default settings) and operations with it, which
efficiently automate designing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412030</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Kafiyatullov</keyname><forenames>Rustem R.</forenames></author><author><keyname>Safin</keyname><forenames>Ilsur T.</forenames></author></authors><title>The modular technology of development of the CAD expansions: protection
  of the buildings from the lightning</title><categories>cs.CE cs.DS</categories><comments>8 pages, 2 figures, in Russian</comments><acm-class>E.2; I.2.1; J.6</acm-class><abstract>  The modular technology of development of the problem-oriented CAD expansions
is applied to a task of designing of protection of the buildings from the
lightning with realization in program system TechnoCAD GlassX. The system model
of the drawings of lightning protection is developed including the structured
parametric representation (properties of objects and their interdependence,
general settings and default settings) and operations with it, which
efficiently automate designing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412031</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The Features of the Complex CAD system of Reconstruction of the
  Industrial Plants</title><categories>cs.CE</categories><comments>6 pages, no figures, in Russian</comments><acm-class>I.2.1; J.6</acm-class><abstract>  The features of designing of reconstruction of the acting plant by its design
department are considered: the results of work are drawings corresponding with
the national standards; large number of the small projects for different acting
objects; variety of the types of the drawings in one project; large paper
archive. The models and methods of developing of the complex CAD system with
friend uniform environment of designing, with setting a profile of operations,
with usage of the general parts of the project, with a series of
problem-oriented subsystems are described on an example of a CAD system
TechnoCAD GlassX
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412032</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The methods of support of the requirements of the Russian standards at
  development of a CAD of industrial objects</title><categories>cs.CE cs.DS</categories><comments>8 pages, 4 figures, in Russian</comments><acm-class>E.2; I.2.1; J.6</acm-class><abstract>  The methods of support of the requirements of the Russian standards in a CAD
of industrial objects are explained, which were implemented in the CAD system
TechnoCAD GlassX with an own graphics core and own structures of data storage.
It is rotined, that the binding of storage structures and program code of a CAD
to the requirements of standards enable not only to fulfil these requirements
in project documentation, but also to increase a degree of compactness of
storage of drawings both on the disk and in the RAM
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412033</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The modelling of the build constructions in a CAD of the renovation of
  the enterprises by means of units in the drawings</title><categories>cs.CE</categories><comments>8 pages, 4 figures, in Russian</comments><acm-class>E.2; I.2.1; J.6</acm-class><abstract>  The parametric model of build constructions and features of design operations
are described for making drawings, which are the common component of the
different parts of the projects of renovation of enterprises. The key moment of
the deep design automation is the using of so-called units in the drawings,
which are joining a visible graphic part and invisible parameters. The model
has passed check during designing of several hundreds of drawings
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412034</id><created>2004-12-08</created><authors><author><keyname>Migunov</keyname><forenames>Vladimir V.</forenames></author></authors><title>The informatization of design works at industry firm during its
  renovation</title><categories>cs.CE</categories><comments>9 pages, 3 figures, in Russian</comments><acm-class>I.2.1; J.6</acm-class><abstract>  The characteristic of design works on firm at its renovation and of the
common directions of their informatization is given. The implantation of a CAD
is selected as the key direction, and the requirements to a complex CAD-system
are stated. The methods of such a CAD-system development are featured, and the
connectedness of this development with the process of integration of
information space of design department of the firm is characterized. The
experience of development and implantation of a complex CAD of renovation of
firms TechnoCAD GlassX lies in a basis of this reviewing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412035</id><created>2004-12-08</created><authors><author><keyname>Amendolia</keyname><forenames>S R</forenames></author><author><keyname>Estrella</keyname><forenames>F</forenames></author><author><keyname>del Frate</keyname><forenames>C</forenames></author><author><keyname>Galvez</keyname><forenames>J</forenames></author><author><keyname>Hassan</keyname><forenames>W</forenames></author><author><keyname>Hauer</keyname><forenames>T</forenames></author><author><keyname>Manset</keyname><forenames>D</forenames></author><author><keyname>McClatchey</keyname><forenames>R</forenames></author><author><keyname>Odeh</keyname><forenames>M</forenames></author><author><keyname>Rogulin</keyname><forenames>D</forenames></author><author><keyname>Solomonides</keyname><forenames>T</forenames></author><author><keyname>Warren</keyname><forenames>R</forenames></author></authors><title>Deployment of a Grid-based Medical Imaging Application</title><categories>cs.DC cs.DB</categories><comments>10 pages, 5 figures. Accepted by the 2005 HealthGrid Conference</comments><acm-class>H2.4;J.3</acm-class><abstract>  The MammoGrid project has deployed its Service-Oriented Architecture
(SOA)-based Grid application in a real environment comprising actual
participating hospitals. The resultant setup is currently being exploited to
conduct rigorous in-house tests in the first phase before handing over the
setup to the actual clinicians to get their feedback. This paper elaborates the
deployment details and the experiences acquired during this phase of the
project. Finally the strategy regarding migration to an upcoming middleware
from EGEE project will be described. This paper concludes by highlighting some
of the potential areas of future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412036</id><created>2004-12-08</created><authors><author><keyname>El-Ghalayini</keyname><forenames>Haya</forenames></author><author><keyname>Odeh</keyname><forenames>Mohammed</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author></authors><title>Reverse Engineering Ontology to Conceptual Data Models</title><categories>cs.DC cs.DB</categories><comments>6 pages, 3 figures. Accepted by the IASTED International Conference
  on Databases and Applications (DBA 2005)</comments><acm-class>H2.4;J.3</acm-class><abstract>  Ontologies facilitate the integration of heterogeneous data sources by
resolving semantic heterogeneity between them. This research aims to study the
possibility of generating a domain conceptual model from a given ontology with
the vision to grow this generated conceptual data model into a global
conceptual model integrating a number of existing data and information sources.
Based on ontologically derived semantics of the BWW model, rules are identified
that map elements of the ontology language (DAML+OIL) to domain conceptual
model elements. This mapping is demonstrated using TAMBIS ontology. A
significant corollary of this study is that it is possible to generate a domain
conceptual model from a given ontology subject to validation that needs to be
performed by the domain specialist before evolving this model into a global
conceptual model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412037</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412037</id><created>2004-12-08</created><updated>2004-12-08</updated><authors><author><keyname>Chua</keyname><forenames>David B.</forenames></author><author><keyname>Kolaczyk</keyname><forenames>Eric D.</forenames></author><author><keyname>Crovella</keyname><forenames>Mark</forenames></author></authors><title>A Statistical Framework for Efficient Monitoring of End-to-End Network
  Properties</title><categories>cs.NI math.ST stat.TH</categories><comments>20 pages, 18 figures</comments><abstract>  Network service providers and customers are often concerned with aggregate
performance measures that span multiple network paths. Unfortunately, forming
such network-wide measures can be difficult, due to the issues of scale
involved. In particular, the number of paths grows too rapidly with the number
of endpoints to make exhaustive measurement practical. As a result, there is
interest in the feasibility of methods that dramatically reduce the number of
paths measured in such situations while maintaining acceptable accuracy.
  In previous work we proposed a statistical framework to efficiently address
this problem, in the context of additive metrics such as delay and loss rate,
for which the per-path metric is a sum of (possibly transformed) per-link
measures. The key to our method lies in the observation and exploitation of
significant redundancy in network paths (sharing of common links).
  In this paper we make three contributions: (1) we generalize the framework to
make it more immediately applicable to network measurements encountered in
practice; (2) we demonstrate that the observed path redundancy upon which our
method is based is robust to variation in key network conditions and
characteristics, including link failures; and (3) we show how the framework may
be applied to address three practical problems of interest to network providers
and customers, using data from an operating network. In particular, we show how
appropriate selection of small sets of path measurements can be used to
accurately estimate network-wide averages of path delays, to reliably detect
network anomalies, and to effectively make a choice between alternative
sub-networks, as a customer choosing between two providers or two ingress
points into a provider network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412038</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412038</id><created>2004-12-08</created><authors><author><keyname>Lai</keyname><forenames>Kevin</forenames></author><author><keyname>Rasmusson</keyname><forenames>Lars</forenames></author><author><keyname>Adar</keyname><forenames>Eytan</forenames></author><author><keyname>Sorkin</keyname><forenames>Stephen</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Tycoon: an Implementation of a Distributed, Market-based Resource
  Allocation System</title><categories>cs.DC cs.OS</categories><acm-class>C.2.4; D.4.1; K.6.4</acm-class><abstract>  Distributed clusters like the Grid and PlanetLab enable the same statistical
multiplexing efficiency gains for computing as the Internet provides for
networking. One major challenge is allocating resources in an economically
efficient and low-latency way. A common solution is proportional share, where
users each get resources in proportion to their pre-defined weight. However,
this does not allow users to differentiate the value of their jobs. This leads
to economic inefficiency. In contrast, systems that require reservations impose
a high latency (typically minutes to hours) to acquire resources.
  We present Tycoon, a market based distributed resource allocation system
based on proportional share. The key advantages of Tycoon are that it allows
users to differentiate the value of their jobs, its resource acquisition
latency is limited only by communication delays, and it imposes no manual
bidding overhead on users. We present experimental results using a prototype
implementation of our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412039</id><created>2004-12-09</created><authors><author><keyname>Chatel</keyname><forenames>Marc</forenames></author><author><keyname>Dagenais</keyname><forenames>Michel</forenames></author><author><keyname>Levert</keyname><forenames>Charles</forenames></author><author><keyname>Pourzandi</keyname><forenames>Makan</forenames></author></authors><title>Security in Carrier Class Server Applications for All-IP Networks</title><categories>cs.NI</categories><comments>Survey paper on the challenges of all IP networks in telecom
  applications</comments><abstract>  A revolution is taking place in telecommunication networks. New services are
appearing on platforms such as third generation cellular phones (3G) and
broadband Internet access. This motivates the transition from mostly switched
to all-IP networks. The replacement of the traditional shallow and well-defined
interface to telephony networks brings accrued flexibility, but also makes the
network accordingly difficult to properly secure. This paper surveys the
implications of this transition on security issues in telecom applications. It
does not give an exhaustive list of security tools or security protocols. Its
goal is rather to initiate the reader to the security issues brought to carrier
class servers by this revolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412040</id><created>2004-12-09</created><authors><author><keyname>Burger</keyname><forenames>J. R.</forenames></author></authors><title>Data-stationary Architecture to Execute Quantum Algorithms Classically</title><categories>cs.AR</categories><acm-class>C.1.2</acm-class><abstract>  This paper presents a data stationary architecture in which each word has an
attached address field. Address fields massively update in parallel to record
data interchanges. Words do not move until memory is read for post processing.
A sea of such cells can test large-scale quantum algorithms, although other
programming is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412041</id><created>2004-12-09</created><updated>2005-01-02</updated><authors><author><keyname>Guo</keyname><forenames>Hai-Feng</forenames></author><author><keyname>Gupta</keyname><forenames>Gopal</forenames></author></authors><title>An Efficient and Flexible Engine for Computing Fixed Points</title><categories>cs.PL cs.AI cs.LO</categories><comments>26 pages</comments><abstract>  An efficient and flexible engine for computing fixed points is critical for
many practical applications. In this paper, we firstly present a goal-directed
fixed point computation strategy in the logic programming paradigm. The
strategy adopts a tabled resolution (or memorized resolution) to mimic the
efficient semi-naive bottom-up computation. Its main idea is to dynamically
identify and record those clauses that will lead to recursive variant calls,
and then repetitively apply those alternatives incrementally until the fixed
point is reached. Secondly, there are many situations in which a fixed point
contains a large number or even infinite number of solutions. In these cases, a
fixed point computation engine may not be efficient enough or feasible at all.
We present a mode-declaration scheme which provides the capabilities to reduce
a fixed point from a big solution set to a preferred small one, or from an
infeasible infinite set to a finite one. The mode declaration scheme can be
characterized as a meta-level operation over the original fixed point. We show
the correctness of the mode declaration scheme. Thirdly, the mode-declaration
scheme provides a new declarative method for dynamic programming, which is
typically used for solving optimization problems. There is no need to define
the value of an optimal solution recursively, instead, defining a general
solution suffices. The optimal value as well as its corresponding concrete
solution can be derived implicitly and automatically using a mode-directed
fixed point computation engine. Finally, this fixed point computation engine
has been successfully implemented in a commercial Prolog system. Experimental
results are shown to indicate that the mode declaration improves both time and
space performances in solving dynamic programming problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412042</id><created>2004-12-10</created><authors><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>Klasson</keyname><forenames>Mikael</forenames></author><author><keyname>Krokhin</keyname><forenames>Andrei</forenames></author></authors><title>The approximability of three-valued MAX CSP</title><categories>cs.CC</categories><abstract>  In the maximum constraint satisfaction problem (Max CSP), one is given a
finite collection of (possibly weighted) constraints on overlapping sets of
variables, and the goal is to assign values from a given domain to the
variables so as to maximize the number (or the total weight, for the weighted
case) of satisfied constraints. This problem is NP-hard in general, and,
therefore, it is natural to study how restricting the allowed types of
constraints affects the approximability of the problem. It is known that every
Boolean (that is, two-valued) Max CSP problem with a finite set of allowed
constraint types is either solvable exactly in polynomial time or else
APX-complete (and hence can have no polynomial time approximation scheme unless
P=NP. It has been an open problem for several years whether this result can be
extended to non-Boolean Max CSP, which is much more difficult to analyze than
the Boolean case. In this paper, we make the first step in this direction by
establishing this result for Max CSP over a three-element domain. Moreover, we
present a simple description of all polynomial-time solvable cases of our
problem. This description uses the well-known algebraic combinatorial property
of supermodularity. We also show that every hard three-valued Max CSP problem
contains, in a certain specified sense, one of the two basic hard Max CSP
problems which are the Maximum k-colourable subgraph problems for k=2,3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412043</id><created>2004-12-10</created><authors><author><keyname>Bagnara</keyname><forenames>Roberto</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author><author><keyname>Mazzi</keyname><forenames>Elena</forenames></author><author><keyname>Zaffanella</keyname><forenames>Enea</forenames></author></authors><title>Widening Operators for Weakly-Relational Numeric Abstractions (Extended
  Abstract)</title><categories>cs.PL</categories><acm-class>F.3.2</acm-class><abstract>  We discuss the divergence problems recently identified in some extrapolation
operators for weakly-relational numeric domains. We identify the cause of the
divergences and point out that resorting to more concrete, syntactic domains
can be avoided by researching suitable algorithms for the elimination of
redundant constraints in the chosen representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412044</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412044</id><created>2004-12-10</created><authors><author><keyname>Bhargavan</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Fournet</keyname><forenames>Cedric</forenames></author><author><keyname>Gordon</keyname><forenames>Andrew D.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>TulaFale: A Security Tool for Web Services</title><categories>cs.CR</categories><comments>26 pages, 4 figures. Appears in Proceedings of the 2nd International
  Symposium on Formal Methods for Components and Objects (FMCS'03), LNCS 3188,
  pp. 197-222</comments><acm-class>D.4.6; C.2.6</acm-class><abstract>  Web services security specifications are typically expressed as a mixture of
XML schemas, example messages, and narrative explanations. We propose a new
specification language for writing complementary machine-checkable descriptions
of SOAP-based security protocols and their properties. Our TulaFale language is
based on the pi calculus (for writing collections of SOAP processors running in
parallel), plus XML syntax (to express SOAP messaging), logical predicates (to
construct and filter SOAP messages), and correspondence assertions (to specify
authentication goals of protocols). Our implementation compiles TulaFale into
the applied pi calculus, and then runs Blanchet's resolution-based protocol
verifier. Hence, we can automatically verify authentication properties of SOAP
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412045</id><created>2004-12-10</created><authors><author><keyname>Gordon</keyname><forenames>Andrew D.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Validating a Web Service Security Abstraction by Typing</title><categories>cs.CR</categories><comments>44 pages. A preliminary version appears in the Proceedings of the
  Workshop on XML Security 2002, pp. 18-29, November 2002</comments><acm-class>D.4.6; C.2.6; D.2.4</acm-class><journal-ref>Formal Aspects of Computing 17 (3), pp. 277-318, 2005</journal-ref><abstract>  An XML web service is, to a first approximation, an RPC service in which
requests and responses are encoded in XML as SOAP envelopes, and transported
over HTTP. We consider the problem of authenticating requests and responses at
the SOAP-level, rather than relying on transport-level security. We propose a
security abstraction, inspired by earlier work on secure RPC, in which the
methods exported by a web service are annotated with one of three security
levels: none, authenticated, or both authenticated and encrypted. We model our
abstraction as an object calculus with primitives for defining and calling web
services. We describe the semantics of our object calculus by translating to a
lower-level language with primitives for message passing and cryptography. To
validate our semantics, we embed correspondence assertions that specify the
correct authentication of requests and responses. By appeal to the type theory
for cryptographic protocols of Gordon and Jeffrey's Cryptyc, we verify the
correspondence assertions simply by typing. Finally, we describe an
implementation of our semantics via custom SOAP headers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412046</id><created>2004-12-10</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Quasiconvex Programming</title><categories>cs.CG</categories><comments>33 pages, 14 figures</comments><acm-class>F.2.2; G.1.6</acm-class><abstract>  We define quasiconvex programming, a form of generalized linear programming
in which one seeks the point minimizing the pointwise maximum of a collection
of quasiconvex functions. We survey algorithms for solving quasiconvex programs
either numerically or via generalizations of the dual simplex method from
linear programming, and describe varied applications of this geometric
optimization technique in meshing, scientific computation, information
visualization, automated algorithm analysis, and robust statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412047</id><created>2004-12-10</created><authors><author><keyname>Rodriguez</keyname><forenames>Marko</forenames></author><author><keyname>Steinbock</keyname><forenames>Daniel</forenames></author></authors><title>A Social Network for Societal-Scale Decision-Making Systems</title><categories>cs.CY cs.DS cs.HC</categories><comments>Dynamically Distributed Democracy algorithm presented in the arena of
  a societal-scale decision support system</comments><acm-class>H.4.2; J.7; K.4.m</acm-class><journal-ref>North American Association for Computational Social and
  Organizational Science Conference Proceedings 2004</journal-ref><abstract>  In societal-scale decision-making systems the collective is faced with the
problem of ensuring that the derived group decision is in accord with the
collective's intention. In modern systems, political institutions have
instatiated representative forms of decision-making to ensure that every
individual in the society has a participatory voice in the decision-making
behavior of the whole--even if only indirectly through representation. An
agent-based simulation demonstrates that in modern representative systems, as
the ratio of representatives increases, there exists an exponential decrease in
the ability for the group to behave in accord with the desires of the whole. To
remedy this issue, this paper provides a novel representative power structure
for decision-making that utilizes a social network and power distribution
algorithm to maintain the collective's perspective over varying degrees of
participation and/or ratios of representation. This work shows promise for the
future development of policy-making systems that are supported by the computer
and network infrastructure of our society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412048</id><created>2004-12-11</created><authors><author><keyname>Formenti</keyname><forenames>Enrico</forenames><affiliation>I3S</affiliation></author><author><keyname>Masson</keyname><forenames>Benoit</forenames><affiliation>I3S</affiliation></author></authors><title>On computing fixed points for generalized sandpiles</title><categories>cs.CC</categories><comments>Presented in DMCS 2004 (Turku, FINLAND)</comments><proxy>ccsd ccsd-00003535</proxy><abstract>  We prove fixed points results for sandpiles starting with arbitrary initial
conditions. We give an effective algorithm for computing such fixed points, and
we refine it in the particular case of SPM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412049</id><created>2004-12-11</created><authors><author><keyname>Janglova</keyname><forenames>Danica</forenames></author></authors><title>Neural Networks in Mobile Robot Motion</title><categories>cs.RO cs.AI</categories><comments>9 Pages</comments><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 1, March 2004, pp.15-22</journal-ref><abstract>  This paper deals with a path planning and intelligent control of an
autonomous robot which should move safely in partially structured environment.
This environment may involve any number of obstacles of arbitrary shape and
size; some of them are allowed to move. We describe our approach to solving the
motion-planning problem in mobile robot control using neural networks-based
technique. Our method of the construction of a collision-free path for moving
robot among obstacles is based on two neural networks. The first neural network
is used to determine the &quot;free&quot; space using ultrasound range finder data. The
second neural network &quot;finds&quot; a safe direction for the next robot section of
the path in the workspace while avoiding the nearest obstacles. Simulation
examples of generated path with proposed techniques will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412050</id><created>2004-12-11</created><authors><author><keyname>Ou</keyname><forenames>Yongsheng</forenames></author><author><keyname>Xu</keyname><forenames>Yangsheng</forenames></author></authors><title>Gyroscopically Stabilized Robot: Balance and Tracking</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 1, March 2004, pp.23-32</journal-ref><abstract>  The single wheel, gyroscopically stabilized robot - Gyrover, is a dynamically
stable but statically unstable, underactuated system. In this paper, based on
the dynamic model of the robot, we investigate two classes of nonholonomic
constraints associated with the system. Then, based on the backstepping
technology, we propose a control law for balance control of Gyrover. Next,
through transferring the systems states from Cartesian coordinate to polar
coordinate, control laws for point-to-point control and line tracking in
Cartesian space are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412051</identifier>
 <datestamp>2011-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412051</id><created>2004-12-11</created><authors><author><keyname>Adria</keyname><forenames>Oliver</forenames></author><author><keyname>Streich</keyname><forenames>Hermann</forenames></author><author><keyname>Hertzberg</keyname><forenames>Joachim</forenames></author></authors><title>Dynamic replanning in uncertain environments for a sewer inspection
  robot</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 1, March 2004, pp.33-38</journal-ref><abstract>  The sewer inspection robot MAKRO is an autonomous multi-segment robot with
worm-like shape driven by wheels. It is currently under development in the
project MAKRO-PLUS. The robot has to navigate autonomously within sewer
systems. Its first tasks will be to take water probes, analyze it onboard, and
measure positions of manholes and pipes to detect polluted-loaded sewage and to
improve current maps of sewer systems. One of the challenging problems is the
controller software, which should enable the robot to navigate in the sewer
system and perform the inspection tasks autonomously, not inflicting any
self-damage. This paper focuses on the route planning and replanning aspect of
the robot. The robots software has four different levels, of which the planning
system is the highest level, and the remaining three are controller levels each
with a different degree of abstraction. The planner coordinates the sequence of
actions that are to be successively executed by the robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412052</id><created>2004-12-11</created><authors><author><keyname>Michel</keyname><forenames>Olivier</forenames></author></authors><title>WebotsTM: Professional Mobile Robot Simulation</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 1, March 2004, pp.39-42</journal-ref><abstract>  Cyberbotics Ltd. develops WebotsTM, a mobile robotics simulation software
that provides you with a rapid prototyping environment for modelling,
programming and simulating mobile robots. The provided robot libraries enable
you to transfer your control programs to several commercially available real
mobile robots. WebotsTM lets you define and modify a complete mobile robotics
setup, even several different robots sharing the same environment. For each
object, you can define a number of properties, such as shape, color, texture,
mass, friction, etc. You can equip each robot with a large number of available
sensors and actuators. You can program these robots using your favorite
development environment, simulate them and optionally transfer the resulting
programs onto your real robots. WebotsTM has been developed in collaboration
with the Swiss Federal Institute of Technology in Lausanne, thoroughly tested,
well documented and continuously maintained for over 7 years. It is now the
main commercial product available from Cyberbotics Ltd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412053</id><created>2004-12-11</created><authors><author><keyname>Ata</keyname><forenames>Atef A.</forenames></author><author><keyname>Johar</keyname><forenames>Habib</forenames></author></authors><title>Dynamic simulation of task constrained of a rigid-flexible manipulator</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 2, June 2004, pp.61-66</journal-ref><abstract>  A rigid-flexible manipulator may be assigned tasks in a moving environment
where the winds or vibrations affect the position and/or orientation of surface
of operation. Consequently, losses of the contact and perhaps degradation of
the performance may occur as references are changed. When the environment is
moving, knowledge of the angle &amp;#945; between the contact surface and the
horizontal is required at every instant. In this paper, different profiles for
the time varying angle &amp;#945; are proposed to investigate the effect of this
change into the contact force and the joint torques of a rigid-flexible
manipulator. The coefficients of the equation of the proposed rotating surface
are changing with time to determine the new X and Y coordinates of the moving
surface as the surface rotates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412054</id><created>2004-12-11</created><authors><author><keyname>Galantucci</keyname><forenames>L. M.</forenames></author><author><keyname>Percoco</keyname><forenames>G.</forenames></author><author><keyname>Spina</keyname><forenames>R.</forenames></author></authors><title>Assembly and Disassembly Planning by using Fuzzy Logic &amp; Genetic
  Algorithms</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 2, June 2004, pp.67-74</journal-ref><abstract>  The authors propose the implementation of hybrid Fuzzy Logic-Genetic
Algorithm (FL-GA) methodology to plan the automatic assembly and disassembly
sequence of products. The GA-Fuzzy Logic approach is implemented onto two
levels. The first level of hybridization consists of the development of a Fuzzy
controller for the parameters of an assembly or disassembly planner based on
GAs. This controller acts on mutation probability and crossover rate in order
to adapt their values dynamically while the algorithm runs. The second level
consists of the identification of theoptimal assembly or disassembly sequence
by a Fuzzy function, in order to obtain a closer control of the technological
knowledge of the assembly/disassembly process. Two case studies were analyzed
in order to test the efficiency of the Fuzzy-GA methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412055</id><created>2004-12-11</created><authors><author><keyname>Kypson</keyname><forenames>Alan P.</forenames></author><author><keyname>Chitwood</keyname><forenames>W. Randolph</forenames><suffix>Jr</suffix></author></authors><title>Robotic Applications in Cardiac Surgery</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 2, June 2004, pp.87-92</journal-ref><abstract>  Traditionally, cardiac surgery has been performed through a median
sternotomy, which allows the surgeon generous access to the heart and
surrounding great vessels. As a paradigm shift in the size and location of
incisions occurs in cardiac surgery, new methods have been developed to allow
the surgeon the same amount of dexterity and accessibility to the heart in
confined spaces and in a less invasive manner. Initially, long instruments
without pivot points were used, however, more recent robotic telemanipulation
systems have been applied that allow for improved dexterity, enabling the
surgeon to perform cardiac surgery from a distance not previously possible. In
this rapidly evolving field, we review the recent history and clinical results
of using robotics in cardiac surgery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412056</id><created>2004-12-11</created><authors><author><keyname>Pashenkov</keyname><forenames>Nikita</forenames></author><author><keyname>Iwamasa</keyname><forenames>Ryuichi</forenames></author></authors><title>One-Chip Solution to Intelligent Robot Control: Implementing Hexapod
  Subsumption Architecture Using a Contemporary Microprocessor</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 2, June 2004, pp. 93-98</journal-ref><abstract>  This paper introduces a six-legged autonomous robot managed by a single
controller and a software core modeled on subsumption architecture. We begin by
discussing the features and capabilities of IsoPod, a new processor for
robotics which has enabled a streamlined implementation of our project. We
argue that this processor offers a unique set of hardware and software
features, making it a practical development platform for robotics in general
and for subsumption-based control architectures in particular. Next, we
summarize original ideas on subsumption architecture implementation for a
six-legged robot, as presented by its inventor Rodney Brooks in 1980s. A
comparison is then made to a more recent example of a hexapod control
architecture based on subsumption. The merits of both systems are analyzed and
a new subsumption architecture layout is formulated as a response. We conclude
with some remarks regarding the development of this project as a hint at new
potentials for intelligent robot design, opened by a recent development in
embedded controller market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412057</id><created>2004-12-11</created><authors><author><keyname>Vukobratovic</keyname><forenames>Miomir</forenames></author><author><keyname>Andric</keyname><forenames>Dejan</forenames></author><author><keyname>Borovac</keyname><forenames>Branislav</forenames></author></authors><title>How to achieve various gait patterns from single nominal</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Robotic Systems, Volume 1,
  Number 2, June 2004, pp. 99-108</journal-ref><abstract>  In this paper is presented an approach to achieving on-line modification of
nominal biped gait without recomputing entire dynamics when steady motion is
performed. Straight, dynamically balanced walk was used as a nominal gait, and
applied modifications were speed-up and slow-down walk and turning left and
right. It is shown that the disturbances caused by these modifications
jeopardize dynamic stability, but they can be simply compensated to enable walk
continuation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412058</id><created>2004-12-13</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author><author><keyname>Huang</keyname><forenames>Joshua Zhexue</forenames></author></authors><title>Clustering Categorical Data Streams</title><categories>cs.DB cs.AI</categories><comments>23 pages. To Appear in Journal of Computational Methods on Science
  and Engineering(JCMSE)</comments><abstract>  The data stream model has been defined for new classes of applications
involving massive data being generated at a fast pace. Web click stream
analysis and detection of network intrusions are two examples. Cluster analysis
on data streams becomes more difficult, because the data objects in a data
stream must be accessed in order and can be read only once or few times with
limited resources. Recently, a few clustering algorithms have been developed
for analyzing numeric data streams. However, to our knowledge to date, no
algorithm exists for clustering categorical data streams. In this paper, we
propose an efficient clustering algorithm for analyzing categorical data
streams. It has been proved that the proposed algorithm uses small memory
footprints. We provide empirical analysis on the performance of the algorithm
in clustering both synthetic and real data streams
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412059</id><created>2004-12-13</created><authors><author><keyname>Gayler</keyname><forenames>Ross W.</forenames></author></authors><title>Vector Symbolic Architectures answer Jackendoff's challenges for
  cognitive neuroscience</title><categories>cs.NE cs.AI</categories><comments>This is a slightly updated version of the paper presented at the
  Joint International Conference on Cognitive Science, 13-17 July 2003,
  University of New South Wales, Sydney, Australia. 6 pages</comments><acm-class>I.5.1; I.2.0, I.2.6</acm-class><abstract>  Jackendoff (2002) posed four challenges that linguistic combinatoriality and
rules of language present to theories of brain function. The essence of these
problems is the question of how to neurally instantiate the rapid construction
and transformation of the compositional structures that are typically taken to
be the domain of symbolic processing. He contended that typical connectionist
approaches fail to meet these challenges and that the dialogue between
linguistic theory and cognitive neuroscience will be relatively unproductive
until the importance of these problems is widely recognised and the challenges
answered by some technical innovation in connectionist modelling. This paper
claims that a little-known family of connectionist models (Vector Symbolic
Architectures) are able to meet Jackendoff's challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412060</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412060</id><created>2004-12-13</created><updated>2005-07-01</updated><authors><author><keyname>Hoesli</keyname><forenames>Daniel</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>Monotonicity Results for Coherent MIMO Rician Channels</title><categories>cs.IT math.IT</categories><comments>14 pages, submitted to IEEE Transactions on Information Theory</comments><abstract>  The dependence of the Gaussian input information rate on the line-of-sight
(LOS) matrix in multiple-input multiple-output coherent Rician fading channels
is explored. It is proved that the outage probability and the mutual
information induced by a multivariate circularly symmetric Gaussian input with
any covariance matrix are monotonic in the LOS matrix D, or more precisely,
monotonic in D'D in the sense of the Loewner partial order. Conversely, it is
also demonstrated that this ordering on the LOS matrices is a necessary
condition for the uniform monotonicity over all input covariance matrices. This
result is subsequently applied to prove the monotonicity of the isotropic
Gaussian input information rate and channel capacity in the singular values of
the LOS matrix. Extensions to multiple-access channels are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412061</id><created>2004-12-13</created><authors><author><keyname>Duchamp</keyname><forenames>Gerard Henry Edmond</forenames><affiliation>LIPN</affiliation></author><author><keyname>Luque</keyname><forenames>Jean-Gabriel</forenames><affiliation>IGM</affiliation></author><author><keyname>Penson</keyname><forenames>Karol A.</forenames><affiliation>LPTL</affiliation></author><author><keyname>Tollu</keyname><forenames>Christophe</forenames><affiliation>LIPN</affiliation></author></authors><title>Free quasi-symmetric functions, product actions and quantum field theory
  of partitions</title><categories>cs.SC math.CO quant-ph</categories><comments>Submitted 28.11.04</comments><proxy>ccsd ccsd-00003552</proxy><abstract>  We examine two associative products over the ring of symmetric functions
related to the intransitive and Cartesian products of permutation groups. As an
application, we give an enumeration of some Feynman type diagrams arising in
Bender's QFT of partitions. We end by exploring possibilities to construct
noncommutative analogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412062</id><created>2004-12-14</created><updated>2005-04-21</updated><authors><author><keyname>Bauland</keyname><forenames>Michael</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author></authors><title>Isomorphic Implication</title><categories>cs.CC</categories><comments>22 pages; corrected typos and minor errors</comments><acm-class>F.2.2</acm-class><abstract>  We study the isomorphic implication problem for Boolean constraints. We show
that this is a natural analog of the subgraph isomorphism problem. We prove
that, depending on the set of constraints, this problem is in P, NP-complete,
or NP-hard, coNP-hard, and in parallel access to NP. We show how to extend the
NP-hardness and coNP-hardness to hardness for parallel access to NP for some
cases, and conjecture that this can be done in all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412063</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412063</id><created>2004-12-15</created><updated>2006-03-08</updated><authors><author><keyname>Huth</keyname><forenames>Michael</forenames></author></authors><title>Labelled transition systems as a Stone space</title><categories>cs.LO</categories><comments>Changes since v2: Metadata update</comments><acm-class>F.3.2; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (January
  26, 2005) lmcs:950</journal-ref><doi>10.2168/LMCS-1(1:1)2005</doi><abstract>  A fully abstract and universal domain model for modal transition systems and
refinement is shown to be a maximal-points space model for the bisimulation
quotient of labelled transition systems over a finite set of events. In this
domain model we prove that this quotient is a Stone space whose compact,
zero-dimensional, and ultra-metrizable Hausdorff topology measures the degree
of bisimilarity such that image-finite labelled transition systems are dense.
Using this compactness we show that the set of labelled transition systems that
refine a modal transition system, its ''set of implementations'', is compact
and derive a compactness theorem for Hennessy-Milner logic on such
implementation sets. These results extend to systems that also have partially
specified state propositions, unify existing denotational, operational, and
metric semantics on partial processes, render robust consistency measures for
modal transition systems, and yield an abstract interpretation of compact sets
of labelled transition systems as Scott-closed sets of modal transition
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412064</id><created>2004-12-15</created><authors><author><keyname>Steinbock</keyname><forenames>Dan</forenames></author><author><keyname>Kaplan</keyname><forenames>Craig</forenames></author><author><keyname>Rodriguez</keyname><forenames>Marko</forenames></author><author><keyname>Diaz</keyname><forenames>Juana</forenames></author><author><keyname>Der</keyname><forenames>Newton</forenames></author><author><keyname>Garcia</keyname><forenames>Suzanne</forenames></author></authors><title>Collective Intelligence Quanitifed for Computer-Mediated Group Problem
  Solving</title><categories>cs.CY cs.HC cs.OH</categories><comments>University of California, Santa Cruz Tech Report UCSC-CRL-02-28</comments><acm-class>H.1.2; H.4.2</acm-class><abstract>  Collective Intelligence (CI) is the ability of a group to exhibit greater
intelligence than its individual members. Expressed by the common saying that
&quot;two minds are better than one,&quot; CI has been a topic of interest for social
psychology and the information sciences. Computer mediation adds a new element
in the form of distributed networks and group support systems. These facilitate
highly organized group activities that were all but impossible before computer
mediation. This paper presents experimental findings on group problem solving
where a distributed software system automatically integrates input from many
humans. In order to quantify Collective Intelligence, we compare the
performance of groups to individuals when solving a mathematically formalized
problem. This study shows that groups can outperform individuals on difficult
but not easy problems, though groups are slower to produce solutions. The
subjects are 57 university students. The task is the 8-Puzzle sliding tile
game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412065</id><created>2004-12-16</created><authors><author><keyname>Chong</keyname><forenames>Stephen</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>A Framework for Creating Natural Language User Interfaces for
  Action-Based Applications</title><categories>cs.CL cs.HC</categories><comments>25 pages, 1 figure. A preliminary version of this paper appeared in
  the Proceedings of the Third International AMAST Workshop on Algebraic
  Methods in Language Processing, TWLT Report 21, pp. 83-98, 2003</comments><acm-class>H.5.2; I.2.7; F.4.2; F.4.1</acm-class><abstract>  In this paper we present a framework for creating natural language interfaces
to action-based applications. Our framework uses a number of reusable
application-independent components, in order to reduce the effort of creating a
natural language interface for a given application. Using a type-logical
grammar, we first translate natural language sentences into expressions in an
extended higher-order logic. These expressions can be seen as executable
specifications corresponding to the original sentences. The executable
specifications are then interpreted by invoking appropriate procedures provided
by the application for which a natural language interface is being created.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412066</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Pina</keyname><forenames>Pedro</forenames></author><author><keyname>Muge</keyname><forenames>Fernando</forenames></author></authors><title>From Feature Extraction to Classification: A multidisciplinary Approach
  applied to Portuguese Granites</title><categories>cs.AI cs.CV</categories><comments>8 pages, 6 figures, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_21.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>SCIA 99, 11th Scandinavian Conf. on Image Analysis, ISBN
  87-88306-42-9, Vol.2, pp. 817-824, Kangerlussuaq, Greenland, 7-11, June 1999</journal-ref><abstract>  The purpose of this paper is to present a complete methodology based on a
multidisciplinary approach, that goes from the extraction of features till the
classification of a set of different portuguese granites. The set of tools to
extract the features that characterise polished surfaces of the granites is
mainly based on mathematical morphology. The classification methodology is
based on a genetic algorithm capable of search the input feature space used by
the nearest neighbour rule classifier. Results show that is adequate to perform
feature reduction and simultaneous improve the recognition rate. Moreover, the
present methodology represents a robust strategy to understand the proper
nature of the images treated, and their discriminant features. KEYWORDS:
Portuguese grey granites, feature extraction, mathematical morphology, feature
reduction, genetic algorithms, nearest neighbour rule classifiers (k-NNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412067</identifier>
 <datestamp>2008-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412067</id><created>2004-12-17</created><authors><author><keyname>Sezgin</keyname><forenames>A.</forenames></author><author><keyname>Oechtering</keyname><forenames>T. J.</forenames></author></authors><title>Complete Characterization of the Equivalent MIMO Channel for
  Quasi-Orthogonal Space-Time Codes</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 54(7), pp.
  3315-3327, July, 2008</journal-ref><abstract>  Recently, a quasi-orthogonal space-time block code (QSTBC) capable of
achieving a significant fraction of the outage mutual information of a
multiple-input-multiple output (MIMO) wireless communication system for the
case of four transmit and one receive antennas was proposed. We generalize
these results to $n_T=2^n$ transmit and an arbitrary number of receive antennas
$n_R$. Furthermore, we completely characterize the structure of the equivalent
channel for the general case and show that for all $n_T=2^n$ and $n_R$ the
eigenvectors of the equivalent channel are fixed and independent from the
channel realization. Furthermore, the eigenvalues of the equivalent channel are
independent identically distributed random variables each following a
noncentral chi-square distribution with $4n_R$ degrees of freedom.
  Based on these important insights into the structure of the QSTBC, we derive
an analytical lower bound for the fraction of outage probability achieved with
QSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR)
values and also for increasing number of receive antennas. We also present an
upper bound, which is tight for high SNR values and derive analytical
expressions for the case of four transmit antennas. Finally, by utilizing the
special structure of the QSTBC we propose a new transmit strategy, which
decouples the signals transmitted from different antennas in order to detect
the symbols separately with a linear ML-detector rather than joint detection,
an up to now only known advantage of orthogonal space-time block codes (OSTBC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412068</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion
  Detection System</title><categories>cs.CR cs.AI</categories><comments>13 pages, 3 figures, Swarm Intelligence and Patterns (SIP)- special
  track at WSTST 2005, Muroran, JAPAN</comments><acm-class>H.3.3; I.2.11;I.5</acm-class><abstract>  Security of computers and the networks that connect them is increasingly
becoming of great significance. Computer security is defined as the protection
of computing systems against threats to confidentiality, integrity, and
availability. There are two types of intruders: the external intruders who are
unauthorized users of the machines they attack, and internal intruders, who
have permission to access the system with some restrictions. Due to the fact
that it is more and more improbable to a system administrator to recognize and
manually intervene to stop an attack, there is an increasing recognition that
ID systems should have a lot to earn on following its basic principles on the
behavior of complex natural systems, namely in what refers to
self-organization, allowing for a real distributed and collective perception of
this phenomena. With that aim in mind, the present work presents a
self-organized ant colony based intrusion detection system (ANTIDS) to detect
intrusions in a network infrastructure. The performance is compared among
conventional soft computing paradigms like Decision Trees, Support Vector
Machines and Linear Genetic Programming to model fast, online and efficient
intrusion detection systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412069</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Campbell</keyname><forenames>Jonathan</forenames></author><author><keyname>Slater</keyname><forenames>John</forenames></author><author><keyname>Gillespie</keyname><forenames>John</forenames></author><author><keyname>Bendezu</keyname><forenames>Ivan F.</forenames></author><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>Swarming around Shellfish Larvae</title><categories>cs.AI cs.CV</categories><comments>11 pages, 4 figures,
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_53.html, submitted to IbPRIA
  2005, Portugal</comments><acm-class>I.5; I.5.3; I.4; I.2.11</acm-class><abstract>  The collection of wild larvae seed as a source of raw material is a major sub
industry of shellfish aquaculture. To predict when, where and in what
quantities wild seed will be available, it is necessary to track the appearance
and growth of planktonic larvae. One of the most difficult groups to identify,
particularly at the species level are the Bivalvia. This difficulty arises from
the fact that fundamentally all bivalve larvae have a similar shape and colour.
Identification based on gross morphological appearance is limited by the
time-consuming nature of the microscopic examination and by the limited
availability of expertise in this field. Molecular and immunological methods
are also being studied. We describe the application of computational pattern
recognition methods to the automated identification and size analysis of
scallop larvae. For identification, the shape features used are binary
invariant moments; that is, the features are invariant to shift (position
within the image), scale (induced either by growth or differential image
magnification) and rotation. Images of a sample of scallop and non-scallop
larvae covering a range of maturities have been analysed. In order to overcome
the automatic identification, as well as to allow the system to receive new
unknown samples at any moment, a self-organized and unsupervised ant-like
clustering algorithm based on Swarm Intelligence is proposed, followed by
simple k-NNR nearest neighbour classification on the final map. Results achieve
a full recognition rate of 100% under several situations (k =1 or 3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412070</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Muge</keyname><forenames>Fernando</forenames></author></authors><title>Less is More - Genetic Optimisation of Nearest Neighbour Classifiers</title><categories>cs.AI cs.CV</categories><comments>9 pages, 7 figures, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_13.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>Proc. RecPad 98 - 10th Portuguese Conference on Pattern
  Recognition, F.Muge, C.Pinto and M.Piedade Eds., ISBN 972-97711-0-3, pp.
  293-301, Lisbon, March 1998</journal-ref><abstract>  The present paper deals with optimisation of Nearest Neighbour rule
Classifiers via Genetic Algorithms. The methodology consists on implement a
Genetic Algorithm capable of search the input feature space used by the NNR
classifier. Results show that is adequate to perform feature reduction and
simultaneous improve the Recognition Rate. Some practical examples prove that
is possible to Recognise Portuguese Granites in 100%, with only 3 morphological
features (from an original set of 117 features), which is well suited for real
time applications. Moreover, the present method represents a robust strategy to
understand the proper nature of the images treated, and their discriminant
features. KEYWORDS: Feature Reduction, Genetic Algorithms, Nearest Neighbour
Rule Classifiers (k-NNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412071</id><created>2004-12-17</created><authors><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>Web Usage Mining Using Artificial Ant Colony Clustering and Genetic
  Programming</title><categories>cs.AI cs.NE</categories><comments>8 pages, 11 figures, at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_48.html</comments><acm-class>I.5; I.5.3; I.4; I.2.11</acm-class><journal-ref>CEC 03 - Congress on Evolutionary Computation, IEEE Press, ISBN
  0780378040, pp.1384-1391, Canberra, Australia, 8-12 Dec. 2003</journal-ref><abstract>  The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management, creating adaptive Web
sites, business and support services, personalization, network traffic flow
analysis and so on. The study of ant colonies behavior and their
self-organizing capabilities is of interest to knowledge retrieval/management
and decision support systems sciences, because it provides models of
distributed adaptive organization, which are useful to solve difficult
optimization, classification, and distributed control problems, among others.
In this paper, we propose an ant clustering algorithm to discover Web usage
patterns (data clusters) and a linear genetic programming approach to analyze
the visitor trends. Empirical results clearly shows that ant colony clustering
performs well when compared to a self-organizing map (for clustering Web usage
patterns) even though the performance accuracy is not that efficient when
comparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web
Usage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear
Genetic Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412072</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Abraham</keyname><forenames>Ajith</forenames></author></authors><title>Swarms on Continuous Data</title><categories>cs.AI cs.NE</categories><comments>6 pages, 3 figures, at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_45.html</comments><acm-class>I.5; I.5.3; I.4; I.2.11</acm-class><journal-ref>CEC 03 - Congress on Evolutionary Computation, IEEE Press, ISBN
  0780378040, pp.1370-1375, Canberra, Australia, 8-12 Dec. 2003</journal-ref><abstract>  While being it extremely important, many Exploratory Data Analysis (EDA)
systems have the inhability to perform classification and visualization in a
continuous basis or to self-organize new data-items into the older ones
(evenmore into new labels if necessary), which can be crucial in KDD -
Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online
forms of Web Applications are just one example). This disadvantge is also
present in more recent approaches using Self-Organizing Maps. On the present
work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems
a robust online classifier is presented, which produces class decisions on a
continuous stream data, allowing for continuous mappings. Results show that
increasingly better results are achieved, as demonstraded by other authors in
different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy,
Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous
Classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412073</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>Self-Organizing the Abstract: Canvas as a Swarm Habitat for Collective
  Memory, Perception and Cooperative Distributed Creativity</title><categories>cs.MM cs.AI</categories><comments>2 pages, 1 figure, Project at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/Artsbot.html. Publication at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_37.html</comments><acm-class>I.2.11</acm-class><journal-ref>in First Art and Science Symposium, Models to Know Reality, J.
  Rekalde, R. Ibanez and A. Simo (Eds.), pp. 59-60, Facultad de Bellas Artes
  EHU/UPV, Universidad del Pais Vasco, 11-12 Dec., Bilbao, Spain, 2003</journal-ref><abstract>  Past experiences under the designation of &quot;Swarm Paintings&quot; conducted in
2001, not only confirmed the possibility of realizing an artificial art (thus
non-human), as introduced into the process the questioning of creative
migration, specifically from the computer monitors to the canvas via a robotic
harm. In more recent self-organized based research we seek to develop and
profound the initial ideas by using a swarm of autonomous robots (ARTsBOT
project 2002-03), that &quot;live&quot; avoiding the purpose of being merely a simple
perpetrator of order streams coming from an external computer, but instead,
that actually co-evolve within the canvas space, acting (that is, laying ink)
according to simple inner threshold stimulus response functions, reacting
simultaneously to the chromatic stimulus present in the canvas environment done
by the passage of their team-mates, as well as by the distributed feedback,
affecting their future collective behaviour. In parallel, and in what respects
to certain types of collective systems, we seek to confirm, in a physically
embedded way, that the emergence of order (even as a concept) seems to be found
at a lower level of complexity, based on simple and basic interchange of
information, and on the local dynamic of parts, who, by self-organizing
mechanisms tend to form an lived whole, innovative and adapting, allowing for
emergent open-ended creative and distributed production. KEYWORDS: ArtSBots
Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm
Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation,
Art and Complexity, ArtBots: The Robot Talent Show.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412074</id><created>2004-12-17</created><authors><author><keyname>Haubert</keyname><forenames>Elizabeth</forenames></author></authors><title>Threats of Human Error in a High-Performance Storage System: Problem
  Statement and Case Study</title><categories>cs.HC cs.OS</categories><comments>13 pages, 1 figure</comments><acm-class>H.1.2</acm-class><abstract>  System administration is a difficult, often tedious, job requiring many
skilled laborers. The data that is protected by system administrators is often
valued at or above the value of the institution maintaining that data. A number
of ethnographic studies have confirmed the skill of these operators, and the
difficulty of providing adequate tools. In an effort to minimize the
maintenance costs, an increasing portion of system administration is subject to
automation - particularly simple, routine tasks such as data backup. While such
tools reduce the risk of errors from carelessness, the same tools may result in
reduced skill and system familiarity in experienced workers. Care should be
taken to ensure that operators maintain system awareness without placing the
operator in a passive, monitoring role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412075</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Merelo</keyname><forenames>Juan J.</forenames></author></authors><title>Self-Organized Stigmergic Document Maps: Environment as a Mechanism for
  Context Learning</title><categories>cs.AI cs.DC</categories><comments>10 pages, 5 figures, at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_42.html</comments><acm-class>I.5; I.5.3; I.4; I.2.11</acm-class><journal-ref>in AEB 2002, 1st Spanish Conference on Evolutionary and
  Bio-Inspired Algorithms, E. Alba, F. Herrera, J.J. Merelo et al. (Eds.), pp.
  284-293, Centro Univ. de Merida, Merida, Spain, 6-8 Feb. 2002</journal-ref><abstract>  Social insect societies and more specifically ant colonies, are distributed
systems that, in spite of the simplicity of their individuals, present a highly
structured social organization. As a result of this organization, ant colonies
can accomplish complex tasks that in some cases exceed the individual
capabilities of a single ant. The study of ant colonies behavior and of their
self-organizing capabilities is of interest to knowledge retrieval/management
and decision support systems sciences, because it provides models of
distributed adaptive organization which are useful to solve difficult
optimization, classification, and distributed control problems, among others.
In the present work we overview some models derived from the observation of
real ants, emphasizing the role played by stigmergy as distributed
communication paradigm, and we present a novel strategy to tackle unsupervised
clustering as well as data retrieval problems. The present ant clustering
system (ACLUSTER) avoids not only short-term memory based strategies, as well
as the use of several artificial ant types (using different speeds), present in
some recent approaches. Moreover and according to our knowledge, this is also
the first application of ant systems into textual document clustering.
KEYWORDS: Swarm Intelligence, Ant Systems, Unsupervised Clustering, Data
Retrieval, Data Mining, Distributed Computing, Document Maps, Textual Document
Clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412076</id><created>2004-12-17</created><authors><author><keyname>Caldas-Pinto</keyname><forenames>J. R.</forenames></author><author><keyname>Pina</keyname><forenames>Pedro</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Ramalho</keyname><forenames>Mario</forenames></author></authors><title>Clustering Techniques for Marbles Classification</title><categories>cs.AI cs.CV</categories><comments>7 pages, 17 figures, at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_41.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>RecPad 2002 -12th Portuguese Conference on Pattern Recognition,
  ISBN 972-789-067-9, Aveiro, Portugal, June 27-28, 2002</journal-ref><abstract>  Automatic marbles classification based on their visual appearance is an
important industrial issue. However, there is no definitive solution to the
problem mainly due to the presence of randomly distributed high number of
different colours and its subjective evaluation by the human expert. In this
paper we present a study of segmentation techniques, we evaluate they overall
performance using a training set and standard quality measures and finally we
apply different clustering techniques to automatically classify the marbles.
KEYWORDS: Segmentation, Clustering, Quadtrees, Learning Vector Quantization
(LVQ), Simulated Annealing (SA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412077</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>On the Implicit and on the Artificial - Morphogenesis and Emergent
  Aesthetics in Autonomous Collective Systems</title><categories>cs.AI cs.MM</categories><comments>33 pages, 7 figures, Project at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/Artsbot.html . Publ. at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_37.html</comments><acm-class>I.2; I.6</acm-class><journal-ref>Chapter 2 in ARCHITOPIA Book, Art, Architecture and Science, J.L.
  Maubant et al. (Eds.), pp. 25-57, INSTITUT D'ART CONTEMPORAIN (France), ISBN
  : 2905985631, Feb. 2002</journal-ref><abstract>  Imagine a &quot;machine&quot; where there is no pre-commitment to any particular
representational scheme: the desired behaviour is distributed and roughly
specified simultaneously among many parts, but there is minimal specification
of the mechanism required to generate that behaviour, i.e. the global behaviour
evolves from the many relations of multiple simple behaviours. A machine that
lives to and from/with Synergy. An artificial super-organism that avoids
specific constraints and emerges within multiple low-level implicit
bio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm
Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot
Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and
Complexity, ArtBots: The Robot Talent Show.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412078</id><created>2004-12-17</created><authors><author><keyname>Renault</keyname><forenames>D.</forenames></author></authors><title>The vertex-transitive TLF-planar graphs</title><categories>cs.DM</categories><comments>Article : 23 pages, 15 figures Appendix : 13 pages, 72 figures
  Submitted to Discrete Mathematics The appendix is accessible at
  http://www.labri.fr/~renault/research/research.html</comments><acm-class>G.2.1; G.2.2</acm-class><abstract>  We consider the class of the topologically locally finite (in short TLF)
planar vertex-transitive graphs, a class containing in particular all the
one-ended planar Cayley graphs and the normal transitive tilings. We
characterize these graphs with a finite local representation and a special kind
of finite state automaton named labeling scheme. As a result, we are able to
enumerate and describe all TLF-planar vertex-transitive graphs of any given
degree. Also, we are able decide to whether any TLF-planar transitive graph is
Cayley or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412079</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>The MC2 Project [Machines of Collective Conscience]: A possible walk, up
  to Life-like Complexity and Behaviour, from bottom, basic and simple
  bio-inspired heuristics - a walk, up into the morphogenesis of information</title><categories>cs.AI cs.MM</categories><comments>14 pages, Project at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/MC2.html</comments><acm-class>I.2.11</acm-class><journal-ref>at UTOPIA Biennial Art Exposition CATALOGUE, Cascais, Portugal,
  July 12-22, 2001</journal-ref><abstract>  Synergy (from the Greek word synergos), broadly defined, refers to combined
or co-operative effects produced by two or more elements (parts or
individuals). The definition is often associated with the holistic conviction
quote that &quot;the whole is greater than the sum of its parts&quot; (Aristotle, in
Metaphysics), or the whole cannot exceed the sum of the energies invested in
each of its parts (e.g. first law of thermodynamics) even if it is more
accurate to say that the functional effects produced by wholes are different
from what the parts can produce alone. Synergy is a ubiquitous phenomena in
nature and human societies alike. One well know example is provided by the
emergence of self-organization in social insects, via direct or indirect
interactions. The latter types are more subtle and defined as stigmergy to
explain task coordination and regulation in the context of nest reconstruction
in termites. An example, could be provided by two individuals, who interact
indirectly when one of them modifies the environment and the other responds to
the new environment at a later time. In other words, stigmergy could be defined
as a particular case of environmental or spatial synergy. The system is purely
holistic, and their properties are intrinsically emergent and autocatalytic. On
the present work we present a &quot;machine&quot; where there is no precommitment to any
particular representational scheme: the desired behaviour is distributed and
roughly specified simultaneously among many parts, but there is minimal
specification of the mechanism required to generate that behaviour, i.e. the
global behaviour evolves from the many relations of multiple simple behaviours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412080</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>The Biological Concept of Neoteny in Evolutionary Colour Image
  Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms</title><categories>cs.AI cs.NE</categories><comments>12 pages, 3 figures, at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_35.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>in Applications of Evolutionary Computation, (Eds.), EuroGP /
  EvoIASP 2001 - 3rd Eur. Works. on Evol. Comp. in Image Analysis and Signal
  Processing, Lake Como, Milan, Italy, Lecture Notes in Computer Science, Vol.
  2037, pp. 364-378, Springer-Verlag, Berlin-Heidelberg, April 18-20, 2001</journal-ref><abstract>  Neoteny, also spelled Paedomorphosis, can be defined in biological terms as
the retention by an organism of juvenile or even larval traits into later life.
In some species, all morphological development is retarded; the organism is
juvenilized but sexually mature. Such shifts of reproductive capability would
appear to have adaptive significance to organisms that exhibit it. In terms of
evolutionary theory, the process of paedomorphosis suggests that larval stages
and developmental phases of existing organisms may give rise, under certain
circumstances, to wholly new organisms. Although the present work does not
pretend to model or simulate the biological details of such a concept in any
way, these ideas were incorporated by a rather simple abstract computational
strategy, in order to allow (if possible) for faster convergence into simple
non-memetic Genetic Algorithms, i.e. without using local improvement procedures
(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic
Algorithm was used for colour image segmentation purposes by using K-mean
unsupervised clustering methods, namely for guiding the evolutionary algorithm
in his search for finding the optimal or sub-optimal data partition. Average
results suggest that the use of neotonic strategies by employing juvenile
genotypes into the later generations and the use of linear-dynamic mutation
rates instead of constant, can increase fitness values by 58% comparing to
classical Genetic Algorithms, independently from the starting population
characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412081</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author></authors><title>Artificial Neoteny in Evolutionary Image Segmentation</title><categories>cs.AI cs.NE</categories><comments>10 pages, 4 figures</comments><acm-class>I.2; I.5</acm-class><journal-ref>SIARP 2000 - 5th IberoAmerican Symp. on Pattern Rec., F. Muge,
  Moises P. and R. Caldas Pinto (Eds.), ISBN 972-97711-1-1, pp. 69-78, Lisbon,
  Portugal, 11-13 Sep. 2000</journal-ref><abstract>  Neoteny, also spelled Paedomorphosis, can be defined in biological terms as
the retention by an organism of juvenile or even larval traits into later life.
In some species, all morphological development is retarded; the organism is
juvenilized but sexually mature. Such shifts of reproductive capability would
appear to have adaptive significance to organisms that exhibit it. In terms of
evolutionary theory, the process of paedomorphosis suggests that larval stages
and developmental phases of existing organisms may give rise, under certain
circumstances, to wholly new organisms. Although the present work does not
pretend to model or simulate the biological details of such a concept in any
way, these ideas were incorporated by a rather simple abstract computational
strategy, in order to allow (if possible) for faster convergence into simple
non-memetic Genetic Algorithms, i.e. without using local improvement procedures
(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic
Algorithm was used for colour image segmentation purposes by using K-mean
unsupervised clustering methods, namely for guiding the evolutionary algorithm
in his search for finding the optimal or sub-optimal data partition. Average
results suggest that the use of neotonic strategies by employing juvenile
genotypes into the later generations and the use of linear-dynamic mutation
rates instead of constant, can increase fitness values by 58% comparing to
classical Genetic Algorithms, independently from the starting population
characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412083</id><created>2004-12-17</created><authors><author><keyname>Marcolino</keyname><forenames>A.</forenames></author><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Ramalho</keyname><forenames>Mario</forenames></author><author><keyname>Pinto</keyname><forenames>J. R. Caldas</forenames></author></authors><title>Line and Word Matching in Old Documents</title><categories>cs.AI cs.CV</categories><comments>12 pages, 7 figures, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_32.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>SIARP 2000 - 5th IberoAmerican Symp. on Pattern Rec., F. Muge,
  Moises P. and R. Caldas Pinto (Eds.), ISBN 972-97711-1-1, pp. 123-135,
  Lisbon, Portugal, 11-13 Sep. 2000</journal-ref><abstract>  This paper is concerned with the problem of establishing an index based on
word matching. It is assumed that the book was digitised as better as possible
and some pre-processing techniques were already applied as line orientation
correction and some noise removal. However two main factor are responsible for
being not possible to apply ordinary optical character recognition techniques
(OCR): the presence of antique fonts and the degraded state of many characters
due to unrecoverable original time degradation. In this paper we make a short
introduction to word segmentation that involves finding the lines that
characterise a word. After we discuss different approaches for word matching
and how they can be combined to obtain an ordered list for candidate words for
the matching. This discussion will be illustrated by examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412084</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Muge</keyname><forenames>Fernando</forenames></author></authors><title>Map Segmentation by Colour Cube Genetic K-Mean Clustering</title><categories>cs.AI cs.NE</categories><comments>4 pages, 1 figure, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_31.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>ECDL 2000 - 4th Eur. Conf. on Research and Advanced Technology for
  Digital Libraries, J. Borbinha and T. Baker (Eds.), ISBN 3-540-41023-6, LNCS
  series, Vol. 1923, pp. 319-323, Springer-Verlag, Heidelberg, Lisbon,
  Portugal, 18-20 Sep. 2000</journal-ref><abstract>  Segmentation of a colour image composed of different kinds of texture regions
can be a hard problem, namely to compute for an exact texture fields and a
decision of the optimum number of segmentation areas in an image when it
contains similar and/or unstationary texture fields. In this work, a method is
described for evolving adaptive procedures for these problems. In many real
world applications data clustering constitutes a fundamental issue whenever
behavioural or feature domains can be mapped into topological domains. We
formulate the segmentation problem upon such images as an optimisation problem
and adopt evolutionary strategy of Genetic Algorithms for the clustering of
small regions in colour feature space. The present approach uses k-Means
unsupervised clustering methods into Genetic Algorithms, namely for guiding
this last Evolutionary Algorithm in his search for finding the optimal or
sub-optimal data partition, task that as we know, requires a non-trivial search
because of its NP-complete nature. To solve this task, the appropriate genetic
coding is also discussed, since this is a key aspect in the implementation. Our
purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and
unsupervised texture segmentation. Some examples in Colour Maps are presented
and overall results discussed. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412085</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412085</id><created>2004-12-17</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author><author><keyname>Langfeld</keyname><forenames>Barbara</forenames></author></authors><title>A class of one-dimensional MDS convolutional codes</title><categories>cs.IT math.IT math.RA</categories><comments>20 pages</comments><abstract>  A class of one-dimensional convolutional codes will be presented. They are
all MDS codes, i. e., have the largest distance among all one-dimensional codes
of the same length n and overall constraint length delta. Furthermore, their
extended row distances are computed, and they increase with slope n-delta. In
certain cases of the algebraic parameters, we will also derive parity check
matrices of Vandermonde type for these codes. Finally, cyclicity in the
convolutional sense will be discussed for our class of codes. It will turn out
that they are cyclic if and only if the field element used in the generator
matrix has order n. This can be regarded as a generalization of the block code
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412086</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Almeida</keyname><forenames>Filipe</forenames></author></authors><title>Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour
  Effect Study on Pattern Recognition</title><categories>cs.AI cs.CV</categories><comments>11 pages, 3 figures, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_29.html</comments><acm-class>I.2; I.5; I.5.3; I.4; I.2.11</acm-class><journal-ref>Proc. of ANTS 2000 - 2nd Int. Works. on Ant Algorithms (From Ant
  Colonies to Artificial Ants), Marco Dorigo, Martin Middendorf, Thomas Stuzle
  (Eds.), pp. 113, Brussels, Belgium, 7-9 Sep. 2000</journal-ref><abstract>  Some recent studies have pointed that, the self-organization of neurons into
brain-like structures, and the self-organization of ants into a swarm are
similar in many respects. If possible to implement, these features could lead
to important developments in pattern recognition systems, where perceptive
capabilities can emerge and evolve from the interaction of many simple local
rules. The principle of the method is inspired by the work of Chialvo and
Millonas who developed the first numerical simulation in which swarm cognitive
map formation could be explained. From this point, an extended model is
presented in order to deal with digital image habitats, in which artificial
ants could be able to react to the environment and perceive it. Evolution of
pheromone fields point that artificial ant colonies could react and adapt
appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence,
Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and
Perception, Image Segmentation, Gestalt Perception Theory, Distributed
Computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412087</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Muge</keyname><forenames>Fernando</forenames></author></authors><title>Image Colour Segmentation by Genetic Algorithms</title><categories>cs.AI cs.CV</categories><comments>5 pages, 1 figure, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_26.html</comments><acm-class>I.2; I.5</acm-class><journal-ref>RecPad 2000 - 11th Portuguese Conf. on Pattern Recognition, in
  Aurelio C. Campilho and A.M. Mendonca (Eds.), ISBN 972-96883-2-5, pp.
  125-129, Porto, Portugal, May 11-12, 2000</journal-ref><abstract>  Segmentation of a colour image composed of different kinds of texture regions
can be a hard problem, namely to compute for an exact texture fields and a
decision of the optimum number of segmentation areas in an image when it
contains similar and/or unstationary texture fields. In this work, a method is
described for evolving adaptive procedures for these problems. In many real
world applications data clustering constitutes a fundamental issue whenever
behavioural or feature domains can be mapped into topological domains. We
formulate the segmentation problem upon such images as an optimisation problem
and adopt evolutionary strategy of Genetic Algorithms for the clustering of
small regions in colour feature space. The present approach uses k-Means
unsupervised clustering methods into Genetic Algorithms, namely for guiding
this last Evolutionary Algorithm in his search for finding the optimal or
sub-optimal data partition, task that as we know, requires a non-trivial search
because of its intrinsic NP-complete nature. To solve this task, the
appropriate genetic coding is also discussed, since this is a key aspect in the
implementation. Our purpose is to demonstrate the efficiency of Genetic
Algorithms to automatic and unsupervised texture segmentation. Some examples in
Colour Maps, Ornamental Stones and in Human Skin Mark segmentation are
presented and overall results discussed. KEYWORDS: Genetic Algorithms, Colour
Image Segmentation, Classification, Clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412088</id><created>2004-12-17</created><authors><author><keyname>Ramos</keyname><forenames>Vitorino</forenames></author><author><keyname>Muge</keyname><forenames>Fernando</forenames></author></authors><title>On Image Filtering, Noise and Morphological Size Intensity Diagrams</title><categories>cs.CV cs.AI</categories><comments>9 pages, 4 figures, Author at
  http://alfa.ist.utl.pt/~cvrm/staff/vramos/ref_25.html</comments><acm-class>I.5; I.2</acm-class><journal-ref>RecPad 2000 - 11th Portuguese Conf. on Pattern Recognition, in
  Aurelio C. Campilho and A.M. Mendonca (Eds.), ISBN 972-96883-2-5, pp.
  483-491, Porto, Portugal, May 11-12, 2000</journal-ref><abstract>  In the absence of a pure noise-free image it is hard to define what noise is,
in any original noisy image, and as a consequence also where it is, and in what
amount. In fact, the definition of noise depends largely on our own aim in the
whole image analysis process, and (perhaps more important) in our
self-perception of noise. For instance, when we perceive noise as disconnected
and small it is normal to use MM-ASF filters to treat it. There is two
evidences of this. First, in many instances there is no ideal and pure
noise-free image to compare our filtering process (nothing but our
self-perception of its pure image); second, and related with this first point,
MM transformations that we chose are only based on our self - and perhaps -
fuzzy notion. The present proposal combines the results of two MM filtering
transformations (FT1, FT2) and makes use of some measures and quantitative
relations on their Size/Intensity Diagrams to find the most appropriate noise
removal process. Results can also be used for finding the most appropriate stop
criteria, and the right sequence of MM operators combination on Alternating
Sequential Filters (ASF), if these measures are applied, for instance, on a
Genetic Algorithm's target function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412089</id><created>2004-12-17</created><authors><author><keyname>Yanenko</keyname><forenames>Evgeny</forenames></author></authors><title>Evolving Categories: Consistent Framework for Representation of Data and
  Algorithms</title><categories>cs.DS</categories><comments>10 pages, 20 pictures</comments><acm-class>E.1; F.1.1; F.4.1; I.1.1</acm-class><abstract>  A concept of &quot;evolving categories&quot; is suggested to build a simple, scalable,
mathematically consistent framework for representing in uniform way both data
and algorithms. A state machine for executing algorithms becomes clear, rich
and powerful semantics, based on category theory, and still allows easy
implementation. Moreover, it gives an original insight into the nature and
semantics of algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412090</id><created>2004-12-17</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Real Time Models of the Asynchronous Circuits: The Delay Theory</title><categories>cs.GL</categories><comments>82 pages, 30 figures</comments><journal-ref>in New Developments in Computer Science Research, Editor Susan
  Shannon, Nova Science Publishers, Inc., New York, 2005</journal-ref><abstract>  The chapter from the book introduces the delay theory, whose purpose is the
modeling of the asynchronous circuits from digital electrical engineering with
ordinary and differential pseudo-boolean equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412091</id><created>2004-12-19</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Dezert</keyname><forenames>Jean</forenames></author></authors><title>The Combination of Paradoxical, Uncertain, and Imprecise Sources of
  Information based on DSmT and Neutro-Fuzzy Inference</title><categories>cs.AI</categories><comments>20 pages</comments><acm-class>I.2.4</acm-class><journal-ref>A version of this paper published in Proceedings of 10th
  International Conference on Fuzzy Theory and Technology (FT&amp;T 2005), Salt
  Lake City, Utah, USA, July 21-26, 2005.</journal-ref><abstract>  The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been, and
still remains today, of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this chapter, we
present a survey of our recent theory of plausible and paradoxical reasoning,
known as Dezert-Smarandache Theory (DSmT) in the literature, developed for
dealing with imprecise, uncertain and paradoxical sources of information. We
focus our presentation here rather on the foundations of DSmT, and on the two
important new rules of combination, than on browsing specific applications of
DSmT available in literature. Several simple examples are given throughout the
presentation to show the efficiency and the generality of this new approach.
The last part of this chapter concerns the presentation of the neutrosophic
logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and
neutrosophic logic are useful tools in decision making after fusioning the
information using the DSm hybrid rule of combination of masses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412092</id><created>2004-12-20</created><authors><author><keyname>Earl</keyname><forenames>A.</forenames></author><author><keyname>Clark</keyname><forenames>P.</forenames></author></authors><title>Mass Storage Management and the Grid</title><categories>cs.DC cs.SE</categories><comments>4 pages, 3 figures, Presented at Computing for High Energy and
  Nuclear Physics 2004 (CHEP '04), Interlaken, Switzerland, September 2004</comments><abstract>  The University of Edinburgh has a significant interest in mass storage
systems as it is one of the core groups tasked with the roll out of storage
software for the UK's particle physics grid, GridPP. We present the results of
a development project to provide software interfaces between the SDSC Storage
Resource Broker, the EU DataGrid and the Storage Resource Manager. This project
was undertaken in association with the eDikt group at the National eScience
Centre, the Universities of Bristol and Glasgow, Rutherford Appleton Laboratory
and the San Diego Supercomputing Center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412093</id><created>2004-12-20</created><authors><author><keyname>Earl</keyname><forenames>A.</forenames></author><author><keyname>Clark</keyname><forenames>P.</forenames></author><author><keyname>Thorn</keyname><forenames>S.</forenames></author></authors><title>ScotGrid: A Prototype Tier 2 Centre</title><categories>cs.AR cs.DC</categories><comments>4 pages, 4 diagrams. Presented at Computing for High Energy and
  Nuclear Physics 2004 (CHEP '04). Interlaken, Switzerland, September 2004</comments><abstract>  ScotGrid is a prototype regional computing centre formed as a collaboration
between the universities of Durham, Edinburgh and Glasgow as part of the UK's
national particle physics grid, GridPP. We outline the resources available at
the three core sites and our optimisation efforts for our user communities. We
discuss the work which has been conducted in extending the centre to embrace
new projects both from particle physics and new user communities and explain
our methodology for doing this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412094</id><created>2004-12-20</created><authors><author><keyname>Baptiste</keyname><forenames>Philippe</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Sourd</keyname><forenames>Francis</forenames></author></authors><title>Preemptive Multi-Machine Scheduling of Equal-Length Jobs to Minimize the
  Average Flow Time</title><categories>cs.DS</categories><report-no>This paper is now part of the report cs.DS/0605078.</report-no><acm-class>F.2.2</acm-class><abstract>  We study the problem of preemptive scheduling of n equal-length jobs with
given release times on m identical parallel machines. The objective is to
minimize the average flow time. Recently, Brucker and Kravchenko proved that
the optimal schedule can be computed in polynomial time by solving a linear
program with O(n^3) variables and constraints, followed by some substantial
post-processing (where n is the number of jobs.) In this note we describe a
simple linear program with only O(mn) variables and constraints. Our linear
program produces directly the optimal schedule and does not require any
post-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412095</id><created>2004-12-21</created><authors><author><keyname>Damian</keyname><forenames>Mirela</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Partitioning Regular Polygons into Circular Pieces II:Nonconvex
  Partitions</title><categories>cs.CG cs.DM</categories><comments>13 pages, 11 figures</comments><acm-class>F.2.2</acm-class><abstract>  We explore optimal circular nonconvex partitions of regular k-gons. The
circularity of a polygon is measured by its aspect ratio: the ratio of the
radii of the smallest circumscribing circle to the largest inscribed disk. An
optimal circular partition minimizes the maximum ratio over all pieces in the
partition. We show that the equilateral triangle has an optimal 4-piece
nonconvex partition, the square an optimal 13-piece nonconvex partition, and
the pentagon has an optimal nonconvex partition with more than 20 thousand
pieces. For hexagons and beyond, we provide a general algorithm that approaches
optimality, but does not achieve it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412096</identifier>
 <datestamp>2008-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412096</id><created>2004-12-21</created><authors><author><keyname>Soloveichik</keyname><forenames>David</forenames></author><author><keyname>Winfree</keyname><forenames>Erik</forenames></author></authors><title>Complexity of Self-Assembled Shapes</title><categories>cs.CC</categories><comments>Extended abstract appears in DNA Computing 10; this is the full
  version and will be submitted elsewhere. 25 pages</comments><acm-class>F.1.1; F.1.3</acm-class><journal-ref>SIAM Journal on Computing 36 (6) 1544-1569, 2007</journal-ref><doi>10.1137/S0097539704446712</doi><abstract>  The connection between self-assembly and computation suggests that a shape
can be considered the output of a self-assembly ``program,'' a set of tiles
that fit together to create a shape. It seems plausible that the size of the
smallest self-assembly program that builds a shape and the shape's
descriptional (Kolmogorov) complexity should be related. We show that when
using a notion of a shape that is independent of scale, this is indeed so: in
the Tile Assembly Model, the minimal number of distinct tile types necessary to
self-assemble a shape, at some scale, can be bounded both above and below in
terms of the shape's Kolmogorov complexity. As part of the proof of the main
result, we sketch a general method for converting a program outputting a shape
as a list of locations into a set of tile types that self-assembles into a
scaled up version of that shape. Our result implies, somewhat
counter-intuitively, that self-assembly of a scaled-up version of a shape often
requires fewer tile types. Furthermore, the independence of scale in
self-assembly theory appears to play the same crucial role as the independence
of running time in the theory of computability. This leads to an elegant
formulation of languages of shapes generated by self-assembly. Considering
functions from integers to shapes, we show that the running-time complexity,
with respect to Turing machines, is polynomially equivalent to the scale
complexity of the same function implemented via self-assembly by a finite set
of tile types. Our results also hold for shapes defined by Wang tiling -- where
there is no sense of a self-assembly process -- except that here time
complexity must be measured with respect to non-deterministic Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412097</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412097</id><created>2004-12-21</created><authors><author><keyname>Soloveichik</keyname><forenames>David</forenames></author><author><keyname>Winfree</keyname><forenames>Erik</forenames></author></authors><title>The Computational Power of Benenson Automata</title><categories>cs.CC</categories><comments>18 pages</comments><acm-class>F.1.1; F.1.3</acm-class><journal-ref>Theoretical Computer Science 244(2-3): 279-297, 2005</journal-ref><abstract>  The development of autonomous molecular computers capable of making
independent decisions in vivo regarding local drug administration may
revolutionize medical science. Recently Benenson at el (2004) have envisioned
one form such a ``smart drug'' may take by implementing an in vitro scheme, in
which a long DNA state molecule is cut repeatedly by a restriction enzyme in a
manner dependent upon the presence of particular short DNA ``rule molecules.''
To analyze the potential of their scheme in terms of the kinds of computations
it can perform, we study an abstraction assuming that a certain class of
restriction enzymes is available and reactions occur without error. We also
discuss how our molecular algorithms could perform with known restriction
enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these
``Benenson automata'' are capable of computing arbitrary Boolean functions.
Further, we show that they are able to compute efficiently exactly those
functions computable by log-depth circuits. Computationally, we formalize a new
variant of limited width branching programs with a molecular implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412098</identifier>
 <datestamp>2007-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412098</id><created>2004-12-21</created><updated>2007-05-30</updated><authors><author><keyname>Cilibrasi</keyname><forenames>Rudi</forenames><affiliation>CWI</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI, University of Amsterdam</affiliation></author></authors><title>The Google Similarity Distance</title><categories>cs.CL cs.AI cs.DB cs.IR cs.LG</categories><comments>15 pages, 10 figures; changed some text/figures/notation/part of
  theorem. Incorporated referees comments. This is the final published version
  up to some minor changes in the galley proofs</comments><acm-class>I.2.4; I.2.7</acm-class><journal-ref>R.L. Cilibrasi, P.M.B. Vitanyi, The Google Similarity Distance,
  IEEE Trans. Knowledge and Data Engineering, 19:3(2007), 370-383</journal-ref><abstract>  Words and phrases acquire meaning from the way they are used in society, from
their relative semantics to other words and phrases. For computers the
equivalent of `society' is `database,' and the equivalent of `use' is `way to
search the database.' We present a new theory of similarity between words and
phrases based on information distance and Kolmogorov complexity. To fix
thoughts we use the world-wide-web as database, and Google as search engine.
The method is also applicable to other search engines and databases. This
theory is then applied to construct a method to automatically extract
similarity, the Google similarity distance, of words and phrases from the
world-wide-web using Google page counts. The world-wide-web is the largest
database on earth, and the context information entered by millions of
independent users averages out to provide automatic semantics of useful
quality. We give applications in hierarchical clustering, classification, and
language translation. We give examples to distinguish between colors and
numbers, cluster names of paintings by 17th century Dutch masters and names of
books by English novelists, the ability to understand emergencies, and primes,
and we demonstrate the ability to do a simple automatic English-Spanish
translation. Finally, we use the WordNet database as an objective baseline
against which to judge the performance of our method. We conduct a massive
randomized trial in binary classification using support vector machines to
learn categories based on our Google distance, resulting in an a mean agreement
of 87% with the expert crafted WordNet categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412099</identifier>
 <datestamp>2008-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412099</id><created>2004-12-22</created><updated>2008-12-27</updated><authors><author><keyname>Mitra</keyname><forenames>Arindam</forenames></author></authors><title>An unbreakable cryptosystem</title><categories>cs.CR</categories><comments>Acknowledgement is due</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkably long-standing problem of cryptography is to generate
completely secure key. It is widely believed that the task cannot be achieved
within classical cryptography. However, there is no proof in support of this
belief. We present an incredibly simple classical cryptosystem which can
generate completely secure key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412100</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412100</id><created>2004-12-22</created><authors><author><keyname>Deussen</keyname><forenames>Peter H.</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Formal Test Purposes and The Validity of Test Cases</title><categories>cs.DS</categories><comments>This paper appeared in the proceedings of the 22nd IFIP WG 6.1
  International Conference on Formal Techniques for Networked and Distributed
  Systems (FORTE 2002), number 2529 Lecture Notes in Computer Science</comments><abstract>  We give a formalization of the notion of test purpose based on (suitably
restricted) Message Sequence Charts. We define the validity of test cases with
respect to such a formal test purpose and provide a simple decision procedure
for validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412101</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412101</id><created>2004-12-22</created><authors><author><keyname>Baader</keyname><forenames>Franz</forenames></author><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>The Inverse Method Implements the Automata Approach for Modal
  Satisfiability</title><categories>cs.LO</categories><comments>A short version of this report has appeared at the First
  International Joint Conference on Automated Reasoning, IJCAR 2001</comments><abstract>  Tableaux-based decision procedures for satisfiability of modal and
description logics behave quite well in practice, but it is sometimes hard to
obtain exact worst-case complexity results using these approaches, especially
for EXPTIME-complete logics. In contrast, automata-based approaches often yield
algorithms for which optimal worst-case complexity can easily be proved.
However, the algorithms obtained this way are usually not only worst-case, but
also best-case exponential: they first construct an automaton that is always
exponential in the size of the input, and then apply the (polynomial) emptiness
test to this large automaton. To overcome this problem, one must try to
construct the automaton &quot;on-the-fly&quot; while performing the emptiness test.
  In this paper we will show that Voronkov's inverse method for the modal logic
K can be seen as an on-the-fly realization of the emptiness test done by the
automata approach for K. The benefits of this result are two-fold. First, it
shows that Voronkov's implementation of the inverse method, which behaves quite
well in practice, is an optimized on-the-fly implementation of the
automata-based satisfiability procedure for K. Second, it can be used to give a
simpler proof of the fact that Voronkov's optimizations do not destroy
completeness of the procedure. We will also show that the inverse method can
easily be extended to handle global axioms, and that the correspondence to the
automata approach still holds in this setting. In particular, the inverse
method yields an EXPTIME-algorithm for satisfiability in K w.r.t. global
axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412102</identifier>
 <datestamp>2013-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412102</id><created>2004-12-22</created><authors><author><keyname>Gutoski</keyname><forenames>Gus</forenames></author><author><keyname>Watrous</keyname><forenames>John</forenames></author></authors><title>Quantum Interactive Proofs with Competing Provers</title><categories>cs.CC quant-ph</categories><comments>13 pages, to appear in STACS 2005</comments><journal-ref>Proceedings of STACS 2005, LNCS vol. 3404, pages 605-616</journal-ref><doi>10.1007/978-3-540-31856-9_50</doi><abstract>  This paper studies quantum refereed games, which are quantum interactive
proof systems with two competing provers: one that tries to convince the
verifier to accept and the other that tries to convince the verifier to reject.
We prove that every language having an ordinary quantum interactive proof
system also has a quantum refereed game in which the verifier exchanges just
one round of messages with each prover. A key part of our proof is the fact
that there exists a single quantum measurement that reliably distinguishes
between mixed states chosen arbitrarily from disjoint convex sets having large
minimal trace distance from one another. We also show how to reduce the
probability of error for some classes of quantum refereed games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412103</identifier>
 <datestamp>2007-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412103</id><created>2004-12-23</created><updated>2007-12-19</updated><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author></authors><title>Chosen-Plaintext Cryptanalysis of a Clipped-Neural-Network-Based Chaotic
  Cipher</title><categories>cs.CR cs.NE nlin.CD</categories><comments>LNCS style, 7 pages, 1 figure (6 sub-figures)</comments><journal-ref>Lecture Notes in Computer Science, vol. 3497, pp. 630-636, 2005</journal-ref><doi>10.1007/11427445_103</doi><abstract>  In ISNN'04, a novel symmetric cipher was proposed, by combining a chaotic
signal and a clipped neural network (CNN) for encryption. The present paper
analyzes the security of this chaotic cipher against chosen-plaintext attacks,
and points out that this cipher can be broken by a chosen-plaintext attack.
Experimental analyses are given to support the feasibility of the proposed
attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412104</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412104</id><created>2004-12-23</created><authors><author><keyname>Somefun</keyname><forenames>Koye</forenames><affiliation>Center for Mathematics and Computer Science</affiliation></author><author><keyname>Klos</keyname><forenames>Tomas</forenames><affiliation>Center for Mathematics and Computer Science</affiliation></author><author><keyname>La Poutr&#xe9;</keyname><forenames>Han</forenames><affiliation>Center for Mathematics and Computer Science</affiliation><affiliation>Eindhoven University of Technology, Eindhoven, The Netherlands</affiliation></author></authors><title>Negotiating over Bundles and Prices Using Aggregate Knowledge</title><categories>cs.MA cs.GT</categories><comments>15 pages, 7 eps figures, Springer llncs documentclass. Extended
  version of the paper published in &quot;E-Commerce and Web Technologies,&quot; Kurt
  Bauknecht, Martin Bichler and Birgit Pr\&quot;{o}ll (eds.). Springer Lecture Notes
  in Computer Science, Volume 3182, Berlin: Springer, p. 218--227</comments><report-no>SEN-E0405</report-no><abstract>  Combining two or more items and selling them as one good, a practice called
bundling, can be a very effective strategy for reducing the costs of producing,
marketing, and selling goods. In this paper, we consider a form of multi-issue
negotiation where a shop negotiates both the contents and the price of bundles
of goods with his customers. We present some key insights about, as well as a
technique for, locating mutually beneficial alternatives to the bundle
currently under negotiation. The essence of our approach lies in combining
historical sales data, condensed into aggregate knowledge, with current data
about the ongoing negotiation process, to exploit these insights. In
particular, when negotiating a given bundle of goods with a customer, the shop
analyzes the sequence of the customer's offers to determine the progress in the
negotiation process. In addition, it uses aggregate knowledge concerning
customers' valuations of goods in general. We show how the shop can use these
two sources of data to locate promising alternatives to the current bundle.
When the current negotiation's progress slows down, the shop may suggest the
most promising of those alternatives and, depending on the customer's response,
continue negotiating about the alternative bundle, or propose another
alternative. Extensive computer simulation experiments show that our approach
increases the speed with which deals are reached, as well as the number and
quality of the deals reached, as compared to a benchmark. In addition, we show
that the performance of our system is robust to a variety of changes in the
negotiation strategies employed by the customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412105</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412105</id><created>2004-12-23</created><authors><author><keyname>Costantini</keyname><forenames>Stefania</forenames></author></authors><title>On the existence of stable models of non-stratified logic programs</title><categories>cs.AI cs.LO</categories><abstract>  This paper introduces a fundamental result, which is relevant for Answer Set
programming, and planning. For the first time since the definition of the
stable model semantics, the class of logic programs for which a stable model
exists is given a syntactic characterization. This condition may have a
practical importance both for defining new algorithms for checking consistency
and computing answer sets, and for improving the existing systems. The approach
of this paper is to introduce a new canonical form (to which any logic program
can be reduced to), to focus the attention on cyclic dependencies. The
technical result is then given in terms of programs in canonical form
(canonical programs), without loss of generality. The result is based on
identifying the cycles contained in the program, showing that stable models of
the overall program are composed of stable models of suitable sub-programs,
corresponding to the cycles, and on defining the Cycle Graph. Each vertex of
this graph corresponds to one cycle, and each edge corresponds to onehandle,
which is a literal containing an atom that, occurring in both cycles, actually
determines a connection between them. In fact, the truth value of the handle in
the cycle where it appears as the head of a rule, influences the truth value of
the atoms of the cycle(s) where it occurs in the body. We can therefore
introduce the concept of a handle path, connecting different cycles. If for
every odd cycle we can find a handle path with certain properties, then the
existence of stable model is guaranteed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412106</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412106</id><created>2004-12-23</created><authors><author><keyname>Somefun</keyname><forenames>Koye</forenames><affiliation>Center for Mathematics and Computer Science</affiliation></author><author><keyname>Klos</keyname><forenames>Tomas</forenames><affiliation>Center for Mathematics and Computer Science</affiliation></author><author><keyname>La Poutr&#xe9;</keyname><forenames>Han</forenames><affiliation>Center for Mathematics and Computer Science</affiliation><affiliation>Eindhoven University of Technology, Eindhoven, The Netherlands</affiliation></author></authors><title>Online Learning of Aggregate Knowledge about Non-linear Preferences
  Applied to Negotiating Prices and Bundles</title><categories>cs.MA cs.GT cs.LG</categories><comments>10 pages, 5 eps figures, ACM Proceedings documentclass, Published in
  &quot;Proc. 6th Int'l Conf. on Electronic Commerce ICEC04, Delft, The
  Netherlands,&quot; M. Janssen, H. Sol, R. Wagenaar (eds.). ACM Press</comments><report-no>SEN-E0415</report-no><abstract>  In this paper, we consider a form of multi-issue negotiation where a shop
negotiates both the contents and the price of bundles of goods with his
customers. We present some key insights about, as well as a procedure for,
locating mutually beneficial alternatives to the bundle currently under
negotiation. The essence of our approach lies in combining aggregate
(anonymous) knowledge of customer preferences with current data about the
ongoing negotiation process. The developed procedure either works with already
obtained aggregate knowledge or, in the absence of such knowledge, learns the
relevant information online. We conduct computer experiments with simulated
customers that have_nonlinear_ preferences. We show how, for various types of
customers, with distinct negotiation heuristics, our procedure (with and
without the necessary aggregate knowledge) increases the speed with which deals
are reached, as well as the number and the Pareto efficiency of the deals
reached compared to a benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412107</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412107</id><created>2004-12-23</created><updated>2005-01-10</updated><authors><author><keyname>Garcia-Cortes</keyname><forenames>L. A.</forenames></author><author><keyname>Cabrillo</keyname><forenames>C.</forenames></author></authors><title>A Monte Carlo algorithm for efficient large matrix inversion</title><categories>cs.DS cs.NA hep-lat</categories><comments>13 pages, no figure. Title corrected</comments><abstract>  This paper introduces a new Monte Carlo algorithm to invert large matrices.
It is based on simultaneous coupled draws from two random vectors whose
covariance is the required inverse. It can be considered a generalization of a
previously reported algorithm for hermitian matrices inversion based in only
one draw. The use of two draws allows the inversion on non-hermitian matrices.
Both the conditions for convergence and the rate of convergence are similar to
the Gauss-Seidel algorithm. Results on two examples are presented, a real
non-symmetric matrix related to quantitative genetics and a complex
non-hermitian matrix relevant for physicists. Compared with other Monte Carlo
algorithms it reveals a large reduction of the processing time showing eight
times faster processing in the examples studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412108</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412108</id><created>2004-12-23</created><authors><author><keyname>Guo</keyname><forenames>Dongning</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Mutual Information and Minimum Mean-square Error in Gaussian Channels</title><categories>cs.IT math.IT</categories><abstract>  This paper deals with arbitrarily distributed finite-power input signals
observed through an additive Gaussian noise channel. It shows a new formula
that connects the input-output mutual information and the minimum mean-square
error (MMSE) achievable by optimal estimation of the input given the output.
That is, the derivative of the mutual information (nats) with respect to the
signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input
statistics. This relationship holds for both scalar and vector signals, as well
as for discrete-time and continuous-time noncausal MMSE estimation. This
fundamental information-theoretic result has an unexpected consequence in
continuous-time nonlinear estimation: For any input signal with finite power,
the causal filtering MMSE achieved at SNR is equal to the average value of the
noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is
chosen uniformly distributed between 0 and SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412109</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412109</id><created>2004-12-24</created><authors><author><keyname>Litinskii</keyname><forenames>L. B.</forenames></author><author><keyname>Magomedov</keyname><forenames>B. M.</forenames></author></authors><title>Global minimization of a quadratic functional: neural network approach</title><categories>cs.NE cs.DM</categories><comments>4 pages, Lecture on 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</comments><report-no>IONT-04-16</report-no><abstract>  The problem of finding out the global minimum of a multiextremal functional
is discussed. One frequently faces with such a functional in various
applications. We propose a procedure, which depends on the dimensionality of
the problem polynomially. In our approach we use the eigenvalues and
eigenvectors of the connection matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412110</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412110</id><created>2004-12-24</created><authors><author><keyname>Alieva</keyname><forenames>D. I.</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>B. V.</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>V. M.</forenames></author><author><keyname>Fonarev</keyname><forenames>A. B.</forenames></author></authors><title>Q-valued neural network as a system of fast identification and pattern
  recognition</title><categories>cs.NE cs.CV</categories><comments>4 pages, Presentation on the 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</comments><report-no>IONT-04-17</report-no><abstract>  An effective neural network algorithm of the perceptron type is proposed. The
algorithm allows us to identify strongly distorted input vector reliably. It is
shown that its reliability and processing speed are orders of magnitude higher
than that of full connected neural networks. The processing speed of our
algorithm exceeds the one of the stack fast-access retrieval algorithm that is
modified for working when there are noises in the input channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412111</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412111</id><created>2004-12-24</created><updated>2005-04-25</updated><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>On the asymptotic accuracy of the union bound</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures. Presented at the 42nd Annual Allerton Conference
  on Communication, Control and Computing (Sept. 29 - Oct. 1, 2004). The paper
  will appear in the conference proceedings. Version 2 differs slightly from
  the published version and is the latest version of this paper</comments><abstract>  A new lower bound on the error probability of maximum likelihood decoding of
a binary code on a binary symmetric channel was proved in Barg and McGregor
(2004, cs.IT/0407011). It was observed in that paper that this bound leads to a
new region of code rates in which the random coding exponent is asymptotically
tight, giving a new region in which the reliability of the BSC is known
exactly. The present paper explains the relation of these results to the union
bound on the error probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412112</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412112</id><created>2004-12-27</created><authors><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Source Coding With Encoder Side Information</title><categories>cs.IT math.IT</categories><comments>48 pages, 9 figures</comments><acm-class>E.4</acm-class><abstract>  We introduce the idea of distortion side information, which does not directly
depend on the source but instead affects the distortion measure. We show that
such distortion side information is not only useful at the encoder, but that
under certain conditions, knowing it at only the encoder is as good as knowing
it at both encoder and decoder, and knowing it at only the decoder is useless.
Thus distortion side information is a natural complement to the signal side
information studied by Wyner and Ziv, which depends on the source but does not
involve the distortion measure. Furthermore, when both types of side
information are present, we characterize the penalty for deviating from the
configuration of encoder-only distortion side information and decoder-only
signal side information, which in many cases is as good as full side
information knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412113</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412113</id><created>2004-12-28</created><authors><author><keyname>Laneman</keyname><forenames>J. Nicholas</forenames></author><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author><author><keyname>Apostolopoulos</keyname><forenames>John G.</forenames></author></authors><title>Source-Channel Diversity for Parallel Channels</title><categories>cs.IT math.IT</categories><comments>48 pages, 14 figures</comments><acm-class>c.2.1</acm-class><abstract>  We consider transmitting a source across a pair of independent, non-ergodic
channels with random states (e.g., slow fading channels) so as to minimize the
average distortion. The general problem is unsolved. Hence, we focus on
comparing two commonly used source and channel encoding systems which
correspond to exploiting diversity either at the physical layer through
parallel channel coding or at the application layer through multiple
description source coding.
  For on-off channel models, source coding diversity offers better performance.
For channels with a continuous range of reception quality, we show the reverse
is true. Specifically, we introduce a new figure of merit called the distortion
exponent which measures how fast the average distortion decays with SNR. For
continuous-state models such as additive white Gaussian noise channels with
multiplicative Rayleigh fading, optimal channel coding diversity at the
physical layer is more efficient than source coding diversity at the
application layer in that the former achieves a better distortion exponent.
  Finally, we consider a third decoding architecture: multiple description
encoding with a joint source-channel decoding. We show that this architecture
achieves the same distortion exponent as systems with optimal channel coding
diversity for continuous-state channels, and maintains the the advantages of
multiple description systems for on-off channels. Thus, the multiple
description system with joint decoding achieves the best performance, from
among the three architectures considered, on both continuous-state and on-off
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412114</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412114</id><created>2004-12-29</created><authors><author><keyname>Rajman</keyname><forenames>Martin</forenames></author><author><keyname>Vesely</keyname><forenames>Martin</forenames></author><author><keyname>Andrews</keyname><forenames>Pierre</forenames></author></authors><title>State of the Art, Evaluation and Recommendations regarding &quot;Document
  Processing and Visualization Techniques&quot;</title><categories>cs.CL</categories><comments>54 pages, Report of Working Group 1 for the European Network of
  Excellence (NoE) in Text Mining and its Applications in Statistics (NEMIS)</comments><acm-class>I.2.7;I.7</acm-class><abstract>  Several Networks of Excellence have been set up in the framework of the
European FP5 research program. Among these Networks of Excellence, the NEMIS
project focuses on the field of Text Mining.
  Within this field, document processing and visualization was identified as
one of the key topics and the WG1 working group was created in the NEMIS
project, to carry out a detailed survey of techniques associated with the text
mining process and to identify the relevant research topics in related research
areas.
  In this document we present the results of this comprehensive survey. The
report includes a description of the current state-of-the-art and practice, a
roadmap for follow-up research in the identified areas, and recommendations for
anticipated technological development in the domain of text mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412115</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412115</id><created>2004-12-29</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author></authors><title>Reductions in Distributed Computing Part I: Consensus and Atomic
  Commitment Tasks</title><categories>cs.DC</categories><comments>27 pages, 3 figures</comments><acm-class>C.4; C.2.4; F.1.3</acm-class><abstract>  We introduce several notions of reduction in distributed computing, and
investigate reduction properties of two fundamental agreement tasks, namely
Consensus and Atomic Commitment.
  We first propose the notion of reduction &quot;a la Karp'', an analog for
distributed computing of the classical Karp reduction. We then define a weaker
reduction which is the analog of Cook reduction. These two reductions are
called K-reduction and C-reduction, respectively.
  We also introduce the notion of C*-reduction which has no counterpart in
classical (namely, non distributed) systems, and which naturally arises when
dealing with symmetric tasks.
  We establish various reducibility and irreducibility theorems with respect to
these three reductions. Our main result is an incomparability statement for
Consensus and Atomic Commitment tasks: we show that they are incomparable with
respect to the C-reduction, except when the resiliency degree is 1, in which
case Atomic Commitment is strictly harder than Consensus. A side consequence of
these results is that our notion of C-reduction is strictly weaker than the one
of K-reduction, even for unsolvable tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412116</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412116</id><created>2004-12-29</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author></authors><title>Reductions in Distributed Computing Part II: k-Threshold Agreement Tasks</title><categories>cs.DC</categories><comments>27 pages, 4 figures</comments><acm-class>C.4; C.2.4; F.1.3</acm-class><abstract>  We extend the results of Part I by considering a new class of agreement
tasks, the so-called k-Threshold Agreement tasks (previously introduced by
Charron-Bost and Le Fessant). These tasks naturally interpolate between Atomic
Commitment and Consensus. Moreover, they constitute a valuable tool to derive
irreducibility results between Consensus tasks only. In particular, they allow
us to show that (A) for a fixed set of processes, the higher the resiliency
degree is, the harder the Consensus task is, and (B) for a fixed resiliency
degree, the smaller the set of processes is, the harder the Consensus task is.
  The proofs of these results lead us to consider new oracle-based reductions,
involving a weaker variant of the C-reduction introduced in Part I. We also
discuss the relationship between our results and previous ones relating
f-resiliency and wait-freedom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412117</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412117</id><created>2004-12-29</created><authors><author><keyname>Andrews</keyname><forenames>Pierre</forenames></author><author><keyname>Rajman</keyname><forenames>Martin</forenames></author></authors><title>Thematic Annotation: extracting concepts out of documents</title><categories>cs.CL</categories><comments>Technical report EPFL/LIA. 81 pages, 16 figures</comments><report-no>IC/2004/68</report-no><acm-class>I.2.7;I.7</acm-class><abstract>  Contrarily to standard approaches to topic annotation, the technique used in
this work does not centrally rely on some sort of -- possibly statistical --
keyword extraction. In fact, the proposed annotation algorithm uses a large
scale semantic database -- the EDR Electronic Dictionary -- that provides a
concept hierarchy based on hyponym and hypernym relations. This concept
hierarchy is used to generate a synthetic representation of the document by
aggregating the words present in topically homogeneous document segments into a
set of concepts best preserving the document's content.
  This new extraction technique uses an unexplored approach to topic selection.
Instead of using semantic similarity measures based on a semantic resource, the
later is processed to extract the part of the conceptual hierarchy relevant to
the document content. Then this conceptual hierarchy is searched to extract the
most relevant set of concepts to represent the topics discussed in the
document. Notice that this algorithm is able to extract generic concepts that
are not directly present in the document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412118</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412118</id><created>2004-12-29</created><authors><author><keyname>Buragohain</keyname><forenames>Chiranjeeb</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Suri</keyname><forenames>Subhash</forenames></author></authors><title>Power Aware Routing for Sensor Databases</title><categories>cs.NI cs.DC</categories><acm-class>C.2.1, C.2.2, F.1.3</acm-class><journal-ref>Proceedings of IEEE INFOCOM 2005, March 13-17, 2005 Miami</journal-ref><abstract>  Wireless sensor networks offer the potential to span and monitor large
geographical areas inexpensively. Sensor network databases like TinyDB are the
dominant architectures to extract and manage data in such networks. Since
sensors have significant power constraints (battery life), and high
communication costs, design of energy efficient communication algorithms is of
great importance. The data flow in a sensor database is very different from
data flow in an ordinary network and poses novel challenges in designing
efficient routing algorithms. In this work we explore the problem of energy
efficient routing for various different types of database queries and show that
in general, this problem is NP-complete. We give a constant factor
approximation algorithm for one class of query, and for other queries give
heuristic algorithms. We evaluate the efficiency of the proposed algorithms by
simulation and demonstrate their near optimal performance for various network
sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412119</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412119</id><created>2004-12-30</created><authors><author><keyname>Vagner</keyname><forenames>Shmuel</forenames></author></authors><title>CDTP Chain Distributed Transfer Protocol</title><categories>cs.NI cs.AR</categories><comments>PDF. 47 pages</comments><report-no>1937340</report-no><acm-class>C.2.2</acm-class><abstract>  The rapid growth of the internet in general and of bandwidth capacity at
internet clients in particular poses increasing computation and bandwidth
demands on internet servers. Internet access technologies like ADSL [DSL],
Cable Modem and Wireless modem allow internet clients to access the internet
with orders of magnitude more bandwidth than using traditional modems. We
present CDTP a distributed transfer protocol that allows clients to cooperate
and therefore remove the strain from the internet server thus achieving much
better performance than traditional transfer protocols (e.g. FTP [FTP]). The
CDTP server and client tools are presented also as well as results of
experiments. Finally a bandwidth measurement technique is presented. CDTP tools
use this technique to differentiate between slow and fast clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412120</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412120</id><created>2004-12-31</created><updated>2005-01-02</updated><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>An estimate of accuracy for interpolant numerical solutions of a PDE
  problem</title><categories>cs.NA math-ph math.MP</categories><comments>13 pages, 1 figure</comments><acm-class>G.1.8</acm-class><journal-ref>APPLIED AND INDUSTRIAL MATHEMATICS IN ITALY, Series on Advances in
  Mathematics for Applied Sciences - Vol. 69, World Scientific Company, 2005,
  56 - 64.</journal-ref><abstract>  In this paper we present an estimate of accuracy for a piecewise polynomial
approximation of a classical numerical solution to a non linear differential
problem. We suppose the numerical solution U is computed using a grid with a
small linear step and interval time Tu, while the polynomial approximation V is
an interpolation of the values of a numerical solution on a less fine grid and
interval time Tv &lt;&lt; Tu. The estimate shows that the interpolant solution V can
be, under suitable hypotheses, a good approximation and in general its
computational cost is much lower of the cost of the fine numerical solution. We
present two possible applications to linear case and periodic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0412121</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0412121</id><created>2004-12-31</created><authors><author><keyname>Treaster</keyname><forenames>Michael</forenames></author><author><keyname>Kiyanclar</keyname><forenames>Nadir</forenames></author><author><keyname>Koenig</keyname><forenames>Gregory A.</forenames></author><author><keyname>Yurcik</keyname><forenames>William</forenames></author></authors><title>A Distributed Economics-based Infrastructure for Utility Computing</title><categories>cs.DC</categories><comments>8 pages, 1 figure</comments><abstract>  Existing attempts at utility computing revolve around two approaches. The
first consists of proprietary solutions involving renting time on dedicated
utility computing machines. The second requires the use of heavy, monolithic
applications that are difficult to deploy, maintain, and use.
  We propose a distributed, community-oriented approach to utility computing.
Our approach provides an infrastructure built on Web Services in which modular
components are combined to create a seemingly simple, yet powerful system. The
community-oriented nature generates an economic environment which results in
fair transactions between consumers and providers of computing cycles while
simultaneously encouraging improvements in the infrastructure of the
computational grid itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501001</id><created>2004-12-31</created><authors><author><keyname>Treaster</keyname><forenames>Michael</forenames></author></authors><title>A Survey of Distributed Intrusion Detection Approaches</title><categories>cs.CR</categories><abstract>  Distributed intrustion detection systems detect attacks on computer systems
by analyzing data aggregated from distributed sources. The distributed nature
of the data sources allows patterns in the data to be seen that might not be
detectable if each of the sources were examined individually. This paper
describes the various approaches that have been developed to share and analyze
data in such systems, and discusses some issues that must be addressed before
fully decentralized distributed intrusion detection systems can be made viable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501002</id><created>2004-12-31</created><authors><author><keyname>Treaster</keyname><forenames>Michael</forenames></author></authors><title>A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel
  Systems</title><categories>cs.DC</categories><comments>11 pages</comments><abstract>  Supercomputing systems today often come in the form of large numbers of
commodity systems linked together into a computing cluster. These systems, like
any distributed system, can have large numbers of independent hardware
components cooperating or collaborating on a computation. Unfortunately, any of
this vast number of components can fail at any time, resulting in potentially
erroneous output. In order to improve the robustness of supercomputing
applications in the presence of failures, many techniques have been developed
to provide resilience to these kinds of system faults. This survey provides an
overview of these various fault-tolerance techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501003</id><created>2004-12-31</created><authors><author><keyname>Burovsky</keyname><forenames>P. A.</forenames></author></authors><title>Implementation of Motzkin-Burger algorithm in Maple</title><categories>cs.CG cs.CC cs.SC</categories><comments>9 pages, 4 additional text files</comments><acm-class>F.2.2; I.3.5; G.1.6</acm-class><abstract>  Subject of this paper is an implementation of a well-known Motzkin-Burger
algorithm, which solves the problem of finding the full set of solutions of a
system of linear homogeneous inequalities. There exist a number of
implementations of this algorithm, but there was no one in Maple, to the best
of the author's knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501004</identifier>
 <datestamp>2008-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501004</id><created>2005-01-03</created><authors><author><keyname>Rodriguez</keyname><forenames>Marko</forenames></author></authors><title>Advances towards a General-Purpose Societal-Scale Human-Collective
  Problem-Solving Engine</title><categories>cs.CY cs.HC</categories><comments>Collective Problem Solving Theory and Social-Network algorithm</comments><acm-class>H.1.2; H.4.2; H.5.3</acm-class><journal-ref>Proceedings of the International Conference on Systems, Man and
  Cybernetics, IEEE SMC, The Hague, Netherlands, volume 1, pages 206-211, ISSN:
  1062-922X, 2004</journal-ref><doi>10.1109/ICSMC.2004.1398298</doi><abstract>  Human collective intelligence has proved itself as an important factor in a
society's ability to accomplish large-scale behavioral feats. As societies have
grown in population-size, individuals have seen a decrease in their ability to
activeily participate in the problem-solving processes of the group.
Representative decision-making structures have been used as a modern solution
to society's inadequate information-processing infrastructure. With computer
and network technologies being further embedded within the fabric of society,
the implementation of a general-purpose societal-scale human-collective
problem-solving engine is envisioned as a means of furthering the
collective-intelligence potential of society. This paper provides both a novel
framework for creating collective intelligence systems and a method for
implementing a representative and expertise system based on social-network
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501005</identifier>
 <datestamp>2007-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501005</id><created>2005-01-03</created><authors><author><keyname>Fernandez</keyname><forenames>Alberto</forenames></author><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author></authors><title>Portfolio selection using neural networks</title><categories>cs.NE</categories><comments>12 pages; submitted to &quot;Computers &amp; Operations Research&quot;</comments><report-no>DEIM-RR-04-004</report-no><journal-ref>Computers &amp; Operations Research 34 (2007) 1177-1191</journal-ref><doi>10.1016/j.cor.2005.06.017</doi><abstract>  In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501006</id><created>2005-01-03</created><authors><author><keyname>Sistla</keyname><forenames>A. Prasad</forenames></author></authors><title>Formal Languages and Algorithms for Similarity based Retrieval from
  Sequence Databases</title><categories>cs.LO cs.DB</categories><acm-class>I.2.4; I.2.3; D.2.4</acm-class><abstract>  The paper considers various formalisms based on Automata, Temporal Logic and
Regular Expressions for specifying queries over sequences. Unlike traditional
binary semantics, the paper presents a similarity based semantics for thse
formalisms. More specifically, a distance measure in the range [0,1] is
associated with a sequence, query pair denoting how closely the sequence
satisfies the query. These measures are defined using a spectrum of normed
vector distance measures. Various distance measures based on the syntax and the
traditional semantics of the query are presented. Efficient algorithms for
computing these distance measure are presented. These algorithms can be
employed for retrieval of sequence from a database that closely satisfy a
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501007</id><created>2005-01-04</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Ungor</keyname><forenames>Alper</forenames></author></authors><title>A Time-Optimal Delaunay Refinement Algorithm in Two Dimensions</title><categories>cs.CG</categories><abstract>  We propose a new refinement algorithm to generate size-optimal
quality-guaranteed Delaunay triangulations in the plane. The algorithm takes
$O(n \log n + m)$ time, where $n$ is the input size and $m$ is the output size.
This is the first time-optimal Delaunay refinement algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501008</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501008</id><created>2005-01-05</created><authors><author><keyname>Masanes</keyname><forenames>Lluis</forenames></author><author><keyname>Acin</keyname><forenames>Antonio</forenames></author></authors><title>Multipartite Secret Correlations and Bound Information</title><categories>cs.CR cs.IT math.IT quant-ph</categories><comments>13 pages</comments><journal-ref>IEEE Trans. Inf. Theory, vol. 52, no. 10, pp. 4686-4694 (2006)</journal-ref><abstract>  We consider the problem of secret key extraction when $n$ honest parties and
an eavesdropper share correlated information. We present a family of
probability distributions and give the full characterization of its
distillation properties. This formalism allows us to design a rich variety of
cryptographic scenarios. In particular, we provide examples of multipartite
probability distributions containing non-distillable secret correlations, also
known as bound information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501009</id><created>2005-01-05</created><authors><author><keyname>Moscu</keyname><forenames>Mircea Alexandru Popescu</forenames></author></authors><title>On The Liniar Time Complexity of Finite Languages</title><categories>cs.CC</categories><abstract>  The present paper presents and proves a proposition concerning the time
complexity of finite languages. It is shown herein, that for any finite
language (a language for which the set of words composing it is finite) there
is a Turing machine that computes the language in such a way that for any input
of length k the machine stops in, at most, k + 1 steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501010</id><created>2005-01-06</created><updated>2005-01-07</updated><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>A Cryptographic Study of Some Digital Signature Schemes</title><categories>cs.CR</categories><comments>This is a Ph.D. thesis,under the supervision of Prof Sunder Lal, Dr.
  B.R. Ambedkar University, I.B.S. Khandri,AGRA- INDIA</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  In this thesis, we propose some directed signature schemes. In addition, we
have discussed their applications in different situations. In this thesis, we
would like to discuss the security aspects during the design process of the
proposed directed digital signature schemes. The security of the most digital
signature schemes widely use in practice is based on the two difficult
problems, viz; the problem of factoring integers (The RSA scheme) and the
problem of finding discrete logarithms over finite fields (The ElGamal scheme).
The proposed works in this thesis is divided into seven chapters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501011</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501011</id><created>2005-01-06</created><authors><author><keyname>Fedorenko</keyname><forenames>Sergei</forenames></author></authors><title>A simple algorithm for decoding Reed-Solomon codes and its relation to
  the Welch-Berlekamp algorithm</title><categories>cs.IT math.IT</categories><comments>7 pages. Submitted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. IT-51, no. 3, pp.
  1196-1198, 2005.</journal-ref><doi>10.1109/TIT.2004.842738</doi><abstract>  A simple and natural Gao algorithm for decoding algebraic codes is described.
Its relation to the Welch-Berlekamp and Euclidean algorithms is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501012</id><created>2005-01-07</created><updated>2005-08-23</updated><authors><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author><author><keyname>Payette</keyname><forenames>Sandy</forenames></author><author><keyname>Shin</keyname><forenames>Edwin</forenames></author><author><keyname>Wilper</keyname><forenames>Chris</forenames></author></authors><title>Fedora: An Architecture for Complex Objects and their Relationships</title><categories>cs.DL cs.MM</categories><comments>25 pages, 8 figures Draft of submission to Journal of Digital
  Libraries Special Issue on Complex Objects</comments><acm-class>H.3.7</acm-class><abstract>  The Fedora architecture is an extensible framework for the storage,
management, and dissemination of complex objects and the relationships among
them. Fedora accommodates the aggregation of local and distributed content into
digital objects and the association of services with objects. This al-lows an
object to have several accessible representations, some of them dy-namically
produced. The architecture includes a generic RDF-based relation-ship model
that represents relationships among objects and their components. Queries
against these relationships are supported by an RDF triple store. The
architecture is implemented as a web service, with all aspects of the complex
object architecture and related management functions exposed through REST and
SOAP interfaces. The implementation is available as open-source soft-ware,
providing the foundation for a variety of end-user applications for digital
libraries, archives, institutional repositories, and learning object systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501013</id><created>2005-01-08</created><updated>2006-08-27</updated><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Lou</keyname><forenames>Der-Chyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author></authors><title>On the security of the Yen-Guo's domino signal encryption algorithm
  (DSEA)</title><categories>cs.CR cs.MM nlin.CD</categories><comments>11 pages, 5 figures</comments><journal-ref>Journal of Systems and Software, vol. 79, no. 2, pp. 253-258, 2006</journal-ref><doi>10.1016/j.jss.2005.04.021</doi><abstract>  Recently, a new domino signal encryption algorithm (DSEA) was proposed for
digital signal transmission, especially for digital images and videos. This
paper analyzes the security of DSEA, and points out the following weaknesses:
1) its security against the brute-force attack was overestimated; 2) it is not
sufficiently secure against ciphertext-only attacks, and only one ciphertext is
enough to get some information about the plaintext and to break the value of a
sub-key; 3) it is insecure against known/chosen-plaintext attacks, in the sense
that the secret key can be recovered from a number of continuous bytes of only
one known/chosen plaintext and the corresponding ciphertext. Experimental
results are given to show the performance of the proposed attacks, and some
countermeasures are discussed to improve DSEA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501014</id><created>2005-01-08</created><updated>2007-03-27</updated><authors><author><keyname>Li</keyname><forenames>Shujun</forenames></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames></author><author><keyname>Cheung</keyname><forenames>Albert</forenames></author><author><keyname>Bhargava</keyname><forenames>Bharat</forenames></author><author><keyname>Lo</keyname><forenames>Kwok-Tung</forenames></author></authors><title>On the Design of Perceptual MPEG-Video Encryption Algorithms</title><categories>cs.MM cs.CR</categories><comments>10 pages, 5 figures, IEEEtran.cls</comments><journal-ref>IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 17, no. 2, pp. 214-223, 2007</journal-ref><doi>10.1109/TCSVT.2006.888840</doi><abstract>  In this paper, some existing perceptual encryption algorithms of MPEG videos
are reviewed and some problems, especially security defects of two recently
proposed MPEG-video perceptual encryption schemes, are pointed out. Then, a
simpler and more effective design is suggested, which selectively encrypts
fixed-length codewords (FLC) in MPEG-video bitstreams under the control of
three perceptibility factors. The proposed design is actually an encryption
configuration that can work with any stream cipher or block cipher. Compared
with the previously-proposed schemes, the new design provides more useful
features, such as strict size-preservation, on-the-fly encryption and multiple
perceptibility, which make it possible to support more applications with
different requirements. In addition, four different measures are suggested to
provide better security against known/chosen-plaintext attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501015</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501015</id><created>2005-01-09</created><authors><author><keyname>Bradonjic</keyname><forenames>Milan</forenames></author></authors><title>Application of Generating Functions and Partial Differential Equations
  in Coding Theory</title><categories>cs.IT math.IT</categories><comments>17 pages, 16 figures</comments><abstract>  In this work we have considered formal power series and partial differential
equations, and their relationship with Coding Theory. We have obtained the
nature of solutions for the partial differential equations for Cycle Poisson
Case. The coefficients for this case have been simulated, and the high tendency
of growth is shown. In the light of Complex Analysis, the Hadamard
Multiplication's Theorem is presented as a new approach to divide the power
sums relating to the error probability, each part of which can be analyzed
later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501016</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501016</id><created>2005-01-10</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>On the weight distribution of convolutional codes</title><categories>cs.IT math.IT math.OC</categories><abstract>  Detailed information about the weight distribution of a convolutional code is
given by the adjacency matrix of the state diagram associated with a controller
canonical form of the code. We will show that this matrix is an invariant of
the code. Moreover, it will be proven that codes with the same adjacency matrix
have the same dimension and the same Forney indices and finally that for
one-dimensional binary convolutional codes the adjacency matrix determines the
code uniquely up to monomial equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501017</identifier>
 <datestamp>2007-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501017</id><created>2005-01-10</created><updated>2007-10-29</updated><authors><author><keyname>Maze</keyname><forenames>G.</forenames></author><author><keyname>Monico</keyname><forenames>C.</forenames></author><author><keyname>Rosenthal</keyname><forenames>J.</forenames></author></authors><title>Public Key Cryptography based on Semigroup Actions</title><categories>cs.CR cs.IT math.IT</categories><comments>20 pages. To appear in Advances in Mathematics of Communications</comments><abstract>  A generalization of the original Diffie-Hellman key exchange in $(\Z/p\Z)^*$
found a new depth when Miller and Koblitz suggested that such a protocol could
be used with the group over an elliptic curve. In this paper, we propose a
further vast generalization where abelian semigroups act on finite sets. We
define a Diffie-Hellman key exchange in this setting and we illustrate how to
build interesting semigroup actions using finite (simple) semirings. The
practicality of the proposed extensions rely on the orbit sizes of the
semigroup actions and at this point it is an open question how to compute the
sizes of these orbits in general and also if there exists a square root attack
in general. In Section 2 a concrete practical semigroup action built from
simple semirings is presented. It will require further research to analyse this
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501018</id><created>2005-01-10</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Bigham</keyname><forenames>Jeffrey</forenames></author><author><keyname>Shnayder</keyname><forenames>Victor</forenames></author></authors><title>Combining Independent Modules in Lexical Multiple-Choice Problems</title><categories>cs.LG cs.CL cs.IR</categories><comments>10 pages, related work available at
  http://www.cs.rutgers.edu/~mlittman/ and http://purl.org/peter.turney/</comments><report-no>NRC-47434</report-no><acm-class>I.2.6; I.2.7; H.3.1; J.5</acm-class><journal-ref>Recent Advances in Natural Language Processing III: Selected
  Papers from RANLP 2003, Eds: N. Nicolov, K. Botcheva, G. Angelova, and R.
  Mitkov, (2004), Current Issues in Linguistic Theory (CILT), 260, John
  Benjamins, 101-110</journal-ref><abstract>  Existing statistical approaches to natural language problems are very coarse
approximations to the true complexity of language processing. As such, no
single technique will be best for all problem instances. Many researchers are
examining ensemble methods that combine the output of multiple modules to
create more accurate solutions. This paper examines three merging rules for
combining probability distributions: the familiar mixture rule, the logarithmic
rule, and a novel product rule. These rules were applied with state-of-the-art
results to two problems used to assess human mastery of lexical semantics --
synonym questions and analogy questions. All three merging rules result in
ensembles that are more accurate than any of their component modules. The
differences among the three rules are not statistically significant, but it is
suggestive that the popular mixture rule is not the best rule for either of the
two problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501019</id><created>2005-01-11</created><authors><author><keyname>Pivovarov</keyname><forenames>G. B.</forenames></author><author><keyname>Trunov</keyname><forenames>S. E.</forenames></author></authors><title>Clustering SPIRES with EqRank</title><categories>cs.DL cs.IR</categories><comments>3 pp</comments><abstract>  SPIRES is the largest database of scientific papers in the subject field of
high energy and nuclear physics. It contains information on the citation graph
of more than half a million of papers (vertexes of the citation graph). We
outline the EqRank algorithm designed to cluster vertexes of directed graphs,
and present the results of EqRank application to the SPIRES citation graph. The
hierarchical clustering of SPIRES yielded by EqRank is used to set up a web
service, which is also outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501020</id><created>2005-01-11</created><authors><author><keyname>Buccafurri</keyname><forenames>Francesco</forenames></author><author><keyname>Lax</keyname><forenames>Gianluca</forenames></author><author><keyname>Sacca'</keyname><forenames>Domenico</forenames></author><author><keyname>Pontieri</keyname><forenames>Luigi</forenames></author><author><keyname>Rosaci</keyname><forenames>Domenico</forenames></author></authors><title>Enhancing Histograms by Tree-Like Bucket Indices</title><categories>cs.DS</categories><comments>26 pages, 9 figures</comments><abstract>  Histograms are used to summarize the contents of relations into a number of
buckets for the estimation of query result sizes. Several techniques (e.g.,
MaxDiff and V-Optimal) have been proposed in the past for determining bucket
boundaries which provide accurate estimations. However, while search strategies
for optimal bucket boundaries are rather sophisticated, no much attention has
been paid for estimating queries inside buckets and all of the above techniques
adopt naive methods for such an estimation. This paper focuses on the problem
of improving the estimation inside a bucket once its boundaries have been
fixed. The proposed technique is based on the addition, to each bucket, of
32-bit additional information (organized into a 4-level tree index), storing
approximate cumulative frequencies at 7 internal intervals of the bucket. Both
theoretical analysis and experimental results show that, among a number of
alternative ways to organize the additional information, the 4-level tree index
provides the best frequency estimation inside a bucket. The index is later
added to two well-known histograms, MaxDiff and V-Optimal, obtaining the
non-obvious result that despite the spatial cost of 4LT which reduces the
number of allowed buckets once the storage space has been fixed, the original
methods are strongly improved in terms of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501021</identifier>
 <datestamp>2010-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501021</id><created>2005-01-11</created><authors><author><keyname>Harting</keyname><forenames>J.</forenames></author><author><keyname>Chin</keyname><forenames>J.</forenames></author><author><keyname>Venturoli</keyname><forenames>M.</forenames></author><author><keyname>Coveney</keyname><forenames>P. V.</forenames></author></authors><title>Large-scale lattice Boltzmann simulations of complex fluids: advances
  through the advent of computational grids</title><categories>cs.DC cond-mat.other cond-mat.soft physics.flu-dyn</categories><comments>18 pages, 9 figures, 4 movies available, accepted for publication in
  Phil. Trans. R. Soc. London Series A</comments><journal-ref>Phil. Trans. R. Soc. London Series A 363 1895-1915 (2005)</journal-ref><doi>10.1098/rsta.2005.1618</doi><abstract>  During the last two years the RealityGrid project has allowed us to be one of
the few scientific groups involved in the development of computational grids.
Since smoothly working production grids are not yet available, we have been
able to substantially influence the direction of software development and grid
deployment within the project. In this paper we review our results from large
scale three-dimensional lattice Boltzmann simulations performed over the last
two years. We describe how the proactive use of computational steering and
advanced job migration and visualization techniques enabled us to do our
scientific work more efficiently. The projects reported on in this paper are
studies of complex fluid flows under shear or in porous media, as well as
large-scale parameter searches, and studies of the self-organisation of liquid
cubic mesophases.
  Movies are available at
http://www.ica1.uni-stuttgart.de/~jens/pub/05/05-PhilTransReview.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501022</id><created>2005-01-11</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Hempel</keyname><forenames>Harald</forenames></author><author><keyname>Nickelsen</keyname><forenames>Arfst</forenames></author></authors><title>Algebraic Properties for Selector Functions</title><categories>cs.CC</categories><comments>More recent version of most of this report appears in SICOMP, but the
  appendix here is not included there</comments><report-no>URCS-TR-778 (January 7, 2004 revision)</report-no><acm-class>F.1.3; F.1.2; F.1.1</acm-class><journal-ref>SICOMP, V. 33, Number 6, pp. 1309--1337, 2004</journal-ref><abstract>  The nondeterministic advice complexity of the P-selective sets is known to be
exactly linear. Regarding the deterministic advice complexity of the
P-selective sets--i.e., the amount of Karp--Lipton advice needed for
polynomial-time machines to recognize them in general--the best current upper
bound is quadratic [Ko, 1983] and the best current lower bound is linear
[Hemaspaandra and Torenvliet, 1996].
  We prove that every associatively P-selective set is commutatively,
associatively P-selective. Using this, we establish an algebraic sufficient
condition for the P-selective sets to have a linear upper bound (which thus
would match the existing lower bound) on their deterministic advice complexity:
If all P-selective sets are associatively P-selective then the deterministic
advice complexity of the P-selective sets is linear. The weakest previously
known sufficient condition was P=NP.
  We also establish related results for algebraic properties of, and advice
complexity of, the nondeterministically selective sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501023</identifier>
 <datestamp>2008-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501023</id><created>2005-01-12</created><updated>2008-07-18</updated><authors><author><keyname>Mitra</keyname><forenames>Arindam</forenames></author></authors><title>No-cloning principal can alone provide security</title><categories>cs.IT math.IT</categories><comments>Acknowledgement is due</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing quantum key distribution schemes need the support of classical
authentication scheme to ensure security. This is a conceptual drawback of
quantum cryptography. It is pointed out that quantum cryptosystem does not need
any support of classical cryptosystem to ensure security. No-cloning principal
can alone provide security in communication. Even no-cloning principle itself
can help to authenticate each bit of information. It implies that quantum
password need not to be a secret password.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501024</identifier>
 <datestamp>2010-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501024</id><created>2005-01-12</created><updated>2005-06-17</updated><authors><author><keyname>Ziegler</keyname><forenames>Martin</forenames></author></authors><title>Effectively Open Real Functions</title><categories>cs.LO</categories><comments>added section on semi-algebraic functions; to appear in Proc.
  http://cca-net.de/cca2005</comments><acm-class>F.4.1</acm-class><journal-ref>pp.827-849 in Journal of Complexity vol.22 (2006)</journal-ref><doi>10.1016/j.jco.2006.05.002</doi><abstract>  A function f is continuous iff the PRE-image f^{-1}[V] of any open set V is
open again. Dual to this topological property, f is called OPEN iff the IMAGE
f[U] of any open set U is open again. Several classical Open Mapping Theorems
in Analysis provide a variety of sufficient conditions for openness.
  By the Main Theorem of Recursive Analysis, computable real functions are
necessarily continuous. In fact they admit a well-known characterization in
terms of the mapping V+-&gt;f^{-1}[V] being EFFECTIVE: Given a list of open
rational balls exhausting V, a Turing Machine can generate a corresponding list
for f^{-1}[V]. Analogously, EFFECTIVE OPENNESS requires the mapping U+-&gt;f[U] on
open real subsets to be effective.
  By effectivizing classical Open Mapping Theorems as well as from application
of Tarski's Quantifier Elimination, the present work reveals several rich
classes of functions to be effectively open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501025</id><created>2005-01-12</created><authors><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Ternovska</keyname><forenames>Eugenia</forenames></author></authors><title>A Logic for Non-Monotone Inductive Definitions</title><categories>cs.AI cs.LO</categories><comments>50 pages, TOCL submission</comments><abstract>  Well-known principles of induction include monotone induction and different
sorts of non-monotone induction such as inflationary induction, induction over
well-founded sets and iterated induction. In this work, we define a logic
formalizing induction over well-founded sets and monotone and iterated
induction. Just as the principle of positive induction has been formalized in
FO(LFP), and the principle of inflationary induction has been formalized in
FO(IFP), this paper formalizes the principle of iterated induction in a new
logic for Non-Monotone Inductive Definitions (ID-logic). The semantics of the
logic is strongly influenced by the well-founded semantics of logic
programming. Our main result concerns the modularity properties of inductive
definitions in ID-logic. Specifically, we formulate conditions under which a
simultaneous definition $\D$ of several relations is logically equivalent to a
conjunction of smaller definitions $\D_1 \land ... \land \D_n$ with disjoint
sets of defined predicates. The difficulty of the result comes from the fact
that predicates $P_i$ and $P_j$ defined in $\D_i$ and $\D_j$, respectively, may
be mutually connected by simultaneous induction. Since logic programming and
abductive logic programming under well-founded semantics are proper fragments
of our logic, our modularity results are applicable there as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501026</id><created>2005-01-13</created><authors><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author></authors><title>On the Sensitivity of Cyclically-Invariant Boolean Functions</title><categories>cs.CC</categories><abstract>  In this paper we construct a cyclically invariant Boolean function whose
sensitivity is $\Theta(n^{1/3})$. This result answers two previously published
questions. Tur\'an (1984) asked if any Boolean function, invariant under some
transitive group of permutations, has sensitivity $\Omega(\sqrt{n})$. Kenyon
and Kutin (2004) asked whether for a ``nice'' function the product of
0-sensitivity and 1-sensitivity is $\Omega(n)$. Our function answers both
questions in the negative.
  We also prove that for minterm-transitive functions (a natural class of
Boolean functions including our example) the sensitivity is $\Omega(n^{1/3})$.
Hence for this class of functions sensitivity and block sensitivity are
polynomially related.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501027</id><created>2005-01-13</created><updated>2005-05-19</updated><authors><author><keyname>Greenberg</keyname><forenames>Evan P.</forenames></author><author><keyname>Cheriton</keyname><forenames>David R.</forenames></author></authors><title>Enforcing Bulk Mail Classification</title><categories>cs.NI</categories><comments>6 pages, changed spin on paper, added new idea (explicit tagging as a
  feature)</comments><abstract>  Spam costs US corporations upwards of $8.9 billion a year, and comprises as
much as 40% of all email received. Solutions exist to reduce the amount of spam
seen by end users, but cannot withstand sophisticated attacks. Worse yet, many
will occasionally misclassify and silently drop legitimate email. Spammers take
advantage of the near-zero cost of sending email to flood the network, knowing
that success even a tiny fraction of the time means a profit. End users,
however, have proven unwilling to pay money to send email to friends and
family.
  We show that it is feasible to extend the existing mail system to reduce the
amount of unwanted email, without misclassifying email, and without charging
well-behaved users. We require that bulk email senders accurately classify each
email message they send as an advertisement with an area of interest or else be
charged a small negative incentive per message delivered. Recipients are able
to filter out email outside their scope of interest, while senders are able to
focus their sendings to the appropriate audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501028</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501028</id><created>2005-01-14</created><authors><author><keyname>de Rooij</keyname><forenames>Steven</forenames></author><author><keyname>Grunwald</keyname><forenames>Peter</forenames></author></authors><title>An Empirical Study of MDL Model Selection with Infinite Parametric
  Complexity</title><categories>cs.LG cs.IT math.IT</categories><comments>23 pages, 11 graphs</comments><acm-class>E.3; G.4</acm-class><abstract>  Parametric complexity is a central concept in MDL model selection. In
practice it often turns out to be infinite, even for quite simple models such
as the Poisson and Geometric families. In such cases, MDL model selection as
based on NML and Bayesian inference based on Jeffreys' prior can not be used.
Several ways to resolve this problem have been proposed. We conduct experiments
to compare and evaluate their behaviour on small sample sizes.
  We find interestingly poor behaviour for the plug-in predictive code; a
restricted NML model performs quite well but it is questionable if the results
validate its theoretical motivation. The Bayesian model with the improper
Jeffreys' prior is the most dependable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501029</id><created>2005-01-14</created><authors><author><keyname>Buccafurri</keyname><forenames>Francesco</forenames></author><author><keyname>Furfaro</keyname><forenames>Filippo</forenames></author><author><keyname>Sacca'</keyname><forenames>Domenico</forenames></author></authors><title>Estimating Range Queries using Aggregate Data with Integrity
  Constraints: a Probabilistic Approach</title><categories>cs.DB</categories><comments>46 pages, 6 figures</comments><acm-class>E.4; G.3; H.3</acm-class><abstract>  The problem of recovering (count and sum) range queries over multidimensional
data only on the basis of aggregate information on such data is addressed. This
problem can be formalized as follows. Suppose that a transformation T producing
a summary from a multidimensional data set is used. Now, given a data set D, a
summary S=T(D) and a range query r on D, the problem consists of studying r by
modelling it as a random variable defined over the sample space of all the data
sets D' such that T(D) = S. The study of such a random variable, done by the
definition of its probability distribution and the computation of its mean
value and variance, represents a well-founded, theoretical probabilistic
approach for estimating the query only on the basis of the available
information (that is the summary S) without assumptions on original data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501030</id><created>2005-01-15</created><authors><author><keyname>Tsarev</keyname><forenames>Sergey P.</forenames></author></authors><title>Generalized Laplace transformations and integration of hyperbolic
  systems of linear partial differential equations</title><categories>cs.SC math.AP nlin.SI</categories><comments>LaTeX, 17 pages, Submitted to ISSAC 2005, Beijing, China, July 24--27
  2005</comments><abstract>  We give a new procedure for generalized factorization and construction of the
complete solution of strictly hyperbolic linear partial differential equations
or strictly hyperbolic systems of such equations in the plane. This procedure
generalizes the classical theory of Laplace transformations of second-order
equations in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501031</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501031</id><created>2005-01-16</created><updated>2005-07-19</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>From truth to computability II</title><categories>cs.LO cs.AI math.LO</categories><acm-class>F.1.1; F.1.2</acm-class><journal-ref>Theoretical Computer Science 379 (2007), pp. 20-52</journal-ref><doi>10.1016/j.tcs.2007.01.004</doi><abstract>  Computability logic is a formal theory of computational tasks and resources.
Formulas in it represent interactive computational problems, and &quot;truth&quot; is
understood as algorithmic solvability. Interactive computational problems, in
turn, are defined as a certain sort games between a machine and its
environment, with logical operators standing for operations on such games.
Within the ambitious program of finding axiomatizations for incrementally rich
fragments of this semantically introduced logic, the earlier article &quot;From
truth to computability I&quot; proved soundness and completeness for system CL3,
whose language has the so called parallel connectives (including negation),
choice connectives, choice quantifiers, and blind quantifiers. The present
paper extends that result to the significantly more expressive system CL4 with
the same collection of logical operators. What makes CL4 expressive is the
presence of two sorts of atoms in its language: elementary atoms, representing
elementary computational problems (i.e. predicates, i.e. problems of zero
degree of interactivity), and general atoms, representing arbitrary
computational problems. CL4 conservatively extends CL3, with the latter being
nothing but the general-atom-free fragment of the former. Removing the blind
(classical) group of quantifiers from the language of CL4 is shown to yield a
decidable logic despite the fact that the latter is still first-order. A
comprehensive online source on computability logic can be found at
http://www.cis.upenn.edu/~giorgi/cl.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501032</id><created>2005-01-16</created><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>On Partially Additive Kleene Algebras</title><categories>cs.LO</categories><comments>23 pages; to be presented at the 8th International Conference on
  Relational Methods in Computer Science (RelMiCS 8)</comments><acm-class>F.3.1;I.1.3</acm-class><abstract>  We define the notion of a partially additive Kleene algebra, which is a
Kleene algebra where the + operation need only be partially defined. These
structures formalize a number of examples that cannot be handled directly by
Kleene algebras. We relate partially additive Kleene algebras to existing
algebraic structures, by exhibiting categorical connections with Kleene
algebras, partially additive categories, and closed semirings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501033</identifier>
 <datestamp>2007-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501033</id><created>2005-01-18</created><authors><author><keyname>Curien</keyname><forenames>Pierre-Louis</forenames><affiliation>PPS</affiliation></author></authors><title>Playful, streamlike computation</title><categories>cs.LO</categories><proxy>ccsd ccsd-00003869</proxy><journal-ref>Domain theory, logic and computation, Kluwer Academic Publishers
  (Ed.) (2003) 1-24</journal-ref><abstract>  We offer a short tour into the interactive interpretation of sequential
programs. We emphasize streamlike computation -- that is, computation of
successive bits of information upon request. The core of the approach surveyed
here dates back to the work of Berry and the author on sequential algorithms on
concrete data structures in the late seventies, culminating in the design of
the programming language CDS, in which the semantics of programs of any type
can be explored interactively. Around one decade later, two major insights of
Cartwright and Felleisen on one hand, and of Lamarche on the other hand gave
new, decisive impulses to the study of sequentiality. Cartwright and Felleisen
observed that sequential algorithms give a direct semantics to control
operators like \&quot;call-cc\&quot; and proposed to include explicit errors both in the
syntax and in the semantics of the language PCF. Lamarche (unpublished)
connected sequential algorithms to linear logic and games. The successful
program of games semantics has spanned over the nineties until now, starting
with syntax-independent characterizations of the term model of PCF by Abramsky,
Jagadeesan, and Malacaria on one hand, and by Hyland and Ong on the other hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501034</identifier>
 <datestamp>2007-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501034</id><created>2005-01-18</created><authors><author><keyname>Curien</keyname><forenames>Pierre-Louis</forenames><affiliation>PPS</affiliation></author></authors><title>Symmetry and interactivity in Programming</title><categories>cs.LO</categories><proxy>ccsd ccsd-00003868</proxy><journal-ref>Bulletin of Symbolic Logic 9, 2 (2003) 169-180</journal-ref><abstract>  We recall some of the early occurrences of the notions of interactivity and
symmetry in the operational and denotational semantics of programming
languages. We suggest some connections with ludics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501035</identifier>
 <datestamp>2007-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501035</id><created>2005-01-18</created><authors><author><keyname>Curien</keyname><forenames>Pierre-Louis</forenames><affiliation>PPS</affiliation></author></authors><title>Introduction to linear logic and ludics, part I</title><categories>cs.LO</categories><proxy>ccsd ccsd-00003920</proxy><journal-ref>Advances in Mathematics (China) 34, 5 (2005) 513-544</journal-ref><abstract>  This two-parts paper offers a survey of linear logic and ludics, which were
introduced by Girard in 1986 and 2001, respectively. Both theories revisit
mathematical logic from first principles, with inspiration from and
applications to computer science. The present part I covers an introduction to
the connectives and proof rules of linear logic, to its decidability
properties, and to its models. Part II will deal with proof nets, a graph-like
representation of proofs which is one of the major innovations of linear logic,
and will present an introduction to ludics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501036</id><created>2005-01-18</created><updated>2005-01-25</updated><authors><author><keyname>Aknine</keyname><forenames>Jose Ghislain Quenum Samir</forenames></author></authors><title>Enabling Agents to Dynamically Select Protocols for Interactions</title><categories>cs.MA cs.SE</categories><abstract>  in this paper we describe a method which allows agents to dynamically select
protocols and roles when they need to execute collaborative tasks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501037</id><created>2005-01-18</created><updated>2005-01-18</updated><authors><author><keyname>Tucci</keyname><forenames>Michele</forenames></author></authors><title>Oligopolistic Competition in an Evolutionary Environment: a Computer
  Simulation</title><categories>cs.CY</categories><comments>PDF, 20 pages, 4 graphs and a post-scriptum</comments><acm-class>K.4.0; J.4; I.6.3</acm-class><abstract>  The following notes contain a computer simulation concerning effective
competition in an evolutionary environment. The scope is to underline the
existence of a side effect pertaining to the competitive processes: the
tendency toward an excess of supply by producers which operate in a strongly
competitive situation. A set of four oligopolistic firms will be employed in
the formal reconstruction. The simulation will operate following the Edmond
Malinvaud &quot;short side&quot; approach, as far as the price adjustment is concerned,
and the sequential Hicksian &quot;weeks&quot; structure with regard of the temporal
characterization. The content of the present paper ought to be considered as a
development of the writing: Michele Tucci, Evolution and Gravitation: a
Computer Simulation of a Non-Walrasian Equilibrium Model, published with the
E-print Archives at arXiv.com (section: Computer Science, registration number:
cs.CY/0209017). In such a paper there can be found some preliminary
considerations regarding the comparison between the evolutionary and the
gravitational paradigms and the evaluation of approaches belonging to rival
schools of economic thought.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501038</id><created>2005-01-19</created><authors><author><keyname>Capelis</keyname><forenames>D. J.</forenames></author></authors><title>Data Tastes Better Seasoned: Introducing the ASH Family of Hashing
  Algorithms</title><categories>cs.CR</categories><comments>5 pages</comments><acm-class>E.3</acm-class><abstract>  Over the recent months it has become clear that the current generation of
cryptographic hashing algorithms are insufficient to meet future needs. The ASH
family of algorithms provides modifications to the existing SHA-2 family. These
modifications are designed with two main goals: 1) Providing increased
collision resistance. 2) Increasing mitigation of security risks
post-collision. The unique public/private sections and salt/pepper design
elements provide increased flexibility for a broad range of applications. The
ASH family is a new generation of cryptographic hashing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501039</identifier>
 <datestamp>2007-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501039</id><created>2005-01-19</created><authors><author><keyname>Curien</keyname><forenames>Pierre-Louis</forenames><affiliation>PPS</affiliation></author></authors><title>Introduction to linear logic and ludics, part II</title><categories>cs.LO</categories><proxy>ccsd ccsd-00003939</proxy><journal-ref>Advances in Mathematics (China) 35, 1 (2006) 1-44</journal-ref><abstract>  This paper is the second part of an introduction to linear logic and ludics,
both due to Girard. It is devoted to proof nets, in the limited, yet central,
framework of multiplicative linear logic and to ludics, which has been recently
developped in an aim of further unveiling the fundamental interactive nature of
computation and logic. We hope to offer a few computer science insights into
this new theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501040</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501040</id><created>2005-01-19</created><updated>2005-06-21</updated><authors><author><keyname>Aceto</keyname><forenames>Luca</forenames></author><author><keyname>Fokkink</keyname><forenames>Wan</forenames></author><author><keyname>Ingolfsdottir</keyname><forenames>Anna</forenames></author><author><keyname>Luttik</keyname><forenames>Bas</forenames></author></authors><title>Split-2 Bisimilarity has a Finite Axiomatization over CCS with&lt;br&gt;
  Hennessy&amp;#39;s Merge</title><categories>cs.LO</categories><acm-class>D.3.1; F.1.1; F.1.2; F.3.2; F.3.4; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (March 9,
  2005) lmcs:981</journal-ref><doi>10.2168/LMCS-1(1:3)2005</doi><abstract>  This note shows that split-2 bisimulation equivalence (also known as timed
equivalence) affords a finite equational axiomatization over the process
algebra obtained by adding an auxiliary operation proposed by Hennessy in 1981
to the recursion, relabelling and restriction free fragment of Milner's
Calculus of Communicating Systems. Thus the addition of a single binary
operation, viz. Hennessy's merge, is sufficient for the finite equational
axiomatization of parallel composition modulo this non-interleaving
equivalence. This result is in sharp contrast to a theorem previously obtained
by the same authors to the effect that the same language is not finitely based
modulo bisimulation equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501041</id><created>2005-01-20</created><authors><author><keyname>Kondratiev</keyname><forenames>A. S.</forenames><affiliation>Moscow Power Engineering Institute</affiliation><affiliation>Altair Naval Research Institute of Radio Electronics</affiliation></author><author><keyname>Polishchuk</keyname><forenames>N. P.</forenames><affiliation>Altair Naval Research Institute of Radio Electronics</affiliation></author></authors><title>Two Iterative Algorithms for Solving Systems of Simultaneous Linear
  Algebraic Equations with Real Matrices of Coefficients</title><categories>cs.NA</categories><comments>5 pages; 0 figures; 4 references</comments><acm-class>G.1.3; G.1.6</acm-class><abstract>  The paper describes two iterative algorithms for solving general systems of M
simultaneous linear algebraic equations (SLAE) with real matrices of
coefficients. The system can be determined, underdetermined, and
overdetermined. Linearly dependent equations are also allowed. Both algorithms
use the method of Lagrange multipliers to transform the original SLAE into a
positively determined function F of real original variables X(i) (i=1,...,N)
and Lagrange multipliers Lambda(i) (i=1,...,M). Function F is differentiated
with respect to variables X(i) and the obtained relationships are used to
express F in terms of Lagrange multipliers Lambda(i). The obtained function is
minimized with respect to variables Lambda(i) with the help of one of two the
following minimization techniques: (1) relaxation method or (2) method of
conjugate gradients by Fletcher and Reeves. Numerical examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501042</id><created>2005-01-20</created><updated>2005-02-09</updated><authors><author><keyname>Bernauer</keyname><forenames>Martin</forenames></author></authors><title>Maintaining Consistency of Data on the Web</title><categories>cs.DB cs.DS</categories><acm-class>E.1; H.2.3; H.2.4</acm-class><abstract>  Increasingly more data is becoming available on the Web, estimates speaking
of 1 billion documents in 2002. Most of the documents are Web pages whose data
is considered to be in XML format, expecting it to eventually replace HTML.
  A common problem in designing and maintaining a Web site is that data on a
Web page often replicates or derives from other data, the so-called base data,
that is usually not contained in the deriving or replicating page.
Consequently, replicas and derivations become inconsistent upon modifying base
data in a Web page or a relational database. For example, after assigning a
thesis to a student and modifying the Web page that describes it in detail, the
thesis is still incorrectly contained in the list of offered thesis, missing in
the list of ongoing thesis, and missing in the advisor's teaching record.
  The thesis presents a solution by proposing a combined approach that provides
for maintaining consistency of data in Web pages that (i) replicate data in
relational databases, or (ii) replicate or derive from data in Web pages. Upon
modifying base data, the modification is immediately pushed to affected Web
pages. There, maintenance is performed incrementally by only modifying the
affected part of the page instead of re-generating the whole page from scratch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501043</identifier>
 <datestamp>2011-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501043</id><created>2005-01-25</created><authors><author><keyname>Drabent</keyname><forenames>W.</forenames></author><author><keyname>Milkowska</keyname><forenames>M.</forenames></author></authors><title>Proving Correctness and Completeness of Normal Programs - a Declarative
  Approach</title><categories>cs.LO cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP). 44
  pages</comments><acm-class>F.3.1; D.1.6; D.2.4</acm-class><journal-ref>Theory and Practice of Logic Programming, 5(6):669-711, 2005</journal-ref><abstract>  We advocate a declarative approach to proving properties of logic programs.
Total correctness can be separated into correctness, completeness and clean
termination; the latter includes non-floundering. Only clean termination
depends on the operational semantics, in particular on the selection rule. We
show how to deal with correctness and completeness in a declarative way,
treating programs only from the logical point of view. Specifications used in
this approach are interpretations (or theories). We point out that
specifications for correctness may differ from those for completeness, as
usually there are answers which are neither considered erroneous nor required
to be computed.
  We present proof methods for correctness and completeness for definite
programs and generalize them to normal programs. For normal programs we use the
3-valued completion semantics; this is a standard semantics corresponding to
negation as finite failure. The proof methods employ solely the classical
2-valued logic. We use a 2-valued characterization of the 3-valued completion
semantics which may be of separate interest. The presented methods are compared
with an approach based on operational semantics. We also employ the ideas of
this work to generalize a known method of proving termination of normal
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501044</id><created>2005-01-20</created><authors><author><keyname>Haubold</keyname><forenames>Alexander</forenames></author><author><keyname>Kender</keyname><forenames>John R.</forenames></author></authors><title>Augmented Segmentation and Visualization for Presentation Videos</title><categories>cs.MM cs.IR</categories><acm-class>H.2.4;H.3.1</acm-class><abstract>  We investigate methods of segmenting, visualizing, and indexing presentation
videos by separately considering audio and visual data. The audio track is
segmented by speaker, and augmented with key phrases which are extracted using
an Automatic Speech Recognizer (ASR). The video track is segmented by visual
dissimilarities and augmented by representative key frames. An interactive user
interface combines a visual representation of audio, video, text, and key
frames, and allows the user to navigate a presentation video. We also explore
clustering and labeling of speaker data and present preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501045</id><created>2005-01-20</created><authors><author><keyname>Clarkson</keyname><forenames>Kenneth L.</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>Improved Approximation Algorithms for Geometric Set Cover</title><categories>cs.CG cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  Given a collection S of subsets of some set U, and M a subset of U, the set
cover problem is to find the smallest subcollection C of S such that M is a
subset of the union of the sets in C. While the general problem is NP-hard to
solve, even approximately, here we consider some geometric special cases, where
usually U = R^d. Extending prior results, we show that approximation algorithms
with provable performance exist, under a certain general condition: that for a
random subset R of S and function f(), there is a decomposition of the portion
of U not covered by R into an expected f(|R|) regions, each region of a
particular simple form. We show that under this condition, a cover of size
O(f(|C|)) can be found. Our proof involves the generalization of shallow
cuttings to more general geometric situations. We obtain constant-factor
approximation algorithms for covering by unit cubes in R^3, for guarding a
one-dimensional terrain, and for covering by similar-sized fat triangles in
R^2. We also obtain improved approximation guarantees for fat triangles, of
arbitrary size, and for a class of fat objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501046</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501046</id><created>2005-01-20</created><authors><author><keyname>Toffoli</keyname><forenames>Tommaso</forenames></author></authors><title>Thermodynamics of used punched tape: A weak and a strong equivalence
  principle</title><categories>cs.IT math.IT</categories><comments>7 pages, 8 figures</comments><abstract>  We study the repeated use of a monotonic recording medium--such as punched
tape or photographic plate--where marks can be added at any time but never
erased. (For practical purposes, also the electromagnetic &quot;ether&quot; falls into
this class.) Our emphasis is on the case where the successive users act
independently and selfishly, but not maliciously; typically, the &quot;first user&quot;
would be a blind natural process tending to degrade the recording medium, and
the &quot;second user&quot; a human trying to make the most of whatever capacity is left.
  To what extent is a length of used tape &quot;equivalent&quot;--for information
transmission purposes--to a shorter length of virgin tape? Can we characterize
a piece of used tape by an appropriate &quot;effective length&quot; and forget all other
details? We identify two equivalence principles. The weak principle is exact,
but only holds for a sequence of infinitesimal usage increments. The strong
principle holds for any amount of incremental usage, but is only approximate;
nonetheless, it is quite accurate even in the worst case and is virtually exact
over most of the range--becoming exact in the limit of heavily used tape.
  The fact that strong equivalence does not hold exactly, but then it does
almost exactly, comes as a bit of a surprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501047</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501047</id><created>2005-01-21</created><authors><author><keyname>Li</keyname><forenames>Husheng</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Impact of Channel Estimation Errors on Multiuser Detection via the
  Replica Method</title><categories>cs.IT math.IT</categories><comments>To appear in the EURASIP Journal on Wireless Communication and
  Networking - Special Issue on Advanced Signal Processing Algorithms for
  Wireless Communications</comments><abstract>  For practical wireless DS-CDMA systems, channel estimation is imperfect due
to noise and interference. In this paper, the impact of channel estimation
errors on multiuser detection (MUD) is analyzed under the framework of the
replica method. System performance is obtained in the large system limit for
optimal MUD, linear MUD and turbo MUD, and is validated by numerical results
for finite systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501048</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501048</id><created>2005-01-21</created><authors><author><keyname>Li</keyname><forenames>Husheng</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Low Complexity Joint Iterative Equalization and Multiuser Detection in
  Dispersive DS-CDMA Channels</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><abstract>  Communications in dispersive direct-sequence code-division multiple-access
(DS-CDMA) channels suffer from intersymbol and multiple-access interference,
which can significantly impair performance. Joint maximum \textit{a posteriori}
probability (MAP) equalization and multiuser detection with error control
decoding can be used to mitigate this interference and to achieve the optimal
bit error rate. Unfortunately, such optimal detection typically requires
prohibitive computational complexity. This problem is addressed in this paper
through the development of a reduced state trellis search detection algorithm,
based on decision feedback from channel decoders. The performance of this
algorithm is analyzed in the large-system limit. This analysis and simulations
show that this low-complexity algorithm can obtain near-optimal performance
under moderate signal-to-noise ratio and attains larger system load capacity
than parallel interference cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501049</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501049</id><created>2005-01-21</created><authors><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hisashi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Performance Evaluation of Impulse Radio UWB Systems with Pulse-Based
  Polarity Randomization</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2005.849197</doi><abstract>  In this paper, the performance of a binary phase shift keyed random
time-hopping impulse radio system with pulse-based polarity randomization is
analyzed. Transmission over frequency-selective channels is considered and the
effects of inter-frame interference and multiple access interference on the
performance of a generic Rake receiver are investigated for both synchronous
and asynchronous systems. Closed form (approximate) expressions for the
probability of error that are valid for various Rake combining schemes are
derived. The asynchronous system is modelled as a chip-synchronous system with
uniformly distributed timing jitter for the transmitted pulses of interfering
users. This model allows the analytical technique developed for the synchronous
case to be extended to the asynchronous case. An approximate closed-form
expression for the probability of bit error, expressed in terms of the
autocorrelation function of the transmitted pulse, is derived for the
asynchronous case. Then, transmission over an additive white Gaussian noise
channel is studied as a special case, and the effects of multiple-access
interference is investigated for both synchronous and asynchronous systems. The
analysis shows that the chip-synchronous assumption can result in
over-estimating the error probability, and the degree of over-estimation mainly
depends on the autocorrelation function of the ultra-wideband pulse and the
signal-to-interference-plus-noise-ratio of the system. Simulations studies
support the approximate analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501050</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501050</id><created>2005-01-21</created><authors><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author><author><keyname>Xiao</keyname><forenames>Jinjun</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy-Efficient Joint Estimation in Sensor Networks: Analog vs. Digital</title><categories>cs.IT math.IT</categories><comments>To appear in Proceedings of the 2005 IEEE International Conference on
  Acoustics, Speech and Signal Processing, Philadelphia, PA, March 19 - 23,
  2005</comments><abstract>  Sensor networks in which energy is a limited resource so that energy
consumption must be minimized for the intended application are considered. In
this context, an energy-efficient method for the joint estimation of an unknown
analog source under a given distortion constraint is proposed. The approach is
purely analog, in which each sensor simply amplifies and forwards the
noise-corrupted analog bservation to the fusion center for joint estimation.
The total transmission power across all the sensor nodes is minimized while
satisfying a distortion requirement on the joint estimate. The energy
efficiency of this analog approach is compared with previously proposed digital
approaches with and without coding. It is shown in our simulation that the
analog approach is more energy-efficient than the digital system without
coding, and in some cases outperforms the digital system with optimal coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501051</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501051</id><created>2005-01-21</created><authors><author><keyname>Jayaweera</keyname><forenames>Sudharman K.</forenames><affiliation>Wichita State University</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Princeton University</affiliation></author></authors><title>On the Capacity of Multiple Antenna Systems in Rician Fading</title><categories>cs.IT math.IT</categories><comments>22 pages, 9 figures, to be published in IEEE Transaction in Wireless
  Communications</comments><abstract>  The effect of Rician-ness on the capacity of multiple antenna systems is
investigated under the assumption that channel state information (CSI) is
available only at the receiver. The average-power-constrained capacity of such
systems is considered under two different assumptions on the knowledge about
the fading available at the transmitter: the case in which the transmitter has
no knowledge of fading at all, and the case in which the transmitter has
knowledge of the distribution of the fading process but not the instantaneous
CSI. The exact capacity is given for the former case while capacity bounds are
derived for the latter case. A new signalling scheme is also proposed for the
latter case and it is shown that by exploiting the knowledge of Rician-ness at
the transmitter via this signalling scheme, significant capacity gain can be
achieved. The derived capacity bounds are evaluated explicitly to provide
numerical results in some representative situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501052</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501052</id><created>2005-01-21</created><updated>2005-01-22</updated><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Stochastic Differential Games in a Non-Markovian Setting</title><categories>cs.IT cs.CE math.IT</categories><comments>To appear in the SIAM Journal on Control and Optimization</comments><abstract>  Stochastic differential games are considered in a non-Markovian setting.
Typically, in stochastic differential games the modulating process of the
diffusion equation describing the state flow is taken to be Markovian. Then
Nash equilibria or other types of solution such as Pareto equilibria are
constructed using Hamilton-Jacobi-Bellman (HJB) equations. But in a
non-Markovian setting the HJB method is not applicable. To examine the
non-Markovian case, this paper considers the situation in which the modulating
process is a fractional Brownian motion. Fractional noise calculus is used for
such models to find the Nash equilibria explicitly. Although fractional
Brownian motion is taken as the modulating process because of its versatility
in modeling in the fields of finance and networks, the approach in this paper
has the merit of being applicable to more general Gaussian stochastic
differential games with only slight conceptual modifications. This work has
applications in finance to stock price modeling which incorporates the effect
of institutional investors, and to stochastic differential portfolio games in
markets in which the stock prices follow diffusions modulated with fractional
Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501053</id><created>2005-01-21</created><updated>2005-02-03</updated><authors><author><keyname>Tropashko</keyname><forenames>Vadim</forenames></author></authors><title>Relational Algebra as non-Distributive Lattice</title><categories>cs.DB</categories><comments>9 pages</comments><abstract>  We reduce the set of classic relational algebra operators to two binary
operations: natural join and generalized union. We further demonstrate that
this set of operators is relationally complete and honors lattice axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501054</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501054</id><created>2005-01-22</created><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Arbitrage in Fractal Modulated Markets When the Volatility is Stochastic</title><categories>cs.IT cs.CE math.IT</categories><comments>To appear in the International Journal of Theoretical and Applied
  Finance</comments><abstract>  In this paper an arbitrage strategy is constructed for the modified
Black-Scholes model driven by fractional Brownian motion or by a time changed
fractional Brownian motion, when the volatility is stochastic. This latter
property allows the heavy tailedness of the log returns of the stock prices to
be also accounted for in addition to the long range dependence introduced by
the fractional Brownian motion. Work has been done previously on this problem
for the case with constant `volatility' and without a time change; here these
results are extended to the case of stochastic volatility models when the
modulator is fractional Brownian motion or a time change of it. (Volatility in
fractional Black-Scholes models does not carry the same meaning as in the
classic Black-Scholes framework, which is made clear in the text.)
  Since fractional Brownian motion is not a semi-martingale, the Black-Scholes
differential equation is not well-defined sense for arbitrary predictable
volatility processes. However, it is shown here that any almost surely
continuous and adapted process having zero quadratic variation can act as an
integrator over functions of the integrator and over the family of continuous
adapted semi-martingales. Moreover it is shown that the integral also has zero
quadratic variation, and therefore that the integral itself can be an
integrator. This property of the integral is crucial in developing the
arbitrage strategy. Since fractional Brownian motion and a time change of
fractional Brownian motion have zero quadratic variation, these results are
applicable to these cases in particular. The appropriateness of fractional
Brownian motion as a means of modeling stock price returns is discussed as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501055</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501055</id><created>2005-01-22</created><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Chen</keyname><forenames>Li</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Consistency Problems for Jump-Diffusion Models</title><categories>cs.IT cs.CE math.IT</categories><comments>To appear in Applied Mathematical Finance</comments><abstract>  In this paper consistency problems for multi-factor jump-diffusion models,
where the jump parts follow multivariate point processes are examined. First
the gap between jump-diffusion models and generalized Heath-Jarrow-Morton (HJM)
models is bridged. By applying the drift condition for a generalized
arbitrage-free HJM model, the consistency condition for jump-diffusion models
is derived. Then we consider a case in which the forward rate curve has a
separable structure, and obtain a specific version of the general consistency
condition. In particular, a necessary and sufficient condition for a
jump-diffusion model to be affine is provided. Finally the Nelson-Siegel type
of forward curve structures is discussed. It is demonstrated that under
regularity condition, there exists no jump-diffusion model consistent with the
Nelson-Siegel curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501056</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501056</id><created>2005-01-21</created><authors><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Large Deviations Approach to Sensor Scheduling for Detection of
  Correlated Random Fields</title><categories>cs.IT math.IT</categories><comments>4 pages with 6 figures, to appear in Proceedings of the 2005 IEEE
  International Conference on Acoustics, Speech and Signal Processing,
  Philadelphia, PA, March 19 - 23, 2005</comments><acm-class>E.4; H.1.1</acm-class><abstract>  The problem of scheduling sensor transmissions for the detection of
correlated random fields using spatially deployed sensors is considered. Using
the large deviations principle, a closed-form expression for the error exponent
of the miss probability is given as a function of the sensor spacing and
signal-to-noise ratio (SNR). It is shown that the error exponent has a distinct
characteristic: at high SNR, the error exponent is monotonically increasing
with respect to sensor spacing, while at low SNR there is an optimal spacing
for scheduled sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501057</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501057</id><created>2005-01-21</created><authors><author><keyname>Fujii</keyname><forenames>Jun Ichi</forenames></author><author><keyname>Nakamoto</keyname><forenames>Ritsuo</forenames></author><author><keyname>Yanagi</keyname><forenames>Kenjiro</forenames></author></authors><title>Concavity of the auxiliary function appearing in quantum reliability
  function an classical-quantum channels</title><categories>cs.IT math.IT</categories><comments>submitted in IEEE Trans. IT</comments><abstract>  Concavity of the auxiliary function which appears in the random coding
exponent as the lower bound of the quantum reliability function for general
quantum states is proven for s between 0 and 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501058</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501058</id><created>2005-01-21</created><authors><author><keyname>Fishler</keyname><forenames>Eran</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Estimation of the Number of Sources in Unbalanced Arrays via Information
  Theoretic Criteria</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2005.853099</doi><abstract>  Estimating the number of sources impinging on an array of sensors is a well
known and well investigated problem. A common approach for solving this problem
is to use an information theoretic criterion, such as Minimum Description
Length (MDL) or the Akaike Information Criterion (AIC). The MDL estimator is
known to be a consistent estimator, robust against deviations from the Gaussian
assumption, and non-robust against deviations from the point source and/or
temporally or spatially white additive noise assumptions. Over the years
several alternative estimation algorithms have been proposed and tested.
Usually, these algorithms are shown, using computer simulations, to have
improved performance over the MDL estimator, and to be robust against
deviations from the assumed spatial model. Nevertheless, these robust
algorithms have high computational complexity, requiring several
multi-dimensional searches.
  In this paper, motivated by real life problems, a systematic approach toward
the problem of robust estimation of the number of sources using information
theoretic criteria is taken. An MDL type estimator that is robust against
deviation from assumption of equal noise level across the array is studied. The
consistency of this estimator, even when deviations from the equal noise level
assumption occur, is proven. A novel low-complexity implementation method
avoiding the need for multi-dimensional searches is presented as well, making
this estimator a favorable choice for practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501059</id><created>2005-01-22</created><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author></authors><title>Under-approximation of the Greatest Fixpoint in Real-Time System
  Verification</title><categories>cs.SE cs.LO</categories><abstract>  Techniques for the efficient successive under-approximation of the greatest
fixpoint in TCTL formulas can be useful in fast refutation of inevitability
properties and vacuity checking. We first give an integrated algorithmic
framework for both under and over-approximate model-checking. We design the
{\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation,
as a unified framework for the evaluation of formulas like
$\exists\pfrr\eta_1$, $\exists\pfrr\pevt\eta_1$, and $\exists\pevt\pfrr\eta_1$.
We then prove the correctness of a new formulation for the characterization of
the NZF predicate based on zone search and the least fixpoint evaluation. The
new formulation then leads to the design of an evaluation algorithm, with the
capability of successive under-approximation, for $\exists\pfrr\eta_1$,
$\exists\pfrr\pevt\eta_1$, and $\exists\pevt\pfrr\eta_1$. We then present
techniques to efficiently search for the zones and to speed up the
under-approximate evaluation of those three formulas. Our experiments show that
the techniques have significantly enhanced the verification performance against
several benchmarks over exact model-checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501060</id><created>2005-01-22</created><authors><author><keyname>Wang</keyname><forenames>Farn</forenames></author></authors><title>Under-approximation of the Greatest Fixpoints in Real-Time System
  Verification</title><categories>cs.SE cs.LO</categories><abstract>  Techniques for the efficient successive under-approximation of the greatest
fixpoint in TCTL formulas can be useful in fast refutation of inevitability
properties and vacuity checking. We first give an integrated algorithmic
framework for both under and over-approximate model-checking. We design the
{\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation,
as a unified framework for the evaluation of formulas like
$\exists\pfrr\eta_1$, $\exists\pfrr\pevt\eta_1$, and $\exists\pevt\pfrr\eta_1$.
We then prove the correctness of a new formulation for the characterization of
the NZF predicate based on zone search and the least fixpoint evaluation. The
new formulation then leads to the design of an evaluation algorithm, with the
capability of successive under-approximation, for $\exists\pfrr\eta_1$,
$\exists\pfrr\pevt\eta_1$, and $\exists\pevt\pfrr\eta_1$. We then present
techniques to efficiently search for the zones and to speed up the
under-approximate evaluation of those three formulas. Our experiments show that
the techniques have significantly enhanced the verification performance against
several benchmarks over exact model-checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501061</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501061</id><created>2005-01-22</created><authors><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hisashi</forenames></author></authors><title>Optimal and Suboptimal Finger Selection Algorithms for MMSE Rake
  Receivers in Impulse Radio Ultra-Wideband Systems</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Wireless Communications and Networking Conference
  (WCNC 2005), New Orleans, LA, March 13-17, 2005</comments><abstract>  Convex relaxations of the optimal finger selection algorithm are proposed for
a minimum mean square error (MMSE) Rake receiver in an impulse radio
ultra-wideband system. First, the optimal finger selection problem is
formulated as an integer programming problem with a non-convex objective
function. Then, the objective function is approximated by a convex function and
the integer programming problem is solved by means of constraint relaxation
techniques. The proposed algorithms are suboptimal due to the approximate
objective function and the constraint relaxation steps. However, they can be
used in conjunction with the conventional finger selection algorithm, which is
suboptimal on its own since it ignores the correlation between multipath
components, to obtain performances reasonably close to that of the optimal
scheme that cannot be implemented in practice due to its complexity. The
proposed algorithms leverage convexity of the optimization problem
formulations, which is the watershed between `easy' and `difficult'
optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501062</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501062</id><created>2005-01-22</created><authors><author><keyname>Fishler</keyname><forenames>Eran</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On The Tradeoff Between Two Types of Processing Gain</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><abstract>  One of the features characterizing almost every multiple access (MA)
communication system is the processing gain. Through the use of spreading
sequences, the processing gain of Random CDMA systems (RCDMA), is devoted to
both bandwidth expansion and orthogonalization of the signals transmitted by
different users. Another type of multiple access system is Impulse Radio (IR).
In many aspects, IR systems are similar to time division multiple access (TDMA)
systems, and the processing gain of IR systems represents the ratio between the
actual transmission time and the total time between two consecutive
ransmissions (on-plus-off to on ratio). While CDMA systems, which constantly
excite the channel, rely on spreading sequences to orthogonalize the signals
transmitted by different users, IR systems transmit a series of short pulses
and the orthogonalization between the signals transmitted by different users is
achieved by the fact that most of the pulses do not collide with each other at
the receiver.
  In this paper, a general class of MA communication systems that use both
types of processing gain is presented, and both IR and RCDMA systems are
demonstrated to be two special cases of this more general class of systems. The
bit error rate (BER) of several receivers as a function of the ratio between
the two types of processing gain is analyzed and compared under the constraint
that the total processing gain of the system is large and fixed. It is
demonstrated that in non inter-symbol interference (ISI) channels there is no
tradeoff between the two types of processing gain. However, in ISI channels a
tradeoff between the two types of processing gain exists. In addition, the
sub-optimality of RCDMA systems in frequency selective channels is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501063</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501063</id><created>2005-01-22</created><authors><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Princeton University</affiliation></author></authors><title>Bandit Problems with Side Observations</title><categories>cs.IT cs.LG math.IT</categories><comments>16 pages, 3 figures. To be published in the IEEE Transactions on
  Automatic Control</comments><doi>10.1109/TAC.2005.844079</doi><abstract>  An extension of the traditional two-armed bandit problem is considered, in
which the decision maker has access to some side information before deciding
which arm to pull. At each time t, before making a selection, the decision
maker is able to observe a random variable X_t that provides some information
on the rewards to be obtained. The focus is on finding uniformly good rules
(that minimize the growth rate of the inferior sampling time) and on
quantifying how much the additional information helps. Various settings are
considered and for each setting, lower bounds on the achievable inferior
sampling time are developed and asymptotically optimal adaptive schemes
achieving these lower bounds are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501064</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501064</id><created>2005-01-22</created><authors><author><keyname>Meshkati</keyname><forenames>Farhad</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author><author><keyname>Schwartz</keyname><forenames>Stuart C.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>A Non-Cooperative Power Control Game for Multi-Carrier CDMA Systems</title><categories>cs.IT math.IT</categories><comments>To appear in Proceedings of the 2005 IEEE Wireless Communications and
  Networking Conference, New Orleans, LA, March 13 - 17, 2005</comments><abstract>  In this work, a non-cooperative power control game for multi-carrier CDMA
systems is proposed. In the proposed game, each user needs to decide how much
power to transmit over each carrier to maximize its overall utility. The
utility function considered here measures the number of reliable bits
transmitted per joule of energy consumed. It is shown that the user's utility
is maximized when the user transmits only on the carrier with the best
&quot;effective channel&quot;. The existence and uniqueness of Nash equilibrium for the
proposed game are investigated and the properties of equilibrium are studied.
Also, an iterative and distributed algorithm for reaching the equilibrium (if
it exists) is presented. It is shown that the proposed approach results in a
significant improvement in the total utility achieved at equilibrium compared
to the case in which each user maximizes its utility over each carrier
independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501065</id><created>2005-01-23</created><authors><author><keyname>Clue</keyname><forenames>Vladimir I</forenames></author></authors><title>Harmonic Analysis</title><categories>cs.NA cs.DM</categories><comments>This new twist in harmonic analysis was primary introduced in
  Milwaukee's conference http://www.eit-conference.info/papers.asp</comments><acm-class>G.1; G.4; I.1</acm-class><abstract>  This paper describes a method of calculating the transforms, currently
obtained via Fourier and reverse Fourier transforms. The method allows
calculating efficiently the transforms of a signal having an arbitrary
dimension of the digital representation by reducing the transform to a
vector-to-circulant matrix multiplying. There is a connection between harmonic
equations in rectangular and polar coordinate systems. The connection
established here and used to create a very robust iterative algorithm for a
conformal mapping calculation. There is also suggested a new ratio (and an
efficient way of computing it) of two oscillative signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501066</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501066</id><created>2005-01-24</created><authors><author><keyname>Gursoy</keyname><forenames>Mustafa Cenk</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>The Noncoherent Rician Fading Channel -- Part I : Structure of the
  Capacity-Achieving Input</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Wireless Communications</comments><abstract>  Transmission of information over a discrete-time memoryless Rician fading
channel is considered where neither the receiver nor the transmitter knows the
fading coefficients. First the structure of the capacity-achieving input
signals is investigated when the input is constrained to have limited
peakedness by imposing either a fourth moment or a peak constraint. When the
input is subject to second and fourth moment limitations, it is shown that the
capacity-achieving input amplitude distribution is discrete with a finite
number of mass points in the low-power regime. A similar discrete structure for
the optimal amplitude is proven over the entire SNR range when there is only a
peak power constraint. The Rician fading with phase-noise channel model, where
there is phase uncertainty in the specular component, is analyzed. For this
model it is shown that, with only an average power constraint, the
capacity-achieving input amplitude is discrete with a finite number of levels.
For the classical average power limited Rician fading channel, it is proven
that the optimal input amplitude distribution has bounded support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501067</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501067</id><created>2005-01-24</created><authors><author><keyname>Gursoy</keyname><forenames>Mustafa Cenk</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>The Noncoherent Rician Fading Channel -- Part II : Spectral Efficiency
  in the Low-Power Regime</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Wireless Communications</comments><abstract>  Transmission of information over a discrete-time memoryless Rician fading
channel is considered where neither the receiver nor the transmitter knows the
fading coefficients. The spectral-efficiency/bit-energy tradeoff in the
low-power regime is examined when the input has limited peakedness. It is shown
that if a fourth moment input constraint is imposed or the input
peak-to-average power ratio is limited, then in contrast to the behavior
observed in average power limited channels, the minimum bit energy is not
always achieved at zero spectral efficiency. The low-power performance is also
characterized when there is a fixed peak limit that does not vary with the
average power. A new signaling scheme that overlays phase-shift keying on
on-off keying is proposed and shown to be optimally efficient in the low-power
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501068</id><created>2005-01-24</created><authors><author><keyname>Aycard</keyname><forenames>Olivier</forenames><affiliation>GRAVIR - Imag, Orpailleur Loria</affiliation></author><author><keyname>Mari</keyname><forenames>Jean-Francois</forenames><affiliation>ORPAILLEUR Loria</affiliation></author><author><keyname>Washington</keyname><forenames>Richard</forenames></author></authors><title>Learning to automatically detect features for mobile robots using
  second-order Hidden Markov Models</title><categories>cs.AI</categories><comments>2004</comments><proxy>ccsd ccsd-00003940</proxy><abstract>  In this paper, we propose a new method based on Hidden Markov Models to
interpret temporal sequences of sensor data from mobile robots to automatically
detect features. Hidden Markov Models have been used for a long time in pattern
recognition, especially in speech recognition. Their main advantages over other
methods (such as neural networks) are their ability to model noisy temporal
signals of variable length. We show in this paper that this approach is well
suited for interpretation of temporal sequences of mobile-robot sensor data. We
present two distinct experiments and results: the first one in an indoor
environment where a mobile robot learns to detect features like open doors or
T-intersections, the second one in an outdoor environment where a different
mobile robot has to identify situations like climbing a hill or crossing a
rock.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501069</id><created>2005-01-24</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Supriya</forenames></author><author><keyname>El-Ansary</keyname><forenames>Sameh</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author><author><keyname>Haridi</keyname><forenames>Seif</forenames></author></authors><title>A Statistical Theory of Chord under Churn</title><categories>cs.NI cond-mat.stat-mech cs.DC</categories><comments>6 pages, In the 4th International Workshop on Peer-to- Peer Systems
  (IPTPS'05), Ithaca, New York, USA, 2005</comments><acm-class>I.6;G.3;E.1</acm-class><abstract>  Most earlier studies of Distributed Hash Tables (DHTs) under churn have
either depended on simulations as the primary investigation tool, or on
establishing bounds for DHTs to function. In this paper, we present a complete
analytical study of churn using a master-equation-based approach, used
traditionally in non-equilibrium statistical mechanics to describe steady-state
or transient phenomena. Simulations are used to verify all theoretical
predictions. We demonstrate the application of our methodology to the Chord
system. For any rate of churn and stabilization rates, and any system size, we
accurately predict the fraction of failed or incorrect successor and finger
pointers and show how we can use these quantities to predict the performance
and consistency of lookups under churn. We also discuss briefly how churn may
actually be of different 'types' and the implications this will have for the
functioning of DHTs in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501070</id><created>2005-01-24</created><authors><author><keyname>Lorenz</keyname><forenames>David H.</forenames></author><author><keyname>Skotiniotis</keyname><forenames>Therapon</forenames></author></authors><title>Extending Design by Contract for Aspect-Oriented Programming</title><categories>cs.SE cs.PL</categories><report-no>NU-CCIS-04-14</report-no><acm-class>D.2.2, D.2.3, D.2.4</acm-class><abstract>  Design by Contract (DbC) and runtime enforcement of program assertions
enables the construction of more robust software. It also enables the
assignment of blame in error reporting. Unfortunately, there is no support for
runtime contract enforcement and blame assignment for Aspect-Oriented
Programming (AOP). Extending DbC to also cover aspects brings forward a
plethora of issues related to the correct order of assertion validation. We
show that there is no generally correct execution sequence of object assertions
and aspect assertions. A further classification of aspects as agnostic,
obedient, or rebellious defines the order of assertion validation that needs to
be followed. We describe the application of this classification in a prototyped
DbC tool for AOP named Cona, where aspects are used for implementing contracts,
and contracts are used for enforcing assertions on aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501071</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501071</id><created>2005-01-24</created><authors><author><keyname>Comaniciu</keyname><forenames>C.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Capacity Regions and Optimal Power Allocation for Groupwise Multiuser
  Detection</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Wireless Communications</comments><acm-class>C.2.1</acm-class><abstract>  In this paper, optimal power allocation and capacity regions are derived for
GSIC (groupwise successive interference cancellation) systems operating in
multipath fading channels, under imperfect channel estimation conditions. It is
shown that the impact of channel estimation errors on the system capacity is
two-fold: it affects the receivers' performance within a group of users, as
well as the cancellation performance (through cancellation errors). An
iterative power allocation algorithm is derived, based on which it can be shown
that the total required received power is minimized when the groups are ordered
according to their cancellation errors, and the first detected group has the
smallest cancellation error.
  Performace/complexity tradeoff issues are also discussed by directly
comparing the system capacity for different implementations: GSIC with linear
minimum-mean-square error (LMMSE) receivers within the detection groups, GSIC
with matched filter receivers, multicode LMMSE systems, and simple all matched
filter receivers systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501072</id><created>2005-01-25</created><authors><author><keyname>Dutoit</keyname><forenames>Dominique</forenames><affiliation>LIPN</affiliation></author><author><keyname>Poibeau</keyname><forenames>Thierry</forenames><affiliation>LIPN</affiliation></author></authors><title>Inferring knowledge from a large semantic network</title><categories>cs.AI</categories><proxy>ccsd ccsd-00004061</proxy><journal-ref>Inferring knowledge from a large semantic network (2002) 232-238</journal-ref><abstract>  In this paper, we present a rich semantic network based on a differential
analysis. We then detail implemented measures that take into account common and
differential features between words. In a last section, we describe some
industrial applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501073</id><created>2005-01-25</created><authors><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>Fruehwirth</keyname><forenames>Thom</forenames></author></authors><title>Optimal Union-Find in Constraint Handling Rules</title><categories>cs.PL cs.CC cs.DS cs.PF</categories><comments>12 pages, 3 figures, to appear in Theory and Practice of Logic
  Programming (TPLP)</comments><abstract>  Constraint Handling Rules (CHR) is a committed-choice rule-based language
that was originally intended for writing constraint solvers. In this paper we
show that it is also possible to write the classic union-find algorithm and
variants in CHR. The programs neither compromise in declarativeness nor
efficiency. We study the time complexity of our programs: they match the
almost-linear complexity of the best known imperative implementations. This
fact is illustrated with experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501074</id><created>2005-01-25</created><updated>2005-02-08</updated><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LMC - IMAG</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>LMC - IMAG</affiliation></author><author><keyname>Wan</keyname><forenames>Zhendong</forenames><affiliation>CIS</affiliation></author></authors><title>Efficient Computation of the Characteristic Polynomial</title><categories>cs.SC</categories><proxy>ccsd ccsd-00004056</proxy><acm-class>F.2.4; B.2.4; G.4</acm-class><abstract>  This article deals with the computation of the characteristic polynomial of
dense matrices over small finite fields and over the integers. We first present
two algorithms for the finite fields: one is based on Krylov iterates and
Gaussian elimination. We compare it to an improvement of the second algorithm
of Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's third
algorithm could improve both complexity and computational time. We use these
results as a basis for the computation of the characteristic polynomial of
integer matrices. We first use early termination and Chinese remaindering for
dense matrices. Then a probabilistic approach, based on integer minimal
polynomial and Hensel factorization, is particularly well suited to sparse
and/or structured matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501075</id><created>2005-01-25</created><updated>2005-05-09</updated><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Simple extractors via constructions of cryptographic pseudo-random
  generators</title><categories>cs.CC cs.CR</categories><comments>21 pages, an extended abstract will appear in Proc. ICALP 2005; small
  corrections, some comments and references added</comments><abstract>  Trevisan has shown that constructions of pseudo-random generators from hard
functions (the Nisan-Wigderson approach) also produce extractors. We show that
constructions of pseudo-random generators from one-way permutations (the
Blum-Micali-Yao approach) can be used for building extractors as well. Using
this new technique we build extractors that do not use designs and
polynomial-based error-correcting codes and that are very simple and efficient.
For example, one extractor produces each output bit separately in $O(\log^2 n)$
time. These extractors work for weak sources with min entropy $\lambda n$, for
arbitrary constant $\lambda &gt; 0$, have seed length $O(\log^2 n)$, and their
output length is $\approx n^{\lambda/3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501076</id><created>2005-01-26</created><authors><author><keyname>Mulmuley</keyname><forenames>Ketan D.</forenames></author><author><keyname>Sohoni</keyname><forenames>Milind</forenames></author></authors><title>Geometric Complexity III: on deciding positivity of
  Littlewood-Richardson coefficients</title><categories>cs.CC math.RT</categories><comments>10 pages</comments><acm-class>F1.3</acm-class><abstract>  We point out that the remarkable Knutson and Tao Saturation Theorem and
polynomial time algorithms for LP have together an important and immediate
consequence in Geometric Complexity Theory. The problem of deciding positivity
of Littlewood-Richardson coefficients for GLn(C) belongs to P. Furthermore, the
algorithm is strongly polynomial.
  The main goal of this article is to explain the significance of this result
in the context of Geometric Complexity Theory. Furthermore, it is also
conjectured that an analogous result holds for arbitrary symmetrizable
Kac-Moody algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501077</id><created>2005-01-26</created><updated>2005-05-27</updated><authors><author><keyname>Smirnov</keyname><forenames>Alexander</forenames></author><author><keyname>Pashkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Chilov</keyname><forenames>Nikolai</forenames></author><author><keyname>Levashova</keyname><forenames>Tatiana</forenames></author><author><keyname>Krizhanovsky</keyname><forenames>Andrew</forenames></author><author><keyname>Kashevnik</keyname><forenames>Alexey</forenames></author></authors><title>Ontology-Based Users &amp; Requests Clustering in Customer Service
  Management System</title><categories>cs.IR cs.CL</categories><comments>15 pages, 4 figures, published in Lecture Notes in Computer Science</comments><acm-class>H.3.3</acm-class><journal-ref>Smirnov A., Pashkin M., Chilov N., Levashova T., Krizhanovsky A.,
  Kashevnik A. 2005. Ontology-Based Users and Requests Clustering in Customer
  Service Management System. Springer-Verlag GmbH, Lecture Notes in Computer
  Science, 3505: 231-246</journal-ref><abstract>  Customer Service Management is one of major business activities to better
serve company customers through the introduction of reliable processes and
procedures. Today this kind of activities is implemented through e-services to
directly involve customers into business processes. Traditionally Customer
Service Management involves application of data mining techniques to discover
usage patterns from the company knowledge memory. Hence grouping of
customers/requests to clusters is one of major technique to improve the level
of company customization. The goal of this paper is to present an efficient for
implementation approach for clustering users and their requests. The approach
uses ontology as knowledge representation model to improve the semantic
interoperability between units of the company and customers. Some fragments of
the approach tested in an industrial company are also presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501078</id><created>2005-01-26</created><authors><author><keyname>Zhou</keyname><forenames>Liang</forenames></author><author><keyname>Ticrea</keyname><forenames>Miruna</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>Multi-document Biography Summarization</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of EMNLP, pp. 434-441, 2004</journal-ref><abstract>  In this paper we describe a biography summarization system using sentence
classification and ideas from information retrieval. Although the individual
techniques are not new, assembling and applying them to generate multi-document
biographies is new. Our system was evaluated in DUC2004. It is among the top
performers in task 5-short summaries focused by person questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501079</id><created>2005-01-27</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author></authors><title>Data Mining for Actionable Knowledge: A Survey</title><categories>cs.DB cs.AI</categories><comments>11 pages</comments><report-no>Tr-05-01</report-no><abstract>  The data mining process consists of a series of steps ranging from data
cleaning, data selection and transformation, to pattern evaluation and
visualization. One of the central problems in data mining is to make the mined
patterns or knowledge actionable. Here, the term actionable refers to the mined
patterns suggest concrete and profitable actions to the decision-maker. That
is, the user can do something to bring direct benefits (increase in profits,
reduction in cost, improvement in efficiency, etc.) to the organization's
advantage. However, there has been written no comprehensive survey available on
this topic. The goal of this paper is to fill the void.
  In this paper, we first present two frameworks for mining actionable
knowledge that are inexplicitly adopted by existing research methods. Then we
try to situate some of the research on this topic from two different
viewpoints: 1) data mining tasks and 2) adopted framework. Finally, we specify
issues that are either not addressed or insufficiently studied yet and conclude
the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501080</id><created>2005-01-27</created><updated>2005-02-02</updated><authors><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author><author><keyname>Krafft</keyname><forenames>Dean B.</forenames></author><author><keyname>Jesuroga</keyname><forenames>Susan</forenames></author><author><keyname>Cornwell</keyname><forenames>Tim</forenames></author><author><keyname>Cramer</keyname><forenames>Ellen J.</forenames></author><author><keyname>Shin</keyname><forenames>Eddie</forenames></author></authors><title>An Information Network Overlay Architecture for the NSDL</title><categories>cs.DL</categories><acm-class>H.3.7</acm-class><abstract>  We describe the underlying data model and implementation of a new
architecture for the National Science Digital Library (NSDL) by the Core
Integration Team (CI). The architecture is based on the notion of an
information network overlay. This network, implemented as a graph of digital
objects in a Fedora repository, allows the representation of multiple
information entities and their relationships. The architecture provides the
framework for contextualization and reuse of resources, which we argue is
essential for the utility of the NSDL as a tool for teaching and learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501081</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501081</id><created>2005-01-27</created><authors><author><keyname>Kind</keyname><forenames>Adriel</forenames></author><author><keyname>Grant</keyname><forenames>Alex</forenames></author></authors><title>A Tree Search Method for Iterative Decoding of Underdetermined Multiuser
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Symposium on Information Theory</comments><abstract>  Application of the turbo principle to multiuser decoding results in an
exchange of probability distributions between two sets of constraints. Firstly,
constraints imposed by the multiple-access channel, and secondly, individual
constraints imposed by each users' error control code. A-posteriori probability
computation for the first set of constraints is prohibitively complex for all
but a small number of users. Several lower complexity approaches have been
proposed in the literature. One class of methods is based on linear filtering
(e.g. LMMSE). A more recent approach is to compute approximations to the
posterior probabilities by marginalising over a subset of sequences (list
detection). Most of the list detection methods are restricted to non-singular
systems. In this paper, we introduce a transformation that permits application
of standard tree-search methods to underdetermined systems. We find that the
resulting tree-search based receiver outperforms existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501082</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501082</id><created>2005-01-28</created><updated>2005-08-08</updated><authors><author><keyname>Jung</keyname><forenames>Peter</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author></authors><title>A Group-Theoretic Approach to the WSSUS Pulse Design Problem</title><categories>cs.IT math.IT</categories><comments>5 pages, final version for 2005 IEEE International Symposium on
  Information Theory; added references for section 2; corrected some typos;
  added more detailed discussion on the relations to quantum information
  theory; added some more references; added additional calculations as an
  appendix; corrected typo in III.A</comments><abstract>  We consider the pulse design problem in multicarrier transmission where the
pulse shapes are adapted to the second order statistics of the WSSUS channel.
Even though the problem has been addressed by many authors analytical insights
are rather limited. First we show that the problem is equivalent to the pure
state channel fidelity in quantum information theory. Next we present a new
approach where the original optimization functional is related to an eigenvalue
problem for a pseudo differential operator by utilizing unitary representations
of the Weyl--Heisenberg group.A local approximation of the operator for
underspread channels is derived which implicitly covers the concepts of pulse
scaling and optimal phase space displacement. The problem is reformulated as a
differential equation and the optimal pulses occur as eigenstates of the
harmonic oscillator Hamiltonian. Furthermore this operator--algebraic approach
is extended to provide exact solutions for different classes of scattering
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501083</id><created>2005-01-28</created><authors><author><keyname>Phipps</keyname><forenames>Jon</forenames></author><author><keyname>Hillmann</keyname><forenames>Diane I.</forenames></author><author><keyname>Paynter</keyname><forenames>Gordon</forenames></author></authors><title>Orchestrating Metadata Enhancement Services: Introducing Lenny</title><categories>cs.DL</categories><abstract>  Harvested metadata often suffers from uneven quality to the point that
utility is compromised. Although some aggregators have developed methods for
evaluating and repairing specific metadata problems, it has been unclear how
these methods might be scaled into services that can be used within an
automated production environment. The National Science Digital Library (NSDL),
as part of its work with INFOMINE, has developed a model of ser-vice
interaction that enables loosely-coupled third party services to provide
metadata enhancements to a central repository, with interactions orchestrated
by a centralized software application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501084</id><created>2005-01-28</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Polleres</keyname><forenames>Axel</forenames></author></authors><title>Towards Automated Integration of Guess and Check Programs in Answer Set
  Programming: A Meta-Interpreter and Applications</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><report-no>1843-04-01</report-no><acm-class>I.2.3; F.4.1</acm-class><abstract>  Answer set programming (ASP) with disjunction offers a powerful tool for
declaratively representing and solving hard problems. Many NP-complete problems
can be encoded in the answer set semantics of logic programs in a very concise
and intuitive way, where the encoding reflects the typical &quot;guess and check&quot;
nature of NP problems: The property is encoded in a way such that polynomial
size certificates for it correspond to stable models of a program. However, the
problem-solving capacity of full disjunctive logic programs (DLPs) is beyond
NP, and captures a class of problems at the second level of the polynomial
hierarchy. While these problems also have a clear &quot;guess and check&quot; structure,
finding an encoding in a DLP reflecting this structure may sometimes be a
non-obvious task, in particular if the &quot;check&quot; itself is a coNP-complete
problem; usually, such problems are solved by interleaving separate guess and
check programs, where the check is expressed by inconsistency of the check
program. In this paper, we present general transformations of head-cycle free
(extended) disjunctive logic programs into stratified and positive (extended)
disjunctive logic programs based on meta-interpretation techniques. The answer
sets of the original and the transformed program are in simple correspondence,
and, moreover, inconsistency of the original program is indicated by a
designated answer set of the transformed program. Our transformations
facilitate the integration of separate &quot;guess&quot; and &quot;check&quot; programs, which are
often easy to obtain, automatically into a single disjunctive logic program.
Our results complement recent results on meta-interpretation in ASP, and extend
methods and techniques for a declarative &quot;guess and check&quot; problem solving
paradigm through ASP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501085</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501085</id><created>2005-01-28</created><updated>2005-08-03</updated><authors><author><keyname>Henkel</keyname><forenames>Oliver</forenames></author></authors><title>Space Frequency Codes from Spherical Codes</title><categories>cs.IT math.IT</categories><comments>5 pages. Final version for the 2005 IEEE International Symposium on
  Information Theory</comments><abstract>  A new design method for high rate, fully diverse ('spherical') space
frequency codes for MIMO-OFDM systems is proposed, which works for arbitrary
numbers of antennas and subcarriers. The construction exploits a differential
geometric connection between spherical codes and space time codes. The former
are well studied e.g. in the context of optimal sequence design in CDMA
systems, while the latter serve as basic building blocks for space frequency
codes. In addition a decoding algorithm with moderate complexity is presented.
This is achieved by a lattice based construction of spherical codes, which
permits lattice decoding algorithms and thus offers a substantial reduction of
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501086</id><created>2005-01-31</created><authors><author><keyname>Kruse</keyname><forenames>Peter M.</forenames></author><author><keyname>Naujoks</keyname><forenames>Andre</forenames></author><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author></authors><title>Clever Search: A WordNet Based Wrapper for Internet Search Engines</title><categories>cs.AI</categories><acm-class>H 3.3, H 5.2</acm-class><journal-ref>Proceedings of 2nd GermaNet Workshop 2005</journal-ref><abstract>  This paper presents an approach to enhance search engines with information
about word senses available in WordNet. The approach exploits information about
the conceptual relations within the lexical-semantic net. In the wrapper for
search engines presented, WordNet information is used to specify user's request
or to classify the results of a publicly available web search engine, like
google, yahoo, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501087</id><created>2005-01-29</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Faults and Improvements of an Enhanced Remote User Authentication Scheme
  Using Smart Cards</title><categories>cs.CR</categories><comments>5 pages</comments><acm-class>C.3, D.4.6,H.2.0,K.6.5</acm-class><abstract>  In 2000, Hwang and Li proposed a remote user authentication scheme using
smart cards to solve the problems of Lamport scheme. Later, Chan- Chang, Shen-
Lin- Hwang and then Chang-Hwang pointed out some attacks on Hwang &amp;#8211;
Li&amp;#8217;s scheme. In 2003, Shen, Lin and Hwang also proposed a modified scheme
to remove these attacks. In the same year, Leung-Cheng-Fong-Chan showed that
modified scheme proposed by Shen-Lin-Hwang is still insecure. In 2004, Awasthi
and Lal enhanced Shen-Lin-Hwang&amp;#8217;s scheme to overcome its security
pitfalls. This paper analyses that the user U/smart card does not provide
complete information for the execution and proper running of the login phase of
the Awasthi- Lal&amp;#8217;s scheme. Furthermore, this paper also modifies the
Awasthi- Lal&amp;#8217;s scheme for the proper functioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501088</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501088</id><created>2005-01-30</created><authors><author><keyname>Shaydurov</keyname><forenames>Alexander</forenames></author></authors><title>Information estimations and analysis of structures</title><categories>cs.IT math.IT</categories><comments>PDF, 5 pages, 2 figures</comments><acm-class>E.4,H.1.1</acm-class><abstract>  In this paper have written the results of the information analysis of
structures. The obtained information estimation (IE) are based on an entropy
measure of C. Shannon. Obtained IE is univalent both for the non-isomorphic and
for the isomorphic graphs, algorithmically, it is asymptotically steady and has
vector character. IE can be used for the solution of the problems ranking of
structures by the preference, the evaluation of the structurization of subject
area, the solution of the problems of structural optimization. Information
estimations and method of the information analysis of structures it can be used
in many fields of knowledge (Electrical Systems and Circuit, Image recognition,
Computer technology, Databases and Bases of knowledge, Organic chemistry,
Biology and others) and it can be base for the structure calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501089</id><created>2005-01-31</created><authors><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author></authors><title>Issues in Exploiting GermaNet as a Resource in Real Applications</title><categories>cs.AI</categories><comments>10 pages, 3 figures</comments><acm-class>H3.1; I.2.7</acm-class><abstract>  This paper reports about experiments with GermaNet as a resource within
domain specific document analysis. The main question to be answered is: How is
the coverage of GermaNet in a specific domain? We report about results of a
field test of GermaNet for analyses of autopsy protocols and present a sketch
about the integration of GermaNet inside XDOC. Our remarks will contribute to a
GermaNet user's wish list.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501090</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501090</id><created>2005-01-30</created><authors><author><keyname>Winstead</keyname><forenames>Chris</forenames></author><author><keyname>Rapley</keyname><forenames>Anthony</forenames></author><author><keyname>Gaudet</keyname><forenames>Vincent C.</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author></authors><title>Stochastic Iterative Decoders</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, submitted to the 2005 International Symposium on
  Information Theory</comments><abstract>  This paper presents a stochastic algorithm for iterative error control
decoding. We show that the stochastic decoding algorithm is an approximation of
the sum-product algorithm. When the code's factor graph is a tree, as with
trellises, the algorithm approaches maximum a-posteriori decoding. We also
demonstrate a stochastic approximations to the alternative update rule known as
successive relaxation. Stochastic decoders have very simple digital
implementations which have almost no RAM requirements. We present example
stochastic decoders for a trellis-based Hamming code, and for a Block Turbo
code constructed from Hamming codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501091</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501091</id><created>2005-01-30</created><updated>2005-06-08</updated><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>A complexity-regularized quantization approach to nonlinear
  dimensionality reduction</title><categories>cs.IT math.IT</categories><comments>5 pages; final version to appear in Proc. ISIT 2005</comments><abstract>  We consider the problem of nonlinear dimensionality reduction: given a
training set of high-dimensional data whose ``intrinsic'' low dimension is
assumed known, find a feature extraction map to low-dimensional space, a
reconstruction map back to high-dimensional space, and a geometric description
of the dimension-reduced data as a smooth manifold. We introduce a
complexity-regularized quantization approach for fitting a Gaussian mixture
model to the training set via a Lloyd algorithm. Complexity regularization
controls the trade-off between adaptation to the local shape of the underlying
manifold and global geometric consistency. The resulting mixture model is used
to design the feature extraction and reconstruction maps and to define a
Riemannian metric on the low-dimensional data. We also sketch a proof of
consistency of our scheme for the purposes of estimating the unknown underlying
pdf of high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501092</identifier>
 <datestamp>2011-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501092</id><created>2005-01-30</created><authors><author><keyname>Earl</keyname><forenames>Matthew G.</forenames></author><author><keyname>D'Andrea</keyname><forenames>Raffaello</forenames></author></authors><title>Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming</title><categories>cs.RO cs.AI cs.MA</categories><comments>12 pages, 13 figures, submitted to IEEE Transactions on Robotics, for
  associated web page see http://control.mae.cornell.edu/earl/milp1</comments><acm-class>I.2.9; I.2.8; I.2.11</acm-class><journal-ref>M. G. Earl and R. D'Andrea, &quot;Multi-Vehicle Cooperative Control
  using Mixed Integer Linear Programming,&quot; In Cooperative Control of
  Distributed Multi-Agent Systems, J. S. Shamma ed., John Wiley &amp; Sons, 2007</journal-ref><abstract>  We present methods to synthesize cooperative strategies for multi-vehicle
control problems using mixed integer linear programming. Complex multi-vehicle
control problems are expressed as mixed logical dynamical systems. Optimal
strategies for these systems are then solved for using mixed integer linear
programming. We motivate the methods on problems derived from an adversarial
game between two teams of robots called RoboFlag. We assume the strategy for
one team is fixed and governed by state machines. The strategy for the other
team is generated using our methods. Finally, we perform an average case
computational complexity study on our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501093</id><created>2005-01-31</created><authors><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author></authors><title>Transforming Business Rules Into Natural Language Text</title><categories>cs.AI</categories><comments>3 pages</comments><acm-class>I.2.7</acm-class><journal-ref>in Proceedings of IWCS-6, 2005</journal-ref><abstract>  The aim of the project presented in this paper is to design a system for an
NLG architecture, which supports the documentation process of eBusiness models.
A major task is to enrich the formal description of an eBusiness model with
additional information needed in an NLG task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501094</id><created>2005-01-31</created><updated>2005-02-01</updated><authors><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author></authors><title>Corpus based Enrichment of GermaNet Verb Frames</title><categories>cs.AI</categories><comments>4 pages</comments><acm-class>I.2.7; I.2.6</acm-class><journal-ref>in Proceedings of LREC 2004</journal-ref><abstract>  Lexical semantic resources, like WordNet, are often used in real applications
of natural language document processing. For example, we integrated GermaNet in
our document suite XDOC of processing of German forensic autopsy protocols. In
addition to the hypernymy and synonymy relation, we want to adapt GermaNet's
verb frames for our analysis. In this paper we outline an approach for the
domain related enrichment of GermaNet verb frames by corpus based syntactic and
co-occurred data analyses of real documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501095</id><created>2005-01-31</created><authors><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author></authors><title>Context Related Derivation of Word Senses</title><categories>cs.AI</categories><comments>5 pages, 2 figures</comments><acm-class>I.2.7; I.2.6</acm-class><journal-ref>in Proceedings of Ontolex- Workshop 2004</journal-ref><abstract>  Real applications of natural language document processing are very often
confronted with domain specific lexical gaps during the analysis of documents
of a new domain. This paper describes an approach for the derivation of domain
specific concepts for the extension of an existing ontology. As resources we
need an initial ontology and a partially processed corpus of a domain. We
exploit the specific characteristic of the sublanguage in the corpus. Our
approach is based on syntactical structures (noun phrases) and compound
analyses to extract information required for the extension of GermaNet's
lexical resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0501096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0501096</id><created>2005-01-31</created><authors><author><keyname>Roesner</keyname><forenames>Dietmar</forenames></author><author><keyname>Kunze</keyname><forenames>Manuela</forenames></author><author><keyname>Kroetzsch</keyname><forenames>Sylke</forenames></author></authors><title>Transforming and Enriching Documents for the Semantic Web</title><categories>cs.AI</categories><comments>10 pages, 1 figure</comments><acm-class>H3.1; I.2.7</acm-class><journal-ref>KI (1), 2004</journal-ref><abstract>  We suggest to employ techniques from Natural Language Processing (NLP) and
Knowledge Representation (KR) to transform existing documents into documents
amenable for the Semantic Web. Semantic Web documents have at least part of
their semantics and pragmatics marked up explicitly in both a machine
processable as well as human readable manner. XML and its related standards
(XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and
methodologies developed for different application scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502001</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502001</id><created>2005-01-31</created><authors><author><keyname>Yang</keyname><forenames>Shengtian</forenames></author><author><keyname>Qiu</keyname><forenames>Peiliang</forenames></author></authors><title>Some Extensions of Gallager's Method to General Sources and Channels</title><categories>cs.IT math.IT</categories><comments>submitted to 2005 IEEE International Symposium on Information Theory</comments><abstract>  The Gallager bound is well known in the area of channel coding. However, most
discussions about it mainly focus on its applications to memoryless channels.
We show in this paper that the bounds obtained by Gallager's method are very
tight even for general sources and channels that are defined in the
information-spectrum theory. Our method is mainly based on the estimations of
error exponents in those bounds, and by these estimations we proved the direct
part of the Slepian-Wolf theorem and channel coding theorem for general sources
and channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502002</id><created>2005-02-01</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>Directed Threshold Multi &amp;#8211; Signature Scheme without SDC</title><categories>cs.CR</categories><comments>13 pages</comments><acm-class>K.6.M.,K.6.5,G.1.0,E.3,D.4.6</acm-class><abstract>  In this paper, we propose a Directed threshold multisignature scheme without
SDC. This signature scheme is applicable when the message is sensitive to the
signature receiver; and the signatures are generated by the cooperation of a
number of people from a given group of senders. In this scheme, any malicious
set of signers cannot impersonate any other set of signers to forge the
signatures. In case of forgery, it is possible to trace the signing set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502003</id><created>2005-02-01</created><authors><author><keyname>Kroeller</keyname><forenames>Alexander</forenames></author><author><keyname>Pfisterer</keyname><forenames>Dennis</forenames></author><author><keyname>Buschmann</keyname><forenames>Carsten</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan</forenames></author></authors><title>Shawn: A new approach to simulating wireless sensor networks</title><categories>cs.DC cs.PF</categories><comments>10 pages, 2 figures, 2 tables, Latex, to appear in Design, Analysis,
  and Simulation of Distributed Systems 2005</comments><acm-class>D.4.7, D.4.8</acm-class><abstract>  We consider the simulation of wireless sensor networks (WSN) using a new
approach. We present Shawn, an open-source discrete-event simulator that has
considerable differences to all other existing simulators. Shawn is very
powerful in simulating large scale networks with an abstract point of view. It
is, to the best of our knowledge, the first simulator to support generic
high-level algorithms as well as distributed protocols on exactly the same
underlying networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502004</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502004</id><created>2005-02-01</created><authors><author><keyname>Grunwald</keyname><forenames>Peter</forenames></author><author><keyname>de Rooij</keyname><forenames>Steven</forenames></author></authors><title>Asymptotic Log-loss of Prequential Maximum Likelihood Codes</title><categories>cs.LG cs.IT math.IT</categories><comments>22 pages, an abstract has been submitted to COLT 2005</comments><acm-class>E.4</acm-class><abstract>  We analyze the Dawid-Rissanen prequential maximum likelihood codes relative
to one-parameter exponential family models M. If data are i.i.d. according to
an (essentially) arbitrary P, then the redundancy grows at rate c/2 ln n. We
show that c=v1/v2, where v1 is the variance of P, and v2 is the variance of the
distribution m* in M that is closest to P in KL divergence. This shows that
prequential codes behave quite differently from other important universal codes
such as the 2-part MDL, Shtarkov and Bayes codes, for which c=1. This behavior
is undesirable in an MDL model selection setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502005</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502005</id><created>2005-02-01</created><updated>2005-06-22</updated><authors><author><keyname>Flum</keyname><forenames>Joerg</forenames></author><author><keyname>Grohe</keyname><forenames>Martin</forenames></author></authors><title>Model-Checking Problems as a Basis for Parameterized Intractability</title><categories>cs.CC cs.LO</categories><comments>Changes in since v2: Metadata updated</comments><acm-class>F.1.3; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (March 7,
  2005) lmcs:899</journal-ref><doi>10.2168/LMCS-1(1:2)2005</doi><abstract>  Most parameterized complexity classes are defined in terms of a parameterized
version of the Boolean satisfiability problem (the so-called weighted
satisfiability problem). For example, Downey and Fellow's W-hierarchy is of
this form. But there are also classes, for example, the A-hierarchy, that are
more naturally characterised in terms of model-checking problems for certain
fragments of first-order logic.
  Downey, Fellows, and Regan were the first to establish a connection between
the two formalisms by giving a characterisation of the W-hierarchy in terms of
first-order model-checking problems. We improve their result and then prove a
similar correspondence between weighted satisfiability and model-checking
problems for the A-hierarchy and the W^*-hierarchy. Thus we obtain very uniform
characterisations of many of the most important parameterized complexity
classes in both formalisms.
  Our results can be used to give new, simple proofs of some of the core
results of structural parameterized complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502006</id><created>2005-02-01</created><authors><author><keyname>Granitto</keyname><forenames>P. M.</forenames></author><author><keyname>Verdes</keyname><forenames>P. F.</forenames></author><author><keyname>Ceccatto</keyname><forenames>H. A.</forenames></author></authors><title>Neural network ensembles: Evaluation of aggregation algorithms</title><categories>cs.AI cs.NE</categories><comments>35 pages, 2 figures, In press AI Journal</comments><abstract>  Ensembles of artificial neural networks show improved generalization
capabilities that outperform those of single networks. However, for aggregation
to be effective, the individual networks must be as accurate and diverse as
possible. An important problem is, then, how to tune the aggregate members in
order to have an optimal compromise between these two conflicting conditions.
We present here an extensive evaluation of several algorithms for ensemble
construction, including new proposals and comparing them with standard methods
in the literature. We also discuss a potential problem with sequential
aggregation algorithms: the non-frequent but damaging selection through their
heuristics of particularly bad ensemble members. We introduce modified
algorithms that cope with this problem by allowing individual weighting of
aggregate members. Our algorithms and their weighted modifications are
favorably tested against other methods in the literature, producing a sensible
improvement in performance on most of the standard statistical databases used
as benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502007</id><created>2005-02-01</created><authors><author><keyname>Shaydurov</keyname><forenames>Alexander</forenames></author></authors><title>Identification of complex systems in the basis of wavelets</title><categories>cs.CE cs.NE</categories><comments>4 pages, 6 figures</comments><acm-class>J.2, J.6</acm-class><abstract>  In this paper is proposed the method of the identification of complex dynamic
systems. Method can be used for the identification of linear and nonlinear
complex dynamic systems for the determined or stochastic signals at the inputs
and the outputs. It is proposed to use a basis of wavelets for obtaining the
impulse transient function (ITF) of system. ITF is considered in the form of
surface in the 3D space. Are given the results of experiments on the
identification of systems in the basis of wavelets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502008</id><created>2005-02-01</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Liu</keyname><forenames>David T.</forenames></author><author><keyname>Nieto-Santisteban</keyname><forenames>Maria</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>DeWitt</keyname><forenames>David</forenames></author><author><keyname>Heber</keyname><forenames>Gerd</forenames></author></authors><title>Scientific Data Management in the Coming Decade</title><categories>cs.DB cs.CE</categories><report-no>Microsoft Technical Report MSR-TR-2005-10</report-no><abstract>  This is a thought piece on data-intensive science requirements for databases
and science centers. It argues that peta-scale datasets will be housed by
science centers that provide substantial storage and processing for scientists
who access the data via smart notebooks. Next-generation science instruments
and simulations will generate these peta-scale datasets. The need to publish
and share data and the need for generic analysis and visualization tools will
finally create a convergence on common metadata standards. Database systems
will be judged by their support of these metadata standards and by their
ability to manage and access peta-scale datasets. The procedural
stream-of-bytes-file-centric approach to data analysis is both too cumbersome
and too serial for such large datasets. Non-procedural query and analysis of
schematized self-describing data is both easier to use and allows much more
parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502009</id><created>2005-02-01</created><authors><author><keyname>Kukol</keyname><forenames>Peter</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>Performance Considerations for Gigabyte per Second Transcontinental
  Disk-to-Disk File Transfers</title><categories>cs.DB cs.PF</categories><report-no>Microsoft Technical Report MSR-TR-2004-62</report-no><abstract>  Moving data from CERN to Pasadena at a gigabyte per second using the next
generation Internet requires good networking and good disk IO. Ten Gbps
Ethernet and OC192 links are in place, so now it is simply a matter of
programming. This report describes our preliminary work and measurements in
configuring the disk subsystem for this effort. Using 24 SATA disks at each
endpoint we are able to locally read and write an NTFS volume is striped across
24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher
performance and higher-capacity systems deliver up to 3.5 GBps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502010</id><created>2005-02-01</created><authors><author><keyname>Barclay</keyname><forenames>Tom</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>TerraServer SAN-Cluster Architecture and Operations Experience</title><categories>cs.DC cs.DB</categories><report-no>Microsoft Technical Report MSR-TR-2004-67</report-no><abstract>  Microsoft TerraServer displays aerial, satellite, and to-pographic images of
the earth in a SQL database available via the Internet. It is one of the most
popular online at-lases, presenting seventeen terabytes of image data from the
United States Geological Survey (USGS). Initially de-ployed in 1998, the system
demonstrated the scalability of PC hardware and software - Windows and SQL
Server - on a single, mainframe-class processor. In September 2000, the
back-end database application was migrated to 4-node active/passive cluster
connected to an 18 terabyte Storage Area Network (SAN). The new configuration
was designed to achieve 99.99% availability for the back-end application. This
paper describes the hardware and software components of the TerraServer Cluster
and SAN, and describes our experience in configuring and operating this system
for three years. Not surprisingly, the hardware and architecture delivered
better than four-9's of availability, but operations mistakes delivered
three-9's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502011</id><created>2005-02-01</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author></authors><title>Where the Rubber Meets the Sky: Bridging the Gap between Databases and
  Science</title><categories>cs.DB cs.CE</categories><report-no>Microsoft Technical Report MSR-TR-2004-110</report-no><journal-ref>IEEE Data Engineering Bulletin, Vol 27.4, Dec. 2004, pp. 3-11</journal-ref><abstract>  Scientists in all domains face a data avalanche - both from better
instruments and from improved simulations. We believe that computer science
tools and computer scientists are in a position to help all the sciences by
building tools and developing techniques to manage, analyze, and visualize
peta-scale scientific information. This article is summarizes our experiences
over the last seven years trying to bridge the gap between database technology
and the needs of the astronomy community in building the World-Wide Telescope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502012</id><created>2005-02-01</created><authors><author><keyname>Kukol</keyname><forenames>Peter</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>Sequential File Programming Patterns and Performance with .NET</title><categories>cs.PF cs.OS</categories><report-no>Microsoft Technical Report MSR-TR-2004-136</report-no><abstract>  Programming patterns for sequential file access in the .NET Framework are
described and the performance is measured. The default behavior provides
excellent performance on a single disk - 50 MBps both reading and writing.
Using large request sizes and doing file pre-allocation when possible have
quantifiable benefits. When one considers disk arrays, .NET unbuffered IO
delivers 800 MBps on a 16-disk array, but buffered IO delivers about 12% of
that performance. Consequently, high-performance file and database utilities
are still forced to use unbuffered IO for maximum sequential performance. The
report is accompanied by downloadable source code that demonstrates the
concepts and code that was used to obtain these measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502013</id><created>2005-02-02</created><authors><author><keyname>Hearn</keyname><forenames>Robert A.</forenames></author></authors><title>Amazons is PSPACE-complete</title><categories>cs.CC cs.GT</categories><comments>7 pages, 5 figures. For 2005 Combinatorial Game Theory Workshop at
  BIRS</comments><acm-class>F.2.2</acm-class><abstract>  Amazons is a board game which combines elements of Chess and Go. It has
become popular in recent years, and has served as a useful platform for both
game-theoretic study and AI games research. Buro showed that simple Amazons
endgames are NP-equivalent, leaving the complexity of the general case as an
open problem.
  We settle this problem, by showing that deciding the outcome of an n x n
Amazons position is PSPACE-hard. We give a reduction from one of the
PSPACE-complete two-player formula games described by Schaefer. Since the
number of moves in an Amazons game is polynomially bounded (unlike Chess and
Go), Amazons is in PSPACE. It is thus on a par with other two-player,
bounded-move, perfect-information games such as Hex, Othello, and Kayles. Our
construction also provides an alternate proof that simple Amazons endgames are
NP-equivalent.
  Our reduction uses a number of amazons polynomial in the input formula
length; a remaining open problem is the complexity of Amazons when only a
constant number of amazons is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502014</id><created>2005-02-03</created><authors><author><keyname>Robert</keyname><forenames>Philippe</forenames><affiliation>RAP UR-R</affiliation></author></authors><title>On the asymptotic behavior of some Algorithms</title><categories>cs.DS math.CA math.PR</categories><comments>November 2004</comments><proxy>ccsd ccsd-00004119</proxy><journal-ref>Random Structures and Algorithms 27 (2005) 235--250</journal-ref><doi>10.1002/rsa.20075</doi><abstract>  A simple approach is presented to study the asymptotic behavior of some
algorithms with an underlying tree structure. It is shown that some asymptotic
oscillating behaviors can be precisely analyzed without resorting to complex
analysis techniques as it is usually done in this context. A new explicit
representation of periodic functions involved is obtained at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502015</id><created>2005-02-03</created><authors><author><keyname>Barrere</keyname><forenames>R.</forenames></author></authors><title>Can Computer Algebra be Liberated from its Algebraic Yoke ?</title><categories>cs.SC cs.CE</categories><comments>8 pages, 2-column presentation, 2 figures</comments><acm-class>G.4; I.1; I.6</acm-class><abstract>  So far, the scope of computer algebra has been needlessly restricted to exact
algebraic methods. Its possible extension to approximate analytical methods is
discussed. The entangled roles of functional analysis and symbolic programming,
especially the functional and transformational paradigms, are put forward. In
the future, algebraic algorithms could constitute the core of extended symbolic
manipulation systems including primitives for symbolic approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502016</id><created>2005-02-03</created><authors><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Stability Analysis for Regularized Least Squares Regression</title><categories>cs.LG</categories><comments>14 pages, 0 figures, 1 class file</comments><abstract>  We discuss stability for a class of learning algorithms with respect to noisy
labels. The algorithms we consider are for regression, and they involve the
minimization of regularized risk functionals, such as L(f) := 1/N sum_i
(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when
y_i is a noisy version of f*(x_i) for some function f* in H, the output of the
algorithm converges to f* as the regularization term and noise simultaneously
vanish. We consider two flavors of this problem, one where a data set of N
points remains fixed, and the other where N -&gt; infinity. For the case where N
-&gt; infinity, we give conditions for convergence to f_E (the function which is
the expectation of y(x) for each x), as lambda -&gt; 0. For the fixed N case, we
describe the limiting 'non-noisy', 'non-regularized' function f*, and give
conditions for convergence. In the process, we develop a set of tools for
dealing with functionals such as L(f), which are applicable to many other
problems in learning theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502017</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502017</id><created>2005-02-03</created><authors><author><keyname>Slonim</keyname><forenames>Noam</forenames></author><author><keyname>Atwal</keyname><forenames>Gurinder S.</forenames></author><author><keyname>Tkacik</keyname><forenames>Gasper</forenames></author><author><keyname>Bialek</keyname><forenames>William</forenames></author></authors><title>Estimating mutual information and multi--information in large networks</title><categories>cs.IT cs.AI cs.CV cs.LG math.IT</categories><abstract>  We address the practical problems of estimating the information relations
that characterize large networks. Building on methods developed for analysis of
the neural code, we show that reliable estimates of mutual information can be
obtained with manageable computational effort. The same methods allow
estimation of higher order, multi--information terms. These ideas are
illustrated by analyses of gene expression, financial markets, and consumer
preferences. In each case, information theoretic measures correlate with
independent, intuitive measures of the underlying structures in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502018</id><created>2005-02-03</created><authors><author><keyname>Nieto-Santisteban</keyname><forenames>Maria A.</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Thakar</keyname><forenames>Aniruddha R.</forenames></author><author><keyname>O'Mullane</keyname><forenames>William J.</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Annis</keyname><forenames>James</forenames></author></authors><title>When Database Systems Meet the Grid</title><categories>cs.DC</categories><comments>better version of the paper (better graphics exceed ArXiv 1MB limit)
  at http://www-db.cs.wisc.edu/cidr/papers/P13.pdf</comments><report-no>Microsoft Technical Report MSR-TR-2004-81</report-no><journal-ref>Proceedings of CIDR 2005 Conference, Asilomar, CA. Jan. 2005, pp
  154-161</journal-ref><abstract>  We illustrate the benefits of combining database systems and Grid
technologies for data-intensive applications. Using a cluster of SQL servers,
we reimplemented an existing Grid application that finds galaxy clusters in a
large astronomical database. The SQL implementation runs an order of magnitude
faster than the earlier Tcl-C-file-based implementation. We discuss why and how
Grid applications can take advantage of database systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502019</id><created>2005-02-03</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Lai</keyname><forenames>Kevin</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>A Price-Anticipating Resource Allocation Mechanism for Distributed
  Shared Clusters</title><categories>cs.DC cs.GT</categories><acm-class>C.2.4; D.4.1; D.4.7; K.6.0</acm-class><journal-ref>Proceedings of the ACM Conference on Electronic Commerce 2005</journal-ref><abstract>  In this paper we formulate the fixed budget resource allocation game to
understand the performance of a distributed market-based resource allocation
system. Multiple users decide how to distribute their budget (bids) among
multiple machines according to their individual preferences to maximize their
individual utility. We look at both the efficiency and the fairness of the
allocation at the equilibrium, where fairness is evaluated through the measures
of utility uniformity and envy-freeness. We show analytically and through
simulations that despite being highly decentralized, such a system converges
quickly to an equilibrium and unlike the social optimum that achieves high
efficiency but poor fairness, the proposed allocation scheme achieves a nice
balance of high degrees of efficiency and fairness at the equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502020</id><created>2005-02-03</created><authors><author><keyname>Sastry</keyname><forenames>K.</forenames></author><author><keyname>O'Reilly</keyname><forenames>U. -M.</forenames></author><author><keyname>Goldberg</keyname><forenames>D. E.</forenames></author></authors><title>Population Sizing for Genetic Programming Based Upon Decision Making</title><categories>cs.AI cs.NE</categories><comments>Final version published in O'Reilly, U.-M., et al. (2004). Genetic
  Programming Theory and Practice II. Boston, MA: Kluwer Academic Publishers.
  49--66</comments><report-no>IlliGAL Report No. 2004028</report-no><abstract>  This paper derives a population sizing relationship for genetic programming
(GP). Following the population-sizing derivation for genetic algorithms in
Goldberg, Deb, and Clark (1992), it considers building block decision making as
a key facet. The analysis yields a GP-unique relationship because it has to
account for bloat and for the fact that GP solutions often use subsolution
multiple times. The population-sizing relationship depends upon tree size,
solution complexity, problem difficulty and building block expression
probability. The relationship is used to analyze and empirically investigate
population sizing for three model GP problems named ORDER, ON-OFF and LOUD.
These problems exhibit bloat to differing extents and differ in whether their
solutions require the use of a building block multiple times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502021</id><created>2005-02-03</created><authors><author><keyname>Abbass</keyname><forenames>H. A.</forenames></author><author><keyname>Sastry</keyname><forenames>K.</forenames></author><author><keyname>Goldberg</keyname><forenames>D. E.</forenames></author></authors><title>Oiling the Wheels of Change: The Role of Adaptive Automatic Problem
  Decomposition in Non--Stationary Environments</title><categories>cs.NE cs.AI</categories><abstract>  Genetic algorithms (GAs) that solve hard problems quickly, reliably and
accurately are called competent GAs. When the fitness landscape of a problem
changes overtime, the problem is called non--stationary, dynamic or
time--variant problem. This paper investigates the use of competent GAs for
optimizing non--stationary optimization problems. More specifically, we use an
information theoretic approach based on the minimum description length
principle to adaptively identify regularities and substructures that can be
exploited to respond quickly to changes in the environment. We also develop a
special type of problems with bounded difficulties to test non--stationary
optimization problems. The results provide new insights into non-stationary
optimization problems and show that a search algorithm which automatically
identifies and exploits possible decompositions is more robust and responds
quickly to changes than a simple genetic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502022</id><created>2005-02-03</created><authors><author><keyname>Sastry</keyname><forenames>K.</forenames></author><author><keyname>Abbass</keyname><forenames>H. A.</forenames></author><author><keyname>Goldberg</keyname><forenames>D. E.</forenames></author></authors><title>Sub-Structural Niching in Non-Stationary Environments</title><categories>cs.NE cs.AI</categories><comments>Final version published in 2005 Australian Artificial Intelligence
  Conference, pp. 873--885</comments><report-no>IlliGAL Report No. 2004035</report-no><abstract>  Niching enables a genetic algorithm (GA) to maintain diversity in a
population. It is particularly useful when the problem has multiple optima
where the aim is to find all or as many as possible of these optima. When the
fitness landscape of a problem changes overtime, the problem is called
non--stationary, dynamic or time--variant problem. In these problems, niching
can maintain useful solutions to respond quickly, reliably and accurately to a
change in the environment. In this paper, we present a niching method that
works on the problem substructures rather than the whole solution, therefore it
has less space complexity than previously known niching mechanisms. We show
that the method is responding accurately when environmental changes occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502023</id><created>2005-02-03</created><authors><author><keyname>Sastry</keyname><forenames>K.</forenames></author><author><keyname>Abbass</keyname><forenames>H. A.</forenames></author><author><keyname>Goldberg</keyname><forenames>D. E.</forenames></author><author><keyname>Johnson</keyname><forenames>D. D.</forenames></author></authors><title>Sub-structural Niching in Estimation of Distribution Algorithms</title><categories>cs.NE cs.AI</categories><report-no>IlliGAL Report No. 2005002</report-no><abstract>  We propose a sub-structural niching method that fully exploits the problem
decomposition capability of linkage-learning methods such as the estimation of
distribution algorithms and concentrate on maintaining diversity at the
sub-structural level. The proposed method consists of three key components: (1)
Problem decomposition and sub-structure identification, (2) sub-structure
fitness estimation, and (3) sub-structural niche preservation. The
sub-structural niching method is compared to restricted tournament selection
(RTS)--a niching method used in hierarchical Bayesian optimization
algorithm--with special emphasis on sustained preservation of multiple global
solutions of a class of boundedly-difficult, additively-separable multimodal
problems. The results show that sub-structural niching successfully maintains
multiple global optima over large number of generations and does so with
significantly less population than RTS. Additionally, the market share of each
of the niche is much closer to the expected level in sub-structural niching
when compared to RTS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502024</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502024</id><created>2005-02-04</created><authors><author><keyname>Horan</keyname><forenames>R.</forenames></author><author><keyname>Tjhai</keyname><forenames>C.</forenames></author><author><keyname>Tomlinson</keyname><forenames>M.</forenames></author><author><keyname>Ambroze</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M.</forenames></author></authors><title>Idempotents, Mattson-Solomon Polynomials and Binary LDPC codes</title><categories>cs.IT math.IT</categories><comments>9 pages, 3 figures</comments><acm-class>H.1.1</acm-class><abstract>  We show how to construct an algorithm to search for binary idempotents which
may be used to construct binary LDPC codes. The algorithm, which allows control
of the key properties of sparseness, code rate and minimum distance, is
constructed in the Mattson-Solomon domain. Some of the new codes, found by
using this technique, are displayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502025</id><created>2005-02-04</created><authors><author><keyname>Kim</keyname><forenames>Hahnsang</forenames></author><author><keyname>Turletti</keyname><forenames>Theirry</forenames></author><author><keyname>Bouali</keyname><forenames>Amar</forenames></author></authors><title>EPspectra: A Formal Toolkit for Developing DSP Software Applications</title><categories>cs.LO cs.SE</categories><comments>31 pages</comments><acm-class>D.2.4, F.3.1, F.4.0, F.4.1, F.4.2</acm-class><abstract>  The software approach to developing Digital Signal Processing (DSP)
applications brings some great features such as flexibility, re-usability of
resources and easy upgrading of applications. However, it requires long and
tedious tests and verification phases because of the increasing complexity of
the software. This implies the need of a software programming environment
capable of putting together DSP modules and providing facilities to debug,
verify and validate the code. The objective of the work is to provide such
facilities as simulation and verification for developing DSP software
applications. This led us to develop an extension toolkit, Epspectra, built
upon Pspectra, one of the first toolkits available to design basic software
radio applications on standard PC workstations. In this paper, we first present
Epspectra, an Esterel-based extension of Pspectra that makes the design and
implementation of portable DSP applications easier. It allows drastic reduction
of testing and verification time while requiring relatively little expertise in
formal verification methods. Second, we demonstrate the use of Epspectra,
taking as an example the radio interface part of a GSM base station. We also
present the verification procedures for the three safety properties of the
implementation programs which have complex control-paths. These have to obey
strict scheduling rules. In addition, Epspectra achieves the verification of
the targeted application since the same model is used for the executable code
generation and for the formal verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502026</identifier>
 <datestamp>2009-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502026</id><created>2005-02-04</created><updated>2009-01-14</updated><authors><author><keyname>Mitra</keyname><forenames>Arindam</forenames></author></authors><title>Quantum mechanics can provide unbiased result</title><categories>cs.CR</categories><comments>~2500 words. I chased the problem over many years. Now it is
  complete. This paper is dedicated to S. Wisner</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Getting an unbiased result is a remarkably long standing problem of
collective observation/measurement. It is pointed out that quantum coin tossing
can generate unbiased result defeating dishonesty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502027</id><created>2005-02-04</created><authors><author><keyname>Lai</keyname><forenames>Kevin</forenames></author></authors><title>Markets are Dead, Long Live Markets</title><categories>cs.OS</categories><comments>Fix rotation of figures</comments><acm-class>D.4.0; D.4.7; K.6.0</acm-class><abstract>  Researchers have long proposed using economic approaches to resource
allocation in computer systems. However, few of these proposals became
operational, let alone commercial. Questions persist about the economic
approach regarding its assumptions, value, applicability, and relevance to
system design. The goal of this paper is to answer these questions. We find
that market-based resource allocation is useful, and more importantly, that
mechanism design and system design should be integrated to produce systems that
are both economically and computationally efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502028</id><created>2005-02-04</created><authors><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author><author><keyname>Bekaert</keyname><forenames>Jeroen</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Balakireva</keyname><forenames>Luda</forenames></author><author><keyname>Schwander</keyname><forenames>Thorsten</forenames></author></authors><title>aDORe: a modular, standards-based Digital Object Repository</title><categories>cs.DL</categories><comments>Draft of submission to Computer Journal</comments><acm-class>H 3.7</acm-class><abstract>  This paper describes the aDORe repository architecture, designed and
implemented for ingesting, storing, and accessing a vast collection of Digital
Objects at the Research Library of the Los Alamos National Laboratory. The
aDORe architecture is highly modular and standards-based. In the architecture,
the MPEG-21 Digital Item Declaration Language is used as the XML-based format
to represent Digital Objects that can consist of multiple datastreams as Open
Archival Information System Archival Information Packages (OAIS AIPs).Through
an ingestion process, these OAIS AIPs are stored in a multitude of autonomous
repositories. A Repository Index keeps track of the creation and location of
all the autonomous repositories, whereas an Identifier Locator registers in
which autonomous repository a given Digital Object or OAIS AIP resides. A
front-end to the complete environment, the OAI-PMH Federator, is introduced for
requesting OAIS Dissemination Information Packages (OAIS DIPs). These OAIS DIPs
can be the stored OAIS AIPs themselves, or transformations thereof. This
front-end allows OAI-PMH harvesters to recurrently and selectively collect
batches of OAIS DIPs from aDORe, and hence to create multiple, parallel
services using the collected objects. Another front-end, the OpenURL Resolver,
is introduced for requesting OAIS Result Sets. An OAIS Result Set is a
dissemination of an individual Digital Object or of its constituent
datastreams. Both front-ends make use of an MPEG-21 Digital Item Processing
Engine to apply services to OAIS AIPs, Digital Objects, or constituent
datastreams that were specified in a dissemination request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502029</id><created>2005-02-07</created><authors><author><keyname>Ondas</keyname><forenames>Radovan</forenames></author><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author></authors><title>Scalability of Genetic Programming and Probabilistic Incremental Program
  Evolution</title><categories>cs.NE cs.AI</categories><comments>Submitted to GECCO-2005</comments><acm-class>I.2.8; I.2.6; G.1.6</acm-class><abstract>  This paper discusses scalability of standard genetic programming (GP) and the
probabilistic incremental program evolution (PIPE). To investigate the need for
both effective mixing and linkage learning, two test problems are considered:
ORDER problem, which is rather easy for any recombination-based GP, and TRAP or
the deceptive trap problem, which requires the algorithm to learn interactions
among subsets of terminals. The scalability results show that both GP and PIPE
scale up polynomially with problem size on the simple ORDER problem, but they
both scale up exponentially on the deceptive problem. This indicates that while
standard recombination is sufficient when no interactions need to be
considered, for some problems linkage learning is necessary. These results are
in agreement with the lessons learned in the domain of binary-string genetic
algorithms (GAs). Furthermore, the paper investigates the effects of
introducing utnnecessary and irrelevant primitives on the performance of GP and
PIPE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502030</identifier>
 <datestamp>2009-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502030</id><created>2005-02-05</created><updated>2009-08-25</updated><authors><author><keyname>G</keyname><forenames>Raju Renjit</forenames></author></authors><title>Fixed Type Theorems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This submission has been withdrawn at the request of the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502031</id><created>2005-02-05</created><updated>2005-04-24</updated><authors><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Logic Column 11: The Finite and the Infinite in Temporal Logic</title><categories>cs.LO</categories><comments>14 pages</comments><acm-class>F.4.1; F.3.1</acm-class><journal-ref>SIGACT News, 36(1), pp. 86-99, 2005</journal-ref><abstract>  This article examines the interpretation of the LTL temporal operators over
finite and infinite sequences. This is used as the basis for deriving a sound
and complete axiomatization for Caret, a recent temporal logic for reasoning
about programs with nested procedure calls and returns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502032</id><created>2005-02-05</created><authors><author><keyname>Mortensen</keyname><forenames>Christian Worm</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Patrascu</keyname><forenames>Mihai</forenames></author></authors><title>On Dynamic Range Reporting in One Dimension</title><categories>cs.DS</categories><comments>18 pages. Full version of a paper that will appear in STOC'05</comments><abstract>  We consider the problem of maintaining a dynamic set of integers and
answering queries of the form: report a point (equivalently, all points) in a
given interval. Range searching is a natural and fundamental variant of integer
search, and can be solved using predecessor search. However, for a RAM with
w-bit words, we show how to perform updates in O(lg w) time and answer queries
in O(lglg w) time. The update time is identical to the van Emde Boas structure,
but the query time is exponentially faster. Existing lower bounds show that
achieving our query time for predecessor search requires doubly-exponentially
slower updates. We present some arguments supporting the conjecture that our
solution is optimal.
  Our solution is based on a new and interesting recursion idea which is &quot;more
extreme&quot; that the van Emde Boas recursion. Whereas van Emde Boas uses a simple
recursion (repeated halving) on each path in a trie, we use a nontrivial, van
Emde Boas-like recursion on every such path. Despite this, our algorithm is
quite clean when seen from the right angle. To achieve linear space for our
data structure, we solve a problem which is of independent interest. We develop
the first scheme for dynamic perfect hashing requiring sublinear space. This
gives a dynamic Bloomier filter (an approximate storage scheme for sparse
vectors) which uses low space. We strengthen previous lower bounds to show that
these results are optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502033</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502033</id><created>2005-02-05</created><authors><author><keyname>Koetter</keyname><forenames>Ralf</forenames></author><author><keyname>Li</keyname><forenames>Wen-Ching W.</forenames></author><author><keyname>Vontobel</keyname><forenames>Pascal O.</forenames></author><author><keyname>Walker</keyname><forenames>Judy L.</forenames></author></authors><title>Pseudo-Codewords of Cycle Codes via Zeta Functions</title><categories>cs.IT math.IT</categories><comments>Presented at Information Theory Workshop (ITW), San Antonio, TX, 2004</comments><acm-class>E.4</acm-class><abstract>  Cycle codes are a special case of low-density parity-check (LDPC) codes and
as such can be decoded using an iterative message-passing decoding algorithm on
the associated Tanner graph. The existence of pseudo-codewords is known to
cause the decoding algorithm to fail in certain instances. In this paper, we
draw a connection between pseudo-codewords of cycle codes and the so-called
edge zeta function of the associated normal graph and show how the Newton
polyhedron of the zeta function equals the fundamental cone of the code, which
plays a crucial role in characterizing the performance of iterative decoding
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502034</id><created>2005-02-07</created><authors><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author></authors><title>Multiobjective hBOA, Clustering, and Scalability</title><categories>cs.NE cs.AI</categories><comments>Also IlliGAL Report No. 2005005 (http://www-illigal.ge.uiuc.edu/).
  Submitted to GECCO-2005</comments><report-no>IlliGAL Report No. 2005005</report-no><acm-class>I.2.8; I.2.6; G.1.6; I.5.3</acm-class><abstract>  This paper describes a scalable algorithm for solving multiobjective
decomposable problems by combining the hierarchical Bayesian optimization
algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and
clustering in the objective space. It is first argued that for good
scalability, clustering or some other form of niching in the objective space is
necessary and the size of each niche should be approximately equal.
Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and
clustering in the objective space. The algorithm mohBOA differs from the
multiobjective variants of BOA and hBOA proposed in the past by including
clustering in the objective space and allocating an approximately equally sized
portion of the population to each cluster. The algorithm mohBOA is shown to
scale up well on a number of problems on which standard multiobjective
evolutionary algorithms perform poorly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502035</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502035</id><created>2005-02-07</created><authors><author><keyname>Tjhai</keyname><forenames>C.</forenames></author><author><keyname>Tomlinson</keyname><forenames>M.</forenames></author><author><keyname>Horan</keyname><forenames>R.</forenames></author><author><keyname>Ambroze</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M.</forenames></author></authors><title>Near Maximum-Likelihood Performance of Some New Cyclic Codes Constructed
  in the Finite-Field Transform Domain</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures and 3 tables. Submitted to the ISCTA'05 conference</comments><abstract>  It is shown that some well-known and some new cyclic codes with orthogonal
parity-check equations can be constructed in the finite-field transform domain.
It is also shown that, for some binary linear cyclic codes, the performance of
the iterative decoder can be improved by substituting some of the dual code
codewords in the parity-check matrix with other dual code codewords formed from
linear combinations. This technique can bring the performance of a code closer
to its maximum-likelihood performance, which can be derived from the erroneous
decoded codeword whose euclidean distance with the respect to the received
block is smaller than that of the correct codeword. For (63,37), (93,47) and
(105,53) cyclic codes, the maximum-likelihood performance is realised with this
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502036</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502036</id><created>2005-02-07</created><authors><author><keyname>Papagiannis</keyname><forenames>E.</forenames></author><author><keyname>Tjhai</keyname><forenames>C.</forenames></author><author><keyname>Ahmed</keyname><forenames>M.</forenames></author><author><keyname>Ambroze</keyname><forenames>M.</forenames></author><author><keyname>Tomlinson</keyname><forenames>M.</forenames></author></authors><title>Improved Iterative Decoding for Perpendicular Magnetic Recording</title><categories>cs.IT math.IT</categories><comments>4 pages and 5 figures. Submitted to ISCTA'05 conference</comments><abstract>  An algorithm of improving the performance of iterative decoding on
perpendicular magnetic recording is presented. This algorithm follows on the
authors' previous works on the parallel and serial concatenated turbo codes and
low-density parity-check codes. The application of this algorithm with
signal-to-noise ratio mismatch technique shows promising results in the
presence of media noise. We also show that, compare to the standard iterative
decoding algorithm, an improvement of within one order of magnitude can be
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502037</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502037</id><created>2005-02-07</created><updated>2005-07-23</updated><authors><author><keyname>Tjhai</keyname><forenames>C.</forenames></author><author><keyname>Tomlinson</keyname><forenames>M.</forenames></author><author><keyname>Horan</keyname><forenames>R.</forenames></author><author><keyname>Ambroze</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M.</forenames></author></authors><title>GF(2^m) Low-Density Parity-Check Codes Derived from Cyclotomic Cosets</title><categories>cs.IT math.IT</categories><comments>Coding</comments><abstract>  Based on the ideas of cyclotomic cosets, idempotents and Mattson-Solomon
polynomials, we present a new method to construct GF(2^m), where m&gt;0 cyclic
low-density parity-check codes. The construction method produces the dual code
idempotent which is used to define the parity-check matrix of the low-density
parity-check code. An interesting feature of this construction method is the
ability to increment the code dimension by adding more idempotents and so
steadily decrease the sparseness of the parity-check matrix. We show that the
constructed codes can achieve performance very close to the
sphere-packing-bound constrained for binary transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502038</id><created>2005-02-07</created><authors><author><keyname>Nikolopoulos</keyname><forenames>Stavros D.</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Charis</forenames></author></authors><title>The Number of Spanning Trees in Kn-complements of Quasi-threshold Graphs</title><categories>cs.DM</categories><comments>13 pages, 2 figures</comments><acm-class>G.2.1; G.2.2</acm-class><journal-ref>Graphs and Combinatorics 20(3): 383-397, 2004</journal-ref><abstract>  In this paper we examine the classes of graphs whose $K_n$-complements are
trees and quasi-threshold graphs and derive formulas for their number of
spanning trees; for a subgraph $H$ of $K_n$, the $K_n$-complement of $H$ is the
graph $K_n-H$ which is obtained from $K_n$ by removing the edges of $H$. Our
proofs are based on the complement spanning-tree matrix theorem, which
expresses the number of spanning trees of a graph as a function of the
determinant of a matrix that can be easily constructed from the adjacency
relation of the graph. Our results generalize previous results and extend the
family of graphs of the form $K_n-H$ admitting formulas for the number of their
spanning trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502039</id><created>2005-02-07</created><authors><author><keyname>Lubachevsky</keyname><forenames>Boris D.</forenames></author></authors><title>Efficient Parallel Simulations of Asynchronous Cellular Arrays</title><categories>cs.DC cond-mat.mtrl-sci cs.PF</categories><comments>26 pages, 10 figures</comments><acm-class>D.1.3; D.4.1; D.4.8; I.6.8</acm-class><journal-ref>Complex Systems, vol.1, no.6, December 1987, pp.1099--1123, S.
  Wolfram, (ed.)</journal-ref><abstract>  A definition for a class of asynchronous cellular arrays is proposed. An
example of such asynchrony would be independent Poisson arrivals of cell
iterations. The Ising model in the continuous time formulation of Glauber falls
into this class. Also proposed are efficient parallel algorithms for simulating
these asynchronous cellular arrays. In the algorithms, one or several cells are
assigned to a processing element (PE), local times for different PEs can be
different. Although the standard serial algorithm by Metropolis, Rosenbluth,
Rosenbluth, Teller, and Teller can simulate such arrays, it is usually believed
to be without an efficient parallel counterpart. However, the proposed parallel
algorithms contradict this belief proving to be both efficient and able to
perform the same task as the standard algorithm. The results of experiments
with the new algorithms are encouraging: the speed-up is greater than 16 using
25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2**14
PEs on a SIMD computer. The algorithm by Bortz, Kalos, and Lebowitz can be
incorporated in the proposed parallel algorithms, further contributing to
speed-up. [In this paper I invented the update-cites-of-local-time-minima
parallel simulation scheme. Now the scheme is becoming popular. Many misprints
of the original 1987 Complex Systems publication are corrected here.-B.L.]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502040</id><created>2005-02-07</created><authors><author><keyname>Xie</keyname><forenames>Gaoyan</forenames></author><author><keyname>Dang</keyname><forenames>Zhe</forenames></author></authors><title>Testing Systems of Concurrent Black-boxes--an Automata-Theoretic and
  Decompositional Approach</title><categories>cs.SE</categories><abstract>  The global testing problem studied in this paper is to seek a definite answer
to whether a system of concurrent black-boxes has an observable behavior in a
given finite (but could be huge) set &quot;Bad&quot;. We introduce a novel approach to
solve the problem that does not require integration testing. Instead, in our
approach, the global testing problem is reduced to testing individual
black-boxes in the system one by one in some given order. Using an
automata-theoretic approach, test sequences for each individual black-box are
generated from the system's description as well as the test results of
black-boxes prior to this black-box in the given order. In contrast to the
conventional compositional/modular verification/testing approaches, our
approach is essentially decompositional.
  Also, our technique is complete, sound, and can be carried out automatically.
Our experiment results show that the total number of tests needed to solve the
global testing problem is substantially small even for an extremely large
&quot;Bad&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502041</id><created>2005-02-07</created><updated>2005-05-28</updated><authors><author><keyname>Patrascu</keyname><forenames>Mihai</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author></authors><title>Logarithmic Lower Bounds in the Cell-Probe Model</title><categories>cs.DS cs.CC</categories><comments>Second version contains significant changes to the presentation. 32
  pages, 1 figure. Journal version of two conference publications: &quot;Tight
  Bounds for the Partial-Sums Problem&quot; Proc. 15th ACM-SIAM Symposium on
  Discrete Algorithms (SODA'04), pp 20-29. &quot;Lower Bounds for Dynamic
  Connectivity&quot; Proc. 36th ACM Symposium on Theory of Computing (STOC'04), pp
  546-553</comments><abstract>  We develop a new technique for proving cell-probe lower bounds on dynamic
data structures. This technique enables us to prove an amortized randomized
Omega(lg n) lower bound per operation for several data structural problems on n
elements, including partial sums, dynamic connectivity among disjoint paths (or
a forest or a graph), and several other dynamic graph problems (by simple
reductions). Such a lower bound breaks a long-standing barrier of Omega(lg n /
lglg n) for any dynamic language membership problem. It also establishes the
optimality of several existing data structures, such as Sleator and Tarjan's
dynamic trees. We also prove the first Omega(log_B n) lower bound in the
external-memory model without assumptions on the data structure (such as the
comparison model). Our lower bounds also give a query-update trade-off curve
matched, e.g., by several data structures for dynamic connectivity in graphs.
We also prove matching upper and lower bounds for partial sums when
parameterized by the word size and the maximum additive change in an update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502042</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502042</id><created>2005-02-07</created><updated>2005-08-26</updated><authors><author><keyname>Peacock</keyname><forenames>Matthew J. M.</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Unified Large System Analysis of MMSE and Adaptive Least Squares
  Receivers for a class of Random Matrix Channels</title><categories>cs.IT math.IT</categories><abstract>  We present a unified large system analysis of linear receivers for a class of
random matrix channels. The technique unifies the analysis of both the
minimum-mean-squared-error (MMSE) receiver and the adaptive least-squares (ALS)
receiver, and also uses a common approach for both random i.i.d. and random
orthogonal precoding. We derive expressions for the asymptotic
signal-to-interference-plus-noise (SINR) of the MMSE receiver, and both the
transient and steady-state SINR of the ALS receiver, trained using either
i.i.d. data sequences or orthogonal training sequences. The results are in
terms of key system parameters, and allow for arbitrary distributions of the
power of each of the data streams and the eigenvalues of the channel
correlation matrix. In the case of the ALS receiver, we allow a diagonal
loading constant and an arbitrary data windowing function. For i.i.d. training
sequences and no diagonal loading, we give a fundamental relationship between
the transient/steady-state SINR of the ALS and the MMSE receivers. We
demonstrate that for a particular ratio of receive to transmit dimensions and
window shape, all channels which have the same MMSE SINR have an identical
transient ALS SINR response. We demonstrate several applications of the
results, including an optimization of information throughput with respect to
training sequence length in coded block transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502043</id><created>2005-02-08</created><authors><author><keyname>Danciger</keyname><forenames>Jeff</forenames></author><author><keyname>Devadoss</keyname><forenames>Satyan L.</forenames></author><author><keyname>Sheehy</keyname><forenames>Don</forenames></author></authors><title>Compatible Triangulations and Point Partitions by Series-Triangular
  Graphs</title><categories>cs.CG cs.DM</categories><comments>11 pages, 8 figures</comments><journal-ref>Computational Geometry: Theory and Applications, 34 (2006) 195-202</journal-ref><abstract>  We introduce series-triangular graph embeddings and show how to partition
point sets with them. This result is then used to improve the upper bound on
the number of Steiner points needed to obtain compatible triangulations of
point sets. The problem is generalized to finding compatible triangulations for
more than two point sets and we show that such triangulations can be
constructed with only a linear number of Steiner points added to each point
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502044</id><created>2005-02-08</created><authors><author><keyname>Buergisser</keyname><forenames>Peter</forenames></author><author><keyname>Lotz</keyname><forenames>Martin</forenames></author></authors><title>The complexity of computing the Hilbert polynomial of smooth
  equidimensional complex projective varieties</title><categories>cs.SC cs.CC</categories><acm-class>I.1; F.1</acm-class><abstract>  We continue the study of counting complexity begun in [Buergisser, Cucker 04]
and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the
complexity of computing the Hilbert polynomial of a homogeneous ideal. We show
that the problem of computing the Hilbert polynomial of a smooth
equidimensional complex projective variety can be reduced in polynomial time to
the problem of counting the number of complex common zeros of a finite set of
multivariate polynomials. Moreover, we prove that the more general problem of
computing the Hilbert polynomial of a homogeneous ideal is polynomial space
hard. This implies polynomial space lower bounds for both the problems of
computing the rank and the Euler characteristic of cohomology groups of
coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC
1999).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502045</id><created>2005-02-08</created><updated>2005-02-08</updated><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Adaptive grids as parametrized scale-free networks</title><categories>cs.NA math-ph math.AP math.MP physics.flu-dyn</categories><comments>10 pages, 3 figures</comments><acm-class>G.1.8</acm-class><abstract>  In this paper we present a possible model of adaptive grids for numerical
resolution of differential problems, using physical or geometrical properties,
as viscosity or velocity gradient of a moving fluid. The relation between the
values of grid step and these entities is based on the mathematical scheme
offered by the model of scale-free networks, due to Barabasi, so that the step
can be connected to the other variables by a constitutive relation. Some
examples and an application are discussed, showing that this approach can be
further developed for treatment of more complex situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502046</id><created>2005-02-09</created><authors><author><keyname>Barradas</keyname><forenames>Hector Ruiz</forenames><affiliation>LSR - IMAG</affiliation></author><author><keyname>Bert</keyname><forenames>Didier</forenames><affiliation>LSR - IMAG</affiliation></author></authors><title>Proof obligations for specification and refinement of liveness
  properties under weak fairness</title><categories>cs.LO</categories><proxy>ccsd ccsd-00004181</proxy><report-no>Rapport LSR IMAG : RR 1071-I LSR 20</report-no><acm-class>ACM: F3.1</acm-class><abstract>  In this report, we present a formal model of fair iteration of events for B
event systems. The model is used to justify proof obligations for basic
liveness properties and preservation under refinement of general liveness
properties. The model of fair iteration of events uses the dovetail operator,
an operator proposed by Broy and Nelson to model fair choice. The proofs are
mainly founded in fixpoint calculations of fair iteration of events and weakest
precondition calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502047</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502047</id><created>2005-02-09</created><updated>2006-03-08</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>The succinctness of first-order logic on linear orders</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (June 29,
  2005) lmcs:676</journal-ref><doi>10.2168/LMCS-1(1:6)2005</doi><abstract>  Succinctness is a natural measure for comparing the strength of different
logics. Intuitively, a logic L_1 is more succinct than another logic L_2 if all
properties that can be expressed in L_2 can be expressed in L_1 by formulas of
(approximately) the same size, but some properties can be expressed in L_1 by
(significantly) smaller formulas.
  We study the succinctness of logics on linear orders. Our first theorem is
concerned with the finite variable fragments of first-order logic. We prove
that:
  (i) Up to a polynomial factor, the 2- and the 3-variable fragments of
first-order logic on linear orders have the same succinctness. (ii) The
4-variable fragment is exponentially more succinct than the 3-variable
fragment. Our second main result compares the succinctness of first-order logic
on linear orders with that of monadic second-order logic. We prove that the
fragment of monadic second-order logic that has the same expressiveness as
first-order logic on linear orders is non-elementarily more succinct than
first-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502048</id><created>2005-02-09</created><authors><author><keyname>Nagarajan</keyname><forenames>Rajagopal</forenames></author><author><keyname>Papanikolaou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Bowen</keyname><forenames>Garry</forenames></author><author><keyname>Gay</keyname><forenames>Simon</forenames></author></authors><title>An Automated Analysis of the Security of Quantum Key Distribution</title><categories>cs.CR quant-ph</categories><acm-class>D.2.4; D.4.6; K.6.5</acm-class><abstract>  This paper discusses the use of computer-aided verification as a practical
means for analysing quantum information systems; specifically, the BB84
protocol for quantum key distribution is examined using this method. This
protocol has been shown to be unconditionally secure against all attacks in an
information-theoretic setting, but the relevant security proof requires a
thorough understanding of the formalism of quantum mechanics and is not easily
adaptable to practical scenarios. Our approach is based on probabilistic
model-checking; we have used the PRISM model-checker to show that, as the
number of qubits transmitted in BB84 is increased, the equivocation of the
eavesdropper with respect to the channel decreases exponentially. We have also
shown that the probability of detecting the presence of an eavesdropper
increases exponentially with the number of qubits. The results presented here
are a testament to the effectiveness of the model-checking approach for systems
where analytical solutions may not be possible or plausible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502049</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502049</id><created>2005-02-09</created><authors><author><keyname>Riera</keyname><forenames>Constanza</forenames></author><author><keyname>Parker</keyname><forenames>Matthew G.</forenames></author></authors><title>Generalised Bent Criteria for Boolean Functions (I)</title><categories>cs.IT math.IT</categories><comments>29 pages, submitted to IEEE Trans. Inform Theory</comments><abstract>  Generalisations of the bent property of a boolean function are presented, by
proposing spectral analysis with respect to a well-chosen set of local unitary
transforms. Quadratic boolean functions are related to simple graphs and it is
shown that the orbit generated by successive Local Complementations on a graph
can be found within the transform spectra under investigation. The flat spectra
of a quadratic boolean function are related to modified versions of its
associated adjacency matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502050</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502050</id><created>2005-02-09</created><authors><author><keyname>Riera</keyname><forenames>Constanza</forenames></author><author><keyname>Petrides</keyname><forenames>George</forenames></author><author><keyname>Parker</keyname><forenames>Matthew G.</forenames></author></authors><title>Generalised Bent Criteria for Boolean Functions (II)</title><categories>cs.IT math.IT</categories><comments>18 pages, submitted to IEEE Trans. Inform. Theory</comments><abstract>  In the first part of this paper [16], some results on how to compute the flat
spectra of Boolean constructions w.r.t. the transforms {I,H}^n, {H,N}^n and
{I,H,N}^n were presented, and the relevance of Local Complementation to the
quadratic case was indicated. In this second part, the results are applied to
develop recursive formulae for the numbers of flat spectra of some structural
quadratics. Observations are made as to the generalised Bent properties of
boolean functions of algebraic degree greater than two, and the number of flat
spectra w.r.t. {I,H,N}^n are computed for some of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502051</id><created>2005-02-09</created><authors><author><keyname>Abbas</keyname><forenames>Zaheer</forenames></author><author><keyname>Umer</keyname><forenames>Muhammad</forenames></author><author><keyname>Odeh</keyname><forenames>Mohammed</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Ahmad</keyname><forenames>Farooq</forenames></author></authors><title>A Semantic Grid-based E-Learning Framework (SELF)</title><categories>cs.DC</categories><comments>8 pages, 2 tables, 3 figures</comments><acm-class>H2.4;J.3</acm-class><abstract>  E-learning can be loosely defined as a wide set of applications and
processes, which uses available electronic media (and tools) to deliver
vocational education and training. With its increasing recognition as an
ubiquitous mode of instruction and interaction in the academic as well as
corporate world, the need for a scaleable and realistic model is becoming
important. In this paper we introduce SELF; a Semantic grid-based E-Learning
Framework. SELF aims to identify the key-enablers in a practical grid-based
E-learning environment and to minimize technological reworking by proposing a
well-defined interaction plan among currently available tools and technologies.
We define a dichotomy with E-learning specific application layers on top and
semantic grid-based support layers underneath. We also map the latest open and
freeware technologies with various components in SELF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502052</id><created>2005-02-09</created><authors><author><keyname>Mogilevsky</keyname><forenames>Dmitry</forenames></author></authors><title>Log Analysis Case Study Using LoGS</title><categories>cs.CR cs.IR</categories><abstract>  A very useful technique a network administrator can use to identify
problematic network behavior is careful analysis of logs of incoming and
outgoing network flows. The challenge one faces when attempting to undertake
this course of action, though, is that large networks tend to generate an
extremely large quantity of network traffic in a very short period of time,
resulting in very large traffic logs which must be analyzed post-generation
with an eye for contextual information which may reveal symptoms of problematic
traffic. A better technique is to perform real-time log analysis using a
real-time context-generating tool such as LoGS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502053</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502053</id><created>2005-02-09</created><authors><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Li</keyname><forenames>Ye Geoffrey</forenames></author><author><keyname>Nakache</keyname><forenames>Yves-Paul</forenames></author><author><keyname>Orlik</keyname><forenames>Philip</forenames></author><author><keyname>Miyake</keyname><forenames>Makoto</forenames></author><author><keyname>Wu</keyname><forenames>Yunnan</forenames></author><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Sheng</keyname><forenames>Harry</forenames></author><author><keyname>Kung</keyname><forenames>S. Y.</forenames></author><author><keyname>Kobayashi</keyname><forenames>H.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander</forenames></author><author><keyname>Zhang</keyname><forenames>Jinyun</forenames></author></authors><title>A low-cost time-hopping impulse radio system for high data rate
  transmission</title><categories>cs.IT math.IT</categories><comments>To appear in EURASIP Journal on Applied Signal Processing (Special
  Issue on UWB - State of the Art)</comments><doi>10.1155/ASP.2005.397</doi><abstract>  We present an efficient, low-cost implementation of time-hopping impulse
radio that fulfills the spectral mask mandated by the FCC and is suitable for
high-data-rate, short-range communications. Key features are: (i) all-baseband
implementation that obviates the need for passband components, (ii) symbol-rate
(not chip rate) sampling, A/D conversion, and digital signal processing, (iii)
fast acquisition due to novel search algorithms, (iv) spectral shaping that can
be adapted to accommodate different spectrum regulations and interference
environments. Computer simulations show that this system can provide 110Mbit/s
at 7-10m distance, as well as higher data rates at shorter distances under FCC
emissions limits. Due to the spreading concept of time-hopping impulse radio,
the system can sustain multiple simultaneous users, and can suppress narrowband
interference effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502054</id><created>2005-02-10</created><authors><author><keyname>Mandoiu</keyname><forenames>Ion I.</forenames></author><author><keyname>Prajescu</keyname><forenames>Claudia</forenames></author><author><keyname>Trinca</keyname><forenames>Dragos</forenames></author></authors><title>Improved Tag Set Design and Multiplexing Algorithms for Universal Arrays</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  In this paper we address two optimization problems arising in the design of
genomic assays based on universal tag arrays. First, we address the universal
array tag set design problem. For this problem, we extend previous formulations
to incorporate antitag-to-antitag hybridization constraints in addition to
constraints on antitag-to-tag hybridization specificity, establish a
constructive upper bound on the maximum number of tags satisfying the extended
constraints, and propose a simple greedy tag selection algorithm. Second, we
give methods for improving the multiplexing rate in large-scale genomic assays
by combining primer selection with tag assignment. Experimental results on
simulated data show that this integrated optimization leads to reductions of up
to 50% in the number of required arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502055</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502055</id><created>2005-02-10</created><authors><author><keyname>Boutros</keyname><forenames>Joseph</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>On quasi-cyclic interleavers for parallel turbo codes</title><categories>cs.IT math.IT</categories><comments>15 pages, 2 eps figures</comments><acm-class>E4</acm-class><journal-ref>IEEE Transactions on Information Theory, IT-52, No 4 (2006) pp.
  1732--1739.</journal-ref><abstract>  We present an interleaving scheme that yields quasi-cyclic turbo codes. We
prove that randomly chosen members of this family yield with probability almost
1 turbo codes with asymptotically optimum minimum distance, i.e. growing as a
logarithm of the interleaver size. These interleavers are also very practical
in terms of memory requirements and their decoding error probabilities for
small block lengths compare favorably with previous interleaving schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502056</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502056</id><created>2005-02-11</created><updated>2005-05-31</updated><authors><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>Co-Authorship Networks in the Digital Library Research Community</title><categories>cs.DL</categories><comments>28 pages, last copyedit before Elsevier IPM print</comments><abstract>  The field of digital libraries (DLs) coalesced in 1994: the first digital
library conferences were held that year, awareness of the World Wide Web was
accelerating, and the National Science Foundation awarded $24 Million (U.S.)
for the Digital Library Initiative (DLI). In this paper we examine the state of
the DL domain after a decade of activity by applying social network analysis to
the co-authorship network of the past ACM, IEEE, and joint ACM/IEEE digital
library conferences. We base our analysis on a common binary undirectional
network model to represent the co-authorship network, and from it we extract
several established network measures. We also introduce a weighted directional
network model to represent the co-authorship network, for which we define
$AuthorRank$ as an indicator of the impact of an individual author in the
network. The results are validated against conference program committee members
in the same period. The results show clear advantages of PageRank and
AuthorRank over degree, closeness and betweenness centrality metrics. We also
investigate the amount and nature of international participation in Joint
Conference on Digital Libraries (JCDL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502057</id><created>2005-02-12</created><authors><author><keyname>Sastry</keyname><forenames>Kumara</forenames></author><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Goldberg</keyname><forenames>David E.</forenames></author></authors><title>Decomposable Problems, Niching, and Scalability of Multiobjective
  Estimation of Distribution Algorithms</title><categories>cs.NE cs.AI</categories><comments>Submitted to Genetic and Evolutionary Computation Conference,
  GECCO-2005</comments><report-no>IlliGAL Report No. 2005004</report-no><abstract>  The paper analyzes the scalability of multiobjective estimation of
distribution algorithms (MOEDAs) on a class of boundedly-difficult
additively-separable multiobjective optimization problems. The paper
illustrates that even if the linkage is correctly identified, massive
multimodality of the search problems can easily overwhelm the nicher and lead
to exponential scale-up. Facetwise models are subsequently used to propose a
growth rate of the number of differing substructures between the two objectives
to avoid the niching method from being overwhelmed and lead to polynomial
scalability of MOEDAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502058</id><created>2005-02-13</created><updated>2005-03-16</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Homan</keyname><forenames>Christopher M.</forenames></author><author><keyname>Kosub</keyname><forenames>Sven</forenames></author><author><keyname>Wagner</keyname><forenames>Klaus W.</forenames></author></authors><title>The Complexity of Computing the Size of an Interval</title><categories>cs.CC cs.DM</categories><comments>This revision fixes a problem in the proof of Theorem 9.6</comments><report-no>URCS-TR-2005-856</report-no><acm-class>F.1.3; F.1.2</acm-class><abstract>  Given a p-order A over a universe of strings (i.e., a transitive, reflexive,
antisymmetric relation such that if (x, y) is an element of A then |x| is
polynomially bounded by |y|), an interval size function of A returns, for each
string x in the universe, the number of strings in the interval between strings
b(x) and t(x) (with respect to A), where b(x) and t(x) are functions that are
polynomial-time computable in the length of x.
  By choosing sets of interval size functions based on feasibility requirements
for their underlying p-orders, we obtain new characterizations of complexity
classes. We prove that the set of all interval size functions whose underlying
p-orders are polynomial-time decidable is exactly #P. We show that the interval
size functions for orders with polynomial-time adjacency checks are closely
related to the class FPSPACE(poly). Indeed, FPSPACE(poly) is exactly the class
of all nonnegative functions that are an interval size function minus a
polynomial-time computable function.
  We study two important functions in relation to interval size functions. The
function #DIV maps each natural number n to the number of nontrivial divisors
of n. We show that #DIV is an interval size function of a polynomial-time
decidable partial p-order with polynomial-time adjacency checks. The function
#MONSAT maps each monotone boolean formula F to the number of satisfying
assignments of F. We show that #MONSAT is an interval size function of a
polynomial-time decidable total p-order with polynomial-time adjacency checks.
  Finally, we explore the related notion of cluster computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502059</id><created>2005-02-13</created><authors><author><keyname>Shtrakov</keyname><forenames>Stanko</forenames></author><author><keyname>Stoilov</keyname><forenames>Anton</forenames></author></authors><title>New approach for Finite Difference Method for Thermal Analysis of
  Passive Solar Systems</title><categories>cs.NA</categories><comments>7 pages, 4 figures</comments><abstract>  Mathematical treatment of massive wall systems is a useful tool for
investigation of these solar applications. The objectives of this work are to
develop (and validate) a numerical solution model for predication the thermal
behaviour of passive solar systems with massive wall, to improve knowledge of
using indirect passive solar systems and assess its energy efficiency according
to climatic conditions in Bulgaria. The problem of passive solar systems with
massive walls is modelled by thermal and mass transfer equations. As a boundary
conditions for the mathematical problem are used equations, which describe
influence of weather data and constructive parameters of building on the
thermal performance of the passive system. The mathematical model is solved by
means of finite differences method and improved solution procedure. In article
are presented results of theoretical and experimental study for developing and
validating a numerical solution model for predication the thermal behaviour of
passive solar systems with massive wall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502060</id><created>2005-02-13</created><authors><author><keyname>Rennard</keyname><forenames>J. -Ph</forenames></author></authors><title>Perspectives for Strong Artificial Life</title><categories>cs.AI</categories><comments>19 pages, 5 figures</comments><acm-class>I.2.0</acm-class><journal-ref>Rennard, J.-Ph., (2004), Perspective for Strong Artificial Life in
  De Castro, L.N. &amp; von Zuben F.J. (Eds), Recent Developments in Biologically
  Inspired Computing, Hershey:IGP, 301-318</journal-ref><abstract>  This text introduces the twin deadlocks of strong artificial life.
Conceptualization of life is a deadlock both because of the existence of a
continuum between the inert and the living, and because we only know one
instance of life. Computationalism is a second deadlock since it remains a
matter of faith. Nevertheless, artificial life realizations quickly progress
and recent constructions embed an always growing set of the intuitive
properties of life. This growing gap between theory and realizations should
sooner or later crystallize in some kind of paradigm shift and then give clues
to break the twin deadlocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502061</id><created>2005-02-14</created><authors><author><keyname>Bar</keyname><forenames>Sagy</forenames></author><author><keyname>Gonen</keyname><forenames>Mira</forenames></author><author><keyname>Wool</keyname><forenames>Avishai</forenames></author></authors><title>A Geographic Directed Preferential Internet Topology Model</title><categories>cs.NI cs.AR</categories><comments>21 pages, 4 figures</comments><abstract>  The goal of this work is to model the peering arrangements between Autonomous
Systems (ASes). Most existing models of the AS-graph assume an undirected
graph. However, peering arrangements are mostly asymmetric Customer-Provider
arrangements, which are better modeled as directed edges. Furthermore, it is
well known that the AS-graph, and in particular its clustering structure, is
influenced by geography.
  We introduce a new model that describes the AS-graph as a directed graph,
with an edge going from the customer to the provider, but also models symmetric
peer-to-peer arrangements, and takes geography into account. We are able to
mathematically analyze its power-law exponent and number of leaves. Beyond the
analysis we have implemented our model as a synthetic network generator we call
GdTang. Experimentation with GdTang shows that the networks it produces are
more realistic than those generated by other network generators, in terms of
its power-law exponent, fractions of customer-provider and symmetric peering
arrangements, and the size of its dense core. We believe that our model is the
first to manifest realistic regional dense cores that have a clear geographic
flavor. Our synthetic networks also exhibit path inflation effects that are
similar to those observed in the real AS graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502062</id><created>2005-02-14</created><authors><author><keyname>Volkmer</keyname><forenames>Markus</forenames></author><author><keyname>Wallner</keyname><forenames>Sebastian</forenames></author></authors><title>Tree Parity Machine Rekeying Architectures</title><categories>cs.CR cs.AR</categories><acm-class>K.4.4.f; K.6.5.a; B.7.1.b; C.3.h; J.9.d</acm-class><journal-ref>IEEE Transactions on Computers Vol. 54 No. 4, pp. 421-427, April
  2005</journal-ref><abstract>  The necessity to secure the communication between hardware components in
embedded systems becomes increasingly important with regard to the secrecy of
data and particularly its commercial use. We suggest a low-cost (i.e. small
logic-area) solution for flexible security levels and short key lifetimes. The
basis is an approach for symmetric key exchange using the synchronisation of
Tree Parity Machines. Fast successive key generation enables a key exchange
within a few milliseconds, given realistic communication channels with a
limited bandwidth. For demonstration we evaluate characteristics of a
standard-cell ASIC design realisation as IP-core in 0.18-micrometer
CMOS-technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502063</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502063</id><created>2005-02-14</created><authors><author><keyname>Tan</keyname><forenames>Peng Hui</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>Nonlinear MMSE Multiuser Detection Based on Multivariate Gaussian
  Approximation</title><categories>cs.IT math.IT</categories><abstract>  In this paper, a class of nonlinear MMSE multiuser detectors are derived
based on a multivariate Gaussian approximation of the multiple access
interference. This approach leads to expressions identical to those describing
the probabilistic data association (PDA) detector, thus providing an
alternative analytical justification for this structure. A simplification to
the PDA detector based on approximating the covariance matrix of the
multivariate Gaussian distribution is suggested, resulting in a soft
interference cancellation scheme. Corresponding multiuser soft-input,
soft-output detectors delivering extrinsic log-likelihood ratios are derived
for application in iterative multiuser decoders. Finally, a large system
performance analysis is conducted for the simplified PDA, showing that the bit
error rate performance of this detector can be accurately predicted and related
to the replica method analysis for the optimal detector. Methods from
statistical neuro-dynamics are shown to provide a closely related alternative
large system prediction. Numerical results demonstrate that for large systems,
the bit error rate is accurately predicted by the analysis and found to be
close to optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502064</id><created>2005-02-14</created><authors><author><keyname>Buls</keyname><forenames>Janis</forenames></author></authors><title>The Lattice of Machine Invariant Sets and Subword Complexity</title><categories>cs.CR cs.CC cs.DM</categories><comments>9 pages, extended abstract published in 67th Workshop on General
  Algebra</comments><acm-class>D.2.8;D.4.6;E.3;F.1.1;F.1.3;F.4.3</acm-class><abstract>  We investigate the lattice of machine invariant classes. This is an infinite
completely distributive lattice but it is not a Boolean lattice. We show the
subword complexity and the growth function create machine invariant classes. So
the lattice would serve as a measure of words cryptographic quality if we like
to identify new stream ciphers suitable for widespread adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502065</id><created>2005-02-14</created><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Konwar</keyname><forenames>Kishori M.</forenames></author><author><keyname>Mandoiu</keyname><forenames>Ion I.</forenames></author><author><keyname>Shvartsman</keyname><forenames>Alex A.</forenames></author></authors><title>Highly Scalable Algorithms for Robust String Barcoding</title><categories>cs.DS</categories><abstract>  String barcoding is a recently introduced technique for genomic-based
identification of microorganisms. In this paper we describe the engineering of
highly scalable algorithms for robust string barcoding. Our methods enable
distinguisher selection based on whole genomic sequences of hundreds of
microorganisms of up to bacterial size on a well-equipped workstation, and can
be easily parallelized to further extend the applicability range to thousands
of bacterial size genomes. Experimental results on both randomly generated and
NCBI genomic data show that whole-genome based selection results in a number of
distinguishers nearly matching the information theoretic lower bounds for the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502066</id><created>2005-02-15</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author></authors><title>On the Complexity of Real Functions</title><categories>cs.CC cs.NA math.NA</categories><acm-class>F. 1.1; F. 4. 1</acm-class><abstract>  We develop a notion of computability and complexity of functions over the
reals, which seems to be very natural when one tries to determine just how
&quot;difficult&quot; a certain function is. This notion can be viewed as an extension of
both BSS computability [Blum, Cucker, Shub, Smale 1998], and bit computability
in the tradition of computable analysis [Weihrauch 2000] as it relies on the
latter but allows some discontinuities and multiple values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502067</id><created>2005-02-15</created><authors><author><keyname>Poland</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Master Algorithms for Active Experts Problems based on Increasing Loss
  Values</title><categories>cs.LG cs.AI</categories><comments>8 two-column pages, latex2e</comments><report-no>IDSIA-01-05</report-no><acm-class>I.2.6; G.3</acm-class><journal-ref>Proc. 14th Dutch-Belgium Conf. on Machine Learning (Benelearn
  2005) 59-66</journal-ref><abstract>  We specify an experts algorithm with the following characteristics: (a) it
uses only feedback from the actions actually chosen (bandit setup), (b) it can
be applied with countably infinite expert classes, and (c) it copes with losses
that may grow in time appropriately slowly. We prove loss bounds against an
adaptive adversary. From this, we obtain master algorithms for &quot;active experts
problems&quot;, which means that the master's actions may influence the behavior of
the adversary. Our algorithm can significantly outperform standard experts
algorithms on such problems. Finally, we combine it with a universal expert
class. This results in a (computationally infeasible) universal master
algorithm which performs - in a certain sense - almost as well as any
computable strategy, for any online problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502068</id><created>2005-02-15</created><authors><author><keyname>Tromp</keyname><forenames>John</forenames></author><author><keyname>Cilibrasi</keyname><forenames>Rudi</forenames></author></authors><title>Limits of Rush Hour Logic Complexity</title><categories>cs.CC</categories><acm-class>F.1.3; F.2</acm-class><abstract>  Rush Hour Logic was introduced in [Flake&amp;Baum99] as a model of computation
inspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontally
or vertically within a parking lot. The authors show how the model supports
polynomial space computation, using certain car configurations as building
blocks to construct boolean circuits for a cpu and memory. They consider the
use of cars of length 3 crucial to their construction, and conjecture that cars
of size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomial
space computation. We settle this conjecture by showing that the required
building blocks are constructible in Size 2 Rush Hour. Furthermore, we consider
Unit Rush Hour, which was hitherto believed to be trivial, show its relation to
maze puzzles, and provide empirical support for its hardness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502069</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502069</id><created>2005-02-15</created><authors><author><keyname>Kroeller</keyname><forenames>Alexander</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Buschmann</keyname><forenames>Carsten</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan</forenames></author><author><keyname>Pfisterer</keyname><forenames>Dennis</forenames></author></authors><title>Koordinatenfreies Lokationsbewusstsein (Localization without
  Coordinates)</title><categories>cs.DC</categories><comments>German, 15 pages, 6 figures, Latex, to appear in Information
  Technology</comments><acm-class>C.2.2; F.2; G.2.2</acm-class><abstract>  Localization is one of the fundamental issues in sensor networks. It is
almost always assumed that it must be solved by assigning coordinates to the
nodes. This article discusses positioning algorithms from a theoretical,
practical and simulative point of view, and identifies difficulties and
limitations. Ideas for more abstract means of location awareness are presented
and the resulting possible improvements for applications are shown. Nodes with
certain topological or environmental properties are clustered, and the
neighborhood structure of the clusters is modeled as a graph. Eines der
fundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer
die Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon
ausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In
diesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein
kritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen
aufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine
abstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird
gezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf
einer graphenbasierten Modellierung des Netzes: Knoten mit bestimmten
topologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und
Clusternachbarschaften dann als Graphen modelliert.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502070</id><created>2005-02-16</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author></authors><title>Bidimensionality, Map Graphs, and Grid Minors</title><categories>cs.DM cs.DS</categories><comments>12 pages</comments><abstract>  In this paper we extend the theory of bidimensionality to two families of
graphs that do not exclude fixed minors: map graphs and power graphs. In both
cases we prove a polynomial relation between the treewidth of a graph in the
family and the size of the largest grid minor. These bounds improve the running
times of a broad class of fixed-parameter algorithms. Our novel technique of
using approximate max-min relations between treewidth and size of grid minors
is powerful, and we show how it can also be used, e.g., to prove a linear
relation between the treewidth of a bounded-genus graph and the treewidth of
its dual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502071</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502071</id><created>2005-02-16</created><authors><author><keyname>Li</keyname><forenames>Husheng</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Analysis of Second-order Statistics Based Semi-blind Channel Estimation
  in CDMA Channels</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2005 Conference on Information Sciences and
  Systems</comments><abstract>  The performance of second order statistics (SOS) based semi-blind channel
estimation in long-code DS-CDMA systems is analyzed. The covariance matrix of
SOS estimates is obtained in the large system limit, and is used to analyze the
large-sample performance of two SOS based semi-blind channel estimation
algorithms. A notion of blind estimation efficiency is also defined and is
examined via simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502072</id><created>2005-02-16</created><authors><author><keyname>OMullane</keyname><forenames>William</forenames></author><author><keyname>Li</keyname><forenames>Nolan</forenames></author><author><keyname>Nieto-Santisteban</keyname><forenames>Maria</forenames></author><author><keyname>Szalay</keyname><forenames>Alex</forenames></author><author><keyname>Thakar</keyname><forenames>Ani</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author></authors><title>Batch is back: CasJobs, serving multi-TB data on the Web</title><categories>cs.DC cs.DB</categories><report-no>Microsoft Technical Report MSR TR 2005 19</report-no><abstract>  The Sloan Digital Sky Survey (SDSS) science database describes over 140
million objects and is over 1.5 TB in size. The SDSS Catalog Archive Server
(CAS) provides several levels of query interface to the SDSS data via the
SkyServer website. Most queries execute in seconds or minutes. However, some
queries can take hours or days, either because they require non-index scans of
the largest tables, or because they request very large result sets, or because
they represent very complex aggregations of the data. These &quot;monster queries&quot;
not only take a long time, they also affect response times for everyone else -
one or more of them can clog the entire system. To ameliorate this problem, we
developed a multi-server multi-queue batch job submission and tracking system
for the CAS called CasJobs. The transfer of very large result sets from queries
over the network is another serious problem. Statistics suggested that much of
this data transfer is unnecessary; users would prefer to store results locally
in order to allow further joins and filtering. To allow local analysis, a
system was developed that gives users their own personal databases (MyDB) at
the server side. Users may transfer data to their MyDB, and then perform
further analysis before extracting it to their own machine. MyDB tables also
provide a convenient way to share results of queries with collaborators without
downloading them. CasJobs is built using SOAP XML Web services and has been in
operation since May 2004.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502073</id><created>2005-02-17</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames><affiliation>IGM</affiliation></author><author><keyname>D&#xe9;sarm&#xe9;nien</keyname><forenames>Jacques</forenames><affiliation>IGM</affiliation></author><author><keyname>Perrin</keyname><forenames>Dominique</forenames><affiliation>IGM</affiliation></author></authors><title>A note on the Burrows-Wheeler transformation</title><categories>cs.DS</categories><comments>2004</comments><proxy>ccsd ccsd-00004249</proxy><report-no>CDP04tcs</report-no><acm-class>ACM E.4 ; ACM G.2.1</acm-class><abstract>  We relate the Burrows-Wheeler transformation with a result in combinatorics
on words known as the Gessel-Reutenauer transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502074</id><created>2005-02-17</created><updated>2005-10-17</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>On sample complexity for computational pattern recognition</title><categories>cs.LG cs.CC</categories><report-no>in proceedings of ALT 2005</report-no><abstract>  In statistical setting of the pattern recognition problem the number of
examples required to approximate an unknown labelling function is linear in the
VC dimension of the target learning class. In this work we consider the
question whether such bounds exist if we restrict our attention to computable
pattern recognition methods, assuming that the unknown labelling function is
also computable. We find that in this case the number of examples required for
a computable method to approximate the labelling function not only is not
linear, but grows faster (in the VC dimension of the class) than any computable
function. No time or space constraints are put on the predictors or target
functions; the only resource we consider is the training examples.
  The task of pattern recognition is considered in conjunction with another
learning problem -- data compression. An impossibility result for the task of
data compression allows us to estimate the sample complexity for pattern
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502075</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502075</id><created>2005-02-17</created><authors><author><keyname>Guha</keyname><forenames>Sudipto</forenames></author></authors><title>How far will you walk to find your shortcut: Space Efficient Synopsis
  Construction Algorithms</title><categories>cs.DS cs.DB</categories><abstract>  In this paper we consider the wavelet synopsis construction problem without
the restriction that we only choose a subset of coefficients of the original
data. We provide the first near optimal algorithm. We arrive at the above
algorithm by considering space efficient algorithms for the restricted version
of the problem. In this context we improve previous algorithms by almost a
linear factor and reduce the required space to almost linear. Our techniques
also extend to histogram construction, and improve the space-running time
tradeoffs for V-Opt and range query histograms. We believe the idea applies to
a broad range of dynamic programs and demonstrate it by showing improvements in
a knapsack-like setting seen in construction of Extended Wavelets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502076</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502076</id><created>2005-02-17</created><updated>2006-07-05</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>Learning nonsingular phylogenies and hidden Markov models</title><categories>cs.LG cs.CE math.PR math.ST q-bio.PE stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/105051606000000024 in the
  Annals of Applied Probability (http://www.imstat.org/aap/) by the Institute
  of Mathematical Statistics (http://www.imstat.org)</comments><report-no>IMS-AAP-AAP0161</report-no><msc-class>60J10, 60J20, 68T05, 92B10 (Primary)</msc-class><journal-ref>Annals of Applied Probability 2006, Vol. 16, No. 2, 583-614</journal-ref><doi>10.1214/105051606000000024</doi><abstract>  In this paper we study the problem of learning phylogenies and hidden Markov
models. We call a Markov model nonsingular if all transition matrices have
determinants bounded away from 0 (and 1). We highlight the role of the
nonsingularity condition for the learning problem. Learning hidden Markov
models without the nonsingularity condition is at least as hard as learning
parity with noise, a well-known learning problem conjectured to be
computationally hard. On the other hand, we give a polynomial-time algorithm
for learning nonsingular phylogenies and hidden Markov models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502077</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502077</id><created>2005-02-18</created><authors><author><keyname>Shental</keyname><forenames>Ori</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shental</keyname><forenames>Noam</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On the Achievable Information Rates of Finite-State Input
  Two-Dimensional Channels with Memory</title><categories>cs.IT math.IT</categories><comments>5 pages, Submitted to ISIT 05</comments><abstract>  The achievable information rate of finite-state input two-dimensional (2-D)
channels with memory is an open problem, which is relevant, e.g., for
inter-symbol-interference (ISI) channels and cellular multiple-access channels.
We propose a method for simulation-based computation of such information rates.
We first draw a connection between the Shannon-theoretic information rate and
the statistical mechanics notion of free energy. Since the free energy of such
systems is intractable, we approximate it using the cluster variation method,
implemented via generalized belief propagation. The derived, fully tractable,
algorithm is shown to provide a practically accurate estimate of the
information rate. In our experimental study we calculate the information rates
of 2-D ISI channels and of hexagonal Wyner cellular networks with binary
inputs, for which formerly only bounds were known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502078</id><created>2005-02-18</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Semantical Characterizations and Complexity of Equivalences in Answer
  Set Programming</title><categories>cs.AI cs.CC</categories><comments>58 pages, 6 tables. The contents were partially published in:
  Proceedings 19th International Conference on Logic Programming (ICLP 2003),
  pp. 224-238, LNCS 2916, Springer, 2003; and Proceedings 9th European
  Conference on Logics in Artificial Intelligence (JELIA 2004), pp. 161-173,
  LNCS 3229, Springer, 2004</comments><report-no>1843-05-01</report-no><acm-class>I.2.3; F.4.1; I.2.4</acm-class><abstract>  In recent research on non-monotonic logic programming, repeatedly strong
equivalence of logic programs P and Q has been considered, which holds if the
programs P union R and Q union R have the same answer sets for any other
program R. This property strengthens equivalence of P and Q with respect to
answer sets (which is the particular case for R is the empty set), and has its
applications in program optimization, verification, and modular logic
programming. In this paper, we consider more liberal notions of strong
equivalence, in which the actual form of R may be syntactically restricted. On
the one hand, we consider uniform equivalence, where R is a set of facts rather
than a set of rules. This notion, which is well known in the area of deductive
databases, is particularly useful for assessing whether programs P and Q are
equivalent as components of a logic program which is modularly structured. On
the other hand, we consider relativized notions of equivalence, where R ranges
over rules over a fixed alphabet, and thus generalize our results to
relativized notions of strong and uniform equivalence. For all these notions,
we consider disjunctive logic programs in the propositional (ground) case, as
well as some restricted classes, provide semantical characterizations and
analyze the computational complexity. Our results, which naturally extend to
answer set semantics for programs with strong negation, complement the results
on strong equivalence of logic programs and pave the way for optimizations in
answer set solvers as a tool for input-based problem solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502079</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502079</id><created>2005-02-18</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Zemor</keyname><forenames>Gilles</forenames></author></authors><title>Multilevel expander codes</title><categories>cs.IT math.IT</categories><journal-ref>&quot;Algebraic Coding Theory and Information Theory,&quot; Providence, RI:
  AMS (2005), pp. 69-83.</journal-ref><abstract>  We define multilevel codes on bipartite graphs that have properties analogous
to multilevel serial concatenations. A decoding algorithm is described that
corrects a proportion of errors equal to half the Blokh-Zyablov bound on the
minimum distance. The error probability of this algorithm has exponent similar
to that of serially concatenated multilevel codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502080</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502080</id><created>2005-02-19</created><authors><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Sensor Configuration and Activation for Field Detection in Large Sensor
  Arrays</title><categories>cs.IT math.IT</categories><comments>7 pages with 11 figures; also to appear in the Fourth International
  Conference on Information Processing in Sensor Networks</comments><acm-class>E. 4; H.11</acm-class><abstract>  The problems of sensor configuration and activation for the detection of
correlated random fields using large sensor arrays are considered. Using
results that characterize the large-array performance of sensor networks in
this application, the detection capabilities of different sensor configurations
are analyzed and compared. The dependence of the optimal choice of
configuration on parameters such as sensor signal-to-noise ratio (SNR), field
correlation, etc., is examined, yielding insights into the most effective
choices for sensor selection and activation in various operating regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502081</id><created>2005-02-20</created><authors><author><keyname>Bertelle</keyname><forenames>Cyrille</forenames><affiliation>LIH</affiliation></author><author><keyname>Duchamp</keyname><forenames>G&#xe9;rard Henry Edmond</forenames><affiliation>LIPN</affiliation></author><author><keyname>Khatatneh</keyname><forenames>Khalaf</forenames><affiliation>LIFAR</affiliation></author></authors><title>Tables, Memorized Semirings and Applications</title><categories>cs.MA cs.DM</categories><proxy>ccsd ccsd-00004303</proxy><acm-class>G2</acm-class><abstract>  We define and construct a new data structure, the tables, this structure
generalizes the (finite) $k$-sets sets of Eilenberg \cite{Ei}, it is versatile
(one can vary the letters, the words and the coefficients). We derive from this
structure a new semiring (with several semiring structures) which can be
applied to the needs of automatic processing multi-agents behaviour problems.
The purpose of this account/paper is to present also the basic elements of this
new structures from a combinatorial point of view. These structures present a
bunch of properties. They will be endowed with several laws namely : Sum,
Hadamard product, Cauchy product, Fuzzy operations (min, max, complemented
product) Two groups of applications are presented. The first group is linked to
the process of &quot;forgetting&quot; information in the tables. The second, linked to
multi-agent systems, is announced by showing a methodology to manage emergent
organization from individual behaviour models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502082</id><created>2005-02-21</created><authors><author><keyname>Konczak</keyname><forenames>Kathrin</forenames></author><author><keyname>Linke</keyname><forenames>Thomas</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Graphs and colorings for answer set programming</title><categories>cs.AI cs.LO</categories><abstract>  We investigate the usage of rule dependency graphs and their colorings for
characterizing and computing answer sets of logic programs. This approach
provides us with insights into the interplay between rules when inducing answer
sets. We start with different characterizations of answer sets in terms of
totally colored dependency graphs that differ in graph-theoretical aspects. We
then develop a series of operational characterizations of answer sets in terms
of operators on partial colorings. In analogy to the notion of a derivation in
proof theory, our operational characterizations are expressed as
(non-deterministically formed) sequences of colorings, turning an uncolored
graph into a totally colored one. In this way, we obtain an operational
framework in which different combinations of operators result in different
formal properties. Among others, we identify the basic strategy employed by the
noMoRe system and justify its algorithmic approach. Furthermore, we distinguish
operations corresponding to Fitting's operator as well as to well-founded
semantics. (To appear in Theory and Practice of Logic Programming (TPLP))
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502083</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502083</id><created>2005-02-21</created><authors><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author><author><keyname>Sahinoglu</keyname><forenames>Zafer</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hisashi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Impulse Radio Systems with Multiple Types of Ultra-Wideband Pulses</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2005 Conference on Information Sciences and
  Systems</comments><abstract>  Spectral properties and performance of multi-pulse impulse radio
ultra-wideband systems with pulse-based polarity randomization are analyzed.
Instead of a single type of pulse transmitted in each frame, multiple types of
pulses are considered, which is shown to reduce the effects of multiple-access
interference. First, the spectral properties of a multi-pulse impulse radio
system is investigated. It is shown that the power spectral density is the
average of spectral contents of different pulse shapes. Then, approximate
closed-form expressions for bit error probability of a multi-pulse impulse
radio system are derived for RAKE receivers in asynchronous multiuser
environments. The theoretical and simulation results indicate that impulse
radio systems that are more robust against multiple-access interference than a
&quot;classical&quot; impulse radio system can be designed with multiple types of
ultra-wideband pulses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502084</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502084</id><created>2005-02-21</created><authors><author><keyname>Wang</keyname><forenames>Chih-Chun</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames><affiliation>Princeton University</affiliation></author></authors><title>On the Typicality of the Linear Code Among the LDPC Coset Code Ensemble</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2005 Conference on Information Sciences and
  Systems</comments><abstract>  Density evolution (DE) is one of the most powerful analytical tools for
low-density parity-check (LDPC) codes on memoryless
binary-input/symmetric-output channels. The case of non-symmetric channels is
tackled either by the LDPC coset code ensemble (a channel symmetrizing
argument) or by the generalized DE for linear codes on non-symmetric channels.
Existing simulations show that the bit error rate performances of these two
different approaches are nearly identical. This paper explains this phenomenon
by proving that as the minimum check node degree $d_c$ becomes sufficiently
large, the performance discrepancy of the linear and the coset LDPC codes is
theoretically indistinguishable. This typicality of linear codes among the LDPC
coset code ensemble provides insight into the concentration theorem of LDPC
coset codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502085</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502085</id><created>2005-02-22</created><authors><author><keyname>Viger</keyname><forenames>Fabien</forenames><affiliation>LIAFA, Regal Ur-R Lip6</affiliation></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames><affiliation>LIAFA</affiliation></author></authors><title>Fast generation of random connected graphs with prescribed degrees</title><categories>cs.NI cond-mat.dis-nn cs.DM</categories><proxy>ccsd ccsd-00004310</proxy><acm-class>G.2.2</acm-class><abstract>  We address here the problem of generating random graphs uniformly from the
set of simple connected graphs having a prescribed degree sequence. Our goal is
to provide an algorithm designed for practical use both because of its ability
to generate very large graphs (efficiency) and because it is easy to implement
(simplicity). We focus on a family of heuristics for which we prove optimality
conditions, and show how this optimality can be reached in practice. We then
propose a different approach, specifically designed for typical real-world
degree distributions, which outperforms the first one. Assuming a conjecture
which we state and argue rigorously, we finally obtain an log-linear algorithm,
which, in spite of being very simple, improves the best known complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502086</id><created>2005-02-22</created><authors><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames></author></authors><title>The Self-Organization of Speech Sounds</title><categories>cs.LG cs.AI cs.CL cs.NE cs.RO math.DS</categories><proxy>ccsd ccsd-00004318</proxy><journal-ref>Journal of Theoretical Biology 233 (2005) Issue 3, Pages 435-449</journal-ref><abstract>  The speech code is a vehicle of language: it defines a set of forms used by a
community to carry information. Such a code is necessary to support the
linguistic interactions that allow humans to communicate. How then may a speech
code be formed prior to the existence of linguistic interactions? Moreover, the
human speech code is discrete and compositional, shared by all the individuals
of a community but different across communities, and phoneme inventories are
characterized by statistical regularities. How can a speech code with these
properties form? We try to approach these questions in the paper, using the
&quot;methodology of the artificial&quot;. We build a society of artificial agents, and
detail a mechanism that shows the formation of a discrete speech code without
pre-supposing the existence of linguistic capacities or of coordinated
interactions. The mechanism is based on a low-level model of sensory-motor
interactions. We show that the integration of certain very simple and non
language-specific neural devices leads to the formation of a speech code that
has properties similar to the human speech code. This result relies on the
self-organizing properties of a generic coupling between perception and
production within agents, and on the interactions between agents. The
artificial system helps us to develop better intuitions on how speech might
have appeared, by showing how self-organization might have helped natural
selection to find speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502087</id><created>2005-02-22</created><authors><author><keyname>Ewaschuk</keyname><forenames>Robert</forenames></author><author><keyname>Turney</keyname><forenames>Peter</forenames></author></authors><title>Self-Replicating Strands that Self-Assemble into User-Specified Meshes</title><categories>cs.NE cs.CE cs.MA</categories><comments>27 pages, issued 2005, Java code available at
  http://purl.org/net/johnnyvon/</comments><report-no>NRC-47442, ERB-1121</report-no><acm-class>I.6.3; I.6.8; J.2; J.3</acm-class><abstract>  It has been argued that a central objective of nanotechnology is to make
products inexpensively, and that self-replication is an effective approach to
very low-cost manufacturing. The research presented here is intended to be a
step towards this vision. In previous work (JohnnyVon 1.0), we simulated
machines that bonded together to form self-replicating strands. There were two
types of machines (called types 0 and 1), which enabled strands to encode
arbitrary bit strings. However, the information encoded in the strands had no
functional role in the simulation. The information was replicated without being
interpreted, which was a significant limitation for potential manufacturing
applications. In the current work (JohnnyVon 2.0), the information in a strand
is interpreted as instructions for assembling a polygonal mesh. There are now
four types of machines and the information encoded in a strand determines how
it folds. A strand may be in an unfolded state, in which the bonds are straight
(although they flex slightly due to virtual forces acting on the machines), or
in a folded state, in which the bond angles depend on the types of machines. By
choosing the sequence of machine types in a strand, the user can specify a
variety of polygonal shapes. A simulation typically begins with an initial
unfolded seed strand in a soup of unbonded machines. The seed strand replicates
by bonding with free machines in the soup. The child strands fold into the
encoded polygonal shape, and then the polygons drift together and bond to form
a mesh. We demonstrate that a variety of polygonal meshes can be manufactured
in the simulation, by simply changing the sequence of machine types in the
seed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502088</id><created>2005-02-22</created><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames></author></authors><title>Towards a Systematic Account of Different Semantics for Logic Programs</title><categories>cs.AI cs.LO</categories><comments>20 pages</comments><acm-class>I.2.3; D.1.6; F.4.1</acm-class><abstract>  In [Hitzler and Wendt 2002, 2005], a new methodology has been proposed which
allows to derive uniform characterizations of different declarative semantics
for logic programs with negation. One result from this work is that the
well-founded semantics can formally be understood as a stratified version of
the Fitting (or Kripke-Kleene) semantics. The constructions leading to this
result, however, show a certain asymmetry which is not readily understood. We
will study this situation here with the result that we will obtain a coherent
picture of relations between different semantics for normal logic programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502089</identifier>
 <datestamp>2011-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502089</id><created>2005-02-22</created><authors><author><keyname>Bardeen</keyname><forenames>M.</forenames></author><author><keyname>Gilbert</keyname><forenames>E.</forenames></author><author><keyname>Jordan</keyname><forenames>T.</forenames></author><author><keyname>Neywoda</keyname><forenames>P.</forenames></author><author><keyname>Quigg</keyname><forenames>E.</forenames></author><author><keyname>Wilde</keyname><forenames>M.</forenames></author><author><keyname>Zhao</keyname><forenames>Y.</forenames></author></authors><title>The QuarkNet/Grid Collaborative Learning e-Lab</title><categories>cs.DC physics.ed-ph</categories><comments>8 pages, 7 figures, 1 table presented at The 2nd International
  Workshop on Collaborative and Learning Applications of Grid Technology and
  Grid Education</comments><report-no>FERMILAB-CONF-04-366-LSS</report-no><journal-ref>Future Gener.Comput.Syst.22:700-708,2006</journal-ref><doi>10.1016/j.future.2006.03.001</doi><abstract>  We describe a case study that uses grid computing techniques to support the
collaborative learning of high school students investigating cosmic rays.
Students gather and upload science data to our e-Lab portal. They explore those
data using techniques from the GriPhyN collaboration. These techniques include
virtual data transformations, workflows, metadata cataloging and indexing, data
product provenance and persistence, as well as job planners. Students use web
browsers and a custom interface that extends the GriPhyN Chiron portal to
perform all of these tasks. They share results in the form of online posters
and ask each other questions in this asynchronous environment. Students can
discover and extend the research of other students, modeling the processes of
modern large-scale scientific collaborations. Also, the e-Lab portal provides
tools for teachers to guide student work throughout an investigation.
http://quarknet.uchicago.edu/elab/cosmic
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502090</id><created>2005-02-24</created><authors><author><keyname>Streit</keyname><forenames>A.</forenames></author><author><keyname>Erwin</keyname><forenames>D.</forenames></author><author><keyname>Lippert</keyname><forenames>Th.</forenames></author><author><keyname>Mallmann</keyname><forenames>D.</forenames></author><author><keyname>Menday</keyname><forenames>R.</forenames></author><author><keyname>Rambadt</keyname><forenames>M.</forenames></author><author><keyname>Riedel</keyname><forenames>M.</forenames></author><author><keyname>Romberg</keyname><forenames>M.</forenames></author><author><keyname>Schuller</keyname><forenames>B.</forenames></author><author><keyname>Wieder</keyname><forenames>Ph.</forenames></author></authors><title>UNICORE - From Project Results to Production Grids</title><categories>cs.DC cs.OS</categories><comments>21 pages, 3 figures</comments><abstract>  The UNICORE Grid-technology provides a seamless, secure and intuitive access
to distributed Grid resources. In this paper we present the recent evolution
from project results to production Grids. At the beginning UNICORE was
developed as a prototype software in two projects funded by the German research
ministry (BMBF). Over the following years, in various European-funded projects,
UNICORE evolved to a full-grown and well-tested Grid middleware system, which
today is used in daily production at many supercomputing centers worldwide.
Beyond this production usage, the UNICORE technology serves as a solid basis in
many European and International research projects, which use existing UNICORE
components to implement advanced features, high level services, and support for
applications from a growing range of domains. In order to foster these ongoing
developments, UNICORE is available as open source under BSD licence at
SourceForge, where new releases are published on a regular basis. This paper is
a review of the UNICORE achievements so far and gives a glimpse on the UNICORE
roadmap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502091</id><created>2005-02-24</created><updated>2005-04-28</updated><authors><author><keyname>Cederquist</keyname><forenames>J. G.</forenames></author><author><keyname>Corin</keyname><forenames>R.</forenames></author><author><keyname>Dekker</keyname><forenames>M. A. C.</forenames></author><author><keyname>Etalle</keyname><forenames>S.</forenames></author><author><keyname>Hartog</keyname><forenames>J. I. den</forenames></author></authors><title>An Audit Logic for Accountability</title><categories>cs.CR cs.LO</categories><comments>To appear in Proceedings of IEEE Policy 2005</comments><abstract>  We describe and implement a policy language. In our system, agents can
distribute data along with usage policies in a decentralized architecture. Our
language supports the specification of conditions and obligations, and also the
possibility to refine policies. In our framework, the compliance with usage
policies is not actively enforced. However, agents are accountable for their
actions, and may be audited by an authority requiring justifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502092</identifier>
 <datestamp>2010-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502092</id><created>2005-02-25</created><authors><author><keyname>Deriaz</keyname><forenames>Erwan</forenames><affiliation>LMC - IMAG</affiliation></author><author><keyname>Perrier</keyname><forenames>Val&#xe9;rie</forenames><affiliation>LMC - IMAG</affiliation></author></authors><title>Divergence-free Wavelets for Navier-Stokes</title><categories>cs.NA</categories><comments>novembre 2004</comments><proxy>ccsd ccsd-00004338</proxy><report-no>1072 - M</report-no><journal-ref>Journal of Turbulence, Volume 7, N 3 2006</journal-ref><doi>10.1080/14685240500260547</doi><abstract>  In this paper, we investigate the use of compactly supported divergence-free
wavelets for the representation of the Navier-Stokes solution. After reminding
the theoretical construction of divergence-free wavelet vectors, we present in
detail the bases and corresponding fast algorithms for 2D and 3D incompressible
flows. In order to compute the nonlinear term, we propose a new method which
provides in practice with the Hodge decomposition of any flow: this
decomposition enables us to separate the incompressible part of the flow from
its orthogonal complement, which corresponds to the gradient component of the
flow. Finally we show numerical tests to validate our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502093</id><created>2005-02-25</created><authors><author><keyname>Mei</keyname><forenames>Alessandro</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Online Permutation Routing in Partitioned Optical Passive Star Networks</title><categories>cs.DC</categories><abstract>  This paper establishes the state of the art in both deterministic and
randomized online permutation routing in the POPS network. Indeed, we show that
any permutation can be routed online on a POPS network either with
$O(\frac{d}{g}\log g)$ deterministic slots, or, with high probability, with
$5c\lceil d/g\rceil+o(d/g)+O(\log\log g)$ randomized slots, where constant
$c=\exp (1+e^{-1})\approx 3.927$. When $d=\Theta(g)$, that we claim to be the
&quot;interesting&quot; case, the randomized algorithm is exponentially faster than any
other algorithm in the literature, both deterministic and randomized ones. This
is true in practice as well. Indeed, experiments show that it outperforms its
rivals even starting from as small a network as a POPS(2,2), and the gap grows
exponentially with the size of the network. We can also show that, under proper
hypothesis, no deterministic algorithm can asymptotically match its
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502094</id><created>2005-02-27</created><authors><author><keyname>Aknine</keyname><forenames>Samir</forenames></author><author><keyname>Shehory</keyname><forenames>Onn</forenames></author></authors><title>Coalition Formation: Concessions, Task Relationships and Complexity
  Reduction</title><categories>cs.MA</categories><abstract>  Solutions to the coalition formation problem commonly assume agent
rationality and, correspondingly, utility maximization. This in turn may
prevent agents from making compromises. As shown in recent studies, compromise
may facilitate coalition formation and increase agent utilities. In this study
we leverage on those new results. We devise a novel coalition formation
mechanism that enhances compromise. Our mechanism can utilize information on
task dependencies to reduce formation complexity. Further, it works well with
both cardinal and ordinal task values. Via experiments we show that the use of
the suggested compromise-based coalition formation mechanism provides
significant savings in the computation and communication complexity of
coalition formation. Our results also show that when information on task
dependencies is used, the complexity of coalition formation is further reduced.
We demonstrate successful use of the mechanism for collaborative information
filtering, where agents combine linguistic rules to analyze documents'
contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502095</id><created>2005-02-28</created><updated>2005-07-20</updated><authors><author><keyname>Giraldi</keyname><forenames>Gilson A.</forenames></author><author><keyname>Marturelli</keyname><forenames>Leandro S.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Paulo S.</forenames></author></authors><title>Gradient Vector Flow Models for Boundary Extraction in 2D Images</title><categories>cs.CV</categories><comments>8 pages, 11 figures</comments><abstract>  The Gradient Vector Flow (GVF) is a vector diffusion approach based on
Partial Differential Equations (PDEs). This method has been applied together
with snake models for boundary extraction medical images segmentation. The key
idea is to use a diffusion-reaction PDE to generate a new external force field
that makes snake models less sensitivity to initialization as well as improves
the snake's ability to move into boundary concavities. In this paper, we
firstly review basic results about convergence and numerical analysis of usual
GVF schemes. We point out that GVF presents numerical problems due to
discontinuities image intensity. This point is considered from a practical
viewpoint from which the GVF parameters must follow a relationship in order to
improve numerical convergence. Besides, we present an analytical analysis of
the GVF dependency from the parameters values. Also, we observe that the method
can be used for multiply connected domains by just imposing the suitable
boundary condition. In the experimental results we verify these theoretical
points and demonstrate the utility of GVF on a segmentation approach that we
have developed based on snakes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0502096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0502096</id><created>2005-02-28</created><authors><author><keyname>van Hemert</keyname><forenames>J. I.</forenames></author></authors><title>Property analysis of symmetric travelling salesman problem instances
  acquired through evolution</title><categories>cs.NE cs.AI</categories><comments>To be published in G. Raidl and J. Gottlieb, editors, Evolutionary
  Computation in Combinatorial Optimization, Springer Lecture Notes on Computer
  Science, pages 122-131. Springer-Verlag, Berlin, 2005</comments><acm-class>G1.6;I.2.8</acm-class><abstract>  We show how an evolutionary algorithm can successfully be used to evolve a
set of difficult to solve symmetric travelling salesman problem instances for
two variants of the Lin-Kernighan algorithm. Then we analyse the instances in
those sets to guide us towards deferring general knowledge about the efficiency
of the two variants in relation to structural properties of the symmetric
travelling sale sman problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503001</id><created>2005-03-01</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but
  actually it is not)</title><categories>cs.CV cs.IR</categories><abstract>  Pattern recognition is generally assumed as an interaction of two inversely
directed image-processing streams: the bottom-up information details gathering
and localization (segmentation) stream, and the top-down information features
aggregation, association and interpretation (recognition) stream. Inspired by
recent evidence from biological vision research and by the insights of
Kolmogorov Complexity theory, we propose a new, just top-down evolving,
procedure of initial image segmentation. We claim that traditional top-down
cognitive reasoning, which is supposed to guide the segmentation process to its
final result, is not at all a part of the image information content evaluation.
And that initial image segmentation is certainly an unsupervised process. We
present some illustrative examples, which support our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503002</id><created>2005-03-01</created><authors><author><keyname>Lobo</keyname><forenames>Lester</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author></authors><title>Local and Global Analysis: Complementary Activities for Increasing the
  Effectiveness of Requirements Verification and Validation</title><categories>cs.SE</categories><comments>6 pages, 3 figures, published: ACM Southeast Conference, March 2005</comments><acm-class>D.2.1; D.2.9</acm-class><abstract>  This paper presents a unique approach to connecting requirements engineering
(RE) activities into a process framework that can be employed to obtain quality
requirements with reduced expenditures of effort and cost. We propose a
two-phase model that is novel in that it introduces the concept of verification
and validation (V&amp;V) early in the requirements life cycle. In the first phase,
we perform V&amp;V immediately following the elicitation of requirements for each
individually distinct system function. Because the first phase focuses on
capturing smaller sets of related requirements iteratively, each corresponding
V&amp;V activity is better focused for detecting and correcting errors in each
requirement set. In the second phase, a complementary verification activity is
initiated; the corresponding focus is on the quality of linkages between
requirements sets rather than on the requirements within the sets.
Consequently, this approach reduces the effort in verification and enhances the
focus on the verification task. Our approach, unlike other models, has a
minimal time delay between the elicitation of requirements and the execution of
the V&amp;V activities. Because of this short time gap, the stakeholders have a
clearer recollection of the requirements, their context and rationale; this
enhances the stakeholder feedback. Furthermore, our model includes activities
that closely align with the effective RE processes employed in the software
industry. Thus, our approach facilitates a better understanding of the flow of
requirements, and provides guidance for the implementation of the RE process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503003</id><created>2005-03-01</created><authors><author><keyname>Lobo</keyname><forenames>Lester</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author></authors><title>An Objectives-Driven Process for Selecting Methods to Support
  Requirements Engineering Activities</title><categories>cs.SE</categories><comments>20 pages, 5 figures, 3 tables, publisheed: 29th Annual IEEE/NASA
  Software Engineering Workshop, April 2005</comments><acm-class>D.2.1; D.2.9</acm-class><abstract>  This paper presents a framework that guides the requirements engineer in the
implementation and execution of an effective requirements generation process.
We achieve this goal by providing a well-defined requirements engineering model
and a criteria based process for optimizing method selection for attendant
activities. Our model, unlike other models, addresses the complete requirements
generation process and consists of activities defined at more adequate levels
of abstraction. Additionally, activity objectives are identified and explicitly
stated - not implied as in the current models. Activity objectives are crucial
as they drive the selection of methods for each activity. Our model also
incorporates a unique approach to verification and validation that enhances
quality and reduces the cost of generating requirements. To assist in the
selection of methods, we have mapped commonly used methods to activities based
on their objectives. In addition, we have identified method selection criteria
and prescribed a reduced set of methods that optimize these criteria for each
activity defined by our requirements generation process. Thus, the defined
approach assists in the task of selecting methods by using selection criteria
to reduce a large collection of potential methods to a smaller, manageable set.
The model and the set of methods, taken together, provide the much needed
guidance for the effective implementation and execution of the requirements
generation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503004</id><created>2005-03-01</created><authors><author><keyname>Lobo</keyname><forenames>Lester</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author></authors><title>Effective Requirements Generation: Synchronizing Early Verification &amp;
  Validation, Methods and Method Selection Criteria</title><categories>cs.SE</categories><comments>46 pages, 7 figures, 7 tables, provides elaborate description of the
  research</comments><acm-class>D.2.1; D.2.9</acm-class><abstract>  This paper presents an approach for the implementation and execution of an
effective requirements generation process. We achieve this goal by providing a
well-defined requirements engineering model that includes verification and
validation (V&amp;V), and analysis. In addition, we identify focused activity
objectives and map popular methods to lower-level activities, and define a
criterion based process for optimizing method selection for attendant
activities. Our model, unlike other models, addresses the complete requirements
generation process and consists of activities defined at more adequate levels
of abstraction. Furthermore, our model also incorporates a unique approach to
V&amp;V that enhances quality and reduces the cost of generating requirements.
Additionally, activity objectives are identified and explicitly stated - not
implied as in the current models. To assist in the selection of an appropriate
set of methods, we have mapped commonly used methods to activities based on
their objectives. Finally, we have identified method selection criteria and
prescribed a reduced set of methods that optimize these criteria for each
activity in our model. Thus, our approach assists in the task of selecting
methods by using selection criteria to reduce a large collection of potential
methods to a smaller, manageable set. The model, clear mapping of methods to
activity objectives, and the criteria based process, taken together, provide
the much needed guidance for the effective implementation and execution of the
requirements generation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503005</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503005</id><created>2005-03-02</created><updated>2005-11-03</updated><authors><author><keyname>Kuyumchyan</keyname><forenames>A.</forenames></author><author><keyname>Isoyan</keyname><forenames>A.</forenames></author><author><keyname>Shulakov</keyname><forenames>E.</forenames></author><author><keyname>Aristov</keyname><forenames>V.</forenames></author><author><keyname>Kondratenkov</keyname><forenames>M.</forenames></author><author><keyname>Snigirev</keyname><forenames>A.</forenames></author><author><keyname>Snigireva</keyname><forenames>I.</forenames></author><author><keyname>Souvorov</keyname><forenames>A.</forenames></author><author><keyname>Tamasaku</keyname><forenames>K.</forenames></author><author><keyname>Yabashi</keyname><forenames>M.</forenames></author><author><keyname>Ishikawa</keyname><forenames>T.</forenames></author><author><keyname>Trouni</keyname><forenames>K.</forenames></author></authors><title>High efficiency and low absorption Fresnel compound zone plates for hard
  X-ray focusing</title><categories>cs.OH</categories><comments>5 pages, 7 figures</comments><report-no>SPIE-4783-11</report-no><acm-class>J.2</acm-class><journal-ref>Proceedings of SPIE, 2002, vol. 4783, p. 92-96</journal-ref><doi>10.1117/12.450480</doi><abstract>  Circular and linear zone plates have been fabricated on the surface of
silicon crystals for the energy of 8 keV by electron beam lithography and deep
ion plasma etching methods. Various variants of compound zone plates with
first, second, third diffraction orders have been made. The zone relief height
is about 10 mkm, the outermost zone width of the zone plate is 0.4 mkm. The
experimental testing of the zone plates has been conducted on SPring-8 and ESRF
synchrotron radiation sources. A focused spot size and diffraction efficiency
measured by knife-edge scanning are accordingly 0.5 mkm and 39% for the first
order circular zone plate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503006</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503006</id><created>2005-03-02</created><authors><author><keyname>Cai</keyname><forenames>J.</forenames></author><author><keyname>Tjhai</keyname><forenames>C.</forenames></author><author><keyname>Tomlinson</keyname><forenames>M.</forenames></author><author><keyname>Ambroze</keyname><forenames>M.</forenames></author><author><keyname>Ahmed</keyname><forenames>M.</forenames></author></authors><title>A New Non-Iterative Decoding Algorithm for the Erasure Channel :
  Comparisons with Enhanced Iterative Methods</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT2005</comments><abstract>  This paper investigates decoding of binary linear block codes over the binary
erasure channel (BEC). Of the current iterative decoding algorithms on this
channel, we review the Recovery Algorithm and the Guess Algorithm. We then
present a Multi-Guess Algorithm extended from the Guess Algorithm and a new
algorithm -- the In-place Algorithm. The Multi-Guess Algorithm can push the
limit to break the stopping sets. However, the performance of the Guess and the
Multi-Guess Algorithm depend on the parity-check matrix of the code.
Simulations show that we can decrease the frame error rate by several orders of
magnitude using the Guess and the Multi-Guess Algorithms when the parity-check
matrix of the code is sparse. The In-place Algorithm can obtain better
performance even if the parity check matrix is dense. We consider the
application of these algorithms in the implementation of multicast and
broadcast techniques on the Internet. Using these algorithms, a user does not
have to wait until the entire transmission has been received.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503007</id><created>2005-03-03</created><authors><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author><author><keyname>Smith</keyname><forenames>Joan</forenames></author><author><keyname>Luce</keyname><forenames>Rick</forenames></author></authors><title>Toward alternative metrics of journal impact: A comparison of download
  and citation data</title><categories>cs.DL</categories><comments>34 pages, submitted to Information Processing and Management, special
  issue on Informetrics</comments><acm-class>H.3.7</acm-class><abstract>  We generated networks of journal relationships from citation and download
data, and determined journal impact rankings from these networks using a set of
social network centrality metrics. The resulting journal impact rankings were
compared to the ISI IF. Results indicate that, although social network metrics
and ISI IF rankings deviate moderately for citation-based journal networks,
they differ considerably for journal networks derived from download data. We
believe the results represent a unique aspect of general journal impact that is
not captured by the ISI IF. These results furthermore raise questions regarding
the validity of the ISI IF as the sole assessment of journal impact, and
suggest the possibility of devising impact metrics based on usage information
in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503008</id><created>2005-03-03</created><authors><author><keyname>Tournier</keyname><forenames>Laurent</forenames><affiliation>LMC - IMAG</affiliation></author></authors><title>Approximation of dynamical systems using S-systems theory : application
  to biological systems</title><categories>cs.SC math.DS q-bio.MN</categories><proxy>ccsd ccsd-00004351</proxy><abstract>  In this paper we propose a new symbolic-numeric algorithm to find positive
equilibria of a n-dimensional dynamical system. This algorithm implies a
symbolic manipulation of ODE in order to give a local approximation of
differential equations with power-law dynamics (S-systems). A numerical
calculus is then needed to converge towards an equilibrium, giving at the same
time a S-system approximating the initial system around this equilibrium. This
algorithm is applied to a real biological example in 14 dimensions which is a
subsystem of a metabolic pathway in Arabidopsis Thaliana.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503009</id><created>2005-03-03</created><authors><author><keyname>Leao</keyname><forenames>R. S. C.</forenames></author><author><keyname>Barbosa</keyname><forenames>V. C.</forenames></author></authors><title>Minimal chordal sense of direction and circulant graphs</title><categories>cs.DM</categories><acm-class>G.2.1; G.2.2</acm-class><journal-ref>Lecture Notes in Computer Science 4162 (2006), 670-680</journal-ref><doi>10.1007/11821069_58</doi><abstract>  A sense of direction is an edge labeling on graphs that follows a globally
consistent scheme and is known to considerably reduce the complexity of several
distributed problems. In this paper, we study a particular instance of sense of
direction, called a chordal sense of direction (CSD). In special, we identify
the class of k-regular graphs that admit a CSD with exactly k labels (a minimal
CSD). We prove that connected graphs in this class are Hamiltonian and that the
class is equivalent to that of circulant graphs, presenting an efficient
(polynomial-time) way of recognizing it when the graphs' degree k is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503010</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503010</id><created>2005-03-03</created><updated>2005-03-07</updated><authors><author><keyname>Krause</keyname><forenames>Wolfram</forenames><affiliation>Corporate Technology, Siemens AG</affiliation><affiliation>FIAS/FIGSS, Universitaet Frankfurt</affiliation></author><author><keyname>Scholz</keyname><forenames>Jan</forenames><affiliation>Corporate Technology, Siemens AG</affiliation><affiliation>ITP, Universitaet Giessen</affiliation></author><author><keyname>Greiner</keyname><forenames>Martin</forenames><affiliation>Corporate Technology, Siemens AG</affiliation></author></authors><title>Optimized network structure and routing metric in wireless multihop ad
  hoc communication</title><categories>cs.NI</categories><comments>25 pages, v2: fixed one small typo in the 'authors' field</comments><doi>10.1016/j.physa.2005.06.085</doi><abstract>  Inspired by the Statistical Physics of complex networks, wireless multihop ad
hoc communication networks are considered in abstracted form. Since such
engineered networks are able to modify their structure via topology control, we
search for optimized network structures, which maximize the end-to-end
throughput performance. A modified version of betweenness centrality is
introduced and shown to be very relevant for the respective modeling. The
calculated optimized network structures lead to a significant increase of the
end-to-end throughput. The discussion of the resulting structural properties
reveals that it will be almost impossible to construct these optimized
topologies in a technologically efficient distributive manner. However, the
modified betweenness centrality also allows to propose a new routing metric for
the end-to-end communication traffic. This approach leads to an even larger
increase of throughput capacity and is easily implementable in a
technologically relevant manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503011</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503011</id><created>2005-03-04</created><authors><author><keyname>Pandey</keyname><forenames>Sandeep</forenames></author><author><keyname>Roy</keyname><forenames>Sourashis</forenames></author><author><keyname>Olston</keyname><forenames>Christopher</forenames></author><author><keyname>Cho</keyname><forenames>Junghoo</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Soumen</forenames></author></authors><title>Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of
  Search Engine Results</title><categories>cs.IR</categories><report-no>CMU-CS-05-116</report-no><acm-class>H.3.3; G.3</acm-class><abstract>  In-degree, PageRank, number of visits and other measures of Web page
popularity significantly influence the ranking of search results by modern
search engines. The assumption is that popularity is closely correlated with
quality, a more elusive concept that is difficult to measure directly.
Unfortunately, the correlation between popularity and quality is very weak for
newly-created pages that have yet to receive many visits and/or in-links.
Worse, since discovery of new content is largely done by querying search
engines, and because users usually focus their attention on the top few
results, newly-created but high-quality pages are effectively ``shut out,'' and
it can take a very long time before they become popular.
  We propose a simple and elegant solution to this problem: the introduction of
a controlled amount of randomness into search result ranking methods. Doing so
offers new pages a chance to prove their worth, although clearly using too much
randomness will degrade result quality and annul any benefits achieved. Hence
there is a tradeoff between exploration to estimate the quality of new pages
and exploitation of pages already known to be of high quality. We study this
tradeoff both analytically and via simulation, in the context of an economic
objective function based on aggregate result quality amortized over time. We
show that a modest amount of randomness leads to improved search results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503012</id><created>2005-03-04</created><updated>2005-03-07</updated><authors><author><keyname>Geerts</keyname><forenames>Floris</forenames></author><author><keyname>Haesevoets</keyname><forenames>Sofie</forenames></author><author><keyname>Kuijpers</keyname><forenames>Bart</forenames></author></authors><title>First-order Complete and Computationally Complete Query Languages for
  Spatio-Temporal Databases</title><categories>cs.DB</categories><comments>Cleaned up source code</comments><acm-class>H.2.3</acm-class><abstract>  We address a fundamental question concerning spatio-temporal database
systems: ``What are exactly spatio-temporal queries?'' We define
spatio-temporal queries to be computable mappings that are also generic,
meaning that the result of a query may only depend to a limited extent on the
actual internal representation of the spatio-temporal data. Genericity is
defined as invariance under groups of geometric transformations that preserve
certain characteristics of spatio-temporal data (e.g., collinearity, distance,
velocity, acceleration, ...). These groups depend on the notions that are
relevant in particular spatio-temporal database applications.
 These transformations also have the distinctive property that they respect the
monotone and unidirectional nature of time.
  We investigate different genericity classes with respect to the constraint
database model for spatio-temporal databases and we identify sound and complete
languages for the first-order and the computable queries in these genericity
classes. We distinguish between genericity determined by time-invariant
transformations, genericity notions concerning physical quantities and
genericity determined by time-dependent transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503013</id><created>2005-03-04</created><authors><author><keyname>Barchet-Estefanel</keyname><forenames>Luiz Angelo</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author><author><keyname>Mouni&#xe9;</keyname><forenames>Gr&#xe9;gory</forenames><affiliation>ID - Imag, Apache Ur-Ra Id Imag</affiliation></author></authors><title>Pr\'{e}diction de Performances pour les Communications Collectives</title><categories>cs.DC</categories><proxy>ccsd ccsd-00004368</proxy><abstract>  Des travaux r\'{e}cents visent l'optimisation des op\'{e}rations de
communication collective dans les environnements de type grille de calcul. La
solution la plus r\'{e}pandue est la s\'{e}paration des communications internes
et externes \`{a} chaque grappe, mais cela n'exclut pas le d\'{e}coupage des
communications en plusieurs couches, pratique efficace d\'{e}montr\'{e}e par
Karonis et al. [10]. Dans les deux cas, la pr\'{e}diction des performances est
un facteur essentiel, soit pour le r\'{e}glage fin des param\`{e}tres de
communication, soit pour le calcul de la distribution et de la hi\'{e}rarchie
des communications. Pour cela, il est tr\`{e}s important d'avoir des
mod\`{e}les pr\'{e}cis des communications collectives, lesquels seront
utilis\'{e}s pour pr\'{e}dire ces performances. Cet article d\'{e}crit notre
exp\'{e}rience sur la mod\'{e}lisation des op\'{e}rations de communication
collective. Nous pr\'{e}sentons des mod\`{e}les de communication pour
diff\'{e}rents patrons de communication collective comme un vers plusieurs, un
vers plusieurs personnalis\'{e} et plusieurs vers plusieurs. Pour \'{e}valuer
la pr\'{e}cision des mod\`{e}les, nous comparons les pr\'{e}dictions obtenues
avec les r\'{e}sultats des exp\'{e}rimentations effectu\'{e}es sur deux
environnements r\'{e}seaux diff\'{e}rents, Fast Ethernet et Myrinet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503014</id><created>2005-03-04</created><authors><author><keyname>Straka</keyname><forenames>Christian W.</forenames></author></authors><title>ADF95: Tool for automatic differentiation of a FORTRAN code designed for
  large numbers of independent variables</title><categories>cs.MS</categories><comments>24 pages, 2 figures, 4 tables, accepted in Computer Physics
  Communications</comments><acm-class>D.1.5; G1.7; G1.8; G.4; J.2</acm-class><doi>10.1016/j.cpc.2005.01.011</doi><abstract>  ADF95 is a tool to automatically calculate numerical first derivatives for
any mathematical expression as a function of user defined independent
variables. Accuracy of derivatives is achieved within machine precision. ADF95
may be applied to any FORTRAN 77/90/95 conforming code and requires minimal
changes by the user. It provides a new derived data type that holds the value
and derivatives and applies forward differencing by overloading all FORTRAN
operators and intrinsic functions. An efficient indexing technique leads to a
reduced memory usage and a substantially increased performance gain over other
available tools with operator overloading. This gain is especially pronounced
for sparse systems with large number of independent variables. A wide class of
numerical simulations, e.g., those employing implicit solvers, can profit from
ADF95.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503015</id><created>2005-03-05</created><authors><author><keyname>van Deursen</keyname><forenames>Arie</forenames></author><author><keyname>Marin</keyname><forenames>Marius</forenames></author><author><keyname>Moonen</keyname><forenames>Leon</forenames></author></authors><title>A Systematic Aspect-Oriented Refactoring and Testing Strategy, and its
  Application to JHotDraw</title><categories>cs.SE cs.PL</categories><comments>25 pages</comments><acm-class>D.2.7; D.2.5; D.1.5</acm-class><abstract>  Aspect oriented programming aims at achieving better modularization for a
system's crosscutting concerns in order to improve its key quality attributes,
such as evolvability and reusability. Consequently, the adoption of
aspect-oriented techniques in existing (legacy) software systems is of interest
to remediate software aging. The refactoring of existing systems to employ
aspect-orientation will be considerably eased by a systematic approach that
will ensure a safe and consistent migration.
 In this paper, we propose a refactoring and testing strategy that supports
such an approach and consider issues of behavior conservation and (incremental)
integration of the aspect-oriented solution with the original system. The
strategy is applied to the JHotDraw open source project and illustrated on a
group of selected concerns. Finally, we abstract from the case study and
present a number of generic refactorings which contribute to an incremental
aspect-oriented refactoring process and associate particular types of
crosscutting concerns to the model and features of the employed aspect
language. The contributions of this paper are both in the area of supporting
migration towards aspect-oriented solutions and supporting the development of
aspect languages that are better suited for such migrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503016</id><created>2005-03-07</created><updated>2005-06-03</updated><authors><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Balakireva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Hochstenbach</keyname><forenames>Patrick</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>File-based storage of Digital Objects and constituent datastreams:
  XMLtapes and Internet Archive ARC files</title><categories>cs.DL</categories><comments>12 pages, 1 figures (camera-ready copy for ECDL 2005)</comments><abstract>  This paper introduces the write-once/read-many XMLtape/ARC storage approach
for Digital Objects and their constituent datastreams. The approach combines
two interconnected file-based storage mechanisms that are made accessible in a
protocol-based manner. First, XML-based representations of multiple Digital
Objects are concatenated into a single file named an XMLtape. An XMLtape is a
valid XML file; its format definition is independent of the choice of the
XML-based complex object format by which Digital Objects are represented. The
creation of indexes for both the identifier and the creation datetime of the
XML-based representation of the Digital Objects facilitates OAI-PMH-based
access to Digital Objects stored in an XMLtape. Second, ARC files, as
introduced by the Internet Archive, are used to contain the constituent
datastreams of the Digital Objects in a concatenated manner. An index for the
identifier of the datastream facilitates OpenURL-based access to an ARC file.
The interconnection between XMLtapes and ARC files is provided by conveying the
identifiers of ARC files associated with an XMLtape as administrative
information in the XMLtape, and by including OpenURL references to constituent
datastreams of a Digital Object in the XML-based representation of that Digital
Object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503017</id><created>2005-03-07</created><updated>2005-04-05</updated><authors><author><keyname>Nikmehr</keyname><forenames>Hooman</forenames></author><author><keyname>Phillips</keyname><forenames>Braden</forenames></author><author><keyname>Lim</keyname><forenames>Cheng-Chew</forenames></author></authors><title>A Fast Combined Decimal Adder/Subtractor</title><categories>cs.OH</categories><comments>This paper has been withdrawn by the authors</comments><acm-class>B.2.4 [Arithmetic and Logic Structures][High-Speed
  Arithmetic][Algorithms]</acm-class><abstract>  This paper has been withdrawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503018</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503018</id><created>2005-03-07</created><updated>2005-12-20</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pucella</keyname><forenames>Riccardo</forenames></author></authors><title>Probabilistic Algorithmic Knowledge</title><categories>cs.AI cs.LO</categories><comments>26 pages. A preliminary version appeared in Proc. 9th Conference on
  Theoretical Aspects of Rationality and Knowledge (TARK'03)</comments><acm-class>I.2.4; G.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 3 (December
  20, 2005) lmcs:1074</journal-ref><doi>10.2168/LMCS-1(3:1)2005</doi><abstract>  The framework of algorithmic knowledge assumes that agents use deterministic
knowledge algorithms to compute the facts they explicitly know. We extend the
framework to allow for randomized knowledge algorithms. We then characterize
the information provided by a randomized knowledge algorithm when its answers
have some probability of being incorrect. We formalize this information in
terms of evidence; a randomized knowledge algorithm returning ``Yes'' to a
query about a fact \phi provides evidence for \phi being true. Finally, we
discuss the extent to which this evidence can be used as a basis for decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503019</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503019</id><created>2005-03-08</created><updated>2005-03-14</updated><authors><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author><author><keyname>Miliou</keyname><forenames>Natalia</forenames></author></authors><title>Duality Bounds on the Cut-Off Rate with Applications to Ricean Fading</title><categories>cs.IT math.IT</categories><abstract>  We propose a technique to derive upper bounds on Gallager's cost-constrained
random coding exponent function. Applying this technique to the non-coherent
peak-power or average-power limited discrete time memoryless Ricean fading
channel, we obtain the high signal-to-noise ratio (SNR) expansion of this
channel's cut-off rate. At high SNR the gap between channel capacity and the
cut-off rate approaches a finite limit. This limit is approximately 0.26 nats
per channel-use for zero specular component (Rayleigh) fading and approaches
0.39 nats per channel-use for very large specular components.
  We also compute the asymptotic cut-off rate of a Rayleigh fading channel when
the receiver has access to some partial side information concerning the fading.
It is demonstrated that the cut-off rate does not utilize the side information
as efficiently as capacity, and that the high SNR gap between the two increases
to infinity as the imperfect side information becomes more and more precise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503020</id><created>2005-03-08</created><authors><author><keyname>Brody</keyname><forenames>Tim</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Earlier Web Usage Statistics as Predictors of Later Citation Impact</title><categories>cs.IR</categories><abstract>  The use of citation counts to assess the impact of research articles is well
established. However, the citation impact of an article can only be measured
several years after it has been published. As research articles are
increasingly accessed through the Web, the number of times an article is
downloaded can be instantly recorded and counted. One would expect the number
of times an article is read to be related both to the number of times it is
cited and to how old the article is. This paper analyses how short-term Web
usage impact predicts medium-term citation impact. The physics e-print archive
(arXiv.org) is used to test this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503021</id><created>2005-03-08</created><authors><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>Fast-Forward on the Green Road to Open Access: The Case Against Mixing
  Up Green and Gold</title><categories>cs.IR</categories><journal-ref>Ariadne 42 January 2005; http://www.ariadne.ac.uk/issue42/harnad/</journal-ref><abstract>  This article is a critique of: &quot;The 'Green' and 'Gold' Roads to Open Access:
The Case for Mixing and Matching&quot; by Jean-Claude Guedon (in Serials Review
30(4) 2004).
  Open Access (OA) means: free online access to all peer-reviewed journal
articles. Jean-Claude Guedon argues against the efficacy of author
self-archiving of peer-reviewed journal articles (the &quot;Green&quot; road to OA). He
suggests instead that we should convert to Open Access Publishing (the &quot;Golden&quot;
road to OA) by &quot;mixing and matching&quot; Green and Gold as follows: o First,
self-archive dissertations (not published, peer-reviewed journal articles). o
Second, identify and tag how those dissertations have been evaluated and
reviewed. o Third, self-archive unrefereed preprints (not published,
peer-reviewed journal articles). o Fourth, develop new mechanisms for
evaluating and reviewing those unrefereed preprints, at multiple levels. The
result will be OA Publishing (Gold). I argue that rather than yet another 10
years of speculation like this, what is actually needed (and imminent) is for
OA self-archiving to be mandated by research funders and institutions so that
the self-archiving of published, peer-reviewed journal articles (Green) can be
fast-forwarded to 100% OA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503022</id><created>2005-03-09</created><updated>2005-03-11</updated><authors><author><keyname>Pfeifer</keyname><forenames>Daniel</forenames></author><author><keyname>Lockemann</keyname><forenames>Peter C.</forenames></author></authors><title>Theory and Practice of Transactional Method Caching</title><categories>cs.DB</categories><report-no>2005-9</report-no><acm-class>H.2.4.o;H.3.4.b;C.4</acm-class><abstract>  Nowadays, tiered architectures are widely accepted for constructing large
scale information systems. In this context application servers often form the
bottleneck for a system's efficiency. An application server exposes an object
oriented interface consisting of set of methods which are accessed by
potentially remote clients. The idea of method caching is to store results of
read-only method invocations with respect to the application server's interface
on the client side. If the client invokes the same method with the same
arguments again, the corresponding result can be taken from the cache without
contacting the server. It has been shown that this approach can considerably
improve a real world system's efficiency.
  This paper extends the concept of method caching by addressing the case where
clients wrap related method invocations in ACID transactions. Demarcating
sequences of method calls in this way is supported by many important
application server standards. In this context the paper presents an
architecture, a theory and an efficient protocol for maintaining full
transactional consistency and in particular serializability when using a method
cache on the client side. In order to create a protocol for scheduling cached
method results, the paper extends a classical transaction formalism. Based on
this extension, a recovery protocol and an optimistic serializability protocol
are derived. The latter one differs from traditional transactional cache
protocols in many essential ways. An efficiency experiment validates the
approach: Using the cache a system's performance and scalability are
considerably improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503023</id><created>2005-03-09</created><updated>2005-12-05</updated><authors><author><keyname>Carlson</keyname><forenames>Josiah</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>The Weighted Maximum-Mean Subtree and Other Bicriterion Subtree Problems</title><categories>cs.CG cs.DS</categories><comments>10 pages</comments><abstract>  We consider problems in which we are given a rooted tree as input, and must
find a subtree with the same root, optimizing some objective function of the
nodes in the subtree. When this function is the sum of constant node weights,
the problem is trivially solved in linear time. When the objective is the sum
of weights that are linear functions of a parameter, we show how to list all
optima for all possible parameter values in O(n log n) time; this parametric
optimization problem can be used to solve many bicriterion optimizations
problems, in which each node has two values xi and yi associated with it, and
the objective function is a bivariate function f(SUM(xi),SUM(yi)) of the sums
of these two values. A special case, when f is the ratio of the two sums, is
the Weighted Maximum-Mean Subtree Problem, or equivalently the Fractional
Prize-Collecting Steiner Tree Problem on Trees; for this special case, we
provide a linear time algorithm for this problem when all weights are positive,
improving a previous O(n log n) solution, and prove that the problem is
NP-complete when negative weights are allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503024</id><created>2005-03-10</created><authors><author><keyname>Tufis</keyname><forenames>Dan</forenames></author><author><keyname>Ion</keyname><forenames>Radu</forenames></author><author><keyname>Ide</keyname><forenames>Nancy</forenames></author></authors><title>Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word
  Alignment, Word Clustering and Aligned Wordnets</title><categories>cs.AI cs.CL</categories><comments>7 pages in Proc. of COLING2005</comments><journal-ref>In proceedings of the 20th International Conference on
  Computational Linguistics, COLING2004, Geneva, 2004, pp. 1312-1318</journal-ref><abstract>  The paper presents a method for word sense disambiguation based on parallel
corpora. The method exploits recent advances in word alignment and word
clustering based on automatic extraction of translation equivalents and being
supported by available aligned wordnets for the languages in the corpus. The
wordnets are aligned to the Princeton Wordnet, according to the principles
established by EuroWordNet. The evaluation of the WSD system, implementing the
method described herein showed very encouraging results. The same system used
in a validation mode, can be used to check and spot alignment errors in
multilingually aligned wordnets as BalkaNet and EuroWordNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503025</id><created>2005-03-10</created><updated>2005-04-13</updated><authors><author><keyname>Yu</keyname><forenames>Jia</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>A Taxonomy of Workflow Management Systems for Grid Computing</title><categories>cs.DC</categories><comments>29 pages, 15 figures</comments><report-no>GRIDS-TR-2005-1, Grid Computing and Distributed Systems Laboratory,
  University of Melbourne, Australia, March 10, 2005</report-no><acm-class>D.1</acm-class><abstract>  With the advent of Grid and application technologies, scientists and
engineers are building more and more complex applications to manage and process
large data sets, and execute scientific experiments on distributed resources.
Such application scenarios require means for composing and executing complex
workflows. Therefore, many efforts have been made towards the development of
workflow management systems for Grid computing. In this paper, we propose a
taxonomy that characterizes and classifies various approaches for building and
executing workflows on Grids. We also survey several representative Grid
workflow systems developed by various projects world-wide to demonstrate the
comprehensiveness of the taxonomy. The taxonomy not only highlights the design
and engineering similarities and differences of state-of-the-art in Grid
workflow systems, but also identifies the areas that need further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503026</identifier>
 <datestamp>2008-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503026</id><created>2005-03-11</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>On Generalized Computable Universal Priors and their Convergence</title><categories>cs.LG cs.CC math.PR</categories><comments>22 pages</comments><report-no>IDSIA-05-05</report-no><acm-class>I.2.6; E.4; G.3</acm-class><journal-ref>Theoretical Computer Science, 364 (2006) 27-41</journal-ref><abstract>  Solomonoff unified Occam's razor and Epicurus' principle of multiple
explanations to one elegant, formal, universal theory of inductive inference,
which initiated the field of algorithmic information theory. His central result
is that the posterior of the universal semimeasure M converges rapidly to the
true sequence generating posterior mu, if the latter is computable. Hence, M is
eligible as a universal predictor in case of unknown mu. The first part of the
paper investigates the existence and convergence of computable universal
(semi)measures for a hierarchy of computability classes: recursive, estimable,
enumerable, and approximable. For instance, M is known to be enumerable, but
not estimable, and to dominate all enumerable semimeasures. We present proofs
for discrete and continuous semimeasures. The second part investigates more
closely the types of convergence, possibly implied by universality: in
difference and in ratio, with probability 1, in mean sum, and for Martin-Loef
random sequences. We introduce a generalized concept of randomness for
individual sequences and use it to exhibit difficulties regarding these issues.
In particular, we show that convergence fails (holds) on generalized-random
sequences in gappy (dense) Bernoulli classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503027</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503027</id><created>2005-03-12</created><updated>2005-03-16</updated><authors><author><keyname>Martinian</keyname><forenames>Emin</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory W.</forenames></author><author><keyname>Chen</keyname><forenames>Brian</forenames></author></authors><title>Authentication with Distortion Criteria</title><categories>cs.IT cs.CR cs.MM math.IT</categories><comments>22 pages, 10 figures</comments><abstract>  In a variety of applications, there is a need to authenticate content that
has experienced legitimate editing in addition to potential tampering attacks.
We develop one formulation of this problem based on a strict notion of
security, and characterize and interpret the associated information-theoretic
performance limits. The results can be viewed as a natural generalization of
classical approaches to traditional authentication. Additional insights into
the structure of such systems and their behavior are obtained by further
specializing the results to Bernoulli and Gaussian cases. The associated
systems are shown to be substantially better in terms of performance and/or
security than commonly advocated approaches based on data hiding and digital
watermarking. Finally, the formulation is extended to obtain efficient layered
authentication system constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503028</id><created>2005-03-14</created><updated>2005-03-23</updated><authors><author><keyname>Dung</keyname><forenames>Phan Minh</forenames><affiliation>Asian Institute of Technology</affiliation></author><author><keyname>Hanh</keyname><forenames>Do Duc</forenames><affiliation>Asian Institute of Technology</affiliation></author><author><keyname>Thang</keyname><forenames>Phan Minh</forenames><affiliation>Asian Institute of Technology</affiliation></author></authors><title>Stabilization of Cooperative Information Agents in Unpredictable
  Environment: A Logic Programming Approach</title><categories>cs.LO cs.MA cs.PL</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><acm-class>F.4.1; I.2.3; I.2.11</acm-class><abstract>  An information agent is viewed as a deductive database consisting of 3 parts:
an observation database containing the facts the agent has observed or sensed
from its surrounding environment, an input database containing the information
the agent has obtained from other agents, and an intensional database which is
a set of rules for computing derived information from the information stored in
the observation and input databases. Stabilization of a system of information
agents represents a capability of the agents to eventually get correct
information about their surrounding despite unpredictable environment changes
and the incapability of many agents to sense such changes causing them to have
temporary incorrect information. We argue that the stabilization of a system of
cooperative information agents could be understood as the convergence of the
behavior of the whole system toward the behavior of a &quot;superagent&quot;, who has the
sensing and computing capabilities of all agents combined. We show that
unfortunately, stabilization is not guaranteed in general, even if the agents
are fully cooperative and do not hide any information from each other. We give
sufficient conditions for stabilization and discuss the consequences of our
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503029</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503029</id><created>2005-03-14</created><authors><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Eichhorn</keyname><forenames>Guenther</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn</forenames></author><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author></authors><title>The Effect of Use and Access on Citations</title><categories>cs.DL</categories><comments>Accepted for publication in Information Processing &amp; Management,
  special issue on scientometrics</comments><acm-class>H.3.7</acm-class><journal-ref>Inform Process Manag 41:1395-1402 (2005)</journal-ref><doi>10.1016/j.ipm.2005.03.010</doi><abstract>  It has been shown (S. Lawrence, 2001, Nature, 411, 521) that journal articles
which have been posted without charge on the internet are more heavily cited
than those which have not been. Using data from the NASA Astrophysics Data
System (ads.harvard.edu) and from the ArXiv e-print archive at Cornell
University (arXiv.org) we examine the causes of this effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503030</id><created>2005-03-14</created><updated>2005-12-06</updated><authors><author><keyname>Pampapathi</keyname><forenames>Rajesh M.</forenames></author><author><keyname>Mirkin</keyname><forenames>Boris</forenames></author><author><keyname>Levene</keyname><forenames>Mark</forenames></author></authors><title>A Suffix Tree Approach to Email Filtering</title><categories>cs.AI cs.CL</categories><comments>Revisions made in the light of reviewer comments. Main changes: (i)
  The extension and elaboration of section 4.4 which describes the scoring
  algorithm; (ii) Favouring the use of false positive and false negative
  performance measures over the use of precision and recall; (iii) The addition
  of ROC curves wherever possible; and (iv) Inclusion of performance statistics
  for algorithm. Re-submitted 5th August 2005</comments><abstract>  We present an approach to email filtering based on the suffix tree data
structure. A method for the scoring of emails using the suffix tree is
developed and a number of scoring and score normalisation functions are tested.
Our results show that the character level representation of emails and classes
facilitated by the suffix tree can significantly improve classification
accuracy when compared with the currently popular methods, such as naive Bayes.
We believe the method can be extended to the classification of documents in
other domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503031</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503031</id><created>2005-03-14</created><updated>2006-02-12</updated><authors><author><keyname>Hu</keyname><forenames>An-swol</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Servetto</keyname><forenames>Sergio D.</forenames><affiliation>Cornell University</affiliation></author></authors><title>On the Scalability of Cooperative Time Synchronization in
  Pulse-Connected Networks</title><categories>cs.IT math.IT nlin.AO</categories><comments>Final version, to appear in the IEEE Transactions on Information
  Theory. This replacement version contains some non-trivial revisions made in
  two rounds of reviews</comments><journal-ref>IEEE Trans. Inform. Theory, 52(6):2725-2748, 2006.</journal-ref><abstract>  The problem of time synchronization in dense wireless networks is considered.
Well established synchronization techniques suffer from an inherent scalability
problem in that synchronization errors grow with an increasing number of hops
across the network. In this work, a model for communication in wireless
networks is first developed, and then the model is used to define a new time
synchronization mechanism. A salient feature of the proposed method is that, in
the regime of asymptotically dense networks, it can average out all random
errors and maintain global synchronization in the sense that all nodes in the
multi-hop network can see identical timing signals. This is irrespective of the
distance separating any two nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503032</id><created>2005-03-14</created><updated>2005-10-28</updated><authors><author><keyname>Bertossi</keyname><forenames>L.</forenames><affiliation>Carleton University</affiliation></author><author><keyname>Bravo</keyname><forenames>L.</forenames><affiliation>Carleton University</affiliation></author><author><keyname>Franconi</keyname><forenames>E.</forenames><affiliation>Free University of Bozen--Bolzano</affiliation></author><author><keyname>Lopatenko</keyname><forenames>A.</forenames><affiliation>Free University of Bozen--Bolzano</affiliation><affiliation>University of Manchester</affiliation></author></authors><title>Complexity and Approximation of Fixing Numerical Attributes in Databases
  Under Integrity Constraints</title><categories>cs.DB cs.CC</categories><comments>35 pages. Extended version of the camera ready version to appear in
  Proc. of the Databases Programming Languages Conference (DBPL 05), Springer
  LNCS volume 3774</comments><abstract>  Consistent query answering is the problem of computing the answers from a
database that are consistent with respect to certain integrity constraints that
the database as a whole may fail to satisfy. Those answers are characterized as
those that are invariant under minimal forms of restoring the consistency of
the database. In this context, we study the problem of repairing databases by
fixing integer numerical values at the attribute level with respect to denial
and aggregation constraints. We introduce a quantitative definition of database
fix, and investigate the complexity of several decision and optimization
problems, including DFP, i.e. the existence of fixes within a given distance
from the original instance, and CQA, i.e. deciding consistency of answers to
aggregate conjunctive queries under different semantics. We provide sharp
complexity bounds, identify relevant tractable cases; and introduce
approximation algorithms for some of those that are intractable. More
specifically, we obtain results like undecidability of existence of fixes for
aggregation constraints; MAXSNP-hardness of DFP, but a good approximation
algorithm for a relevant special case; and intractability but good
approximation for CQA for aggregate queries for one database atom denials (plus
built-ins).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503033</id><created>2005-03-15</created><authors><author><keyname>Afantenos</keyname><forenames>Stergos D.</forenames></author><author><keyname>Liontou</keyname><forenames>Konstantina</forenames></author><author><keyname>Salapata</keyname><forenames>Maria</forenames></author><author><keyname>Karkaletsis</keyname><forenames>Vangelis</forenames></author></authors><title>An Introduction to the Summarization of Evolving Events: Linear and
  Non-linear Evolution</title><categories>cs.CL cs.IR</categories><comments>10 pages, 3 figures. To be pulished in Natural Language Understanding
  and Cognitive Science (NLUCS - 2005) conference</comments><journal-ref>Edited by Bernadete Sharp, Proceedings of the 2nd International
  Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2005.
  Maiami, Florida, USA: INSTICC Press. pp 91-99.</journal-ref><abstract>  This paper examines the summarization of events that evolve through time. It
discusses different types of evolution taking into account the time in which
the incidents of an event are happening and the different sources reporting on
the specific event. It proposes an approach for multi-document summarization
which employs ``messages'' for representing the incidents of an event and
cross-document relations that hold between messages according to certain
conditions. The paper also outlines the current version of the summarization
system we are implementing to realize this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503034</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503034</id><created>2005-03-16</created><authors><author><keyname>Poulin</keyname><forenames>David</forenames></author><author><keyname>Touchette</keyname><forenames>Hugo</forenames></author></authors><title>Comment on &quot;Some non-conventional ideas about algorithmic complexity&quot;</title><categories>cs.CC</categories><comments>Short comment, 2 pages</comments><abstract>  We comment on a recent paper by D'Abramo [Chaos, Solitons &amp; Fractals, 25
(2005) 29], focusing on the author's statement that an algorithm can produce a
list of strings containing at least one string whose algorithmic complexity is
greater than that of the entire list. We show that this statement, although
perplexing, is not as paradoxical as it seems when the definition of
algorithmic complexity is applied correctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503035</id><created>2005-03-17</created><authors><author><keyname>Bogomolnaia</keyname><forenames>Anna</forenames></author><author><keyname>Breton</keyname><forenames>Michel Le</forenames></author><author><keyname>Savvateev</keyname><forenames>Alexei</forenames></author><author><keyname>Weber</keyname><forenames>Shlomo</forenames></author></authors><title>The egalitarian sharing rule in provision of public projects</title><categories>cs.GT</categories><comments>7 pages</comments><abstract>  In this note we consider a society that partitions itself into disjoint
jurisdictions, each choosing a location of its public project and a taxation
scheme to finance it. The set of public project is multi-dimensional, and their
costs could vary from jurisdiction to jurisdiction. We impose two principles,
egalitarianism, that requires the equalization of the total cost for all agents
in the same jurisdiction, and efficiency, that implies the minimization of the
aggregate total cost within jurisdiction. We show that these two principles
always yield a core-stable partition but a Nash stable partition may fail to
exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503036</id><created>2005-03-17</created><updated>2005-10-30</updated><authors><author><keyname>Corin</keyname><forenames>R.</forenames></author><author><keyname>Etalle</keyname><forenames>S.</forenames></author><author><keyname>Hartel</keyname><forenames>P. H.</forenames></author><author><keyname>Mader</keyname><forenames>A.</forenames></author></authors><title>Timed Analysis of Security Protocols</title><categories>cs.CR</categories><abstract>  We propose a method for engineering security protocols that are aware of
timing aspects. We study a simplified version of the well-known Needham
Schroeder protocol and the complete Yahalom protocol, where timing information
allows the study of different attack scenarios. We model check the protocols
using UPPAAL. Further, a taxonomy is obtained by studying and categorising
protocols from the well known Clark Jacob library and the Security Protocol
Open Repository (SPORE) library. Finally, we present some new challenges and
threats that arise when considering time in the analysis, by providing a novel
protocol that uses time challenges and exposing a timing attack over an
implementation of an existing security protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503037</id><created>2005-03-17</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author></authors><title>Mining Top-k Approximate Frequent Patterns</title><categories>cs.DB cs.AI</categories><comments>13 pages</comments><report-no>TR-2005-0315</report-no><abstract>  Frequent pattern (itemset) mining in transactional databases is one of the
most well-studied problems in data mining. One obstacle that limits the
practical usage of frequent pattern mining is the extremely large number of
patterns generated. Such a large size of the output collection makes it
difficult for users to understand and use in practice. Even restricting the
output to the border of the frequent itemset collection does not help much in
alleviating the problem. In this paper we address the issue of overwhelmingly
large output size by introducing and studying the following problem: mining
top-k approximate frequent patterns. The union of the power sets of these k
sets should satisfy the following conditions: (1) including itemsets with
larger support as many as possible and (2) including itemsets with smaller
support as less as possible. An integrated objective function is designed to
combine these two objectives. Consequently, we derive the upper bounds on
objective function and present an approximate branch-and-bound method for
finding the feasible solution. We give empirical evidence showing that our
formulation and approximation methods work well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503038</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503038</id><created>2005-03-17</created><authors><author><keyname>Grigoryants</keyname><forenames>Armen</forenames></author></authors><title>On a Kronecker products sum distance bounds</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><abstract>  A binary linear error correcting codes represented by two code families
Kronecker products sum are considered. The dimension and distance of new code
is investigated. Upper and lower bounds of distance are obtained. Some examples
are given. It is shown that some classic constructions are the private cases of
considered one. The subclass of codes with equal lower and upper distance
bounds is allocated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503039</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503039</id><created>2005-03-17</created><updated>2011-12-19</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Notes for Miscellaneous Lectures</title><categories>cs.DM</categories><comments>5 pages</comments><journal-ref>Older version: The Grace of Quadratic Norms: Some Examples. In
  Pillars of Computer Science. Ed: A.Avron, N.Dershowitz, A.Rabinovich. Lecture
  Notes in Computer Science, 4800:457-459, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here I share a few notes I used in various course lectures, talks, etc. Some
may be just calculations that in the textbooks are more complicated, scattered,
or less specific; others may be simple observations I found useful or curious.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503040</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503040</id><created>2005-03-18</created><authors><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Schwartz</keyname><forenames>Stuart C.</forenames></author><author><keyname>Greenstein</keyname><forenames>Larry J.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Uplink Throughput in a Single-Macrocell/Single-Microcell CDMA System,
  with Application to Data Access Points</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><abstract>  This paper studies a two-tier CDMA system in which the microcell base is
converted into a data access point (DAP), i.e., a limited-range base station
that provides high-speed access to one user at a time. The microcell (or DAP)
user operates on the same frequency as the macrocell users and has the same
chip rate. However, it adapts its spreading factor, and thus its data rate, in
accordance with interference conditions. By contrast, the macrocell serves
multiple simultaneous data users, each with the same fixed rate. The
achieveable throughput for individual microcell users is examined and a simple,
accurate approximation for its probability distribution is presented.
Computations for average throughputs, both per-user and total, are also
presented. The numerical results highlight the impact of a desensitivity
parameter used in the base-selection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503041</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503041</id><created>2005-03-18</created><authors><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Greenstein</keyname><forenames>Larry J.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Schwartz</keyname><forenames>Stuart C.</forenames></author></authors><title>Soft Handoff and Uplink Capacity in a Two-Tier CDMA System</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><abstract>  This paper examines the effect of soft handoff on the uplink user capacity of
a CDMA system consisting of a single macrocell in which a single hotspot
microcell is embedded. The users of these two base stations operate over the
same frequency band. In the soft handoff scenario studied here, both macrocell
and microcell base stations serve each system user and the two received copies
of a desired user's signal are summed using maximal ratio combining. Exact and
approximate analytical methods are developed to compute uplink user capacity.
Simulation results demonstrate a 20% increase in user capacity compared to hard
handoff. In addition, simple, approximate methods are presented for estimating
soft handoff capacity and are shown to be quite accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503042</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503042</id><created>2005-03-18</created><updated>2005-03-19</updated><authors><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author><author><keyname>Greenstein</keyname><forenames>Larry J.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Schwartz</keyname><forenames>Stuart C.</forenames></author></authors><title>Uplink User Capacity in a CDMA System with Hotspot Microcells: Effects
  of Finite Transmit Power and Dispersion</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><abstract>  This paper examines the uplink user capacity in a two-tier code division
multiple access (CDMA) system with hotspot microcells when user terminal power
is limited and the wireless channel is finitely-dispersive. A
finitely-dispersive channel causes variable fading of the signal power at the
output of the RAKE receiver. First, a two-cell system composed of one macrocell
and one embedded microcell is studied and analytical methods are developed to
estimate the user capacity as a function of a dimensionless parameter that
depends on the transmit power constraint and cell radius. Next, novel
analytical methods are developed to study the effect of variable fading, both
with and without transmit power constraints. Finally, the analytical methods
are extended to estimate uplink user capacity for multicell CDMA systems,
composed of multiple macrocells and multiple embedded microcells. In all cases,
the analysis-based estimates are compared with and confirmed by simulation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503043</id><created>2005-03-18</created><updated>2005-03-29</updated><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Complexity Issues in Finding Succinct Solutions of PSPACE-Complete
  Problems</title><categories>cs.AI cs.CC cs.LO</categories><acm-class>I.2.3; I.2.4; I.2.8; F.1.3; F.2.2</acm-class><abstract>  We study the problem of deciding whether some PSPACE-complete problems have
models of bounded size. Contrary to problems in NP, models of PSPACE-complete
problems may be exponentially large. However, such models may take polynomial
space in a succinct representation. For example, the models of a QBF are
explicitely represented by and-or trees (which are always of exponential size)
but can be succinctely represented by circuits (which can be polynomial or
exponential). We investigate the complexity of deciding the existence of such
succinct models when a bound on size is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503044</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503044</id><created>2005-03-18</created><authors><author><keyname>Jia</keyname><forenames>Haixia</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Strain</keyname><forenames>Doug</forenames></author></authors><title>Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively</title><categories>cs.AI cond-mat.other cond-mat.stat-mech</categories><comments>6 pages, 7 figures</comments><abstract>  To test incomplete search algorithms for constraint satisfaction problems
such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances.
A simple way to do this is to choose a random truth assignment A, and then
choose clauses randomly from among those satisfied by A. However, this method
tends to produce easy problems, since the majority of literals point toward the
``hidden'' assignment A. Last year, Achlioptas, Jia and Moore proposed a
problem generator that cancels this effect by hiding both A and its complement.
While the resulting formulas appear to be just as hard for DPLL algorithms as
random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT
in only polynomial time. Here we propose a new method to cancel the attraction
to A, by choosing a clause with t &gt; 0 literals satisfied by A with probability
proportional to q^t for some q &lt; 1. By varying q, we can generate formulas
whose variables have no bias, i.e., which are equally likely to be true or
false; we can even cause the formula to ``deceptively'' point away from A. We
present theoretical and experimental results suggesting that these formulas are
exponentially hard both for DPLL algorithms and for incomplete algorithms such
as WalkSAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503045</id><created>2005-03-18</created><authors><author><keyname>Graham</keyname><forenames>G. E.</forenames><affiliation>Fermi National Accelerator Laboratory</affiliation></author><author><keyname>Afaq</keyname><forenames>M. Anzar</forenames><affiliation>Fermi National Accelerator Laboratory</affiliation></author><author><keyname>Evans</keyname><forenames>David</forenames><affiliation>Fermi National Accelerator Laboratory</affiliation></author><author><keyname>Guglielmo</keyname><forenames>Gerald</forenames><affiliation>Fermi National Accelerator Laboratory</affiliation></author><author><keyname>Wicklund</keyname><forenames>Eric</forenames><affiliation>Fermi National Accelerator Laboratory</affiliation></author><author><keyname>Love</keyname><forenames>Peter</forenames><affiliation>University of Lancaster</affiliation></author></authors><title>Contextual Constraint Modeling in Grid Application Workflows</title><categories>cs.DC</categories><comments>19 pages, 5 figures; Submitted to Concurrency and Computation:
  Practice and Experience for the Special Issue on Workflows at GGF10, cpe859</comments><acm-class>D.1.3; D.2.12</acm-class><abstract>  This paper introduces a new mechanism for specifying constraints in
distributed workflows. By introducing constraints in a contextual form, it is
shown how different people and groups within collaborative communities can
cooperatively constrain workflows. A comparison with existing state-of-the-art
workflow systems is made. These ideas are explored in practice with an
illustrative example from High Energy Physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503046</id><created>2005-03-19</created><authors><author><keyname>Achlioptas</keyname><forenames>Dimitris</forenames></author><author><keyname>Jia</keyname><forenames>Haixia</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Hiding Satisfying Assignments: Two are Better than One</title><categories>cs.AI cond-mat.dis-nn cond-mat.stat-mech cs.CC</categories><comments>Preliminary version appeared in AAAI 2004</comments><abstract>  The evaluation of incomplete satisfiability solvers depends critically on the
availability of hard satisfiable instances. A plausible source of such
instances consists of random k-SAT formulas whose clauses are chosen uniformly
from among all clauses satisfying some randomly chosen truth assignment A.
Unfortunately, instances generated in this manner tend to be relatively easy
and can be solved efficiently by practical heuristics. Roughly speaking, as the
formula's density increases, for a number of different algorithms, A acts as a
stronger and stronger attractor. Motivated by recent results on the geometry of
the space of satisfying truth assignments of random k-SAT and NAE-k-SAT
formulas, we introduce a simple twist on this basic model, which appears to
dramatically increase its hardness. Namely, in addition to forbidding the
clauses violated by the hidden assignment A, we also forbid the clauses
violated by its complement, so that both A and complement of A are satisfying.
It appears that under this &quot;symmetrization'' the effects of the two attractors
largely cancel out, making it much harder for algorithms to find any truth
assignment. We give theoretical and experimental evidence supporting this
assertion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503047</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503047</id><created>2005-03-21</created><authors><author><keyname>Peraki</keyname><forenames>Christina</forenames><affiliation>Cornell University</affiliation></author><author><keyname>Servetto</keyname><forenames>Sergio D.</forenames><affiliation>Cornell University</affiliation></author></authors><title>On Multiflows in Random Unit-Disk Graphs, and the Capacity of Some
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory, March 2005</comments><abstract>  We consider the capacity problem for wireless networks. Networks are modeled
as random unit-disk graphs, and the capacity problem is formulated as one of
finding the maximum value of a multicommodity flow. In this paper, we develop a
proof technique based on which we are able to obtain a tight characterization
of the solution to the linear program associated with the multiflow problem, to
within constants independent of network size. We also use this proof method to
analyze network capacity for a variety of transmitter/receiver architectures,
for which we obtain some conclusive results. These results contain as a special
case (and strengthen) those of Gupta and Kumar for random networks, for which a
new derivation is provided using only elementary counting and discrete
probability tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503048</id><created>2005-03-21</created><updated>2006-03-31</updated><authors><author><keyname>Mitra</keyname><forenames>Arindam</forenames></author></authors><title>Is entanglement necessary to have unconditional security in quantum bit
  commitment ?</title><categories>cs.CR</categories><comments>Pdf. 2 pages. Revised</comments><abstract>  A simple un-entanglement based quantum bit commitment scheme is presented.
Although commitment is unconditionally secure but concealment is not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503049</identifier>
 <datestamp>2007-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503049</id><created>2005-03-21</created><updated>2007-11-01</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author><author><keyname>Saxena</keyname><forenames>Amitabh</forenames></author></authors><title>Enforcing and Defying Associativity, Commutativity, Totality, and Strong
  Noninvertibility for One-Way Functions in Complexity Theory</title><categories>cs.CC</categories><comments>Updated version</comments><report-no>URCS-TR-2005-854</report-no><acm-class>F.1.3</acm-class><abstract>  Rabi and Sherman [RS97,RS93] proved that the hardness of factoring is a
sufficient condition for there to exist one-way functions (i.e., p-time
computable, honest, p-time noninvertible functions; this paper is in the
worst-case model, not the average-case model) that are total, commutative, and
associative but not strongly noninvertible. In this paper we improve the
sufficient condition to ``P does not equal NP.''
  More generally, in this paper we completely characterize which types of
one-way functions stand or fall together with (plain) one-way
functions--equivalently, stand or fall together with P not equaling NP. We look
at the four attributes used in Rabi and Sherman's seminal work on algebraic
properties of one-way functions (see [RS97,RS93]) and subsequent
papers--strongness (of noninvertibility), totality, commutativity, and
associativity--and for each attribute, we allow it to be required to hold,
required to fail, or ``don't care.'' In this categorization there are 3^4 = 81
potential types of one-way functions. We prove that each of these 81
feature-laden types stand or fall together with the existence of (plain)
one-way functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503050</id><created>2005-03-21</created><authors><author><keyname>Makaruk</keyname><forenames>Hanna</forenames></author><author><keyname>Owczarek</keyname><forenames>Robert</forenames></author><author><keyname>Sakhanenko</keyname><forenames>Nikita</forenames></author></authors><title>Systematic Method for Path-Complete White Box Testing</title><categories>cs.SE</categories><comments>24 pages, 3 figures, submitted to Software Testing, Verification and
  Reliability (STVR)</comments><report-no>LA-UR-05-1850</report-no><acm-class>D.2.5</acm-class><abstract>  A systematic, language-independent method of finding a minimal set of paths
covering the code of a sequential program is proposed for application in White
Box testing. Execution of all paths from the set ensures also statement
coverage. Execution fault marks problematic areas of the code. The method
starts from a UML activity diagram of a program. The diagram is transformed
into a directed graph: graph's nodes substitute decision and action points;
graph's directed edges substitute action arrows.
  The number of independent paths equals easy-to-compute cyclomatic complexity
of the graph. Association of a vector to each path creates a path vector space.
Independence of the paths is equivalent to linear independence of the vectors.
It is sufficient to test any base of the path space to complete the procedure.
An effective algorithm for choosing the base paths is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503051</id><created>2005-03-21</created><authors><author><keyname>Zhou</keyname><forenames>Fengfeng</forenames></author><author><keyname>Chen</keyname><forenames>Guoliang</forenames></author><author><keyname>Xu</keyname><forenames>Yinlong</forenames></author></authors><title>Construction of Small Worlds in the Physical Topology of Wireless
  Networks</title><categories>cs.NI</categories><abstract>  The concept of small worlds is introduced into the physical topology of
wireless networks in this work. A. Helmy provided two con- struction schemes of
small worlds for the wireless networks, link rewiring and link addition, but he
mainly focused on the virtual topology. Based on the broadcasting nature of the
radio transmission, we propose a con- struction scheme of small worlds for the
physical topology of Multiple- Input Multiple-Output (MIMO) wireless networks.
Besides the topology- related topics, we also evaluate the reduction of the
power required by a request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503052</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503052</id><created>2005-03-22</created><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Gu</keyname><forenames>Xiaoyang</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Mayordomo</keyname><forenames>Elvira</forenames></author><author><keyname>Moser</keyname><forenames>Philippe</forenames></author></authors><title>Zeta-Dimension</title><categories>cs.CC cs.IT math.IT</categories><comments>21 pages</comments><abstract>  The zeta-dimension of a set A of positive integers is the infimum s such that
the sum of the reciprocals of the s-th powers of the elements of A is finite.
  Zeta-dimension serves as a fractal dimension on the positive integers that
extends naturally usefully to discrete lattices such as the set of all integer
lattice points in d-dimensional space.
  This paper reviews the origins of zeta-dimension (which date to the
eighteenth and nineteenth centuries) and develops its basic theory, with
particular attention to its relationship with algorithmic information theory.
New results presented include extended connections between zeta-dimension and
classical fractal dimensions, a gale characterization of zeta-dimension, and a
theorem on the zeta-dimensions of pointwise sums and products of sets of
positive integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503053</id><created>2005-03-22</created><authors><author><keyname>Miravet</keyname><forenames>Carlos</forenames><affiliation>EPS-UAM, Madrid, Spain</affiliation></author><author><keyname>Rodriguez</keyname><forenames>Francisco B.</forenames><affiliation>EPS-UAM, Madrid, Spain</affiliation></author></authors><title>A hybrid MLP-PNN architecture for fast image superresolution</title><categories>cs.CV cs.MM</categories><comments>8 pages with 4 figures. ICANN/ICONIP 2003</comments><acm-class>I.4.5; I.2.6; I.5.1</acm-class><journal-ref>Lect. Notes Comput. Sc. 2714 (2003) 401-408</journal-ref><abstract>  Image superresolution methods process an input image sequence of a scene to
obtain a still image with increased resolution. Classical approaches to this
problem involve complex iterative minimization procedures, typically with high
computational costs. In this paper is proposed a novel algorithm for
super-resolution that enables a substantial decrease in computer load. First, a
probabilistic neural network architecture is used to perform a scattered-point
interpolation of the image sequence data. The network kernel function is
optimally determined for this problem by a multi-layer perceptron trained on
synthetic data. Network parameters dependence on sequence noise level is
quantitatively analyzed. This super-sampled image is spatially filtered to
correct finite pixel size effects, to yield the final high-resolution estimate.
Results on a real outdoor sequence are presented, showing the quality of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503054</id><created>2005-03-22</created><authors><author><keyname>Overhauser</keyname><forenames>A. W.</forenames></author></authors><title>Analytic Definition of Curves and Surfaces by Parabolic Blending</title><categories>cs.GR cs.GL</categories><comments>9 pages, 4 figures, technical report</comments><report-no>SL 68-40</report-no><abstract>  A procedure for interpolating between specified points of a curve or surface
is described. The method guarantees slope continuity at all junctions. A
surface panel divided into p x q contiguous patches is completely specified by
the coordinates of (p+1) x (q+1) points. Each individual patch, however,
depends parametrically on the coordinates of 16 points, allowing shape
flexibility and global conformity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503055</identifier>
 <datestamp>2009-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503055</id><created>2005-03-22</created><updated>2009-06-15</updated><authors><author><keyname>Amato</keyname><forenames>Gianluca</forenames></author><author><keyname>Scozzari</keyname><forenames>Francesca</forenames></author></authors><title>Optimality in Goal-Dependent Analysis of Sharing</title><categories>cs.PL cs.LO</categories><acm-class>F.3.2; D.1.6</acm-class><journal-ref>Theory and Practice of Logic Programming, volume 9, issue 05, pp.
  617-689, 2009</journal-ref><doi>10.1017/S1471068409990111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We face the problems of correctness, optimality and precision for the static
analysis of logic programs, using the theory of abstract interpretation. We
propose a framework with a denotational, goal-dependent semantics equipped with
two unification operators for forward unification (calling a procedure) and
backward unification (returning from a procedure). The latter is implemented
through a matching operation. Our proposal clarifies and unifies many different
frameworks and ideas on static analysis of logic programming in a single,
formal setting. On the abstract side, we focus on the domain Sharing by Jacobs
and Langen and provide the best correct approximation of all the primitive
semantic operators, namely, projection, renaming, forward and backward
unification. We show that the abstract unification operators are strictly more
precise than those in the literature defined over the same abstract domain. In
some cases, our operators are more precise than those developed for more
complex domains involving linearity and freeness.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503056</id><created>2005-03-22</created><authors><author><keyname>Miravet</keyname><forenames>Carlos</forenames><affiliation>SENER, Madrid, Spain</affiliation></author><author><keyname>Coiras</keyname><forenames>Enrique</forenames><affiliation>SENER, Madrid, Spain</affiliation></author><author><keyname>Santamaria</keyname><forenames>Javier</forenames><affiliation>SENER, Madrid, Spain</affiliation></author></authors><title>Semi-automatic vectorization of linear networks on rasterized
  cartographic maps</title><categories>cs.CV cs.MM</categories><comments>12 pages with 13 figures, in Spanish</comments><acm-class>I.4.6</acm-class><journal-ref>Revista de Teledeteccion, 10 (1998)</journal-ref><abstract>  A system for semi-automatic vectorization of linear networks (roads, rivers,
etc.) on rasterized cartographic maps is presented. In this system, human
intervention is limited to a graphic, interactive selection of the color
attributes of the information to be obtained. Using this data, the system
performs a preliminary extraction of the linear network, which is subsequently
completed, refined and vectorized by means of an automatic procedure. Results
on maps of different sources and scales are included.
  -----
  Se presenta un sistema semi-automatico de vectorizacion de redes de objetos
lineales (carreteras, rios, etc.) en mapas cartograficos digitalizados. En este
sistema, la intervencion humana queda reducida a la seleccion grafica
interactiva de los atributos de color de la informacion a obtener. Con estos
datos, el sistema realiza una extraccion preliminar de la red lineal, que se
completa, refina y vectoriza mediante un procedimiento automatico. Se presentan
resultados de la aplicacion del sistema sobre imagenes digitalizadas de mapas
de distinta procedencia y escala.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503057</id><created>2005-03-22</created><authors><author><keyname>Mandoiu</keyname><forenames>Ion I.</forenames></author><author><keyname>Trinca</keyname><forenames>Dragos</forenames></author></authors><title>Exact and Approximation Algorithms for DNA Tag Set Design</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  In this paper we propose new solution methods for designing tag sets for use
in universal DNA arrays. First, we give integer linear programming formulations
for two previous formalizations of the tag set design problem, and show that
these formulations can be solved to optimality for instance sizes of practical
interest by using general purpose optimization packages. Second, we note the
benefits of periodic tags, and establish an interesting connection between the
tag design problem and the problem of packing the maximum number of
vertex-disjoint directed cycles in a given graph. We show that combining a
simple greedy cycle packing algorithm with a previously proposed alphabetic
tree search strategy yields an increase of over 40% in the number of tags
compared to previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503058</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503058</id><created>2005-03-22</created><updated>2005-12-16</updated><authors><author><keyname>Schwartz</keyname><forenames>Moshe</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>On the Stopping Distance and the Stopping Redundancy of Codes</title><categories>cs.IT cs.DM math.IT</categories><comments>11 pages, 1 figure; to appear in the IEEE Transactions on Information
  Theory, March 2006</comments><acm-class>E.4</acm-class><abstract>  It is now well known that the performance of a linear code $C$ under
iterative decoding on a binary erasure channel (and other channels) is
determined by the size of the smallest stopping set in the Tanner graph for
$C$. Several recent papers refer to this parameter as the \emph{stopping
distance} $s$ of $C$. This is somewhat of a misnomer since the size of the
smallest stopping set in the Tanner graph for $C$ depends on the corresponding
choice of a parity-check matrix. It is easy to see that $s \le d$, where $d$ is
the minimum Hamming distance of $C$, and we show that it is always possible to
choose a parity-check matrix for $C$ (with sufficiently many dependent rows)
such that $s = d$. We thus introduce a new parameter, termed the \emph{stopping
redundancy} of $C$, defined as the minimum number of rows in a parity-check
matrix $H$ for $C$ such that the corresponding stopping distance $s(H)$ attains
its largest possible value, namely $s(H) = d$.
  We then derive general bounds on the stopping redundancy of linear codes. We
also examine several simple ways of constructing codes from other codes, and
study the effect of these constructions on the stopping redundancy.
Specifically, for the family of binary Reed-Muller codes (of all orders), we
prove that their stopping redundancy is at most a constant times their
conventional redundancy. We show that the stopping redundancies of the binary
and ternary extended Golay codes are at most 35 and 22, respectively. Finally,
we provide upper and lower bounds on the stopping redundancy of MDS codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503059</id><created>2005-03-23</created><updated>2006-04-03</updated><authors><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Laurent</forenames><affiliation>CEGELY</affiliation></author></authors><title>Les repr\'{e}sentations g\'{e}n\'{e}tiques d'objets : simples analogies
  ou mod\`{e}les pertinents ? Le point de vue de l'
  &quot;\'{e}volutique&quot;.&lt;br&gt;&amp;ndash;&amp;ndash;&amp;ndash;&lt;br&gt;Genetic representations of
  objects : simple analogies or efficient models ? The &quot;evolutic&quot; point of view</title><categories>cs.AI nlin.AO</categories><proxy>ccsd ccsd-00004547</proxy><abstract>  Depuis une trentaine d'ann\'{e}es, les ing\'{e}nieurs utilisent couramment
des analogies avec l'\'{e}volution naturelle pour optimiser des dispositifs
techniques. Le plus souvent, ces m\'{e}thodes &quot;g\'{e}n\'{e}tiques&quot; ou
&quot;\'{e}volutionnaires&quot; sont consid\'{e}r\'{e}es uniquement du point de vue
pratique, comme des m\'{e}thodes d'optimisation performantes, qu'on peut
utiliser \`{a} la place d'autres m\'{e}thodes (gradients, simplexes, ...). Dans
cet article, nous essayons de montrer que les sciences et les techniques, mais
aussi les organisations humaines, et g\'{e}n\'{e}ralement tous les syst\`{e}mes
complexes, ob\'{e}issent \`{a} des lois d'\'{e}volution dont la
g\'{e}n\'{e}tique est un bon mod\`{e}le repr\'{e}sentatif, m\^{e}me si
g\^{e}nes et chromosomes sont &quot;virtuels&quot; : ainsi loin d'\^{e}tre seulement un
outil ponctuel d'aide \`{a} la synth\`{e}se de solutions technologiques, la
repr\'{e}sentation g\'{e}n\'{e}tique est-elle un mod\`{e}le dynamique global de
l'\'{e}volution du monde fa\c{c}onn\'{e} par l'agitation
humaine.&amp;ndash;&amp;ndash;&amp;ndash;&amp;ndash;For thirty years, engineers commonly use
analogies with natural evolution to optimize technical devices. More often that
not, these &quot;genetic&quot; or &quot;evolutionary&quot; methods are only view as efficient
tools, which could replace other optimization techniques (gradient methods,
simplex, ...). In this paper, we try to show that sciences, techniques, human
organizations, and more generally all complex systems, obey to evolution rules,
whose the genetic is a good representative model, even if genes and chromosomes
are &quot;virtual&quot;. Thus, the genetic representation is not only a specific tool
helping for the design of technological solutions, but also a global and
dynamic model for the action of the human agitation on our world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503060</id><created>2005-03-23</created><authors><author><keyname>Nguyen</keyname><forenames>Quan Son</forenames><affiliation>Hanoi University of Technology, Vietnam</affiliation></author></authors><title>Multi-Dimensional Hash Chains and Application to Micropayment Schemes</title><categories>cs.CR</categories><comments>9 pages with 2 figures. Accepted for the International Workshop on
  Coding and Cryptography (WCC'2005), March 2005 in Bergen, Norway</comments><acm-class>K.4.4; D.4.6</acm-class><abstract>  One-way hash chains have been used in many micropayment schemes due to their
simplicity and efficiency. In this paper we introduce the notion of
multi-dimensional hash chains, which is a new generalization of traditional
one-way hash chains. We show that this construction has storage-computational
complexity of O(logN) per chain element, which is comparable with the best
result reported in recent literature. Based on multi-dimensional hash chains,
we then propose two cash-like micropayment schemes, which have a number of
advantages in terms of efficiency and security. We also point out some possible
improvements to PayWord and similar schemes by using multi-dimensional hash
chains
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503061</id><created>2005-03-23</created><authors><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Winsborough</keyname><forenames>William H.</forenames></author></authors><title>Integrity Constraints in Trust Management</title><categories>cs.CR cs.DB</categories><comments>An extended abstract appears in the proc. of the 10th ACM Symp. on
  Access Control Models and Technologies (SACMAT). 2005</comments><acm-class>K.6.5; D.4.6</acm-class><abstract>  We introduce the use, monitoring, and enforcement of integrity constraints in
trust management-style authorization systems. We consider what portions of the
policy state must be monitored to detect violations of integrity constraints.
Then we address the fact that not all participants in a trust management system
can be trusted to assist in such monitoring, and show how many integrity
constraints can be monitored in a conservative manner so that trusted
participants detect and report if the system enters a policy state from which
evolution in unmonitored portions of the policy could lead to a constraint
violation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503062</id><created>2005-03-23</created><updated>2005-03-23</updated><authors><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>On the Complexity of Nonrecursive XQuery and Functional Query Languages
  on Complex Values</title><categories>cs.DB cs.CC</categories><comments>Long version of PODS 2005 paper</comments><acm-class>F.4.1, H.2.3, I.7.2</acm-class><abstract>  This paper studies the complexity of evaluating functional query languages
for complex values such as monad algebra and the recursion-free fragment of
XQuery.
  We show that monad algebra with equality restricted to atomic values is
complete for the class TA[2^{O(n)}, O(n)] of problems solvable in linear
exponential time with a linear number of alternations. The monotone fragment of
monad algebra with atomic value equality but without negation is complete for
nondeterministic exponential time. For monad algebra with deep equality, we
establish TA[2^{O(n)}, O(n)] lower and exponential-space upper bounds.
  Then we study a fragment of XQuery, Core XQuery, that seems to incorporate
all the features of a query language on complex values that are traditionally
deemed essential. A close connection between monad algebra on lists and Core
XQuery (with ``child'' as the only axis) is exhibited, and it is shown that
these languages are expressively equivalent up to representation issues. We
show that Core XQuery is just as hard as monad algebra w.r.t. combined
complexity, and that it is in TC0 if the query is assumed fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503063</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503063</id><created>2005-03-23</created><authors><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Randomly Spread CDMA: Asymptotics via Statistical Physics</title><categories>cs.IT math.IT</categories><comments>To be published in IEEE Transactions on Information Theory</comments><abstract>  This paper studies randomly spread code-division multiple access (CDMA) and
multiuser detection in the large-system limit using the replica method
developed in statistical physics. Arbitrary input distributions and flat fading
are considered. A generic multiuser detector in the form of the posterior mean
estimator is applied before single-user decoding. The generic detector can be
particularized to the matched filter, decorrelator, linear MMSE detector, the
jointly or the individually optimal detector, and others. It is found that the
detection output for each user, although in general asymptotically non-Gaussian
conditioned on the transmitted symbol, converges as the number of users go to
infinity to a deterministic function of a &quot;hidden&quot; Gaussian statistic
independent of the interferers. Thus the multiuser channel can be decoupled:
Each user experiences an equivalent single-user Gaussian channel, whose
signal-to-noise ratio suffers a degradation due to the multiple-access
interference. The uncoded error performance (e.g., symbol-error-rate) and the
mutual information can then be fully characterized using the degradation
factor, also known as the multiuser efficiency, which can be obtained by
solving a pair of coupled fixed-point equations identified in this paper. Based
on a general linear vector channel model, the results are also applicable to
MIMO channels such as in multiantenna systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503064</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503064</id><created>2005-03-24</created><updated>2006-01-31</updated><authors><author><keyname>Lun</keyname><forenames>Desmond S.</forenames></author><author><keyname>Ratnakar</keyname><forenames>Niranjan</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Koetter</keyname><forenames>Ralf</forenames></author><author><keyname>Karger</keyname><forenames>David R.</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Ahmed</keyname><forenames>Ebad</forenames></author><author><keyname>Zhao</keyname><forenames>Fang</forenames></author></authors><title>Minimum-Cost Multicast over Coded Packet Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>17 pages, 6 figures; to appear in IEEE Transactions on Information
  Theory (special issue on Networking and Information Theory); revised version,
  with major changes</comments><journal-ref>IEEE Trans. Inform. Theory, vol. 52, no. 6, pp. 2608-2623, June
  2006</journal-ref><doi>10.1109/TIT.2006.874523</doi><abstract>  We consider the problem of establishing minimum-cost multicast connections
over coded packet networks, i.e. packet networks where the contents of outgoing
packets are arbitrary, causal functions of the contents of received packets. We
consider both wireline and wireless packet networks as well as both static
multicast (where membership of the multicast group remains constant for the
duration of the connection) and dynamic multicast (where membership of the
multicast group changes in time, with nodes joining and leaving the group).
  For static multicast, we reduce the problem to a polynomial-time solvable
optimization problem, and we present decentralized algorithms for solving it.
These algorithms, when coupled with existing decentralized schemes for
constructing network codes, yield a fully decentralized approach for achieving
minimum-cost multicast. By contrast, establishing minimum-cost static multicast
connections over routed packet networks is a very difficult problem even using
centralized computation, except in the special cases of unicast and broadcast
connections.
  For dynamic multicast, we reduce the problem to a dynamic programming problem
and apply the theory of dynamic programming to suggest how it may be solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503065</id><created>2005-03-24</created><authors><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LMC - IMAG</affiliation></author><author><keyname>Echahed</keyname><forenames>Rachid</forenames><affiliation>Leibniz - IMAG</affiliation></author><author><keyname>Prost</keyname><forenames>Frederic</forenames><affiliation>Leibniz - IMAG</affiliation></author></authors><title>Data-Structure Rewriting</title><categories>cs.PL cs.DS</categories><proxy>ccsd ccsd-00004558</proxy><acm-class>D1, D3, E1, F3.3, I1</acm-class><abstract>  We tackle the problem of data-structure rewriting including pointer
redirections. We propose two basic rewrite steps: (i) Local Redirection and
Replacement steps the aim of which is redirecting specific pointers determined
by means of a pattern, as well as adding new information to an existing data ;
and (ii) Global Redirection steps which are aimed to redirect all pointers
targeting a node towards another one. We define these two rewriting steps
following the double pushout approach. We define first the category of graphs
we consider and then define rewrite rules as pairs of graph homomorphisms of
the form &quot;L &lt;- K -&gt;R&quot;. Unfortunately, inverse pushouts (complement pushouts)
are not unique in our setting and pushouts do not always exist. Therefore, we
define rewriting steps so that a rewrite rule can always be performed once a
matching is found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503066</id><created>2005-03-24</created><authors><author><keyname>Ahmadinia</keyname><forenames>Ali</forenames></author><author><keyname>Bobda</keyname><forenames>Christophe</forenames></author><author><keyname>Ding</keyname><forenames>Ji</forenames></author><author><keyname>Majer</keyname><forenames>Mateusz</forenames></author><author><keyname>Teich</keyname><forenames>Juergen</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>van der Veen</keyname><forenames>Jan</forenames></author></authors><title>A Practical Approach for Circuit Routing on Dynamic Reconfigurable
  Devices</title><categories>cs.AR</categories><comments>7 pages, 7 figures, 2 tables, Latex, to appear in International
  Workshop on Rapid System Prototyping (RSP 2005)</comments><acm-class>B.7; C.5; C.3</acm-class><abstract>  Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
 (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503067</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503067</id><created>2005-03-24</created><updated>2006-01-24</updated><authors><author><keyname>Jeffrey</keyname><forenames>Alan</forenames></author><author><keyname>Rathke</keyname><forenames>Julian</forenames></author></authors><title>Contextual equivalence for higher-order pi-calculus revisited</title><categories>cs.PL</categories><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (April 21,
  2005) lmcs:768</journal-ref><doi>10.2168/LMCS-1(1:4)2005</doi><abstract>  The higher-order pi-calculus is an extension of the pi-calculus to allow
communication of abstractions of processes rather than names alone. It has been
studied intensively by Sangiorgi in his thesis where a characterisation of a
contextual equivalence for higher-order pi-calculus is provided using labelled
transition systems and normal bisimulations. Unfortunately the proof technique
used there requires a restriction of the language to only allow finite types.
We revisit this calculus and offer an alternative presentation of the labelled
transition system and a novel proof technique which allows us to provide a
fully abstract characterisation of contextual equivalence using labelled
transitions and bisimulations for higher-order pi-calculus with recursive types
also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503068</id><created>2005-03-24</created><authors><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>A Survey of Reverse Engineering and Program Comprehension</title><categories>cs.SE</categories><comments>originally released as web-only in 1996</comments><abstract>  Reverse engineering has been a standard practice in the hardware community
for some time. It has only been within the last ten years that reverse
engineering, or &quot;program comprehension&quot;, has grown into the current
sub-discipline of software engineering. Traditional software engineering is
primarily focused on the development and design of new software. However, most
programmers work on software that other people have designed and developed. Up
to 50% of a software maintainers time can be spent determining the intent of
source code. The growing demand to reevaluate and reimplement legacy software
systems, brought on by the proliferation of clientserver and World Wide Web
technologies, has underscored the need for reverse engineering tools and
techniques. This paper introduces the terminology of reverse engineering and
gives some of the obstacles that make reverse engineering difficult. Although
reverse engineering remains heavily dependent on the human component, a number
of automated tools are presented that aid the reverse engineer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503069</id><created>2005-03-24</created><authors><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Harrison</keyname><forenames>Terry L.</forenames></author><author><keyname>McFarland</keyname><forenames>Nathan</forenames></author></authors><title>mod_oai: An Apache Module for Metadata Harvesting</title><categories>cs.DL</categories><abstract>  We describe mod_oai, an Apache 2.0 module that implements the Open Archives
Initiative Protocol for Metadata Harvesting (OAI-PMH). OAIPMH is the de facto
standard for metadata exchange in digital libraries and allows repositories to
expose their contents in a structured, application-neutral format with
semantics optimized for accurate incremental harvesting. Current
implementations of OAI-PMH are either separate applications that access an
existing repository, or are built-in to repository software packages. mod_oai
is different in that it optimizes harvesting web content by building OAI-PMH
capability into the Apache server. We discuss the implications of adding
harvesting capability to an Apache server and describe our initial experimental
results accessing a departmental web site using both web crawling and OAIPMH
harvesting techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503070</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503070</id><created>2005-03-24</created><authors><author><keyname>Neirotti</keyname><forenames>Juan P.</forenames></author><author><keyname>Saad</keyname><forenames>David</forenames></author></authors><title>Improved message passing for inference in densely connected systems</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><doi>10.1209/epl/i2005-10148-5</doi><abstract>  An improved inference method for densely connected systems is presented. The
approach is based on passing condensed messages between variables, representing
macroscopic averages of microscopic messages. We extend previous work that
showed promising results in cases where the solution space is contiguous to
cases where fragmentation occurs. We apply the method to the signal detection
problem of Code Division Multiple Access (CDMA) for demonstrating its
potential. A highly efficient practical algorithm is also derived on the basis
of insight gained from the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503071</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503071</id><created>2005-03-26</created><updated>2005-09-29</updated><authors><author><keyname>Predd</keyname><forenames>Joel B.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Consistency in Models for Distributed Learning under Communication
  Constraints</title><categories>cs.IT cs.LG math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory</comments><acm-class>G.3; I.2.6; I.2.11; I.5.1</acm-class><abstract>  Motivated by sensor networks and other distributed settings, several models
for distributed learning are presented. The models differ from classical works
in statistical pattern recognition by allocating observations of an independent
and identically distributed (i.i.d.) sampling process amongst members of a
network of simple learning agents. The agents are limited in their ability to
communicate to a central fusion center and thus, the amount of information
available for use in classification or regression is constrained. For several
basic communication models in both the binary classification and regression
frameworks, we question the existence of agent decision rules and fusion rules
that result in a universally consistent ensemble. The answers to this question
present new issues to consider with regard to universal consistency. Insofar as
these models present a useful picture of distributed scenarios, this paper
addresses the issue of whether or not the guarantees provided by Stone's
Theorem in centralized environments hold in distributed settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503072</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503072</id><created>2005-03-26</created><authors><author><keyname>Predd</keyname><forenames>Joel B.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Distributed Learning in Wireless Sensor Networks</title><categories>cs.IT cs.LG math.IT</categories><comments>Published in the Proceedings of the 42nd Annual Allerton Conference
  on Communication, Control and Computing, University of Illinois, 2004</comments><acm-class>C.2.1; I.2.6; G3</acm-class><doi>10.1109/MSP.2006.1657817</doi><abstract>  The problem of distributed or decentralized detection and estimation in
applications such as wireless sensor networks has often been considered in the
framework of parametric models, in which strong assumptions are made about a
statistical description of nature. In certain applications, such assumptions
are warranted and systems designed from these models show promise. However, in
other scenarios, prior knowledge is at best vague and translating such
knowledge into a statistical model is undesirable. Applications such as these
pave the way for a nonparametric study of distributed detection and estimation.
In this paper, we review recent work of the authors in which some elementary
models for distributed learning are considered. These models are in the spirit
of classical work in nonparametric statistics and are applicable to wireless
sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503073</id><created>2005-03-26</created><updated>2005-05-25</updated><authors><author><keyname>Toth</keyname><forenames>Viktor</forenames></author></authors><title>Tensor manipulation in GPL Maxima</title><categories>cs.SC</categories><comments>26 pages (some formulae fixed, terminology usage corrected, comments
  added on improved tensor contraction algorithm)</comments><abstract>  GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA.
GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but
these were in various states of disrepair. One of the two packages, CTENSOR,
implemented component-based tensor manipulation; the other, ITENSOR, treated
tensor symbols as opaque, manipulating them based on their index properties.
The present paper describes the state in which these packages were found, the
steps that were needed to make the packages fully functional again, and the new
functionality that was implemented to make them more versatile. A third
package, ATENSOR, was also implemented; fully compatible with the identically
named package in the commercial version of MACSYMA, ATENSOR implements abstract
tensor algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503074</id><created>2005-03-28</created><authors><author><keyname>Tilak</keyname><forenames>Sameer</forenames></author><author><keyname>Pisupati</keyname><forenames>Bhanu</forenames></author><author><keyname>Chiu</keyname><forenames>Kenneth</forenames></author><author><keyname>Brown</keyname><forenames>Geoffrey</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael</forenames></author></authors><title>A File System Abstraction for Sense and Respond Systems</title><categories>cs.NI cs.OS</categories><comments>6 pages, 3 figures Workshop on End-to-End, Sense-and-Respond Systems,
  Applications, and Services In conjunction with MobiSys '05</comments><acm-class>D.4.3 Distributed file systems; C.2.1 Wireless communication</acm-class><abstract>  The heterogeneity and resource constraints of sense-and-respond systems pose
significant challenges to system and application development. In this paper, we
present a flexible, intuitive file system abstraction for organizing and
managing sense-and-respond systems based on the Plan 9 design principles. A key
feature of this abstraction is the ability to support multiple views of the
system via filesystem namespaces. Constructed logical views present an
application-specific representation of the network, thus enabling high-level
programming of the network. Concurrently, structural views of the network
enable resource-efficient planning and execution of tasks. We present and
motivate the design using several examples, outline research challenges and our
research plan to address them, and describe the current state of
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503075</id><created>2005-03-28</created><updated>2005-07-01</updated><authors><author><keyname>Ng</keyname><forenames>W. -Y.</forenames></author><author><keyname>Lin</keyname><forenames>W. K.</forenames></author><author><keyname>Chiu</keyname><forenames>D. M.</forenames></author></authors><title>Statistical Modelling of Information Sharing: Community, Membership and
  Content</title><categories>cs.NI cond-mat.stat-mech physics.soc-ph</categories><comments>accepted in International Symposium on Computer Performance,
  Modeling, Measurements and Evaluation, Juan-les-Pins, France, October-2005</comments><abstract>  File-sharing systems, like many online and traditional information sharing
communities (e.g. newsgroups, BBS, forums, interest clubs), are dynamical
systems in nature. As peers get in and out of the system, the information
content made available by the prevailing membership varies continually in
amount as well as composition, which in turn affects all peers' join/leave
decisions. As a result, the dynamics of membership and information content are
strongly coupled, suggesting interesting issues about growth, sustenance and
stability.
  In this paper, we propose to study such communities with a simple statistical
model of an information sharing club. Carrying their private payloads of
information goods as potential supply to the club, peers join or leave on the
basis of whether the information they demand is currently available.
Information goods are chunked and typed, as in a file sharing system where
peers contribute different files, or a forum where messages are grouped by
topics or threads. Peers' demand and supply are then characterized by
statistical distributions over the type domain.
  This model reveals interesting critical behaviour with multiple equilibria. A
sharp growth threshold is derived: the club may grow towards a sustainable
equilibrium only if the value of an order parameter is above the threshold, or
shrink to emptiness otherwise. The order parameter is composite and comprises
the peer population size, the level of their contributed supply, the club's
efficiency in information search, the spread of supply and demand over the type
domain, as well as the goodness of match between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503076</id><created>2005-03-28</created><authors><author><keyname>Meingast</keyname><forenames>Marci</forenames></author><author><keyname>Geyer</keyname><forenames>Christopher</forenames></author><author><keyname>Sastry</keyname><forenames>Shankar</forenames></author></authors><title>Geometric Models of Rolling-Shutter Cameras</title><categories>cs.CV cs.RO</categories><abstract>  Cameras with rolling shutters are becoming more common as low-power, low-cost
CMOS sensors are being used more frequently in cameras. The rolling shutter
means that not all scanlines are exposed over the same time interval. The
effects of a rolling shutter are noticeable when either the camera or objects
in the scene are moving and can lead to systematic biases in projection
estimation. We develop a general projection equation for a rolling shutter
camera and show how it is affected by different types of camera motion. In the
case of fronto-parallel motion, we show how that camera can be modeled as an
X-slit camera. We also develop approximate projection equations for a non-zero
angular velocity about the optical axis and approximate the projection equation
for a constant velocity screw motion. We demonstrate how the rolling shutter
effects the projective geometry of the camera and in turn the
structure-from-motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503077</id><created>2005-03-28</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author><author><keyname>Riley</keyname><forenames>Michael</forenames></author></authors><title>Weighted Automata in Text and Speech Processing</title><categories>cs.CL cs.HC</categories><acm-class>F.1.1; F.4.3; H.5.2</acm-class><journal-ref>Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted
  Automata in Text and Speech Processing. In Proceedings of the 12th biennial
  European Conference on Artificial Intelligence (ECAI-96), Workshop on
  Extended finite state models of language. Budapest, Hungary, 1996. John Wiley
  and Sons, Chichester</journal-ref><abstract>  Finite-state automata are a very effective tool in natural language
processing. However, in a variety of applications and especially in speech
precessing, it is necessary to consider more general machines in which arcs are
assigned weights or costs. We briefly describe some of the main theoretical and
algorithmic aspects of these machines. In particular, we describe an efficient
composition algorithm for weighted transducers, and give examples illustrating
the value of determinization and minimization algorithms for weighted automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503078</id><created>2005-03-29</created><authors><author><keyname>Pagliosa</keyname><forenames>Angelo Luis</forenames></author><author><keyname>de Sa</keyname><forenames>Claudio Cesar</forenames></author><author><keyname>Sasse</keyname><forenames>Fernando D.</forenames></author></authors><title>Obtaining Membership Functions from a Neuron Fuzzy System extended by
  Kohonen Network</title><categories>cs.NE</categories><comments>6 pages, 6 figures, 5th Congress of Logic Applied to Technology
  (LAPTEC 2005) Himeji, Japan, April 2-6, 2005</comments><acm-class>C.1.3; I.2.6</acm-class><abstract>  This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network
(NFN-MK), an hybrid computational model that combines fuzzy system technique
and artificial neural networks. Its main task consists in the automatic
generation of membership functions, in particular, triangle forms, aiming a
dynamic modeling of a system. The model is tested by simulating real systems,
here represented by a nonlinear mathematical function. Comparison with the
results obtained by traditional neural networks, and correlated studies of
neurofuzzy systems applied in system identification area, shows that the NFN-MK
model has a similar performance, despite its greater simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503079</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503079</id><created>2005-03-29</created><authors><author><keyname>Prikhod'ko</keyname><forenames>A. A.</forenames></author><author><keyname>Prikhod'ko</keyname><forenames>N. A.</forenames></author></authors><title>Space-time databases modeling global semantic networks</title><categories>cs.IT cs.IR math.IT</categories><comments>9 pages, 2 figures, best view in PDF</comments><acm-class>E.1; H.1.1; H.3.4; H.3.7</acm-class><abstract>  This paper represents an approach to creating global knowledge systems, using
new philosophy and infrastructure of global distributed semantic network (frame
knowledge representation system) based on the space-time database construction.
The main idea of the space-time database environment introduced in the paper is
to bind a document (an information frame, a knowledge) to a special kind of
entity, that we call permanent entity, -- an object without history and
evolution, described by a &quot;point&quot; in the generalized, informational space-time
(not an evolving object in the real space having history). For documents
(information) it means that document content is unchangeable, and documents are
absolutely persistent. This approach leads to new knowledge representation and
retreival techniques. We discuss the way of applying the concept to a global
distributed scientific library and scientific workspace. Some practical aspects
of the work are elaborated by the open IT project at
http://sourceforge.net/projects/gil/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503080</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503080</id><created>2005-03-29</created><authors><author><keyname>Hermann</keyname><forenames>Uwe</forenames></author><author><keyname>Katzenbeisser</keyname><forenames>Stefan</forenames></author><author><keyname>Schallhart</keyname><forenames>Christian</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author></authors><title>Enforcing Semantic Integrity on Untrusted Clients in Networked Virtual
  Environments</title><categories>cs.CR</categories><abstract>  During the last years, large-scale simulations of realistic physical
environments which support the interaction of multiple participants over the
Internet have become increasingly available and economically significant, most
notably in the computer gaming industry. Such systems, commonly called
networked virtual environments (NVEs), are usually based on a client-server
architecture where for performance reasons and bandwidth restrictions, the
simulation is partially deferred to the clients. This inevitable architectural
choice renders the simulation vulnerable to attacks against the semantic
integrity of the simulation: malicious clients may attempt to compromise the
physical and logical laws governing the simulation, or to alter the causality
of events a posteriori. In this paper, we initiate the systematic study of
semantic integrity in NVEs from a security point of view. We argue that naive
policies to enforce semantic integrity involve intolerable network load, and
are therefore not practically feasible. We present a new semantic integrity
protocol based on cryptographic primitives which enables the server system to
audit the local computations of the clients on demand. Our approach facilitates
low network and CPU load, incurs reasonable engineering overhead, and maximally
decouples the auditing process from the soft real time constraints of the
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503081</id><created>2005-03-29</created><authors><author><keyname>He</keyname><forenames>Zengyou</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Deng</keyname><forenames>Shengchun</forenames></author></authors><title>An Optimization Model for Outlier Detection in Categorical Data</title><categories>cs.DB cs.AI</categories><comments>12 pages</comments><report-no>Tr-05-0329</report-no><abstract>  The task of outlier detection is to find small groups of data objects that
are exceptional when compared with rest large amount of data. Detection of such
outliers is important for many applications such as fraud detection and
customer migration. Most existing methods are designed for numeric data. They
will encounter problems with real-life applications that contain categorical
data. In this paper, we formally define the problem of outlier detection in
categorical data as an optimization problem from a global viewpoint. Moreover,
we present a local-search heuristic based algorithm for efficiently finding
feasible solutions. Experimental results on real datasets and large synthetic
datasets demonstrate the superiority of our model and algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503082</id><created>2005-03-29</created><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author><author><keyname>Boettcher</keyname><forenames>Stefan</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Spines of Random Constraint Satisfaction Problems: Definition and
  Connection with Computational Complexity</title><categories>cs.CC cond-mat.dis-nn cs.AI</categories><comments>A revised version of this paper will appear in Annals of Mathematics
  and Artificial Intelligence</comments><abstract>  We study the connection between the order of phase transitions in
combinatorial problems and the complexity of decision algorithms for such
problems. We rigorously show that, for a class of random constraint
satisfaction problems, a limited connection between the two phenomena indeed
exists. Specifically, we extend the definition of the spine order parameter of
Bollobas et al. to random constraint satisfaction problems, rigorously showing
that for such problems a discontinuity of the spine is associated with a
$2^{\Omega(n)}$ resolution complexity (and thus a $2^{\Omega(n)}$ complexity of
DPLL algorithms) on random instances. The two phenomena have a common
underlying cause: the emergence of ``large'' (linear size) minimally
unsatisfiable subformulas of a random formula at the satisfiability phase
transition.
  We present several further results that add weight to the intuition that
random constraint satisfaction problems with a sharp threshold and a continuous
spine are ``qualitatively similar to random 2-SAT''. Finally, we argue that it
is the spine rather than the backbone parameter whose continuity has
implications for the decision complexity of combinatorial problems, and we
provide experimental evidence that the two parameters can behave in a different
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503083</id><created>2005-03-29</created><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Coarse and Sharp Thresholds of Boolean Constraint Satisfaction Problems</title><categories>cs.DM cs.CC</categories><comments>A revised version of this paper will appear in Discrete Applied
  Mathematics</comments><abstract>  We study threshold properties of random constraint satisfaction problems
under a probabilistic model due to Molloy. We give a sufficient condition for
the existence of a sharp threshold that leads (for boolean constraints) to a
necessary and sufficient for the existence of a sharp threshold in the case
where constraint templates are applied with equal probability, solving thus an
open problem of Creignou and Daude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503084</id><created>2005-03-30</created><authors><author><keyname>Nefyodov</keyname><forenames>P.</forenames></author><author><keyname>Reztsov</keyname><forenames>V.</forenames></author><author><keyname>Riabinina</keyname><forenames>O.</forenames></author></authors><title>The Peculiarities of Nonstationary Formation of Inhomogeneous Structures
  of Charged Particles in the Electrodiffusion Processes</title><categories>cs.CE</categories><comments>6 pages, 6 figures The example of our scientific work</comments><abstract>  In this paper the distribution of charged particles is constructed under the
approximation of ambipolar diffusion. The results of mathematical modelling in
two-dimensional case taking into account the velocities of the system are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503085</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503085</id><created>2005-03-30</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>Dynamic Shannon Coding</title><categories>cs.IT math.IT</categories><comments>6 pages; conference version presented at ESA 2004; journal version
  submitted to IEEE Transactions on Information Theory</comments><acm-class>E.4</acm-class><abstract>  We present a new algorithm for dynamic prefix-free coding, based on Shannon
coding. We give a simple analysis and prove a better upper bound on the length
of the encoding produced than the corresponding bound for dynamic Huffman
coding. We show how our algorithm can be modified for efficient
length-restricted coding, alphabetic coding and coding with unequal letter
costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503086</id><created>2005-03-30</created><updated>2006-01-16</updated><authors><author><keyname>Morlier</keyname><forenames>Joseph</forenames><affiliation>LRBB</affiliation></author></authors><title>Segmentation of the Homogeneity of a Signal Using a Piecewise Linear
  Recognition Tool</title><categories>cs.NA</categories><comments>This preprint could help me to receive comments on my work in order
  to enhance the paper to be submitted</comments><proxy>ccsd ccsd-00004598</proxy><acm-class>I.5</acm-class><abstract>  In this paper a new method of detection of homogeneous zones and singularity
parts of a 1D signal is proposed. The entropy function is used to transform
signal in piecewise linear one. The multiple regression permits to detect lines
and project them in the Hough parameters space in order to easily recognise
homogeneous zone and abrupt changes of the signal. Two application examples are
analysed, the first is a classical fractal signal and the other is issued from
a dynamic mechanical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503087</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503087</id><created>2005-03-30</created><updated>2011-08-28</updated><authors><author><keyname>Filla</keyname><forenames>Reno</forenames><affiliation>Volvo Wheel Loaders AB</affiliation></author><author><keyname>Ericsson</keyname><forenames>Allan</forenames><affiliation>Volvo Wheel Loaders AB</affiliation></author><author><keyname>Palmberg</keyname><forenames>Jan-Ove</forenames><affiliation>Linkoping University</affiliation></author></authors><title>Dynamic Simulation of Construction Machinery: Towards an Operator Model</title><categories>cs.CE</categories><acm-class>I.6.3; I.6.5; J.6</acm-class><journal-ref>International Fluid Power Exhibition 2005 Technical Conference,
  pp. 429-438</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic simulation of complete wheel loaders, one interesting aspect,
specific for the working task, is the momentary power distribution between
drive train and hydraulics, which is balanced by the operator.
  This paper presents the initial results to a simulation model of a human
operator. Rather than letting the operator model follow a predefined path with
control inputs at given points, it follows a collection of general rules that
together describe the machine's working cycle in a generic way. The advantage
of this is that the working task description and the operator model itself are
independent of the machine's technical parameters. Complete sub-system
characteristics can thus be changed without compromising the relevance and
validity of the simulation. Ultimately, this can be used to assess a machine's
total performance, fuel efficiency and operability already in the concept phase
of the product development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503088</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503088</id><created>2005-03-31</created><updated>2005-12-22</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>General non-asymptotic and asymptotic formulas in channel resolvability
  and identification capacity and their application to wire-tap channel</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Vol. 52, No. 4, 1562-1575
  (2006)</journal-ref><abstract>  Several non-asymptotic formulas are established in channel resolvability and
identification capacity, and they are applied to wire-tap channel. By using
these formulas, the $\epsilon$ capacities of the above three problems are
considered in the most general setting, where no structural assumptions such as
the stationary memoryless property are made on a channel. As a result, we solve
an open problem proposed in Han &amp; Verdu and Han. Moreover, we obtain lower
bounds of the exponents of error probability and the wire-tapper's information
in wire-tap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503089</identifier>
 <datestamp>2010-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503089</id><created>2005-03-31</created><updated>2006-08-20</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Second order asymptotics in fixed-length source coding and intrinsic
  randomness</title><categories>cs.IT math.IT</categories><journal-ref>Transactions on Information Theory, 54, 4619 - 4637 (2008)</journal-ref><abstract>  Second order asymptotics of fixed-length source coding and intrinsic
randomness is discussed with a constant error constraint. There was a
difference between optimal rates of fixed-length source coding and intrinsic
randomness, which never occurred in the first order asymptotics. In addition,
the relation between uniform distribution and compressed data is discussed
based on this fact. These results are valid for general information sources as
well as independent and identical distributions. A universal code attaining the
second order optimal rate is also constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503090</id><created>2005-03-31</created><authors><author><keyname>Arrowsmith</keyname><forenames>David</forenames></author><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author><author><keyname>Sorrentino</keyname><forenames>Francesco</forenames></author></authors><title>Effects of variations of load distribution on network performance</title><categories>cs.NI</categories><comments>4 pages, 4 figures, included in conference proceedings ISCAS 2005,
  Kobe Japan</comments><abstract>  This paper is concerned with the characterization of the relationship between
topology and traffic dynamics. We use a model of network generation that allows
the transition from random to scale free networks. Specifically, we consider
three different topological types of network: random, scale-free with \gamma =
3, scale-free with \gamma = 2. By using a novel LRD traffic generator, we
observe best performance, in terms of transmission rates and delivered packets,
in the case of random networks. We show that, even if scale-free networks are
characterized by shorter characteristic-path- length (the lower the exponent,
the lower the path-length), they show worst performances in terms of
communication. We conjecture this could be explained in terms of changes in the
load distribution, defined here as the number of shortest paths going through a
given vertex. In fact, that distribu- tion is characterized by (i) a decreasing
mean (ii) an increas- ing standard deviation, as the networks becomes
scale-free (especially scale-free networks with low exponents). The use of a
degree-independent server also discriminates against a scale-free structure. As
a result, since the model is un- controlled, most packets will go through the
same vertices, favoring the onset of congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503091</id><created>2005-03-31</created><authors><author><keyname>Okamoto</keyname><forenames>Tatsuaki</forenames></author><author><keyname>Kashima</keyname><forenames>Ryo</forenames></author></authors><title>Resource Bounded Unprovability of Computational Lower Bounds</title><categories>cs.CC cs.LO</categories><comments>78 pages</comments><acm-class>F.1.3; F.4.1; F.2.2</acm-class><abstract>  This paper introduces new notions of asymptotic proofs,
PT(polynomial-time)-extensions, PTM(polynomial-time Turing
machine)-omega-consistency, etc. on formal theories of arithmetic including PA
(Peano Arithmetic). This paper shows that P not= NP (more generally, any
super-polynomial-time lower bound in PSPACE) is unprovable in a
PTM-omega-consistent theory T, where T is a consistent PT-extension of PA. This
result gives a unified view to the existing two major negative results on
proving P not= NP, Natural Proofs and relativizable proofs, through the two
manners of characterization of PTM-omega-consistency. We also show that the
PTM-omega-consistency of T cannot be proven in any PTM-omega-consistent theory
S, where S is a consistent PT-extension of T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0503092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0503092</id><created>2005-03-31</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author><author><keyname>Song</keyname><forenames>Joyce</forenames></author></authors><title>Monotonic and Nonmonotonic Preference Revision</title><categories>cs.DB cs.AI</categories><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  We study here preference revision, considering both the monotonic case where
the original preferences are preserved and the nonmonotonic case where the new
preferences may override the original ones. We use a relational framework in
which preferences are represented using binary relations (not necessarily
finite). We identify several classes of revisions that preserve order axioms,
for example the axioms of strict partial or weak orders. We consider
applications of our results to preference querying in relational databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504001</identifier>
 <datestamp>2008-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504001</id><created>2005-03-31</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author></authors><title>Probabilistic and Team PFIN-type Learning: General Properties</title><categories>cs.LG</categories><comments>49 pages, 1 figure, journal version of COLT'96 paper</comments><acm-class>F.1.1, I.2.6</acm-class><journal-ref>Journal of Computer and System Sciences, 74(4):457-489, 2008</journal-ref><abstract>  We consider the probability hierarchy for Popperian FINite learning and study
the general properties of this hierarchy. We prove that the probability
hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and
p_2 and answers whether PFIN-type learning with the probability of success p_1
is equivalent to PFIN-type learning with the probability of success p_2.
  To prove our result, we analyze the topological structure of the probability
hierarchy. We prove that it is well-ordered in descending ordering and
order-equivalent to ordinal epsilon_0. This shows that the structure of the
hierarchy is very complicated.
  Using similar methods, we also prove that, for PFIN-type learning, team
learning and probabilistic learning are of the same power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504002</id><created>2005-04-01</created><authors><author><keyname>Han</keyname><forenames>Seon-Yeong</forenames></author><author><keyname>Abu-Ghazaleh</keyname><forenames>Nael</forenames></author></authors><title>On the Effect of Fading on Ad hoc Networking</title><categories>cs.NI</categories><comments>12 pages; 14 figures</comments><abstract>  Most MANET (Mobile Ad hoc NETwork) research assumes idealized propagation
models. Experimental results have shown significant divergence from simulation
results due to the effect of signal fading in realistic wireless communication
channels. In this paper, we characterize the impact of fading on protocol
performance. We first study the effect of fading on MAC performance and show
that its effect can be dominating. One of our important conclusions is that
eliminating RTS/CTS packets results in more effective operation under fading.
We also identify an unfairness problem that arises due to backoffs in the
presence of fading. Moreover, fading results in several subtle interactions
between the MAC and routing layers. We identify several of these problems and
make observations about effective approaches for addressing them. For example,
the criteria for determining the best path should not only consider the link
status but also the link order. In addition, because routing protocols rely on
MAC level transmission failure (when the retry limit is exceeded), route
failure errors are often generated unnecessarily. Finally, because MAC level
broadcasts are unreliable, they are especially vulnerable to fading. We analyze
these effects and outline preliminary solutions to them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504003</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504003</id><created>2005-04-01</created><authors><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Berger</keyname><forenames>Toby</forenames></author><author><keyname>Hemami</keyname><forenames>Sheila</forenames></author></authors><title>Multiple Description Quantization via Gram-Schmidt Orthogonalization</title><categories>cs.IT math.IT</categories><comments>48 pages; submitted to IEEE Transactions on Information Theory</comments><abstract>  The multiple description (MD) problem has received considerable attention as
a model of information transmission over unreliable channels. A general
framework for designing efficient multiple description quantization schemes is
proposed in this paper. We provide a systematic treatment of the El Gamal-Cover
(EGC) achievable MD rate-distortion region, and show that any point in the EGC
region can be achieved via a successive quantization scheme along with
quantization splitting. For the quadratic Gaussian case, the proposed scheme
has an intrinsic connection with the Gram-Schmidt orthogonalization, which
implies that the whole Gaussian MD rate-distortion region is achievable with a
sequential dithered lattice-based quantization scheme as the dimension of the
(optimal) lattice quantizers becomes large. Moreover, this scheme is shown to
be universal for all i.i.d. smooth sources with performance no worse than that
for an i.i.d. Gaussian source with the same variance and asymptotically optimal
at high resolution. A class of low-complexity MD scalar quantizers in the
proposed general framework also is constructed and is illustrated
geometrically; the performance is analyzed in the high resolution regime, which
exhibits a noticeable improvement over the existing MD scalar quantization
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504004</id><created>2005-04-02</created><authors><author><keyname>Bostelmann</keyname><forenames>Henning</forenames></author></authors><title>Statistical analysis of quality measures for mobile ad hoc networks</title><categories>cs.NI cs.DM</categories><comments>Master's thesis; 78 pages, 10 figures</comments><acm-class>C.2.1; G.3</acm-class><abstract>  How can the quality of a mobile ad hoc network (MANET) be quantified? This
work aims at an answer based on the lower network layers, i.e. on connectivity
between the wireless nodes, using statistical methods. A number of different
quality measures are introduced and classified according to their scaling
behaviour. They are analysed in a statistical model of a 1-dimensional MANET
system (corresponding e.g. to cars on a road). Neglecting boundary effects, the
model turns out to be exactly solvable, so that explicit analytical results for
the quality levels can be obtained both at fixed system size and in the limit
of large systems. In particular, this improves estimates known in the
literature for the probability of connectedness of 1-dimensional MANETs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504005</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504005</id><created>2005-04-02</created><authors><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author><author><keyname>Astola</keyname><forenames>Jaakko</forenames></author><author><keyname>Egiazarian</keyname><forenames>Karen</forenames></author></authors><title>Fast Codes for Large Alphabets</title><categories>cs.IT math.IT</categories><comments>published</comments><abstract>  We address the problem of constructing a fast lossless code in the case when
the source alphabet is large. The main idea of the new scheme may be described
as follows. We group letters with small probabilities in subsets (acting as
super letters) and use time consuming coding for these subsets only, whereas
letters in the subsets have the same code length and therefore can be coded
fast. The described scheme can be applied to sources with known and unknown
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504006</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504006</id><created>2005-04-03</created><authors><author><keyname>Ryabko</keyname><forenames>B. Ya.</forenames></author><author><keyname>Monarev</keyname><forenames>V. A.</forenames></author></authors><title>Using Information Theory Approach to Randomness Testing</title><categories>cs.IT math.IT</categories><comments>Journal of Statistical Planning and Inference,2005, (accepted)</comments><abstract>  We address the problem of detecting deviations of binary sequence from
randomness,which is very important for random number (RNG) and pseudorandom
number generators (PRNG). Namely, we consider a null hypothesis $H_0$ that a
given bit sequence is generated by Bernoulli source with equal probabilities of
0 and 1 and the alternative hypothesis $H_1$ that the sequence is generated by
a stationary and ergodic source which differs from the source under $H_0$. We
show that data compression methods can be used as a basis for such testing and
describe two new tests for randomness, which are based on ideas of universal
coding. Known statistical tests and suggested ones are applied for testing
PRNGs. Those experiments show that the power of the new tests is greater than
of many known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504007</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504007</id><created>2005-04-03</created><authors><author><keyname>Turner</keyname><forenames>David Michael</forenames></author><author><keyname>Prevelakis</keyname><forenames>Vassilis</forenames></author><author><keyname>Keromytis</keyname><forenames>Angelos D.</forenames></author></authors><title>The Bandwidth Exchange Architecture</title><categories>cs.NI cs.CR</categories><comments>8 pages, 6 figures</comments><report-no>DU-CS-05-03</report-no><abstract>  New applications for the Internet such as video on demand, grid computing
etc. depend on the availability of high bandwidth connections with acceptable
Quality of Service (QoS). There appears to be, therefore, a requirement for a
market where bandwidth-related transactions can take place. For this market to
be effective, it must be efficient for both the provider (seller) and the user
(buyer) of the bandwidth. This implies that: (a) the buyer must have a wide
choice of providers that operate in a competitive environment, (b) the seller
must be assured that a QoS transaction will be paid by the customer, and (c)
the QoS transaction establishment must have low overheads so that it may be
used by individual customers without a significant burden to the provider.
  In order to satisfy these requirements, we propose a framework that allows
customers to purchase bandwidth using an open market where providers advertise
links and capacities and customers bid for these services. The model is close
to that of a commodities market that offers both advance bookings (futures) and
a spot market. We explore the mechanisms that can support such a model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504008</identifier>
 <datestamp>2009-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504008</id><created>2005-04-04</created><updated>2009-08-25</updated><authors><author><keyname>G</keyname><forenames>Raju Renjit.</forenames></author></authors><title>Super Object Oriented Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This submission has been withdrawn at the request of the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504009</id><created>2005-04-04</created><authors><author><keyname>Srinivasan</keyname><forenames>N.</forenames></author><author><keyname>Sanjeevakumar</keyname><forenames>C.</forenames></author><author><keyname>Sudarsan</keyname><forenames>L.</forenames></author><author><keyname>Rajan</keyname><forenames>M. Kasi</forenames></author><author><keyname>Venkatesh</keyname><forenames>R.</forenames></author></authors><title>Towards a Group Theoretic Quantum Encryption Scheme Based on Generalized
  Hidden Subgroup Problem</title><categories>cs.DM cs.CR</categories><comments>8 pages, work of undergraduate students</comments><abstract>  This paper introduces a completely new approach to encryption based on group
theoretic quantum framework. Quantum cryptography has essentially focused only
on key distribution and proceeded with classical encryption algorithm with the
generated key. Here, we present a first step towards a quantum encryption
scheme based on the solution for the hidden subgroup problem. The shared secret
key K from QKD evolves as a generator for a subgroup H of a group G, in
combination of the plain text data modeled as group elements. The key K helps
in regeneration of the plain data on the receiver's side based on subgroup
reconstruction. This paper models all quantum computations using group
representations. A non-constructive proof is attempted towards the security of
the encryption scheme. We also address the issues involved in a such a venture
into the realms of Quantum data encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504010</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504010</id><created>2005-04-04</created><authors><author><keyname>Boykin</keyname><forenames>P. Oscar</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani P.</forenames></author></authors><title>Reversible Fault-Tolerant Logic</title><categories>cs.IT math.IT quant-ph</categories><comments>10 pages, to appear in DSN 2005</comments><abstract>  It is now widely accepted that the CMOS technology implementing irreversible
logic will hit a scaling limit beyond 2016, and that the increased power
dissipation is a major limiting factor. Reversible computing can potentially
require arbitrarily small amounts of energy. Recently several nano-scale
devices which have the potential to scale, and which naturally perform
reversible logic, have emerged. This paper addresses several fundamental issues
that need to be addressed before any nano-scale reversible computing systems
can be realized, including reliability and performance trade-offs and
architecture optimization. Many nano-scale devices will be limited to only near
neighbor interactions, requiring careful optimization of circuits. We provide
efficient fault-tolerant (FT) circuits when restricted to both 2D and 1D.
Finally, we compute bounds on the entropy (and hence, heat) generated by our FT
circuits and provide quantitative estimates on how large can we make our
circuits before we lose any advantage over irreversible computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504011</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504011</id><created>2005-04-05</created><authors><author><keyname>Wadayama</keyname><forenames>Tadashi</forenames></author></authors><title>Average Coset Weight Distribution of Combined LDPC Matrix Ensemble</title><categories>cs.IT math.IT</categories><comments>28 pages, The work will be presented in part at Hawaii, IEICE and
  SITA Joint Conference on Information Theory, May, 2005</comments><abstract>  In this paper, the average coset weight distribution (ACWD) of structured
ensembles of LDPC (Low-density Parity-Check) matrix, which is called combined
ensembles, is discussed. A combined ensemble is composed of a set of simpler
ensembles such as a regular bipartite ensemble. Two classes of combined
ensembles have prime importance; a stacked ensemble and a concatenated
ensemble, which consists of set of stacked matrices and concatenated matrices,
respectively. The ACWD formulas of these ensembles is shown in this paper. Such
formulas are key tools to evaluate the ACWD of a complex combined ensemble.
  From the ACWD of an ensemble, we can obtain some detailed properties of a
code (e.g., weight of coset leaders) which is not available from an average
weight distribution. Moreover, it is shown that the analysis based on the ACWD
is indispensable to evaluate the average weight distribution of some classes of
combined ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504012</id><created>2005-04-05</created><authors><author><keyname>Gomes</keyname><forenames>Luiz H.</forenames></author><author><keyname>Castro</keyname><forenames>Fernando D. O.</forenames></author><author><keyname>Almeida</keyname><forenames>Rodrigo B.</forenames></author><author><keyname>Bettencourt</keyname><forenames>Luis M. A.</forenames></author><author><keyname>Almeida</keyname><forenames>Virgilio A. F.</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara M.</forenames></author></authors><title>Improving Spam Detection Based on Structural Similarity</title><categories>cs.CR</categories><abstract>  We propose a new detection algorithm that uses structural relationships
between senders and recipients of email as the basis for the identification of
spam messages. Users and receivers are represented as vectors in their
reciprocal spaces. A measure of similarity between vectors is constructed and
used to group users into clusters. Knowledge of their classification as past
senders/receivers of spam or legitimate mail, comming from an auxiliary
detection algorithm, is then used to label these clusters probabilistically.
This knowledge comes from an auxiliary algorithm. The measure of similarity
between the sender and receiver sets of a new message to the center vector of
clusters is then used to asses the possibility of that message being legitimate
or spam. We show that the proposed algorithm is able to correct part of the
false positives (legitimate messages classified as spam) using a testbed of one
week smtp log.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504013</identifier>
 <datestamp>2007-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504013</id><created>2005-04-05</created><updated>2007-08-18</updated><authors><author><keyname>Kelley</keyname><forenames>Christine A.</forenames></author><author><keyname>Sridhara</keyname><forenames>Deepak</forenames></author></authors><title>Pseudocodewords of Tanner graphs</title><categories>cs.IT math.IT</categories><comments>To appear in Nov. 2007 issue of IEEE Transactions on Information
  Theory</comments><abstract>  This papers presents a detailed analysis of pseudocodewords of Tanner graphs.
Pseudocodewords arising on the iterative decoder's computation tree are
distinguished from pseudocodewords arising on finite degree lifts. Lower bounds
on the minimum pseudocodeword weight are presented for the BEC, BSC, and AWGN
channel. Some structural properties of pseudocodewords are examined, and
pseudocodewords and graph properties that are potentially problematic with
min-sum iterative decoding are identified. An upper bound on the minimum degree
lift needed to realize a particular irreducible lift-realizable pseudocodeword
is given in terms of its maximal component, and it is shown that all
irreducible lift-realizable pseudocodewords have components upper bounded by a
finite value $t$ that is dependent on the graph structure. Examples and
different Tanner graph representations of individual codes are examined and the
resulting pseudocodeword distributions and iterative decoding performances are
analyzed. The results obtained provide some insights in relating the structure
of the Tanner graph to the pseudocodeword distribution and suggest ways of
designing Tanner graphs with good minimum pseudocodeword weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504014</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504014</id><created>2005-04-05</created><updated>2005-10-03</updated><authors><author><keyname>Barros</keyname><forenames>Joao</forenames><affiliation>University of Porto, Portugal;</affiliation></author><author><keyname>Servetto</keyname><forenames>Sergio D.</forenames><affiliation>Cornell University</affiliation></author></authors><title>Network Information Flow with Correlated Sources</title><categories>cs.IT math.IT</categories><comments>Final version, to appear in the IEEE Transactions on Information
  Theory -- contains (very) minor changes based on the last round of reviews</comments><journal-ref>IEEE Trans. Inform. Theory, 52(1):155-170, 2006.</journal-ref><abstract>  In this paper, we consider a network communications problem in which multiple
correlated sources must be delivered to a single data collector node, over a
network of noisy independent point-to-point channels. We prove that perfect
reconstruction of all the sources at the sink is possible if and only if, for
all partitions of the network nodes into two subsets S and S^c such that the
sink is always in S^c, we have that H(U_S|U_{S^c}) &lt; \sum_{i\in S,j\in S^c}
C_{ij}. Our main finding is that in this setup a general source/channel
separation theorem holds, and that Shannon information behaves as a classical
network flow, identical in nature to the flow of water in pipes. At first
glance, it might seem surprising that separation holds in a fairly general
network situation like the one we study. A closer look, however, reveals that
the reason for this is that our model allows only for independent
point-to-point channels between pairs of nodes, and not multiple-access and/or
broadcast channels, for which separation is well known not to hold. This
``information as flow'' view provides an algorithmic interpretation for our
results, among which perhaps the most important one is the optimality of
implementing codes using a layered protocol stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504015</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504015</id><created>2005-04-05</created><authors><author><keyname>Xu</keyname><forenames>Fang</forenames></author><author><keyname>Davidson</keyname><forenames>Tim</forenames></author><author><keyname>Zhang</keyname><forenames>Jian-Kang</forenames></author><author><keyname>Wong</keyname><forenames>K. Max</forenames></author></authors><title>Design of Block Transceivers with Decision Feedback Detection</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, to appear in the IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2005.861779</doi><abstract>  This paper presents a method for jointly designing the transmitter-receiver
pair in a block-by-block communication system that employs (intra-block)
decision feedback detection. We provide closed-form expressions for
transmitter-receiver pairs that simultaneously minimize the arithmetic mean
squared error (MSE) at the decision point (assuming perfect feedback), the
geometric MSE, and the bit error rate of a uniformly bit-loaded system at
moderate-to-high signal-to-noise ratios. Separate expressions apply for the
``zero-forcing'' and ``minimum MSE'' (MMSE) decision feedback structures. In
the MMSE case, the proposed design also maximizes the Gaussian mutual
information and suggests that one can approach the capacity of the block
transmission system using (independent instances of) the same (Gaussian) code
for each element of the block. Our simulation studies indicate that the
proposed transceivers perform significantly better than standard transceivers,
and that they retain their performance advantages in the presence of error
propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504016</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504016</id><created>2005-04-05</created><updated>2006-04-26</updated><authors><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Leyba</keyname><forenames>David</forenames></author></authors><title>Shortened Array Codes of Large Girth</title><categories>cs.DM cs.IT math.IT</categories><comments>16 pages; 8 figures; to appear in IEEE Transactions on Information
  Theory, Aug 2006</comments><abstract>  One approach to designing structured low-density parity-check (LDPC) codes
with large girth is to shorten codes with small girth in such a manner that the
deleted columns of the parity-check matrix contain all the variables involved
in short cycles. This approach is especially effective if the parity-check
matrix of a code is a matrix composed of blocks of circulant permutation
matrices, as is the case for the class of codes known as array codes. We show
how to shorten array codes by deleting certain columns of their parity-check
matrices so as to increase their girth. The shortening approach is based on the
observation that for array codes, and in fact for a slightly more general class
of LDPC codes, the cycles in the corresponding Tanner graph are governed by
certain homogeneous linear equations with integer coefficients. Consequently,
we can selectively eliminate cycles from an array code by only retaining those
columns from the parity-check matrix of the original code that are indexed by
integer sequences that do not contain solutions to the equations governing
those cycles. We provide Ramsey-theoretic estimates for the maximum number of
columns that can be retained from the original parity-check matrix with the
property that the sequence of their indices avoid solutions to various types of
cycle-governing equations. This translates to estimates of the rate penalty
incurred in shortening a code to eliminate cycles. Simulation results show that
for the codes considered, shortening them to increase the girth can lead to
significant gains in signal-to-noise ratio in the case of communication over an
additive white Gaussian noise channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504017</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504017</id><created>2005-04-05</created><authors><author><keyname>Sikora</keyname><forenames>Marcin</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>A new SISO algorithm with application to turbo equalization</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, submitted to 2005 IEEE International Symposium on
  Information Theory</comments><abstract>  In this paper we propose a new soft-input soft-output equalization algorithm,
offering very good performance/complexity tradeoffs. It follows the structure
of the BCJR algorithm, but dynamically constructs a simplified trellis during
the forward recursion. In each trellis section, only the M states with the
strongest forward metric are preserved, similar to the M-BCJR algorithm. Unlike
the M-BCJR, however, the remaining states are not deleted, but rather merged
into the surviving states. The new algorithm compares favorably with the
reduced-state BCJR algorithm, offering better performance and more flexibility,
particularly for systems with higher order modulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504018</id><created>2005-04-06</created><authors><author><keyname>Brunet</keyname><forenames>Olivier</forenames><affiliation>Leibniz - IMAG</affiliation></author></authors><title>A Rule-Based Logic for Quantum Information</title><categories>cs.LO quant-ph</categories><proxy>ccsd ccsd-00004639</proxy><abstract>  In the present article, we explore a new approach for the study of
orthomodular lattices, where we replace the problematic conjunction by a binary
operator, called the Sasaki projection. We present a characterization of
orthomodular lattices based on the use of an algebraic version of the Sasaki
projection operator (together with orthocomplementation) rather than on the
conjunction. We then define of a new logic, which we call Sasaki Orthologic,
which is closely related to quantum logic, and provide a rule-based definition
of this logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504019</id><created>2005-04-06</created><authors><author><keyname>Wang</keyname><forenames>Guilin</forenames></author><author><keyname>Bao</keyname><forenames>Feng</forenames></author><author><keyname>Ma</keyname><forenames>Changshe</forenames></author><author><keyname>Chen</keyname><forenames>Kefei</forenames></author></authors><title>Efficient Authenticated Encryption Schemes with Public Verifiability</title><categories>cs.CR</categories><comments>Early version appears in the Proc. of The 60th IEEE Vehicular
  Technology Conference (VTC 2004-Fall) - Wireless Technologies for Global
  Security. IEEE, 2004</comments><abstract>  An authenticated encryption scheme allows messages to be encrypted and
authenticated simultaneously. In 2003, Ma and Chen proposed such a scheme with
public verifiability. That is, in their scheme the receiver can efficiently
prove to a third party that a message is indeed originated from a specific
sender. In this paper, we first identify two security weaknesses in the Ma-Chen
authenticated encryption scheme. Then, based on the Schnorr signature, we
proposed an efficient and secure improved scheme such that all the desired
security requirements are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504020</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504020</id><created>2005-04-06</created><updated>2005-04-29</updated><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>The Viterbi Algorithm: A Personal History</title><categories>cs.IT math.IT</categories><comments>8 pages; presented at Viterbi Conference, University of Southern
  California, Los Angeles, March 8, 2005</comments><abstract>  The story of the Viterbi algorithm (VA) is told from a personal perspective.
Applications both within and beyond communications are discussed. In brief
summary, the VA has proved to be an extremely important algorithm in a
surprising variety of fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504021</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504021</id><created>2005-04-06</created><authors><author><keyname>Huang</keyname><forenames>Xiaofei</forenames></author></authors><title>Near Perfect Decoding of LDPC Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to the 2005 IEEE International Symposium on
  Information Theory</comments><abstract>  Cooperative optimization is a new way for finding global optima of
complicated functions of many variables. It has some important properties not
possessed by any conventional optimization methods. It has been successfully
applied in solving many large scale optimization problems in image processing,
computer vision, and computational chemistry. This paper shows the application
of this optimization principle in decoding LDPC codes, which is another hard
combinatorial optimization problem. In our experiments, it significantly
out-performed the sum-product algorithm, the best known method for decoding
LDPC codes. Compared to the sum-product algorithm, our algorithm reduced the
error rate further by three fold, improved the speed by six times, and lowered
error floors dramatically in the decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504022</id><created>2005-04-06</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>A Matter of Opinion: Sentiment Analysis and Business Intelligence
  (position paper)</title><categories>cs.CL</categories><comments>2 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Presented at the IBM Faculty Summit on the Architecture of
  On-Demand Business, May 2004</journal-ref><abstract>  A general-audience introduction to the area of &quot;sentiment analysis&quot;, the
computational treatment of subjective, opinion-oriented language (an example
application is determining whether a review is &quot;thumbs up&quot; or &quot;thumbs down&quot;).
Some challenges, applications to business-intelligence tasks, and potential
future directions are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504023</id><created>2005-04-06</created><authors><author><keyname>Giotis</keyname><forenames>Ioannis</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author></authors><title>Correlation Clustering with a Fixed Number of Clusters</title><categories>cs.DS</categories><comments>16 pages</comments><acm-class>F.2.2; G.1.2; G.1.6</acm-class><abstract>  We continue the investigation of problems concerning correlation clustering
or clustering with qualitative information, which is a clustering formulation
that has been studied recently. The basic setup here is that we are given as
input a complete graph on n nodes (which correspond to nodes to be clustered)
whose edges are labeled + (for similar pairs of items) and - (for dissimilar
pairs of items). Thus we have only as input qualitative information on
similarity and no quantitative distance measure between items. The quality of a
clustering is measured in terms of its number of agreements, which is simply
the number of edges it correctly classifies, that is the sum of number of -
edges whose endpoints it places in different clusters plus the number of +
edges both of whose endpoints it places within the same cluster.
  In this paper, we study the problem of finding clusterings that maximize the
number of agreements, and the complementary minimization version where we seek
clusterings that minimize the number of disagreements. We focus on the
situation when the number of clusters is stipulated to be a small constant k.
Our main result is that for every k, there is a polynomial time approximation
scheme for both maximizing agreements and minimizing disagreements. (The
problems are NP-hard for every k &gt;= 2.) The main technical work is for the
minimization version, as the PTAS for maximizing agreements follows along the
lines of the property tester for Max k-CUT.
  In contrast, when the number of clusters is not specified, the problem of
minimizing disagreements was shown to be APX-hard, even though the maximization
version admits a PTAS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504024</id><created>2005-04-07</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Brand</keyname><forenames>Sebastian</forenames></author></authors><title>Constraint-Based Qualitative Simulation</title><categories>cs.AI cs.LO</categories><comments>10 pages, to appear at the conference TIME 2005</comments><acm-class>I.2.4; I.6.2; I.6.7; F.4.1; D.3.2</acm-class><abstract>  We consider qualitative simulation involving a finite set of qualitative
relations in presence of complete knowledge about their interrelationship. We
show how it can be naturally captured by means of constraints expressed in
temporal logic and constraint satisfaction problems. The constraints relate at
each stage the 'past' of a simulation with its 'future'. The benefit of this
approach is that it readily leads to an implementation based on constraint
technology that can be used to generate simulations and to answer queries about
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504025</identifier>
 <datestamp>2009-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504025</id><created>2005-04-07</created><updated>2009-08-25</updated><authors><author><keyname>G</keyname><forenames>Raju Renjit.</forenames></author></authors><title>Incorporating LINQ, State Diagrams Templating and Package Extension Into
  Java</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This submission has been withdrawn at the request of the author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504026</id><created>2005-04-07</created><updated>2006-08-21</updated><authors><author><keyname>Cheng</keyname><forenames>Yongxi</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yin</keyname><forenames>Yiqun Lisa</forenames></author></authors><title>Searching Monotone Multi-dimensional Arrays</title><categories>cs.DS cs.DM</categories><comments>13 pages, 2 figures; same results, presentation improved, add two
  figures</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  In this paper we investigate the problem of searching monotone
multi-dimensional arrays. We generalize Linial and Saks' search algorithm
\cite{LS1} for monotone 3-dimensional arrays to $d$-dimensions with $d\geq 4$.
Our new search algorithm is asymptotically optimal for $d=4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504027</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504027</id><created>2005-04-07</created><updated>2005-06-28</updated><authors><author><keyname>Dalmau</keyname><forenames>Victor</forenames></author></authors><title>Linear Datalog and Bounded Path Duality of Relational Structures</title><categories>cs.LO cs.CC</categories><acm-class>F.1.3; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 1, Issue 1 (April 29,
  2005) lmcs:778</journal-ref><doi>10.2168/LMCS-1(1:5)2005</doi><abstract>  In this paper we systematically investigate the connections between logics
with a finite number of variables, structures of bounded pathwidth, and linear
Datalog Programs. We prove that, in the context of Constraint Satisfaction
Problems, all these concepts correspond to different mathematical embodiments
of a unique robust notion that we call bounded path duality. We also study the
computational complexity implications of the notion of bounded path duality. We
show that every constraint satisfaction problem $\csp(\best)$ with bounded path
duality is solvable in NL and that this notion explains in a uniform way all
families of CSPs known to be in NL. Finally, we use the results developed in
the paper to identify new problems in NL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504028</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504028</id><created>2005-04-07</created><updated>2005-04-14</updated><authors><author><keyname>Peleg</keyname><forenames>Michael</forenames></author><author><keyname>Sanderovich</keyname><forenames>Amichai</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>On Extrinsic Information of Good Codes Operating Over Discrete
  Memoryless Channels</title><categories>cs.IT math.IT</categories><report-no>CCIT-525</report-no><abstract>  We show that the Extrinsic Information about the coded bits of any good
(capacity achieving) code operating over a wide class of discrete memoryless
channels (DMC) is zero when channel capacity is below the code rate and
positive constant otherwise, that is, the Extrinsic Information Transfer (EXIT)
chart is a step function of channel quality, for any capacity achieving code.
It follows that, for a common class of iterative receivers where the error
correcting decoder must operate at first iteration at rate above capacity (such
as in turbo equalization, turbo channel estimation, parallel and serial
concatenated coding and the like), classical good codes which achieve capacity
over the DMC are not effective and should be replaced by different new ones.
Another meaning of the results is that a good code operating at rate above
channel capacity falls apart into its individual transmitted symbols in the
sense that all the information about a coded transmitted symbol is contained in
the corresponding received symbol and no information about it can be inferred
from the other received symbols. The binary input additive white Gaussian noise
channel is treated in part 1 of this report. Part 2 extends the results to the
symmetric binary channel and to the binary erasure channel and provides an
heuristic extension to wider class of channel models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504029</id><created>2005-04-08</created><updated>2007-04-22</updated><authors><author><keyname>Mosk-Aoyama</keyname><forenames>Damon</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Fast Distributed Algorithms for Computing Separable Functions</title><categories>cs.NI cs.DC cs.DS</categories><comments>15 pages</comments><abstract>  The problem of computing functions of values at the nodes in a network in a
totally distributed manner, where nodes do not have unique identities and make
decisions based only on local information, has applications in sensor,
peer-to-peer, and ad-hoc networks. The task of computing separable functions,
which can be written as linear combinations of functions of individual
variables, is studied in this context. Known iterative algorithms for averaging
can be used to compute the normalized values of such functions, but these
algorithms do not extend in general to the computation of the actual values of
separable functions.
  The main contribution of this paper is the design of a distributed randomized
algorithm for computing separable functions. The running time of the algorithm
is shown to depend on the running time of a minimum computation algorithm used
as a subroutine. Using a randomized gossip mechanism for minimum computation as
the subroutine yields a complete totally distributed algorithm for computing
separable functions. For a class of graphs with small spectral gap, such as
grid graphs, the time used by the algorithm to compute averages is of a smaller
order than the time required by a known iterative averaging scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504030</identifier>
 <datestamp>2007-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504030</id><created>2005-04-08</created><updated>2007-05-08</updated><authors><author><keyname>Mooij</keyname><forenames>Joris M.</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Sufficient conditions for convergence of the Sum-Product Algorithm</title><categories>cs.IT cs.AI math.IT</categories><comments>15 pages, 5 figures. Major changes and new results in this revised
  version. Submitted to IEEE Transactions on Information Theory</comments><acm-class>I.2.3; F.2.1</acm-class><journal-ref>IEEE Transactions on Information Theory, 53(12):4422-4437 Dec.
  2007</journal-ref><doi>10.1109/TIT.2007.909166</doi><abstract>  We derive novel conditions that guarantee convergence of the Sum-Product
algorithm (also known as Loopy Belief Propagation or simply Belief Propagation)
to a unique fixed point, irrespective of the initial messages. The
computational complexity of the conditions is polynomial in the number of
variables. In contrast with previously existing conditions, our results are
directly applicable to arbitrary factor graphs (with discrete variables) and
are shown to be valid also in the case of factors containing zeros, under some
additional conditions. We compare our bounds with existing ones, numerically
and, if possible, analytically. For binary variables with pairwise
interactions, we derive sufficient conditions that take into account local
evidence (i.e., single variable factors) and the type of pair interactions
(attractive or repulsive). It is shown empirically that this bound outperforms
existing bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504031</id><created>2005-04-08</created><authors><author><keyname>Giraldi</keyname><forenames>Gilson Antonio</forenames></author><author><keyname>de Oliveira</keyname><forenames>Antonio Alberto Fernandes</forenames></author></authors><title>Convexity Analysis of Snake Models Based on Hamiltonian Formulation</title><categories>cs.CV cs.GR</categories><comments>17 pages, 3 figures, technical report</comments><acm-class>I.4; I.4.6;I.4.8</acm-class><abstract>  This paper presents a convexity analysis for the dynamic snake model based on
the Potential Energy functional and the Hamiltonian formulation of the
classical mechanics. First we see the snake model as a dynamical system whose
singular points are the borders we seek. Next we show that a necessary
condition for a singular point to be an attractor is that the energy functional
is strictly convex in a neighborhood of it, that means, if the singular point
is a local minimum of the potential energy. As a consequence of this analysis,
a local expression relating the dynamic parameters and the rate of convergence
arises. Such results link the convexity analysis of the potential energy and
the dynamic snake model and point forward to the necessity of a physical
quantity whose convexity analysis is related to the dynamic and which
incorporate the velocity space. Such a quantity is exactly the (conservative)
Hamiltonian of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504032</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504032</id><created>2005-04-09</created><authors><author><keyname>Fossorier</keyname><forenames>Marc</forenames></author></authors><title>Critical Point for Maximum Likelihood Decoding of Linear Block Codes</title><categories>cs.IT math.IT</categories><comments>to appear IEEE Communications Letters</comments><abstract>  In this letter, the SNR value at which the error performance curve of a soft
decision maximum likelihood decoder reaches the slope corresponding to the code
minimum distance is determined for a random code. Based on this value, referred
to as the critical point, new insight about soft bounded distance decoding of
random-like codes (and particularly Reed-Solomon codes) is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504033</id><created>2005-04-10</created><authors><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Mehmood</keyname><forenames>Atif</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Rehman</keyname><forenames>Waqas ur</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author><author><keyname>Zafar</keyname><forenames>Muhammad Adeel</forenames></author></authors><title>Resource Management Services for a Grid Analysis Environment</title><categories>cs.DC</categories><comments>8 pages, 7 figures. Workshop on Web and Grid Services for Scientific
  Data Analysis at the Int Conf on Parallel Processing (ICPP05). Norway June
  2005</comments><acm-class>H2.4;J.3</acm-class><abstract>  Selecting optimal resources for submitting jobs on a computational Grid or
accessing data from a data grid is one of the most important tasks of any Grid
middleware. Most modern Grid software today satisfies this responsibility and
gives a best-effort performance to solve this problem. Almost all decisions
regarding scheduling and data access are made by the software automatically,
giving users little or no control over the entire process. To solve this
problem, a more interactive set of services and middleware is desired that
provides users more information about Grid weather, and gives them more control
over the decision making process. This paper presents a set of services that
have been developed to provide more interactive resource management
capabilities within the Grid Analysis Environment (GAE) being developed
collaboratively by Caltech, NUST and several other institutes. These include a
steering service, a job monitoring service and an estimator service that have
been designed and written using a common Grid-enabled Web Services framework
named Clarens. The paper also presents a performance analysis of the developed
services to show that they have indeed resulted in a more interactive and
powerful system for user-centric Grid-enabled physics analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504034</id><created>2005-04-10</created><authors><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Iqbal</keyname><forenames>Saima</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Shah</keyname><forenames>S. Yousaf</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author><author><keyname>Willers</keyname><forenames>Ian</forenames></author></authors><title>Heterogeneous Relational Databases for a Grid-enabled Analysis
  Environment</title><categories>cs.DC</categories><comments>8 pages, 6 figures, 1 table. Workshop on Web and Grid Services for
  Scientific Data Analysis at the Int Conf on Parallel Processing (ICPP05).
  Norway June 2005</comments><acm-class>H2.4;J.3</acm-class><abstract>  Grid based systems require a database access mechanism that can provide
seamless homogeneous access to the requested data through a virtual data access
system, i.e. a system which can take care of tracking the data that is stored
in geographically distributed heterogeneous databases. This system should
provide an integrated view of the data that is stored in the different
repositories by using a virtual data access mechanism, i.e. a mechanism which
can hide the heterogeneity of the backend databases from the client
applications. This paper focuses on accessing data stored in disparate
relational databases through a web service interface, and exploits the features
of a Data Warehouse and Data Marts. We present a middleware that enables
applications to access data stored in geographically distributed relational
databases without being aware of their physical locations and underlying
schema. A web service interface is provided to enable applications to access
this middleware in a language and platform independent way. A prototype
implementation was created based on Clarens [4], Unity [7] and POOL [8]. This
ability to access the data stored in the distributed relational databases
transparently is likely to be a very powerful one for Grid users, especially
the scientific community wishing to collate and analyze data distributed over
the Grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504035</id><created>2005-04-11</created><authors><author><keyname>Legg</keyname><forenames>Shane</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Fitness Uniform Deletion: A Simple Way to Preserve Diversity</title><categories>cs.NE cs.AI</categories><comments>8 two-column pages, 19 figures</comments><report-no>IDSIA-11-04</report-no><acm-class>I.2.M</acm-class><journal-ref>Proc. Genetic and Evolutionary Computation Conference (GECCO 2005)
  1271-1278</journal-ref><abstract>  A commonly experienced problem with population based optimisation methods is
the gradual decline in population diversity that tends to occur over time. This
can slow a system's progress or even halt it completely if the population
converges on a local optimum from which it cannot escape. In this paper we
present the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat
unconventional approach to this problem. Under FUDS the deletion operation is
modified to only delete those individuals which are &quot;common&quot; in the sense that
there exist many other individuals of similar fitness in the population. This
makes it impossible for the population to collapse to a collection of highly
related individuals with similar fitness. Our experimental results on a range
of optimisation problems confirm this, in particular for deceptive optimisation
problems the performance is significantly more robust to variation in the
selection intensity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504036</id><created>2005-04-11</created><authors><author><keyname>Belew</keyname><forenames>Richard K.</forenames></author></authors><title>Scientific impact quantity and quality: Analysis of two sources of
  bibliographic data</title><categories>cs.IR cs.DL</categories><comments>12 pages, 1 table, 6 figures</comments><acm-class>H.3.3; H.3.7; H.5.4</acm-class><abstract>  Attempts to understand the consequence of any individual scientist's activity
within the long-term trajectory of science is one of the most difficult
questions within the philosophy of science. Because scientific publications
play such as central role in the modern enterprise of science, bibliometric
techniques which measure the ``impact'' of an individual publication as a
function of the number of citations it receives from subsequent authors have
provided some of the most useful empirical data on this question. Until
recently, Thompson/ISI has provided the only source of large-scale ``inverted''
bibliographic data of the sort required for impact analysis. In the end of
2004, Google introduced a new service, GoogleScholar, making much of this same
data available. Here we analyze 203 publications, collectively cited by more
than 4000 other publications. We show surprisingly good agreement between data
citation counts provided by the two services. Data quality across the systems
is analyzed, and potentially useful complementarities between are considered.
The additional robustness offered by multiple sources of such data promises to
increase the utility of these measurements as open citation protocols and open
access increase their impact on electronic scientific publication practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504037</id><created>2005-04-11</created><updated>2006-10-31</updated><authors><author><keyname>Murthy</keyname><forenames>K. P. N.</forenames></author><author><keyname>Janani</keyname><forenames>M.</forenames></author><author><keyname>Priya</keyname><forenames>B. Shenbga</forenames></author></authors><title>Bayesian Restoration of Digital Images Employing Markov Chain Monte
  Carlo a Review</title><categories>cs.CV cond-mat.stat-mech physics.comp-ph</categories><comments>42 pages; 16 figures; revised version with several typos removed and
  mistakes in equations corrected</comments><abstract>  A review of Bayesian restoration of digital images based on Monte Carlo
techniques is presented. The topics covered include Likelihood, Prior and
Posterior distributions, Poisson, Binay symmetric channel, and Gaussian channel
models of Likelihood distribution,Ising and Potts spin models of Prior
distribution, restoration of an image through Posterior maximization,
statistical estimation of a true image from Posterior ensembles, Markov Chain
Monte Carlo methods and cluster algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504038</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504038</id><created>2005-04-11</created><updated>2007-10-08</updated><authors><author><keyname>Manthey</keyname><forenames>Bodo</forenames></author></authors><title>On Approximating Restricted Cycle Covers</title><categories>cs.CC cs.DM</categories><comments>To appear in SIAM Journal on Computing. Minor changes</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  A cycle cover of a graph is a set of cycles such that every vertex is part of
exactly one cycle. An L-cycle cover is a cycle cover in which the length of
every cycle is in the set L. The weight of a cycle cover of an edge-weighted
graph is the sum of the weights of its edges.
  We come close to settling the complexity and approximability of computing
L-cycle covers. On the one hand, we show that for almost all L, computing
L-cycle covers of maximum weight in directed and undirected graphs is APX-hard
and NP-hard. Most of our hardness results hold even if the edge weights are
restricted to zero and one.
  On the other hand, we show that the problem of computing L-cycle covers of
maximum weight can be approximated within a factor of 2 for undirected graphs
and within a factor of 8/3 in the case of directed graphs. This holds for
arbitrary sets L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504039</id><created>2005-04-11</created><authors><author><keyname>Grozin</keyname><forenames>A. G.</forenames></author></authors><title>TeXmacs-maxima interface</title><categories>cs.SC cs.MS hep-ph</categories><comments>html file, 32 png screenshots</comments><abstract>  This tutorial presents features of the new and improved TeXmacs-maxima
interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or
later).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504040</id><created>2005-04-11</created><authors><author><keyname>Leguay</keyname><forenames>Jeremie</forenames></author><author><keyname>Friedman</keyname><forenames>Timur</forenames></author><author><keyname>Conan</keyname><forenames>Vania</forenames></author></authors><title>DTN Routing in a Mobility Pattern Space</title><categories>cs.NI</categories><comments>8 pages, preprint</comments><abstract>  Routing in Delay Tolerant Networks (DTNs) benefits considerably if one can
take advantage of knowledge concerning node mobility. The main contribution of
this paper is the definition of a generic routing scheme for DTNs using a
high-dimensional Euclidean space constructed upon nodes' mobility patterns. For
example, nodes are represented as points having as coordinates their
probability of being found in each possible location. We present simulation
results indicating that such a scheme can be beneficial in a scenario inspired
by studies done on real mobility traces. This work should open the way to
further use of the virtual space formalism in DTN routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504041</id><created>2005-04-11</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Schult</keyname><forenames>Joachim</forenames></author></authors><title>Learning Polynomial Networks for Classification of Clinical
  Electroencephalograms</title><categories>cs.AI cs.NE</categories><journal-ref>J Soft Computing 2005</journal-ref><abstract>  We describe a polynomial network technique developed for learning to classify
clinical electroencephalograms (EEGs) presented by noisy features. Using an
evolutionary strategy implemented within Group Method of Data Handling, we
learn classification models which are comprehensively described by sets of
short-term polynomials. The polynomial models were learnt to classify the EEGs
recorded from Alzheimer and healthy patients and recognize the EEG artifacts.
Comparing the performances of our technique and some machine learning methods
we conclude that our technique can learn well-suited polynomial models which
experts can find easy-to-understand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504042</id><created>2005-04-11</created><authors><author><keyname>Schetinin</keyname><forenames>V.</forenames></author><author><keyname>Fieldsend</keyname><forenames>J. E.</forenames></author><author><keyname>Partridge</keyname><forenames>D.</forenames></author><author><keyname>Krzanowski</keyname><forenames>W. J.</forenames></author><author><keyname>Everson</keyname><forenames>R. M.</forenames></author><author><keyname>Bailey</keyname><forenames>T. C.</forenames></author><author><keyname>Hernandez</keyname><forenames>A.</forenames></author></authors><title>The Bayesian Decision Tree Technique with a Sweeping Strategy</title><categories>cs.AI cs.LG</categories><abstract>  The uncertainty of classification outcomes is of crucial importance for many
safety critical applications including, for example, medical diagnostics. In
such applications the uncertainty of classification can be reliably estimated
within a Bayesian model averaging technique that allows the use of prior
information. Decision Tree (DT) classification models used within such a
technique gives experts additional information by making this classification
scheme observable. The use of the Markov Chain Monte Carlo (MCMC) methodology
of stochastic sampling makes the Bayesian DT technique feasible to perform.
However, in practice, the MCMC technique may become stuck in a particular DT
which is far away from a region with a maximal posterior. Sampling such DTs
causes bias in the posterior estimates, and as a result the evaluation of
classification uncertainty may be incorrect. In a particular case, the negative
effect of such sampling may be reduced by giving additional prior information
on the shape of DTs. In this paper we describe a new approach based on sweeping
the DTs without additional priors on the favorite shape of DTs. The
performances of Bayesian DT techniques with the standard and sweeping
strategies are compared on a synthetic data as well as on real datasets.
Quantitatively evaluating the uncertainty in terms of entropy of class
posterior probabilities, we found that the sweeping strategy is superior to the
standard strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504043</id><created>2005-04-11</created><authors><author><keyname>Schetinin</keyname><forenames>V.</forenames></author><author><keyname>Partridge</keyname><forenames>D.</forenames></author><author><keyname>Krzanowski</keyname><forenames>W. J.</forenames></author><author><keyname>Everson</keyname><forenames>R. M.</forenames></author><author><keyname>Fieldsend</keyname><forenames>J. E.</forenames></author><author><keyname>Bailey</keyname><forenames>T. C.</forenames></author><author><keyname>Hernandez</keyname><forenames>A.</forenames></author></authors><title>Experimental Comparison of Classification Uncertainty for Randomised and
  Bayesian Decision Tree Ensembles</title><categories>cs.AI cs.LG</categories><comments>IDEAL-2004</comments><abstract>  In this paper we experimentally compare the classification uncertainty of the
randomised Decision Tree (DT) ensemble technique and the Bayesian DT technique
with a restarting strategy on a synthetic dataset as well as on some datasets
commonly used in the machine learning community. For quantitative evaluation of
classification uncertainty, we use an Uncertainty Envelope dealing with the
class posterior distribution and a given confidence probability. Counting the
classifier outcomes, this technique produces feasible evaluations of the
classification uncertainty. Using this technique in our experiments, we found
that the Bayesian DT technique is superior to the randomised DT ensemble
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504044</id><created>2005-04-11</created><authors><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Steenberg</keyname><forenames>Conrad</forenames></author><author><keyname>van Lingen</keyname><forenames>Frank</forenames></author><author><keyname>Newman</keyname><forenames>Harvey</forenames></author><author><keyname>Bunn</keyname><forenames>Julian</forenames></author><author><keyname>Ali</keyname><forenames>Arshad</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author><author><keyname>Rehman</keyname><forenames>Waqas ur</forenames></author><author><keyname>Khan</keyname><forenames>Faisal</forenames></author><author><keyname>In</keyname><forenames>Jang Uk</forenames></author></authors><title>JClarens: A Java Framework for Developing and Deploying Web Services for
  Grid Computing</title><categories>cs.DC</categories><comments>8 pages, 4 figures. Paper at the 3rd IEEE International Conference on
  Web Services (ICWS05). Florida, USA. July 2005</comments><acm-class>H2.4;J.3</acm-class><abstract>  High Energy Physics (HEP) and other scientific communities have adopted
Service Oriented Architectures (SOA) as part of a larger Grid computing effort.
This effort involves the integration of many legacy applications and
programming libraries into a SOA framework. The Grid Analysis Environment (GAE)
is such a service oriented architecture based on the Clarens Grid Services
Framework and is being developed as part of the Compact Muon Solenoid (CMS)
experiment at the Large Hadron Collider (LHC) at European Laboratory for
Particle Physics (CERN). Clarens provides a set of authorization, access
control, and discovery services, as well as XMLRPC and SOAP access to all
deployed services. Two implementations of the Clarens Web Services Framework
(Python and Java) offer integration possibilities for a wide range of
programming languages. This paper describes the Java implementation of the
Clarens Web Services Framework called JClarens. and several web services of
interest to the scientific and Grid community that have been deployed using
JClarens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504045</identifier>
 <datestamp>2009-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504045</id><created>2005-04-12</created><authors><author><keyname>Wehner</keyname><forenames>Stephanie</forenames><affiliation>CWI, Amsterdam</affiliation></author></authors><title>Analyzing Worms and Network Traffic using Compression</title><categories>cs.CR</categories><comments>12 pages, LaTeX, two columns</comments><journal-ref>Journal of Computer Security, Vol 15, Number 3, 303-320, 2007</journal-ref><abstract>  Internet worms have become a widespread threat to system and network
operations. In order to fight them more efficiently, it is necessary to analyze
newly discovered worms and attack patterns. This paper shows how techniques
based on Kolmogorov Complexity can help in the analysis of internet worms and
network traffic. Using compression, different species of worms can be clustered
by type. This allows us to determine whether an unknown worm binary could in
fact be a later version of an existing worm in an extremely simple, automated,
manner. This may become a useful tool in the initial analysis of malicious
binaries. Furthermore, compression can also be useful to distinguish different
types of network traffic and can thus help to detect traffic anomalies: Certain
anomalies may be detected by looking at the compressibility of a network
session alone. We furthermore show how to use compression to detect malicious
network sessions that are very similar to known intrusion attempts. This
technique could become a useful tool to detect new variations of an attack and
thus help to prevent IDS evasion. We provide two new plugins for Snort which
demonstrate both approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504046</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504046</id><created>2005-04-12</created><updated>2006-02-16</updated><authors><author><keyname>Gemelos</keyname><forenames>George M.</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>On the Entropy Rate of Pattern Processes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions of Information Theory</comments><abstract>  We study the entropy rate of pattern sequences of stochastic processes, and
its relationship to the entropy rate of the original process. We give a
complete characterization of this relationship for i.i.d. processes over
arbitrary alphabets, stationary ergodic processes over discrete alphabets, and
a broad family of stationary ergodic processes over uncountable alphabets. For
cases where the entropy rate of the pattern process is infinite, we
characterize the possible growth rate of the block entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504047</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504047</id><created>2005-04-12</created><updated>2005-05-26</updated><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Nichols</keyname><forenames>Jared</forenames></author></authors><title>Pushdown dimension</title><categories>cs.IT cs.CC math.IT</categories><comments>10 page main body; 12 page appendix of proofs</comments><abstract>  This paper develops the theory of pushdown dimension and explores its
relationship with finite-state dimension. Pushdown dimension is trivially
bounded above by finite-state dimension for all sequences, since a pushdown
gambler can simulate any finite-state gambler. We show that for every rational
0 &lt; d &lt; 1, there exists a sequence with finite-state dimension d whose pushdown
dimension is at most d/2. This establishes a quantitative analogue of the
well-known fact that pushdown automata decide strictly more languages than
finite automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504048</id><created>2005-04-12</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author></authors><title>Oracles Are Subtle But Not Malicious</title><categories>cs.CC quant-ph</categories><comments>20 pages, 1 figure</comments><acm-class>F.1.2; F.1.3</acm-class><abstract>  Theoretical computer scientists have been debating the role of oracles since
the 1970's. This paper illustrates both that oracles can give us nontrivial
insights about the barrier problems in circuit complexity, and that they need
not prevent us from trying to solve those problems.
  First, we give an oracle relative to which PP has linear-sized circuits, by
proving a new lower bound for perceptrons and low- degree threshold
polynomials. This oracle settles a longstanding open question, and generalizes
earlier results due to Beigel and to Buhrman, Fortnow, and Thierauf. More
importantly, it implies the first nonrelativizing separation of &quot;traditional&quot;
complexity classes, as opposed to interactive proof classes such as MIP and
MA-EXP. For Vinodchandran showed, by a nonrelativizing argument, that PP does
not have circuits of size n^k for any fixed k. We present an alternative proof
of this fact, which shows that PP does not even have quantum circuits of size
n^k with quantum advice. To our knowledge, this is the first nontrivial lower
bound on quantum circuit size.
  Second, we study a beautiful algorithm of Bshouty et al. for learning Boolean
circuits in ZPP^NP. We show that the NP queries in this algorithm cannot be
parallelized by any relativizing technique, by giving an oracle relative to
which ZPP^||NP and even BPP^||NP have linear-size circuits. On the other hand,
we also show that the NP queries could be parallelized if P=NP. Thus, classes
such as ZPP^||NP inhabit a &quot;twilight zone,&quot; where we need to distinguish
between relativizing and black-box techniques. Our results on this subject have
implications for computational learning theory as well as for the circuit
minimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504049</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504049</id><created>2005-04-12</created><authors><author><keyname>Shamir</keyname><forenames>Gil I.</forenames></author></authors><title>Bounds on the Entropy of Patterns of I.I.D. Sequences</title><categories>cs.IT math.IT</categories><comments>submitted to ITW2005</comments><abstract>  Bounds on the entropy of patterns of sequences generated by independently
identically distributed (i.i.d.) sources are derived. A pattern is a sequence
of indices that contains all consecutive integer indices in increasing order of
first occurrence. If the alphabet of a source that generated a sequence is
unknown, the inevitable cost of coding the unknown alphabet symbols can be
exploited to create the pattern of the sequence. This pattern can in turn be
compressed by itself. The bounds derived here are functions of the i.i.d.
source entropy, alphabet size, and letter probabilities. It is shown that for
large alphabets, the pattern entropy must decrease from the i.i.d. one. The
decrease is in many cases more significant than the universal coding redundancy
bounds derived in prior works. The pattern entropy is confined between two
bounds that depend on the arrangement of the letter probabilities in the
probability space. For very large alphabets whose size may be greater than the
coded pattern length, all low probability letters are packed into one symbol.
The pattern entropy is upper and lower bounded in terms of the i.i.d. entropy
of the new packed alphabet. Correction terms, which are usually negligible, are
provided for both upper and lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504050</id><created>2005-04-13</created><updated>2006-01-15</updated><authors><author><keyname>Lanese</keyname><forenames>Ivan</forenames></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames></author></authors><title>Mapping Fusion and Synchronized Hyperedge Replacement into Logic
  Programming</title><categories>cs.LO cs.PL</categories><comments>44 pages, 8 figures, to appear in a special issue of Theory and
  Practice of Logic Programming, minor revision</comments><acm-class>F.1.1; F.4.1</acm-class><abstract>  In this paper we compare three different formalisms that can be used in the
area of models for distributed, concurrent and mobile systems. In particular we
analyze the relationships between a process calculus, the Fusion Calculus,
graph transformations in the Synchronized Hyperedge Replacement with Hoare
synchronization (HSHR) approach and logic programming. We present a translation
from Fusion Calculus into HSHR (whereas Fusion Calculus uses Milner
synchronization) and prove a correspondence between the reduction semantics of
Fusion Calculus and HSHR transitions. We also present a mapping from HSHR into
a transactional version of logic programming and prove that there is a full
correspondence between the two formalisms. The resulting mapping from Fusion
Calculus to logic programming is interesting since it shows the tight analogies
between the two formalisms, in particular for handling name generation and
mobility. The intermediate step in terms of HSHR is convenient since graph
transformations allow for multiple, remote synchronizations, as required by
Fusion Calculus semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504051</id><created>2005-04-13</created><authors><author><keyname>Argyros</keyname><forenames>Tassos S.</forenames></author><author><keyname>Cheriton</keyname><forenames>David R.</forenames></author></authors><title>A Scalable Stream-Oriented Framework for Cluster Applications</title><categories>cs.DC cs.DB cs.NI cs.OS cs.PL</categories><abstract>  This paper presents a stream-oriented architecture for structuring cluster
applications. Clusters that run applications based on this architecture can
scale to tenths of thousands of nodes with significantly less performance loss
or reliability problems. Our architecture exploits the stream nature of the
data flow and reduces congestion through load balancing, hides latency behind
data pushes and transparently handles node failures. In our ongoing work, we
are developing an implementation for this architecture and we are able to run
simple data mining applications on a cluster simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504052</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Schult</keyname><forenames>Joachim</forenames></author><author><keyname>Scheidt</keyname><forenames>Burkhart</forenames></author><author><keyname>Kuriakin</keyname><forenames>Valery</forenames></author></authors><title>Learning Multi-Class Neural-Network Models from Electroencephalograms</title><categories>cs.NE cs.LG</categories><comments>KES-2003</comments><abstract>  We describe a new algorithm for learning multi-class neural-network models
from large-scale clinical electroencephalograms (EEGs). This algorithm trains
hidden neurons separately to classify all the pairs of classes. To find best
pairwise classifiers, our algorithm searches for input variables which are
relevant to the classification problem. Despite patient variability and heavily
overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns
correctly classified 80.8% of the training and 80.1% of the testing examples.
Additionally, the neural-network model provides a probabilistic interpretation
of decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504053</id><created>2005-04-13</created><authors><author><keyname>Zharkova</keyname><forenames>V. V.</forenames></author><author><keyname>Schetinin</keyname><forenames>V.</forenames></author></authors><title>A Neural-Network Technique for Recognition of Filaments in Solar Images</title><categories>cs.NE</categories><abstract>  We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504054</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author></authors><title>Learning from Web: Review of Approaches</title><categories>cs.NE cs.LG</categories><abstract>  Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504055</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author></authors><title>A Learning Algorithm for Evolving Cascade Neural Networks</title><categories>cs.NE cs.AI</categories><journal-ref>Neural Processing Letter 17:21-31, 2003. Kluwer</journal-ref><abstract>  A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504056</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>V.</forenames></author></authors><title>Self-Organizing Multilayered Neural Networks of Optimal Complexity</title><categories>cs.NE cs.AI</categories><abstract>  The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504057</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Brazhnikov</keyname><forenames>Anatoly</forenames></author></authors><title>Diagnostic Rule Extraction Using Neural Networks</title><categories>cs.NE cs.AI</categories><abstract>  The neural networks have trained on incomplete sets that a doctor could
collect. Trained neural networks have correctly classified all the presented
instances. The number of intervals entered for encoding the quantitative
variables is equal two. The number of features as well as the number of neurons
and layers in trained neural networks was minimal. Trained neural networks are
adequately represented as a set of logical formulas that more comprehensible
and easy-to-understand. These formulas are as the syndrome-complexes, which may
be easily tabulated and represented as a diagnostic table that the doctors
usually use. Decision rules provide the evaluations of their confidence in
which interested a doctor. Conducted clinical researches have shown that
iagnostic decisions produced by symbolic rules have coincided with the doctor's
conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504058</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author></authors><title>Polynomial Neural Networks Learnt to Classify EEG Signals</title><categories>cs.NE cs.AI</categories><abstract>  A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504059</id><created>2005-04-13</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author></authors><title>A Neural Network Decision Tree for Learning Concepts from EEG Data</title><categories>cs.NE cs.AI</categories><abstract>  To learn the multi-class conceptions from the electroencephalogram (EEG) data
we developed a neural network decision tree (DT), that performs the linear
tests, and a new training algorithm. We found that the known methods fail
inducting the classification models when the data are presented by the features
some of them are irrelevant, and the classes are heavily overlapped. To train
the DT, our algorithm exploits a bottom up search of the features that provide
the best classification accuracy of the linear tests. We applied the developed
algorithm to induce the DT from the large EEG dataset consisted of 65 patients
belonging to 16 age groups. In these recordings each EEG segment was
represented by 72 calculated features. The DT correctly classified 80.8% of the
training and 80.1% of the testing examples. Correspondingly it correctly
classified 89.2% and 87.7% of the EEG recordings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504060</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504060</id><created>2005-04-13</created><updated>2005-08-17</updated><authors><author><keyname>Gemelos</keyname><forenames>George</forenames></author><author><keyname>Sigurjonsson</keyname><forenames>Styrmir</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Universal Minimax Discrete Denoising under Channel Uncertainty</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions of Information Theory</comments><abstract>  The goal of a denoising algorithm is to recover a signal from its
noise-corrupted observations. Perfect recovery is seldom possible and
performance is measured under a given single-letter fidelity criterion. For
discrete signals corrupted by a known discrete memoryless channel, the DUDE was
recently shown to perform this task asymptotically optimally, without knowledge
of the statistical properties of the source. In the present work we address the
scenario where, in addition to the lack of knowledge of the source statistics,
there is also uncertainty in the channel characteristics. We propose a family
of discrete denoisers and establish their asymptotic optimality under a minimax
performance criterion which we argue is appropriate for this setting. As we
show elsewhere, the proposed schemes can also be implemented computationally
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504061</id><created>2005-04-13</created><authors><author><keyname>Afantenos</keyname><forenames>Stergos D.</forenames></author><author><keyname>Karkaletsis</keyname><forenames>Vangelis</forenames></author><author><keyname>Stamatopoulos</keyname><forenames>Panagiotis</forenames></author></authors><title>Summarization from Medical Documents: A Survey</title><categories>cs.CL cs.IR</categories><comments>21 pages, 4 tables</comments><journal-ref>Artificial Intelligence in Medicine, Volume 33, Issue 2, February
  2005, Pages 157-177</journal-ref><doi>10.1016/j.artmed.2004.07.017</doi><abstract>  Objective:
  The aim of this paper is to survey the recent work in medical documents
summarization.
  Background:
  During the last decade, documents summarization got increasing attention by
the AI research community. More recently it also attracted the interest of the
medical research community as well, due to the enormous growth of information
that is available to the physicians and researchers in medicine, through the
large and growing number of published journals, conference proceedings, medical
sites and portals on the World Wide Web, electronic medical records, etc.
  Methodology:
  This survey gives first a general background on documents summarization,
presenting the factors that summarization depends upon, discussing evaluation
issues and describing briefly the various types of summarization techniques. It
then examines the characteristics of the medical domain through the different
types of medical documents. Finally, it presents and discusses the
summarization techniques used so far in the medical domain, referring to the
corresponding systems and their characteristics.
  Discussion and conclusions:
  The paper discusses thoroughly the promising paths for future research in
medical documents summarization. It mainly focuses on the issue of scaling to
large collections of documents in various languages and from different media,
on personalization issues, on portability to new sub-domains, and on the
integration of summarization technology in practical applications
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504062</id><created>2005-04-13</created><authors><author><keyname>Dinur</keyname><forenames>Irit</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Regev</keyname><forenames>Oded</forenames></author></authors><title>Conditional Hardness for Approximate Coloring</title><categories>cs.CC math.PR</categories><abstract>  We study the coloring problem: Given a graph G, decide whether $c(G) \leq q$
or $c(G) \ge Q$, where c(G) is the chromatic number of G. We derive conditional
hardness for this problem for any constant $3 \le q &lt; Q$. For $q\ge 4$, our
result is based on Khot's 2-to-1 conjecture [Khot'02]. For $q=3$, we base our
hardness result on a certain `fish shaped' variant of his conjecture.
  We also prove that the problem almost coloring is hard for any constant
$\eps&gt;0$, assuming Khot's Unique Games conjecture. This is the problem of
deciding for a given graph, between the case where one can 3-color all but a
$\eps$ fraction of the vertices without monochromatic edges, and the case where
the graph contains no independent set of relative size at least $\eps$.
  Our result is based on bounding various generalized noise-stability
quantities using the invariance principle of Mossel et al [MOO'05].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504063</id><created>2005-04-14</created><authors><author><keyname>Palotai</keyname><forenames>Zs.</forenames></author><author><keyname>Farkas</keyname><forenames>Cs.</forenames></author><author><keyname>Lorincz</keyname><forenames>A.</forenames></author></authors><title>Selection in Scale-Free Small World</title><categories>cs.LG cs.IR</categories><comments>24 pages, 3 figures</comments><acm-class>H.3.3</acm-class><abstract>  In this paper we compare the performance characteristics of our selection
based learning algorithm for Web crawlers with the characteristics of the
reinforcement learning algorithm. The task of the crawlers is to find new
information on the Web. The selection algorithm, called weblog update, modifies
the starting URL lists of our crawlers based on the found URLs containing new
information. The reinforcement learning algorithm modifies the URL orderings of
the crawlers based on the received reinforcements for submitted documents. We
performed simulations based on data collected from the Web. The collected
portion of the Web is typical and exhibits scale-free small world (SFSW)
structure. We have found that on this SFSW, the weblog update algorithm
performs better than the reinforcement learning algorithm. It finds the new
information faster than the reinforcement learning algorithm and has better new
information/all submitted documents ratio. We believe that the advantages of
the selection algorithm over reinforcement learning algorithm is due to the
small world property of the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504064</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Schult</keyname><forenames>Joachim</forenames></author><author><keyname>Brazhnikov</keyname><forenames>Anatoly</forenames></author></authors><title>Neural-Network Techniques for Visual Mining Clinical
  Electroencephalograms</title><categories>cs.AI</categories><abstract>  In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs), the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504065</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Fieldsend</keyname><forenames>Jonathan E.</forenames></author><author><keyname>Partridge</keyname><forenames>Derek</forenames></author><author><keyname>Krzanowski</keyname><forenames>Wojtek J.</forenames></author><author><keyname>Everson</keyname><forenames>Richard M.</forenames></author><author><keyname>Bailey</keyname><forenames>Trevor C.</forenames></author><author><keyname>Hernandez</keyname><forenames>Adolfo</forenames></author></authors><title>Estimating Classification Uncertainty of Bayesian Decision Tree
  Technique on Financial Data</title><categories>cs.AI</categories><abstract>  Bayesian averaging over classification models allows the uncertainty of
classification outcomes to be evaluated, which is of crucial importance for
making reliable decisions in applications such as financial in which risks have
to be estimated. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the diversity of a
classifier ensemble and the required performance. The interpretability of
classification models can also give useful information for experts responsible
for making reliable classifications. For this reason Decision Trees (DTs) seem
to be attractive classification models. The required diversity of the DT
ensemble can be achieved by using the Bayesian model averaging all possible
DTs. In practice, the Bayesian approach can be implemented on the base of a
Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior
distribution. For sampling large DTs, the MCMC method is extended by Reversible
Jump technique which allows inducing DTs under given priors. For the case when
the prior information on the DT size is unavailable, the sweeping technique
defining the prior implicitly reveals a better performance. Within this Chapter
we explore the classification uncertainty of the Bayesian MCMC techniques on
some datasets from the StatLog Repository and real financial data. The
classification uncertainty is compared within an Uncertainty Envelope technique
dealing with the class posterior distribution and a given confidence
probability. This technique provides realistic estimates of the classification
uncertainty which can be easily interpreted in statistical terms with the aim
of risk evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504066</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Fieldsend</keyname><forenames>Jonathan E.</forenames></author><author><keyname>Partridge</keyname><forenames>Derek</forenames></author><author><keyname>Krzanowski</keyname><forenames>Wojtek J.</forenames></author><author><keyname>Everson</keyname><forenames>Richard M.</forenames></author><author><keyname>Bailey</keyname><forenames>Trevor C.</forenames></author><author><keyname>Hernandez</keyname><forenames>Adolfo</forenames></author></authors><title>Comparison of the Bayesian and Randomised Decision Tree Ensembles within
  an Uncertainty Envelope Technique</title><categories>cs.AI</categories><journal-ref>Journal of Mathematical Modelling and Algorithms, 2005</journal-ref><abstract>  Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of
classification outcomes that is of crucial importance for safety critical
applications. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the classifier diversity and
the required performance. The interpretability of MCSs can also give useful
information for experts responsible for making reliable classifications. For
this reason Decision Trees (DTs) seem to be attractive classification models
for experts. The required diversity of MCSs exploiting such classification
models can be achieved by using two techniques, the Bayesian model averaging
and the randomised DT ensemble. Both techniques have revealed promising results
when applied to real-world problems. In this paper we experimentally compare
the classification uncertainty of the Bayesian model averaging with a
restarting strategy and the randomised DT ensemble on a synthetic dataset and
some domain problems commonly used in the machine learning community. To make
the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo
technique. The classification uncertainty is evaluated within an Uncertainty
Envelope technique dealing with the class posterior distribution and a given
confidence probability. Exploring a full posterior distribution, this technique
produces realistic estimates which can be easily interpreted in statistical
terms. In our experiments we found out that the Bayesian DTs are superior to
the randomised DT ensembles within the Uncertainty Envelope technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504067</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author></authors><title>An Evolving Cascade Neural Network Technique for Cleaning Sleep
  Electroencephalograms</title><categories>cs.NE cs.AI</categories><journal-ref>Natural Computing Application, 2005</journal-ref><abstract>  Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable
of selecting informative features are described. The ECNN initially learns with
one input node and then evolves by adding new inputs as well as new hidden
neurons. The resultant ECNN has a near minimal number of hidden neurons and
inputs. The algorithm is successfully used for training ECNN to recognise
artefacts in sleep electroencephalograms (EEGs) which were visually labelled by
EEG-viewers. In our experiments, the ECNN outperforms the standard
neural-network as well as evolutionary techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504068</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>V.</forenames></author><author><keyname>Kostunin</keyname><forenames>A.</forenames></author></authors><title>Self-Organization of the Neuron Collective of Optimal Complexity</title><categories>cs.NE cs.AI</categories><comments>NOLTA-1996, Japan</comments><abstract>  The optimal complexity of neural networks is achieved when the
self-organization principles is used to eliminate the contradictions existing
in accordance with the K. Godel theorem about incompleteness of the systems
based on axiomatics. The principle of S. Beer exterior addition the Heuristic
Group Method of Data Handling by A. Ivakhnenko realized is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504069</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Schult</keyname><forenames>Joachim</forenames></author></authors><title>A Neural-Network Technique to Learn Concepts from Electroencephalograms</title><categories>cs.NE cs.AI cs.LG</categories><report-no>Theory in Biosciences, 2005</report-no><abstract>  A new technique is presented developed to learn multi-class concepts from
clinical electroencephalograms. A desired concept is represented as a neuronal
computational model consisting of the input, hidden, and output neurons. In
this model the hidden neurons learn independently to classify the
electroencephalogram segments presented by spectral and statistical features.
This technique has been applied to the electroencephalogram data recorded from
65 sleeping healthy newborns in order to learn a brain maturation concept of
newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these
data have been used for learning and testing the concept, respectively. As a
result, the concept has correctly classified 80.1% of the testing segments or
87.7% of the 65 records.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504070</id><created>2005-04-14</created><authors><author><keyname>Schetinin</keyname><forenames>Vitaly</forenames></author><author><keyname>Schult</keyname><forenames>Joachim</forenames></author></authors><title>The Combined Technique for Detection of Artifacts in Clinical
  Electroencephalograms of Sleeping Newborns</title><categories>cs.NE cs.AI cs.LG</categories><abstract>  In this paper we describe a new method combining the polynomial neural
network and decision tree techniques in order to derive comprehensible
classification rules from clinical electroencephalograms (EEGs) recorded from
sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement,
muscle and noise artifacts and as a consequence some EEG features are
irrelevant to classification problems. Combining the polynomial network and
decision tree techniques, we discover comprehensible classification rules
whilst also attempting to keep their classification error down. This technique
is shown to outperform a number of commonly used machine learning technique
applied to automatically recognize artifacts in the sleep EEGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504071</id><created>2005-04-14</created><authors><author><keyname>Kang</keyname><forenames>Byeong Ho</forenames></author><author><keyname>Hoffmann</keyname><forenames>Achim</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Takahira</forenames></author><author><keyname>Yeap</keyname><forenames>Wai Kiang</forenames></author></authors><title>Proceedings of the Pacific Knowledge Acquisition Workshop 2004</title><categories>cs.AI</categories><abstract>  Artificial intelligence (AI) research has evolved over the last few decades
and knowledge acquisition research is at the core of AI research. PKAW-04 is
one of three international knowledge acquisition workshops held in the
Pacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong
emphasis on incremental knowledge acquisition, machine learning, neural nets
and active mining.
  The proceedings contain 19 papers that were selected by the program committee
among 24 submitted papers. All papers were peer reviewed by at least two
reviewers. The papers in these proceedings cover the methods and tools as well
as the applications related to develop expert systems or knowledge based
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504072</id><created>2005-04-14</created><authors><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author><author><keyname>Chow</keyname><forenames>Edmond</forenames></author><author><keyname>Eliassi-Rad</keyname><forenames>Tina</forenames></author></authors><title>Knowledge Representation Issues in Semantic Graphs for Relationship
  Detection</title><categories>cs.AI physics.soc-ph</categories><comments>9 pages, 2 tables, 7 figures</comments><journal-ref>Papers from the 2005 AAAI Spring Symposium, AAAI Press, 2005, pp.
  91-98</journal-ref><abstract>  An important task for Homeland Security is the prediction of threat
vulnerabilities, such as through the detection of relationships between
seemingly disjoint entities. A structure used for this task is a &quot;semantic
graph&quot;, also known as a &quot;relational data graph&quot; or an &quot;attributed relational
graph&quot;. These graphs encode relationships as &quot;typed&quot; links between a pair of
&quot;typed&quot; nodes. Indeed, semantic graphs are very similar to semantic networks
used in AI. The node and link types are related through an ontology graph (also
known as a schema). Furthermore, each node has a set of attributes associated
with it (e.g., &quot;age&quot; may be an attribute of a node of type &quot;person&quot;).
Unfortunately, the selection of types and attributes for both nodes and links
depends on human expertise and is somewhat subjective and even arbitrary. This
subjectiveness introduces biases into any algorithm that operates on semantic
graphs. Here, we raise some knowledge representation issues for semantic graphs
and provide some possible solutions using recently developed ideas in the field
of complex networks. In particular, we use the concept of transitivity to
evaluate the relevance of individual links in the semantic graph for detecting
relationships. We also propose new statistical measures for semantic graphs and
illustrate these semantic measures on graphs constructed from movies and
terrorism data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504073</id><created>2005-04-14</created><authors><author><keyname>Seada</keyname><forenames>Karim</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Rendezvous Regions: A Scalable Architecture for Resource Discovery and
  Service Location in Large-Scale Mobile Networks</title><categories>cs.NI</categories><abstract>  In large-scale wireless networks such as mobile ad hoc and sensor networks,
efficient and robust service discovery and data-access mechanisms are both
essential and challenging. Rendezvous-based mechanisms provide a valuable
solution for provisioning a wide range of services. In this paper, we describe
Rendezvous Regions (RRs) - a novel scalable rendezvous-based architecture for
wireless networks. RR is a general architecture proposed for service location
and bootstrapping in ad hoc networks, in addition to data-centric storage,
configuration, and task assignment in sensor networks. In RR the network
topology is divided into geographical regions, where each region is responsible
for a set of keys representing the services or data of interest. Each key is
mapped to a region based on a hash-table-like mapping scheme. A few elected
nodes inside each region are responsible for maintaining the mapped
information. The service or data provider stores the information in the
corresponding region and the seekers retrieve it from there. We run extensive
detailed simulations, and high-level simulations and analysis, to investigate
the design space, and study the architecture in various environments including
node mobility and failures. We evaluate it against other approaches to identify
its merits and limitations. The results show high success rate and low overhead
even with dynamics. RR scales to large number of nodes and is highly robust and
efficient to node failures. It is also robust to node mobility and location
inaccuracy with a significant advantage over point-based rendezvous mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504074</id><created>2005-04-15</created><authors><author><keyname>Rodriguez</keyname><forenames>Carlos</forenames></author></authors><title>Metalinguistic Information Extraction for Terminology</title><categories>cs.CL cs.AI cs.IR</categories><comments>Presented at CompuTerm 2004, COLING. Geneve</comments><acm-class>I.2.7</acm-class><abstract>  This paper describes and evaluates the Metalinguistic Operation Processor
(MOP) system for automatic compilation of metalinguistic information from
technical and scientific documents. This system is designed to extract
non-standard terminological resources that we have called Metalinguistic
Information Databases (or MIDs), in order to help update changing glossaries,
knowledge bases and ontologies, as well as to reflect the metastable dynamics
of special-domain knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504075</id><created>2005-04-15</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>Dichotomy for Voting Systems</title><categories>cs.GT cs.CC cs.MA</categories><report-no>URCS-TR-2005-861</report-no><acm-class>I.2.11; F.1.3; F.2.2</acm-class><abstract>  Scoring protocols are a broad class of voting systems. Each is defined by a
vector $(\alpha_1,\alpha_2,...,\alpha_m)$, $\alpha_1 \geq \alpha_2 \geq &gt;...
\geq \alpha_m$, of integers such that each voter contributes $\alpha_1$ points
to his/her first choice, $\alpha_2$ points to his/her second choice, and so on,
and any candidate receiving the most points is a winner.
  What is it about scoring-protocol election systems that makes some have the
desirable property of being NP-complete to manipulate, while others can be
manipulated in polynomial time? We find the complete, dichotomizing answer:
Diversity of dislike. Every scoring-protocol election system having two or more
point values assigned to candidates other than the favorite--i.e., having
$||\{\alpha_i \condition 2 \leq i \leq m\}||\geq 2$--is NP-complete to
manipulate. Every other scoring-protocol election system can be manipulated in
polynomial time. In effect, we show that--other than trivial systems (where all
candidates alway tie), plurality voting, and plurality voting's transparently
disguised translations--\emph{every} scoring-protocol election system is
NP-complete to manipulate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504076</id><created>2005-04-16</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>An Improved Remote User Authentication Scheme Using Smart Cards</title><categories>cs.CR</categories><comments>16 pages, no figures</comments><acm-class>C.3; D.4.6; H.2.0; K.6.5</acm-class><abstract>  In 2000, Hwang and Li proposed a new remote user authentication scheme using
smart cards. In the same year, Chan and Cheng pointed out that Hwang and
Li&amp;#8217;s scheme is not secure against the masquerade attack. Further, in
2003, Shen, Lin and Hwang pointed out a different type of attack on Hwang and
Li&amp;#8217;s scheme and presented a modified scheme to remove its security
pitfalls. This paper presents an improved scheme which is secure against
Chan-Cheng and all the extended attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504077</id><created>2005-04-16</created><authors><author><keyname>Kumar</keyname><forenames>Manoj</forenames></author></authors><title>The Modified Scheme is still vulnerable to the parallel Session Attack</title><categories>cs.CR</categories><comments>12 pages, no figures</comments><acm-class>C.3; D.4.6; H.2.0; K.6.5</acm-class><abstract>  In 2002, Chien&amp;#8211;Jan&amp;#8211;Tseng introduced an efficient remote user
authentication scheme using smart cards. Further, in 2004, W. C. Ku and S. M.
Chen proposed an efficient remote user authentication scheme using smart cards
to solve the security problems of Chien et al.&amp;#8217;s scheme. Recently, Hsu
and Yoon et al. pointed out the security weakness of the Ku and Chen&amp;#8217;s
scheme Furthermore, Yoon et al. modified the password change phase of Ku and
Chen&amp;#8217;s scheme and they also proposed a new efficient remote user
authentication scheme using smart cards. This paper analyzes that the modified
scheme of Yoon et al. still vulnerable to parallel session attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504078</id><created>2005-04-16</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Poland</keyname><forenames>Jan</forenames></author></authors><title>Adaptive Online Prediction by Following the Perturbed Leader</title><categories>cs.AI cs.LG</categories><comments>25 pages</comments><report-no>IDSIA-10-05</report-no><acm-class>I.2.6; G.3</acm-class><journal-ref>Journal of Machine Learning Research 6 (2005) 639--660</journal-ref><abstract>  When applying aggregating strategies to Prediction with Expert Advice, the
learning rate must be adaptively tuned. The natural choice of
sqrt(complexity/current loss) renders the analysis of Weighted Majority
derivatives quite complicated. In particular, for arbitrary weights there have
been no results proven so far. The analysis of the alternative &quot;Follow the
Perturbed Leader&quot; (FPL) algorithm from Kalai &amp; Vempala (2003) (based on
Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate
and both finite expert classes with uniform weights and countable expert
classes with arbitrary weights. For the former setup, our loss bounds match the
best known results so far, while for the latter our results are new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504079</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504079</id><created>2005-04-17</created><updated>2005-04-21</updated><authors><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author><author><keyname>Astola</keyname><forenames>Jaakko</forenames></author></authors><title>Prediction of Large Alphabet Processes and Its Application to Adaptive
  Source Coding</title><categories>cs.IT math.IT</categories><comments>submitted</comments><abstract>  The problem of predicting a sequence $x_1,x_2,...$ generated by a discrete
source with unknown statistics is considered. Each letter $x_{t+1}$ is
predicted using information on the word $x_1x_2... x_t$ only. In fact, this
problem is a classical problem which has received much attention. Its history
can be traced back to Laplace. We address the problem where each $x_i$ belongs
to some large (or even infinite) alphabet. A method is presented for which the
precision is greater than for known algorithms, where precision is estimated by
the Kullback-Leibler divergence. The results can readily be translated to
results about adaptive coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504080</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504080</id><created>2005-04-17</created><authors><author><keyname>Perera</keyname><forenames>Rasika</forenames></author><author><keyname>Pollock</keyname><forenames>Tony</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara</forenames></author></authors><title>Performance of Gaussian Signalling in Non Coherent Rayleigh Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>06 pages, 04 figures, Correspondence</comments><report-no>CLN: 5-340</report-no><abstract>  The mutual information of a discrete time memoryless Rayleigh fading channel
is considered, where neither the transmitter nor the receiver has the knowledge
of the channel state information except the fading statistics. We present the
mutual information of this channel in closed form when the input distribution
is complex Gaussian, and derive a lower bound in terms of the capacity of the
corresponding non fading channel and the capacity when the perfect channel
state information is known at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504081</identifier>
 <datestamp>2007-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504081</id><created>2005-04-17</created><authors><author><keyname>Earl</keyname><forenames>Matthew</forenames></author><author><keyname>D'Andrea</keyname><forenames>Raffaello</forenames></author></authors><title>A Decomposition Approach to Multi-Vehicle Cooperative Control</title><categories>cs.RO</categories><comments>36 pages, 19 figures, for associated web page see
  http://control.mae.cornell.edu/earl/decomp</comments><acm-class>I.2.9; I.2.8; I.2.11</acm-class><journal-ref>M. G. Earl and R. D'Andrea, &quot;A Decomposition Approach to
  Multi-Vehicle Cooperative Control,&quot; Robotics and Autonomous Systems, Volume
  55, Issue 4, pages 276-291, April 2007.</journal-ref><doi>10.1016/j.robot.2006.11.002</doi><abstract>  We present methods that generate cooperative strategies for multi-vehicle
control problems using a decomposition approach. By introducing a set of tasks
to be completed by the team of vehicles and a task execution method for each
vehicle, we decomposed the problem into a combinatorial component and a
continuous component. The continuous component of the problem is captured by
task execution, and the combinatorial component is captured by task assignment.
In this paper, we present a solver for task assignment that generates
near-optimal assignments quickly and can be used in real-time applications. To
motivate our methods, we apply them to an adversarial game between two teams of
vehicles. One team is governed by simple rules and the other by our algorithms.
In our study of this game we found phase transitions, showing that the task
assignment problem is most difficult to solve when the capabilities of the
adversaries are comparable. Finally, we implement our algorithms in a
multi-level architecture with a variable replanning rate at each level to
provide feedback on a dynamically changing and uncertain environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504082</id><created>2005-04-18</created><authors><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames><affiliation>Leibniz - IMAG</affiliation></author><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>Leibniz - IMAG</affiliation></author><author><keyname>Reed</keyname><forenames>Bruce</forenames><affiliation>Leibniz - IMAG</affiliation></author><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames><affiliation>Leibniz - IMAG</affiliation></author></authors><title>Coloring Artemis graphs</title><categories>cs.DM</categories><proxy>ccsd ccsd-00004741</proxy><acm-class>G.2.2</acm-class><abstract>  We consider the class A of graphs that contain no odd hole, no antihole, and
no ``prism'' (a graph consisting of two disjoint triangles with three disjoint
paths between them). We show that the coloring algorithm found by the second
and fourth author can be implemented in time O(n^2m) for any graph in A with n
vertices and m edges, thereby improving on the complexity proposed in the
original paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504083</id><created>2005-04-18</created><authors><author><keyname>Weiming</keyname><forenames>Zhang</forenames></author><author><keyname>Shiqu</keyname><forenames>Li</forenames></author></authors><title>On the Unicity Distance of Stego Key</title><categories>cs.CR</categories><comments>8 pages, 3 figures</comments><acm-class>D.2.11; E.3</acm-class><abstract>  Steganography is about how to send secret message covertly. And the purpose
of steganalysis is to not only detect the existence of the hidden message but
also extract it. So far there have been many reliable detecting methods on
various steganographic algorithms, while there are few approaches that can
extract the hidden information. In this paper, the difficulty of extracting
hidden information, which is essentially a kind of privacy, is analyzed with
information-theoretic method in the terms of unicity distance of steganographic
key (abbreviated stego key). A lower bound for the unicity distance is
obtained, which shows the relations between key rate, message rate, hiding
capacity and difficulty of extraction. Furthermore the extracting attack to
steganography is viewed as a special kind of cryptanalysis, and an effective
method on recovering the stego key of popular LSB replacing steganography in
spatial images is presented by combining the detecting technique of
steganalysis and correlation attack of cryptanalysis together. The analysis for
this method and experimental results on steganographic software ``Hide and Seek
4.1&quot; are both accordant with the information-theoretic conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504084</id><created>2005-04-18</created><updated>2005-09-12</updated><authors><author><keyname>Rodriguez</keyname><forenames>Marko A.</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>The Convergence of Digital-Libraries and the Peer-Review Process</title><categories>cs.DL cs.CY</categories><comments>Journal of Information Science [in press]</comments><report-no>http://jis.sagepub.com/cgi/content/abstract/32/2/149</report-no><acm-class>H.3.7</acm-class><journal-ref>Journal of Information Science, 32(2), pp. 149-159, 2006.</journal-ref><doi>10.1177/0165551506062327</doi><abstract>  Pre-print repositories have seen a significant increase in use over the past
fifteen years across multiple research domains. Researchers are beginning to
develop applications capable of using these repositories to assist the
scientific community above and beyond the pure dissemination of information.
The contribution set forth by this paper emphasizes a deconstructed publication
model in which the peer-review process is mediated by an OAI-PMH peer-review
service. This peer-review service uses a social-network algorithm to determine
potential reviewers for a submitted manuscript and for weighting the relative
influence of each participating reviewer's evaluations. This paper also
suggests a set of peer-review specific metadata tags that can accompany a
pre-print's existing metadata record. The combinations of these contributions
provide a unique repository-centric peer-review model that fits within the
widely deployed OAI-PMH framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504085</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504085</id><created>2005-04-18</created><updated>2005-06-10</updated><authors><author><keyname>Sethuraman</keyname><forenames>Vignesh</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Capacity per Unit Energy of Fading Channels with a Peak Constraint</title><categories>cs.IT math.IT</categories><comments>Journal version of paper presented in ISIT 2003 - now accepted for
  publication in IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2005.853329</doi><abstract>  A discrete-time single-user scalar channel with temporally correlated
Rayleigh fading is analyzed. There is no side information at the transmitter or
the receiver. A simple expression is given for the capacity per unit energy, in
the presence of a peak constraint. The simple formula of Verdu for capacity per
unit cost is adapted to a channel with memory, and is used in the proof. In
addition to bounding the capacity of a channel with correlated fading, the
result gives some insight into the relationship between the correlation in the
fading process and the channel capacity. The results are extended to a channel
with side information, showing that the capacity per unit energy is one nat per
Joule, independently of the peak power constraint.
  A continuous-time version of the model is also considered. The capacity per
unit energy subject to a peak constraint (but no bandwidth constraint) is given
by an expression similar to that for discrete time, and is evaluated for
Gauss-Markov and Clarke fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504086</id><created>2005-04-19</created><authors><author><keyname>Pelckmans</keyname><forenames>Kristiaan</forenames></author><author><keyname>Goethals</keyname><forenames>Ivan</forenames></author><author><keyname>De Brabanter</keyname><forenames>Jos</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Componentwise Least Squares Support Vector Machines</title><categories>cs.LG cs.AI</categories><comments>22 pages. Accepted for publication in Support Vector Machines: Theory
  and Applications, ed. L. Wang, 2005</comments><acm-class>I.2.6</acm-class><abstract>  This chapter describes componentwise Least Squares Support Vector Machines
(LS-SVMs) for the estimation of additive models consisting of a sum of
nonlinear components. The primal-dual derivations characterizing LS-SVMs for
the estimation of the additive model result in a single set of linear equations
with size growing in the number of data-points. The derivation is elaborated
for the classification as well as the regression case. Furthermore, different
techniques are proposed to discover structure in the data by looking for sparse
components in the model based on dedicated regularization schemes on the one
hand and fusion of the componentwise LS-SVMs training with a validation
criterion on the other hand. (keywords: LS-SVMs, additive models,
regularization, structure detection)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504087</id><created>2005-04-19</created><updated>2005-04-21</updated><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>Improved direct sum theorem in classical communication complexity</title><categories>cs.OH</categories><comments>Withdrawn due to critical error</comments><abstract>  Withdrawn due to critical error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504088</id><created>2005-04-20</created><authors><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI, University of Amsterdam, and National ICT Australia</affiliation></author></authors><title>Time, Space, and Energy in Reversible Computing</title><categories>cs.CC</categories><comments>LaTeX, 10 pages, Proc. 2005 ACM International Conference on Computing
  Frontiers, Ischia, Italy, 4-6 May 2005 (1st Intn'l Workshop on Reversible
  Computing)</comments><acm-class>F.1; F.2</acm-class><abstract>  We survey results of a quarter century of work on computation by reversible
general-purpose computers (in this setting Turing machines), and general
reversible simulation of irreversible computations, with respect to energy-,
time- and space requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504089</id><created>2005-04-20</created><updated>2005-07-07</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI, University of Amsterdam, and National ICT Australia</affiliation></author></authors><title>Universal Similarity</title><categories>cs.IR cs.AI cs.CL physics.data-an</categories><comments>LaTeX, 5 pages, Proc. ITW2005 - IEEE ITSOC Information Theory
  Workshop 2005 on Coding and Complexity, 29th Aug. - 1st Sept., 2005, Rotorua,
  New Zealand. (invited). The second version has minor changes, and corrections
  getting rid of typos: in (2) we &quot;max&quot; in the denominator is replaced by the
  correct &quot;min&quot;, some refs to (1) in the text should have been to (2)</comments><acm-class>I.2; I.5; J.5; H.2; H.3; H.4; H.2.8; E.2; E.4</acm-class><abstract>  We survey a new area of parameter-free similarity distance measures useful in
data-mining, pattern recognition, learning and automatic semantics extraction.
Given a family of distances on a set of objects, a distance is universal up to
a certain precision for that family if it minorizes every distance in the
family between every two objects in the set, up to the stated precision (we do
not require the universal distance to be an element of the family). We consider
similarity distances for two types of objects: literal objects that as such
contain all of their meaning, like genomes or books, and names for objects. The
latter may have literal embodyments like the first type, but may also be
abstract like ``red'' or ``christianity.'' For the first type we consider a
family of computable distance measures corresponding to parameters expressing
similarity according to particular features between pairs of literal objects.
For the second type we consider similarity distances generated by web users
corresponding to particular semantic relations between the (names for) the
designated objects. For both families we give universal similarity distance
measures, incorporating all particular distance measures in the family. In the
first case the universal distance is based on compression and in the second
case it is based on Google page counts related to search terms. In both cases
experiments on a massive scale give evidence of the viability of the
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504090</id><created>2005-04-21</created><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Discrete Morse Theory for free chain complexes</title><categories>cs.DM math.RA</categories><journal-ref>Comptes Rendus Mathematique, Acad. Sci. Paris, Ser. I 340 (2005),
  pp. 867-872</journal-ref><abstract>  We extend the combinatorial Morse complex construction to the arbitrary free
chain complexes, and give a short, self-contained, and elementary proof of the
quasi-isomorphism between the original chain complex and its Morse complex.
  Even stronger, the main result states that, if $C_*$ is a free chain complex,
and $\cm$ an acyclic matching, then $C_*=C_*^\cm\oplus T_*$, where $C_*^\cm$ is
the Morse complex generated by the critical elements, and $T_*$ is an acyclic
complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504091</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504091</id><created>2005-04-21</created><authors><author><keyname>DeStefano</keyname><forenames>Joseph</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>A Probabilistic Upper Bound on Differential Entropy</title><categories>cs.IT math.IT</categories><abstract>  A novel, non-trivial, probabilistic upper bound on the entropy of an unknown
one-dimensional distribution, given the support of the distribution and a
sample from that distribution, is presented. No knowledge beyond the support of
the unknown distribution is required, nor is the distribution required to have
a density. Previous distribution-free bounds on the cumulative distribution
function of a random variable given a sample of that variable are used to
construct the bound. A simple, fast, and intuitive algorithm for computing the
entropy bound from a sample is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504092</id><created>2005-04-23</created><authors><author><keyname>Korotkikh</keyname><forenames>Victor</forenames></author><author><keyname>Korotkikh</keyname><forenames>Galina</forenames></author><author><keyname>Bond</keyname><forenames>Darryl</forenames></author></authors><title>On Optimality Condition of Complex Systems: Computational Evidence</title><categories>cs.CC cond-mat.dis-nn nlin.AO</categories><comments>5 pages, 4 figures</comments><abstract>  A general condition determining the optimal performance of a complex system
has not yet been found and the possibility of its existence is unknown. To
contribute in this direction, an optimization algorithm as a complex system is
presented. The performance of the algorithm for any problem is controlled as a
convex function with a single optimum. To characterize the performance
optimums, certain quantities of the algorithm and the problem are suggested and
interpreted as their complexities. An optimality condition of the algorithm is
computationally found: if the algorithm shows its best performance for a
problem, then the complexity of the algorithm is in a linear relationship with
the complexity of the problem. The optimality condition provides a new
perspective to the subject by recognizing that the relationship between certain
quantities of the complex system and the problem may determine the optimal
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504093</id><created>2005-04-23</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K</forenames></author><author><keyname>Lal</keyname><forenames>Sunder</forenames></author></authors><title>A Multi-proxy Signature Scheme for Partial delegation with Warrant</title><categories>cs.CR</categories><comments>Old Version 6 Pages</comments><abstract>  In some cases, the original signer may delegate its signing power to a
specified proxy group while ensuring individual accountability of each
participantsigner. The proxy signature scheme that achieves such purpose is
called the multi-proxy signature scheme and the signature generated by the
specified proxy group is called multi-proxy signature for the original signer.
Recently such scheme has been discussed by Lin et al. Lins scheme is based on
partial delegation by Mambo et al. In present chapter we introduce a new
multi-proxy signature scheme, which requires less computational overhead in
comparison to Lin et al, and also fulfill the requirement of partial delegation
with warrant simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504094</id><created>2005-04-23</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K</forenames></author></authors><title>A New Remote User Authentication Scheme Using Smart Cards with Check
  Digits</title><categories>cs.CR</categories><comments>3 Pages. Not Published</comments><abstract>  Since 1981, when Lamport introduced the remote user authentication scheme
using table, a plenty of schemes had been proposed with table and without table
using. In 1993, Chang and Wu [5] introduced Remote password authentication
scheme with smart cards. A number of remote authentication schemes with smart
cards have been proposed since then. These schemes allow a valid user to login
a remote server and access the services provided by the remote server. But
still there is no scheme to authenticate the remote proxy user. In this paper
we propose firstly, a protocol to authenticate a proxy user remotely using
smartcards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504095</id><created>2005-04-23</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K</forenames></author><author><keyname>Lal</keyname><forenames>Sunder</forenames></author></authors><title>An Efficient Scheme for Sensitive Message Transmission using Blind
  Signcryption</title><categories>cs.CR</categories><comments>5 Pages. Intenational conference on sommunication. India 22 Dec 2004,
  Sastra Deemed Univ. Kumabakonam</comments><abstract>  Blind signature schemes enable a useful protocol that guarantee the anonymity
of the participants while Signcryption offers authentication of message and
confidentiality of messages at the same time and more efficiently. In this
paper, we present a blind signcryption scheme that combines the functionality
of blind signature and signcryption. This blind Signcryption is useful for
applications that are based on anonymity untracebility and unlinkability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504096</id><created>2005-04-25</created><updated>2005-12-07</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Torenvliet</keyname><forenames>Leen</forenames></author></authors><title>P-Selectivity, Immunity, and the Power of One Bit</title><categories>cs.CC</categories><report-no>URCS-TR-2005-864</report-no><acm-class>F.1.3</acm-class><abstract>  We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is
not EXP/1-immune. That is, we prove that some infinite P-selective set has no
infinite EXP-time subset, but we also prove that every infinite P-selective set
has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so
fragile that it is pierced by a single bit of information.
  The above claims follow from broader results that we obtain about the
immunity of the P-selective sets. In particular, we prove that for every
recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is
not \Pi_2^p/1-immune.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504097</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504097</id><created>2005-04-23</created><authors><author><keyname>Awasthi</keyname><forenames>Amit K</forenames></author><author><keyname>Lal</keyname><forenames>Sunder</forenames></author></authors><title>ID-based Ring Signature and Proxy Ring Signature Schemes from Bilinear
  Pairings</title><categories>cs.CR</categories><comments>Published with ePrint Archive</comments><doi>10.13140/2.1.2549.1529</doi><abstract>  In 2001, Rivest et al. firstly introduced the concept of ring signatures. A
ring signature is a simplified group signature without any manager. It protects
the anonymity of a signer. The first scheme proposed by Rivest et al. was based
on RSA cryptosystem and certificate based public key setting. The first ring
signature scheme based on DLP was proposed by Abe, Ohkubo, and Suzuki. Their
scheme is also based on the general certificate-based public key setting too.
In 2002, Zhang and Kim proposed a new ID-based ring signature scheme using
pairings. Later Lin and Wu proposed a more efficient ID-based ring signature
scheme. Both these schemes have some inconsistency in computational aspect.
  In this paper we propose a new ID-based ring signature scheme and a proxy
ring signature scheme. Both the schemes are more efficient than existing one.
These schemes also take care of the inconsistencies in above two schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504099</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504099</id><created>2005-04-24</created><authors><author><keyname>Mhatre</keyname><forenames>Vivek P.</forenames></author><author><keyname>Rosenberg</keyname><forenames>Catherine P.</forenames></author></authors><title>The Capacity of Random Ad hoc Networks under a Realistic Link Layer
  Model</title><categories>cs.IT cs.NI math.IT</categories><comments>12 pages, 6 figures, submitted to IEEE Transactions on Information
  Theory, April 21st 2005</comments><abstract>  The problem of determining asymptotic bounds on the capacity of a random ad
hoc network is considered. Previous approaches assumed a threshold-based link
layer model in which a packet transmission is successful if the SINR at the
receiver is greater than a fixed threshold. In reality, the mapping from SINR
to packet success probability is continuous. Hence, over each hop, for every
finite SINR, there is a non-zero probability of packet loss. With this more
realistic link model, it is shown that for a broad class of routing and
scheduling schemes, a fixed fraction of hops on each route have a fixed
non-zero packet loss probability. In a large network, a packet travels an
asymptotically large number of hops from source to destination. Consequently,
it is shown that the cumulative effect of per-hop packet loss results in a
per-node throughput of only O(1/n) (instead of Theta(1/sqrt{n log{n}})) as
shown previously for the threshold-based link model).
  A scheduling scheme is then proposed to counter this effect. The proposed
scheme improves the link SINR by using conservative spatial reuse, and improves
the per-node throughput to O(1/(K_n sqrt{n log{n}})), where each cell gets a
transmission opportunity at least once every K_n slots, and K_n tends to
infinity as n tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504100</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504100</id><created>2005-04-25</created><updated>2005-06-07</updated><authors><author><keyname>Bao</keyname><forenames>Sheng</forenames></author><author><keyname>Chen</keyname><forenames>Shi</forenames></author><author><keyname>Jing</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Ren</keyname><forenames>Ran</forenames></author></authors><title>A DNA Sequence Compression Algorithm Based on LUT and LZ77</title><categories>cs.IT math.IT</categories><comments>13 pages,3 tables,with source code of our algorithm</comments><acm-class>J.3; E.4</acm-class><abstract>  This article introduces a new DNA sequence compression algorithm which is
based on LUT and LZ77 algorithm. Combined a LUT-based pre-coding routine and
LZ77 compression routine,this algorithm can approach a compression ratio of
1.9bits \slash base and even lower.The biggest advantage of this algorithm is
fast execution, small memory occupation and easy implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504101</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504101</id><created>2005-04-25</created><updated>2005-08-18</updated><authors><author><keyname>Znidaric</keyname><forenames>Marko</forenames></author></authors><title>Single-solution Random 3-SAT Instances</title><categories>cs.AI cs.CC cs.DM</categories><comments>18 pages, 9 figures; considerable revision</comments><abstract>  We study a class of random 3-SAT instances having exactly one solution. The
properties of this ensemble considerably differ from those of a random 3-SAT
ensemble. It is numerically shown that the running time of several complete and
stochastic local search algorithms monotonically increases as the clause
density is decreased. Therefore, there is no easy-hard-easy pattern of hardness
as for standard random 3-SAT ensemble. Furthermore, the running time for short
single-solution formulas increases with the problem size much faster than for
random 3-SAT formulas from the phase transition region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504102</identifier>
 <datestamp>2007-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504102</id><created>2005-04-25</created><authors><author><keyname>Danielsen</keyname><forenames>Lars Eirik</forenames><affiliation>University of Bergen</affiliation></author><author><keyname>Parker</keyname><forenames>Matthew G.</forenames><affiliation>University of Bergen</affiliation></author></authors><title>Spectral Orbits and Peak-to-Average Power Ratio of Boolean Functions
  with respect to the {I,H,N}^n Transform</title><categories>cs.IT math.IT</categories><comments>Presented at Sequences and Their Applications, SETA'04, Seoul, South
  Korea, October 2004. 17 pages, 10 figures</comments><journal-ref>In Sequences and Their Applications -- SETA 2004, edited by T.
  Helleseth, D. Sarwate, H.-Y. Song, and K. Yang, Lecture Notes in Comput.
  Sci., volume 3486, pp. 373--388, Springer-Verlag, Berlin, May 2005.</journal-ref><doi>10.1007/11423461_28</doi><abstract>  We enumerate the inequivalent self-dual additive codes over GF(4) of
blocklength n, thereby extending the sequence A090899 in The On-Line
Encyclopedia of Integer Sequences from n = 9 to n = 12. These codes have a
well-known interpretation as quantum codes. They can also be represented by
graphs, where a simple graph operation generates the orbits of equivalent
codes. We highlight the regularity and structure of some graphs that correspond
to codes with high distance. The codes can also be interpreted as quadratic
Boolean functions, where inequivalence takes on a spectral meaning. In this
context we define PAR_IHN, peak-to-average power ratio with respect to the
{I,H,N}^n transform set. We prove that PAR_IHN of a Boolean function is
equivalent to the the size of the maximum independent set over the associated
orbit of graphs. Finally we propose a construction technique to generate
Boolean functions with low PAR_IHN and algebraic degree higher than 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504103</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504103</id><created>2005-04-26</created><updated>2006-01-24</updated><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Kenyon</keyname><forenames>Claire</forenames></author><author><keyname>Noga</keyname><forenames>John</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Oblivious Medians via Online Bidding</title><categories>cs.DS</categories><comments>extended abstract, conference: LATIN 2006</comments><acm-class>G.1.6; G.2.2; F.2.2</acm-class><journal-ref>Algorithmica 50(4):455-478(2008)</journal-ref><doi>10.1007/s00453-007-9005-x</doi><abstract>  Following Mettu and Plaxton, we study online algorithms for the k-medians
problem. Such an algorithm must produce a nested sequence F_1\subseteq
F_2\subseteq...\subseteq F_n of sets of facilities. Mettu and Plaxton show that
online metric medians has a (roughly) 40-competitive deterministic
polynomial-time algorithm. We give improved algorithms, including a
(24+\epsilon)-competitive deterministic polynomial-time algorithm and a
5.44-competitive, randomized, non-polynomial-time algorithm.
  We also consider the competitive ratio with respect to size. An algorithm is
s-size-competitive if, for each k, the cost of F_k is at most the minimum cost
of any set of k facilities, while the size of F_k is at most s k. We present
optimally competitive algorithms for this problem.
  Our proofs reduce online medians to the following online bidding problem:
faced with some unknown threshold T&gt;0, an algorithm must submit ``bids'' b&gt;0
until it submits a bid as large as T. The algorithm pays the sum of its bids.
We describe optimally competitive algorithms for online bidding.
  Our results on cost-competitive online medians extend to approximately metric
distance functions, online fractional medians, and online bicriteria
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504104</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504104</id><created>2005-04-27</created><updated>2005-09-27</updated><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Kenyon</keyname><forenames>Claire</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>The reverse greedy algorithm for the metric k-median problem</title><categories>cs.DS</categories><comments>to appear in IPL. preliminary version in COCOON '05</comments><acm-class>G.1.6; G.2.2; F.2.2</acm-class><journal-ref>Information Processing Letters 97:68-72(2006)</journal-ref><doi>10.1016/j.ipl.2005.09.009</doi><abstract>  The Reverse Greedy algorithm (RGreedy) for the k-median problem works as
follows. It starts by placing facilities on all nodes. At each step, it removes
a facility to minimize the resulting total distance from the customers to the
remaining facilities. It stops when k facilities remain. We prove that, if the
distance function is metric, then the approximation ratio of RGreedy is between
?(log n/ log log n) and O(log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504105</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504105</id><created>2005-04-27</created><authors><author><keyname>Worley</keyname><forenames>G Gordon</forenames><suffix>III</suffix></author></authors><title>Wikis in Tuple Spaces</title><categories>cs.DC cs.MM</categories><comments>To appear at WMSCI 2005</comments><abstract>  We consider storing the pages of a wiki in a tuple space and the effects this
might have on the wiki experience. In particular, wiki pages are stored in
tuples with a few identifying values such as title, author, revision date,
content, etc. and pages are retrieved by sending the tuple space templates,
such as one that gives the title but nothing else, leaving the tuple space to
resolve to a single tuple. We use a tuple space wiki to avoid deadlocks,
infinite loops, and wasted efforts when page edit contention arises and examine
how a tuple space wiki changes the wiki experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504106</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504106</id><created>2005-04-28</created><authors><author><keyname>Cycon</keyname><forenames>Hans L.</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Waehlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Palkow</keyname><forenames>Mark</forenames></author><author><keyname>Regensburg</keyname><forenames>Henrik</forenames></author></authors><title>A Distributed Multimedia Communication System and its Applications to
  E-Learning</title><categories>cs.MM cs.NI</categories><comments>Including 6 figures</comments><acm-class>C.2.2; C.2.4; H.4.3</acm-class><journal-ref>IEEE International Symposium on Consumer Electronics, Sept. 1-3,
  2004, Page(s):425 - 429</journal-ref><abstract>  In this paper we report on a multimedia communication system including a
VCoIP (Video Conferencing over IP) software with a distributed architecture and
its applications for teaching scenarios. It is a simple, ready-to-use scheme
for distributed presenting, recording and streaming multimedia content. We also
introduce and investigate concepts and experiments to IPv6 user and session
mobility, with the special focus on real-time video group communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504107</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504107</id><created>2005-04-28</created><updated>2005-10-12</updated><authors><author><keyname>Alvarez-Hamelin</keyname><forenames>Jos&#xe9; Ignacio</forenames><affiliation>LPTO</affiliation></author><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames><affiliation>LPTO</affiliation></author><author><keyname>Barrat</keyname><forenames>Alain</forenames><affiliation>LPTO</affiliation></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames><affiliation>LPTO</affiliation></author></authors><title>k-core decomposition: a tool for the visualization of large scale
  networks</title><categories>cs.NI cs.GR</categories><proxy>ccsd</proxy><journal-ref>Advances in Neural Information Processing Systems 18, Canada
  (2006) 41</journal-ref><abstract>  We use the k-core decomposition to visualize large scale complex networks in
two dimensions. This decomposition, based on a recursive pruning of the least
connected vertices, allows to disentangle the hierarchical structure of
networks by progressively focusing on their central cores. By using this
strategy we develop a general visualization algorithm that can be used to
compare the structural properties of various networks and highlight their
hierarchical structure. The low computational complexity of the algorithm,
O(n+e), where 'n' is the size of the network, and 'e' is the number of edges,
makes it suitable for the visualization of very large sparse networks. We apply
the proposed visualization tool to several real and synthetic graphs, showing
its utility in finding specific structural fingerprints of computer generated
and real world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504108</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504108</id><created>2005-04-29</created><authors><author><keyname>Messie</keyname><forenames>Derek</forenames><affiliation>Syracuse University</affiliation></author><author><keyname>Oh</keyname><forenames>Jae C.</forenames><affiliation>Syracuse University</affiliation></author></authors><title>Cooperative Game Theory within Multi-Agent Systems for Systems
  Scheduling</title><categories>cs.AI cs.MA</categories><comments>Fourth International Conference on Hybrid Intelligent Systems (HIS),
  Kitakyushu, Japan, December, 2004</comments><abstract>  Research concerning organization and coordination within multi-agent systems
continues to draw from a variety of architectures and methodologies. The work
presented in this paper combines techniques from game theory and multi-agent
systems to produce self-organizing, polymorphic, lightweight, embedded agents
for systems scheduling within a large-scale real-time systems environment.
Results show how this approach is used to experimentally produce optimum
real-time scheduling through the emergent behavior of thousands of agents.
These results are obtained using a SWARM simulation of systems scheduling
within a High Energy Physics experiment consisting of 2500 digital signal
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504109</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504109</id><created>2005-04-29</created><authors><author><keyname>Messie</keyname><forenames>Derek</forenames><affiliation>Syracuse University</affiliation></author><author><keyname>Jung</keyname><forenames>Mina</forenames><affiliation>Syracuse University</affiliation></author><author><keyname>Oh</keyname><forenames>Jae C.</forenames><affiliation>Syracuse University</affiliation></author><author><keyname>Shetty</keyname><forenames>Shweta</forenames><affiliation>Vanderbilt University</affiliation></author><author><keyname>Nordstrom</keyname><forenames>Steven</forenames><affiliation>Vanderbilt University</affiliation></author><author><keyname>Haney</keyname><forenames>Michael</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author></authors><title>Prototype of Fault Adaptive Embedded Software for Large-Scale Real-Time
  Systems</title><categories>cs.SE</categories><comments>2nd Workshop on Engineering of Autonomic Systems (EASe), in the 12th
  Annual IEEE International Conference and Workshop on the Engineering of
  Computer Based Systems (ECBS), Washington, DC, April, 2005</comments><abstract>  This paper describes a comprehensive prototype of large-scale fault adaptive
embedded software developed for the proposed Fermilab BTeV high energy physics
experiment. Lightweight self-optimizing agents embedded within Level 1 of the
prototype are responsible for proactive and reactive monitoring and mitigation
based on specified layers of competence. The agents are self-protecting,
detecting cascading failures using a distributed approach. Adaptive,
reconfigurable, and mobile objects for reliablility are designed to be
self-configuring to adapt automatically to dynamically changing environments.
These objects provide a self-healing layer with the ability to discover,
diagnose, and react to discontinuities in real-time processing. A generic
modeling environment was developed to facilitate design and implementation of
hardware resource specifications, application data flow, and failure mitigation
strategies. Level 1 of the planned BTeV trigger system alone will consist of
2500 DSPs, so the number of components and intractable fault scenarios involved
make it impossible to design an `expert system' that applies traditional
centralized mitigative strategies based on rules capturing every possible
system state. Instead, a distributed reactive approach is implemented using the
tools and methodologies developed by the Real-Time Embedded Systems group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504110</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504110</id><created>2005-04-29</created><updated>2006-02-15</updated><authors><author><keyname>Ioannou</keyname><forenames>Lawrence M.</forenames></author></authors><title>Computing finite-dimensional bipartite quantum separability</title><categories>cs.DS quant-ph</categories><comments>Replaced orginal archive submission with PhD thesis, which subsumes
  and mildly corrects it</comments><abstract>  Ever since entanglement was identified as a computational and cryptographic
resource, effort has been made to find an efficient way to tell whether a given
density matrix represents an unentangled, or separable, state. Essentially,
this is the quantum separability problem.
  Chapters 1 to 3 motivate a new interior-point algorithm which, given the
expected values of a subset of an orthogonal basis of observables of an
otherwise unknown quantum state, searches for an entanglement witness in the
span of the subset of observables. When all the expected values are known, the
algorithm solves the separability problem. In Chapter 4, I give the motivation
for the algorithm and show how it can be used in a particular physical scenario
to detect entanglement (or decide separability) of an unknown quantum state
using as few quantum resources as possible. I then explain the intuitive idea
behind the algorithm and relate it to the standard algorithms of its kind. I
end the chapter with a comparison of the complexities of the algorithms
surveyed in Chapter 3. Finally, in Chapter 5, I present the details of the
algorithm and discuss its performance relative to standard methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0504111</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0504111</id><created>2005-04-29</created><authors><author><keyname>Seada</keyname><forenames>Karim</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Efficient and Robust Geocasting Protocols for Sensor Networks</title><categories>cs.NI</categories><abstract>  Geocasting is the delivery of packets to nodes within a certain geographic
area. For many applications in wireless ad hoc and sensor networks, geocasting
is an important and frequent communication service. The challenging problem in
geocasting is distributing the packets to all the nodes within the geocast
region with high probability but with low overhead. According to our study we
notice a clear tradeoff between the proportion of nodes in the geocast region
that receive the packet and the overhead incurred by the geocast packet
especially at low densities and irregular distributions. We present two novel
protocols for geocasting that achieve high delivery rate and low overhead by
utilizing the local location information of nodes to combine geographic routing
mechanisms with region flooding. We show that the first protocol
Geographic-Forwarding-Geocast (GFG) has close-to-minimum overhead in dense
networks and that the second protocol Geographic-Forwarding-Perimeter-Geocast
(GFPG) provides guaranteed delivery without global flooding or global network
information even at low densities and with the existence of region gaps or
obstacles. An adaptive version of the second protocol (GFPG*) has the desirable
property of perfect delivery at all densities and close-to-minimum overhead at
high densities. We evaluate our mechanisms and compare them using simulation to
other proposed geocasting mechanisms. The results show the significant
improvement in delivery rate (up to 63% higher delivery percentage in low
density networks) and reduction in overhead (up to 80% reduction) achieved by
our mechanisms. We hope for our protocols to become building block mechanisms
for dependable sensor network architectures that require robust efficient
geocast services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0505001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0505001</id><created>2005-04-29</created><authors><author><keyname>da Silva</keyname><forenames>Roberto</forenames></author><author><keyname>Baraviera</keyname><forenames>Alexandre Tavares</forenames></author><author><keyname>Dahmen</keyname><forenames>Silvio R.</forenames></author></authors><title>Modelling investment in artificial stock markets: Analytical and
  Numerical Results</title><categories>cs.CE</categories><comments>19 pages 10 figures</comments><abstract>  In this article we study the behavior of a group of economic agents in the
context of cooperative game theory, interacting according to rules based on the
Potts Model with suitable modifications. Each agent can be thought of as
belonging to a chain, where agents can only interact with their nearest
neighbors (periodic boundary conditions are imposed). Each agent can invest an
amount &amp;#963;_{i}=0,...,q-1. Using the transfer matrix method we study
analytically, among other things, the behavior of the investment as a function
of a control parameter (denoted &amp;#946;) for the cases q=2 and 3. For q&gt;3
numerical evaluation of eigenvalues and high precision numerical derivatives
are used in order to assess this information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0505002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0505002</id><created>2005-04-29</created><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>Tight Lower Bounds for Query Processing on Streaming and External Memory
  Data</title><categories>cs.DB cs.CC</categories><comments>25 pages, 4 figures, to appear in Proc. ICALP 2005; extended version
  with appendix</comments><acm-class>H.2.3; I.7.2</acm-class><abstract>  We study a clean machine model for external memory and stream processing. We
show that the number of scans of the external data induces a strict hierarchy
(as long as work space is sufficiently small, e.g., polylogarithmic in the size
of the input). We also show that neither joins nor sorting are feasible if the
product of the number $r(n)$ of scans of the external memory and the size
$s(n)$ of the internal memory buffers is sufficiently small, e.g., of size
$o(\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath
evaluation and filtering.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="97000" completeListSize="102538">1122234|98001</resumptionToken>
</ListRecords>
</OAI-PMH>
