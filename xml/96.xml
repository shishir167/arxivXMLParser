<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:05:26Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|95001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103011</id><created>2001-03-12</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>A Machine-Learning Approach to Estimating the Referential Properties of
  Japanese Noun Phrases</title><categories>cs.CL</categories><comments>9 pages. Computation and Language. This paper is included in the book
  entitled by &quot;Computational Linguistics and Intelligent Text Processing,
  Second International Conference, CICLing 2001, Mexico City, February 2001
  Proceedings&quot;, Alexander Gelbukh (Ed.), Springer Publisher, ISSN 0302-9743,
  ISBN 3-540-41687-0</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>CICLing'2001, Mexico City, February 2001</journal-ref><abstract>  The referential properties of noun phrases in the Japanese language, which
has no articles, are useful for article generation in Japanese-English machine
translation and for anaphora resolution in Japanese noun phrases. They are
generally classified as generic noun phrases, definite noun phrases, and
indefinite noun phrases. In the previous work, referential properties were
estimated by developing rules that used clue words. If two or more rules were
in conflict with each other, the category having the maximum total score given
by the rules was selected as the desired category. The score given by each rule
was established by hand, so the manpower cost was high. In this work, we
automatically adjusted these scores by using a machine-learning method and
succeeded in reducing the amount of manpower needed to adjust these scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103012</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103012</id><created>2001-03-12</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Kanzaki</keyname><forenames>Kyoko</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Meaning Sort - Three examples: dictionary construction, tagged corpus
  construction, and information presentation system</title><categories>cs.CL</categories><comments>14 pages. Computation and Language. This paper is included in the
  book entitled by &quot;Computational Linguistics and Intelligent Text Processing,
  Second International Conference&quot;, Springer Publisher</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>CICLing'2001, Mexico City, February 2001</journal-ref><abstract>  It is often useful to sort words into an order that reflects relations among
their meanings as obtained by using a thesaurus. In this paper, we introduce a
method of arranging words semantically by using several types of `{\sf is-a}'
thesauri and a multi-dimensional thesaurus. We also describe three major
applications where a meaning sort is useful and show the effectiveness of a
meaning sort. Since there is no doubt that a word list in meaning-order is
easier to use than a word list in some random order, a meaning sort, which can
easily produce a word list in meaning-order, must be useful and effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103013</id><created>2001-03-12</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Utiyama</keyname><forenames>Masao</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Ozaku</keyname><forenames>Hiromi</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>CRL at Ntcir2</title><categories>cs.CL</categories><comments>11 pages. Computation and Language. This paper describes our results
  of information retrieval in the NTCIR2 contest</comments><acm-class>H.3.3; I.2.7</acm-class><abstract>  We have developed systems of two types for NTCIR2. One is an enhenced version
of the system we developed for NTCIR1 and IREX. It submitted retrieval results
for JJ and CC tasks. A variety of parameters were tried with the system. It
used such characteristics of newspapers as locational information in the CC
tasks. The system got good results for both of the tasks. The other system is a
portable system which avoids free parameters as much as possible. The system
submitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system
automatically determined the number of top documents and the weight of the
original query used in automatic-feedback retrieval. It also determined
relevant terms quite robustly. For EJ and JE tasks, it used document expansion
to augment the initial queries. It achieved good results, except on the CC
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103014</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103014</id><created>2001-03-12</created><authors><author><keyname>Chiao</keyname><forenames>Raymond Y.</forenames></author><author><keyname>Hickmann</keyname><forenames>Jandir M.</forenames></author><author><keyname>Solli</keyname><forenames>Daniel</forenames></author></authors><title>Faster-than-light effects and negative group delays in optics and
  electronics, and their applications</title><categories>cs.PF</categories><comments>13 pages, 5 figures, 2001 Photonic West Plenary Talk</comments><acm-class>B.7.0</acm-class><doi>10.1117/12.432562</doi><abstract>  Recent manifestations of apparently faster-than-light effects confirmed our
predictions that the group velocity in transparent optical media can exceed c.
Special relativity is not violated by these phenomena. Moreover, in the
electronic domain, the causality principle does not forbid negative group
delays of analytic signals in electronic circuits, in which the peak of an
output pulse leaves the exit port of a circuit before the peak of the input
pulse enters the input port. Furthermore, pulse distortion for these
superluminal analytic signals can be negligible in both the optical and
electronic domains. Here we suggest an extension of these ideas to the
microelectronic domain. The underlying principle is that negative feedback can
be used to produce negative group delays. Such negative group delays can be
used to cancel out the positive group delays due to transistor latency (e.g.,
the finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),
as well as the propagation delays due to the interconnects between transistors.
Using this principle, it is possible to speed up computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103015</id><created>2001-03-14</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Fitness Uniform Selection to Preserve Genetic Diversity</title><categories>cs.AI cs.DC cs.LG q-bio</categories><comments>13 LaTeX pages, 1 eps figure</comments><report-no>IDSIA-01-01</report-no><acm-class>I.2; I.2.6; I.2.8; F.2</acm-class><journal-ref>Proceedings of the 2002 Congress on Evolutionary Computation
  (CEC-2002) 783-788</journal-ref><abstract>  In evolutionary algorithms, the fitness of a population increases with time
by mutating and recombining individuals and by a biased selection of more fit
individuals. The right selection pressure is critical in ensuring sufficient
optimization progress on the one hand and in preserving genetic diversity to be
able to escape from local optima on the other. We propose a new selection
scheme, which is uniform in the fitness values. It generates selection pressure
towards sparsely populated fitness regions, not necessarily towards higher
fitness, as is the case for all other selection schemes. We show that the new
selection scheme can be much more effective than standard selection schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103016</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103016</id><created>2001-03-20</created><authors><author><keyname>Adamic</keyname><forenames>L. A.</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Lukose</keyname><forenames>R. M.</forenames><affiliation>HP Sand Hill Labs, Palo Alto, CA</affiliation></author><author><keyname>Puniyani</keyname><forenames>A. R.</forenames><affiliation>Stanford University</affiliation></author><author><keyname>Huberman</keyname><forenames>B. A.</forenames><affiliation>HP Sand Hill Labs, Palo Alto, CA</affiliation></author></authors><title>Search in Power-Law Networks</title><categories>cs.NI cond-mat.dis-nn cond-mat.stat-mech cs.PF</categories><comments>17 pages, 14 figures</comments><acm-class>C.2.1</acm-class><journal-ref>Phys. Rev. E 64, 046135 (2001)</journal-ref><doi>10.1103/PhysRevE.64.046135</doi><abstract>  Many communication and social networks have power-law link distributions,
containing a few nodes which have a very high degree and many with low degree.
The high connectivity nodes play the important role of hubs in communication
and networking, a fact which can be exploited when designing efficient search
algorithms. We introduce a number of local search strategies which utilize high
degree nodes in power-law graphs and which have costs which scale sub-linearly
with the size of the graph. We also demonstrate the utility of these strategies
on the Gnutella peer-to-peer network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103017</id><created>2001-03-22</created><authors><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author></authors><title>Nice point sets can have nasty Delaunay triangulations</title><categories>cs.CG</categories><comments>11 pages, 8 figures, to appear in Proc. SCG '01</comments><acm-class>F.2.2;G.2.m</acm-class><abstract>  We consider the complexity of Delaunay triangulations of sets of points in
R^3 under certain practical geometric constraints. The spread of a set of
points is the ratio between the longest and shortest pairwise distances. We
show that in the worst case, the Delaunay triangulation of n points in R^3 with
spread D has complexity Omega(min{D^3, nD, n^2}) and O(min{D^4, n^2}). For the
case D = Theta(sqrt{n}), our lower bound construction consists of a uniform
sample of a smooth convex surface with bounded curvature. We also construct a
family of smooth connected surfaces such that the Delaunay triangulation of any
good point sample has near-quadratic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103018</id><created>2001-03-26</created><authors><author><keyname>Diekert</keyname><forenames>Volker</forenames></author><author><keyname>Gutierrez</keyname><forenames>Claudio</forenames></author><author><keyname>Hagenah</keyname><forenames>Christian</forenames></author></authors><title>The Existential Theory of Equations with Rational Constraints in Free
  Groups is PSPACE-Complete</title><categories>cs.DS cs.LO</categories><comments>45 pages. LaTeX source</comments><acm-class>F.2.2; F.4</acm-class><abstract>  It is known that the existential theory of equations in free groups is
decidable. This is a famous result of Makanin. On the other hand it has been
shown that the scheme of his algorithm is not primitive recursive. In this
paper we present an algorithm that works in polynomial space, even in the more
general setting where each variable has a rational constraint, that is, the
solution has to respect a specification given by a regular word language. Our
main result states that the existential theory of equations in free groups with
rational constraints is PSPACE-complete. We obtain this result as a corollary
of the corresponding statement about free monoids with involution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103019</id><created>2001-03-27</created><authors><author><keyname>Chu</keyname><forenames>Francis</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>On the NP-completeness of Finding an Optimal Strategy in Games with
  Common Payoffs</title><categories>cs.GT cs.CC cs.DC</categories><comments>To appear, International Journal of Game Theory</comments><acm-class>D.1.3</acm-class><abstract>  Consider a very simple class of (finite) games: after an initial move by
nature, each player makes one move. Moreover, the players have common
interests: at each node, all the players get the same payoff. We show that the
problem of determining whether there exists a joint strategy where each player
has an expected payoff of at least r is NP-complete as a function of the number
of nodes in the extensive-form representation of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103020</id><created>2001-03-27</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Belief Revision: A Critique</title><categories>cs.AI cs.LO</categories><comments>An early version of the paper appeared in KR '96</comments><acm-class>I.2.4, F.4.1</acm-class><journal-ref>Journal of Logic, Language, and Information, vol. 8, 1999, pp.
  401-420</journal-ref><abstract>  We examine carefully the rationale underlying the approaches to belief change
taken in the literature, and highlight what we view as methodological problems.
We argue that to study belief change carefully, we must be quite explicit about
the ``ontology'' or scenario underlying the belief change process. This is
something that has been missing in previous work, with its focus on postulates.
Our analysis shows that we must pay particular attention to two issues that
have often been taken for granted: The first is how we model the agent's
epistemic state. (Do we use a set of beliefs, or a richer structure, such as an
ordering on worlds? And if we use a set of beliefs, in what language are these
beliefs are expressed?) We show that even postulates that have been called
``beyond controversy'' are unreasonable when the agent's beliefs include
beliefs about her own epistemic state as well as the external world. The second
is the status of observations. (Are observations known to be true, or just
believed? In the latter case, how firm is the belief?) Issues regarding the
status of observations arise particularly when we consider iterated belief
revision, and we must confront the possibility of revising by p and then by
not-p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103021</id><created>2001-03-27</created><updated>2001-04-03</updated><authors><author><keyname>Harrelson</keyname><forenames>Chris</forenames><affiliation>UC Berkeley</affiliation></author><author><keyname>Kerenidis</keyname><forenames>Iordanis</forenames><affiliation>UC Berkeley</affiliation></author></authors><title>Quantum Clock Synchronization with one qubit</title><categories>cs.CC quant-ph</categories><comments>LaTeX, 5 pages</comments><acm-class>F.2</acm-class><abstract>  The clock synchronization problem is to determine the time difference T
between two spatially separated parties. We improve on I. Chuang's quantum
clock synchronization algorithm and show that it is possible to obtain T to n
bits of accuracy while communicating only one qubit in one direction and using
an O(2^n) frequency range. We also prove a quantum lower bound of \Omega(2^n)
for the product of the transmitted qubits and the range of frequencies, thus
showing that our algorithm is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103022</id><created>2001-03-28</created><authors><author><keyname>Allcock</keyname><forenames>Bill</forenames></author><author><keyname>Bester</keyname><forenames>Joe</forenames></author><author><keyname>Bresnahan</keyname><forenames>John</forenames></author><author><keyname>Chervenak</keyname><forenames>Ann L.</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author><author><keyname>Kesselman</keyname><forenames>Carl</forenames></author><author><keyname>Meder</keyname><forenames>Sam</forenames></author><author><keyname>Nefedova</keyname><forenames>Veronika</forenames></author><author><keyname>Quesnel</keyname><forenames>Darcy</forenames></author><author><keyname>Tuecke</keyname><forenames>Steven</forenames></author></authors><title>Secure, Efficient Data Transport and Replica Management for
  High-Performance Data-Intensive Computing</title><categories>cs.DC cs.DB</categories><comments>15 pages</comments><report-no>ANL/MCS-P871-0201</report-no><acm-class>C.1.4; E.1</acm-class><abstract>  An emerging class of data-intensive applications involve the geographically
dispersed extraction of complex scientific information from very large
collections of measured or computed data. Such applications arise, for example,
in experimental physics, where the data in question is generated by
accelerators, and in simulation science, where the data is generated by
supercomputers. So-called Data Grids provide essential infrastructure for such
applications, much as the Internet provides essential services for applications
such as e-mail and the Web. We describe here two services that we believe are
fundamental to any Data Grid: reliable, high-speed transporet and replica
management. Our high-speed transport service, GridFTP, extends the popular FTP
protocol with new features required for Data Grid applciations, such as
striping and partial file access. Our replica management service integrates a
replica catalog with GridFTP transfers to provide for the creation,
registration, location, and management of dataset replicas. We present the
design of both services and also preliminary performance results. Our
implementations exploit security and other services provided by the Globus
Toolkit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103023</id><created>2001-03-28</created><authors><author><keyname>Sepesi</keyname><forenames>Greg</forenames></author></authors><title>A Dualheap Selection Algorithm - A Call for Analysis</title><categories>cs.DS cs.DC</categories><comments>6 pages, 13 figures</comments><acm-class>F.2.2; E.1</acm-class><abstract>  An algorithm is presented that efficiently solves the selection problem:
finding the k-th smallest member of a set. Relevant to a divide-and-conquer
strategy, the algorithm also partitions a set into small and large valued
subsets. Applied recursively, this partitioning results in a sorted set. The
algorithm's applicability is therefore much broader than just the selection
problem. The presented algorithm is based upon R.W. Floyd's 1964 algorithm that
constructs a heap from the bottom-up. Empirically, the presented algorithm's
performance appears competitive with the popular quickselect algorithm, a
variant of C.A.R. Hoare's 1962 quicksort algorithm. Furthermore, constructing a
heap from the bottom-up is an inherently parallel process (processors can work
independently and simultaneously on subheap construction), suggesting a
performance advantage with parallel implementations. Given the presented
algorithm's broad applicability, simplicity, serial performance, and parallel
nature, further study is warranted. Specifically, worst-case analysis is an
important but still unsolved problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103024</id><created>2001-03-28</created><authors><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author></authors><title>Notes on computing peaks in k-levels and parametric spanning trees</title><categories>cs.CG cs.DS</categories><comments>ACM SCG'01</comments><acm-class>F2.2</acm-class><abstract>  We give an algorithm to compute all the local peaks in the $k$-level of an
arrangement of $n$ lines in $O(n \log n) + \tilde{O}((kn)^{2/3})$ time. We can
also find $\tau$ largest peaks in $O(n \log ^2 n) + \tilde{O}((\tau n)^{2/3})$
time. Moreover, we consider the longest edge in a parametric minimum spanning
tree (in other words, a bottleneck edge for connectivity), and give an
algorithm to compute the parameter value (within a given interval)
maximizing/minimizing the length of the longest edge in MST. The time
complexity is $\tilde{O}(n^{8/7}k^{1/7} + n k^{1/3})$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103025</id><created>2001-03-29</created><authors><author><keyname>Foster</keyname><forenames>Ian</forenames></author><author><keyname>Kesselman</keyname><forenames>Carl</forenames></author><author><keyname>Tuecke</keyname><forenames>Steven</forenames></author></authors><title>The Anatomy of the Grid - Enabling Scalable Virtual Organizations</title><categories>cs.AR cs.DC</categories><comments>24 pages, 5 figures</comments><report-no>ANL/MCS-P870-0201</report-no><acm-class>C.1.4; C.2.4</acm-class><abstract>  &quot;Grid&quot; computing has emerged as an important new field, distinguished from
conventional distributed computing by its focus on large-scale resource
sharing, innovative applications, and, in some cases, high-performance
orientation. In this article, we define this new field. First, we review the
&quot;Grid problem,&quot; which we define as flexible, secure, coordinated resource
sharing among dynamic collections of individuals, institutions, and
resources-what we refer to as virtual organizations. In such settings, we
encounter unique authentication, authorization, resource access, resource
discovery, and other challenges. It is this class of problem that is addressed
by Grid technologies. Next, we present an extensible and open Grid
architecture, in which protocols, services, application programming interfaces,
and software development kits are categorized according to their roles in
enabling resource sharing. We describe requirements that we believe any such
mechanisms must satisfy, and we discuss the central role played by the
intergrid protocols that enable interoperability among different Grid systems.
Finally, we discuss how Grid technologies relate to other contemporary
technologies, including enterprise integration, application service provider,
storage service provider, and peer-to-peer computing. We maintain that Grid
concepts and technologies complement and have much to contribute to these other
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0103026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0103026</id><created>2001-03-29</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames></author></authors><title>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</title><categories>cs.CL</categories><comments>Proceedings of the Second Meeting of the North American Chapter of
  the Association for Computational Linguistics (NAACL-01), June 2-7, 2001,
  Pittsburgh, PA; 8 pages</comments><acm-class>I.2.7</acm-class><abstract>  This paper presents a corpus-based approach to word sense disambiguation
where a decision tree assigns a sense to an ambiguous word based on the bigrams
that occur nearby. This approach is evaluated using the sense-tagged corpora
from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate
than the average results reported for 30 of 36 words, and is more accurate than
the best results for 19 of 36 words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104001</id><created>2001-03-31</created><authors><author><keyname>Demetrescu</keyname><forenames>Camil</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author></authors><title>Mantaining Dynamic Matrices for Fully Dynamic Transitive Closure</title><categories>cs.DS cs.DM</categories><comments>52 pages, 5 figures</comments><report-no>TR-DIS-03-01</report-no><acm-class>F.2.2; G.2.2</acm-class><abstract>  In this paper we introduce a general framework for casting fully dynamic
transitive closure into the problem of reevaluating polynomials over matrices.
With this technique, we improve the best known bounds for fully dynamic
transitive closure. In particular, we devise a deterministic algorithm for
general directed graphs that achieves $O(n^2)$ amortized time for updates,
while preserving unit worst-case cost for queries. In case of deletions only,
our algorithm performs updates faster in O(n) amortized time.
  Our matrix-based approach yields an algorithm for directed acyclic graphs
that breaks through the $O(n^2)$ barrier on the single-operation complexity of
fully dynamic transitive closure. We can answer queries in $O(n^\epsilon)$ time
and perform updates in $O(n^{\omega(1,\epsilon,1)-\epsilon}+n^{1+\epsilon})$
time, for any $\epsilon\in[0,1]$, where $\omega(1,\epsilon,1)$ is the exponent
of the multiplication of an $n\times n^{\epsilon}$ matrix by an
$n^{\epsilon}\times n$ matrix. The current best bounds on
$\omega(1,\epsilon,1)$ imply an $O(n^{0.58})$ query time and an $O(n^{1.58})$
update time. Our subquadratic algorithm is randomized, and has one-side error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104002</id><created>2001-04-02</created><authors><author><keyname>Vazhkudai</keyname><forenames>Sudharshan</forenames></author><author><keyname>Tuecke</keyname><forenames>Steven</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>Replica Selection in the Globus Data Grid</title><categories>cs.DC</categories><comments>8 pages, 6 figures</comments><report-no>ANL/MCS-P869-0201</report-no><acm-class>C.1.4</acm-class><abstract>  The Globus Data Grid architecture provides a scalable infrastructure for the
management of storage resources and data that are distributed across Grid
environments. These services are designed to support a variety of scientific
applications, ranging from high-energy physics to computational genomics, that
require access to large amounts of data (terabytes or even petabytes) with
varied quality of service requirements. By layering on a set of core services,
such as data transport, security, and replica cataloging, one can construct
various higher-level services. In this paper, we discuss the design and
implementation of a high-level replica selection service that uses information
regarding replica location and user preferences to guide selection from among
storage replica alternatives. We first present a basic replica selection
service design, then show how dynamic information collected using Globus
information service capabilities concerning storage system properties can help
improve and optimize the selection process. We demonstrate the use of Condor's
ClassAds resource description and matchmaking mechanism as an efficient tool
for representing and matching storage resource capabilities and policies
against application requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104003</id><created>2001-04-02</created><authors><author><keyname>Rosenblueth</keyname><forenames>David A.</forenames></author></authors><title>Chain Programs for Writing Deterministic Metainterpreters</title><categories>cs.LO cs.PL</categories><comments>30 pages. To appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>D.1.6; F.3.2; D.3.2; D.3.4</acm-class><abstract>  Many metainterpreters found in the logic programming literature are
nondeterministic in the sense that the selection of program clauses is not
determined. Examples are the familiar &quot;demo&quot; and &quot;vanilla&quot; metainterpreters.
For some applications this nondeterminism is convenient. In some cases,
however, a deterministic metainterpreter, having an explicit selection of
clauses, is needed. Such cases include (1) conversion of OR parallelism into
AND parallelism for &quot;committed-choice&quot; processors, (2) logic-based,
imperative-language implementation of search strategies, and (3) simulation of
bounded-resource reasoning.
  Deterministic metainterpreters are difficult to write because the programmer
must be concerned about the set of unifiers of the children of a node in the
derivation tree. We argue that it is both possible and advantageous to write
these metainterpreters by reasoning in terms of object programs converted into
a syntactically restricted form that we call &quot;chain&quot; form, where we can forget
about unification, except for unit clauses. We give two transformations
converting logic programs into chain form, one for &quot;moded&quot; programs (implicit
in two existing exhaustive-traversal methods for committed-choice execution),
and one for arbitrary definite programs. As illustrations of our approach we
show examples of the three applications mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104004</id><created>2001-04-02</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>Secure Counting: counting members of a subset without revealing their
  identities</title><categories>cs.CR</categories><acm-class>E.3</acm-class><abstract>  Suppose there is a group of N people some of whom possess a specific
property. For example, their wealth is above or below a threshold, they voted
for a particular candidate, they have a certain disease, etc. The group wants
to find out how many of its members posses the property -- without revealing
the identities. Unless of course it turns out that all members do or do not
have the attribute of interest. However, in all other cases the counting
algorithm should guarantee that nobody can find out if a particular individual
possesses the property unless all the other N-1 members of the group collude.
  The present article describes a method to solve the confidential counting
problem with only 3*N-2 pairwise communications, or 2*N broadcasts (the last
N-1 pairwise communications are merely to announce the result). The counting
algorithm does not require any trusted third parties. All communications
between parties involved can be conducted in public without compromising the
security of counting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104005</id><created>2001-04-03</created><authors><author><keyname>van Zaanen</keyname><forenames>Menno</forenames></author></authors><title>Bootstrapping Structure using Similarity</title><categories>cs.LG cs.CL</categories><comments>11 pages</comments><acm-class>I.2, I.2.6, I.2.7</acm-class><journal-ref>Computational Linguistics in the Netherlands 1999 - Selected
  Papers from the Tenth CLIN Meeting, pages 235-245</journal-ref><abstract>  In this paper a new similarity-based learning algorithm, inspired by string
edit-distance (Wagner and Fischer, 1974), is applied to the problem of
bootstrapping structure from scratch. The algorithm takes a corpus of
unannotated sentences as input and returns a corpus of bracketed sentences. The
method works on pairs of unstructured sentences or sentences partially
bracketed by the algorithm that have one or more words in common. It finds
parts of sentences that are interchangeable (i.e. the parts of the sentences
that are different in both sentences). These parts are taken as possible
constituents of the same type. While this corresponds to the basic
bootstrapping step of the algorithm, further structure may be learned from
comparison with other (similar) sentences.
  We used this method for bootstrapping structure from the flat sentences of
the Penn Treebank ATIS corpus, and compared the resulting structured sentences
to the structured sentences in the ATIS corpus. Similarly, the algorithm was
tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision
on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS
corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104006</id><created>2001-04-03</created><authors><author><keyname>van Zaanen</keyname><forenames>Menno</forenames></author></authors><title>ABL: Alignment-Based Learning</title><categories>cs.LG cs.CL</categories><comments>7 pages</comments><acm-class>I.2; I.2.6; I.2.7</acm-class><journal-ref>Proceedings of the 18th International Conference on Computational
  Linguistics (COLING); Saarbrucken, Germany. pages 961-967</journal-ref><abstract>  This paper introduces a new type of grammar learning algorithm, inspired by
string edit distance (Wagner and Fischer, 1974). The algorithm takes a corpus
of flat sentences as input and returns a corpus of labelled, bracketed
sentences. The method works on pairs of unstructured sentences that have one or
more words in common. When two sentences are divided into parts that are the
same in both sentences and parts that are different, this information is used
to find parts that are interchangeable. These parts are taken as possible
constituents of the same type. After this alignment learning step, the
selection learning step selects the most probable constituents from all
possible constituents.
  This method was used to bootstrap structure on the ATIS corpus (Marcus et
al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands
for Public Transport Information System.) corpus (Bonnema et al., 1997). While
the results are encouraging (we obtained up to 89.25 % non-crossing brackets
precision), this paper will point out some of the shortcomings of our approach
and will suggest possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104007</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104007</id><created>2001-04-03</created><authors><author><keyname>van Zaanen</keyname><forenames>Menno</forenames></author></authors><title>Bootstrapping Syntax and Recursion using Alignment-Based Learning</title><categories>cs.LG cs.CL</categories><comments>8 pages</comments><acm-class>I.2; I.2.6; I.2.7</acm-class><journal-ref>Proceedings of the Seventeenth International Conference on Machine
  Learning. pages 1063-1070</journal-ref><abstract>  This paper introduces a new type of unsupervised learning algorithm, based on
the alignment of sentences and Harris's (1951) notion of interchangeability.
The algorithm is applied to an untagged, unstructured corpus of natural
language sentences, resulting in a labelled, bracketed version of the corpus.
Firstly, the algorithm aligns all sentences in the corpus in pairs, resulting
in a partition of the sentences consisting of parts of the sentences that are
similar in both sentences and parts that are dissimilar. This information is
used to find (possibly overlapping) constituents. Next, the algorithm selects
(non-overlapping) constituents. Several instances of the algorithm are applied
to the ATIS corpus (Marcus et al., 1993) and the OVIS (Openbaar Vervoer
Informatie Systeem (OVIS) stands for Public Transport Information System.)
corpus (Bonnema et al., 1997). Apart from the promising numerical results, the
most striking result is that even the simplest algorithm based on alignment
learns recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104008</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104008</id><created>2001-04-03</created><authors><author><keyname>Bauerdick</keyname><forenames>L. A. T.</forenames></author><author><keyname>Fox-Murphy</keyname><forenames>Adrian</forenames></author><author><keyname>Haas</keyname><forenames>Tobias</forenames></author><author><keyname>Stonjek</keyname><forenames>Stefan</forenames></author><author><keyname>Tassi</keyname><forenames>Enrico</forenames></author></authors><title>Event Indexing Systems for Efficient Selection and Analysis of HERA Data</title><categories>cs.DB cs.IR</categories><comments>Accepted for publication in Computer Physics Communications</comments><report-no>DESY 01-045</report-no><acm-class>H.2.4; H.3.1; H.3.3; H.3.4; J.2; H.2.8</acm-class><journal-ref>Comput.Phys.Commun. 137 (2001) 236-246</journal-ref><doi>10.1016/S0010-4655(01)00162-X</doi><abstract>  The design and implementation of two software systems introduced to improve
the efficiency of offline analysis of event data taken with the ZEUS Detector
at the HERA electron-proton collider at DESY are presented. Two different
approaches were made, one using a set of event directories and the other using
a tag database based on a commercial object-oriented database management
system. These are described and compared. Both systems provide quick direct
access to individual collision events in a sequential data store of several
terabytes, and they both considerably improve the event analysis efficiency. In
particular the tag database provides a very flexible selection mechanism and
can dramatically reduce the computing time needed to extract small subsamples
from the total event sample. Gains as large as a factor 20 have been obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104009</id><created>2001-04-03</created><authors><author><keyname>Mirza</keyname><forenames>Batul J.</forenames></author><author><keyname>Keller</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Evaluating Recommendation Algorithms by Graph Analysis</title><categories>cs.IR cs.DM cs.DS</categories><acm-class>H.4.2</acm-class><abstract>  We present a novel framework for evaluating recommendation algorithms in
terms of the `jumps' that they make to connect people to artifacts. This
approach emphasizes reachability via an algorithm within the implicit graph
structure underlying a recommender dataset, and serves as a complement to
evaluation in terms of predictive accuracy. The framework allows us to consider
questions relating algorithmic parameters to properties of the datasets. For
instance, given a particular algorithm `jump,' what is the average path length
from a person to an artifact? Or, what choices of minimum ratings and jumps
maintain a connected graph? We illustrate the approach with a common jump
called the `hammock' using movie recommender datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104010</id><created>2001-04-03</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames></author></authors><title>Type Arithmetics: Computation based on the theory of types</title><categories>cs.CL</categories><comments>1 HTML page, 1 C++ source code file</comments><acm-class>F.3.3; F.4.2; D.3.3</acm-class><abstract>  The present paper shows meta-programming turn programming, which is rich
enough to express arbitrary arithmetic computations. We demonstrate a type
system that implements Peano arithmetics, slightly generalized to negative
numbers. Certain types in this system denote numerals. Arithmetic operations on
such types-numerals - addition, subtraction, and even division - are expressed
as type reduction rules executed by a compiler. A remarkable trait is that
division by zero becomes a type error - and reported as such by a compiler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104011</id><created>2001-04-06</created><authors><author><keyname>Belding</keyname><forenames>Theodore C.</forenames></author></authors><title>Potholes on the Royal Road</title><categories>cs.NE nlin.AO</categories><comments>8 pages; to appear in GECCO 2001</comments><report-no>CSCS-2001-001</report-no><acm-class>I.2.m</acm-class><abstract>  It is still unclear how an evolutionary algorithm (EA) searches a fitness
landscape, and on what fitness landscapes a particular EA will do well. The
validity of the building-block hypothesis, a major tenet of traditional genetic
algorithm theory, remains controversial despite its continued use to justify
claims about EAs. This paper outlines a research program to begin to answer
some of these open questions, by extending the work done in the royal road
project. The short-term goal is to find a simple class of functions which the
simple genetic algorithm optimizes better than other optimization methods, such
as hillclimbers. A dialectical heuristic for searching for such a class is
introduced. As an example of using the heuristic, the simple genetic algorithm
is compared with a set of hillclimbers on a simple subset of the
hyperplane-defined functions, the pothole functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104012</id><created>2001-04-07</created><authors><author><keyname>Andersen</keyname><forenames>David G.</forenames></author><author><keyname>Bansal</keyname><forenames>Deepak</forenames></author><author><keyname>Curtis</keyname><forenames>Dorothy</forenames></author><author><keyname>Seshan</keyname><forenames>Srinivasan</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Hari</forenames></author></authors><title>System Support for Bandwidth Management and Content Adaptation in
  Internet Applications</title><categories>cs.NI cs.OS</categories><comments>14 pages, appeared in OSDI 2000</comments><acm-class>D.4.4</acm-class><journal-ref>Proc. OSDI 2000</journal-ref><abstract>  This paper describes the implementation and evaluation of an operating system
module, the Congestion Manager (CM), which provides integrated network flow
management and exports a convenient programming interface that allows
applications to be notified of, and adapt to, changing network conditions. We
describe the API by which applications interface with the CM, and the
architectural considerations that factored into the design. To evaluate the
architecture and API, we describe our implementations of TCP; a streaming
layered audio/video application; and an interactive audio application using the
CM, and show that they achieve adaptive behavior without incurring much
end-system overhead. All flows including TCP benefit from the sharing of
congestion information, and applications are able to incorporate new
functionality such as congestion control and adaptive behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104013</id><created>2001-04-09</created><authors><author><keyname>Matsuno</keyname><forenames>Koichiro</forenames></author></authors><title>Shooting Over or Under the Mark: Towards a Reliable and Flexible
  Anticipation in the Economy</title><categories>cs.CE</categories><acm-class>J.1</acm-class><journal-ref>Int. J. Comp. Anticipatory Syst. 5 (2000), 305-314</journal-ref><abstract>  The real monetary economy is grounded upon monetary flow equilibration or the
activity of actualizing monetary flow continuity at each economic agent except
for the central bank. Every update of monetary flow continuity at each agent
constantly causes monetary flow equilibration at the neighborhood agents. Every
monetary flow equilibration as the activity of shooting the mark identified as
monetary flow continuity turns out to be off the mark, and constantly generate
the similar activities in sequence. Monetary flow equilibration ceaselessly
reverberating in the economy performs two functions. One is to seek an
organization on its own, and the other is to perturb the ongoing organization.
Monetary flow equilibration as the agency of seeking and perturbing its
organization also serves as a means of predicting its behavior. The likely
organizational behavior could be the one that remains most robust against
monetary flow equilibration as an agency of applying perturbations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104014</id><created>2001-04-09</created><authors><author><keyname>Matsuno</keyname><forenames>Koichiro</forenames></author></authors><title>Tracing a Faint Fingerprint of the Invisible Hand?</title><categories>cs.CE</categories><acm-class>J.1</acm-class><abstract>  Any economic agent constituting the monetary economy maintains the activity
of monetary flow equilibration for fulfilling the condition of monetary flow
continuity in the record, except at the central bank. At the same time,
monetary flow equilibration at one economic agent constantly induces at other
agents in the economy further flow disequilibrium to be eliminated
subsequently. We propose the rate of monetary flow disequilibration as a figure
measuring the progressive movement of the economy. The rate of disequilibration
was read out of both the Japanese and the United States monetary economy
recorded over the last fifty years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104015</id><created>2001-04-17</created><updated>2001-05-22</updated><authors><author><keyname>Kim</keyname><forenames>Gene</forenames></author><author><keyname>Kim</keyname><forenames>MyungHo</forenames></author></authors><title>Application of Support Vector Machine to detect an association between a
  disease or trait and multiple SNP variations</title><categories>cs.CC q-bio</categories><acm-class>J.3</acm-class><abstract>  After the completion of human genome sequence was anounced, it is evident
that interpretation of DNA sequences is an immediate task to work on. For
understanding their signals, improvement of present sequence analysis tools and
developing new ones become necessary. Along this current trend, we attack one
of the fundamental questions, which set of SNP(single nucleotide polymorphism)
variations is related to a specific disease or trait is. For, in the whole DNA
sequence, it is known that people have different DNAs only at SNP locations,
and moreover, the total SNPs are less than 5 millions, finding an association
between SNP variations and certain disease or trait is believed to be one of
the essential steps not only for genetic researches but for drug design and
discovery. In this paper, we are going to present a method of detecting whether
there is an association between multiple SNP variations and a trait or disease.
The method exploits the Support Vector Machine which has been attracting lots
of attentions recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104016</id><created>2001-04-18</created><updated>2003-08-25</updated><authors><author><keyname>Peterson</keyname><forenames>Ian R.</forenames></author></authors><title>The Gibbs Representation of 3D Rotations</title><categories>cs.DS cs.CG</categories><comments>10 pages, 0 figures, PDF 3.0</comments><report-no>CMBE-01-IRP3</report-no><acm-class>B.2.4; F.2.1; G.1.0; I.4.0</acm-class><abstract>  This paper revisits the little-known Gibbs-Rodrigues representation of
rotations in a three-dimensional space and demonstrates a set of algorithms for
handling it. In this representation the rotation is itself represented as a
three-dimensional vector. The vector is parallel to the axis of rotation and
its three components transform covariantly on change of coordinates. The
mapping from rotations to vectors is 1:1 apart from computation error. The
discontinuities of the representation require special handling but are not
problematic. The rotation matrix can be generated efficiently from the vector
without the use of transcendental functions, and vice-versa. The representation
is more efficient than Euler angles, has affinities with Hassenpflug's Argyris
angles and is very closely related to the quaternion representation. While the
quaternion representation avoids the discontinuities inherent in any
3-component representation, this problem is readily overcome. The present paper
gives efficient algorithms for computing the set of rotations which map a given
vector to another of the same length and the rotation which maps a given pair
of vectors to another pair of the same length and subtended angle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104017</id><created>2001-04-18</created><authors><author><keyname>Schaerf</keyname><forenames>Andrea</forenames><affiliation>University of Udine, Italy</affiliation></author></authors><title>Local Search Techniques for Constrained Portfolio Selection Problems</title><categories>cs.CE cs.AI</categories><comments>22 pages, 3 figures</comments><acm-class>J.1; I.2.8</acm-class><abstract>  We consider the problem of selecting a portfolio of assets that provides the
investor a suitable balance of expected return and risk. With respect to the
seminal mean-variance model of Markowitz, we consider additional constraints on
the cardinality of the portfolio and on the quantity of individual shares. Such
constraints better capture the real-world trading system, but make the problem
more difficult to be solved with exact methods. We explore the use of local
search techniques, mainly tabu search, for the portfolio selection problem. We
compare and combine previous work on portfolio selection that makes use of the
local search approach and we propose new algorithms that combine different
neighborhood relations. In addition, we show how the use of randomization and
of a simple form of adaptiveness simplifies the setting of a large number of
critical parameters. Finally, we show how our techniques perform on public
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104018</id><created>2001-04-23</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Several new domain-type and boundary-type numerical discretization
  schemes with radial basis function</title><categories>cs.NA cs.CE</categories><comments>Welcome any comments to wenc@ifi.uio.no or chenw6@hotmail.com</comments><acm-class>G.1.3; G.1.8</acm-class><abstract>  This paper is concerned with a few novel RBF-based numerical schemes
discretizing partial differential equations. For boundary-type methods, we
derive the indirect and direct symmetric boundary knot methods (BKM). The
resulting interpolation matrix of both is always symmetric irrespective of
boundary geometry and conditions. In particular, the direct BKM applies the
practical physical variables rather than expansion coefficients and becomes
very competitive to the boundary element method. On the other hand, based on
the multiple reciprocity principle, we invent the RBF-based boundary particle
method (BPM) for general inhomogeneous problems without a need using inner
nodes. The direct and symmetric BPM schemes are also developed.
  For domain-type RBF discretization schemes, by using the Green integral we
develop a new Hermite RBF scheme called as the modified Kansa method (MKM),
which differs from the symmetric Hermite RBF scheme in that the MKM discretizes
both governing equation and boundary conditions on the same boundary nodes. The
local spline version of the MKM is named as the finite knot method (FKM). Both
MKM and FKM significantly reduce calculation errors at nodes adjacent to
boundary. In addition, the nonsingular high-order fundamental or general
solution is strongly recommended as the RBF in the domain-type methods and dual
reciprocity method approximation of particular solution relating to the BKM.
  It is stressed that all the above discretization methods of boundary-type and
domain-type are symmetric, meshless, and integration-free. The spline-based
schemes will produce desirable symmetric sparse banded interpolation matrix. In
appendix, we present a Hermite scheme to eliminate edge effect on the RBF
geometric modeling and imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104019</id><created>2001-04-27</created><authors><author><keyname>Florian</keyname><forenames>Radu</forenames></author><author><keyname>Yarowsky</keyname><forenames>David</forenames></author></authors><title>Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based
  Adaptation</title><categories>cs.CL</categories><comments>8 pages, 29 figures, presented at ACL99, College Park, Maryland</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 37th Annual Meeting of the ACL, pages 167-174,
  College Park, Maryland</journal-ref><abstract>  This paper presents a novel method of generating and applying hierarchical,
dynamic topic-based language models. It proposes and evaluates new cluster
generation, hierarchical smoothing and adaptive topic-probability estimation
techniques. These combined models help capture long-distance lexical
dependencies. Experiments on the Broadcast News corpus show significant
improvement in perplexity (10.5% overall and 33.5% on target vocabulary).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104020</id><created>2001-04-27</created><authors><author><keyname>Florian</keyname><forenames>Radu</forenames></author><author><keyname>Henderson</keyname><forenames>John C.</forenames></author><author><keyname>Ngai</keyname><forenames>Grace</forenames></author></authors><title>Coaxing Confidences from an Old Friend: Probabilistic Classifications
  from Transformation Rule Lists</title><categories>cs.CL cs.AI</categories><comments>9 pages, 4 figures, presented at EMNLP 2000</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Fifth Conference on Empirical Methods in
  Natural Language Processing, pages 26-34, Hong Kong (2000)</journal-ref><abstract>  Transformation-based learning has been successfully employed to solve many
natural language processing problems. It has many positive features, but one
drawback is that it does not provide estimates of class membership
probabilities.
  In this paper, we present a novel method for obtaining class membership
probabilities from a transformation-based rule list classifier. Three
experiments are presented which measure the modeling accuracy and cross-entropy
of the probabilistic classifier on unseen data and the degree to which the
output probabilities from the classifier can be used to estimate confidences in
its classification decisions.
  The results of these experiments show that, for the task of text chunking,
the estimates produced by this technique are more informative than those
generated by a state-of-the-art decision tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104021</id><created>2001-04-30</created><updated>2002-06-04</updated><authors><author><keyname>Stone</keyname><forenames>Matthew</forenames></author></authors><title>Disjunction and modular goal-directed proof search</title><categories>cs.LO</categories><acm-class>F.4.1; I.2.3; I.2.4</acm-class><abstract>  This paper explores goal-directed proof search in first-order multi-modal
logic. The key issue is to design a proof system that respects the modularity
and locality of assumptions of many modal logics. By forcing ambiguities to be
considered independently, modular disjunctions in particular can be used to
construct efficiently executable specifications in reasoning tasks involving
partial information that otherwise might require prohibitive search. To achieve
this behavior requires prior proof-theoretic justifications of logic
programming to be extended, strengthened, and combined with proof-theoretic
analyses of modal deduction in a novel way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0104022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0104022</id><created>2001-04-30</created><authors><author><keyname>Stone</keyname><forenames>Matthew</forenames></author><author><keyname>Doran</keyname><forenames>Christine</forenames></author><author><keyname>Webber</keyname><forenames>Bonnie</forenames></author><author><keyname>Bleam</keyname><forenames>Tonia</forenames></author><author><keyname>Palmer</keyname><forenames>Martha</forenames></author></authors><title>Microplanning with Communicative Intentions: The SPUD System</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><abstract>  The process of microplanning encompasses a range of problems in Natural
Language Generation (NLG), such as referring expression generation, lexical
choice, and aggregation, problems in which a generator must bridge underlying
domain-specific representations and general linguistic representations. In this
paper, we describe a uniform approach to microplanning based on declarative
representations of a generator's communicative intent. These representations
describe the results of NLG: communicative intent associates the concrete
linguistic structure planned by the generator with inferences that show how the
meaning of that structure communicates needed information about some
application domain in the current discourse context. Our approach, implemented
in the SPUD (sentence planning using description) microplanner, uses the
lexicalized tree-adjoining grammar formalism (LTAG) to connect structure to
meaning and uses modal logic programming to connect meaning to context. At the
same time, communicative intent representations provide a resource for the
process of NLG. Using representations of communicative intent, a generator can
augment the syntax, semantics and pragmatics of an incomplete sentence
simultaneously, and can assess its progress on the various problems of
microplanning incrementally. The declarative formulation of communicative
intent translates into a well-defined methodology for designing grammatical and
conceptual resources which the generator can use to achieve desired
microplanning behavior in a specified domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105001</id><created>2001-05-02</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Utiyama</keyname><forenames>Masao</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Correction of Errors in a Modality Corpus Used for Machine Translation
  by Using Machine-learning Method</title><categories>cs.CL</categories><comments>9 pages. Computation and Language. This paper is the English
  translation of our Japanese papar</comments><acm-class>H.3.3; I.2.7</acm-class><abstract>  We performed corpus correction on a modality corpus for machine translation
by using such machine-learning methods as the maximum-entropy method. We thus
constructed a high-quality modality corpus based on corpus correction. We
compared several kinds of methods for corpus correction in our experiments and
developed a good method for corpus correction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105002</id><created>2001-05-02</created><authors><author><keyname>Brill</keyname><forenames>Eric</forenames></author><author><keyname>Ngai</keyname><forenames>Grace</forenames></author></authors><title>Man [and Woman] vs. Machine: A Case Study in Base Noun Phrase Learning</title><categories>cs.CL</categories><comments>8 pages, 2 figures, presented at ACL 1999</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 37th Annual Meeting of the Association of
  Computational Linguistics, pages 65-72, College Park, MD, USA (1999)</journal-ref><abstract>  A great deal of work has been done demonstrating the ability of machine
learning algorithms to automatically extract linguistic knowledge from
annotated corpora. Very little work has gone into quantifying the difference in
ability at this task between a person and a machine. This paper is a first step
in that direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105003</id><created>2001-05-02</created><authors><author><keyname>Ngai</keyname><forenames>Grace</forenames></author><author><keyname>Yarowsky</keyname><forenames>David</forenames></author></authors><title>Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun
  Phrase Chunking</title><categories>cs.CL cs.AI</categories><comments>9 pages, 4 figures, appeared in ACL2000</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 38th Annual Meeting of the Association for
  Computational Linguistics, pages 117-125, Hong Kong (2000)</journal-ref><abstract>  This paper presents a comprehensive empirical comparison between two
approaches for developing a base noun phrase chunker: human rule writing and
active learning using interactive real-time human annotation. Several novel
variations on active learning are investigated, and underlying cost models for
cross-modal machine learning comparison are presented and explored. Results
show that it is more efficient and more successful by several measures to train
a system using active learning annotation rather than hand-crafted rule writing
at a comparable level of human labor investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105004</id><created>2001-05-02</created><authors><author><keyname>Nagel</keyname><forenames>Kai</forenames></author><author><keyname>Rickert</keyname><forenames>Marcus</forenames></author></authors><title>Parallel implementation of the TRANSIMS micro-simulation</title><categories>cs.CE</categories><acm-class>I.6;J.2;J.4</acm-class><abstract>  This paper describes the parallel implementation of the TRANSIMS traffic
micro-simulation. The parallelization method is domain decomposition, which
means that each CPU of the parallel computer is responsible for a different
geographical area of the simulated region. We describe how information between
domains is exchanged, and how the transportation network graph is partitioned.
An adaptive scheme is used to optimize load balancing. We then demonstrate how
computing speeds of our parallel micro-simulations can be systematically
predicted once the scenario and the computer architecture are known. This makes
it possible, for example, to decide if a certain study is feasible with a
certain computing budget, and how to invest that budget. The main ingredients
of the prediction are knowledge about the parallel implementation of the
micro-simulation, knowledge about the characteristics of the partitioning of
the transportation network graph, and knowledge about the interaction of these
quantities with the computer system. In particular, we investigate the
differences between switched and non-switched topologies, and the effects of
10 Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel
computing, transportation planning, TRANSIMS
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105005</id><created>2001-05-04</created><authors><author><keyname>Daud&#xe9;</keyname><forenames>J.</forenames><affiliation>TALP Research Center, Universitat Polit&#xe8;cnica de Catalunya</affiliation></author><author><keyname>Padr&#xf3;</keyname><forenames>L.</forenames><affiliation>TALP Research Center, Universitat Polit&#xe8;cnica de Catalunya</affiliation></author><author><keyname>Rigau</keyname><forenames>G.</forenames><affiliation>TALP Research Center, Universitat Polit&#xe8;cnica de Catalunya</affiliation></author></authors><title>A Complete WordNet1.5 to WordNet1.6 Mapping</title><categories>cs.CL</categories><comments>6 pages, 5 figures. To appear in proceedings of NAACL'01 Workshop on
  WordNet and Other Lexical Resources</comments><acm-class>I.2.7</acm-class><abstract>  We describe a robust approach for linking already existing lexical/semantic
hierarchies. We use a constraint satisfaction algorithm (relaxation labelling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In this paper we present the
complete mapping of the nominal, verbal, adjectival and adverbial parts of
WordNet 1.5 onto WordNet 1.6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105006</id><created>2001-05-04</created><authors><author><keyname>Ward</keyname><forenames>M. P.</forenames></author></authors><title>Reverse Engineering from Assembler to Formal Specifications via Program
  Transformations</title><categories>cs.SE cs.PL</categories><comments>10 pages</comments><acm-class>D.2.7;D.3.2</acm-class><journal-ref>7th Working Conference on Reverse Engineering 2000, 23--25 Nov
  2000, Brisbane, Queensland, Australia. IEEE Computer Society</journal-ref><abstract>  The FermaT transformation system, based on research carried out over the last
sixteen years at Durham University, De Montfort University and Software
Migrations Ltd., is an industrial-strength formal transformation engine with
many applications in program comprehension and language migration. This paper
is a case study which uses automated plus manually-directed transformations and
abstractions to convert an IBM 370 Assembler code program into a very
high-level abstract specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105007</id><created>2001-05-04</created><authors><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author></authors><title>Analysis of Polymorphically Typed Logic Programs Using ACI-Unification</title><categories>cs.LO</categories><comments>27 pages</comments><acm-class>D.1.6; F.3.2; F.3.3</acm-class><abstract>  Analysis of (partial) groundness is an important application of abstract
interpretation. There are several proposals for improving the precision of such
an analysis by exploiting type information, icluding our own work with Hill and
King, where we had shown how the information present in the type declarations
of a program can be used to characterise the degree of instantiation of a term
in a precise and yet inherently finite way. This approach worked for
polymorphically typed programs as in Goedel or HAL. Here, we recast this
approach following works by Codish, Lagoon and Stuckey. To formalise which
properties of terms we want to characterise, we use labelling functions, which
are functions that extract subterms from a term along certain paths. An
abstract term collects the results of all labelling functions of a term. For
the analysis, programs are executed on abstract terms instead of the concrete
ones, and usual unification is replaced by unification modulo an equality
theory which includes the well-known ACI-theory. Thus we generalise the works
by Codish, Lagoon and Stuckey w.r.t. the type systems considered and relate the
works among each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105008</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105008</id><created>2001-05-05</created><authors><author><keyname>Zhao</keyname><forenames>Jianjun</forenames></author></authors><title>Applying Slicing Technique to Software Architectures</title><categories>cs.SE</categories><comments>12 pages, 5 figures</comments><acm-class>D.2.4; D.2.5; D.2.7; D.2.11</acm-class><journal-ref>Proceedings of the 4th IEEE International Conference on
  Engineering of Complex Computer Systems, pp.87-98, August 1998</journal-ref><abstract>  Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural specifications) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintenance, and reuse will become an important issue.
This paper introduces a new form of slicing, named architectural slicing, to
aid architectural understanding and reuse. In contrast to traditional slicing,
architectural slicing is designed to operate on the architectural specification
of a software system, rather than the source code of a program. Architectural
slicing provides knowledge about the high-level structure of a software system,
rather than the low-level implementation details of a program. In order to
compute an architectural slice, we present the architecture information flow
graph which can be used to represent information flows in a software
architecture. Based on the graph, we give a two-phase algorithm to compute an
architectural slice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105009</id><created>2001-05-05</created><authors><author><keyname>Zhao</keyname><forenames>Jianjun</forenames></author></authors><title>Using Dependence Analysis to Support Software Architecture Understanding</title><categories>cs.SE</categories><comments>8 pages, 5 figures</comments><acm-class>D.2.4; D.2.5; D.2.7; D.2.11</acm-class><journal-ref>In M. Li (Ed.), &quot;New Technologies on Computer Software,&quot;
  pp.135-142, International Academic Publishers, September 1997</journal-ref><abstract>  Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural descriptions) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintaining, and reusing will become an important
issue. In this paper we introduce a new dependence analysis technique, named
architectural dependence analysis to support software architecture development.
In contrast to traditional dependence analysis, architectural dependence
analysis is designed to operate on an architectural description of a software
system, rather than the source code of a conventional program. Architectural
dependence analysis provides knowledge of dependences for the high-level
architecture of a software system, rather than the low-level implementation
details of a conventional program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105010</id><created>2001-05-05</created><authors><author><keyname>Zhao</keyname><forenames>Jianjun</forenames></author></authors><title>On Assessing the Complexity of Software Architectures</title><categories>cs.SE</categories><comments>4 pages</comments><acm-class>D.2.8; D.2.11</acm-class><journal-ref>Proceedings of the 3rd International Software Architecture
  Workshop (ISAW3), pp.163-166, ACM SIGSOFT, November 1998</journal-ref><abstract>  This paper proposes some new architectural metrics which are appropriate for
evaluating the architectural attributes of a software system. The main feature
of our approach is to assess the complexity of a software architecture by
analyzing various types of architectural dependences in the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105011</id><created>2001-05-07</created><authors><author><keyname>Goualard</keyname><forenames>Frederic</forenames></author></authors><title>Component Programming and Interoperability in Constraint Solver Design</title><categories>cs.PL</categories><comments>11 pages, 1 figure, paper accepted at the 6th Annual workshop of the
  ERCIM Working Group on Constraints</comments><acm-class>D.3.3; D.2</acm-class><abstract>  Prolog was once the main host for implementing constraint solvers.
  It seems that it is no longer so. To be useful, constraint solvers have to be
integrable into industrial applications written in imperative or
object-oriented languages; to be efficient, they have to interact with other
solvers. To meet these requirements, many solvers are now implemented in the
form of extensible object-oriented libraries. Following Pfister and Szyperski,
we argue that ``objects are not enough,'' and we propose to design solvers as
component-oriented libraries. We illustrate our approach by the description of
the architecture of a prototype, and we assess its strong points and
weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105012</id><created>2001-05-07</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames></author></authors><title>Joint and conditional estimation of tagging and parsing models</title><categories>cs.CL</categories><comments>8 pages, Proceedings of the ACL 2001</comments><acm-class>H.5.2</acm-class><abstract>  This paper compares two different ways of estimating statistical language
models. Many statistical NLP tagging and parsing models are estimated by
maximizing the (joint) likelihood of the fully-observed training data. However,
since these applications only require the conditional probability
distributions, these distributions can in principle be learnt by maximizing the
conditional likelihood of the training data. Perhaps somewhat surprisingly,
models estimated by maximizing the joint were superior to models estimated by
maximizing the conditional, even though some of the latter models intuitively
had access to ``more information''.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105013</id><created>2001-05-07</created><authors><author><keyname>Dolev</keyname><forenames>Shlomi</forenames></author><author><keyname>Herman</keyname><forenames>Ted</forenames></author></authors><title>Dijkstra's Self-Stabilizing Algorithm in Unsupportive Environments</title><categories>cs.DC</categories><comments>12 pages, uses eepic.sty</comments><report-no>TR-01-03</report-no><acm-class>D.1.3; B.1.3; B.8.1; D.4.5</acm-class><abstract>  The first self-stabilizing algorithm [Dij73] assumed the existence of a
central daemon, that activates one processor at time to change state as a
function of its own state and the state of a neighbor. Subsequent research has
reconsidered this algorithm without the assumption of a central daemon, and
under different forms of communication, such as the model of link registers. In
all of these investigations, one common feature is the atomicity of
communication, whether by shared variables or read/write registers. This paper
weakens the atomicity assumptions for the communication model, proposing
versions of [Dij73] that tolerate various weaker forms of atomicity. First, a
solution for the case of regular registers is presented. Then the case of safe
registers is considered, with both negative and positive results presented. The
paper also presents an implementation of [Dij73] based on registers that have
probabilistically correct behavior, which requires a notion of weak
stabilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105014</id><created>2001-05-07</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Errata and supplements to: Orthonormal RBF Wavelet and Ridgelet-like
  Series and Transforms for High-Dimensional Problems</title><categories>cs.NA cs.CE</categories><comments>Welcome any comments to wenc@ifi.uio.no</comments><acm-class>G1.3; G1.8</acm-class><abstract>  In recent years some attempts have been done to relate the RBF with wavelets
in handling high dimensional multiscale problems. To the author's knowledge,
however, the orthonormal and bi-orthogonal RBF wavelets are still missing in
the literature. By using the nonsingular general solution and singular
fundamental solution of differential operator, recently the present author,
refer. 3, made some substantial headway to derive the orthonormal RBF wavelets
series and transforms. The methodology can be generalized to create the RBF
wavelets by means of the orthogonal convolution kernel function of various
integral operators. In particular, it is stressed that the presented RBF
wavelets does not apply the tensor product to handle multivariate problems at
all.
  This note is to correct some errata in reference 3 and also to supply a few
latest advances in the study of orthornormal RBF wavelet transforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105015</id><created>2001-05-08</created><authors><author><keyname>van Hoeve</keyname><forenames>W. J.</forenames></author></authors><title>The alldifferent Constraint: A Survey</title><categories>cs.PL cs.AI</categories><comments>12 pages, 3 figures, paper accepted at the 6th Annual workshop of the
  ERCIM Working Group on Constraints</comments><acm-class>D.3.3</acm-class><abstract>  The constraint of difference is known to the constraint programming community
since Lauriere introduced Alice in 1978. Since then, several solving strategies
have been designed for this constraint. In this paper we give both a practical
overview and an abstract comparison of these different strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105016</id><created>2001-05-08</created><authors><author><keyname>Roark</keyname><forenames>Brian</forenames></author></authors><title>Probabilistic top-down parsing and language modeling</title><categories>cs.CL</categories><comments>28 pages, 6 tables, 8 figures. To appear in Computational Linguistics
  27(2), June 2001</comments><acm-class>I.2.7</acm-class><abstract>  This paper describes the functioning of a broad-coverage probabilistic
top-down parser, and its application to the problem of language modeling for
speech recognition. The paper first introduces key notions in language modeling
and probabilistic parsing, and briefly reviews some previous approaches to
using syntactic structure for language modeling. A lexicalized probabilistic
top-down parser is then presented, which performs very well, in terms of both
the accuracy of returned parses and the efficiency with which they are found,
relative to the best broad-coverage statistical parsers. A new language model
which utilizes probabilistic top-down parsing is then outlined, and empirical
results show that it improves upon previous work in test corpus perplexity.
Interpolation with a trigram model yields an exceptional improvement relative
to the improvement observed by other models, demonstrating the degree to which
the information captured by our parsing model is orthogonal to that captured by
a trigram model. A small recognition experiment also demonstrates the utility
of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105017</id><created>2001-05-08</created><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Optimization Over Zonotopes and Training Support Vector Machines</title><categories>cs.CG cs.AI</categories><comments>To appear in Workshop on Algorithms and Data Structures, 2001</comments><acm-class>F.2.2; G.1.6; I.2.6; I.5.1</acm-class><abstract>  We make a connection between classical polytopes called zonotopes and Support
Vector Machine (SVM) classifiers. We combine this connection with the ellipsoid
method to give some new theoretical results on training SVMs. We also describe
some special properties of soft margin C-SVMs as parameter C goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105018</id><created>2001-05-09</created><authors><author><keyname>Kristol</keyname><forenames>David M.</forenames></author></authors><title>HTTP Cookies: Standards, Privacy, and Politics</title><categories>cs.SE cs.CY</categories><acm-class>C.2.2; K.2</acm-class><journal-ref>ACM Transactions on Internet Technology, Vol. 1, #2, November 2001</journal-ref><abstract>  How did we get from a world where cookies were something you ate and where
&quot;non-techies&quot; were unaware of &quot;Netscape cookies&quot; to a world where cookies
are a hot-button privacy issue for many computer users? This paper will
describe how HTTP &quot;cookies&quot; work, and how Netscape's original specification
evolved into an IETF Proposed Standard. I will also offer a personal
perspective on how what began as a straightforward technical specification
turned into a political flashpoint when it tried to address non-technical
issues such as privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105019</id><created>2001-05-09</created><authors><author><keyname>Roark</keyname><forenames>Brian</forenames></author></authors><title>Robust Probabilistic Predictive Syntactic Processing</title><categories>cs.CL</categories><comments>Ph.D. Thesis, Brown University, Advisor: Mark Johnson. 140 pages, 40
  figures, 27 tables</comments><acm-class>I.2.7</acm-class><abstract>  This thesis presents a broad-coverage probabilistic top-down parser, and its
application to the problem of language modeling for speech recognition. The
parser builds fully connected derivations incrementally, in a single pass from
left-to-right across the string. We argue that the parsing approach that we
have adopted is well-motivated from a psycholinguistic perspective, as a model
that captures probabilistic dependencies between lexical items, as part of the
process of building connected syntactic structures. The basic parser and
conditional probability models are presented, and empirical results are
provided for its parsing accuracy on both newspaper text and spontaneous
telephone conversations. Modifications to the probability model are presented
that lead to improved performance. A new language model which uses the output
of the parser is then defined. Perplexity and word error rate reduction are
demonstrated over trigram models, even when the trigram is trained on
significantly more data. Interpolation on a word-by-word basis with a trigram
model yields additional improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105020</id><created>2001-05-09</created><updated>2002-02-07</updated><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Ma</keyname><forenames>Shilong</forenames></author><author><keyname>Sui</keyname><forenames>Yuefei</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>A Logical Framework for Convergent Infinite Computations</title><categories>cs.LO cs.PL</categories><comments>17 pages. Welcome any comments to kexu@nlsde.buaa.edu.cn</comments><acm-class>F.4.1, D.1.6</acm-class><abstract>  Classical computations can not capture the essence of infinite computations
very well. This paper will focus on a class of infinite computations called
convergent infinite computations}. A logic for convergent infinite computations
is proposed by extending first order theories using Cauchy sequences, which has
stronger expressive power than the first order logic. A class of fixed points
characterizing the logical properties of the limits can be represented by means
of infinite-length terms defined by Cauchy sequences. We will show that the
limit of sequence of first order theories can be defined in terms of distance,
similar to the $\epsilon-N$ style definition of limits in real analysis. On the
basis of infinitary terms, a computation model for convergent infinite
computations is proposed. Finally, the interpretations of logic programs are
extended by introducing real Herbrand models of logic programs and a sufficient
condition for computing a real Herbrand model of Horn logic programs using
convergent infinite computation is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105021</id><created>2001-05-11</created><authors><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author><author><keyname>Jaulin</keyname><forenames>Luc</forenames></author></authors><title>Solving Composed First-Order Constraints from Discrete-Time Robust
  Control</title><categories>cs.LO cs.AI cs.CE</categories><comments>Presented at the Sixth Annual Workshop of the ERCIM Working Group on
  Constraints</comments><acm-class>F.4.1;I.2.8</acm-class><abstract>  This paper deals with a problem from discrete-time robust control which
requires the solution of constraints over the reals that contain both universal
and existential quantifiers. For solving this problem we formulate it as a
program in a (fictitious) constraint logic programming language with explicit
quantifier notation. This allows us to clarify the special structure of the
problem, and to extend an algorithm for computing approximate solution sets of
first-order constraints over the reals to exploit this structure. As a result
we can deal with inputs that are clearly out of reach for current symbolic
solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105022</id><created>2001-05-11</created><authors><author><keyname>Fu</keyname><forenames>Li Min</forenames></author></authors><title>Multi-Channel Parallel Adaptation Theory for Rule Discovery</title><categories>cs.AI</categories><comments>21 pages, 1 figure, 7 tables</comments><acm-class>I2.6</acm-class><abstract>  In this paper, we introduce a new machine learning theory based on
multi-channel parallel adaptation for rule discovery. This theory is
distinguished from the familiar parallel-distributed adaptation theory of
neural networks in terms of channel-based convergence to the target rules. We
show how to realize this theory in a learning system named CFRule. CFRule is a
parallel weight-based model, but it departs from traditional neural computing
in that its internal knowledge is comprehensible. Furthermore, when the model
converges upon training, each channel converges to a target rule. The model
adaptation rule is derived by multi-level parallel weight optimization based on
gradient descent. Since, however, gradient descent only guarantees local
optimization, a multi-channel regression-based optimization strategy is
developed to effectively deal with this problem. Formally, we prove that the
CFRule model can explicitly and precisely encode any given rule set. Also, we
prove a property related to asynchronous parallel convergence, which is a
critical element of the multi-channel parallel adaptation theory for rule
learning. Thanks to the quantizability nature of the CFRule model, rules can be
extracted completely and soundly via a threshold-based mechanism. Finally, the
practical application of the theory is demonstrated in DNA promoter recognition
and hepatitis prognosis prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105023</id><created>2001-05-14</created><authors><author><keyname>Dupuy</keyname><forenames>Sylvain</forenames></author><author><keyname>Egges</keyname><forenames>Arjan</forenames></author><author><keyname>Legendre</keyname><forenames>Vincent</forenames></author><author><keyname>Nugues</keyname><forenames>Pierre</forenames></author></authors><title>Generating a 3D Simulation of a Car Accident from a Written Description
  in Natural Language: the CarSim System</title><categories>cs.CL</categories><comments>8 pages, ACL 2001, Workshop on Temporal and Spatial Information
  Processing</comments><acm-class>I.2.7; H.5.1</acm-class><abstract>  This paper describes a prototype system to visualize and animate 3D scenes
from car accident reports, written in French. The problem of generating such a
3D simulation can be divided into two subtasks: the linguistic analysis and the
virtual scene generation. As a means of communication between these two
modules, we first designed a template formalism to represent a written accident
report. The CarSim system first processes written reports, gathers relevant
information, and converts it into a formal description. Then, it creates the
corresponding 3D scene and animates the vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105024</id><created>2001-05-14</created><authors><author><keyname>Brand</keyname><forenames>Sebastian</forenames></author></authors><title>Constraint Propagation in Presence of Arrays</title><categories>cs.PL cs.DS</categories><comments>10 pages. Accepted at the 6th Annual Workshop of the ERCIM Working
  Group on Constraints, 2001</comments><acm-class>D.3.3; E.1</acm-class><abstract>  We describe the use of array expressions as constraints, which represents a
consequent generalisation of the &quot;element&quot; constraint. Constraint propagation
for array constraints is studied theoretically, and for a set of domain
reduction rules the local consistency they enforce, arc-consistency, is proved.
An efficient algorithm is described that encapsulates the rule set and so
inherits the capability to enforce arc-consistency from the rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105025</id><created>2001-05-15</created><authors><author><keyname>Kwee</keyname><forenames>Ivo</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Market-Based Reinforcement Learning in Partially Observable Worlds</title><categories>cs.AI cs.LG cs.MA cs.NE</categories><comments>8 LaTeX pages, 2 postscript figures</comments><report-no>IDSIA-10-01</report-no><acm-class>I.2</acm-class><journal-ref>Lecture Notes in Computer Science (LNCS 2130), Proceeding of the
  International Conference on Artificial Neural Networks ICANN (2001) 865-873</journal-ref><abstract>  Unlike traditional reinforcement learning (RL), market-based RL is in
principle applicable to worlds described by partially observable Markov
Decision Processes (POMDPs), where an agent needs to learn short-term memories
of relevant previous events in order to execute optimal actions. Most previous
work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here
we reimplement a recent approach to market-based RL and for the first time
evaluate it in a toy POMDP setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105026</id><created>2001-05-17</created><authors><author><keyname>Kettebekov</keyname><forenames>S.</forenames></author><author><keyname>Sharma</keyname><forenames>R.</forenames></author></authors><title>Toward Natural Gesture/Speech Control of a Large Display</title><categories>cs.CV cs.HC</categories><comments>Engineering for Human-Computer Interaction (EHCI'01),Toronto, Canada.
  May 11-14, 2001. Lecture Notes in Computer Science, Springer Verlag. 14 pages</comments><acm-class>H.5.2; I.5.4; I.2.7</acm-class><abstract>  In recent years because of the advances in computer vision research, free
hand gestures have been explored as means of human-computer interaction (HCI).
Together with improved speech processing technology it is an important step
toward natural multimodal HCI. However, inclusion of non-predefined continuous
gestures into a multimodal framework is a challenging problem. In this paper,
we propose a structured approach for studying patterns of multimodal language
in the context of a 2D-display control. We consider systematic analysis of
gestures from observable kinematical primitives to their semantics as pertinent
to a linguistic structure. Proposed semantic classification of co-verbal
gestures distinguishes six categories based on their spatio-temporal deixis. We
discuss evolution of a computational framework for gesture and speech
integration which was used to develop an interactive testbed (iMAP). The
testbed enabled elicitation of adequate, non-sequential, multimodal patterns in
a narrative mode of HCI. Conducted user studies illustrate significance of
accounting for the temporal alignment of gesture and speech parts in semantic
mapping. Furthermore, co-occurrence analysis of gesture/speech production
suggests syntactic organization of gestures at the lexical level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105027</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105027</id><created>2001-05-17</created><authors><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayan</forenames></author></authors><title>Bounds on sample size for policy evaluation in Markov environments</title><categories>cs.LG cs.AI cs.CC</categories><comments>14 pages</comments><acm-class>G.2;G.1.6;I.2;I.2.6</acm-class><journal-ref>COLT 2001: The Fourteenth Annual Conference on Computational
  Learning Theory</journal-ref><abstract>  Reinforcement learning means finding the optimal course of action in
Markovian environments without knowledge of the environment's dynamics.
Stochastic optimization algorithms used in the field rely on estimates of the
value of a policy. Typically, the value of a policy is estimated from results
of simulating that very policy in the environment. This approach requires a
large amount of simulation as different points in the policy space are
considered. In this paper, we develop value estimators that utilize data
gathered when using one policy to estimate the value of using another policy,
resulting in much more data-efficient algorithms. We consider the question of
accumulating a sufficient experience and give PAC-style bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105028</id><created>2001-05-18</created><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Keller</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Mirza</keyname><forenames>Batul J.</forenames></author><author><keyname>Grama</keyname><forenames>Ananth Y.</forenames></author><author><keyname>Karypis</keyname><forenames>George</forenames></author></authors><title>When being Weak is Brave: Privacy in Recommender Systems</title><categories>cs.CR cs.DS</categories><acm-class>H.4.2</acm-class><abstract>  We explore the conflict between personalization and privacy that arises from
the existence of weak ties. A weak tie is an unexpected connection that
provides serendipitous recommendations. However, information about weak ties
could be used in conjunction with other sources of data to uncover identities
and reveal other personal information. In this article, we use a
graph-theoretic model to study the benefit and risk from weak ties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105029</id><created>2001-05-21</created><authors><author><keyname>Halperin</keyname><forenames>Eran</forenames></author><author><keyname>Nathaniel</keyname><forenames>Ram</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Coloring k-colorable graphs using relatively small palettes</title><categories>cs.DS cs.CC cs.DM</categories><comments>16 pages, 2 figures. A preliminary version of this paper appeared in
  the proceedings the of 12th ACM-SIAM Symposium on Discrete Algorithm
  (SODA'01) under a slightly different title</comments><acm-class>F.2.2;G.2.2;G.3;G.1.6</acm-class><abstract>  We obtain the following new coloring results:
  * A 3-colorable graph on $n$ vertices with maximum degree~$\Delta$ can be
colored, in polynomial time, using $O((\Delta \log\Delta)^{1/3} \cdot\log{n})$
colors. This slightly improves an $O((\Delta^{{1}/{3}}
\log^{1/2}\Delta)\cdot\log{n})$ bound given by Karger, Motwani and Sudan.  More
generally, $k$-colorable graphs with maximum degree $\Delta$ can be colored, in
polynomial time, using $O((\Delta^{1-{2}/{k}}\log^{1/k}\Delta) \cdot\log{n})$
colors.
  * A 4-colorable graph on $n$ vertices can be colored, in polynomial time,
using $\Ot(n^{7/19})$ colors. This improves an $\Ot(n^{2/5})$ bound given again
by Karger, Motwani and Sudan. More generally, $k$-colorable graphs on $n$
vertices can be colored, in polynomial time, using $\Ot(n^{\alpha_k})$ colors,
where $\alpha_5=97/207$, $\alpha_6=43/79$, $\alpha_7=1391/2315$,
$\alpha_8=175/271$, ...
  The first result is obtained by a slightly more refined probabilistic
analysis of the semidefinite programming based coloring algorithm of Karger,
Motwani and Sudan. The second result is obtained by combining the coloring
algorithm of Karger, Motwani and Sudan, the combinatorial coloring algorithms
of Blum and an extension of a technique of Alon and Kahale (which is based on
the Karger, Motwani and Sudan algorithm) for finding relatively large
independent sets in graphs that are guaranteed to have very large independent
sets. The extension of the Alon and Kahale result may be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105030</id><created>2001-05-21</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Simons</keyname><forenames>Gary</forenames></author></authors><title>The OLAC Metadata Set and Controlled Vocabularies</title><categories>cs.CL cs.DL</categories><comments>12 pages, 5 figures</comments><acm-class>H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5</acm-class><journal-ref>Proceedings of the ACL/EACL Workshop on Sharing Tools and
  Resources for Research and Education, Toulouse, July 2001, Association for
  Computational Linguistics</journal-ref><abstract>  As language data and associated technologies proliferate and as the language
resources community rapidly expands, it has become difficult to locate and
reuse existing resources. Are there any lexical resources for such-and-such a
language? What tool can work with transcripts in this particular format? What
is a good format to use for linguistic data of this type? Questions like these
dominate many mailing lists, since web search engines are an unreliable way to
find language resources. This paper describes a new digital infrastructure for
language resource discovery, based on the Open Archives Initiative, and called
OLAC -- the Open Language Archives Community. The OLAC Metadata Set and the
associated controlled vocabularies facilitate consistent description and
focussed searching. We report progress on the metadata set and controlled
vocabularies, describing current issues and soliciting input from the language
resources community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105031</id><created>2001-05-23</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>State Analysis and Aggregation Study for Multicast-based Micro Mobility</title><categories>cs.NI</categories><comments>15 pages, 17 figures, Keywords Micro Mobility, Multicast State,
  Efficient Handoff, Network Simulation, State Aggregation</comments><acm-class>C.2;C.2.1;C.2.2</acm-class><abstract>  IP mobility addresses the problem of changing the network point-of-attachment
transparently during movement. Mobile IP is the proposed standard by IETF.
Several studies, however, have shown that Mobile IP has several drawbacks, such
as triangle routing and poor handoff performance. Multicast-based mobility has
been proposed as a promising solution to the above problems, incurring less
end-to-end delays and fast smooth handoff. Nonetheless, such architecture
suffers from multicast state scalability problems with the growth in number of
mobile nodes. This architecture also requires ubiquitous multicast deployment
and more complex security measures. To alleviate these problems, we propose an
intra-domain multicast-based mobility solution. A mobility proxy allocates a
multicast address for each mobile that moves to its domain. The mobile uses
this multicast address within a domain for micro mobility. Also, aggregation is
considered to reduce the multicast state. We conduct multicast state analysis
to study the efficiency of several aggregation techniques. We use extensive
simulation to evaluate our protocol's performance over a variety of real and
generated topologies. We take aggregation gain as metric for our evaluation.
  Our simulation results show that in general leaky aggregation obtains better
gains than perfect aggregation. Also, we notice that aggregation gain increases
with the increase in number of visiting mobile nodes and with the decrease in
number of mobility proxies within a domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105032</id><created>2001-05-24</created><authors><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Eung</forenames></author><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author></authors><title>Learning to Cooperate via Policy Search</title><categories>cs.LG cs.MA</categories><comments>8 pages, 5 figures</comments><acm-class>I.2;I.2.9;I.2.11</acm-class><journal-ref>Sixteenth Conference on Uncertainty in Artificial Intelligence,
  2000</journal-ref><abstract>  Cooperative games are those in which both agents share the same payoff
structure. Value-based reinforcement-learning algorithms, such as variants of
Q-learning, have been applied to learning cooperative games, but they only
apply when the game state is completely observable to both agents. Policy
search methods are a reasonable alternative to value-based methods for
partially observable environments. In this paper, we provide a gradient-based
distributed policy-search method for cooperative games and compare the notion
of local optimum to that of Nash equilibrium. We demonstrate the effectiveness
of this method experimentally in a small, partially observable simulated soccer
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105033</id><created>2001-05-25</created><authors><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>Lectures on Reduce and Maple at UAM I - Mexico</title><categories>cs.SC cs.MS gr-qc</categories><comments>61 pages; supplementary material can be found at
  http://www.neuroinformatik.ruhr-uni-bochum.de/PEOPLE/mt/work/1999mexico/</comments><acm-class>I.1.3</acm-class><abstract>  These lectures give a brief introduction to the Computer Algebra systems
Reduce and Maple. The aim is to provide a systematic survey of most important
commands and concepts. In particular, this includes a discussion of
simplification schemes and the handling of simplification and substitution
rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification
rules).
  Another emphasis is on the different implementations of tensor calculi and
the exterior calculus by Reduce and Maple and their application in Gravitation
theory and Differential Geometry.
  I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa,
Departamento de Fisica, Mexico, in November 1999.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105034</id><created>2001-05-29</created><authors><author><keyname>Greenberg</keyname><forenames>Ronald I.</forenames></author><author><keyname>Guan</keyname><forenames>Lee</forenames></author></authors><title>On the Area of Hypercube Layouts</title><categories>cs.DC</categories><comments>8 pages, 4 figures, LaTeX</comments><acm-class>C.1.2</acm-class><journal-ref>condensed and revised in Information Processing Letters, v. 84, n.
  1, pp. 41--46, Sep. 2002</journal-ref><abstract>  This paper precisely analyzes the wire density and required area in standard
layout styles for the hypercube. The most natural, regular layout of a
hypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses
floor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of
tracks per row can be reduced by 1 with a less regular design.) This paper also
gives a simple formula for the wire density at any cut position and a full
characterization of all places where the wire density is maximized (which does
not occur at the bisection).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105035</id><created>2001-05-29</created><authors><author><keyname>Kromer</keyname><forenames>Victor</forenames></author></authors><title>Historical Dynamics of Lexical System as Random Walk Process</title><categories>cs.CL</categories><comments>4 pages, 1 table, 3 figures. Submitted to conference &quot;Language in
  Sinchrony and Diachrony&quot;, to be held in Petrozavodsk State Pedagogical
  University (Russia), 15-17 October, 2001. (In Russian)</comments><acm-class>I.2.7</acm-class><abstract>  It is offered to consider word meanings changes in diachrony as
semicontinuous random walk with reflecting and swallowing screens. The basic
characteristics of word life cycle are defined. Verification of the model has
been realized on the data of Russian words distribution on various age periods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105036</identifier>
 <datestamp>2008-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105036</id><created>2001-05-30</created><updated>2001-06-28</updated><authors><author><keyname>Buccafurri</keyname><forenames>Francesco</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author></authors><title>Disjunctive Logic Programs with Inheritance</title><categories>cs.LO cs.AI</categories><comments>28 pages; will be published in Theory and Practice of Logic
  Programming</comments><acm-class>D.1.6; I.2.3; I.2.4</acm-class><journal-ref>Theory and Practice of Logic Programming 2(3):293-321, 2002</journal-ref><doi>10.1017/S1471068402001394</doi><abstract>  The paper proposes a new knowledge representation language, called DLP&lt;,
which extends disjunctive logic programming (with strong negation) by
inheritance. The addition of inheritance enhances the knowledge modeling
features of the language providing a natural representation of default
reasoning with exceptions.
  A declarative model-theoretic semantics of DLP&lt; is provided, which is shown
to generalize the Answer Set Semantics of disjunctive logic programs.
  The knowledge modeling features of the language are illustrated by encoding
classical nonmonotonic problems in DLP&lt;.
  The complexity of DLP&lt; is analyzed, proving that inheritance does not cause
any computational overhead, as reasoning in DLP&lt; has exactly the same
complexity as reasoning in disjunctive logic programming. This is confirmed by
the existence of an efficient translation from DLP&lt; to plain disjunctive logic
programming. Using this translation, an advanced KR system supporting the DLP&lt;
language has been implemented on top of the DLV system and has subsequently
been integrated into DLV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0105037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0105037</id><created>2001-05-31</created><authors><author><keyname>Tur</keyname><forenames>G.</forenames></author><author><keyname>Hakkani-Tur</keyname><forenames>D.</forenames></author><author><keyname>Stolcke</keyname><forenames>A.</forenames></author><author><keyname>Shriberg</keyname><forenames>E.</forenames></author></authors><title>Integrating Prosodic and Lexical Cues for Automatic Topic Segmentation</title><categories>cs.CL</categories><comments>27 pages, 8 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Computation Linguistics 27(1), 31-57, March 2001</journal-ref><abstract>  We present a probabilistic model that uses both prosodic and lexical cues for
the automatic segmentation of speech into topically coherent units. We propose
two methods for combining lexical and prosodic information using hidden Markov
models and decision trees. Lexical information is obtained from a speech
recognizer, and prosodic features are extracted automatically from speech
waveforms. We evaluate our approach on the Broadcast News corpus, using the
DARPA-TDT evaluation metrics. Results show that the prosodic model alone is
competitive with word-based segmentation methods. Furthermore, we achieve a
significant reduction in error by combining the prosodic and word-based
knowledge sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106001</id><created>2001-06-01</created><authors><author><keyname>Creignou</keyname><forenames>Nadia</forenames></author><author><keyname>Daude</keyname><forenames>Herve</forenames></author><author><keyname>Dubois</keyname><forenames>Olivier</forenames></author></authors><title>Approximating the satisfiability threshold for random k-XOR-formulas</title><categories>cs.DM</categories><comments>15 pages, 1 figure</comments><acm-class>F.2.1; G.2m; G.3</acm-class><abstract>  In this paper we study random linear systems with $k$ variables per equation
over the finite field GF(2), or equivalently $k$-XOR-CNF formulas. In a
previous paper Creignou and Daud\'e proved that the phase transition for the
consistency (satisfiability) of such systems (formulas) exhibits a sharp
threshold. Here we prove that the phase transition occurs as the number of
equations (clauses) is proportional to the number of variables. For any $k\ge
3$ we establish first estimates for the critical ratio. For $k=3$ we get 0.93
as an upper bound, 0.89 as a lower bound, whereas experiments suggest that the
critical ratio is approximately 0.92.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106002</id><created>2001-06-01</created><updated>2001-08-21</updated><authors><author><keyname>Bockmayr</keyname><forenames>Alexander</forenames></author><author><keyname>Pisaruk</keyname><forenames>Nicolai</forenames></author></authors><title>Solving Assembly Line Balancing Problems by Combining IP and CP</title><categories>cs.DM</categories><comments>10 pages, Sixth Annual Workshop of the ERCIM Working Group on
  Constraints, Prague, June 2001</comments><acm-class>F.2.2; I.2.8</acm-class><abstract>  Assembly line balancing problems consist in partitioning the work necessary
to assemble a number of products among different stations of an assembly line.
We present a hybrid approach for solving such problems, which combines
constraint programming and integer programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106003</id><created>2001-06-03</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>He</keyname><forenames>W.</forenames></author></authors><title>A note on radial basis function computing</title><categories>cs.CE cs.CG</categories><acm-class>G.1.3; G.1.8</acm-class><journal-ref>Int. J. Nonlinear Modelling in Sci. &amp; Engng., 1(1), 2001</journal-ref><abstract>  This note carries three purposes involving our latest advances on the radial
basis function (RBF) approach. First, we will introduce a new scheme employing
the boundary knot method (BKM) to nonlinear convection-diffusion problem. It is
stressed that the new scheme directly results in a linear BKM formulation of
nonlinear problems by using response point-dependent RBFs, which can be solved
by any linear solver. Then we only need to solve a single nonlinear algebraic
equation for desirable unknown at some inner node of interest. The numerical
results demonstrate high accuracy and efficiency of this nonlinear BKM
strategy. Second, we extend the concepts of distance function, which include
time-space and variable transformation distance functions. Finally, we
demonstrate that if the nodes are symmetrically placed, the RBF coefficient
matrices have either centrosymmetric or skew centrosymmetric structures. The
factorization features of such matrices lead to a considerable reduction in the
RBF computing effort. A simple approach is also presented to reduce the
ill-conditioning of RBF interpolation matrices in general cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106004</id><created>2001-06-02</created><authors><author><keyname>Rudova</keyname><forenames>Hana</forenames></author></authors><title>Soft Scheduling</title><categories>cs.AI cs.PL</categories><comments>10 pages; accepted to the Sixth Annual Workshop of the ERCIM Working
  Group on Constraints</comments><acm-class>I.2.8; F.4.1; I.6.5</acm-class><abstract>  Classical notions of disjunctive and cumulative scheduling are studied from
the point of view of soft constraint satisfaction. Soft disjunctive scheduling
is introduced as an instance of soft CSP and preferences included in this
problem are applied to generate a lower bound based on existing discrete
capacity resource. Timetabling problems at Purdue University and Faculty of
Informatics at Masaryk University considering individual course requirements of
students demonstrate practical problems which are solved via proposed methods.
Implementation of general preference constraint solver is discussed and first
computational results for timetabling problem are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106005</id><created>2001-06-07</created><authors><author><keyname>Daskalopulu</keyname><forenames>Aspassia</forenames></author><author><keyname>Sergot</keyname><forenames>Marek</forenames></author></authors><title>The Representation of Legal Contracts</title><categories>cs.AI cs.CY</categories><acm-class>I.2</acm-class><journal-ref>AI &amp; Society, vol. 11, Nos 1/2, pp. 6-17, 1997</journal-ref><abstract>  The paper outlines ongoing research on logic-based tools for the analysis and
representation of legal contracts of the kind frequently encountered in
large-scale engineering projects and complex, long-term trading agreements. We
consider both contract formation and contract performance, in each case
identifying the representational issues and the prospects for providing
automated support tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106006</id><created>2001-06-07</created><authors><author><keyname>Daskalopulu</keyname><forenames>Aspassia</forenames></author><author><keyname>Sergot</keyname><forenames>Marek</forenames></author></authors><title>A Constraint-Driven System for Contract Assembly</title><categories>cs.AI</categories><acm-class>I.2; H.4.1</acm-class><journal-ref>Proc. 5th International Conference on Artificial Intelligence and
  Law, ACM Press, pp. 62-69, 1995</journal-ref><abstract>  We present an approach for modelling the structure and coarse content of
legal documents with a view to providing automated support for the drafting of
contracts and contract database retrieval. The approach is designed to be
applicable where contract drafting is based on model-form contracts or on
existing examples of a similar type. The main features of the approach are: (1)
the representation addresses the structure and the interrelationships between
the constituent parts of contracts, but not the text of the document itself;
(2) the representation of documents is separated from the mechanisms that
manipulate it; and (3) the drafting process is subject to a collection of
explicitly stated constraints that govern the structure of the documents. We
describe the representation of document instances and of 'generic documents',
which are data structures used to drive the creation of new document instances,
and we show extracts from a sample session to illustrate the features of a
prototype system implemented in MacProlog.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106007</id><created>2001-06-07</created><authors><author><keyname>Reed</keyname><forenames>Chris</forenames></author><author><keyname>Daskalopulu</keyname><forenames>Aspassia</forenames></author></authors><title>Modelling Contractual Arguments</title><categories>cs.AI</categories><acm-class>I.2</acm-class><journal-ref>Proceedings of the 4th International Conference on Argumentation,
  SICSAT, pp. 686-692, 1998</journal-ref><abstract>  One influential approach to assessing the &quot;goodness&quot; of arguments is offered
by the Pragma-Dialectical school (p-d) (Eemeren &amp; Grootendorst 1992). This can
be compared with Rhetorical Structure Theory (RST) (Mann &amp; Thompson 1988), an
approach that originates in discourse analysis. In p-d terms an argument is
good if it avoids committing a fallacy, whereas in RST terms an argument is
good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for
providing only a partially functional account of argument, and similar
criticisms have been raised in the Natural Language Generation (NLG)
community-particularly by Moore &amp; Pollack (1992)- with regards to its account
of intentionality in text in general. Mann and Thompson themselves note that
although RST can be successfully applied to a wide range of texts from diverse
domains, it fails to characterise some types of text, most notably legal
contracts. There is ongoing research in the Artificial Intelligence and Law
community exploring the potential for providing electronic support to contract
negotiators, focusing on long-term, complex engineering agreements (see for
example Daskalopulu &amp; Sergot 1997). This paper provides a brief introduction to
RST and illustrates its shortcomings with respect to contractual text. An
alternative approach for modelling argument structure is presented which not
only caters for contractual text, but also overcomes the aforementioned
limitations of RST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106008</id><created>2001-06-07</created><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Computing Functional and Relational Box Consistency by Structured
  Propagation in Atomic Constraint Systems</title><categories>cs.PL cs.AI</categories><comments>Presented at the Sixth Annual Workshop of the ERCIM Working Group on
  Constraints. 12 pages</comments><report-no>Univ. of Victoria Computer Science Dept Technical Report DCS-266-IR</report-no><acm-class>D.3.2; D.3.3; F.4.1</acm-class><abstract>  Box consistency has been observed to yield exponentially better performance
than chaotic constraint propagation in the interval constraint system obtained
by decomposing the original expression into primitive constraints. The claim
was made that the improvement is due to avoiding decomposition. In this paper
we argue that the improvement is due to replacing chaotic iteration by a more
structured alternative.
  To this end we distinguish the existing notion of box consistency from
relational box consistency. We show that from a computational point of view it
is important to maintain the functional structure in constraint systems that
are associated with a system of equations. So far, it has only been considered
computationally important that constraint propagation be fair. With the
additional structure of functional constraint systems, one can define and
implement computationally effective, structured, truncated constraint
propagations. The existing algorithm for box consistency is one such. Our
results suggest that there are others worth investigating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106009</id><created>2001-06-07</created><authors><author><keyname>Daskalopulu</keyname><forenames>Aspassia</forenames></author></authors><title>Model Checking Contractual Protocols</title><categories>cs.SE cs.LO</categories><acm-class>D.2.4;I.2.4</acm-class><journal-ref>In Breuker, Leenes &amp; Winkels (eds) Legal Knowledge and Information
  Systems, JURIX 2000: The 13th Annual Conference, Frontiers in Artificial
  Intelligence and Applications Series, IOS Press, pp. 35-47, 2000</journal-ref><abstract>  This paper discusses how model checking, a technique used for the
verification of behavioural requirements of dynamic systems, can be usefully
deployed for the verification of contracts. A process view of agreements
between parties is taken, whereby a contract is modelled as it evolves over
time in terms of actions or more generally events that effect changes in its
state. Modelling is done with Petri Nets in the spirit of other research work
on the representation of trade procedures. The paper illustrates all the phases
of the verification technique through an example and argues that the approach
is useful particularly in the context of pre-contractual negotiation and
contract drafting. The work reported here is part of a broader project on the
development of logic-based tools for the analysis and representation of legal
contracts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106010</id><created>2001-06-07</created><authors><author><keyname>Daskalopulu</keyname><forenames>Aspassia</forenames></author></authors><title>Modelling Legal Contracts as Processes</title><categories>cs.AI cs.LO</categories><acm-class>I.2.4</acm-class><journal-ref>11th International Conference and Workshop on Database and Expert
  Systems Applications, IEEE C. S. Press, pp. 1074-1079, 2000</journal-ref><abstract>  This paper concentrates on the representation of the legal relations that
obtain between parties once they have entered a contractual agreement and their
evolution as the agreement progresses through time. Contracts are regarded as
process and they are analysed in terms of the obligations that are active at
various points during their life span. An informal notation is introduced that
summarizes conveniently the states of an agreement as it evolves over time.
Such a representation enables us to determine what the status of an agreement
is, given an event or a sequence of events that concern the performance of
actions by the agents involved. This is useful both in the context of contract
drafting (where parties might wish to preview how their agreement might evolve)
and in the context of contract performance monitoring (where parties might with
to establish what their legal positions are once their agreement is in force).
The discussion is based on an example that illustrates some typical patterns of
contractual obligations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106011</id><created>2001-06-07</created><authors><author><keyname>Schuler</keyname><forenames>William</forenames></author></authors><title>Computational properties of environment-based disambiguation</title><categories>cs.CL cs.HC</categories><comments>8 pages, published in Proceedings of the 39th Annual Meeting of the
  ACL 2001</comments><acm-class>I.2.7; H.5.2</acm-class><abstract>  The standard pipeline approach to semantic processing, in which sentences are
morphologically and syntactically resolved to a single tree before they are
interpreted, is a poor fit for applications such as natural language
interfaces. This is because the environment information, in the form of the
objects and events in the application's run-time environment, cannot be used to
inform parsing decisions unless the input sentence is semantically analyzed,
but this does not occur until after parsing in the single-tree semantic
architecture. This paper describes the computational properties of an
alternative architecture, in which semantic analysis is performed on all
possible interpretations during parsing, in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106012</id><created>2001-06-07</created><authors><author><keyname>Angiulli</keyname><forenames>F.</forenames></author><author><keyname>Ben-Eliyahu-Zohary</keyname><forenames>R.</forenames></author><author><keyname>Ianni</keyname><forenames>G.</forenames></author><author><keyname>Palopoli</keyname><forenames>L.</forenames></author></authors><title>Computational Properties of Metaquerying Problems</title><categories>cs.DB cs.CC</categories><comments>32 pages. Partial and preliminary version appeared in Proc. of 19th
  Symposium on Principles of Database Systems, 2000, Dallas, pp. 237-244</comments><report-no>ISICNR n.7, 2001</report-no><acm-class>H.2.3;F.2.0;H.2.8</acm-class><abstract>  Metaquerying is a datamining technology by which hidden dependencies among
several database relations can be discovered. This tool has already been
successfully applied to several real-world applications. Recent papers provide
only preliminary results about the complexity of metaquerying. In this paper we
define several variants of metaquerying that encompass, as far as we know, all
variants defined in the literature. We study both the combined complexity and
the data complexity of these variants. We show that, under the combined
complexity measure, metaquerying is generally intractable (unless P=NP), lying
sometimes quite high in the complexity hierarchies (as high as NP^PP),
depending on the characteristics of the plausibility index. However, we are
able to single out some tractable and interesting metaquerying cases (whose
combined complexity is LOGCFL-complete). As for the data complexity of
metaquerying, we prove that, in general, this is in TC0, but lies within AC0 in
some simpler cases. Finally, we discuss implementation of metaqueries, by
providing algorithms to answer them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106013</id><created>2001-06-08</created><authors><author><keyname>Ismailova</keyname><forenames>Larissa</forenames></author></authors><title>The Set of Equations to Evaluate Objects</title><categories>cs.LO cs.PL cs.SC</categories><comments>5 pages</comments><acm-class>D.3.1; D.3.2</acm-class><journal-ref>Proceedings of the 3-rd International Workshop on Computer Science
  and Information Technologies CSIT'2001, Ufa, Yangantau, Ruissia</journal-ref><abstract>  The notion of an equational shell is studied to involve the objects and their
environment. Appropriate methods are studied as valid embeddings of refined
objects. The refinement process determines the linkages between the variety of
possible representations giving rise to variants of computations. The case
study is equipped with the adjusted equational systems that validate the
initial applicative framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106014</id><created>2001-06-08</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>L.T.Kuzin: Research Program</title><categories>cs.DM cs.AI cs.SE</categories><comments>10 pages</comments><acm-class>A.0; F.0; I.2</acm-class><journal-ref>Proceedings of the 1st International Workshop on Computer Science
  and Information Technologies CSIT'99. Moscow, Russia, January 18--22, 1999.
  Vol. 1, pp. 97--106</journal-ref><abstract>  Lev T. Kuzin (1928--1997) is one of the founders of modern cybernetics and
information science in Russia. He was awarded and honored the USSR State Prize
for inspiring vision into the future of technical cybernetics and his invention
and innovation of key technologies.
  The last years he interested in the computational models of geometrical and
algebraic nature and their applications in various branches of computer science
and information technologies. In the recent years the interest in computation
models based on object notion has grown tremendously stimulating an interest to
Kuzin's ideas. This year of 50th Anniversary of Cybernetics and on the occasion
of his 70th birthday on September 12, 1998 seems especially appropriate for
discussing Kuzin's Research Program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106015</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106015</id><created>2001-06-10</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Organizing Encyclopedic Knowledge based on the Web and its Application
  to Question Answering</title><categories>cs.CL</categories><comments>8 pages, Proceedings of the 39th Annual Meeting of the Association
  for Computational Linguistics (To appear)</comments><acm-class>I.2.7; H.3.3; H.3.4; H.3.5</acm-class><journal-ref>Proceedings of the 39th Annual Meeting of the Association for
  Computational Linguistics (ACL-EACL 2001), pp.196-203, July. 2001</journal-ref><abstract>  We propose a method to generate large-scale encyclopedic knowledge, which is
valuable for much NLP research, based on the Web. We first search the Web for
pages containing a term in question. Then we use linguistic patterns and HTML
structures to extract text fragments describing the term. Finally, we organize
extracted term descriptions based on word senses and domains. In addition, we
apply an automatically generated encyclopedia to a question answering system
targeting the Japanese Information-Technology Engineers Examination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106016</id><created>2001-06-10</created><authors><author><keyname>Novikov</keyname><forenames>Vjacheslav M.</forenames></author></authors><title>File mapping Rule-based DBMS and Natural Language Processing</title><categories>cs.CL cs.AI cs.DB cs.IR cs.LG cs.PL</categories><comments>17 pages, 3 figures</comments><acm-class>D.3.2; H.2.4</acm-class><abstract>  This paper describes the system of storage, extract and processing of
information structured similarly to the natural language. For recursive
inference the system uses the rules having the same representation, as the
data. The environment of storage of information is provided with the File
Mapping (SHM) mechanism of operating system. In the paper the main principles
of construction of dynamic data structure and language for record of the
inference rules are stated; the features of available implementation are
considered and the description of the application realizing semantic
information retrieval on the natural language is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106017</id><created>2001-06-10</created><authors><author><keyname>Ismailova</keyname><forenames>Larissa</forenames></author><author><keyname>Zinchenko</keyname><forenames>Konstantin</forenames></author></authors><title>An object evaluator to generate flexible applications</title><categories>cs.LO cs.PL</categories><acm-class>H.1; H.2</acm-class><journal-ref>Proceedings of the 1-st East-European Symposium on Advances in
  Databases and Information Systems, ADBIS'97, St.-Petersburg, September 2--5,
  1997, Vol. 1, pp. 141--148</journal-ref><abstract>  This paper contains a brief discussion of an object evaluator which is based
on principles of evaluations in a category. The main tool system referred as
the Application Development Environment (ADE) is used to build database
applications involving the graphical user interface (GUI). The separation of a
database access and the user interface is reached by distinguishing the
potential and actual objects. The variety of applications may be generated that
communicate with different and distinct desktop databases. The commutative
diagrams' technique allows to involve retrieval and call of the delayed
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106018</id><created>2001-06-10</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>Building the access pointers to a computation environment</title><categories>cs.LO cs.PL</categories><acm-class>D.3.1; F.1; F.4.1</acm-class><journal-ref>Proceedings of the 1st East-European Symposium on Advances in
  Databases and Information Systems, ADBIS'97, St.-Petersburg, September 2--5,
  1997, Russia. Vol. 1, pp. 117--122</journal-ref><abstract>  A common object technique equipped with the categorical and computational
styles is briefly outlined. An object is evaluated by embedding in a host
computational environment which is the domain-ranged structure. An embedded
object is accessed by the pointers generated within the host system. To assist
with an easy extract the result of the evaluation a pre-embedded object is
generated. It is observed as the decomposition into substitutional part and
access function part which are generated during the object evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106019</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106019</id><created>2001-06-10</created><updated>2008-04-22</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Hearn</keyname><forenames>Robert A.</forenames></author></authors><title>Playing Games with Algorithms: Algorithmic Combinatorial Game Theory</title><categories>cs.CC cs.DM cs.DS math.CO</categories><comments>42 pages, 18 figures. Major revision to survey, and new author. To
  appear in Games of No Chance III</comments><acm-class>F.2.2; G.2.1; F.1.3</acm-class><abstract>  Combinatorial games lead to several interesting, clean problems in algorithms
and complexity theory, many of which remain open. The purpose of this paper is
to provide an overview of the area to encourage further research. In
particular, we begin with general background in Combinatorial Game Theory,
which analyzes ideal play in perfect-information games, and Constraint Logic,
which provides a framework for showing hardness. Then we survey results about
the complexity of determining ideal play in these games, and the related
problems of solving puzzles, in terms of both polynomial-time algorithms and
computational intractability results. Our review of background and survey of
algorithmic results are by no means complete, but should serve as a useful
primer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106020</id><created>2001-06-11</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Stockinger</keyname><forenames>Heinz</forenames></author><author><keyname>Giddy</keyname><forenames>Jonathan</forenames></author><author><keyname>Abrams</keyname><forenames>David</forenames></author></authors><title>Economic Models for Management of Resources in Grid Computing</title><categories>cs.DC</categories><comments>Technical Track on Commercial Applications for High-Performance
  Computing, SPIE International Symposium on The Convergence of Information
  Technologies and Communications (ITCom 2001), August 20-24, 2001, Denver,
  Colorado, USA</comments><acm-class>C.2.4</acm-class><abstract>  The accelerated development in Grid and peer-to-peer computing has positioned
them as promising next generation computing platforms. They enable the creation
of Virtual Enterprises (VE) for sharing resources distributed across the world.
However, resource management, application development and usage models in these
environments is a complex undertaking. This is due to the geographic
distribution of resources that are owned by different organizations. The
resource owners of each of these resources have different usage or access
policies and cost models, and varying loads and availability. In order to
address complex resource management issues, we have proposed a computational
economy framework for resource allocation and for regulating supply and demand
in Grid computing environments. The framework provides mechanisms for
optimizing resource provider and consumer objective functions through trading
and brokering services. In a real world market, there exist various economic
models for setting the price for goods based on supply-and-demand and their
value to the user. They include commodity market, posted price, tenders and
auctions. In this paper, we discuss the use of these models for interaction
between Grid components in deciding resource value and the necessary
infrastructure to realize them. In addition to normal services offered by Grid
computing systems, we need an infrastructure to support interaction protocols,
allocation mechanisms, currency, secure banking, and enforcement services.
Furthermore, we demonstrate the usage of some of these economic models in
resource brokering through Nimrod/G deadline and cost-based scheduling for two
different optimization strategies on the World Wide Grid (WWG) testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106021</id><created>2001-06-11</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>Object-oriented solutions</title><categories>cs.LO cs.DB cs.PL</categories><acm-class>D.3.1; F.1; F.4.1; D.1.1; H.2.1</acm-class><journal-ref>Proceedings of the 2-nd International Workshop on Advances in
  Databases and Information Systems, ADBIS'95, Moscow, June 27 --30, 1995, Vol.
  1, pp. 235--246</journal-ref><abstract>  In this paper are briefly outlined the motivations, mathematical ideas in
use, pre-formalization and assumptions, object-as-functor construction, `soft'
types and concept constructions, case study for concepts based on variable
domains, extracting a computational background, and examples of evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106022</id><created>2001-06-11</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>One More Revolution to Make: Free Scientific Publishing</title><categories>cs.GL</categories><comments>Taken from
  http://www.acm.org/pubs/citations/journals/cacm/2001-44-5/p25-apt/ Posted
  with permission of the ACM</comments><acm-class>A.0</acm-class><journal-ref>Communications of ACM, 44(5), pp. 25-28</journal-ref><abstract>  Computer scientists are in the position to create new, free high-quality
journals. So what would it take?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106023</id><created>2001-06-11</created><authors><author><keyname>Ismailova</keyname><forenames>Larissa</forenames></author><author><keyname>Zinchenko</keyname><forenames>Konstantin</forenames></author></authors><title>Object-oriented tools for advanced applications</title><categories>cs.LO cs.DB cs.PL</categories><acm-class>D.3.1; F.1; F.4.1</acm-class><journal-ref>Proceedings of the 3-rd International Workshop on Advances in
  Databases and Information Systems, ADBIS'96, Moscow, September 10 --13, 1996,
  Vol. 2, pp. 27--31</journal-ref><abstract>  This paper contains a brief discussion of the Application Development
Environment (ADE) that is used to build database applications involving the
graphical user interface (GUI). ADE computing separates the database access and
the user interface. The variety of applications may be generated that
communicate with different and distinct desktop databases. The advanced
techniques allows to involve remote or stored procedures retrieval and call.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106024</id><created>2001-06-11</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>Objects and their computational framework</title><categories>cs.LO cs.DB cs.PL</categories><acm-class>D.3.1; F.1; F.4.1; D.1.1; H.2.1</acm-class><journal-ref>Proceedings of the 3-rd International Workshop on Advances in
  Databases and Information Systems, ADBIS'96, Moscow, September 10 --13, 1996,
  Vol. 1, pp. 66--74</journal-ref><abstract>  Most of the object notions are embedded into a logical domain, especially
when dealing with a database theory. Thus, their properties within a
computational domain are not yet studied properly. The main topic of this paper
is to analyze different concepts of the distinct computational primitive frames
to extract the useful object properties and their possible advantages. Some
important metaoperators are used to unify the approaches and to establish their
possible correspondences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106025</id><created>2001-06-11</created><authors><author><keyname>Dimopoulos</keyname><forenames>Yannis</forenames></author><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author></authors><title>Information Integration and Computational Logic</title><categories>cs.AI</categories><comments>53 Pages</comments><acm-class>H.2.m;I.2.m</acm-class><abstract>  Information Integration is a young and exciting field with enormous research
and commercial significance in the new world of the Information Society. It
stands at the crossroad of Databases and Artificial Intelligence requiring
novel techniques that bring together different methods from these fields.
Information from disparate heterogeneous sources often with no a-priori common
schema needs to be synthesized in a flexible, transparent and intelligent way
in order to respond to the demands of a query thus enabling a more informed
decision by the user or application program. The field although relatively
young has already found many practical applications particularly for
integrating information over the World Wide Web. This paper gives a brief
introduction of the field highlighting some of the main current and future
research issues and application areas. It attempts to evaluate the current and
potential role of Computational Logic in this and suggests some of the problems
where logic-based techniques could be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106026</id><created>2001-06-12</created><authors><author><keyname>Ismailova</keyname><forenames>Larissa</forenames></author><author><keyname>Zinchenko</keyname><forenames>Konstantin</forenames></author><author><keyname>Bourmistrova</keyname><forenames>Lioubouv</forenames></author></authors><title>Event Driven Computations for Relational Query Language</title><categories>cs.LO cs.DB cs.PL</categories><acm-class>D.3.1; F.1; F.4.1</acm-class><journal-ref>Proceedings of the 1-st International Workshop on Computer Science
  and Information Technologies CSIT'99, Moscow, Russia, 1999. Vol.1, pp. 43--52</journal-ref><abstract>  This paper deals with an extended model of computations which uses the
parameterized families of entities for data objects and reflects a preliminary
outline of this problem. Some topics are selected out, briefly analyzed and
arranged to cover a general problem. The authors intended more to discuss the
particular topics, their interconnection and computational meaning as a panel
proposal, so that this paper is not yet to be evaluated as a closed journal
paper. To save space all the technical and implementation features are left for
the future paper.
  Data object is a schematic entity and modelled by the partial function. A
notion of type is extended by the variable domains which depend on events and
types. A variable domain is built from the potential and schematic individuals
and generates the valid families of types depending on a sequence of events.
Each valid type consists of the actual individuals which are actual relatively
the event or script. In case when a type depends on the script then
corresponding view for data objects is attached, otherwise a snapshot is
generated. The type thus determined gives an upper range for typed variables so
that the local ranges are event driven resulting is the families of actual
individuals. An expressive power of the query language is extended using the
extensional and intentional relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106027</id><created>2001-06-12</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>Event Driven Objects</title><categories>cs.LO cs.DB cs.SE</categories><acm-class>D.3.1; F.1; F.4.1; D.1.1; H.2.1</acm-class><journal-ref>Proceedings of the 1-st International Workshop on Computer Science
  and Information Technologies CSIT'99, Moscow, Russia, 1999. Vol.1, pp. 88--97</journal-ref><abstract>  A formal consideration in this paper is given for the essential notations to
characterize the object that is distinguished in a problem domain. The distinct
object is represented by another idealized object, which is a schematic
element. When the existence of an element is significant, then a class of these
partial elements is dropped down into actual, potential and virtual objects.
The potential objects are gathered into the variable domains which are the
extended ranges for unbound variables. The families of actual objects are shown
to be parameterized with the types and events. The transitions between events
are shown to be driven by the scripts. A computational framework arises which
is described by the commutative diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106028</id><created>2001-06-12</created><authors><author><keyname>Rasmusson</keyname><forenames>Lars</forenames></author></authors><title>Pricing Virtual Paths with Quality-of-Service Guarantees as Bundle
  Derivatives</title><categories>cs.NI cs.CE</categories><comments>22 pages (15 in main tex and 7 appendix), 5 postscript figures</comments><acm-class>C.2.3; C.4</acm-class><abstract>  We describe a model of a communication network that allows us to price
complex network services as financial derivative contracts based on the spot
price of the capacity in individual routers. We prove a theorem of a Girsanov
transform that is useful for pricing linear derivatives on underlying assets,
which can be used to price many complex network services, and it is used to
price an option that gives access to one of several virtual channels between
two network nodes, during a specified future time interval. We give the
continuous time hedging strategy, for which the option price is independent of
the service providers attitude towards risk. The option price contains the
density function of a sum of lognormal variables, which has to be evaluated
numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106029</id><created>2001-06-12</created><authors><author><keyname>Ismailova</keyname><forenames>Larissa</forenames></author><author><keyname>Kosikov</keyname><forenames>Sergey</forenames></author><author><keyname>Zinchenko</keyname><forenames>Konstantin</forenames></author><author><keyname>Mikhaylov</keyname><forenames>Alexey</forenames></author><author><keyname>Bourmistrova</keyname><forenames>Lioubouv</forenames></author><author><keyname>Berezovskaya</keyname><forenames>Anastassiya</forenames></author></authors><title>Building Views with Description Logics in ADE: Application Development
  Environment</title><categories>cs.LO cs.DB cs.DS</categories><acm-class>D.3.1; F.1; F.4.1; D.1.1; H.2.1</acm-class><journal-ref>Proceedings of the 2-nd International Workshop on Computer Science
  and Information Technologies CSIT'2000, Ufa, Yangantau, Russia, 2000. Vol.1,
  pp. 153--161</journal-ref><abstract>  Any of views is formally defined within description logics that were
established as a family of logics for modeling complex hereditary structures
and have a suitable expressive power. This paper considers the Application
Development Environment (ADE) over generalized variable concepts that are used
to build database applications involving the supporting views. The front-end
user interacts the database via separate ADE access mechanism intermediated by
view support. The variety of applications may be generated that communicate
with different and distinct desktop databases in a data warehouse. The advanced
techniques allows to involve remote or stored procedures retrieval and call.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106030</id><created>2001-06-12</created><authors><author><keyname>Wolfengagen</keyname><forenames>Viacheslav</forenames></author></authors><title>Logic, Individuals and Concepts</title><categories>cs.LO cs.DB cs.DM cs.SE</categories><acm-class>D.3.1; F.1; F.4.1; D.1.1; H.2.1</acm-class><journal-ref>Proceedings of the 2-nd International Workshop on Computer Science
  and Information Technologies CSIT'2000, Ufa, Yangantau, Russia, 2000. Vol.1,
  pp. 141--145</journal-ref><abstract>  This extended abstract gives a brief outline of the connections between the
descriptions and variable concepts. Thus, the notion of a concept is extended
to include both the syntax and semantics features. The evaluation map in use is
parameterized by a kind of computational environment, the index, giving rise to
indexed concepts. The concepts are inhabited into language by the descriptions
from the higher order logic. In general the idea of object-as-functor should
assist the designer to outline a programming tool in conceptual shell style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106031</id><created>2001-06-13</created><authors><author><keyname>Tobies</keyname><forenames>Stephan</forenames></author></authors><title>Complexity Results and Practical Algorithms for Logics in Knowledge
  Representation</title><categories>cs.LO cs.AI</categories><comments>Ph.D. Thesis</comments><acm-class>F.4.1;I.2.3; I.2.4; F.2.2</acm-class><abstract>  Description Logics (DLs) are used in knowledge-based systems to represent and
reason about terminological knowledge of the application domain in a
semantically well-defined manner. In this thesis, we establish a number of
novel complexity results and give practical algorithms for expressive DLs that
provide different forms of counting quantifiers.
  We show that, in many cases, adding local counting in the form of qualifying
number restrictions to DLs does not increase the complexity of the inference
problems, even if binary coding of numbers in the input is assumed. On the
other hand, we show that adding different forms of global counting restrictions
to a logic may increase the complexity of the inference problems dramatically.
  We provide exact complexity results and a practical, tableau based algorithm
for the DL SHIQ, which forms the basis of the highly optimized DL system iFaCT.
  Finally, we describe a tableau algorithm for the clique guarded fragment
(CGF), which we hope will serve as the basis for an efficient implementation of
a CGF reasoner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106032</id><created>2001-06-13</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Hinged Kite Mirror Dissection</title><categories>cs.CG math.MG</categories><comments>8 pages, 7 figures</comments><acm-class>F.2.2</acm-class><abstract>  Any two polygons of equal area can be partitioned into congruent sets of
polygonal pieces, and in many cases one can connect the pieces by flexible
hinges while still allowing the connected set to form both polygons. However it
is open whether such a hinged dissection always exists. We solve a special case
of this problem, by showing that any asymmetric polygon always has a hinged
dissection to its mirror image. Our dissection forms a chain of kite-shaped
pieces, found by a circle-packing algorithm for quadrilateral mesh generation.
A hinged mirror dissection of a polygon with n sides can be formed with O(n)
kites in O(n log n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106033</id><created>2001-06-13</created><updated>2005-12-08</updated><authors><author><keyname>Yee</keyname><forenames>Kenton K.</forenames></author></authors><title>location.location.location: Internet Addresses as Evolving Property</title><categories>cs.CY cs.HC cs.IR</categories><comments>Related articles and background materials may be obtained from
  http://www.columbia.edu/~kky2001/pubs.html</comments><acm-class>C.0; J.4; K.0; K.1; K.4; K.4.4; K.5; K.5.1; K.7.4</acm-class><journal-ref>Southern California Interdisciplinary Law Journal (1998) Volume 6,
  No. 2, pp. 201-243</journal-ref><abstract>  I describe recent developments in the rules governing registration and
ownership of Internet and World Wide Web addresses or &quot;domain names.&quot; I
consider the idea that &quot;virtual&quot; properties like domain names are more similar
to real estate than to trademarks. Therefore, it would be economically
efficient to grant domain name owners stronger rights than those of trademarks
and copyright holders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106034</id><created>2001-06-14</created><updated>2003-12-10</updated><authors><author><keyname>Biskup</keyname><forenames>Joachim</forenames></author><author><keyname>Paredaens</keyname><forenames>Jan</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Solving equations in the relational algebra</title><categories>cs.LO cs.DB</categories><comments>Minor revision, accepted for publication in SIAM Journal on Computing</comments><acm-class>F.4.2; H.2.3</acm-class><abstract>  Enumerating all solutions of a relational algebra equation is a natural and
powerful operation which, when added as a query language primitive to the
nested relational algebra, yields a query language for nested relational
databases, equivalent to the well-known powerset algebra. We study
\emph{sparse} equations, which are equations with at most polynomially many
solutions. We look at their complexity, and compare their expressive power with
that of similar notions in the powerset algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106035</id><created>2001-06-14</created><authors><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Waller</keyname><forenames>Emmanuel</forenames></author></authors><title>Polymorphic type inference for the relational algebra</title><categories>cs.LO cs.DB</categories><acm-class>D.3.3, H.2.3</acm-class><abstract>  We give a polymorphic account of the relational algebra. We introduce a
formalism of ``type formulas'' specifically tuned for relational algebra
expressions, and present an algorithm that computes the ``principal'' type for
a given expression. The principal type of an expression is a formula that
specifies, in a clear and concise manner, all assignments of types (sets of
attributes) to relation names, under which a given relational algebra
expression is well-typed, as well as the output type that expression will have
under each of these assignments. Topics discussed include complexity and
polymorphic expressive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106036</id><created>2001-06-15</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Convergence and Error Bounds for Universal Prediction of Nonbinary
  Sequences</title><categories>cs.LG cs.AI cs.CC math.PR</categories><comments>11 LaTeX pages</comments><report-no>IDSIA-07-01</report-no><acm-class>F.2.3</acm-class><journal-ref>Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th
  Eurpean Conf. on Machine Learning (ECML) (2001) 239-250</journal-ref><abstract>  Solomonoff's uncomputable universal prediction scheme $\xi$ allows to predict
the next symbol $x_k$ of a sequence $x_1...x_{k-1}$ for any Turing computable,
but otherwise unknown, probabilistic environment $\mu$. This scheme will be
generalized to arbitrary environmental classes, which, among others, allows the
construction of computable universal prediction schemes $\xi$. Convergence of
$\xi$ to $\mu$ in a conditional mean squared sense and with $\mu$ probability 1
is proven. It is shown that the average number of prediction errors made by the
universal $\xi$ scheme rapidly converges to those made by the best possible
informed $\mu$ scheme. The schemes, theorems and proofs are given for general
finite alphabet, which results in additional complications as compared to the
binary case. Several extensions of the presented theory and results are
outlined. They include general loss functions and bounds, games of chance,
infinite alphabet, partial and delayed prediction, classification, and more
active systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106037</id><created>2001-06-15</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Hempel</keyname><forenames>Harald</forenames></author></authors><title>Using the No-Search Easy-Hard Technique for Downward Collapse</title><categories>cs.CC</categories><comments>22 pages. Also appears as URCS technical report</comments><acm-class>F.1.3; F.1.2</acm-class><abstract>  The top part of the preceding figure [figure appears in actual paper] shows
some classes from the (truth-table) bounded-query and boolean hierarchies. It
is well-known that if either of these hierarchies collapses at a given level,
then all higher levels of that hierarchy collapse to that same level. This is a
standard ``upward translation of equality'' that has been known for over a
decade. The issue of whether these hierarchies can translate equality {\em
downwards\/} has proven vastly more challenging. In particular, with regard to
the figure above, consider the following claim:
  $$P_{m-tt}^{\Sigma_k^p} = P_{m+1-tt}^{\Sigma_k^p} \implies
  DIFF_m(\Sigma_k^p) coDIFF_m(\Sigma_k^p) = BH(\Sigma_k^p). (*) $$
  This claim, if true, says that equality translates downwards between levels
of the bounded-query hierarchy and the boolean hierarchy levels that (before
the fact) are immediately below them.
  Until recently, it was not known whether (*) {\em ever\/} held, except for
the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and
Hempel \cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all
$m$, for $k &gt; 2$. Buhrman and Fortnow~\cite{buh-for:j:two-queries} then showed
that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that
for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle
relative to which ``for $k=1$, (*) holds for all $m$'' fails
\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be
strengthened to $k=1$ by any relativizable proof technique. The new downward
translation we obtain also tightens the collapse in the polynomial hierarchy
implied by a collapse in the bounded-query hierarchy of the second level of the
polynomial hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106038</id><created>2001-06-15</created><authors><author><keyname>Mackie</keyname><forenames>David M.</forenames></author></authors><title>Simple and Effective Distributed Computing with a Scheduling Service</title><categories>cs.DC</categories><comments>5 pgs., 5 figs., a step-by-step &quot;how-to&quot; report plus a case study,
  for networked PCs with Windows</comments><acm-class>C.m; D.m</acm-class><abstract>  High-throughput computing projects require the solution of large numbers of
problems. In many cases, these problems can be solved on desktop PCs, or can be
broken down into independent &quot;PC-solvable&quot; sub-problems. In such cases, the
projects are high-performance computing projects, but only because of the sheer
number of the needed calculations. We briefly describe our efforts to increase
the throughput of one such project. We then explain how to easily set up a
distributed computing facility composed of standard networked PCs running
Windows 95, 98, 2000, or NT. The facility requires no special software or
hardware, involves little or no re-coding of application software, and operates
almost invisibly to the owners of the PCs. Depending on the number and quality
of PCs recruited, performance can rival that of supercomputers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106039</id><created>2001-06-17</created><authors><author><keyname>Ando</keyname><forenames>Rie Kubota</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Iterative Residual Rescaling: An Analysis and Generalization of LSI</title><categories>cs.CL cs.IR</categories><comments>To appear in the proceedings of SIGIR 2001. 11 pages</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>Proceedings of the 24th SIGIR, pp. 154--162, 2001.</journal-ref><abstract>  We consider the problem of creating document representations in which
inter-document similarity measurements correspond to semantic similarity. We
first present a novel subspace-based framework for formalizing this task. Using
this framework, we derive a new analysis of Latent Semantic Indexing (LSI),
showing a precise relationship between its performance and the uniformity of
the underlying distribution of documents over topics. This analysis helps
explain the improvements gained by Ando's (2000) Iterative Residual Rescaling
(IRR) algorithm: IRR can compensate for distributional non-uniformity. A
further benefit of our framework is that it provides a well-motivated,
effective method for automatically determining the rescaling factor IRR depends
on, leading to further improvements. A series of experiments over various
settings and with several evaluation metrics validates our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106040</id><created>2001-06-19</created><authors><author><keyname>Sakkis</keyname><forenames>G.</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>I.</forenames></author><author><keyname>Paliouras</keyname><forenames>G.</forenames></author><author><keyname>Karkaletsis</keyname><forenames>V.</forenames></author><author><keyname>Spyropoulos</keyname><forenames>C. D.</forenames></author><author><keyname>Stamatopoulos</keyname><forenames>P.</forenames></author></authors><title>Stacking classifiers for anti-spam filtering of e-mail</title><categories>cs.CL cs.AI</categories><acm-class>H.4.3; I.2.6; I.2.7; I.5.4; K.4.1</acm-class><journal-ref>Proceedings of &quot;Empirical Methods in Natural Language Processing&quot;
  (EMNLP 2001), L. Lee and D. Harman (Eds.), pp. 44-50, Carnegie Mellon
  University, Pittsburgh, PA, 2001</journal-ref><abstract>  We evaluate empirically a scheme for combining classifiers, known as stacked
generalization, in the context of anti-spam filtering, a novel cost-sensitive
application of text categorization. Unsolicited commercial e-mail, or &quot;spam&quot;,
floods mailboxes, causing frustration, wasting bandwidth, and exposing minors
to unsuitable content. Using a public corpus, we show that stacking can improve
the efficiency of automatically induced anti-spam filters, and that such
filters can be used in real-life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106041</id><created>2001-06-19</created><authors><author><keyname>Grosse</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author><author><keyname>Wechsung</keyname><forenames>Gerd</forenames></author></authors><title>Computing Complete Graph Isomorphisms and Hamiltonian Cycles from
  Partial Ones</title><categories>cs.CC</categories><comments>12 pages, appears as part of a ICTCS 2001 paper of the same authors</comments><acm-class>F.1.3; F.2.2</acm-class><abstract>  We prove that computing a single pair of vertices that are mapped onto each
other by an isomorphism $\phi$ between two isomorphic graphs is as hard as
computing $\phi$ itself. This result optimally improves upon a result of
G\'{a}l et al. We establish a similar, albeit slightly weaker, result about
computing complete Hamiltonian cycles of a graph from partial Hamiltonian
cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106042</id><created>2001-06-19</created><authors><author><keyname>McCune</keyname><forenames>William</forenames></author></authors><title>MACE 2.0 Reference Manual and Guide</title><categories>cs.LO cs.SC</categories><comments>10 pages</comments><report-no>ANL/MCS-TM-249</report-no><acm-class>I.2.3; I.2.8</acm-class><abstract>  MACE is a program that searches for finite models of first-order statements.
The statement to be modeled is first translated to clauses, then to relational
clauses; finally for the given domain size, the ground instances are
constructed. A Davis-Putnam-Loveland-Logeman procedure decides the
propositional problem, and any models found are translated to first-order
models. MACE is a useful complement to the theorem prover Otter, with Otter
searching for proofs and MACE looking for countermodels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106043</id><created>2001-06-20</created><authors><author><keyname>Krymolowski</keyname><forenames>Yuval</forenames></author></authors><title>Using the Distribution of Performance for Studying Statistical NLP
  Systems and Corpora</title><categories>cs.CL</categories><comments>To be presented in ACL/EACL Workshop on Evaluation for Language and
  Dialogue Systems</comments><acm-class>G.3; I.2.6; I.2.7</acm-class><abstract>  Statistical NLP systems are frequently evaluated and compared on the basis of
their performances on a single split of training and test data. Results
obtained using a single split are, however, subject to sampling noise. In this
paper we argue in favour of reporting a distribution of performance figures,
obtained by resampling the training data, rather than a single number. The
additional information from distributions can be used to make statistically
quantified statements about differences across parameter settings, systems, and
corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106044</id><created>2001-06-20</created><authors><author><keyname>Even-Zohar</keyname><forenames>Yair</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>A Sequential Model for Multi-Class Classification</title><categories>cs.AI cs.CL cs.LG</categories><acm-class>I.2.6;I.2.67</acm-class><abstract>  Many classification problems require decisions among a large number of
competing classes. These tasks, however, are not handled well by general
purpose learning methods and are usually addressed in an ad-hoc fashion. We
suggest a general approach -- a sequential learning model that utilizes
classifiers to sequentially restrict the number of competing classes while
maintaining, with high probability, the presence of the true outcome in the
candidates set. Some theoretical and computational properties of the model are
discussed and we argue that these are important in NLP-like domains. The
advantages of the model are illustrated in an experiment in part-of-speech
tagging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106045</id><created>2001-06-21</created><updated>2006-02-07</updated><authors><author><keyname>Grosse</keyname><forenames>Andre</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author><author><keyname>Wechsung</keyname><forenames>Gerd</forenames></author></authors><title>A Note on the Complexity of Computing the Smallest Four-Coloring of
  Planar Graphs</title><categories>cs.CC</categories><comments>9 pages, appears in part in an ICTCS 2001 paper by the same authors,
  minor revision of the above</comments><acm-class>F.1.3; F.2.2</acm-class><abstract>  We show that computing the lexicographically first four-coloring for planar
graphs is P^{NP}-hard. This result optimally improves upon a result of Khuller
and Vazirani who prove this problem to be NP-hard, and conclude that it is not
self-reducible in the sense of Schnorr, assuming P \neq NP. We discuss this
application to non-self-reducibility and provide a general related result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106046</id><created>2001-06-21</created><authors><author><keyname>Geerts</keyname><forenames>Floris</forenames></author></authors><title>Expressing the cone radius in the relational calculus with real
  polynomial constraints</title><categories>cs.DB cs.LO</categories><comments>9 pages</comments><acm-class>H.2.3; H.2.8</acm-class><abstract>  We show that there is a query expressible in first-order logic over the reals
that returns, on any given semi-algebraic set A, for every point a radius
around which A is conical. We obtain this result by combining famous results
from calculus and real algebraic geometry, notably Sard's theorem and Thom's
first isotopy lemma, with recent algorithmic results by Rannou.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106047</id><created>2001-06-21</created><authors><author><keyname>Ratnaparkhi</keyname><forenames>Adwait</forenames></author></authors><title>Modeling informational novelty in a conversational system with a hybrid
  statistical and grammar-based approach to natural language generation</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the NAACL Workshop on Adaptation in Dialogue
  Systems, June 4, 2001, Pittsburgh, PA, USA</journal-ref><abstract>  We present a hybrid statistical and grammar-based system for surface natural
language generation (NLG) that uses grammar rules, conditions on using those
grammar rules, and corpus statistics to determine the word order. We also
describe how this surface NLG module is implemented in a prototype
conversational system, and how it attempts to model informational novelty by
varying the word order. Using a combination of rules and statistical
information, the conversational system expresses the novel information
differently than the given information, based on the run-time dialog state. We
also discuss our plans for evaluating the generation strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106048</id><created>2001-06-25</created><authors><author><keyname>Naidenko</keyname><forenames>V. G.</forenames></author><author><keyname>Orlovich</keyname><forenames>Yu. L.</forenames></author></authors><title>On some optimization problems for star-free graphs</title><categories>cs.CC cs.DM</categories><comments>6 pages, in Russian</comments><acm-class>G.1.2; G.2.2</acm-class><abstract>  It is shown that in star-free graphs the maximum independent set problem, the
minimum dominating set problem and the minimum independent dominating set
problem are approximable up to constant factor by any maximal independent set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106049</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106049</id><created>2001-06-25</created><updated>2001-12-06</updated><authors><author><keyname>Naidenko</keyname><forenames>V. G.</forenames></author></authors><title>Recursively Undecidable Properties of NP</title><categories>cs.CC</categories><comments>3 pages</comments><acm-class>F.0; F.1.3</acm-class><abstract>  We show that there cannot be any algorithm that for a given nondeterministic
polynomial-time Turing machine determinates whether or not the language
recognized by this machine belongs to P
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106050</id><created>2001-06-25</created><updated>2002-07-22</updated><authors><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author><author><keyname>Ruggieri</keyname><forenames>Salvatore</forenames></author><author><keyname>Smaus</keyname><forenames>Jan-Georg</forenames></author></authors><title>Classes of Terminating Logic Programs</title><categories>cs.LO cs.PL</categories><comments>50 pages. The following mistake was corrected: In figure 5, the first
  clause for insert was insert([],X,[X])</comments><acm-class>D.1.6; D.2.4; F.3.1</acm-class><journal-ref>Theory and Practice of Logic Programming, 2(3), 369-418, 2002</journal-ref><abstract>  Termination of logic programs depends critically on the selection rule, i.e.
the rule that determines which atom is selected in each resolution step. In
this article, we classify programs (and queries) according to the selection
rules for which they terminate. This is a survey and unified view on different
approaches in the literature. For each class, we present a sufficient, for most
classes even necessary, criterion for determining that a program is in that
class. We study six classes: a program strongly terminates if it terminates for
all selection rules; a program input terminates if it terminates for selection
rules which only select atoms that are sufficiently instantiated in their input
positions, so that these arguments do not get instantiated any further by the
unification; a program local delay terminates if it terminates for local
selection rules which only select atoms that are bounded w.r.t. an appropriate
level mapping; a program left-terminates if it terminates for the usual
left-to-right selection rule; a program exists-terminates if there exists a
selection rule for which it terminates; finally, a program has bounded
nondeterminism if it only has finitely many refutations. We propose a
semantics-preserving transformation from programs with bounded nondeterminism
into strongly terminating programs. Moreover, by unifying different formalisms
and making appropriate assumptions, we are able to establish a formal hierarchy
between the different classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106051</id><created>2001-06-25</created><authors><author><keyname>Gertz</keyname><forenames>E. Michael</forenames></author><author><keyname>Gill</keyname><forenames>Philip E.</forenames></author><author><keyname>Muetherig</keyname><forenames>Julia</forenames></author></authors><title>Users Guide for SnadiOpt: A Package Adding Automatic Differentiation to
  Snopt</title><categories>cs.MS</categories><comments>pages i-iv, 1-23</comments><report-no>ANL/MCS-TM-245</report-no><acm-class>G.1.6; G.1.4</acm-class><abstract>  SnadiOpt is a package that supports the use of the automatic differentiation
package ADIFOR with the optimization package Snopt. Snopt is a general-purpose
system for solving optimization problems with many variables and constraints.
It minimizes a linear or nonlinear function subject to bounds on the variables
and sparse linear or nonlinear constraints. It is suitable for large-scale
linear and quadratic programming and for linearly constrained optimization, as
well as for general nonlinear programs. The method used by Snopt requires the
first derivatives of the objective and constraint functions to be available.
The SnadiOpt package allows users to avoid the time-consuming and error-prone
process of evaluating and coding these derivatives. Given Fortran code for
evaluating only the values of the objective and constraints, SnadiOpt
automatically generates the code for evaluating the derivatives and builds the
relevant Snopt input files and sparse data structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106052</id><created>2001-06-26</created><authors><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>Acceptability with general orderings</title><categories>cs.PL cs.LO</categories><comments>To appear in &quot;Computational Logic: From Logic Programming into the
  Future&quot;</comments><acm-class>D.1.6; D.2.4</acm-class><abstract>  We present a new approach to termination analysis of logic programs. The
essence of the approach is that we make use of general orderings (instead of
level mappings), like it is done in transformational approaches to logic
program termination analysis, but we apply these orderings directly to the
logic program and not to the term-rewrite system obtained through some
transformation. We define some variants of acceptability, based on general
orderings, and show how they are equivalent to LD-termination. We develop a
demand driven, constraint-based approach to verify these
acceptability-variants.
  The advantage of the approach over standard acceptability is that in some
cases, where complex level mappings are needed, fairly simple orderings may be
easily generated. The advantage over transformational approaches is that it
avoids the transformation step all together.
  {\bf Keywords:} termination analysis, acceptability, orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106053</id><created>2001-06-26</created><authors><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author></authors><title>Inference of termination conditions for numerical loops</title><categories>cs.PL cs.LO</categories><comments>Presented at WST2001</comments><report-no>CW 308</report-no><acm-class>D.1.6; D.2.4</acm-class><abstract>  We present a new approach to termination analysis of numerical computations
in logic programs. Traditional approaches fail to analyse them due to non
well-foundedness of the integers. We present a technique that allows to
overcome these difficulties. Our approach is based on transforming a program in
way that allows integrating and extending techniques originally developed for
analysis of numerical computations in the framework of query-mapping pairs with
the well-known framework of acceptability. Such an integration not only
contributes to the understanding of termination behaviour of numerical
computations, but also allows to perform a correct analysis of such
computations automatically, thus, extending previous work on a
constraints-based approach to termination. In the last section of the paper we
discuss possible extensions of the technique, including incorporating general
term orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106054</id><created>2001-06-26</created><authors><author><keyname>Soshnikov</keyname><forenames>Dmitri</forenames></author></authors><title>Software Toolkit for Building Embedded and Distributed Knowledge-based
  Systems</title><categories>cs.AI cs.DC cs.MA</categories><comments>9 pages, 3 figures; check http://www.soshnikov.com for details</comments><acm-class>I.2; D.2.12</acm-class><journal-ref>Soshnikov D. Software Toolkit for Building Distributed and
  Embedded Knowledge-Based Systems. In Proceedings of the 2nd International
  Workshop on Computer Science and Information Technologies, Ufa, USATU
  Publishers, 2000. pp. 103--111</journal-ref><abstract>  The paper discusses the basic principles and the architecture of the software
toolkit for constructing knowledge-based systems which can be used
cooperatively over computer networks and also embedded into larger software
systems in different ways. Presented architecture is based on frame knowledge
representation and production rules, which also allows to interface high-level
programming languages and relational databases by exposing corresponding
classes or database tables as frames. Frames located on the remote computers
can also be transparently accessed and used in inference, and the dynamic
knowledge for specific frames can also be transferred over the network. The
issues of implementation of such a system are addressed, which use Java
programming language, CORBA and XML for external knowledge representation.
Finally, some applications of the toolkit are considered, including e-business
approach to knowledge sharing, intelligent web behaviours, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106055</id><created>2001-06-28</created><authors><author><keyname>Gopalan</keyname><forenames>Raj P.</forenames></author><author><keyname>Nuruddin</keyname><forenames>Tariq</forenames></author><author><keyname>Sucahyo</keyname><forenames>Yudho Giri</forenames></author></authors><title>A Seamless Integration of Association Rule Mining with Database Systems</title><categories>cs.DB</categories><comments>15 pages</comments><acm-class>H.2.8</acm-class><abstract>  The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106056</id><created>2001-06-28</created><updated>2002-03-15</updated><authors><author><keyname>Tromp</keyname><forenames>John</forenames><affiliation>CWI and BioInformatics Solutions</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Randomized Two-Process Wait-Free Test-and-Set</title><categories>cs.DC</categories><comments>9 pages, 4 figures, LaTeX source; Submitted</comments><acm-class>C.2; F.2.2</acm-class><abstract>  We present the first explicit, and currently simplest, randomized algorithm
for 2-process wait-free test-and-set. It is implemented with two 4-valued
single writer single reader atomic variables. A test-and-set takes at most 11
expected elementary steps, while a reset takes exactly 1 elementary step. Based
on a finite-state analysis, the proofs of correctness and expected length are
compressed into one table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106057</identifier>
 <datestamp>2010-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106057</id><created>2001-06-28</created><authors><author><keyname>Warner</keyname><forenames>Simeon</forenames><affiliation>LANL</affiliation></author></authors><title>Exposing and harvesting metadata using the OAI metadata harvesting
  protocol: A tutorial</title><categories>cs.DL</categories><comments>13 pages, 1 figure. Example programs included (download source).
  HEPLW version (HTML) available online at
  http://library.cern.ch/HEPLW/4/papers/3/</comments><acm-class>H.3.7</acm-class><journal-ref>High Energy Physics Libraries Webzine, Issue 4, June 2001</journal-ref><abstract>  In this article I outline the ideas behind the Open Archives Initiative
metadata harvesting protocol (OAIMH), and attempt to clarify some common
misconceptions. I then consider how the OAIMH protocol can be used to expose
and harvest metadata. Perl code examples are given as practical illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106058</id><created>2001-06-28</created><authors><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Giuli</keyname><forenames>T. J.</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Enabling the Long-Term Archival of Signed Documents through Time
  Stamping</title><categories>cs.DC cs.CR</categories><comments>25 pages, 10 figures, unpublished</comments><acm-class>C.2.4; K.6.5; H.3.4</acm-class><abstract>  In this paper we describe how to build a trusted reliable distributed service
across administrative domains in a peer-to-peer network. The application we use
to motivate our work is a public key time stamping service called Prokopius.
The service provides a secure, verifiable but distributable stable archive that
maintains time stamped snapshots of public keys over time. This in turn allows
clients to verify time stamped documents or certificates that rely on formerly
trusted public keys that are no longer in service or where the signer no longer
exists. We find that such a service can time stamp the snapshots of public keys
in a network of 148 nodes at the granularity of a couple of days, even in the
worst case where an adversary causes the maximal amount of damage allowable
within our fault model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0106059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0106059</id><created>2001-06-29</created><authors><author><keyname>Christiansen</keyname><forenames>Henning</forenames></author></authors><title>CHR as grammar formalism. A first report</title><categories>cs.PL cs.CL</categories><comments>12 pages. Presented at ERCIM Workshop on Constraints, Prague, Czech
  Republic, June 18-20, 2001</comments><acm-class>I.2.7;D.3.2;F.4.1;F.4.2</acm-class><journal-ref>Proc. of ERCIM Workshop on Constraints, Prague, Czech Republic,
  June 18-20, 2001</journal-ref><abstract>  Grammars written as Constraint Handling Rules (CHR) can be executed as
efficient and robust bottom-up parsers that provide a straightforward,
non-backtracking treatment of ambiguity. Abduction with integrity constraints
as well as other dynamic hypothesis generation techniques fit naturally into
such grammars and are exemplified for anaphora resolution, coordination and
text interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107001</id><created>2001-07-02</created><updated>2001-11-06</updated><authors><author><keyname>Field</keyname><forenames>Tony</forenames></author><author><keyname>Harder</keyname><forenames>Uli</forenames></author><author><keyname>Harrison</keyname><forenames>Peter</forenames></author></authors><title>Analysis of Network Traffic in Switched Ethernet Systems</title><categories>cs.PF cs.NI</categories><comments>12 pages, 14 figures, using infocom.cls (included) and graphicx (not
  included); all power spectrum plots replaced with new ones; minor typos in
  figure captions corrected</comments><acm-class>C.2.5; C.4; D.2.8; G.3</acm-class><abstract>  A 100 Mbps Ethernet link between a college campus and the outside world was
monitored with a dedicated PC and the measured data analysed for its
statistical properties. Similar measurements were taken at an internal node of
the network. The networks in both cases are a full-duplex switched Ethernet.
Inter-event interval histograms and power spectra of the throughput aggregated
for 10ms bins were used to analyse the measured traffic. For most investigated
cases both methods reveal that the traffic behaves according to a power law.
The results will be used in later studies to parameterise models for network
traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107002</id><created>2001-07-02</created><authors><author><keyname>Granvilliers</keyname><forenames>Laurent</forenames></author><author><keyname>Monfroy</keyname><forenames>Eric</forenames></author></authors><title>Enhancing Constraint Propagation with Composition Operators</title><categories>cs.AI</categories><comments>14 pages</comments><acm-class>F.4.1; D.3.3</acm-class><abstract>  Constraint propagation is a general algorithmic approach for pruning the
search space of a CSP. In a uniform way, K. R. Apt has defined a computation as
an iteration of reduction functions over a domain. He has also demonstrated the
need for integrating static properties of reduction functions (commutativity
and semi-commutativity) to design specialized algorithms such as AC3 and DAC.
We introduce here a set of operators for modeling compositions of reduction
functions. Two of the major goals are to tackle parallel computations, and
dynamic behaviours (such as slow convergence).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107003</id><created>2001-07-02</created><updated>2001-07-11</updated><authors><author><keyname>Kilian</keyname><forenames>Joe</forenames></author><author><keyname>Petrank</keyname><forenames>Erez</forenames></author><author><keyname>Rackoff</keyname><forenames>Charles</forenames></author></authors><title>Lower Bounds for Zero-knowledge on the Internet</title><categories>cs.CR</categories><acm-class>D.4.6</acm-class><abstract>  We consider zero knowledge interactive proofs in a richer, more realistic
communication environment. In this setting, one may simultaneously engage in
many interactive proofs, and these proofs may take place in an asynchronous
fashion. It is known that zero-knowledge is not necessarily preserved in such
an environment; we show that for a large class of protocols, it cannot be
preserved. Any 4 round (computational) zero-knowledge interactive proof (or
argument) for a non-trivial language L is not black-box simulatable in the
asynchronous setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107004</id><created>2001-07-03</created><authors><author><keyname>Kilian</keyname><forenames>Joe</forenames></author><author><keyname>Petrank</keyname><forenames>Erez</forenames></author><author><keyname>Richardson</keyname><forenames>Ransom</forenames></author></authors><title>On Concurrent and Resettable Zero-Knowledge Proofs for NP</title><categories>cs.CR</categories><comments>This paper is a join of two works. The preliminary versions of these
  works appeared in the Proceeedings of Advances in Cryptology - EUROCRYPT
  '99}, May 1999, Lecture Notes in Computer Science Vol. 1592 Springer 1999,
  pp. 415-431, and in the Proceedings of the thirty third annual ACM Symposium
  on Theory of Computing, ACM Press, 2001</comments><acm-class>D.4.6</acm-class><abstract>  A proof is concurrent zero-knowledge if it remains zero-knowledge when many
copies of the proof are run in an asynchronous environment, such as the
Internet. It is known that zero-knowledge is not necessarily preserved in such
an environment. Designing concurrent zero-knowledge proofs is a fundamental
issue in the study of zero-knowledge since known zero-knowledge protocols
cannot be run in a realistic modern computing environment. In this paper we
present a concurrent zero-knowledge proof systems for all languages in NP.
Currently, the proof system we present is the only known proof system that
retains the zero-knowledge property when copies of the proof are allowed to run
in an asynchronous environment. Our proof system has $\tilde{O}(\log^2 k)$
rounds (for a security parameter $k$), which is almost optimal, as it is shown
by Canetti Kilian Petrank and Rosen that black-box concurrent zero-knowledge
requires $\tilde{\Omega}(\log k)$ rounds.
  Canetti, Goldreich, Goldwasser and Micali introduced the notion of {\em
resettable} zero-knowledge, and modified an earlier version of our proof system
to obtain the first resettable zero-knowledge proof system. This protocol
requires $k^{\theta(1)}$ rounds. We note that their technique also applies to
our current proof system, yielding a resettable zero-knowledge proof for NP
with $\tilde{O}(\log^2 k)$ rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107005</id><created>2001-07-03</created><authors><author><keyname>Fernandez-Amoros</keyname><forenames>David</forenames></author><author><keyname>Gonzalo</keyname><forenames>Julio</forenames></author><author><keyname>Verdejo</keyname><forenames>Felisa</forenames></author></authors><title>The Role of Conceptual Relations in Word Sense Disambiguation</title><categories>cs.CL</categories><comments>12 pages, 3 figures, published in the proceedings of NLDB'01</comments><acm-class>I.2.7</acm-class><abstract>  We explore many ways of using conceptual distance measures in Word Sense
Disambiguation, starting with the Agirre-Rigau conceptual density measure. We
use a generalized form of this measure, introducing many (parameterized)
refinements and performing an exhaustive evaluation of all meaningful
combinations. We finally obtain a 42% improvement over the original algorithm,
and show that measures of conceptual distance are not worse indicators for
sense disambiguation than measures based on word-coocurrence (exemplified by
the Lesk algorithm). Our results, however, reinforce the idea that only a
combination of different sources of knowledge might eventually lead to accurate
word sense disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107006</id><created>2001-07-03</created><authors><author><keyname>Breck</keyname><forenames>Eric</forenames></author><author><keyname>Light</keyname><forenames>Marc</forenames></author><author><keyname>Mann</keyname><forenames>Gideon S.</forenames></author><author><keyname>Riloff</keyname><forenames>Ellen</forenames></author><author><keyname>Anand</keyname><forenames>Brianne Brown Pranav</forenames></author><author><keyname>Rooth</keyname><forenames>Mats</forenames></author><author><keyname>Thelen</keyname><forenames>Michael</forenames></author></authors><title>Looking Under the Hood : Tools for Diagnosing your Question Answering
  Engine</title><categories>cs.CL</categories><comments>Revision of paper appearing in the Proceedings of the Workshop on
  Open-Domain Question Answering</comments><acm-class>I.2.7</acm-class><abstract>  In this paper we analyze two question answering tasks : the TREC-8 question
answering task and a set of reading comprehension exams. First, we show that
Q/A systems perform better when there are multiple answer opportunities per
question. Next, we analyze common approaches to two subproblems: term overlap
for answer sentence identification, and answer typing for short answer
extraction. We present general tools for analyzing the strengths and
limitations of techniques for these subproblems. Our results quantify the
limitations of both term overlap and answer typing to distinguish between
competing answer candidates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107007</id><created>2001-07-03</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Nolte</keyname><forenames>Andreas</forenames></author><author><keyname>Tate</keyname><forenames>Stephen R.</forenames></author></authors><title>The Risk Profile Problem for Stock Portfolio Optimization</title><categories>cs.CE cs.DM cs.DS</categories><comments>A preliminary version of this work appeared in Proceedings of the
  32nd Annual ACM Symposium on Theory of Computing, pages 228--234, 2000. The
  final version will appear in E. J. Kontoghiorghes, B. Rustem, and S. Siokos,
  editors, Applied Optimization: Computational Methods in Decision-Making,
  Economics and Finance. Kluwer Academic Publishers, Dordrecht, 2002</comments><acm-class>E1; F2; G.1.6; G.1.10; G.2; G.3; J.4</acm-class><abstract>  This work initiates research into the problem of determining an optimal
investment strategy for investors with different attitudes towards the
trade-offs of risk and profit. The probability distribution of the return
values of the stocks that are considered by the investor are assumed to be
known, while the joint distribution is unknown. The problem is to find the best
investment strategy in order to minimize the probability of losing a certain
percentage of the invested capital based on different attitudes of the
investors towards future outcomes of the stock market.
  For portfolios made up of two stocks, this work shows how to exactly and
quickly solve the problem of finding an optimal portfolio for aggressive or
risk-averse investors, using an algorithm based on a fast greedy solution to a
maximum flow problem. However, an investor looking for an average-case
guarantee (so is neither aggressive or risk-averse) must deal with a more
difficult problem. In particular, it is #P-complete to compute the distribution
function associated with the average-case bound. On the positive side,
approximate answers can be computed by using random sampling techniques similar
to those for high-dimensional volume estimation. When k&gt;2 stocks are
considered, it is proved that a simple solution based on the same flow concepts
as the 2-stock algorithm would imply that P = NP, so is highly unlikely. This
work gives approximation algorithms for this case as well as exact algorithms
for some important special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107008</identifier>
 <datestamp>2007-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107008</id><created>2001-07-04</created><updated>2007-09-25</updated><authors><author><keyname>Durand</keyname><forenames>Bruno</forenames></author><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Complex Tilings</title><categories>cs.CC cs.DM</categories><comments>An extended abstract of a weaker version of this article appeared in
  Proceedings of the Annual ACM Symposium on Theory of Computing (STOC), 2001</comments><acm-class>F.1.1; G.2.1</acm-class><abstract>  We study the minimal complexity of tilings of a plane with a given tile set.
We note that every tile set admits either no tiling or some tiling with O(n)
Kolmogorov complexity of its n-by-n squares. We construct tile sets for which
this bound is tight: all n-by-n squares in all tilings have complexity at least
n. This adds a quantitative angle to classical results on non-recursivity of
tilings -- that we also develop in terms of Turing degrees of unsolvability.
  Keywords: Tilings, Kolmogorov complexity, recursion theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107009</id><created>2001-07-05</created><authors><author><keyname>Khan</keyname><forenames>A. I.</forenames></author><author><keyname>Spindler</keyname><forenames>R.</forenames></author></authors><title>A Blueprint for Building Serverless Applications on the Net</title><categories>cs.DC cs.NI</categories><comments>23 pages, 9 Figures</comments><acm-class>C.2.4; C.1.4; D.2.11</acm-class><abstract>  A peer-to-peer application architecture is proposed that has the potential to
eliminate the back-end servers for hosting services on the Internet. The
proposed application architecture has been modeled as a distributed system for
delivering an Internet service. The service thus created, though chaotic and
fraught with uncertainties, would be highly scalable and capable of achieving
unprecedented levels of robustness and viability with the increase in the
number of the users. The core issues relating to the architecture, such as
service discovery, distributed application architecture components, and
inter-application communications, have been analysed. It is shown that the
communications for the coordination of various functions, among the cooperating
instances of the application, may be optimised using a divide-and-conquer
strategy. Finally, the areas where future work needs to be directed have been
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107010</id><created>2001-07-05</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames><affiliation>UC Berkeley</affiliation></author></authors><title>Algorithms for Boolean Function Query Properties</title><categories>cs.CC cs.DS</categories><comments>13 pages, no figures, earlier version submitted to SIAM J. Comp</comments><acm-class>F.1.2; F.1.3; F.2.2</acm-class><abstract>  We present new algorithms to compute fundamental properties of a Boolean
function given in truth-table form. Specifically, we give an O(N^2.322 log N)
algorithm for block sensitivity, an O(N^1.585 log N) algorithm for `tree
decomposition,' and an O(N) algorithm for `quasisymmetry.' These algorithms are
based on new insights into the structure of Boolean functions that may be of
independent interest. We also give a subexponential-time algorithm for the
space-bounded quantum query complexity of a Boolean function. To prove this
algorithm correct, we develop a theory of limited-precision representation of
unitary operators, building on work of Bernstein and Vazirani.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107011</id><created>2001-07-06</created><authors><author><keyname>Clementi</keyname><forenames>Andrea E. F.</forenames></author><author><keyname>Monti</keyname><forenames>Angelo</forenames></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames></author></authors><title>Distributed Broadcast in Wireless Networks with Unknown Topology</title><categories>cs.DS cs.DM</categories><comments>27 pages, 1 figure, 1 table</comments><acm-class>C.2.1; F.2.2</acm-class><abstract>  A multi-hop synchronous wirelss network is said to be unknown if the nodes
have no knowledge of the topology. A basic task in wireless network is that of
broadcasting a message (created by a fixed source node) to all nodes of the
network. The multi-broadcast that consists in performing a set of r independent
broadcasts. In this paper, we study the completion and the termination time of
distributed protocols for both the (single) broadcast and the multi-broadcast
operations on unknown networks as functions of the number of nodes n, the
maximum eccentricity D, the maximum in-degree Delta, and the congestion c of
the networks. We establish new connections between these operations and some
combinatorial concepts, such as selective families, strongly-selective families
(also known as superimposed codes), and pairwise r-different families. Such
connections, combined with a set of new lower and upper bounds on the size of
the above families, allow us to derive new lower bounds and new distributed
protocols for the broadcast and multi-broadcast operations. In particular, our
upper bounds are almost tight and improve exponentially over the previous
bounds when D and Delta are polylogarithmic in n. Network topologies having
``small'' eccentricity and ``small'' degree (such as bounded-degree expanders)
are often used in practice to achieve efficient communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107012</id><created>2001-07-09</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>Three-Stage Quantitative Neural Network Model of the Tip-of-the-Tongue
  Phenomenon</title><categories>cs.CL cs.AI q-bio.NC q-bio.QM</categories><comments>Proceedings of the IX-th International Conference
  Knowledge-Dialog-Solution (KDS-2001), held on June 19-22, 2001 in
  St-Petersburg, Russia, pages 158-165 (in Russian)</comments><acm-class>I.2.7</acm-class><abstract>  A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is shortly described, and its stochastic nature
was demonstrated. A way to calculate strength and appearance probability of
tip-of-the-tongue states, neural network mechanism of feeling-of-knowing
phenomenon are proposed. The model synthesizes memory, psycholinguistic, and
metamemory approaches, bridges speech errors and naming chronometry research
traditions. A model analysis of a tip-of-the-tongue case from Anton Chekhov's
short story 'A Horsey Name' is performed. A new 'throw-up-one's-arms effect' is
defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107013</id><created>2001-07-10</created><updated>2001-07-12</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>The Logic Programming Paradigm and Prolog</title><categories>cs.PL cs.AI</categories><comments>37 pages; unpublished</comments><acm-class>D.1.6; D.3.2</acm-class><abstract>  This is a tutorial on logic programming and Prolog appropriate for a course
on programming languages for students familiar with imperative programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107014</id><created>2001-07-10</created><authors><author><keyname>Etalle</keyname><forenames>Sandro</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Meo</keyname><forenames>Maria Chiara</forenames></author></authors><title>Transformations of CCP programs</title><categories>cs.PL cs.AI cs.LO</categories><comments>To appear in ACM TOPLAS</comments><acm-class>I.2.2; D.1.3; D.3.2</acm-class><abstract>  We introduce a transformation system for concurrent constraint programming
(CCP). We define suitable applicability conditions for the transformations
which guarantee that the input/output CCP semantics is preserved also when
distinguishing deadlocked computations from successful ones and when
considering intermediate results of (possibly) non-terminating computations.
  The system allows us to optimize CCP programs while preserving their intended
meaning: In addition to the usual benefits that one has for sequential
declarative languages, the transformation of concurrent programs can also lead
to the elimination of communication channels and of synchronization points, to
the transformation of non-deterministic computations into deterministic ones,
and to the crucial saving of computational space. Furthermore, since the
transformation system preserves the deadlock behavior of programs, it can be
used for proving deadlock freeness of a given program wrt a class of queries.
To this aim it is sometimes sufficient to apply our transformations and to
specialize the resulting program wrt the given queries in such a way that the
obtained program is trivially deadlock free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107015</id><created>2001-07-11</created><authors><author><keyname>Svenson</keyname><forenames>Pontus</forenames></author></authors><title>From Neel to NPC: Colouring Small Worlds</title><categories>cs.CC cond-mat.stat-mech</categories><comments>4 pages, 6 figures</comments><acm-class>F.2.m</acm-class><abstract>  In this note, we present results for the colouring problem on small world
graphs created by rewiring square, triangular, and two kinds of cubic (with
coordination numbers 5 and 6) lattices. As the rewiring parameter p tends to 1,
we find the expected crossover to the behaviour of random graphs with
corresponding connectivity. However, for the cubic lattices there is a region
near p=0 for which the graphs are colourable. This could in principle be used
as an additional heuristic for solving real world colouring or scheduling
problems. Small worlds with connectivity 5 and p ~ 0.1 provide an interesting
ensemble of graphs whose colourability is hard to determine. For square
lattices, we get good data collapse plotting the fraction of colourable graphs
against the rescaled parameter parameter $p N^{-\nu}$ with $\nu = 1.35$. No
such collapse can be obtained for the data from lattices with coordination
number 5 or 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107016</id><created>2001-07-15</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author><author><keyname>Dejean</keyname><forenames>Herve</forenames></author></authors><title>Introduction to the CoNLL-2001 Shared Task: Clause Identification</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>In: Walter Daelemans and Remi Zajac (eds.), Proceedings of
  CoNLL-2001, Toulouse, France, 2001, pp. 53-57</journal-ref><abstract>  We describe the CoNLL-2001 shared task: dividing text into clauses. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107017</id><created>2001-07-15</created><authors><author><keyname>Nerbonne</keyname><forenames>John</forenames></author><author><keyname>Belz</keyname><forenames>Anja</forenames></author><author><keyname>Cancedda</keyname><forenames>Nicola</forenames></author><author><keyname>Dejean</keyname><forenames>Herve</forenames></author><author><keyname>Hammerton</keyname><forenames>James</forenames></author><author><keyname>Koeling</keyname><forenames>Rob</forenames></author><author><keyname>Konstantopoulos</keyname><forenames>Stasinos</forenames></author><author><keyname>Osborne</keyname><forenames>Miles</forenames></author><author><keyname>Thollard</keyname><forenames>Franck</forenames></author><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author></authors><title>Learning Computational Grammars</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>In: Walter Daelemans and Remi Zajac (eds.), Proceedings of
  CoNLL-2001, Toulouse, France, 2001, pp. 97-104</journal-ref><abstract>  This paper reports on the &quot;Learning Computational Grammars&quot; (LCG) project, a
postdoc network devoted to studying the application of machine learning
techniques to grammars suitable for computational use. We were interested in a
more systematic survey to understand the relevance of many factors to the
success of learning, esp. the availability of annotated data, the kind of
dependencies in the data, and the availability of knowledge bases (grammars).
We focused on syntax, esp. noun phrase (NP) syntax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107018</id><created>2001-07-15</created><authors><author><keyname>Hammerton</keyname><forenames>James</forenames></author><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author></authors><title>Combining a self-organising map with memory-based learning</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>In: Walter Daelemans and Remi Zajac (eds.), Proceedings of
  CoNLL-2001, Toulouse, France, 2001, pp. 9-14</journal-ref><abstract>  Memory-based learning (MBL) has enjoyed considerable success in corpus-based
natural language processing (NLP) tasks and is thus a reliable method of
getting a high-level of performance when building corpus-based NLP systems.
However there is a bottleneck in MBL whereby any novel testing item has to be
compared against all the training items in memory base. For this reason there
has been some interest in various forms of memory editing whereby some method
of selecting a subset of the memory base is employed to reduce the number of
comparisons. This paper investigates the use of a modified self-organising map
(SOM) to select a subset of the memory items for comparison. This method
involves reducing the number of comparisons to a value proportional to the
square root of the number of training items. The method is tested on the
identification of base noun-phrases in the Wall Street Journal corpus, using
sections 15 to 18 for training and section 20 for testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107019</id><created>2001-07-16</created><updated>2001-07-16</updated><authors><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author><author><keyname>McKeown</keyname><forenames>Kathleen R.</forenames></author><author><keyname>Klavans</keyname><forenames>Judith L.</forenames></author></authors><title>Applying Natural Language Generation to Indicative Summarization</title><categories>cs.CL</categories><comments>8 pages, published in Proc. of 8th European Workshop on NLG</comments><acm-class>I.2.7</acm-class><abstract>  The task of creating indicative summaries that help a searcher decide whether
to read a particular document is a difficult task. This paper examines the
indicative summarization task from a generation perspective, by first analyzing
its required content via published guidelines and corpus analysis. We show how
these summaries can be factored into a set of document features, and how an
implemented content planner uses the topicality document feature to create
indicative multidocument query-based summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107020</id><created>2001-07-17</created><authors><author><keyname>Ngai</keyname><forenames>Grace</forenames></author><author><keyname>Florian</keyname><forenames>Radu</forenames></author></authors><title>Transformation-Based Learning in the Fast Lane</title><categories>cs.CL</categories><comments>8 pages, 2 figures, presented at NAACL 2001</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Second Conference of the North American Chapter
  of the Association for Computational Linguistics, pages 40-47, Pittsburgh,
  PA, USA</journal-ref><abstract>  Transformation-based learning has been successfully employed to solve many
natural language processing problems. It achieves state-of-the-art performance
on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often
intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the
training time of a transformation-based learner without sacrificing
performance. The paper compares and contrasts the training time needed and
performance achieved by our modified learner with two other systems: a standard
transformation-based learner, and the ICA system \cite{hepple00:tbl}. The
results of these experiments show that our system is able to achieve a
significant improvement in training time while still achieving the same
performance as a standard transformation-based learner. This is a valuable
contribution to systems and algorithms which utilize transformation-based
learning at any part of the execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107021</id><created>2001-07-17</created><authors><author><keyname>Florian</keyname><forenames>Radu</forenames></author><author><keyname>Ngai</keyname><forenames>Grace</forenames></author></authors><title>Multidimensional Transformation-Based Learning</title><categories>cs.CL</categories><comments>8 pages, 2 figures, presented at CONLL 2001</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 5th Computational Natural Language Learning
  Workshop (CoNNL-2001), pages 1-8, Toulouse, France</journal-ref><abstract>  This paper presents a novel method that allows a machine learning algorithm
following the transformation-based learning paradigm \cite{brill95:tagging} to
be applied to multiple classification tasks by training jointly and
simultaneously on all fields. The motivation for constructing such a system
stems from the observation that many tasks in natural language processing are
naturally composed of multiple subtasks which need to be resolved
simultaneously; also tasks usually learned in isolation can possibly benefit
from being learned in a joint framework, as the signals for the extra tasks
usually constitute inductive bias.
  The proposed algorithm is evaluated in two experiments: in one, the system is
used to jointly predict the part-of-speech and text chunks/baseNP chunks of an
English corpus; and in the second it is used to learn the joint prediction of
word segment boundaries and part-of-speech tagging for Chinese. The results
show that the simultaneous learning of multiple tasks does achieve an
improvement in each task upon training the same tasks sequentially. The
part-of-speech tagging result of 96.63% is state-of-the-art for individual
systems on the particular train/test split.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107022</id><created>2001-07-17</created><authors><author><keyname>Bruni</keyname><forenames>Roberto</forenames></author><author><keyname>Montanari</keyname><forenames>Ugo</forenames></author><author><keyname>Rossi</keyname><forenames>Francesca</forenames></author></authors><title>An interactive semantics of logic programming</title><categories>cs.LO cs.PL</categories><comments>42 pages, 24 figure, 3 tables, to appear in the CUP journal of Theory
  and Practice of Logic Programming</comments><acm-class>D.1.6; D.3.2; D.3.3; F.3.2</acm-class><abstract>  We apply to logic programming some recently emerging ideas from the field of
reduction-based communicating systems, with the aim of giving evidence of the
hidden interactions and the coordination mechanisms that rule the operational
machinery of such a programming paradigm. The semantic framework we have chosen
for presenting our results is tile logic, which has the advantage of allowing a
uniform treatment of goals and observations and of applying abstract
categorical tools for proving the results. As main contributions, we mention
the finitary presentation of abstract unification, and a concurrent and
coordinated abstract semantics consistent with the most common semantics of
logic programming. Moreover, the compositionality of the tile semantics is
guaranteed by standard results, as it reduces to check that the tile systems
associated to logic programs enjoy the tile decomposition property. An
extension of the approach for handling constraint systems is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107023</identifier>
 <datestamp>2010-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107023</id><created>2001-07-18</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Hart</keyname><forenames>George W.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Vertex-Unfoldings of Simplicial Polyhedra</title><categories>cs.CG cs.DM</categories><comments>10 pages; 7 figures; 8 references</comments><report-no>Smith Tech. Rep. 071</report-no><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Discrete Geometry: In honor of W. Kuperberg's 60th birthday, Pure
  and Appl. Math. 253, Marcel Dekker, pp. 215-228, 2003</journal-ref><abstract>  We present two algorithms for unfolding the surface of any polyhedron, all of
whose faces are triangles, to a nonoverlapping, connected planar layout. The
surface is cut only along polyhedron edges. The layout is connected, but it may
have a disconnected interior: the triangles are connected at vertices, but not
necessarily joined along edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107024</id><created>2001-07-18</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Enumerating Foldings and Unfoldings between Polygons and Polytopes</title><categories>cs.CG cs.DM</categories><comments>12 pages; 10 figures; 10 references. Revision of version in
  Proceedings of the Japan Conference on Discrete and Computational Geometry,
  Tokyo, Nov. 2000, pp. 9-12. See also cs.CG/0007019</comments><acm-class>F.2.2; G.2.1</acm-class><journal-ref>Graphs and Combinatorics 18(1) 93-104 (2002)</journal-ref><abstract>  We pose and answer several questions concerning the number of ways to fold a
polygon to a polytope, and how many polytopes can be obtained from one polygon;
and the analogous questions for unfolding polytopes to polygons. Our answers
are, roughly: exponentially many, or nondenumerably infinite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107025</id><created>2001-07-19</created><updated>2001-10-30</updated><authors><author><keyname>Boldo</keyname><forenames>Sylvie</forenames></author><author><keyname>Daumas</keyname><forenames>Marc</forenames></author><author><keyname>Moreau-Finot</keyname><forenames>Claire</forenames></author><author><keyname>Thery</keyname><forenames>Laurent</forenames></author></authors><title>Computer validated proofs of a toolset for adaptable arithmetic</title><categories>cs.MS</categories><comments>21 pages, web links</comments><acm-class>G.4</acm-class><abstract>  Most existing implementations of multiple precision arithmetic demand that
the user sets the precision {\em a priori}. Some libraries are said adaptable
in the sense that they dynamically change the precision of each intermediate
operation individually to deliver the target accuracy according to the actual
inputs. We present in this text a new adaptable numeric core inspired both from
floating point expansions and from on-line arithmetic.
  The numeric core is cut down to four tools. The tool that contains arithmetic
operations is proved to be correct. The proofs have been formally checked by
the Coq assistant. Developing the proofs, we have formally proved many results
published in the literature and we have extended a few of them. This work may
let users (i) develop application specific adaptable libraries based on the
toolset and / or (ii) write new formal proofs based on the set of validated
facts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107026</id><created>2001-07-19</created><authors><author><keyname>Marek</keyname><forenames>Victor</forenames></author><author><keyname>Pivkina</keyname><forenames>Inna</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Annotated revision programs</title><categories>cs.AI cs.LO</categories><comments>30 pages, to appear in Artificial Intelligence Journal</comments><acm-class>I.2.4;I.2.3</acm-class><journal-ref>Artificial Intelligence Journal, 138 (2002), pp. 149-180.</journal-ref><abstract>  Revision programming is a formalism to describe and enforce updates of belief
sets and databases. That formalism was extended by Fitting who assigned
annotations to revision atoms. Annotations provide a way to quantify the
confidence (probability) that a revision atom holds. The main goal of our paper
is to reexamine the work of Fitting, argue that his semantics does not always
provide results consistent with intuition, and to propose an alternative
treatment of annotated revision programs. Our approach differs from that
proposed by Fitting in two key aspects: we change the notion of a model of a
program and we change the notion of a justified revision. We show that under
this new approach fundamental properties of justified revisions of standard
revision programs extend to the annotated case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107027</id><created>2001-07-19</created><updated>2001-07-31</updated><authors><author><keyname>Lonc</keyname><forenames>Zbigniew</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Fixed-parameter complexity of semantics for logic programs</title><categories>cs.LO cs.AI</categories><comments>Submission to ACM TOCL; full version of the paper published in the
  Proceedings of ICLP 2001 (Springer Verlag)</comments><acm-class>F.1.3;I.2.4;D.1.6</acm-class><journal-ref>ACM Transactions on Computational Logic, 4 (2003), pp. 91-119.</journal-ref><abstract>  A decision problem is called parameterized if its input is a pair of strings.
One of these strings is referred to as a parameter. The problem: given a
propositional logic program P and a non-negative integer k, decide whether P
has a stable model of size no more than k, is an example of a parameterized
decision problem with k serving as a parameter. Parameterized problems that are
NP-complete often become solvable in polynomial time if the parameter is fixed.
The problem to decide whether a program P has a stable model of size no more
than k, where k is fixed and not a part of input, can be solved in time
O(mn^k), where m is the size of P and n is the number of atoms in P. Thus, this
problem is in the class P. However, algorithms with the running time given by a
polynomial of order k are not satisfactory even for relatively small values of
k.
  The key question then is whether significantly better algorithms (with the
degree of the polynomial not dependent on k) exist. To tackle it, we use the
framework of fixed-parameter complexity. We establish the fixed-parameter
complexity for several parameterized decision problems involving models,
supported models and stable models of logic programs. We also establish the
fixed-parameter complexity for variants of these problems resulting from
restricting attention to Horn programs and to purely negative programs. Most of
the problems considered in the paper have high fixed-parameter complexity.
Thus, it is unlikely that fixing bounds on models (supported models, stable
models) will lead to fast algorithms to decide the existence of such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107028</id><created>2001-07-19</created><authors><author><keyname>East</keyname><forenames>Deborah</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Propositional satisfiability in answer-set programming</title><categories>cs.AI cs.LO</categories><comments>15 pages, Proceedings of KI 2001 (Springer Verlag)</comments><acm-class>F.4.1;D.1.6;I.2.4</acm-class><abstract>  We show that propositional logic and its extensions can support answer-set
programming in the same way stable logic programming and disjunctive logic
programming do. To this end, we introduce a logic based on the logic of
propositional schemata and on a version of the Closed World Assumption. We call
it the extended logic of propositional schemata with CWA (PS+, in symbols). An
important feature of this logic is that it supports explicit modeling of
constraints on cardinalities of sets. In the paper, we characterize the class
of problems that can be solved by finite PS+ theories. We implement a
programming system based on the logic PS+ and design and implement a solver for
processing theories in PS+. We present encouraging performance results for our
approach --- we show it to be competitive with smodels, a state-of-the-art
answer-set programming system based on stable logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107029</id><created>2001-07-19</created><authors><author><keyname>Truszczynski</keyname><forenames>Deborah East. Miroslaw</forenames></author></authors><title>aspps --- an implementation of answer-set programming with propositional
  schemata</title><categories>cs.AI cs.LO</categories><comments>4 pages; Proceedings of LPNMR 2001 (Springer Verlag)</comments><acm-class>F.4.1;I.2.4;D.I.6</acm-class><abstract>  We present an implementation of an answer-set programming paradigm, called
aspps (short for answer-set programming with propositional schemata). The
system aspps is designed to process PS+ theories. It consists of two basic
modules. The first module, psgrnd, grounds an PS+ theory. The second module,
referred to as aspps, is a solver. It computes models of ground PS+ theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107030</id><created>2001-07-20</created><updated>2002-12-24</updated><authors><author><keyname>Van Assche</keyname><forenames>G.</forenames><affiliation>ULB</affiliation></author><author><keyname>Cardinal</keyname><forenames>J.</forenames><affiliation>ULB</affiliation></author><author><keyname>Cerf</keyname><forenames>N. J.</forenames><affiliation>ULB</affiliation><affiliation>JPL/Caltech</affiliation></author></authors><title>Reconciliation of a Quantum-Distributed Gaussian Key</title><categories>cs.CR quant-ph</categories><comments>8 pages, 4 figures. Submitted to the IEEE for possible publication.
  Revised version to improve its clarity</comments><acm-class>E.3</acm-class><journal-ref>IEEE Trans. Inform. Theory, vol. 50, p. 394, Feb. 2004</journal-ref><doi>10.1109/TIT.2003.822618</doi><abstract>  Two parties, Alice and Bob, wish to distill a binary secret key out of a list
of correlated variables that they share after running a quantum key
distribution protocol based on continuous-spectrum quantum carriers. We present
a novel construction that allows the legitimate parties to get equal bit
strings out of correlated variables by using a classical channel, with as few
leaked information as possible. This opens the way to securely correcting
non-binary key elements. In particular, the construction is refined to the case
of Gaussian variables as it applies directly to recent continuous-variable
protocols for quantum key distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107031</id><created>2001-07-21</created><authors><author><keyname>Biedl</keyname><forenames>Therese C.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Fleischer</keyname><forenames>Rudolf</forenames></author><author><keyname>Jacobsen</keyname><forenames>Lars</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author></authors><title>The Complexity of Clickomania</title><categories>cs.CC cs.DM cs.DS</categories><comments>15 pages, 3 figures. To appear in More Games of No Chance, edited by
  R. J. Nowakowski</comments><acm-class>F.2.2; F.1.3; F.1.1; G.2.1</acm-class><abstract>  We study a popular puzzle game known variously as Clickomania and Same Game.
Basically, a rectangular grid of blocks is initially colored with some number
of colors, and the player repeatedly removes a chosen connected monochromatic
group of at least two square blocks, and any blocks above it fall down. We show
that one-column puzzles can be solved, i.e., the maximum possible number of
blocks can be removed, in linear time for two colors, and in polynomial time
for an arbitrary number of colors. On the other hand, deciding whether a puzzle
is solvable (all blocks can be removed) is NP-complete for two columns and five
colors, or five columns and three colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107032</id><created>2001-07-23</created><authors><author><keyname>Marx</keyname><forenames>Zvika</forenames></author><author><keyname>Dagan</keyname><forenames>Ido</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim</forenames></author></authors><title>Coupled Clustering: a Method for Detecting Structural Correspondence</title><categories>cs.LG cs.CL cs.IR</categories><comments>html with 5 figures</comments><acm-class>H.3.3; I.2.6; I.2.7; I.5.3; I.5.4</acm-class><journal-ref>In: C. E. Brodley and A. P. Danyluk (eds.), Proceedings of the
  18th International Conference on Machine Learning (ICML 2001), pp. 353-360</journal-ref><abstract>  This paper proposes a new paradigm and computational framework for
identification of correspondences between sub-structures of distinct composite
systems. For this, we define and investigate a variant of traditional data
clustering, termed coupled clustering, which simultaneously identifies
corresponding clusters within two data sets. The presented method is
demonstrated and evaluated for detecting topical correspondences in textual
corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107033</id><created>2001-07-25</created><authors><author><keyname>Rivin</keyname><forenames>Igor</forenames></author></authors><title>Yet another zeta function and learning</title><categories>cs.LG cs.DM math.PR</categories><acm-class>I.2.6; G.3</acm-class><abstract>  We study the convergence speed of the batch learning algorithm, and compare
its speed to that of the memoryless learning algorithm and of learning with
memory (as analyzed in joint work with N. Komarova). We obtain precise results
and show in particular that the batch learning algorithm is never worse than
the memoryless learning algorithm (at least asymptotically). Its performance
vis-a-vis learning with full memory is less clearcut, and depends on
certainprobabilistic assumptions. These results necessitate theintroduction of
the moment zeta function of a probability distribution and the study of some of
its properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107034</id><created>2001-07-26</created><authors><author><keyname>Dolan</keyname><forenames>Elizabeth D.</forenames></author></authors><title>NEOS Server 4.0 Administrative Guide</title><categories>cs.DC</categories><comments>45 pages including front matter, 3 figures</comments><report-no>ANL/MCS-TM-250</report-no><acm-class>C.2.4</acm-class><abstract>  The NEOS Server 4.0 provides a general Internet-based client/server as a link
between users and software applications. The administrative guide covers the
fundamental principals behind the operation of the NEOS Server, installation
and trouble-shooting of the Server software, and implementation details of
potential interest to a NEOS Server administrator. The guide also discusses
making new software applications available through the Server, including areas
of concern to remote solver administrators such as maintaining security,
providing usage instructions, and enforcing reasonable restrictions on jobs.
The administrative guide is intended both as an introduction to the NEOS Server
and as a reference for use when running the Server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107035</id><created>2001-07-29</created><authors><author><keyname>Lopatenko</keyname><forenames>A.</forenames></author></authors><title>Semantic Web Content Accessibility Guidelines for Current Research
  Information Systems (CRIS)</title><categories>cs.NI cs.DL</categories><comments>25 pages</comments><acm-class>D.2.12;E.2;H.2.4</acm-class><journal-ref>Second Interim Report of Extencion Centre, Vienna University of
  Technology, 2001</journal-ref><abstract>  The most exciting challenge for CRIS is to create a service for research
information which should be wide-spread, distributed and actual like Google,
but at the same time structured, trusted, with a complex search and navigation
similar to today CRIS application. The core technology for such a &quot;new&quot; CRIS is
the semantic web technology to integrate database contents with HTML and XML
web pages for being provided to the research interested public. One (at the
moment the best) possible way is to use RDF (Resource Description Framework)
which is also recommended by the W3 consortium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0107036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0107036</id><created>2001-07-29</created><updated>2001-07-31</updated><authors><author><keyname>Grozin</keyname><forenames>A. G.</forenames></author></authors><title>TeXmacs interfaces to Maxima, MuPAD and REDUCE</title><categories>cs.SC cs.MS hep-ph</categories><comments>Talk at 5 Int. Workshop on Computer Algebra and its Applications to
  Physics, Dubna, June 28-30; 9 pages, prepared in TeXmacs and exported as
  LaTeX, 6 PostScript figures included</comments><acm-class>I.1.3;G.4</acm-class><abstract>  GNU TeXmacs is a free wysiwyg word processor providing an excellent
typesetting quality of texts and formulae. It can also be used as an interface
to Computer Algebra Systems (CASs). In the present work, interfaces to three
general-purpose CASs have been implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108001</id><created>2001-08-01</created><authors><author><keyname>Allen</keyname><forenames>Gabrielle</forenames><affiliation>Max-Planck-Institut f&#xfc;r Gravitationsphysik</affiliation></author><author><keyname>Angulo</keyname><forenames>David</forenames><affiliation>Univ of Chicago</affiliation></author><author><keyname>Foster</keyname><forenames>Ian</forenames><affiliation>Univ of Chicago</affiliation><affiliation>Argonne Natnl Lab</affiliation></author><author><keyname>Lanfermann</keyname><forenames>Gerd</forenames><affiliation>Max-Planck-Institut f&#xfc;r Gravitationsphysik</affiliation></author><author><keyname>Liu</keyname><forenames>Chuang</forenames><affiliation>Univ of Chicago</affiliation></author><author><keyname>Radke</keyname><forenames>Thomas</forenames><affiliation>Max-Planck-Institut f&#xfc;r Gravitationsphysik</affiliation></author><author><keyname>Seidel</keyname><forenames>Ed</forenames><affiliation>Max-Planck-Institut f&#xfc;r Gravitationsphysik</affiliation></author><author><keyname>Shalf</keyname><forenames>John</forenames><affiliation>Lawrence Berkeley Natnl Lab</affiliation></author></authors><title>The Cactus Worm: Experiments with Dynamic Resource Discovery and
  Allocation in a Grid Environment</title><categories>cs.DC</categories><comments>14 pages, 5 figures, to be published in International Journal of
  Supercomputing Applications</comments><report-no>TR-2001-28</report-no><acm-class>D.1.3</acm-class><abstract>  The ability to harness heterogeneous, dynamically available &quot;Grid&quot; resources
is attractive to typically resource-starved computational scientists and
engineers, as in principle it can increase, by significant factors, the number
of cycles that can be delivered to applications. However, new adaptive
application structures and dynamic runtime system mechanisms are required if we
are to operate effectively in Grid environments. In order to explore some of
these issues in a practical setting, we are developing an experimental
framework, called Cactus, that incorporates both adaptive application
structures for dealing with changing resource characteristics and adaptive
resource selection mechanisms that allow applications to change their resource
allocations (e.g., via migration) when performance falls outside specified
limits. We describe here the adaptive resource selection mechanisms and
describe how they are used to achieve automatic application migration to
&quot;better&quot; resources following performance degradation. Our results provide
insights into the architectural structures required to support adaptive
resource selection. In addition, we suggest that this &quot;Cactus Worm&quot; is an
interesting challenge problem for Grid computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108002</id><created>2001-08-02</created><authors><author><keyname>Haldar</keyname><forenames>Sibsankar</forenames><affiliation>Bell Labs</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Bounded Concurrent Timestamp Systems Using Vector Clocks</title><categories>cs.DC</categories><comments>LaTeX source, 35 pages; To apper in: J. Assoc. Comp. Mach</comments><acm-class>F.1.2; C.2.4; B.3.2;B.4.3;D.1.3;D.4.1;D.4.4</acm-class><abstract>  Shared registers are basic objects used as communication mediums in
asynchronous concurrent computation. A concurrent timestamp system is a higher
typed communication object, and has been shown to be a powerful tool to solve
many concurrency control problems. It has turned out to be possible to
construct such higher typed objects from primitive lower typed ones. The next
step is to find efficient constructions. We propose a very efficient wait-free
construction of bounded concurrent timestamp systems from 1-writer multireader
registers. This finalizes, corrects, and extends, a preliminary bounded
multiwriter construction proposed by the second author in 1986. That work
partially initiated the current interest in wait-free concurrent objects, and
introduced a notion of discrete vector clocks in distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108003</id><created>2001-08-07</created><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Perugini</keyname><forenames>Saverio</forenames></author></authors><title>The Partial Evaluation Approach to Information Personalization</title><categories>cs.IR cs.PL</categories><comments>Comprehensive overview of the PIPE model for personalization</comments><acm-class>D.3.4; H.4.2; H5.2; H5.4</acm-class><abstract>  Information personalization refers to the automatic adjustment of information
content, structure, and presentation tailored to an individual user. By
reducing information overload and customizing information access,
personalization systems have emerged as an important segment of the Internet
economy. This paper presents a systematic modeling methodology - PIPE
(`Personalization is Partial Evaluation') - for personalization.
Personalization systems are designed and implemented in PIPE by modeling an
information-seeking interaction in a programmatic representation. The
representation supports the description of information-seeking activities as
partial information and their subsequent realization by partial evaluation, a
technique for specializing programs. We describe the modeling methodology at a
conceptual level and outline representational choices. We present two
application case studies that use PIPE for personalizing web sites and describe
how PIPE suggests a novel evaluation criterion for information system designs.
Finally, we mention several fundamental implications of adopting the PIPE model
for personalization and when it is (and is not) applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108004</id><created>2001-08-07</created><authors><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Links tell us about lexical and semantic Web content</title><categories>cs.IR cs.DL</categories><comments>10 pages, 4 figures</comments><acm-class>H.3.1; H.3.3</acm-class><abstract>  The latest generation of Web search tools is beginning to exploit hypertext
link information to improve ranking\cite{Brin98,Kleinberg98} and
crawling\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden
assumption behind such approaches, a correlation between the graph structure of
the Web and its content, has not been tested explicitly despite increasing
research on Web topology\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I
formalize and quantitatively validate two conjectures drawing connections from
link information to lexical and semantic Web content. The clink-content
conjecture states that a page is similar to the pages that link to it, i.e.,
one can infer the lexical content of a page by looking at the pages that link
to it. I also show that lexical inferences based on link cues are quite
heterogeneous across Web communities. The link-cluster conjecture states that
pages about the same topic are clustered together, i.e., one can infer the
meaning of a page by looking at its neighbours. These results explain the
success of the newest search technologies and open the way for more dynamic and
scalable methods to locate information in a topic or user driven way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108005</id><created>2001-08-09</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames></author></authors><title>A Bit of Progress in Language Modeling</title><categories>cs.CL</categories><comments>73 pages, extended version of paper to appear in Computer Speech and
  Language</comments><report-no>MSR-TR-2001-72</report-no><acm-class>I.2.7</acm-class><abstract>  In the past several years, a number of different language modeling
improvements over simple trigram models have been found, including caching,
higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and
clustering. We present explorations of variations on, or of the limits of, each
of these techniques, including showing that sentence mixture models may have
more potential. While all of these techniques have been studied separately,
they have rarely been studied in combination. We find some significant
interactions, especially with smoothing and clustering techniques. We compare a
combination of all techniques together to a Katz smoothed trigram model with no
count cutoffs. We achieve perplexity reductions between 38% and 50% (1 bit of
entropy), depending on training data size, as well as a word error rate
reduction of 8.9%. Our perplexity reductions are perhaps the highest reported
compared to a fair baseline. This is the extended version of the paper; it
contains additional details and proofs, and is designed to be a good
introduction to the state of the art in language modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108006</id><created>2001-08-09</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames></author></authors><title>Classes for Fast Maximum Entropy Training</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of ICASSP-2001, Utah, May 2001</journal-ref><abstract>  Maximum entropy models are considered by many to be one of the most promising
avenues of language modeling research. Unfortunately, long training times make
maximum entropy research difficult. We present a novel speedup technique: we
change the form of the model to use classes. Our speedup works by creating two
maximum entropy models, the first of which predicts the class of each word, and
the second of which predicts the word itself. This factoring of the model leads
to fewer non-zero indicator functions, and faster normalization, achieving
speedups of up to a factor of 35 over one of the best previous techniques. It
also results in typically slightly lower perplexities. The same trick can be
used to speed training of other machine learning techniques, e.g. neural
networks, applied to any problem with a large number of outputs, such as
language modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108007</id><created>2001-08-12</created><authors><author><keyname>Tucker</keyname><forenames>J. V.</forenames><affiliation>University of Wales, Swansea</affiliation></author><author><keyname>Zucker</keyname><forenames>J. I.</forenames><affiliation>McMaster University, Hamilton, Canada</affiliation></author></authors><title>Abstract versus Concrete Computation on Metric Partial Algebras</title><categories>cs.LO</categories><comments>75 pages, AMSTeX, 3 figures</comments><report-no>McMaster Dept of Computing &amp; Software Tech Report CAS-01-01-JZ</report-no><acm-class>F.1.1;F.4.1</acm-class><abstract>  A model of computation is abstract if, when applied to any algebra, the
resulting programs for computable functions and sets on that algebra are
invariant under isomorphisms, and hence do not depend on a representation for
the algebra. Otherwise it is concrete. Intuitively, concrete models depend on
the implementation of the algebra.
  The difference is particularly striking in the case of topological partial
algebras, and notably in algebras over the reals. We investigate the
relationship between abstract and concrete models of partial metric algebras.
In the course of this investigation, interesting aspects of continuity,
extensionality and non-determinism are uncovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108008</id><created>2001-08-14</created><authors><author><keyname>Eiter</keyname><forenames>T.</forenames></author><author><keyname>Fink</keyname><forenames>M.</forenames></author><author><keyname>Sabbatini</keyname><forenames>G.</forenames></author><author><keyname>Tompits</keyname><forenames>H.</forenames></author></authors><title>Using Methods of Declarative Logic Programming for Intelligent
  Information Agents</title><categories>cs.MA cs.AI</categories><comments>66 pages, 1 figure, to be published in &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>I.2</acm-class><abstract>  The search for information on the web is faced with several problems, which
arise on the one hand from the vast number of available sources, and on the
other hand from their heterogeneity. A promising approach is the use of
multi-agent systems of information agents, which cooperatively solve advanced
information-retrieval problems. This requires capabilities to address complex
tasks, such as search and assessment of sources, query planning, information
merging and fusion, dealing with incomplete information, and handling of
inconsistency. In this paper, our interest is in the role which some methods
from the field of declarative logic programming can play in the realization of
reasoning capabilities for information agents. In particular, we are interested
in how they can be used and further developed for the specific needs of this
application domain. We review some existing systems and current projects, which
address information-integration problems. We then focus on declarative
knowledge-representation methods, and review and evaluate approaches from logic
programming and nonmonotonic reasoning for information agents. We discuss
advantages and drawbacks, and point out possible extensions and open issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108009</id><created>2001-08-17</created><authors><author><keyname>Kohring</keyname><forenames>G. A.</forenames></author></authors><title>Artificial Neurons with Arbitrarily Complex Internal Structures</title><categories>cs.NE q-bio.NC</categories><comments>22 pages, 2 figures</comments><acm-class>I.5.1</acm-class><journal-ref>Neurocomputing, vol. 47, pp. 103-118 (2002).</journal-ref><abstract>  Artificial neurons with arbitrarily complex internal structure are
introduced. The neurons can be described in terms of a set of internal
variables, a set activation functions which describe the time evolution of
these variables and a set of characteristic functions which control how the
neurons interact with one another. The information capacity of attractor
networks composed of these generalized neurons is shown to reach the maximum
allowed bound. A simple example taken from the domain of pattern recognition
demonstrates the increased computational power of these neurons. Furthermore, a
specific class of generalized neurons gives rise to a simple transformation
relating attractor networks of generalized neurons to standard three layer
feed-forward networks. Given this correspondence, we conjecture that the
maximum information capacity of a three layer feed-forward network is 2 bits
per weight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108010</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108010</id><created>2001-08-21</created><updated>2002-04-19</updated><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Couperus</keyname><forenames>Peter</forenames></author><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard</forenames></author></authors><title>A Note on Tiling under Tomographic Constraints</title><categories>cs.CC</categories><comments>added one author and a few theorems</comments><acm-class>F.2.2; G.2.1</acm-class><abstract>  Given a tiling of a 2D grid with several types of tiles, we can count for
every row and column how many tiles of each type it intersects. These numbers
are called the_projections_. We are interested in the problem of reconstructing
a tiling which has given projections. Some simple variants of this problem,
involving tiles that are 1x1 or 1x2 rectangles, have been studied in the past,
and were proved to be either solvable in polynomial time or NP-complete. In
this note we make progress toward a comprehensive classification of various
tiling reconstruction problems, by proving NP-completeness results for several
sets of tiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108011</id><created>2001-08-21</created><authors><author><keyname>Igel</keyname><forenames>Christian</forenames></author><author><keyname>Toussaint</keyname><forenames>Marc</forenames></author></authors><title>On Classes of Functions for which No Free Lunch Results Hold</title><categories>cs.NE math.OC nlin.AO</categories><comments>8 pages, 1 figure, see http://www.neuroinformatik.ruhr-uni-bochum.de/</comments><acm-class>G.1.6</acm-class><abstract>  In a recent paper it was shown that No Free Lunch results hold for any subset
F of the set of all possible functions from a finite set X to a finite set Y
iff F is closed under permutation of X. In this article, we prove that the
number of those subsets can be neglected compared to the overall number of
possible subsets. Further, we present some arguments why problem classes
relevant in practice are not likely to be closed under permutation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108012</identifier>
 <datestamp>2007-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108012</id><created>2001-08-22</created><authors><author><keyname>Crocetti</keyname><forenames>Giancarlo</forenames></author></authors><title>A polynomial axles-detection algorithm for a four-contacts treadle</title><categories>cs.OH</categories><comments>Removed by arXiv admin</comments><abstract>  This submission was removed because it contained proprietary information that
was distributed without permission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108013</id><created>2001-08-22</created><updated>2002-12-20</updated><authors><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author></authors><title>Convergent Approximate Solving of First-Order Constraints by Approximate
  Quantifiers</title><categories>cs.LO cs.AI</categories><acm-class>F.4.1; I.2.3</acm-class><abstract>  Exactly solving first-order constraints (i.e., first-order formulas over a
certain predefined structure) can be a very hard, or even undecidable problem.
In continuous structures like the real numbers it is promising to compute
approximate solutions instead of exact ones. However, the quantifiers of the
first-order predicate language are an obstacle to allowing approximations to
arbitrary small error bounds. In this paper we solve the problem by modifying
the first-order language and replacing the classical quantifiers with
approximate quantifiers. These also have two additional advantages: First, they
are tunable, in the sense that they allow the user to decide on the trade-off
between precision and efficiency. Second, they introduce additional
expressivity into the first-order language by allowing reasoning over the size
of solution sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108014</id><created>2001-08-22</created><authors><author><keyname>George</keyname><forenames>Lisa M.</forenames></author></authors><title>What's Fit To Print: The Effect Of Ownership Concentration On Product
  Variety In Daily Newspaper Markets</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-097</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper examines the effect of ownership concentration on product
position, product variety and readership in markets for daily newspapers. US
antitrust policy presumes that mergers reduce the amount and diversity of
content available to consumers. However, the effects of consolidation in
differentiated product markets cannot be determined solely from theory. Because
multi-product firms internalize business stealing, mergers may encourage firms
to reposition products, leading to more, not less, variety. Using data on
reporter assignments from 1993-1999, results show that differentiation and
variety increase with concentration. Moreover, there is evidence that
additional variety increases readership, suggesting that concentration benefits
consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108015</id><created>2001-08-22</created><authors><author><keyname>Rosenfeld</keyname><forenames>Jeffrey M.</forenames></author></authors><title>Spiders and Crawlers and Bots, Oh My: The Economic Efficiency and Public
  Policy of Contracts that Restrict Data Collection</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m</acm-class><abstract>  Recent trends reveal the search by companies for a legal hook to prevent the
undesired and unauthorized copying of information posted on websites. In the
center of this controversy are metasites, websites that display prices for a
variety of vendors. Metasites function by implementing shopbots, which extract
pricing data from other vendors' websites. Technological mechanisms have proved
unsuccessful in blocking shopbots, and in response, websites have asserted a
variety of legal claims. Two recent cases, which rely on the troublesome
trespass to chattels doctrine, suggest that contract law may provide a less
demanding legal method of preventing the search of websites by data robots. If
blocking collection of pricing data is as simple as posting an online contract,
the question arises whether this end result is desirable and legally viable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108016</id><created>2001-08-24</created><authors><author><keyname>Qadeer</keyname><forenames>Shaz</forenames></author></authors><title>Verifying Sequential Consistency on Shared-Memory Multiprocessors by
  Model Checking</title><categories>cs.DC cs.AR</categories><comments>29 pages</comments><acm-class>B.3.3; C.1.2</acm-class><abstract>  The memory model of a shared-memory multiprocessor is a contract between the
designer and programmer of the multiprocessor. The sequential consistency
memory model specifies a total order among the memory (read and write) events
performed at each processor. A trace of a memory system satisfies sequential
consistency if there exists a total order of all memory events in the trace
that is both consistent with the total order at each processor and has the
property that every read event to a location returns the value of the last
write to that location.
  Descriptions of shared-memory systems are typically parameterized by the
number of processors, the number of memory locations, and the number of data
values. It has been shown that even for finite parameter values, verifying
sequential consistency on general shared-memory systems is undecidable. We
observe that, in practice, shared-memory systems satisfy the properties of
causality and data independence. Causality is the property that values of read
events flow from values of write events. Data independence is the property that
all traces can be generated by renaming data values from traces where the
written values are distinct from each other. If a causal and data independent
system also has the property that the logical order of write events to each
location is identical to their temporal order, then sequential consistency can
be verified algorithmically. Specifically, we present a model checking
algorithm to verify sequential consistency on such systems for a finite number
of processors and memory locations and an arbitrary number of data values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108017</id><created>2001-08-25</created><authors><author><keyname>Rubin</keyname><forenames>Aviel D.</forenames></author></authors><title>Security Considerations for Remote Electronic Voting over the Internet</title><categories>cs.CR</categories><acm-class>K.4.1</acm-class><abstract>  This paper discusses the security considerations for remote electronic voting
in public elections. In particular, we examine the feasibility of running
national federal elections over the Internet. The focus of this paper is on the
limitations of the current deployed infrastructure in terms of the security of
the hosts and the Internet itself. We conclude that at present, our
infrastructure is inadequate for remote Internet voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108018</id><created>2001-08-27</created><authors><author><keyname>Zha</keyname><forenames>H.</forenames></author><author><keyname>He</keyname><forenames>X.</forenames></author><author><keyname>Ding</keyname><forenames>C.</forenames></author><author><keyname>Gu</keyname><forenames>M.</forenames></author><author><keyname>Simon</keyname><forenames>H.</forenames></author></authors><title>Bipartite graph partitioning and data clustering</title><categories>cs.IR cs.LG</categories><comments>Proceedings of ACM CIKM 2001, the Tenth International Conference on
  Information and Knowledge Management, 2001</comments><acm-class>H.3.3; G.1.3; G.2.2</acm-class><abstract>  Many data types arising from data mining applications can be modeled as
bipartite graphs, examples include terms and documents in a text corpus,
customers and purchasing items in market basket analysis and reviewers and
movies in a movie recommender system. In this paper, we propose a new data
clustering method based on partitioning the underlying bipartite graph. The
partition is constructed by minimizing a normalized sum of edge weights between
unmatched pairs of vertices of the bipartite graph. We show that an approximate
solution to the minimization problem can be obtained by computing a partial
singular value decomposition (SVD) of the associated edge weight matrix of the
bipartite graph. We point out the connection of our clustering algorithm to
correspondence analysis used in multivariate analysis. We also briefly discuss
the issue of assigning data objects to multiple clusters. In the experimental
results, we apply our clustering algorithm to the problem of document
clustering to illustrate its effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108019</id><created>2001-08-27</created><authors><author><keyname>Ong</keyname><forenames>E.</forenames></author><author><keyname>Lusk</keyname><forenames>E.</forenames></author><author><keyname>Gropp</keyname><forenames>W.</forenames></author></authors><title>Scalable Unix Commands for Parallel Processors: A High-Performance
  Implementation</title><categories>cs.DC</categories><comments>9 pages, 2 figures</comments><report-no>ANL/MCS-P885-0601</report-no><acm-class>D.1.3</acm-class><journal-ref>in Recent Advances in Parallel Virtual Machine and Message Passing
  Interface, eds. Y. Cotronis and J. Dongarra, Lecture Notes in Computer
  Science, Vol. 2131, Springer-Verlag, pp. 410-418, Sept. 2001.</journal-ref><abstract>  We describe a family of MPI applications we call the Parallel Unix Commands.
These commands are natural parallel versions of common Unix user commands such
as ls, ps, and find, together with a few similar commands particular to the
parallel environment. We describe the design and implementation of these
programs and present some performance results on a 256-node Linux cluster. The
Parallel Unix Commands are open source and freely available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108020</identifier>
 <datestamp>2010-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108020</id><created>2001-08-27</created><updated>2002-07-24</updated><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author></authors><title>Flipping Cubical Meshes</title><categories>cs.CG</categories><comments>20 pages, 24 figures. Expanded journal version of paper from 10th
  International Meshing Roundtable. This version removes some unwanted
  paragraph breaks from the previous version; the text is unchanged</comments><acm-class>F.2.2; G.1.8</acm-class><journal-ref>Engineering with Computers 18(3):173-187, 2002</journal-ref><doi>10.1007/s003660200016</doi><abstract>  We define and examine flip operations for quadrilateral and hexahedral
meshes, similar to the flipping transformations previously used in triangular
and tetrahedral mesh generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108021</id><created>2001-08-28</created><authors><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 42</title><categories>cs.CG cs.DM</categories><comments>7 pages; 72 references</comments><acm-class>F.2.2; G.2</acm-class><journal-ref>SIGACT News, 32(3) Issue, 120 Sep. 2001, 63--72</journal-ref><abstract>  A compendium of thirty previously published open problems in computational
geometry is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108022</id><created>2001-08-28</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author></authors><title>Portability of Syntactic Structure for Language Modeling</title><categories>cs.CL</categories><comments>ICASSP 2001, Salt Lake City; 4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>ICASSP 2001 Proceedings</journal-ref><abstract>  The paper presents a study on the portability of statistical syntactic
knowledge in the framework of the structured language model (SLM). We
investigate the impact of porting SLM statistics from the Wall Street Journal
(WSJ) to the Air Travel Information System (ATIS) domain. We compare this
approach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data
and to using a small amount of data manually parsed at UPenn for gathering the
intial SLM statistics. Surprisingly, despite the fact that it performs modestly
in perplexity (PPL), the model initialized on WSJ parses outperforms the other
initialization methods based on in-domain annotated data, achieving a
significant 0.4% absolute and 7% relative reduction in word error rate (WER)
over a baseline system whose word error rate is 5.8%; the improvement measured
relative to the minimum WER achievable on the N-best lists we worked with is
12%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0108023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0108023</id><created>2001-08-29</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Mahajan</keyname><forenames>Milind</forenames></author></authors><title>Information Extraction Using the Structured Language Model</title><categories>cs.CL cs.IR</categories><comments>EMNLP'01, Pittsburgh; 8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>EMNLP/NAACL 2001 Conference Proceedings</journal-ref><abstract>  The paper presents a data-driven approach to information extraction (viewed
as template filling) using the structured language model (SLM) as a statistical
parser. The task of template filling is cast as constrained parsing using the
SLM. The model is automatically trained from a set of sentences annotated with
frame/slot labels and spans. Training proceeds in stages: first a constrained
syntactic parser is trained such that the parses on training data meet the
specified semantic spans, then the non-terminal labels are enriched to contain
semantic information and finally a constrained syntactic+semantic parser is
trained on the parse trees resulting from the previous stage. Despite the small
amount of training data used, the model is shown to outperform the slot level
accuracy of a simple semantic grammar authored manually for the MiPad ---
personal information management --- task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109001</id><created>2001-09-02</created><authors><author><keyname>Tucker</keyname><forenames>J. V.</forenames><affiliation>University of Wales, Swansea</affiliation></author><author><keyname>Zucker</keyname><forenames>J. I.</forenames><affiliation>McMaster University, Hamilton, Canada</affiliation></author></authors><title>Abstract Computability, Algebraic Specification and Initiality</title><categories>cs.LO</categories><comments>To appear in ACM Transactions on Computational Logic (57 pages;
  AMSTeX)</comments><acm-class>F.1.1;F.3.1;F.4.1</acm-class><abstract>  computable functions are defined by abstract finite deterministic algorithms
on many-sorted algebras. We show that there exist finite universal algebraic
specifications that specify uniquely (up to isomorphism) (i) all abstract
computable functions on any many-sorted algebra; and (ii) all functions
effectively approximable by abstract computable functions on any metric
algebra.
  We show that there exist universal algebraic specifications for all the
classically computable functions on the set R of real numbers. The algebraic
specifications used are mainly bounded universal equations and conditional
equations. We investigate the initial algebra semantics of these
specifications, and derive situations where algebraic specifications define
precisely the computable functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109002</id><created>2001-09-03</created><authors><author><keyname>Herescu</keyname><forenames>Oltea Mihaela</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>Probabilistic asynchronous pi-calculus</title><categories>cs.PL</categories><comments>Report version (longer and more complete than the FoSSaCs 2000
  version)</comments><acm-class>D.1.3;D.3.2;D.3.3</acm-class><journal-ref>Jerzy Tiuryn, editor, Proceedings of FOSSACS 2000 (Part of ETAPS
  2000), volume 1784 of Lecture Notes in Computer Science, pages 146--160.
  Springer-Verlag, 2000</journal-ref><abstract>  We propose an extension of the asynchronous pi-calculus with a notion of
random choice. We define an operational semantics which distinguishes between
probabilistic choice, made internally by the process, and nondeterministic
choice, made externally by an adversary scheduler. This distinction will allow
us to reason about the probabilistic correctness of algorithms under certain
schedulers. We show that in this language we can solve the electoral problem,
which was proved not possible in the asynchronous $\pi$-calculus. Finally, we
show an implementation of the probabilistic asynchronous pi-calculus in a
Java-like language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109003</id><created>2001-09-03</created><authors><author><keyname>Herescu</keyname><forenames>Oltea Mihaela</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>On the generalized dining philosophers problem</title><categories>cs.PL</categories><acm-class>D.4.1;C.2.4</acm-class><journal-ref>Proc. of the 20th ACM Symposium on Principles of Distributed
  Computing (PODC), pages 81-89, ACM, 2001</journal-ref><abstract>  We consider a generalization of the dining philosophers problem to arbitrary
connection topologies. We focus on symmetric, fully distributed systems, and we
address the problem of guaranteeing progress and lockout-freedom, even in
presence of adversary schedulers, by using randomized algorithms. We show that
the well-known algorithms of Lehmann and Rabin do not work in the generalized
case, and we propose an alternative algorithm based on the idea of letting the
philosophers assign a random priority to their adjacent forks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109004</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109004</id><created>2001-09-04</created><authors><author><keyname>Luo</keyname><forenames>X. Q.</forenames><affiliation>Zhongshan University</affiliation></author><author><keyname>Gregory</keyname><forenames>E. B.</forenames><affiliation>Zhongshan University</affiliation></author><author><keyname>Yang</keyname><forenames>J. C.</forenames><affiliation>Guoxun Ltd</affiliation></author><author><keyname>Wang</keyname><forenames>Y. L.</forenames><affiliation>Guoxun Ltd</affiliation></author><author><keyname>Chang</keyname><forenames>D.</forenames><affiliation>Guoxun Ltd</affiliation></author><author><keyname>Lin</keyname><forenames>Y.</forenames><affiliation>Guoxun Ltd</affiliation></author></authors><title>Parallel Computing on a PC Cluster</title><categories>cs.DC hep-ph</categories><comments>3 pages, uses Latex and aipproc.cls</comments><acm-class>D.1.3</acm-class><journal-ref>Advanced Computing and Analysis Techniques in Physics Research:
  VII International Workshop; ACAT 2000, American Institute of Physics (2001)
  270-272</journal-ref><doi>10.1063/1.1405325</doi><abstract>  The tremendous advance in computer technology in the past decade has made it
possible to achieve the performance of a supercomputer on a very small budget.
We have built a multi-CPU cluster of Pentium PC capable of parallel
computations using the Message Passing Interface (MPI). We will discuss the
configuration, performance, and application of the cluster to our work in
physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109005</id><created>2001-09-05</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames><affiliation>University of Southern California</affiliation></author></authors><title>Architectural Framework for Large-Scale Multicast in Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>10 pages, 4 figures</comments><acm-class>C.2; C.2.1</acm-class><abstract>  Emerging ad hoc networks are infrastructure-less networks consisting of
wireless devices with various power constraints, capabilities and mobility
characteristics. An essential capability in future ad hoc networks is the
ability to provide scalable multicast services. This paper presents a novel
adaptive architecture to support multicast services in large-scale wide-area ad
hoc networks. Existing works on multicast in ad hoc networks address only small
size networks. Our main design goals are scalability, robustness and
efficiency. We propose a self-configuring hierarchy extending zone-based
routing with the notion of contacts based on the small world graphs phenomenon
and new metrics of stability and mobility. We introduce a new geographic-based
multicast address allocation scheme coupled with adaptive anycast based on
group popularity. Our scheme is the first of its kind and promises efficient
and robust operation in the common case. Also, based on the new concept of
rendezvous regions, we provide a bootstrap mechanism for the multicast service;
a challenge generally ignored in previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109006</id><created>2001-09-05</created><authors><author><keyname>Eiter</keyname><forenames>T.</forenames></author><author><keyname>Fink</keyname><forenames>M.</forenames></author><author><keyname>Sabbatini</keyname><forenames>G.</forenames></author><author><keyname>Tompits</keyname><forenames>H.</forenames></author></authors><title>On Properties of Update Sequences Based on Causal Rejection</title><categories>cs.AI</categories><comments>59 pages, 2 figures, 3 tables, to be published in &quot;Theory and
  Practice of Logic Programming&quot;</comments><acm-class>I.2</acm-class><abstract>  We consider an approach to update nonmonotonic knowledge bases represented as
extended logic programs under answer set semantics. New information is
incorporated into the current knowledge base subject to a causal rejection
principle enforcing that, in case of conflicts, more recent rules are preferred
and older rules are overridden. Such a rejection principle is also exploited in
other approaches to update logic programs, e.g., in dynamic logic programming
by Alferes et al. We give a thorough analysis of properties of our approach, to
get a better understanding of the causal rejection principle. We review
postulates for update and revision operators from the area of theory change and
nonmonotonic reasoning, and some new properties are considered as well. We then
consider refinements of our semantics which incorporate a notion of minimality
of change. As well, we investigate the relationship to other approaches,
showing that our approach is semantically equivalent to inheritance programs by
Buccafurri et al. and that it coincides with certain classes of dynamic logic
programs, for which we provide characterizations in terms of graph conditions.
Therefore, most of our results about properties of causal rejection principle
apply to these approaches as well. Finally, we deal with computational
complexity of our approach, and outline how the update semantics and its
refinements can be implemented on top of existing logic programming engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109007</id><created>2001-09-05</created><updated>2001-09-25</updated><authors><author><keyname>Noll</keyname><forenames>A. Michael</forenames></author></authors><title>Voice vs Data: Estimates of Media Usage and Network Traffic</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-005</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The popular conception is that data traffic nearly, if not already, exceeds
voice traffic on backbone networks. However, the results of research reported
in this paper imply that voice traffic greatly exceeds data traffic when real
users are asked to estimate their usage of a wide variety of media. Media usage
was surveyed for students in New York City and in Los Angeles. Other than
significant differences in radio listening, e-mails, and downloads, the usage
was quite similar. Telephone usage (wired and wireless) was nearly an hour per
day. When converted to bits, the telephone traffic was much greater than the
data traffic over the Internet. This paper reports on the details of the two
user studies. The traffic implications of the results are estimated. The
finding that voice exceeds data will then be reconciled with the popular
opposite conception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109008</id><created>2001-09-06</created><authors><author><keyname>Mini</keyname><forenames>Federico</forenames></author></authors><title>The Role of Incentives for Opening Monopoly Markets: Comparing GTE and
  BOC Cooperation with Local Entrants</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-100</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  While the 1996 Telecommunications Act requires all incumbent local telephone
companies to cooperate with local entrants, section 271 of the Act provides the
Bell companies (but not GTE) additional incentives to cooperate. Using an
original data set, I compare the negotiations of AT&amp;T, as a local entrant, with
GTE and with the Bell companies in states where both operate. My results
suggest that the differential incentives matter: The Bells accommodate entry
more than does GTE, as evidenced in quicker agreements, less litigation, and
more favorable prices offered for network access. Consistent with this, there
is more entry into Bell territories
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109009</id><created>2001-09-06</created><authors><author><keyname>Gandal</keyname><forenames>Neil</forenames></author><author><keyname>Shapiro</keyname><forenames>Carl</forenames></author></authors><title>The Effect of Native Language on Internet Usage</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-038</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Our goal is to distinguish between the following two hypotheses: (A) The
Internet will remain disproportionately in English and will, over time, cause
more people to learn English as second language and thus solidify the role of
English as a global language. This outcome will prevail even though there are
more native Chinese and Spanish speakers than there are native English
speakers. (B) As the Internet matures, it will more accurately reflect the
native languages spoken around the world (perhaps weighted by purchasing power)
and will not promote English as a global language.
  English's &quot;early lead&quot; on the web is more likely to persist if those who are
not native English speakers frequently access the large number of English
language web sites that are currently available. In that case, many existing
web sites will have little incentive to develop non-English versions of their
sites, and new sites will tend to gravitate towards English. The key empirical
question, therefore, is whether individuals whose native language is not
English use the Web, or certain types of Web sites, less than do native English
speakers. In order to examine this issue empirically, we employ a unique data
set on Internet use at the individual level in Canada from Media Metrix. Canada
provides an ideal setting to examine this issue because English is one of the
two official languages.
  Our preliminary results suggest that English web sites are not a barrier to
Internet use for French-speaking Quebecois. These preliminary results are
consistent with the scenario in which the Internet will promote English as a
global language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109010</id><created>2001-09-09</created><updated>2002-09-13</updated><authors><author><keyname>Webber</keyname><forenames>Bonnie</forenames></author><author><keyname>Stone</keyname><forenames>Matthew</forenames></author><author><keyname>Joshi</keyname><forenames>Aravind</forenames></author><author><keyname>Knott</keyname><forenames>Alistair</forenames></author></authors><title>Anaphora and Discourse Structure</title><categories>cs.CL</categories><comments>45 pages, 17 figures. Revised resubmission to Computational
  Linguistics</comments><acm-class>I.2.7</acm-class><abstract>  We argue in this paper that many common adverbial phrases generally taken to
signal a discourse relation between syntactically connected units within
discourse structure, instead work anaphorically to contribute relational
meaning, with only indirect dependence on discourse structure. This allows a
simpler discourse structure to provide scaffolding for compositional semantics,
and reveals multiple ways in which the relational meaning conveyed by adverbial
connectives can interact with that associated with discourse structure. We
conclude by sketching out a lexicalised grammar for discourse that facilitates
discourse interpretation as a product of compositional rules, anaphor
resolution and inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109011</id><created>2001-09-09</created><authors><author><keyname>Naor</keyname><forenames>Moni</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author></authors><title>Communication Complexity and Secure Function Evaluation</title><categories>cs.CR cs.CC</categories><acm-class>D.4.6</acm-class><abstract>  We suggest two new methodologies for the design of efficient secure
protocols, that differ with respect to their underlying computational models.
In one methodology we utilize the communication complexity tree (or branching
for f and transform it into a secure protocol. In other words, &quot;any function f
that can be computed using communication complexity c can be can be computed
securely using communication complexity that is polynomial in c and a security
parameter&quot;. The second methodology uses the circuit computing f, enhanced with
look-up tables as its underlying computational model. It is possible to
simulate any RAM machine in this model with polylogarithmic blowup. Hence it is
possible to start with a computation of f on a RAM machine and transform it
into a secure protocol.
  We show many applications of these new methodologies resulting in protocols
efficient either in communication or in computation. In particular, we
exemplify a protocol for the &quot;millionaires problem&quot;, where two participants
want to compare their values but reveal no other information. Our protocol is
more efficient than previously known ones in either communication or
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109012</id><created>2001-09-10</created><authors><author><keyname>Geist</keyname><forenames>Michael</forenames></author></authors><title>Is There a There There: Towards Greater Certainty for Internet
  Jurisdiction</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-017</report-no><acm-class>K.4.m Miscellaneous</acm-class><journal-ref>16 (3) Berkeley Tech. LJ (forthcoming 2001)</journal-ref><abstract>  The unique challenge presented by the Internet is that compliance with local
laws is rarely sufficient to assure a business that it has limited its exposure
to legal risk. The paper identifies why the challenge of adequately accounting
for the legal risk arising from Internet jurisdiction has been aggravated in
recent years by the adoption of the Zippo legal framework, commonly referred to
as the passive versus active test. The test provides parties with only limited
guidance and often results in detrimental judicial decisions from a policy
perspective. Given the inadequacies of the Zippo passive versus active test,
the paper argues that it is now fitting to identify a more effective standard
for determining when it is appropriate to assert jurisdiction in cases
involving predominantly Internet-based contacts. The solution submitted in the
paper is to move toward a targeting-based analysis. Unlike the Zippo approach,
a targeting analysis would seek to identify the intentions of the parties and
to assess the steps taken to either enter or avoid a particular jurisdiction.
Targeting would also lessen the reliance on effects-based analysis, the source
of considerable uncertainty since Internet-based activity can ordinarily be
said to create some effects in most jurisdictions. To identify the appropriate
criteria for a targeting test, the paper recommends returning to the core
jurisdictional principle -- foreseeability. Foreseeability in the targeting
context depends on three factors -- contracts, technology, and actual or
implied knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109013</id><created>2001-09-11</created><authors><author><keyname>Gangemi</keyname><forenames>Aldo</forenames></author><author><keyname>Guarino</keyname><forenames>Nicola</forenames></author><author><keyname>Oltramari</keyname><forenames>Alessandro</forenames></author></authors><title>Conceptual Analysis of Lexical Taxonomies: The Case of WordNet Top-Level</title><categories>cs.CL cs.IR</categories><comments>12 pages, 2 figures, 2 tables, submitted to FOIS 2001</comments><acm-class>H3.1</acm-class><abstract>  In this paper we propose an analysis and an upgrade of WordNet's top-level
synset taxonomy. We briefly review WordNet and identify its main semantic
limitations. Some principles from a forthcoming OntoClean methodology are
applied to the ontological analysis of WordNet. A revised top-level taxonomy is
proposed, which is meant to be more conceptually rigorous, cognitively
transparent, and efficiently exploitable in several applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109014</id><created>2001-09-13</created><authors><author><keyname>van der Linden</keyname><forenames>Janet</forenames><affiliation>The Open University</affiliation></author></authors><title>Assigning Satisfaction Values to Constraints: An Algorithm to Solve
  Dynamic Meta-Constraints</title><categories>cs.PL cs.AI</categories><comments>11 pages. Proceedings ERCIM WG on Constraints (Prague, June 2001)</comments><acm-class>D.3.2; D3.3</acm-class><abstract>  The model of Dynamic Meta-Constraints has special activity constraints which
can activate other constraints. It also has meta-constraints which range over
other constraints. An algorithm is presented in which constraints can be
assigned one of five different satisfaction values, which leads to the
assignment of domain values to the variables in the CSP. An outline of the
model and the algorithm is presented, followed by some initial results for two
problems: a simple classic CSP and the Car Configuration Problem. The algorithm
is shown to perform few backtracks per solution, but to have overheads in the
form of historical records required for the implementation of state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109015</id><created>2001-09-13</created><authors><author><keyname>Carreras</keyname><forenames>Xavier</forenames></author><author><keyname>Marquez</keyname><forenames>Lluis</forenames></author></authors><title>Boosting Trees for Anti-Spam Email Filtering</title><categories>cs.CL</categories><comments>7 pages, 13 figures</comments><acm-class>I.2.7;I.5.4</acm-class><journal-ref>Proceedings of RANLP-2001, pp. 58-64, Bulgaria, 2001</journal-ref><abstract>  This paper describes a set of comparative experiments for the problem of
automatically filtering unwanted electronic mail messages. Several variants of
the AdaBoost algorithm with confidence-rated predictions [Schapire &amp; Singer,
99] have been applied, which differ in the complexity of the base learners
considered. Two main conclusions can be drawn from our experiments: a) The
boosting-based methods clearly outperform the baseline learning algorithms
(Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very
high levels of the F1 measure; b) Increasing the complexity of the base
learners allows to obtain better ``high-precision'' classifiers, which is a
very important issue when misclassification costs are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109016</id><created>2001-09-13</created><authors><author><keyname>Webbink</keyname><forenames>Douglas W.</forenames></author></authors><title>Communications Convergence, Spectrum Use and Regulatory Constraints, Or
  Property Rights, Flexible Spectrum Use and Satellite v. Terrestrial Uses and
  Users</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-030</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  As far as many consumers and businessmen and women are concerned,
increasingly wireline and wireless services, including those provided by
terrestrial and satellite systems, are considered to be substitutes and
sometimes complements, regardless of the laws and regulations applicable to
them. At the same time, many writers and even government agencies (such as the
FCC) have suggested that users of the spectrum should be given more
property-like rights in the use of the spectrum and at a minimum should be
given much more flexibility in how they may use the spectrum. Two recent
developments have important implications with respect to &quot;convergence,&quot;
spectrum property rights and flexible use of the spectrum. The first
development involves several proposals to provide terrestrial wireless services
within spectrum in use or planned to be used to provide satellite services. The
second development is the passage of the 2000 ORBIT Act which specifically
forbids the use of license auctions to select among mutually exclusive
applicants to provide international or global satellite communications service.
The purpose of this paper is to discuss some of the questions raised by these
two events, but not necessarily to provide definitive answers or solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109017</id><created>2001-09-13</created><authors><author><keyname>Gropp</keyname><forenames>William D.</forenames></author></authors><title>Learning from the Success of MPI</title><categories>cs.DC</categories><comments>12 pages, 1 figure</comments><report-no>ANL/MCS-P903-0801</report-no><acm-class>D.1.3</acm-class><abstract>  The Message Passing Interface (MPI) has been extremely successful as a
portable way to program high-performance parallel computers. This success has
occurred in spite of the view of many that message passing is difficult and
that other approaches, including automatic parallelization and directive-based
parallelism, are easier to use. This paper argues that MPI has succeeded
because it addresses all of the important issues in providing a parallel
programming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109018</id><created>2001-09-14</created><authors><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Exact Complexity of Exact-Four-Colorability</title><categories>cs.CC</categories><comments>5 pages</comments><acm-class>F.1.3; F.2.2</acm-class><abstract>  Let $M_k \seq \nats$ be a given set that consists of $k$ noncontiguous
integers. Define $\exactcolor{M_k}$ to be the problem of determining whether
$\chi(G)$, the chromatic number of a given graph $G$, equals one of the $k$
elements of the set $M_k$ exactly. In 1987, Wagner \cite{wag:j:min-max} proved
that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete, where $M_k = \{6k+1, 6k+3,
&gt;..., 8k-1 \}$ and $\bhlevel{2k}$ is the $2k$th level of the boolean hierarchy
over $\np$. In particular, for $k = 1$, it is DP-complete to determine whether
$\chi(G) = 7$, where $\DP = \bhlevel{2}$. Wagner raised the question of how
small the numbers in a $k$-element set $M_k$ can be chosen such that
$\exactcolor{M_k}$ still is $\bhlevel{2k}$-complete. In particular, for $k =
1$, he asked if it is DP-complete to determine whether $\chi(G) = 4$. In this
note, we solve this question of Wagner and determine the precise threshold $t
\in \{4, 5, 6, 7\}$ for which the problem $\exactcolor{\{t\}}$ jumps from NP to
DP-completeness: It is DP-complete to determine whether $\chi(G) = 4$, yet
$\exactcolor{\{3\}}$ is in $\np$. More generally, for each $k \geq 1$, we show
that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete for $M_k = \{3k+1, 3k+3,...,
5k-1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109019</id><created>2001-09-14</created><authors><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author><author><keyname>Metz</keyname><forenames>Edu</forenames></author><author><keyname>Ran</keyname><forenames>Alexander</forenames></author></authors><title>Tracing Execution of Software for Design Coverage</title><categories>cs.SE</categories><comments>Short version of this paper to be published in Proceedings of 16th
  IEEE International Conference on Automated Software Engineering (ASE 2001).
  13 pages, 9 figures</comments><acm-class>D.2.5</acm-class><abstract>  Test suites are designed to validate the operation of a system against
requirements. One important aspect of a test suite design is to ensure that
system operation logic is tested completely. A test suite should drive a system
through all abstract states to exercise all possible cases of its operation.
This is a difficult task. Code coverage tools support test suite designers by
providing the information about which parts of source code are covered during
system execution. Unfortunately, code coverage tools produce only source code
coverage information. For a test engineer it is often hard to understand what
the noncovered parts of the source code do and how they relate to requirements.
We propose a generic approach that provides design coverage of the executed
software simplifying the development of new test suites. We demonstrate our
approach on common design abstractions such as statecharts, activity diagrams,
message sequence charts and structure diagrams. We implement the design
coverage using Third Eye tracing and trace analysis framework. Using design
coverage, test suites could be created faster by focussing on untested design
elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109020</id><created>2001-09-15</created><authors><author><keyname>Vaillant</keyname><forenames>Pascal</forenames></author></authors><title>Modelling Semantic Association and Conceptual Inheritance for Semantic
  Analysis</title><categories>cs.CL</categories><comments>8 pages, 2 figures, LaTeX 2e using Springer LNCS class, with packages
  epsf and amssymb. Proceedings of the 4th International Conference on Text,
  Speech and Dialogue (TSD 2001), Zelezna Ruda, Czech Republic, 10-13 Sept.
  2001</comments><acm-class>H.3.1; I.2.7</acm-class><journal-ref>Springer LNCS (LNAI) 2166 (2001), 54-61</journal-ref><abstract>  Allowing users to interact through language borders is an interesting
challenge for information technology. For the purpose of a computer assisted
language learning system, we have chosen icons for representing meaning on the
input interface, since icons do not depend on a particular language. However, a
key limitation of this type of communication is the expression of articulated
ideas instead of isolated concepts. We propose a method to interpret sequences
of icons as complex messages by reconstructing the relations between concepts,
so as to build conceptual graphs able to represent meaning and to be used for
natural language sentence generation. This method is based on an electronic
dictionary containing semantic information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109021</id><created>2001-09-17</created><updated>2001-09-17</updated><authors><author><keyname>Mueller</keyname><forenames>Milton L.</forenames></author></authors><title>Competing DNS Roots: Creative Destruction or Just Plain Destruction?</title><categories>cs.CY</categories><report-no>TPRC-2001-029</report-no><acm-class>K.4.m</acm-class><abstract>  The Internet Domain Name System (DNS) is a hierarchical name space that
enables the assignment of unique, mnemonic identifiers to Internet hosts and
the consistent mapping of these names to IP addresses. The root of the domain
name system is the top of the hierarchy and is currently managed by a
quasi-private centralized regulatory authority, the Internet Corporation for
Assigned Names and Numbers (ICANN). This paper identifies and discusses the
economic and policy issues raised by competing DNS roots. The paper provides a
precise definition of root-competition and shows that multiple roots are a
species of standards competition, in which network externalities play a major
role. The paper performs a structural analysis of the different forms that
competing DNS roots can take and their effects on end-user compatibility. It
then explores the policy implications of the various forms of competition.
  The thesis of the paper is that root competition is caused by a severe
disjunction between the demand for and supply of top-level domain names. ICANN
has authorized a tiny number of new top-level domains (7) and subjected their
operators to excruciatingly slow and expensive contractual negotiations. The
growth of alternate DNS roots is an attempt to bypass that bottleneck. The
paper arrives at the policy conclusion that competition among DNS roots should
be permitted and is a healthy outlet for inefficiency or abuses of power by the
dominant root administrator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109022</id><created>2001-09-17</created><authors><author><keyname>Muller</keyname><forenames>Tomas</forenames></author><author><keyname>Bartak</keyname><forenames>Roman</forenames></author></authors><title>Interactive Timetabling</title><categories>cs.PL cs.AI</categories><comments>12 pages. Proceedings ERCIM WG on Constraints (Prague, June 2001)</comments><acm-class>D.3.2; D.3.3; F2.2</acm-class><abstract>  Timetabling is a typical application of constraint programming whose task is
to allocate activities to slots in available resources respecting various
constraints like precedence and capacity. In this paper we present a basic
concept, a constraint model, and the solving algorithms for interactive
timetabling. Interactive timetabling combines automated timetabling (the
machine allocates the activities) with user interaction (the user can interfere
with the process of timetabling). Because the user can see how the timetabling
proceeds and can intervene this process, we believe that such approach is more
convenient than full automated timetabling which behaves like a black-box. The
contribution of this paper is twofold: we present a generic model to describe
timetabling (and scheduling in general) problems and we propose an interactive
algorithm for solving such problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109023</id><created>2001-09-17</created><authors><author><keyname>Atserias</keyname><forenames>Jordi</forenames></author><author><keyname>Padro</keyname><forenames>Lluis</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Integrating Multiple Knowledge Sources for Robust Semantic Parsing</title><categories>cs.CL cs.AI</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of Euroconference on Recent Advances in Natural
  Language Processing (RANLP'01), p.8-14. Tzigov Chark, Bulgaria. Sept. 2001</journal-ref><abstract>  This work explores a new robust approach for Semantic Parsing of unrestricted
texts. Our approach considers Semantic Parsing as a Consistent Labelling
Problem (CLP), allowing the integration of several knowledge types (syntactic
and semantic) obtained from different sources (linguistic and statistic). The
current implementation obtains 95% accuracy in model identification and 72% in
case-role filling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109024</id><created>2001-09-17</created><authors><author><keyname>Beffara</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Kacem</keyname><forenames>Hassen</forenames></author><author><keyname>Kirchner</keyname><forenames>Claude</forenames></author></authors><title>Verification of Timed Automata Using Rewrite Rules and Strategies</title><categories>cs.PL</categories><acm-class>I.2.3</acm-class><abstract>  ELAN is a powerful language and environment for specifying and prototyping
deduction systems in a language based on rewrite rules controlled by
strategies. Timed automata is a class of continuous real-time models of
reactive systems for which efficient model-checking algorithms have been
devised. In this paper, we show that these algorithms can very easily be
prototyped in the ELAN system. This paper argues through this example that
rewriting based systems relying on rules and strategies are a good framework to
prototype, study and test rather efficiently symbolic model-checking
algorithms, i.e. algorithms which involve combination of graph exploration
rules, deduction rules, constraint solving techniques and decision procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109025</id><created>2001-09-18</created><authors><author><keyname>Bartak</keyname><forenames>Roman</forenames></author></authors><title>Dynamic Global Constraints: A First View</title><categories>cs.PL cs.AI</categories><comments>11 pages. Proceedings ERCIM WG on Constraints (Prague, June 2001)</comments><acm-class>D.3.2; D.3.3; D.1.6</acm-class><abstract>  Global constraints proved themselves to be an efficient tool for modelling
and solving large-scale real-life combinatorial problems. They encapsulate a
set of binary constraints and using global reasoning about this set they filter
the domains of involved variables better than arc consistency among the set of
binary constraints. Moreover, global constraints exploit semantic information
to achieve more efficient filtering than generalised consistency algorithms for
n-ary constraints. Continued expansion of constraint programming (CP) to
various application areas brings new challenges for design of global
constraints. In particular, application of CP to advanced planning and
scheduling (APS) requires dynamic additions of new variables and constraints
during the process of constraint satisfaction and, thus, it would be helpful if
the global constraints could adopt new variables. In the paper, we give a
motivation for such dynamic global constraints and we describe a dynamic
version of the well-known alldifferent constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109026</id><created>2001-09-18</created><authors><author><keyname>Simpson</keyname><forenames>Seamus</forenames></author><author><keyname>Wilkinson</keyname><forenames>Rorden</forenames></author></authors><title>Conceptualising Regulatory Change - Explaining Shifts in
  Telecommunications Governance</title><categories>cs.CY</categories><comments>29th TPRC conference, 2001</comments><report-no>TPRC-2001-043</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Drawing on perspectives from telecommunications policy and neo-Gramscian
understandings of international political economy, this paper offers an
explanation and analysis of the shifting patterns of regulation which have been
evident in the telecommunications sector in recent years. It aims to illustrate
explain and explore the implications of the movement of regulatory sovereignty
away from the nation-state, through regional conduits, to global organisations
in the crystallisation of a world system of telecommunications governance.
  Our central argument is that telecommunications governance has evolved from a
regulatory arena characterised, in large part, by national diversity, to one
wherein a more convergent global multilayered system is emerging. We suggest
that the epicentre of this regulatory system is the relatively new World Trade
Organisation (WTO). Working in concert with the WTO are existing
well-established nodes regulation. In further complement, we see regional
regulatory projects, notably the European Union (EU), as important conduits and
nodes of regulation in the consolidation of a global regulatory regime.
  By way of procedure, we first explore the utility of a neo-Gramscian approach
for understanding the development of global regulatory frameworks. Second, we
survey something of the recent history - and, in extension, conventional wisdom
- of telecommunications regulation at national and regional levels. Third, we
demonstrate how a multilayered system of global telecommunications regulation
has emerged centred around the regulatory authority of the WTO. Finally, we
offer our concluding comments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109027</id><created>2001-09-18</created><authors><author><keyname>Mei</keyname><forenames>Alessandro</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Routing Permutations in Partitioned Optical Passive Star Networks</title><categories>cs.DC cs.DS</categories><comments>8 pages, 3 figures</comments><acm-class>C.1.4</acm-class><abstract>  It is shown that a POPS network with g groups and d processors per group can
efficiently route any permutation among the n=dg processors. The number of
slots used is optimal in the worst case, and is at most the double of the
optimum for all permutations p such that p(i)&lt;&gt;i for all i.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109028</id><created>2001-09-18</created><authors><author><keyname>Michalareas</keyname><forenames>T.</forenames></author><author><keyname>Sacks</keyname><forenames>L.</forenames></author></authors><title>Random Walks in Routing Landscapes</title><categories>cs.NI cs.CC</categories><comments>5 pages, 4 figures</comments><acm-class>C.2.2</acm-class><abstract>  In this paper we present a combinatorial optimisation view on the routing
problem for connectionless packet networks by using the metaphor of a
landscape. We examine the main properties of the routing landscapes as we
define them and how they can help us on the evaluation of the problem
difficulty and the generation of effective algorithms. We also present the
random walk statistical technique to evaluate the main properties of those
landscapes and a number of examples to demonstrate the use of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109029</id><created>2001-09-18</created><authors><author><keyname>Agirre</keyname><forenames>E.</forenames></author><author><keyname>Martinez</keyname><forenames>D.</forenames></author></authors><title>Learning class-to-class selectional preferences</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Workshop &quot;Computational Natural Language
  Learning&quot; (CoNLL-2001). In conjunction with ACL'2001/EACL'2001. Toulouse,
  France. 6-7th July 2001</journal-ref><abstract>  Selectional preference learning methods have usually focused on word-to-class
relations, e.g., a verb selects as its subject a given nominal class. This
papers extends previous statistical models to class-to-class preferences, and
presents a model that learns selectional preferences for classes of verbs. The
motivation is twofold: different senses of a verb may have different
preferences, and some classes of verbs can share preferences. The model is
tested on a word sense disambiguation task which uses subject-verb and
object-verb relationships extracted from a small sense-disambiguated corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109030</id><created>2001-09-18</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Knowledge Sources for Word Sense Disambiguation</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Fourth International Conference TSD 2001, Plzen
  (Pilsen), Czech Republic, September 2001. Published in the Springer Verlag
  Lecture Notes in Computer Science series. Vaclav Matousek, Pavel Mautner,
  Roman Moucek, Karel Tauser (eds.)</journal-ref><abstract>  Two kinds of systems have been defined during the long history of WSD:
principled systems that define which knowledge types are useful for WSD, and
robust systems that use the information sources at hand, such as, dictionaries,
light-weight ontologies or hand-tagged corpora. This paper tries to systematize
the relation between desired knowledge types and actual information sources. We
also compare the results for a wide range of algorithms that have been
evaluated on a common test setting in our research group. We hope that this
analysis will help change the shift from systems based on information sources
to systems based on knowledge sources. This study might also shed some light on
semi-automatic acquisition of desired knowledge types from existing resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109031</id><created>2001-09-18</created><updated>2001-09-19</updated><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Ansa</keyname><forenames>Olatz</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Enriching WordNet concepts with topic signatures</title><categories>cs.CL</categories><comments>Author list corrected</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the NAACL workshop on WordNet and Other lexical
  Resources: Applications, Extensions and Customizations. Pittsburg, 2001</journal-ref><abstract>  This paper explores the possibility of enriching the content of existing
ontologies. The overall goal is to overcome the lack of topical links among
concepts in WordNet. Each concept is to be associated to a topic signature,
i.e., a set of related words with associated weights. The signatures can be
automatically constructed from the WWW or from sense-tagged corpora. Both
approaches are compared and evaluated on a word sense disambiguation task. The
results show that it is possible to construct clean signatures from the WWW
using some filtering techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109032</id><created>2001-09-18</created><authors><author><keyname>Katz</keyname><forenames>James</forenames></author><author><keyname>Rice</keyname><forenames>Ronald E.</forenames></author><author><keyname>Aspden</keyname><forenames>Philip</forenames></author></authors><title>The Internet, 1995-2000: Access, Civic Involvement, and Social
  Interaction</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-015</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Our research, which began fielding surveys in 1995, and which have been
repeated with variation in 1996, 1997 and 2000, was apparently the first to use
national random telephone survey methods to track social and community aspects
of Internet use, and to compare users and non-users. It also seems to be among
the first that used these methods to compare users with non-users in regards to
communication, social and community issues. The work has been largely supported
by grants from the Markle Foundation of New York City as well as the Robert
Wood Johnson Foundation.
  Abridged, see full text for complete abstract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109033</id><created>2001-09-18</created><authors><author><keyname>Fages</keyname><forenames>Francois</forenames></author></authors><title>CLP versus LS on Log-based Reconciliation Problems</title><categories>cs.PL</categories><comments>Article presented at the 6th ERCIM workshop of the Constraint Group,
  Prague, Czech Republic, June 2001</comments><acm-class>D.1.6; D.3.2</acm-class><abstract>  Nomadic applications create replicas of shared objects that evolve
independently while they are disconnected. When reconnecting, the system has to
reconcile the divergent replicas. In the log-based approach to reconciliation,
such as in the IceCube system, the input is a common initial state and logs of
actions that were performed on each replica. The output is a consistent global
schedule that maximises the number of accepted actions. The reconciler merges
the logs according to the schedule, and replays the operations in the merged
log against the initial state, yielding to a reconciled common final state.
  In this paper, we show the NP-completeness of the log-based reconciliation
problem and present two programs for solving it. Firstly, a constraint logic
program (CLP) that uses integer constraints for expressing precedence
constraints, boolean constraints for expressing dependencies between actions,
and some heuristics for guiding the search. Secondly, a stochastic local search
method with Tabu heuristic (LS), that computes solutions in an incremental
fashion but does not prove optimality. One difficulty in the LS modeling lies
in the handling of both boolean variables and integer variables, and in the
handling of the objective function which differs from a max-CSP problem.
Preliminary evaluation results indicate better performance for the CLP program
which, on somewhat realistic benchmarks, finds nearly optimal solutions up to a
thousands of actions and proves optimality up to a hundreds of actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109034</id><created>2001-09-19</created><authors><author><keyname>Kreuz</keyname><forenames>Ingo</forenames></author><author><keyname>Roller</keyname><forenames>Dieter</forenames></author></authors><title>Relevant Knowledge First - Reinforcement Learning and Forgetting in
  Knowledge Based Configuration</title><categories>cs.AI cs.LG</categories><comments>pdf-file, 33 pages, 17 figures</comments><acm-class>H.4.2; I.2.0; I.2.1; I.2.6; I.2.8</acm-class><abstract>  In order to solve complex configuration tasks in technical domains, various
knowledge based methods have been developed. However their applicability is
often unsuccessful due to their low efficiency. One of the reasons for this is
that (parts of the) problems have to be solved again and again, instead of
being &quot;learnt&quot; from preceding processes. However, learning processes bring with
them the problem of conservatism, for in technical domains innovation is a
deciding factor in competition. On the other hand a certain amount of
conservatism is often desired since uncontrolled innovation as a rule is also
detrimental. This paper proposes the heuristic RKF (Relevant Knowledge First)
for making decisions in configuration processes based on the so-called
relevance of objects in a knowledge base. The underlying relevance-function has
two components, one based on reinforcement learning and the other based on
forgetting (fading). Relevance of an object increases with its successful use
and decreases with age when it is not used. RKF has been developed to speed up
the configuration process and to improve the quality of the solutions relative
to the reward value that is given by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109035</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109035</id><created>2001-09-19</created><authors><author><keyname>Frieden</keyname><forenames>Rob</forenames></author></authors><title>Revenge of the Bell Heads: How the Net Heads Lost Control of the
  Internet</title><categories>cs.CY</categories><acm-class>K.4.m Miscellaneous</acm-class><abstract>  A dichotomy in regulatory treatment and corporate cultures exists between
Internet Service Providers (ISPs) and telecommunication carriers. Telephone
company executives (Bell Heads) may resent regulation, but they accept their
fate and work creatively to exploit anomalies and opportunities to secure a
regulation-conferred competitive advantage. Most ISP executives (Net Heads)
appear to embrace a libertarian attitude, strongly opposing any government
involvement. Despite the clash of cultures, the telecommunications and Internet
worlds have merged. Such convergence jeopardizes the ability of Net Heads to
avoid some degree of regulation, particularly when they offer services
functionally equivalent to what their Bell Head counterparts offer.
  This paper will assess the regulatory consequences when telecommunication and
Internet services converge in the marketplace and in terms of operating
technologies. The paper identifies commercial developments in the Internet to
support the view that the Internet has become more hierarchical and more like
telecommunication networks. The paper concludes that telecommunication carriers
will display superior skill in working the regulatory process to their
advantage. The paper suggests that Bell Heads will outmaneuver Net Heads
particularly when the revenue siphoning effect of Internet-mediated services
offsets the revenues generated from ISP leases of telecommunication
transmission capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109036</id><created>2001-09-19</created><updated>2001-10-17</updated><authors><author><keyname>Ennis</keyname><forenames>Sean F.</forenames></author></authors><title>Competition and Price Dispersion in International Long Distance Calling</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-024</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper examines the relationship between changes in telecommunications
provider concentration on international long distance routes and changes in
prices on those routes. Overall, decreased concentration is associated with
significantly lower prices to consumers of long distance services. However, the
relationship between concentration and price varies according to the type of
long distance plan considered. For the international flagship plans frequently
selected by more price-conscious consumers of international long distance,
increased competition on a route is associated with lower prices. In contrast,
for the basic international plans that are the default selection for consumers,
increased competition on a route is actually associated with higher prices.
Thus, somewhat surprisingly, price dispersion appears to increase as
competition increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109037</id><created>2001-09-19</created><authors><author><keyname>Lemley</keyname><forenames>Mark A.</forenames></author></authors><title>Antitrust, Intellectual Property and Standard-Setting Organizations</title><categories>cs.CY</categories><comments>29th TPRC Conference 2001</comments><report-no>TPRC-2001-001</report-no><acm-class>K.4.m</acm-class><abstract>  Standard-setting organizations (SSOs) regularly encounter situations in which
one or more companies claim to own proprietary rights that cover a proposed
industry standard. The industry cannot adopt the standard without the
permission of the intellectual property owner (or owners).
  How SSOs respond to those who assert intellectual property rights is
critically important. Whether or not private companies retain intellectual
property rights in group standards will determine whether a standard is &quot;open&quot;
or &quot;closed.&quot; It will determine who can sell compliant products, and it may well
influence whether the standard adopted in the market is one chosen by a group
or one offered by a single company. SSO rules governing intellectual property
rights will also affect how standards change as technology improves.
  Given the importance of SSO rules governing intellectual property rights,
there has been surprisingly little treatment of SSOs or their intellectual
property rules in the legal literature. My aim in this article is to fill that
void. To do so, I have surveyed the intellectual property policies of dozens of
SSOs, primarily but not exclusively in the computer networking and
telecommunications industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109038</id><created>2001-09-20</created><authors><author><keyname>Cherry</keyname><forenames>Barbara A.</forenames></author></authors><title>Crisis of Public Utility Deregulation and the Unrecognized Welfare State</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-034</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Successful achievement of public policies requires satisfaction of conditions
affecting political feasibility for policy adoption and maintenance as well as
economic viability of the desired activity or enterprise. This paper discusses
the difficulties of satisfying these joint constraints given the legacy of the
common law doctrines of &quot;just price&quot; and &quot;businesses affected with a public
interest.&quot; In this regard, it is helpful to view traditional public utility
regulation as a form of welfare state regulation, as it suffers from similar
political problems from policy retrenchment. The retrenchment problems are
examined in the context of the electricity crisis in California as well as the
passage and implementation of the Telecommunications Act of 1996. As expected,
retrenchment from low residential retail rates - the most universalistic
benefit for customers - faces the greatest political resistance. The societal
trade-offs between monopoly and competition must be reexamined in light of the
greater instability and political difficulties under a deregulatory regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109039</id><created>2001-09-20</created><authors><author><keyname>Constable</keyname><forenames>John</forenames></author><author><keyname>Aoyama</keyname><forenames>Hideaki</forenames></author></authors><title>Testing for Mathematical Lineation in Jim Crace's &quot;Quarantine&quot; and T. S.
  Eliot's &quot;Four Quartets&quot;</title><categories>cs.CL</categories><comments>19 pages, 8 figures in LaTeX2e and EPS formats</comments><acm-class>H.3.1;I.2.7;I.5.4</acm-class><abstract>  The mathematical distinction between prose and verse may be detected in
writings that are not apparently lineated, for example in T. S. Eliot's &quot;Burnt
Norton&quot;, and Jim Crace's &quot;Quarantine&quot;. In this paper we offer comments on
appropriate statistical methods for such work, and also on the nature of formal
innovation in these two texts. Additional remarks are made on the roots of
lineation as a metrical form, and on the prose-verse continuum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109040</id><created>2001-09-20</created><authors><author><keyname>Srikanta</keyname><forenames>B. J.</forenames></author><author><keyname>Haritsa</keyname><forenames>Jayant</forenames></author><author><keyname>Sen</keyname><forenames>Udaysankar</forenames></author></authors><title>The Building of BODHI, a Bio-diversity Database System</title><categories>cs.DB q-bio.PE</categories><comments>29 pages, 5 figures</comments><report-no>TR-2001-02</report-no><acm-class>H.2.4</acm-class><abstract>  We have recently built a database system called BODHI, intended to store
plant bio-diversity information. It is based on an object-oriented modeling
approach and is developed completely around public-domain software. The unique
feature of BODHI is that it seamlessly integrates diverse types of data,
including taxonomic characteristics, spatial distributions, and genetic
sequences, thereby spanning the entire range from molecular to organism-level
information. A variety of sophisticated indexing strategies are incorporated to
efficiently access the various types of data, and a rule-based query processor
is employed for optimizing query execution. In this paper, we report on our
experiences in building BODHI and on its performance characteristics for a
representative set of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109041</id><created>2001-09-20</created><updated>2001-09-27</updated><authors><author><keyname>Galperin</keyname><forenames>Hernan</forenames></author><author><keyname>Bar</keyname><forenames>Francois</forenames></author></authors><title>Open Access beyond cable: The case of Interactive TV</title><categories>cs.MM</categories><comments>typos corrected, content changed section 3(a), new abstract</comments><report-no>TPRC-2001-039</report-no><acm-class>K.4.1</acm-class><abstract>  In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented &quot;walled gardens&quot; offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109042</id><created>2001-09-21</created><updated>2001-12-26</updated><authors><author><keyname>Zheng</keyname><forenames>Qingguo</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author><author><keyname>Ma</keyname><forenames>Shilong</forenames></author></authors><title>Intelligent Search of Correlated Alarms from Database containing Noise
  Data</title><categories>cs.NI cs.AI</categories><comments>15 pages,4 figures</comments><acm-class>C.2.3</acm-class><abstract>  Alarm correlation plays an important role in improving the service and
reliability in modern telecommunications networks. Most previous research of
alarm correlation didn't consider the effect of noise data in Database. This
paper focuses on the method of discovering alarm correlation rules from
database containing noise data. We firstly define two parameters Win_freq and
Win_add as the measure of noise data and then present the Robust_search
algorithm to solve the problem. At different size of Win_freq and Win_add,
experiments with alarm data containing noise data show that the Robust_search
Algorithm can discover the more rules with the bigger size of Win_add. We also
experimentally compare two different interestingness measures of confidence and
correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109043</id><created>2001-09-21</created><authors><author><keyname>Lee</keyname><forenames>Hokyu</forenames></author><author><keyname>Sawhney</keyname><forenames>Harmeet</forenames></author></authors><title>PUC Autonomy and Policy Innovation: Local Telephone Competition in
  Arkansas and New York</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-026</report-no><acm-class>K.4.m</acm-class><abstract>  In the pre-divestiture era, the regulatory environment in the U.S. was fairly
uniform and harmonious with the FCC setting the course and the accommodative
state PUCs making corresponding changes in their own policies. The divestiture
fractured this monolithic system as it forced the PUCs to respond to new forces
unleashed in their own backyards. Soon there was great diversity in the overall
regulatory landscape. Within this new environment, there is considerable
disparity among the PUCs in terms of their ability to implement new ideas. This
paper seeks to understand the structural factors that influence the latitude of
regulatory action by PUCs via a comparative study of local telephone
competition policy making in Arkansas and New York. The analysis suggests that
the presence or absence of countervailing forces determines the relative
autonomy the PUCs enjoy and thereby their ability to introduce new ideas into
their states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109044</id><created>2001-09-22</created><authors><author><keyname>Hwang</keyname><forenames>Junseok</forenames></author><author><keyname>Mueller</keyname><forenames>Milton</forenames></author><author><keyname>Yoon</keyname><forenames>Gunyoung</forenames></author><author><keyname>Kim</keyname><forenames>Joonmin</forenames></author></authors><title>Analyzing ENUM Service and Administration from the Bottom Up: The
  addressing system for IP telephony and beyond</title><categories>cs.CY</categories><comments>29th TPRC Conference</comments><report-no>TPRC-2001-063</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  ENUM creates many new market opportunities and raises several important
policy issues related to the implementation and administration of the ENUM
database and services. Recent World Telecommunications Policy Forum 2001 dealt
with the emergence of ENUM as an important numbering issue of IP telephony.
This paper prepares some important emerging issues of ENUM administration and
policy by taking an empirical research approach from the bottom up.
  We will identify potential key ENUM services, and estimating the size of the
service market opportunities created by the availability of PSTN-IP addressing
and mapping mechanisms, particularly in the context of IP telephony. Also, we
analyze the possible administrative models and relationship scenarios among
different ENUM players such as Registry(ies), Registrars, Telephone Service
Providers, ENUM Application Service Providers, etc. Then, we will assess the
effects of various administrative model architectures of ENUM service by
looking at the market opportunities and motivations of the players. From the
empirical findings, we will draw the implications on transactions among
different kinds of ENUM service providers. Finally, the results of the model
analysis will be used for the discussion of policy related issues around the
ENUM and IP telephony services.
  Keywords: IP Telephony, ENUM, Internet Policy, Numbering and Addressing
System, Service and Market Study, Administration Model, Empirical Market Study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109045</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109045</id><created>2001-09-21</created><authors><author><keyname>Tan</keyname><forenames>Zixiang Alex</forenames></author></authors><title>Product Cycle, Wintelism, and Cross-national Production Networks (CPN)
  for Developing Countries-- China's Telecom Manufacturing Industry as A Case</title><categories>cs.CY</categories><comments>18 pages 5 figures</comments><report-no>TPRC-2001-059</report-no><acm-class>K.6.1</acm-class><abstract>  Focusing on the telecom manufacturing industry in China, this paper contends
that the existing literature needs to be expanded in order to explain the
Chinese case. First, product cycle theory could be applied to explain
multinational corporations' strategies of importing and localizing their
products in China in order to take advantage of lower labor costs and often
more significantly to break barriers to the Chinese market. Second, there are
no significant indicators pointing to local multinational subsidiaries and
indigenous manufacturers serving as a substantial part of the cross-national
production networks in the global telecom industry yet, although there are some
signs of potential development. Third, the success of &quot;Wintelism&quot; and the
maturity of cross-national production networks in the global market have had
significant impacts on China's indigenous industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109046</id><created>2001-09-22</created><authors><author><keyname>Compaine</keyname><forenames>Benjamin</forenames></author><author><keyname>Smith</keyname><forenames>Emma</forenames></author></authors><title>Internet Radio: A New Engine for Content Diversity?</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2078</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  While traditional radio stations are subject to extensive government
regulations, Internet radio stations remain largely unregulated. As Internet
radio usage has increased certain stakeholders have begun to argue that these
Internet radio broadcasters are providing significant and diverse programming
to American audiences and that government regulation of spectrum-using radio
station ownership may be further relaxed.
  One of the primary justifications for regulation of ownership has been to
protect diversity in broadcasting. This study hypothesizes that Internet radio
broadcasting does add diversity to the radio broadcasting industry and that it
should be considered as relevant by regulators.
  This study evaluates the role of Internet radio broadcasters according to
five criteria intended to gauge the level of diversity being delivered to
listeners online. By measuring the levels of format, channel, ownership,
location and language diversity among Internet radio stations, it is possible
to draw benchmark lessons about the new medium's ability to provide Americans
with diverse broadcasting options.
  The study finds that Internet radio broadcasters are in fact adding
measurable diversity to the radio broadcasting industry. Internet broadcasters
are providing audiences with access to an increasing number of stations,
owners, formats, and language choices, and it is likely that technologies
aiding in the mobility of access as well as broadband evolution will reinforce
these findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109047</id><created>2001-09-22</created><authors><author><keyname>Cohen</keyname><forenames>Tracy</forenames></author></authors><title>Between a rock and a hard place: assessing the application of domestic
  policy and South Africa's commitments under the WTO'S Basic
  Telecommunications Agreement</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-007</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  South Africa adopted the GATS Basic Agreement on Telecommunications and the
regulatory principles in 1998. Obligations undertaken by South Africa mirrored
the framework for the gradual telecommunications reform process that was begun
in 1996. In the light of two threatened actions for anti-competitive practices
in violation of the Agreement, this paper reviews the nature of the commitments
undertaken by South Africa and assesses the country's compliance to date. This
paper also seeks to explore the tension that arises between domestic policy
reforms and international trade aspirations. It is argued that the dynamic
produced through this tension affords domestic governments a mechanism with
which to balance the seemingly opposing goals of competition and development.
It is further argued that the broad regulatory principles, adopted by all
signatories and often criticized for lack of precision, facilitate this fine
balancing and affords domestic governments an opportunity to advance sovereign
concerns while pursuing international trade ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109048</id><created>2001-09-22</created><authors><author><keyname>Aufderheide</keyname><forenames>Patricia</forenames></author></authors><title>Competition and Commons: The Post-Telecom Act Public Interest, in and
  after the AOLTW Merger</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2030</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  In asserting a competitive market environment as a justification for
regulatory forbearance, the Telecommunications Act of 1996 finally articulated
a clear standard for the FCC's public interest standard, one of the most
protean concepts in communications. This seeming clarity has not, however,
inhibited intense political conflict over the term. This paper examines public
and regulatory debate over the AOL Time Warner merger as an example of the way
in which the linkage between competitions and commons policy becomes relevant
to communications policy, particularly in relation to mass media, and discusses
interpretations of the public interest in the current FCC. The paper proposes
that the Telecom Act's goal of fostering economic competition among information
service providers, and the democratic ideal of nurturing public relationships
and behaviors can be linked. Competition policy that creates the opportunity
for untrammeled interactivity also provides a sine qua non to nurture the
social phenomenon of the commons. The linked concepts of competition and
commons could also provide useful ways to interpret the public interest in
policy arenas as spectrum allocation and intellectual property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109049</id><created>2001-09-22</created><authors><author><keyname>Baer</keyname><forenames>Walter S.</forenames></author></authors><title>Signing Initiative Petitions Online: Possibilities, Problems and
  Prospects</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2054</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Many people expect the Internet to change American politics, most likely in
the direction of increasing direct citizen participation and forcing government
officials to respond more quickly to voter concerns. A recent California
initiative with these objectives would authorize use of encrypted digital
signatures over the Internet to qualify candidates, initiatives, and other
ballot measures. Proponents of Internet signature gathering say it will
significantly lower the cost of qualifying initiatives and thereby reduce the
influence of organized, well-financed interest groups. They also believe it
will increase both public participation in the political process and public
understanding about specific measures. However, opponents question whether
Internet security is adequate to prevent widespread abuse and argue that the
measure would create disadvantages for those who lack access to the Internet.
Beyond issues of security, cost, and access lie larger questions about the
effects of Internet signature gathering on direct democracy. Would it encourage
greater and more informed public participation in the political process? Or
would it flood voters with ballot measures and generally worsen current
problems with the initiative process itself? Because we lack good data on these
questions, answers to them today are largely conjectural. We can be fairly
sure, however, that Internet petition signing, like Internet voting, will have
unintended consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109050</id><created>2001-09-23</created><authors><author><keyname>Jain</keyname><forenames>Rekha S.</forenames></author><author><keyname>Das</keyname><forenames>Pinaki</forenames></author></authors><title>A Framework for Assessing Universal Service Obligations: A Developing
  Country Perspective</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-045</report-no><acm-class>K.4.m</acm-class><abstract>  A critical element of most national telecom policy objectives is advancing
universal service. In a multi-operator context, this is usually operationalized
through Universal Service Obligations (USO), by which various operators are
mandated to provide a part of their services to rural areas or to high cost to
serve customers at &quot;affordable&quot; prices.
  This paper highlights the various issues in USO from a developing country
perspective. The first part of this paper gives an overview of USO practices
and issues. The second part reviews the Telecom Regulatory Authority of India's
recommendations on USO cost estimation. In the third part, the paper analyzes
characteristics of rural exchanges with a view to evolve a framework for
assessing USO. This framework is applicable to developing countries as the
study carried out in this paper is in the context of a developing country
characterized by low telecom penetration and non-availability of data with
regulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109051</id><created>2001-09-23</created><authors><author><keyname>Waterman</keyname><forenames>David</forenames></author></authors><title>Internet TV: Business Models and Program Content</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2118</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Internet technology should eventually provide important improvements over
established media not only in the efficiency of broadband delivery, and of
particularimportance, in the efficiency of business models that can be used to
collect money for that programming. I identify five economic characteristics of
Internet technology that should lead to these greater efficiencies: (1) lower
delivery costs and reduced capacity constraints, (2) more efficient
interactivity, (3) more efficient advertising and sponsorship, (4) more
efficient direct pricing and bundling, and (5) lower costs of copying and
sharing.
  The most successful Internet TV business models are likely to involve
syndication to or from other media, and also international distribution. In the
broader context, Internet TV is another syndication outlet by which program
suppliers can segment their overall markets and thus support higher production
investments. Many innovative and more sharply focused programs will surely
prosper on Internet TV, but the attractiveness to audiences of high production
value programming will tend to advantage broad appeal programming, such as
Hollywood movies. Historical evidence about the performance of cable television
and videocassettes is presented to support these points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109052</id><created>2001-09-23</created><authors><author><keyname>Cogburn</keyname><forenames>Derrick L.</forenames></author></authors><title>Globalization and Governance in Cyberspace: Mapping the Processes of
  Emergent Regime Formation in Global Information and Communications Policy</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2277</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper develops a theoretical perspective on globalization and the
Information Society and combines it with a critical usage of international
regime theory as a heuristic for understanding the current historical period of
transition from an international telecommunicaitons regime (Cowhey, 1990; 1994)
to a new and complex regime aimed at providing governance for the Global
Information Infrastructure and Global Information Society (GII/GIS). In
analyzing the principles, values, norms, rules, collective decision-making
prdocedures, and enforcement mechanisms of the emergent GII/GIS regime, this
paper differentiates between three regime levels: (1) Macro-Regime--global; (2)
Mezzo-Regime--regional and sub-regional; and (3) Micro-Regime--national. The
paper employs a case-study approach to explore some of the specific national
responses (i.e. South Africa) to this regime transition, with an analysis of
potential best practices and lessons learned for other emerging economies. Key
findings in this paper are: (1) that a range of social, political, economic,
and technological factors are eroding the existing international
telecommunications regime (e.g., VOIP;, call-back, VSATs, accounting rate
restructuring, pressure for applicaitons development, and SMMEs); (2) a new
regime for global information and communicaitons policy is emerging, but is
being driven not by the broad possibilities of the Information Society, but by
the more specific interests of global and multi-national corporations related
to global e-commerce; (3) numerous strategic responses have been developed at
national, subregional, and regional levels to the challenges of this transition
in both developed and developing regions; and (4) without a collaborative
response, the developing world will be further marginalized by this new regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109053</id><created>2001-09-23</created><authors><author><keyname>Ward</keyname><forenames>Michael R.</forenames></author><author><keyname>Chen</keyname><forenames>Yu-Ching</forenames></author></authors><title>Price Increases from Online Privacy</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-014</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Consumers value keeping some information about them private from potential
marketers. E-commerce dramatically increases the potential for marketers to
accumulate otherwise private information about potential customers. Online
marketers claim that this information enables them to better market their
products. Policy makers are currently drafting rules to regulate the way in
which these marketers can collect, store, and share this information. However,
there is little evidence yet either of consumers' valuation of their privacy or
of the benefits they might reap through better target marketing. We provide a
framework for measuring a portion of the benefits from allowing marketers to
make better use of consumer information. Target marketing is likely to reduce
consumer search costs, improve consumer product selection decisions, and lower
the marketing costs of goods sold. Our model allows us to estimate the value to
consumers of only the latter, price reductions from more efficient marketing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109054</id><created>2001-09-24</created><authors><author><keyname>Sheu</keyname><forenames>Tair-Rong</forenames></author><author><keyname>Carley</keyname><forenames>Kathleen</forenames></author></authors><title>Monopoly Power on the Web - A Preliminary Investigation of Search Engines</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-035</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  E-Commerce challenges traditional approaches to assessing monopolistic
practices due to the rapid rate of growth, rapid change in technology,
difficulty in assessing market share for information products like web sites,
and high degree of interconnectivity and alliance formation among corporations.
This paper has provided a fundamental framework that integrates a network and
economic perspective to the search engine market.
  The findings indicate that (1) despite an increasing number of search
engines, barriers to entry seem high, largely due to the exponential growth in
the number of web sites and the non-scalability of the current search
technology and collective switching costs; (2) older search engine sites tend
typically to have more features to lock in users. Using standard economic
indicators (CR4=58% and HHI=1163), the industry looks close to being plagued by
anticompetitive practices. But based on a network adjusted HHI constructed in
this paper, its value, 870, suggests that there is less cause for concern.
  Based on all indicators, it suggests that Yahoo would be a contender. Other
possible contenders are MSN and Netscape. On the basis of results to date, some
search engines keep increasing their audience reach while others don't. The
trend shows that some search engines may dominate the search engine market. We
suggest conducting research in the coverage performance of search engines and
investigate &quot;information search cost&quot; as a performance indicator of search
techniques. In addition, we suggest paying attention to any anticompetitive
conduct (e.g. product bundling) that may lesson competition and reduce consumer
welfare. The combination of network theory and economic theory to study the
search engine market is a particularly powerful approach for E-Commerce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109055</id><created>2001-09-24</created><authors><author><keyname>Gandal</keyname><forenames>Neil</forenames><affiliation>Tel Aviv University</affiliation></author><author><keyname>Salant</keyname><forenames>David</forenames><affiliation>NERA Economic Consulting</affiliation></author><author><keyname>Waverman</keyname><forenames>Leonard</forenames><affiliation>London Business School and NERA Economic Consulting</affiliation></author></authors><title>Standardization versus Coverage in Wireless Telephone Networks</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-010</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The issue of market-based versus mandated standards has been addressed in
many settings. In most settings in which network effects are present,
compatibility across platforms has been a key determinant of the success or
failure of a particular technology. In the case of wireless telecommunications,
however, interconnection and the availability of the relevant infrastructure
can be a substitute for compatibility.
  In this paper, we examine the tradeoff between mandated standards and
interconnection. We first provide institutional background; we then empirically
examine whether, other things being equal, penetration rates were lower (or
higher) for countries with multiple incompatible digital standards. We finally
discuss the implications of our results for the current debate about 3G
standards, in which CDMA2000, which is backed mainly by US firms, competes with
WCDMA, which is backed by the European Community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109056</id><created>2001-09-24</created><updated>2001-10-24</updated><authors><author><keyname>Russell</keyname><forenames>Andrew L.</forenames></author></authors><title>Ideological and Policy Origins of the Internet, 1957-1969</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-087</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper examines the ideological and policy consensus that shaped
computing research funded by the Information Processing Techniques Office
(IPTO) within the Department of Defense's Advanced Research Projects Agency
(ARPA). This historical case study of the period between Sputnik and the
creation of the ARPANET shows how military, scientific, and academic values
shaped the institutions and relations of a foundational period in the creation
of the Internet.
  The paper probes three areas: the ideology of the science policy consensus,
the institutional philosophy of IPTO under J. C. R. Licklider, and the ways
that this consensus and philosophy shaped IPTO research in the period leading
to the creation of the ARPANET. By examining the intellectual, cultural, and
institutional details of the consensus that governed IPTO research between 1957
and 1969, we can understand the ways that these values defined the range of
possibilities for network computing.
  The influence of the social values expressed by these actors was decisive:
that government had an obligation to support a broad base of scientific
research to promote both the public good and the national defense; that
IPTO-sponsored computing research would accomplish both military and scientific
objectives; and that IPTO could leverage its power within this consensus to
create a network to share resources and unite researchers over geographical
distance. A greater awareness of the ways that &quot;consensus&quot; worked in this
period -- the &quot;pre-history&quot; of the Internet -- provides a richer context for
evaluating the unique features of the Internet, such as its open architecture,
collegial culture, and standards-based governance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109057</id><created>2001-09-24</created><updated>2001-10-25</updated><authors><author><keyname>Viard</keyname><forenames>V. Brian</forenames></author></authors><title>Do Switching Costs Make Markets More or Less Competitive?: The Case of
  800-Number Portability</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-085</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Do switching costs reduce or intensify price competition in markets where
firms charge the same price to old and new consumers? Theoretically, the answer
could be either &quot;yes&quot; or &quot;no,&quot; due to two opposing incentives in firms' pricing
decisions. The firm would like to charge a higher price to previous purchasers
who are &quot;locked-in&quot; and a lower price to unattached consumers who offer higher
future profitability. I demonstrate this ambiguity in an infinite-horizon
theoretical model.
  800- (toll-free) number portability provides empirical evidence to answer
this question. Before portability, a customer had to change numbers to change
service providers. This imposed significant switching costs on users, who
generally invested heavily to publicize these numbers. In May 1993 a new
database made 800-numbers portable. This drop in switching costs and
regulations that precluded price discrimination between old and new consumers
provide an empirical test of switching costs' effect on price competition.
  I use contracts for virtual private network (VPN) services to test how AT&amp;T
adjusted its prices for toll-free services in response to portability.
Preliminarily (awaiting completion of data collection), I find that AT&amp;T
reduced margins for VPN contracts containing toll-free services relative to
those that did not as the portability date approached. This implies that the
switching costs due to non-portability made the market less competitive. These
results suggest that, despite toll-free services growing rapidly during this
time period, AT&amp;T's incentive to charge a higher price to &quot;locked-in&quot; consumers
exceeded its incentive to capture new consumers in the high switching costs era
of non-portability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109058</id><created>2001-09-24</created><updated>2001-10-02</updated><authors><author><keyname>Previte</keyname><forenames>J.</forenames></author><author><keyname>Hearn</keyname><forenames>G.</forenames></author><author><keyname>Dann</keyname><forenames>S.</forenames></author></authors><title>Profiling Internet Users' Participation in Social Change Agendas: An
  application of Q methodology</title><categories>cs.CY</categories><comments>Paper submitted for the TPRC conference October 2001</comments><acm-class>K.4.m</acm-class><abstract>  New computer-mediated channels of communication are action oriented and have
the ability to deliver information and dialogue - moderated and unmoderated -
which can facilitate the bringing together of a series of society's
stakeholders, opinion leaders and change agents, who have the ability to
influence social action. However, existing online studies have been limited in
explaining Internet users' willingness to participate in social change agendas
online. They have relied predominately on basic demographic descriptors such as
age, education, income and access to technology and have ignored, social,
psychological and attitudinal variables that may explain online participation
and social change. The authors propose a Q methodology research approach better
evaluate Internet users participation in online social change agendas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109059</id><created>2001-09-24</created><authors><author><keyname>Kosmidis</keyname><forenames>Michelle S.</forenames></author></authors><title>Bringing the Internet to Schools: US and EU policies</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-076</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The Internet is changing rapidly the way people around the world communicate,
learn, and work. Yet the tremendous benefits of the Internet are not shared
equally by all. One way to close the gap of the &quot;digital divide&quot; is to ensure
Internet access to all schools from an early age. While both the USA and EU
have embraced the promotion of Internet access to schools, the two have decided
to finance it differently. This paper shows that the main costs of Internet
access to schools are not communications-related (telecommunications and
Internet services) but rather non-communications-related (hardware, educational
training, software). This paper goes on to discuss whether the identified costs
should be financed in any way by the universal service obligations funded by
the telecommunications industry/sector/consumers (sector specific) or a general
governmental budget (educational budget).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109060</id><created>2001-09-24</created><authors><author><keyname>Fernandez</keyname><forenames>Antonio J.</forenames></author><author><keyname>Hill</keyname><forenames>Patricia M.</forenames></author></authors><title>Branching: the Essence of Constraint Solving</title><categories>cs.PL</categories><comments>18 pages, 2 figures, Proceedings ERCIM Workshop on Constraints
  (Prague, June 2001)</comments><acm-class>D.3.3; D.3.2</acm-class><abstract>  This paper focuses on the branching process for solving any constraint
satisfaction problem (CSP). A parametrised schema is proposed that (with
suitable instantiations of the parameters) can solve CSP's on both finite and
infinite domains. The paper presents a formal specification of the schema and a
statement of a number of interesting properties that, subject to certain
conditions, are satisfied by any instances of the schema.
  It is also shown that the operational procedures of many constraint systems
including cooperative systems) satisfy these conditions.
  Moreover, the schema is also used to solve the same CSP in different ways by
means of different instantiations of its parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109061</id><created>2001-09-24</created><authors><author><keyname>Sinai</keyname><forenames>Todd</forenames></author><author><keyname>Waldfogel</keyname><forenames>Joel</forenames></author></authors><title>Geography and the Internet: Is the Internet a Substitute or a Complement
  for Cities?</title><categories>cs.CY</categories><comments>29th TPTC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  By combining persons around the world into a single market, the Internet may
serve as a substitute for urban agglomeration. That is, the Internet may level
the consumption playing field between large, variety-laden and small,
variety-starved markets. However, if local content on the Internet is more
prevalent in larger markets, then the Internet may be a complement for urban
agglomeration. Characterizing the nature of available content using Media
Metrix web page visits by about 13,500 households, we document that
substantially more online local content is available in larger markets.
Combining this with CPS Internet use data, we find statistically significant
direct evidence of both complementarity and substitutability: Individuals are
more likely to connect in markets with more local online content; and holding
local online content constant, are less likely to connect in larger markets. We
also find that individuals connect to overcome local isolation: Blacks are more
likely to connect, relative to whites, when they comprise a smaller fraction of
local population, making the Internet is a substitute for agglomeration of
preference minorities within cities, if not cities themselves. On balance we
find that the substitution and complementarity effects offset each other so
that the Internet does not promote or discourage agglomeration in larger
markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109062</id><created>2001-09-24</created><authors><author><keyname>Gupta</keyname><forenames>Rajni</forenames></author></authors><title>India Attempts to Give a Jump-start to its Derailed Telecommunications
  Liberalization Process</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><acm-class>K.4.m Miscellaneous</acm-class><abstract>  After the 1991 economic policy made a shift from a closed economic model to a
market-oriented model. The government invited private sector to participate in
reforming its telecom sector. However, the government took a half-hearted
approach in overhauling the legal and regulatory regime, suitable for
competitive regime or in framing the 1994 Telecom Policy.
  Competition was allowed in cellular and basis services. The ministry and the
incumbent (DOT) issued licenses to their competitors. Lack of transparency in
issuing licenses and unrealistic license fee derailed the reforms process and
led to wasteful litigation. The courts did not support the regulator and
virtually made its role redundant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109063</id><created>2001-09-24</created><authors><author><keyname>Cawley</keyname><forenames>Richard</forenames></author></authors><title>Universal service, specific services on generic networks, some logic
  begins to emerge in the policy area</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-046</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  It has proved to be difficult to translate the lessons from the literature on
universal service into the policy framework because of political interests and
regulatory capture. Neither the USA or Europe has made a very good job of
devising a clean framework and the WTO agreement is sparing in this area. A
number of pressures in the European context have enabled a more systematic
approach to emerge, that exploits the academic work. They include the need for
the European regulatory framework to encompass E. European countries where
network development and income levels are much lower, the desire to encompass
Internet within the universal service regulatory framework, a willingness to
design a framework that covers all communications networks and remove the
telecommunications bias, thereby forcing issues of economic neutrality to the
fore. The paper systematically goes through a number of key areas and
principles of regulation and how they are being designed to deal with a range
of national situations. They include, defining the scope of universal service
and the principles by which it might be modified in the light of technological
and economic developments; incorporating latitude for intervention outside this
defined scope, defining incentive and designation methods to encourage the
efficient supply of elements of universal service obligations, interpreting
affordability in the context of price and income levels that diverge
considerably, requiring both allocative efficiency and competitive neutrality,
formulating alternative financing methods including general government
financing and value added tax type methods which can co-exist and provide
comparative policy yardsticks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109064</id><created>2001-09-24</created><authors><author><keyname>Malone</keyname><forenames>Laurence J.</forenames></author></authors><title>Commonalities: The R.E.A. and High-Speed Rural Internet Access</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2128</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper explores commonalities between the creation of the Rural
Electrification Administration and the similar dilemma of providing an
affordable infrastructure for high-speed Internet access in places where profit
incentives do not exist. In the case of the R.E.A., the necessity for an
aggressive federal initiative to wire rural America, where the market for
electricity had failed, is revisited as the missing incentives are identified
and explored. We then examine the incentive-poor similarities between rural
electrification and rural high-speed Internet access through how consumers
currently and prospectively gain access to broadband Internet service. The
regulatory environment created by the Telecommunications Act of 1996 and the
Federal Communications Commission is considered. Although the FCC is required
(Section 254.b.3) to take regulatory measures to ensure comparable and
affordable access to the Internet for all Americans, the historical
similarities and comparative analysis of rural electrification and high-speed
Internet access suggests the goal of universal service is unlikely to be met in
the near future. Regulatory disincentives to build such networks are present,
driven in part by market realities and in part by competitive restrictions in
the Telecommunications Act of 1996. Finally, we pose the question of whether a
federal effort equivalent to the R.E.A. is needed to ensure that residents of
sparsely populated areas, like their predecessors in the 1930s, are not
comparatively disadvantaged in the first decades of the 21st century. The paper
concludes with a proposal to accelerate the deployment of broadband
infrastructure in rural America.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109065</id><created>2001-09-24</created><authors><author><keyname>Anandalingam</keyname><forenames>G.</forenames></author></authors><title>On the Use of Vickrey Auctions for Spectrum Allocation in Developing
  Countries</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2146</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  In this paper, we assess the applicability of auctions based on the Vickrey
second price model for allocating wireless spectrum in developing countries. We
first provide an overview of auction models for allocating resources. We then
examine the experience of auctioning spectrum in different countries. Based on
this examination, we posit some axioms that seem to have to be satisfied when
allocating spectrum in most developing countries. In light of these axioms, we
provide a critical evaluation of using Vickrey second- price auctions to
allocate spectrum in developing countries. We suggest the use of a new auction
mechanism, the Vickrey &quot;share auction&quot; which will satisfy many of these axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109066</id><created>2001-09-24</created><authors><author><keyname>Szczygiel</keyname><forenames>Tomasz</forenames></author></authors><title>CLP Approaches to 2D Angle Placements</title><categories>cs.PL</categories><comments>Presented at the 6th Annual Workshop of the ERCIM Working Group on
  Constraints, 2001</comments><acm-class>D.3.3</acm-class><abstract>  The paper presents two CLP approaches to 2D angle placements, implemented in
CHIP v.5.3. The first is based on the classical (rectangular) cumulative global
constraint, the second on the new trapezoidal cumulative global constraint.
Both approaches are applied to a specific presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109067</id><created>2001-09-24</created><authors><author><keyname>Weiss</keyname><forenames>Martin B. H.</forenames></author><author><keyname>Kim</keyname><forenames>Hak-Ju</forenames></author></authors><title>Voice over IP in the Local Exchange: A Case Study</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-053</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  There have been a small number of cost studies of Voice over IP (VoIP) in the
academic literature. Generally, they have been for abstract networks, have not
been focused on the public switched telephone network, or they have not
included the operating costs. This paper presents the operating cost portion of
our ongoing research project comparing circuit-switched and IP network costs
for an existing local exchange carrier.
  We have found that (1) The operating cost differential between IP and circuit
switching for this LEC will be small; and (2) A substantial majority of a
telco's operating cost lies in customer service and outside plant maintenance,
which will be incurred equally in both networks in a pure substitution
scenario. Thus, the operating cost difference lies in the actual cost
differences of the switching technologies. This appears to be less than 10%-15%
of the total operating cost of the network. Thus, even if the cost differences
for substitute services were large, the overall impact on the telco's financial
performance would be small. But IP has some hidden benefits on the operations
side. Most notably, data and voice services could be managed with the same
systems infrastructure, meaning that the incremental operations cost of rolling
out new services would likely be much lower, since it would all be IP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109068</id><created>2001-09-24</created><authors><author><keyname>Hargittai</keyname><forenames>Eszter</forenames></author></authors><title>Second-Level Digital Divide: Mapping Differences in People's Online
  Skills</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-083</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Much of the existing approach to the digital divide suffers from an important
limitation. It is based on a binary classification of Internet use by only
considering whether someone is or is not an Internet user. To remedy this
shortcoming, this project looks at the differences in people's level of skill
with respect to finding information online. Findings suggest that people search
for content in a myriad of ways and there is a large variance in how long
people take to find various types of information online. Data are collected to
see how user demographics, users' social support networks, people's experience
with the medium, and their autonomy of use influence their level of user
sophistication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109069</id><created>2001-09-24</created><authors><author><keyname>Economides</keyname><forenames>Nicholas</forenames></author></authors><title>United States v. Microsoft: A Failure of Antitrust in the New Economy</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001, for a complete abstract go to
  http://www.stern.nyu.edu/networks/</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper analyzes the law and economics of United States v. Microsoft, a
landmark case of antitrust intervention in network industries. [abridged]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109070</id><created>2001-09-24</created><authors><author><keyname>Weiser</keyname><forenames>Phil</forenames></author></authors><title>Networks Unplugged: Towards A Model of Compatibility Regulation Between
  Information Platforms</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Networks Unplugged: Towards A Model of Compatibility Regulation Between
Information Platforms This Article outlines a basic model for regulating
interoperability between rival information platforms. In so doing, it insists
that antitrust, intellectual property, and telecommunications regulation all
must follow the same set of principles to facilitate competition between rival
standards where possible, mandating or allowing cooperation only where
necessary to facilitate competition within a standard when network-level
competition is infeasible. To date, the antitrust regime best approximates the
type of model I have in mind, but sound competition policy requires that
telecommunications regulation and intellectual property law follow its basic
principles as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109071</id><created>2001-09-24</created><authors><author><keyname>Garcia-Murillo</keyname><forenames>Martha</forenames></author><author><keyname>MacInnes</keyname><forenames>Ian</forenames></author></authors><title>The Impact of Incentives in the Telecommunications Act of 1996 on
  Corporate Strategies</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2308968915</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Rules are necessary to provide or shape the incentives of individuals and
organizations. This is particularly true when free markets lead to undesirable
outcomes. The Telecommunications Act of 1996 attempted to create incentives to
foster competition. Ambiguity as well as the timing of the Act has led to
delays in the clarification of rules and the rapid obsolescence of the
document. The paper presents the strategies that common carriers adopted to try
to tilt regulation in their favor, slow the entry of competitors, maintain
their market leadership, and expand into other segments. Some of the strategies
analyzed include lobbying efforts, court challenges, and lack of cooperation
with new entrants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109072</identifier>
 <datestamp>2008-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109072</id><created>2001-09-24</created><authors><author><keyname>Momigliano</keyname><forenames>Alberto</forenames></author><author><keyname>Pfenning</keyname><forenames>Frank</forenames></author></authors><title>Higher-Order Pattern Complement and the Strict Lambda-Calculus</title><categories>cs.LO cs.PL</categories><comments>37 pages</comments><report-no>University of Leicester Technical Report 2001/22</report-no><acm-class>D.3.3;D.1.6;F.4.1</acm-class><journal-ref>ACM Trans. Comput. Log. 4(4): 493-529 (2003)</journal-ref><doi>10.1145/937555.937559</doi><abstract>  We address the problem of complementing higher-order patterns without
repetitions of existential variables. Differently from the first-order case,
the complement of a pattern cannot, in general, be described by a pattern, or
even by a finite set of patterns. We therefore generalize the simply-typed
lambda-calculus to include an internal notion of strict function so that we can
directly express that a term must depend on a given variable. We show that, in
this more expressive calculus, finite sets of patterns without repeated
variables are closed under complement and intersection. Our principal
application is the transformational approach to negation in higher-order logic
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109073</id><created>2001-09-24</created><authors><author><keyname>Piscitello</keyname><forenames>Lucia</forenames></author><author><keyname>Sgobbi</keyname><forenames>Francesca</forenames></author></authors><title>E-Business and SMEs: Preliminary Evidence from Selected Italian
  Districts</title><categories>cs.CY</categories><comments>29th Research Conference on Communication, Information and Internet
  Policy, October 27-29, 2001, Alexandria, Virginia Area: Economic Growth and
  Development</comments><report-no>TPRC-2001-050</report-no><acm-class>A.0; J.4</acm-class><abstract>  The debate on the Information Society shows large agreement on the assumption
that the promised benefits will fully display only if the diffusion of ICTs and
the Internet will involve all the actors of the socio-economic system.
Accordingly, special emphasis is put on the participation of small and medium
enterprises (SMEs), but also on public administrations (PAs) as promoters and
catalysts of private initiatives. As for SMEs, public intervention concerns
both the promotion of fully competitive e-markets and the solution of market
failures. However, effective and efficient intervention requires specific
information on SMEs' approach to e-commerce, often depending upon specific
sector and local condition and in most cases still lacking. In order to
identify the need and the scope for public intervention, the paper focuses on a
peculiar SMEs-intensive productive environment: the manufacturing industrial
district, which traditionally constitutes an examples of winning SMEs' network,
characterised by common industrial culture and intense input-output
interactions. The paper presents empirical evidence from the Italian districts
of Como (textile industry) and Lumezzane (metalwork industry). The research
results show that pro-active entrepreneurs are creatively exploring the
opportunities offered by the Internet to promote their businesses. However, it
is also clear that the transition to the Internet economy still involves a
reduced percentage of potential participants, and that institutional actions
are needed in order to foster a larger participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109074</id><created>2001-09-24</created><updated>2001-10-22</updated><authors><author><keyname>Oberlander</keyname><forenames>Susan</forenames></author></authors><title>Indicators of Independence in Regulatory Commissions</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001 This revision has minor editorial changes</comments><report-no>TPRC-2001-048</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Independent regulatory commissions such as the Federal Communications
Commission (FCC) must produce policies that reflect technical expertise, legal
precedent, and stakeholder input. Given these situational imperatives, how does
the FCC implement independence in its decision-making? This research explicates
some of the underlying rules, resources, and relationships within the
environment in which the agency is embedded that influence agency work
practices to operationalize independence. Research such as this may be helpful
in the creation of new, or for assessment of existing, regulatory commissions,
but only if great attention is paid not only to institutional structure, but
also to the practice of staff in the agency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109075</id><created>2001-09-24</created><authors><author><keyname>Froomkin</keyname><forenames>A. Michael</forenames></author><author><keyname>Lemley</keyname><forenames>Mark A.</forenames></author></authors><title>ICANN and Antitrust</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-041</report-no><acm-class>K.4.m</acm-class><abstract>  The Internet Corporation for Assigned Names and Numbers (ICANN) is a private
non-profit company which, pursuant to contracts with the US government, acts as
the de facto regulator for DNS policy. ICANN decides what TLDs will be made
available to users, and which registrars will be permitted to offer those TLDs
for sale. In this article we focus on a hitherto-neglected implication of
ICANN's assertion that it is a private rather than a public actor: its
potential liability under the U.S. antitrust laws, and the liability of those
who transact with it. ICANN argues that it is not as closely tied to the
government as NSI and IANA were in the days before ICANN was created. If this
is correct, it seems likely that ICANN will not benefit from the antitrust
immunity those actors enjoyed. Some of ICANN's regulatory actions may restrain
competition, e.g. its requirement that applicants for new gTLDs demonstrate
that their proposals would not enable competitive (alternate) roots and ICANN's
preventing certain types of non-price competition among registrars (requiring
the UDRP). ICANN's rule adoption process might be characterized as
anticompetitive collusion by existing registrars, who are likely not be subject
to the Noerr-Pennington lobbying exemption. Whether ICANN has in fact violated
the antitrust laws depends on whether it is an antitrust state actor, whether
the DNS is an essential facility, and on whether it can shelter under
precedents that protect standard-setting bodies. If (as seems likely) a private
ICANN and those who petition it are subject to antitrust law, everyone involved
in the process needs to review their conduct with an eye towards legal
liability. ICANN should act very differently with respect to both the UDRP and
the competitive roots if it is to avoid restraining trade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109076</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109076</id><created>2001-09-24</created><updated>2001-09-24</updated><authors><author><keyname>Venkatesh</keyname><forenames>Murali</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Hee</forenames></author></authors><title>Out of the Loop: Problems in the development of next generation
  community networks</title><categories>cs.CY</categories><comments>14 pages</comments><report-no>TPRC-2001-078</report-no><acm-class>K.4.3</acm-class><abstract>  Drawing on an ongoing longitudinal research study, we discuss problems in the
development of five next generation community networking projects in central
New York. The projects were funded under a state program to diffuse broadband
technologies in economically depressed areas of the state. The networks are
technologically complex and entail high costs for subscribers. The political
economy of the development process has biased the subscriber base toward the
resource rich and away from the resource poor, and toward tried-and-tested uses
like Internet and intra-organizational connectivity and away from
community-oriented uses. These trends raise troubling questions about network
ontology and function, and about the relation between the network and its
physical host community. The need for appropriate social policy, and new
planning practices, is argued to effect desired change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109077</id><created>2001-09-24</created><updated>2001-10-23</updated><authors><author><keyname>Benkler</keyname><forenames>Yochai</forenames></author></authors><title>Coase's Penguin, or Linux and the Nature of the Firm</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-019</report-no><acm-class>K.4.1;k.4.3</acm-class><abstract>  The paper explains why open source software is an instance of a potentially
broader phenomenon. Specifically, I suggest that nonproprietary peer-production
of information and cultural materials will likely be a ubiquitous phenomenon in
a pervasively networked society. I describe a number of such enterprises, at
various stages of the information production value chain. These enterprises
suggest that incentives to engage in nonproprietary peer production are trivial
as long as enough contributors can be organized to contribute. This implies
that the limit on the reach of peer production efforts is the modularity,
granularity, and cost of integration of a good produced, not its total cost. I
also suggest reasons to think that peer-production can have systematic
advantages over both property-based markets and corporate managerial
hierarchies as a method of organizing information and cultural production in a
networked environment, because it is a better mechanism for clearing
information about human capital available to work on existing information
inputs to produce new outputs, and because it permits largers sets of agents to
use larger sets of resources where there are increasing returns to the scale of
both the set of agents and the set of resources available for work on projects.
As capital costs and communications costs decrease in importance as factors of
information production, the relative advantage of peer production in clearing
human capital becomes more salient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109078</id><created>2001-09-24</created><authors><author><keyname>Yurcik</keyname><forenames>William</forenames></author><author><keyname>Doss</keyname><forenames>David</forenames></author></authors><title>Internet Attacks: A Policy Framework for Rules of Engagement</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001 19 pages</comments><report-no>TPRC-2001-089</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Information technology is redefining national security and the use of force
by state and nonstate actors. The use of force over the Internet warrants
analysis given recent terrorist attacks. At the same time that information
technology empowers states and their commercial enterprises, information
technology makes infrastructures supported by computer systems increasingly
accessible, interdependent, and more vulnerable to malicious attack. The
Computer Security Institute and the FBI jointly estimate that financial losses
attributed to malicious attack amounted to $378 million in 2000. International
Law clearly permits a state to respond in self-defense when attacked by another
state through the Internet, however, such attacks may not always rise to the
scope, duration, and intensity threshold of an armed attack that may justify a
use of force in self-defense.
  This paper presents a policy framework to analyze the rules of engagement for
Internet attacks. We describe the state of Internet security, incentives for
asymmetric warfare, and the development of international law for conflict
management and armed conflict. We focus on options for future rules of
engagement specific to Information Warfare.
  We conclude with four policy recommendations for Internet attack rules of
engagement: (1) the U.S. should pursue international definitions of &quot;force&quot; and
&quot;armed attack&quot; in the Information Warfare context; (2) the U.S. should pursue
international cooperation for the joint investigation and prosecution of
Internet attacks; (3) the U.S. must balance offensive opportunities against
defensive vulnerabilities; and (4) the U.S. should prepare strategic plans now
rather than making policy decisions in real-time during an Internet attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109079</id><created>2001-09-24</created><authors><author><keyname>Dowding</keyname><forenames>Martin</forenames></author></authors><title>National Information Infrastructure Development in Canada and the U.S.:
  Redefining Universal Service and Universal Access in the Age of
  Techno-Economic Convergence</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This exploratory and descriptive research compares the policy-making
processes and policy recommendations regarding universal service and universal
access developed by the U.S. National Information Infrastructure Advisory
Council (NIIAC) and the Canadian Information Highway Advisory Council (IHAC) in
conjunction with related federal government agencies. Created in 1993 and 1994,
respectively, the Councils were charged with &quot;bringing forward&quot; the concepts of
universal service and universal access to adjust to the effects of
deregulation, new and converged Information and Communication Technologies
(ICTs), and neo-liberal economic competition and globalization that included
acknowledging the private sector as the primary creator of the Information
Highway.This qualitative study used as its methodology organizational, policy,
narrative, and discourse analyses to create a picture of what universal service
and universal access were and what they became in the hands of NIIAC, IHAC. The
U.S. had started with a more clearly defined universal service tradition than
Canada, and undertook a more complex policy-making process with more
experienced personnel. It was also clear that IHAC had in many ways followed
the U.S. model and arrived at many similar recommendations as NIIAC. Because of
the inevitability of technical, economic, and social change related to the
Information Highway, no definitive outcome to the Universal Service and
Universal Access &quot;story&quot; can be determined. Because the Canadian government did
not follow up on some of IHAC's most crucial recommendations, the Canadian
Information Highway &quot;story,&quot; in particular, has been left less complete than
that of the U.S.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109080</id><created>2001-09-24</created><authors><author><keyname>Clay</keyname><forenames>Karen</forenames></author><author><keyname>Smith</keyname><forenames>Michael</forenames></author><author><keyname>Wolff</keyname><forenames>Eric</forenames></author></authors><title>Lead, Follow, or Go Your Own Way: Empirical Evidence Against
  Leader-Follower Behavior in Electronic Markets</title><categories>cs.CY</categories><comments>29 TPRC Conference, 2001</comments><report-no>TPRC-2001-047</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109081</id><created>2001-09-24</created><updated>2001-10-22</updated><authors><author><keyname>Chandan</keyname><forenames>Sam</forenames></author><author><keyname>Hogendorn</keyname><forenames>Christiaan</forenames></author></authors><title>Pricing and Network Externalities in Peer-to-Peer Communications
  Networks (Draft)</title><categories>cs.CY cs.NI</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-044</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper analyzes the pricing of transit traffic in wireless peer-to-peer
networks using the concepts of direct and indirect network externalities. We
first establish that without any pricing mechanism, congestion externalities
overwhelm other network effects in a wireless data network. We show that
peering technology will mitigate the congestion and allow users to take
advantage of more the positive network externalities. However, without pricing,
the peering equilibrium breaks down just like a bucket brigade made up of
free-riding agents. With pricing and perfect competition, a peering equilibrium
is possible and allows many more users on the network at the same time.
However, the congestion externality is still a problem, so peering organized
through a club may be the best solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109082</id><created>2001-09-24</created><updated>2001-10-25</updated><authors><author><keyname>Gillett</keyname><forenames>Sharon Eisner</forenames></author><author><keyname>Tseng</keyname><forenames>Emy</forenames></author></authors><title>Asymmetric Regulation on Steroids: U.S. Competition Policy and Fiber to
  the Home</title><categories>cs.CY cs.NI</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-062</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Fiber to the Home (FTTH) describes a set of emerging technologies with the
potential to affect competition in local access. On one hand, the high cost of
deploying fiber to the residence suggests limitations on facilities-based
competition among FTTH networks. On the other hand, FTTH opens up new
possibilities for service-level competition, defined as the sharing of a single
network infrastructure by multiple higher-layer service providers, whether of
the same or different services. Yet technology is hardly an exogenous factor
that independently shapes future local access competition; the regulatory
environment also plays a key role. By shaping expectations about future
competitive requirements, current regulations influence network operators'
deployment choices among competing FTTH technologies, as well as design choices
made by vendors and standards bodies for technologies still under
development.The current regulatory approach to FTTH is far from consistent.
Network operators likely to deploy FTTH include Incumbent Local Exchange
Carriers (ILECs), incumbent cable operators, competitive access providers
(including CLECs), independent telephone companies, and municipalities. This
paper reviews the rules related to service-level competition that apply to each
of these categories. In essence, the paper finds that if current regulatory
trends continue, asymmetries in the regulation of service-level competition
will be on steroids by the time FTTH starts being more commonly deployed.
Current regulatory requirements are either non-existent, or extremely detailed
and technology- and service- specific (e.g. UNEs). We argue that neither of
these approaches is likely to achieve the desired result for FTTH, given the
current state of flux in emerging FTTH technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109083</id><created>2001-09-24</created><authors><author><keyname>Davis</keyname><forenames>Charles N.</forenames></author></authors><title>Electronic Access to Information and the Privacy Paradox: Rethinking
  :'Practical Obscurity'</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-096</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This article addresses the U.S. Supreme Court's central purpose formulation
in Reporters Committee v. Department of Justice under the federal Freedom of
Information Act. By examining all lower federal court opinions interpreting
Reporters Committee and by analyzing the effects of the Court's opinion on the
implementation of the FOIA, the paper finds that the Court's opinion has
greatly narrowed the scope of the FOIA and limited the power of EFOIA to
democratize electronic information. To assist in remedying the damage to the
public interest in freedom of information, the author urges judicial
consideration of a Privacy Act case, Tobey v. NRB, that more subtly treats
information collected by government about individuals. The paper concludes that
the Privacy Act demonstrates clearly that information, particularly
computerized databases, can not be treated categorically for purposes of
access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109084</id><created>2001-09-24</created><authors><author><keyname>Horrigan</keyname><forenames>John B.</forenames></author></authors><title>The Internet and Community Networks: Case Studies of Five U.S. Cities</title><categories>cs.DB</categories><comments>29th TPRC Conference</comments><report-no>TPRC-2001-027</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,
and Washington, DC) and explores strategies being employed by community
activists and local governments to create and sustain community networking
projects. In some cities, community networking initiatives are relatively
mature, while in others they are in early or intermediate stages. The paper
looks at several factors that help explain the evolution of community networks
in cities:
  1) Local government support; 2) Federal support 3) Degree of community
activism, often reflected by public-private partnerships that help support
community networks.
  In addition to these (more or less) measurable elements of local support, the
case studies enable description of the different objectives of community
networks in different cities. Several community networking projects aim to
improve the delivery of government services (e.g., Portland and Cleveland),
some have a job-training focus (e.g., Austin, Washington, DC), others are
oriented very explicitly toward community building (Nashville, DC), and others
toward neighborhood entrepreneurship (Portland and Cleveland).
  The paper ties the case studies together by asking whether community
technology initiatives contribute to social capital in the cities studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109085</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109085</id><created>2001-09-24</created><updated>2001-10-20</updated><authors><author><keyname>Allen</keyname><forenames>David</forenames></author></authors><title>Policy for access: Framing the question</title><categories>cs.CY</categories><comments>13 pages; abstract expanded</comments><report-no>TPRC-2001-008</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Five years after the '96 Telecommunications Act, we still find precious
little local facilities-based competition. In response there are calls in
Congress and even from the FCC for new legislation to &quot;free the Bells.&quot;
However, the same ideology drove policy, not just five years ago, but also
almost twenty years back with the first modern push for &quot;freedom,&quot; namely
divestiture.
  How might we frame the question of policy for local access to engender a more
fruitful approach? The starting point for this analysis is the network--not
bits and bytes, but the human network. With the human network as starting
point, the unit of analysis is the community--specifically, the individual in a
tension with community. There are two core ideas.
  The first takes a behavioral approach to the economics--and the relative
share between beneficial chaos and order, in economic affairs, becomes
explicit.
  If the first main idea provides a conceptual base for open source, the second
core idea distinguishes open source from open design, ie at the information
'frontier' we push forward.
  The resulting policy frame for access is worked out in the detailed, concrete
steps of an extended thought experiment. A small town setting (Concord,
Massachusetts) grounds the discussion in the real world. The purpose overall is
to stimulate new thinking which may break out of the conundrum where periodic
rounds to legislate 'freedom' produce the opposite, recursively. The ultimate
aim is better fit between our analytically-driven expectations and economic
outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109086</id><created>2001-09-24</created><updated>2001-10-26</updated><authors><author><keyname>Wagner</keyname><forenames>R. Polk</forenames></author><author><keyname>Struve</keyname><forenames>Catherine T.</forenames></author></authors><title>Realspace Sovereigns in Cyberspace: The Case of Domain Names</title><categories>cs.CY</categories><comments>Version 2.1: extensive revisions / formatting changes</comments><report-no>TPRC-2001-084</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  In this piece, we take up the case of the domain name system as an example of
challenges and solutions for realspace sovereigns in cyberspace. First, we
analyze the 'in rem' provision of the US Anticybersquatting Consumer Protection
Act (ACPA), which purports to expand the scope of the ACPA to encompass
disputes with little direct connection with the United States. In reality,
there exist no cases of foreign cybersquatting as to which the 'in rem'
provision will be both applicable and constitutional. Instead the ACPA 'in rem'
provision is notable primarily for its aggressive assertion of jurisdiction,
leading us to consider the (often overlooked) role of realspace sovereigns in
the regulation of the domain names system. By mapping the logical control over
the domain names system-the distributed hierarchy that is the basis of the
system's design-onto realspace territory, the potential for sovereign
regulation of the system becomes apparent. We argue that the regulatory
significance of geography, and the essentially arbitrary nature of the present
territorial locations of the key components of the domain name system implies
the future segmentation of the domain name system, and the resulting dramatic
decrease in its value. Accordingly, we argue that realspace sovereigns (and
especially the United States) have strong interests in avoiding segmentation,
and thus must seek to coordinate the regulation of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109087</id><created>2001-09-24</created><updated>2001-10-26</updated><authors><author><keyname>Kavanaugh</keyname><forenames>Andrea L.</forenames></author></authors><title>Civic Engagement among Early Internet Adopters: Trend or Phase?</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper brings evidence to bear on the question of the long-term effects
of Internet diffusion on civic engagement in geographic communities. It draws
on findings from survey data collected in four U.S. towns and cities in fall
2000 where community computer networking is established. The study shows that
early adopters of the Internet are more likely to engage in civic activities
and to have higher levels of community involvement than later adopters.
Further, early adopters are more likely to use the Internet to increase their
community involvement and political participation. Later adopters in all four
sites show less involvement in their local community and less interest in
political activity and information, online or offline. These findings reinforce
those of the Kohut (1999) study showing that later adopters are less civic
minded and more interested than early adopters in consumer and commercial
applications, such as shopping and entertainment. The evidence in these four
sites is consistent with earlier findings in Blacksburg, Virginia (Kavanaugh,
2000; Patterson and Kavanaugh, 2001; Kavanaugh and Patterson, 2001) and other
studies of early innovation adopters (Rogers, 1983; Kohut, 1999; Valente, 1995,
among others). The results reported in this paper lend weight to the argument
that increases in civic engagement and community involvement are due primarily
to the behavior of early adopters, making such increases a phase, not a trend.
As later adopters come on line, use of the Internet for community involvement
or civic engagement decreases. In the long term, we can expect that Internet
access may have only a modest effect on community involvement and civic
engagement in geographic communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109088</id><created>2001-09-24</created><authors><author><keyname>Park</keyname><forenames>Sangin</forenames></author></authors><title>Value of Usage and Seller's Listing Behavior in Internet Auctions</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-072</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  In this paper, we aim to empirically examine the value of website usage and
sellers' listing behavior in the two leading Internet auctions sites, eBay and
Yahoo!Auctions. The descriptive data analysis of the seller's equilibrium
listing behavior indicates that a seller's higher expected auction revenue from
eBay is correlated with a lager number of potential bidders measured by website
usage per listing. Our estimation results, based on the logarithm
specifications of sellers' expected auction revenues and potential bidders'
website usage, show that in a median case, (i) 1 percent increase of the unique
visitors (page views) per listed item induces 0.022 (0.007) percent increase of
a seller's expected auction revenue; and (ii) 1 percent increase of sellers'
listings induces 1.99 (4.74) percent increase of the unique visitors (page
views). Since increased expected auction revenues will induce more listings, we
can infer positive feedback effects between the number of listings and website
usage. Consequently, Yahoo!Auctions, which has substantially less listings, has
greater incentives to increase listings via these feedback effects which are
reflected in its fee schedules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109089</id><created>2001-09-24</created><authors><author><keyname>Swire</keyname><forenames>Peter P.</forenames></author></authors><title>What Should be Hidden and Open in Computer Security: Lessons from
  Deception, the Art of War, Law, and Economic Theory</title><categories>cs.CR cs.CY</categories><comments>TPRC. Email: pswire@law.gwu.edu web:
  http://www.osu.edu/units/law/swire.htm</comments><report-no>TPRC-2001-004</report-no><acm-class>K.4.1; K.5.2</acm-class><abstract>  &quot;What Should be Hidden and Open in Computer Security: Lessons from Deception,
the Art of War, Law, and Economic Theory&quot; Peter P. Swire, George Washington
University.
  Imagine a military base. It is defended against possible attack. Do we expect
the base to reveal the location of booby traps and other defenses? No. But for
many computer applications,a software developer will need to reveal a great
deal about the code to get other system owners to trust the code and know how
to operate with it.
  This article examines these conflicting intuitions and develops a theory
about what should be open and hidden in computer security. Part I of the paper
shows how substantial openness is typical for major computer security topics,
such as firewalls, packaged software, and encryption. Part II shows what
factors will lead to openness or hiddenness in computer security.
  Part III presents an economic analysis of the issue of what should be open in
computer security. The owner who does not reveal the booby traps is like a
monopolist, while the open-source software supplier is in a competitive market.
This economic approach allows us to identify possible market failures in how
much openness occurs for computer security.
  Part IV examines the contrasting approaches of Sun Tzu and Clausewitz to the
role of hiddenness and deception in military strategy. The computer security,
economic, and military strategy approaches thus each show factors relevant to
what should be kept hidden in computer security. Part V then applies the theory
to a range of current legal and technical issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109090</id><created>2001-09-24</created><authors><author><keyname>Strover</keyname><forenames>Sharon</forenames></author><author><keyname>Oden</keyname><forenames>Michael</forenames></author><author><keyname>Inagaki</keyname><forenames>Nobuya</forenames></author></authors><title>Telecommunications and rural economies: Findings from the Appalachian
  region</title><categories>cs.CY</categories><comments>29th TPRC Conference</comments><report-no>TPRC-2001-080</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This research investigates the relationship between telecommunications
infrastructure, economic conditions, and federal and state policies and
initiatives. It presents a detailed look at the telecommunications environment
of the Appalachian region, particularly focusing on broadband technologies. A
strong, positive association exists between telecommunications infrastructure
and economic status. The effects of federal and state universal service
policies are examined, as well as some of the ways states have leveraged their
own infrastructure to improve telecommunications capabilities in their region.
Other state and local telecommunications-related programs are noted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109091</id><created>2001-09-24</created><updated>2001-10-01</updated><authors><author><keyname>McTaggart</keyname><forenames>Craig</forenames></author></authors><title>E PLURIBUS ENUM: Unifying International Telecommunications Networks and
  Governance</title><categories>cs.CY cs.NI</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-064</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  ENUM effectively bridges the telephone and Internet worlds by placing
telephone numbers from the ITU Rec. E.164 public telecommunication numbering
plan into the Internet Domain Name System (DNS) as domain names. ENUM
potentially presents significant public policy issues at both the domestic and
international levels. Ultimately, it should not matter whether ENUM is
approached as a telecommunications issue or an Internet issue because: (1) they
are becoming the same thing technically, and (2) they engage the same global
public interests. For the same reasons as apply to traditional
telecommunications, and even to the Internet itself, public oversight of ENUM
naming, numbering, and addressing resources is justified both by technical
necessity and the interests of consumer protection (particularly personal
privacy) and competition at higher service layers. A single, coordinated global
DNS domain for at least Tier 0 (the international level) of the ENUM names
hierarchy should be designated by public authorities. Many of the technical
characteristics and policy considerations relevant at the ENUM Tier 0 and 1
zones are also directly applicable to the Internet's IP address space and DNS
root (or Tier 0) zone - key shared elements of the Internet's logical
infrastructure. Despite the fundamentally international nature of the
Internet's logical infrastructure layer, and the purported privatization of
administration of its IP address space and the DNS, Internet governance is not
yet truly international. The ENUM policy debate illustrates the need for
authoritative international public oversight of public communications network
logical infrastructure, including that of traditional telecommunications, the
Internet, and ENUM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109092</id><created>2001-09-24</created><authors><author><keyname>Netanel</keyname><forenames>Neil</forenames></author></authors><title>Is the Commercial Mass Media Necessary, or Even Desirable, for Liberal
  Democracy?</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC - 2001 - XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Is a commercial mass media, dependent on the market for its sustenance,
necessary, or even desirable, for liberal democracy? Yochai Benkler has argued
that a decentralized, peer-to-peer system of communications and information is
both possible with digital technology and preferable to a system based on
commercial mass media. He has contended in fact that the presence of
politically powerful, copyright-rich mass media imposes significant barriers to
the development of peer-to-peer information-sharing networks. In contrast, I
have argued that the commercial mass media play an important, and perhaps even
vital, role in liberal democracy by galvanizing public opinion, serving as a
watchdog against government and corporate wrongdoing, agenda-setting (which
enables public discourse), and serving as a relatively trustworthy source of
information. This paper seeks to push the ball forward on this issue. It first
isolates and enumerates the contributions that the commercial mass media are
said to make towards liberal democracy. It then briefly assesses the extent to
which the commercial mass media actually fulfills these constitutive functions.
It then asks whether alternative institutions might serve some or all of these
functions just as well or better. In so doing, it looks both to the past and
the future. First, it examines the political party-supported press that thrived
in the United States through much of the 19th century. Second, it examines
government-funded mass media. Third, it looks, skeptically, at possibilities
for peer-to-peer sharing of information and opinion in the digital network
environment. I conclude that, despite the weaknesses of commercial mass media,
an information policy suitable to liberal democracy should include a plurality
of types of voices, including commercial mass media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109093</id><created>2001-09-24</created><updated>2002-04-14</updated><authors><author><keyname>Katkin</keyname><forenames>Ken</forenames></author></authors><title>Open Access to Monopoly Cable Platforms Versus Direct Access To
  Competitive International Telecommunications Satellite Facilities: A Study In
  Contrasts</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001 (Revised April 14, 2002)</comments><report-no>TPRC-2001-079</report-no><acm-class>K.4.m</acm-class><abstract>  In 1999, the FCC authorized direct access to INTELSAT, allowing INTELSAT's
U.S. customers and competitors to bypass INTELSAT's U.S. retail affiliate
(COMSAT), and to take satellite capacity at wholesale prices directly from
INTELSAT. This policy was modeled in many respects on the access and unbundling
requirements applicable to domestic incumbent local exchange carriers (ILECs)
under the Telecommunications Act of 1996. At the same time, incumbent domestic
cable TV system operators have not been required to provide wholesale open
access to competitive Internet Service Providers (ISPs) seeking to provide
residential broadband Internet service through existing proprietary cable
facilities. Yet the policy arguments favoring open access to incumbent domestic
cable systems appear to be stronger than those favoring direct access to
INTELSAT.
  For example, it may be fairly debated whether entrenched cable system
operators are now positioned to unfairly leverage their dominance in the
multichannel video programming distribution (MVPD) market to thwart competition
in the broadband ISP market, as some cable open access advocates assert. In
contrast, it is clear that no analogous issues of tying were implicated by
INTELSAT in 1999, when its position in the international telecommunications
market was substantially nondominant and, in any event, it had no new product
to tie to its established offerings. Similarly, while it may be debated whether
or not a cable plant is a bottleneck facility that gatekeeps broadband Internet
for many residential users, it is beyond cavil that INTELSAT in 1999 controlled
virtually no such bottleneck facilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109094</id><created>2001-09-24</created><authors><author><keyname>Piragibe</keyname><forenames>Clelia</forenames></author></authors><title>Competition and Globalization Brazilian Telecommunications Policy at
  Crossroads</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-090</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The current pattern of competition in the Brazilian telecommunications market
was defined by a regulatory reform implemented in the second half of the
nineties. The telecommunications regulatory reform discussed in the paper
promoted the privatization of Telebras System and fostered competition in the
Brazilian market under the stricted supervision of the new regulator Anatel.
Notwithstanding, the regulatory Brazilian scheme is at a crossroads. From 2002
on an open market approach will be implemented in telecom arena. We analyse the
main aspects of those change, from the firms perspective and also in a broad
scenario under influence of the World Trade Organization system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109095</id><created>2001-09-24</created><updated>2001-10-28</updated><authors><author><keyname>Shiman</keyname><forenames>Daniel R.</forenames></author><author><keyname>Rosenworcel</keyname><forenames>Jessica</forenames></author></authors><title>Assessing the Effectiveness of Section 271 Five Years After the
  Telecommunications Act of 1996</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-057</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  A major goal of the Telecommunications Act of 1996 is to promote competition
in both the local exchange and long distance wireline markets. In section 271
Congress permitted the Bell Operating Companies (BOCs) to enter the long
distance market only if they demonstrate to the FCC that they have complied
with the market-opening requirements of section 251. This paper examines the
logic behind section 271, to determine if it is a reasonable means of achieving
increased competition in both the local and long distance markets, given the
technical characteristics of the industry and the legal and informational
constraints on regulators who must ensure compliance. It also provides an
update on the extent of competitive entry in the local exchange market five
years after enactment of the Act.
  In this paper we examine a variety of schemes for ensuring BOC compliance
that Congress could have used. Given the characteristics of the industry and
the limitations on regulators' ability to observe BOC's efforts, we determine
that the use of a prize such as BOC entry into long distance is a superior
incentive mechanism. We further determine that conditioning a BOC's long
distance entry on its demonstrating compliance with section 251 is a logical
method of protecting the long distance market against a BOC discriminating
against long distance competitors once it has gained entry. The statistical
evidence we look at, using data we have collected on ILEC lines sold to CLECs
for POTS services, appears to confirm that section 271 has thus far been
effective in ensuring compliance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109096</id><created>2001-09-24</created><authors><author><keyname>Christensen</keyname><forenames>Tony</forenames></author><author><keyname>McCormick</keyname><forenames>Peter</forenames></author></authors><title>CyberCampaigns and Canadian Politics: Still Waiting?</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-058</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The early election call in the fall of 2000 provided the perfect opportunity
to study the impact the Internet has had on election campaigning in Canada.
With the explosion of use the Net has seen since the 1997 general election,
Canadian federal parties stood at the threshold of a new age in election
campaigning. Pundits such as Rheingold (1993) have argued that the Internet
will provide citizens with a way to bypass traditional media and gain
unmediated access to each parties political message as well as providing a
forum for citizens to engage the parties, and each other in deliberative
debate.
  Through a longitudinal analysis of party web pages and telephone interviews
with party staffers, we analyze the role the Internet played in the election
campaigns of Canada's federal parties. Our findings indicate that the parties
are still focusing on providing online features that talk at the voter instead
of engaging them in any type of meaningful discourse. Most of these sites were
exceptionally similar in their structure and in the type of content they
provided. Generally, these sites served as digital archives for campaign
material created with other media in mind and despite the multimedia
capabilities of the Internet, these sites tended to be overwhelmingly text
oriented. In line with Stromer-Galley's (2000) discussion of why candidates in
the U.S. avoid online interaction, we also argue that little incentive exists
to motivate parties to engage in any meaningful interaction with voters online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109097</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109097</id><created>2001-09-24</created><authors><author><keyname>Horwitz</keyname><forenames>Robert B.</forenames></author></authors><title>'Negotiated Liberalization': Stakeholder Politics and Communication
  Sector Reform in South Africa</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-009</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The paper examines the South African transition from apartheid to democracy
through the lens of the reform of the communication sector. Through a set of
participatory stakeholder consultative processes, the institutions of
broadcasting, telecommunications, print press, and state information agency
underwent reform in a process I refer to as 'negotiated liberalization.'
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109098</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109098</id><created>2001-09-24</created><updated>2001-10-23</updated><authors><author><keyname>Phillips</keyname><forenames>David J.</forenames></author></authors><title>The Influence of Policy Regimes on the Development and Social
  Implications of Privacy Enhancing Technologies</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-067</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  As privacy issues have gained social salience, entrepreneurs have begun to
offer privacy enhancing technologies (PETs) and the U.S. has begun to enact
privacy legislation.
  But &quot;privacy&quot; is an ambiguous notion. In the liberal tradition, it is an
individualistic value protecting citizens from intrusion into a realm of
autonomy. A feminist critique suggests that the social utility of privacy is to
exclude certain issues from the public realm. Sociologists suggest that privacy
is about identity management, while political economists suggest that the most
salient privacy issue is the use of personal information to normalize and
rationalize populations according to the needs of capital.
  While PETs have been developed for use by individual consumers, recently
developers are focusing on the business to business market, where demand is
stoked by the existence of new privacy regulations. These new laws tend to
operationalize privacy in terms of &quot;personally identifiable information.&quot; The
new generation of PETs reflect and reify that definition. This, in turn, has
implications for the everyday understandings of privacy and the constitution of
identity and social life.
  In particular, this socio-technical practice may strengthen the ability of
data holders to rationalize populations and create self-serving social
categories. At the same time, they may permit individuals to negotiate these
categories outside of panoptic vision. They may also encourage public
discussion and awareness of these created social categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109099</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109099</id><created>2001-09-24</created><updated>2002-03-10</updated><authors><author><keyname>Weinberg</keyname><forenames>Jonathan</forenames></author></authors><title>ICANN as Regulator</title><categories>cs.CY</categories><comments>v.3; only slightly revised from v.2; originally presented at 29th
  TPRC Conference, 2001</comments><report-no>TPRC-2001-012</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper tells the story leading to ICANN's selection of seven new Internet
top level domains in November 2000. In implementing proposals to expand the
name space, ICANN adopted an approach far different from Jon Postel's
lightweight proposals. ICANN staff, in setting the ground rules for considering
new gTLDs, emphasized that only a few applicants would be allowed in, and
imposed strict threshold requirements. Staff determined that the Board should
pick TLDs by looking at all relevant aspects of every proposal, and deciding
which ones presented the best overall combination of a variety of
incommensurable factors. Aspects of the resulting process were predictable:
Anyone familiar with the FCC comparative hearing process for broadcast licenses
can attest that this sort of ad hoc comparison is necessarily subjective,
lending itself to arbitrariness and biased application. Yet the process had
advantages that appealed to ICANN decision-makers. The Board members would be
free to take their best shots, in a situationally sensitive manner, at
advancing the policies they thought important. The approach allowed ICANN to
maintain the greatest degree of control. The end result, though, was a process
stunning in its arbitrariness, a bad parody of fact-bound, situationally
sensitive (rather than rules-based) decision-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109100</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109100</id><created>2001-09-24</created><authors><author><keyname>Tan</keyname><forenames>Zixiang Alex</forenames></author></authors><title>Comparison of Wireless Standards-Setting --United States Versus Europe</title><categories>cs.CY</categories><comments>11 pages</comments><report-no>tprc-2001-060</report-no><acm-class>K.4.1</acm-class><abstract>  When decisions about developing standards are left to individual firms, there
are many advantages including a flexible response to market evolution,
accommodation to rapid technology change, and avoidance of costly coordination.
However, the market process does not necessarily lead to compatible standards,
especially when there is no clear market dominator. Standards Institutions
generally facilitate better communication among market participants, which may
discourage early and/or primitive incompatible standards to emerge in the
market. However, expenditures on institutional standards-setting can lead to
diminishing returns, or even be counter-productive because institutional
standards usually take longer to produce and are slower to respond to
technology development. Government agencies can specify compulsory standards to
avoid incompatibility. In addition, governments can tie their policy on
standardization closely with their industrial, trade, and/or regulatory
policies. However, many exogenous factors handicap a government's effective
involvement. Government often fails to respond to the dynamics of technology
development and consumer demand, and to pick up the &quot;right technology&quot;. This
study first discusses why two different models have emerged in the two regions.
It then examines outcomes and implications of the two models, including impacts
on domestic service deployment, global trade competition, technology
innovations, and strategies of multinational corporations. The goal is to
conduct an empirical comparison between the two contrasting models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109101</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109101</id><created>2001-09-24</created><authors><author><keyname>McKnight</keyname><forenames>Lee W.</forenames></author><author><keyname>Linsenmayer</keyname><forenames>Raymond</forenames></author><author><keyname>Lehr</keyname><forenames>William</forenames></author></authors><title>Best Effort versus Spectrum Markets: Best Effort versus Spectrum
  Markets: The Cases of European MVNOs and (Ultra)Wideband Unlicensed Services</title><categories>cs.CY cs.NI</categories><comments>29th TPRC Conference, 2001</comments><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper compares two models for delivering broadband wireless services:
best effort vs. QoS guaranteed services. The 'best effort' services we refer to
in this paper are more commonly known as unlicensed wireless services, while
the 'Quality of Service guaranteed' services are more commonly referred to as
traditional landline telephony, as well as cellular telephone services of
either the second or third generation.
  This paper highlights the differing 'market' versus 'engineering'
philosophies implicit in alternative wireless service architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109102</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109102</id><created>2001-09-24</created><updated>2001-10-02</updated><authors><author><keyname>Noam</keyname><forenames>Eli M.</forenames></author></authors><title>The Next Frontier for Openness: Wireless Communications</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-094</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  For wireless communications, the FCC has fostered competition rather than
openness. This has permitted the emergence of vertically integrated end-to-end
providers, creating problems of reduced hardware innovation, software
applications, user choice, and content access. To deal with these emerging
issues and create multi-level forms of competition, one policy is likely to
suffice: a Carterfone for wireless, coupled with more unlicensed spectrum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109103</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109103</id><created>2001-09-24</created><updated>2001-10-25</updated><authors><author><keyname>Ratto</keyname><forenames>Matt</forenames></author></authors><title>Leveraging Software, Advocating Ideology: Free Software and Open Source</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-036</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper uses the software program Linux and the discourse around it in
order to examine how software programs can be used to articulate and defend
social and economic positions. Although I do not use the term &quot;expression&quot; in a
strict legal sense, I claim that in order to make policy decisions involving
software, it is important to understand how the functionality of software is
expressive. Another way to state this is that software programs like Linux are
socially meaningful through functionality and talk about functionality.
  In section I, I review some recent legal scholarship about software and
explain why understanding how embedded technical expression works is important
given the increasingly large role of software in governance. In section II, I
explore software construction as a combination of social and technical
practices. In section III, I describe some examples of meaning-making around
the software program Linux. In section IV I conclude with a short description
of why such analyses are important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109104</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109104</id><created>2001-09-24</created><authors><author><keyname>Fuentes-Bautista</keyname><forenames>Martha</forenames></author></authors><title>Universal Service in times of Reform: Affordability and accessibility of
  telecommunication services in Latin America</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-099</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  By surveying the universal service policies in six Latin American countries,
this study explores the evolution of the concept during the rollout of the
telecommunication reform in the last decade. Country profiles and a set of
universal service indicators provide a frame for discussing issues of
accessibility and affordability of telephone service in the region. This study
found that the reconfiguration of national networks fostered by liberalization
policies offered risks and opportunities to achieve universal service goals.
The diversification of access points and services enhanced users choices but
price rebalancing and lack of Universal Service Obligations (USO) to target
groups with special needs depressed the demand and threatened to exclude
significant parts of the population. The situation requires the reformulation
of USO incorporating all technological solutions existing in the market, and
factors from the consumer-demand accounting for the urban-rural continuum, and
different social and economic strata. This study identifies the emergence of a
second generation of USO targeting some of these needs. However, more
competition and special tariff plans for the poor need to be incorporated to
the options available in the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109105</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109105</id><created>2001-09-24</created><updated>2001-10-30</updated><authors><author><keyname>Hjelm</keyname><forenames>Bjorn</forenames></author></authors><title>Standards and Intellectual Property Rights in the Age of Global
  Communication - A Review of the International Standardization of
  Third-Generation Mobile System</title><categories>cs.CY</categories><comments>29th TPRC conference, 2001</comments><report-no>TPRC-2001-092</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  When the European Telecommunications Standards Institute (ETSI) selected a
radio access technology based on Wideband Code-Division Multiple Access
(WCDMA), sponsored by European telecommunications equipment manufactures
Ericsson and Nokia, for its third-generation wireless communications system, a
bitter dispute developed between ETSI and Qualcommm Inc. Qualcomm threatened to
withhold its intellectual property on the CDMA technology unless the Europeans
agreed to make the radio access technology backward compatible with cdmaOne,
Qualcomm's favored version of CDMA. A dispute over intellectual property rights
over key CDMA techniques also erupted between Ericsson and Qualcomm and both
filed patent infringement in US Court. The dispute halted the standards
activity and has troubled operators worldwide as well as the International
Telecommunications Union (ITU).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109106</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109106</id><created>2001-09-24</created><updated>2001-10-01</updated><authors><author><keyname>Asvanund</keyname><forenames>Atip</forenames></author><author><keyname>Clay</keyname><forenames>Karen</forenames></author><author><keyname>Krishnan</keyname><forenames>Ramayya</forenames></author><author><keyname>Smith</keyname><forenames>Michael</forenames></author></authors><title>Bigger May Not Be Better: An Empirical Analysis of Optimal Membership
  Rules in Peer-To-Peer Networks</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-049</report-no><acm-class>K.4.m miscellaneous</acm-class><abstract>  Peer to peer networks will become an increasingly important distribution
channel for consumer information goods and may play a role in the distribution
of information within corporations. Our research analyzes optimal membership
rules for these networks in light of positive and negative externalities
additional users impose on the network. Using a dataset gathered from the six
largest OpenNap-based networks, we find that users impose a positive network
externality based on the desirability of the content they provide and a
negative network externality based on demands they place on the network.
Further we find that the marginal value of additional users is declining and
the marginal cost is increasing in the number of current users. This suggests
that multiple small networks may serve user communities more efficiently than
single monolithic networks and that network operators may wish to specialize in
their content and restrict membership based on capacity constraints and user
content desirability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109107</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109107</id><created>2001-09-25</created><updated>2001-09-26</updated><authors><author><keyname>Burk</keyname><forenames>Dan L.</forenames></author><author><keyname>Lemley</keyname><forenames>Mark A.</forenames></author></authors><title>Is Patent Law Technology Specific?</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-002</report-no><acm-class>K.4.1</acm-class><abstract>  Although patent law purports to cover all manner of technologies, we have
noticed recent divergence in the standards applied to biotechnology and to
software patents: the Federal Circuit has applied a very permissive standard of
obviousness in biotechnology, but a highly restrictive disclosure requirement.
The opposite holds true for software patents, which seems to us exactly
contrary to sound policy for either industry. These patent standards are
grounded in the legal fiction of the &quot;person having ordinary skill in the art&quot;
or PHOSITA. We discuss the appropriateness of the PHOSITA standard, concluding
that it properly lends flexibility to the patent system. We then discuss the
difficulty of applying this standard in different industries, offering
suggestions as to how it might be modified to avoid the problems seen in
biotechnology and software patents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109108</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109108</id><created>2001-09-24</created><authors><author><keyname>Bauer</keyname><forenames>Johannes M.</forenames></author></authors><title>Spectrum auctions, pricing and network expansion in wireless
  telecommunications</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-070</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper examines the effects of licensing conditions, in particular of
spectrum fees, on the pricing and diffusion of mobile communications services.
Seemingly exorbitant sums paid for 3G licenses in the UK, Germany in 2000 and
similarly high fees paid by U.S. carriers in the re-auctioning of PCS licenses
early in 2001 raised concerns as to the impacts of the market entry regime on
the mobile communications market.
  The evidence from the GSM and PCS markets reviewed in this paper suggests
that market entry fees do indeed influence the subsequent development of the
market. We discuss three potential transmission channels by which license fees
can influence the price and quantity of service sold in a wireless market: an
increase in average cost, an increase in incremental costs, and impacts of sunk
costs on the emerging market structure.
  From this conceptual debate, an empirical model is developed and tested using
cross-sectional data for the residential mobile voice market. We utilize a
structural equation approach, modeling the supply and demand relationships
subject to the constraint that supply equals demand. The results confirm the
existence of a positive effect of license fees on the cost of supply. However,
we also find that higher market concentration has a positive effect on the
overall supply in the market, perhaps supporting a Schumpeterian view that a
certain degree of market concentration facilitates efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109109</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109109</id><created>2001-09-24</created><updated>2002-04-16</updated><authors><author><keyname>Shah</keyname><forenames>Rajiv C.</forenames></author><author><keyname>Kesan</keyname><forenames>Jay P.</forenames></author></authors><title>The Role of Institutions in the Design of Communication Technologies</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-086</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Communication technologies contain embedded values that affect our society's
fundamental values, such as privacy, freedom of speech, and the protection of
intellectual property. Researchers have shown the design of technologies is not
autonomous but shaped by conflicting social groups. Consequently, communication
technologies contain different values when designed by different social groups.
Continuing in this vein, we show that the institutions where communication
technologies are designed and developed are an important source of the values
of communication technologies. We use the term code to collectively refer to
the hardware and software of communication technologies. Institutions differ in
their motivations, structure, and susceptibility to external influences. First,
we focus on the political, economic, social, and legal influences during the
development of code. The institutional reactions to these influences are
embodied in code. Second, we focus on the decision-making issues in the review
process for code. This process determines the code's content and affects the
dissemination of code through the decision whether to publicly release the
code. We found these factors vary by institution. As a result, institutions
differ in the values that they incorporate into code or communications
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109110</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109110</id><created>2001-09-24</created><authors><author><keyname>Prior</keyname><forenames>Markus</forenames></author></authors><title>Efficient Choice, Inefficient Democracy? The Implications of Cable and
  Internet Access for Political Knowledge and Voter Turnout</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-025</report-no><acm-class>K.4.m</acm-class><abstract>  This paper explains why, despite a marked increase in available political
information on cable television and the Internet, citizens' levels of political
knowledge have, at best, remained stagnant (Delli Carpini &amp; Keeter, 1996).
Since the availability of entertainment content has increased too, the effect
of new media on knowledge and vote likelihood should be determined by people's
relative preferences for entertainment and information. Access to new media
should increase knowledge and vote likelihood among people who prefer news. At
the same time, it is hypothesized to have a negative effect on knowledge and
turnout for people who prefer entertainment content. Hypotheses are tested by
building a measure of Relative Entertainment Preference (REP) from existing NES
and Pew survey data. Results support the predicted interaction effect of media
environment (cable and/or Internet access) and motivation (REP) on political
knowledge and turnout. In particular, people who prefer entertainment to news
and have access to cable television and the Internet are less knowledgeable and
less likely to vote than any other group of people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109111</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109111</id><created>2001-09-25</created><updated>2001-10-27</updated><authors><author><keyname>O'Donnell</keyname><forenames>Shawn</forenames></author><author><keyname>Donahue</keyname><forenames>Hugh Carter</forenames></author><author><keyname>Ferrigno-Stack</keyname><forenames>Josephine</forenames></author></authors><title>Quality of service monitoring: Performance metrics across proprietary
  content domains</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-074</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  We propose a quality of service (QoS) monitoring program for broadband access
to measure the impact of proprietary network spaces. Our paper surveys other
QoS policy initiatives, including those in the airline, and wireless and
wireline telephone industries, to situate broadband in the context of other
markets undergoing regulatory devolution. We illustrate how network
architecture can create impediments to open communications, and how QoS
monitoring can detect such effects. We present data from a field test of
QoS-monitoring software now in development. We suggest QoS metrics to gauge
whether information &quot;walled gardens&quot; represent a real threat for dividing the
Internet into proprietary spaces.
  To demonstrate our proposal, we are placing our software on the computers of
a sample of broadband subscribers. The software periodically conducts a battery
of tests that assess the quality of connections from the subscriber's computer
to various content sites. Any systematic differences in connection quality
between affiliated and non-affiliated content sites would warrant research into
the behavioral implications of those differences.
  QoS monitoring is timely because the potential for the Internet to break into
a loose network of proprietary content domains appears stronger than ever.
Recent court rulings and policy statements suggest a growing trend towards
relaxed scrutiny of mergers and the easing or elimination of content ownership
rules. This policy environment could lead to a market with a small number of
large, vertically integrated network operators, each pushing its proprietary
content on subscribers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109112</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109112</id><created>2001-09-25</created><authors><author><keyname>Forbes</keyname><forenames>Scott C.</forenames></author></authors><title>Elusive Threats: Security Weaknesses of Commercial Cellular Networks</title><categories>cs.CY</categories><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Commercial cellular telecommunications networks are routinely used as key
means of voice and data transport by both businesses and the Public. Despite
advances in encryption and other security measures, these commercial cellular
networks remain extremely vulnerable to malicious attacks and the loss of user
and data privacy and integrity. Such losses can result not only in mere
inconvenience, but in serious corporate and national security breaches. Because
of the potentially catastrophic nature of such security breaches, it behooves
cellular network operators, distributors, and users to explore these network
risk areas and mitigate them to the extent that current resources allow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109113</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109113</id><created>2001-09-25</created><authors><author><keyname>Nicholas</keyname><forenames>Kyle</forenames></author></authors><title>Digital Arroyos: An Examination of State Policy and Regulated Market
  Boundaries in Constructing Rural Internet Access</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This focused study on state-level policy and access patterns contributes to a
fuller understanding of how these invisible barriers work to structure access
and define rural communities. Combining both quantitative and qualitative data,
this study examines the role of geo-policy barriers in one of the largest and
most rural states in the nation.
  Expanded Area Service policies are state policies wherein phone customers can
expand their local calling area. Because useful Internet access requires a
flat-price connection, EAS policies can play a crucial role in connecting
citizens to one another. EAS policies (including Texas') tend to vary along
five dimensions (community of interest, customer scope, directionality, pricing
mechanism and policy scope). EAS policies that rely on regulated market
boundaries for definition can generate gross inequities in rural Internet
access. Interviews with Internet Service Providers in a case study of 25 rural
communities reveals that LATA and exchange boundaries, along with
geographically restricted infrastructure investments, curtail service provision
in remote areas. A statistical analysis of 1300 telephone exchanges, including
208 rural telephone exchanges in Texas reveals that the farther a community
lies from a metropolitan area the less likely they are to have reliable
Internet access
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109114</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109114</id><created>2001-09-25</created><authors><author><keyname>Nadel</keyname><forenames>Mark S.</forenames></author></authors><title>The Consumer Product Selection Process in an Internet Age: Obstacles to
  Maximum Effectiveness &amp; Policy Options</title><categories>cs.CY</categories><report-no>TPRC-2001-033</report-no><acm-class>K.4.m</acm-class><journal-ref>Harvard Journal of Law &amp; Technology, vol. 14 p183-266 (Fall 2000)</journal-ref><abstract>  Intermediaries, like real estate agents, Consumer Reports, and Zagats, have
long helped buyers to identify their most suitable options. Now, the
combination of databases and the Internet enables them to serve consumers
dramatically more effectively. This article begins by offering a three-part
framework for understanding the evolving forms of selection assistance. It then
focuses on numerous potential obstacles that could prevent shoppers from
enjoying the full benefits of these developing technologies. While concluding
that adjustments to business strategies and the enforcement of existing laws
can effectively overcome most of these impediments, the article identifies
several areas where proactive government action may be desirable, such as to
prevent the emergence of anticompetitive entry barriers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109115</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109115</id><created>2001-09-26</created><updated>2001-10-22</updated><authors><author><keyname>Stumpf</keyname><forenames>Ulrich</forenames></author></authors><title>Prospects for Improving Competition in Mobile Roaming</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-022</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  The ability to make international roaming calls is of increasing importance
to customers in Europe. This contrasts with various complaints that retail
prices of roaming calls are rigid and excessive. The focus if the paper is on
wholesale roaming, which is the prime determinant of retail roaming prices. The
paper analyses the structural conditions of wholesale roaming markets that have
impaired incentives to competition, namely high combined market share of the
two leading GSM 900 operators combined with second mover disadvantages for new
entrant GSM 1800 operators, and demand externalities. The paper argues that a
number of developments are under way that are likely to modify this situation
in the future. With the introduction of SIM over-the-air programming, home
mobile operators will be able to direct customers to networks with the lowest
charges. As dual mode handsets become ubiquitous and as new entrant GSM 1800
operators reach nationwide coverage, second-mover disadvantages will disappear.
Given the relatively small roaming volumes that GSM 1800 operators currently
provide, they should have an incentive to lower charges in exchange for
preferred roaming status. On the demand side of wholesale roaming markets, it
will be the larger GSM 900 operators, and in particular those with a
pan-European footprint, that will ask for lower charges in exchange for
preferred roaming status. This could discriminate against mobile operators in
downstream retail markets that do not have a pan-European footprint and that
lack the bargaining power. However, arbitrage by roaming brokers, new entry and
wider geographical markets on the retail roaming level will work against this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0109116</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0109116</id><created>2001-09-26</created><authors><author><keyname>Sharma</keyname><forenames>Gaurav</forenames></author><author><keyname>Trussell</keyname><forenames>H. Joel</forenames></author></authors><title>Digital Color Imaging</title><categories>cs.CV cs.GR</categories><acm-class>A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2</acm-class><journal-ref>IEEE Trans. Image Proc., vol. 6, no. 7, pp. 901-932, Jul. 1997</journal-ref><doi>10.1109/83.597268</doi><abstract>  This paper surveys current technology and research in the area of digital
color imaging. In order to establish the background and lay down terminology,
fundamental concepts of color perception and measurement are first presented
us-ing vector-space notation and terminology. Present-day color recording and
reproduction systems are reviewed along with the common mathematical models
used for representing these devices. Algorithms for processing color images for
display and communication are surveyed, and a forecast of research trends is
attempted. An extensive bibliography is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110001</id><created>2001-09-28</created><updated>2001-10-26</updated><authors><author><keyname>Nissenbaum</keyname><forenames>Helen</forenames></author><author><keyname>Friedman</keyname><forenames>Batya</forenames></author><author><keyname>Felten</keyname><forenames>Edward</forenames></author></authors><title>Computer Security: Competing Concepts</title><categories>cs.CY</categories><comments>14 pages</comments><acm-class>K.4.0;K.4.1;K.4.2;K.4.4</acm-class><abstract>  This paper focuses on a tension we discovered in the philosophical part of
our multidisciplinary project on values in web-browser security. Our project
draws on the methods and perspectives of empirical social science, computer
science, and philosophy to identify values embodied in existing web-browser
security and also to prescribe changes to existing systems (in particular,
Mozilla) so that values relevant to web-browser systems are better served than
presently they are. The tension, which we had not seen explicitly addressed in
any other work on computer security, emerged when we set out to extract from
the concept of security the set values that ought to guide the shape of
web-browser security. We found it impossible to construct an internally
consistent set of values until we realized that two robust -- and in places
competing -- conceptions of computer security were influencing our thinking. We
needed to pry these apart and make a primary commitment to one. One conception
of computer security invokes the ordinary meaning of security. According to it,
computer security should protect people -- computer users -- against dangers,
harms, and threats. Clearly this ordinary conception of security is already
informing much of the work and rhetoric surrounding computer security. But
another, substantively richer conception, also defines the aims and trajectory
of computer security -- computer security as an element of national security.
Although, like the ordinary conception, this one is also concerned with
protection against threats, its primary subject is the state, not the
individual. The two conceptions suggest divergent system-specifications, not
for all mechanisms but a significant few.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110002</id><created>2001-09-30</created><authors><author><keyname>Venturelli</keyname><forenames>Shalini</forenames></author></authors><title>Inventing E-Regulation in the US, EU and East Asia: Conflicting Social
  Visions of the Internet &amp; the Information Society</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-040</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper attempts to assess the international approach to Internet policy
in the context of distinctive socio-political frameworks evolving in the US,
the European Union (EU), and East Asia. The comparative review will develop a
set of underlying structural models of the Information Society particular to
each region, along with an analysis of their defining characteristics in
relation to one another. This examination demonstrates how each region, given
its regulatory legacy, has elected a different mix of good and bad
socio-political choices in public policy for the Internet. Despite the range
and diversity of paths to e-regulation suggested in these choices, none
adequately addresses the underlying issue of how to promote an innovative
society that is open to broad social participation. The paper evaluates
principal weaknesses in these regional models of Internet policy and argues the
need for re-conceptualizing the cultural, political and economic approach to
the new information space of the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110003</id><created>2001-10-01</created><authors><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames></author><author><keyname>Ramer</keyname><forenames>Arthur</forenames></author><author><keyname>Hoffmann</keyname><forenames>Achim</forenames></author></authors><title>The temporal calculus of conditional objects and conditional events</title><categories>cs.AI cs.LO</categories><comments>34 pages, 8 figures</comments><acm-class>I.2.3;I.2.4;F.4.1</acm-class><abstract>  We consider the problem of defining conditional objects (a|b), which would
allow one to regard the conditional probability Pr(a|b) as a probability of a
well-defined event rather than as a shorthand for Pr(ab)/Pr(b). The next issue
is to define boolean combinations of conditional objects, and possibly also the
operator of further conditioning. These questions have been investigated at
least since the times of George Boole, leading to a number of formalisms
proposed for conditional objects, mostly of syntactical, proof-theoretic vein.
  We propose a unifying, semantical approach, in which conditional events are
(projections of) Markov chains, definable in the three-valued extension of the
past tense fragment of propositional linear time logic, or, equivalently, by
three-valued counter-free Moore machines. Thus our conditional objects are
indeed stochastic processes, one of the central notions of modern probability
theory.
  Our model fulfills early ideas of Bruno de Finetti and, moreover, as we show
in a separate paper, all the previously proposed algebras of conditional events
can be isomorphically embedded in our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110004</id><created>2001-10-01</created><authors><author><keyname>Tyszkiewicz</keyname><forenames>Jerzy</forenames></author><author><keyname>Hoffmann</keyname><forenames>Achim</forenames></author><author><keyname>Ramer</keyname><forenames>Arthur</forenames></author></authors><title>Embedding conditional event algebras into temporal calculus of
  conditionals</title><categories>cs.AI cs.LO</categories><comments>45 pages, 7 figures</comments><acm-class>I.2.3;I.2.4;F.4.1</acm-class><abstract>  In this paper we prove that all the existing conditional event algebras embed
into a three-valued extension of temporal logic of discrete past time, which
the authors of this paper have proposed in anothe paper as a general model of
conditional events.
  First of all, we discuss the descriptive incompleteness of the cea's. In this
direction, we show that some important notions, like independence of
conditional events, cannot be properly addressed in the framework of
conditional event algebras, while they can be precisely formulated and analyzed
in the temporal setting.
  We also demonstrate that the embeddings allow one to use Markov chain
algorithms (suitable for the temporal calculus) for computing probabilities of
complex conditional expressions of the embedded conditional event algebras, and
that these algorithms can outperform those previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110005</id><created>2001-10-02</created><authors><author><keyname>Yamasaki</keyname><forenames>Tomohiro</forenames></author><author><keyname>Kobayashi</keyname><forenames>Hirotada</forenames></author><author><keyname>Imai</keyname><forenames>Hiroshi</forenames></author></authors><title>Two-way Quantum One-counter Automata</title><categories>cs.CC quant-ph</categories><comments>LaTeX2e, 14 pages, 3 figures</comments><acm-class>F.1.1</acm-class><abstract>  After the first treatments of quantum finite state automata by Moore and
Crutchfield and by Kondacs and Watrous, a number of papers study the power of
quantum finite state automata and their variants. This paper introduces a model
of two-way quantum one-counter automata (2Q1CAs), combining the model of
two-way quantum finite state automata (2QFAs) by Kondacs and Watrous and the
model of one-way quantum one-counter automata (1Q1CAs) by Kravtsev. We give the
definition of 2Q1CAs with well-formedness conditions. It is proved that 2Q1CAs
are at least as powerful as classical two-way deterministic one-counter
automata (2D1CAs), that is, every language L recognizable by 2D1CAs is
recognized by 2Q1CAs with no error. It is also shown that several
non-context-free languages including {a^n b^{n^2}} and {a^n b^{2^n}} are
recognizable by 2Q1CAs with bounded error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110006</id><created>2001-10-02</created><authors><author><keyname>Pereira</keyname><forenames>Pedro</forenames></author><author><keyname>Maz&#xf3;n</keyname><forenames>Cristina</forenames></author></authors><title>Electronic Commerce, Consumer Search and Retailing Cost Reduction</title><categories>cs.HC</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110007</id><created>2001-10-02</created><authors><author><keyname>Castro</keyname><forenames>Carlos</forenames></author><author><keyname>Manzano</keyname><forenames>Sebastian</forenames></author></authors><title>Variable and Value Ordering When Solving Balanced Academic Curriculum
  Problems</title><categories>cs.PL</categories><comments>12 pages, 4 figures</comments><report-no>DI-UTFSM TR 2001/1</report-no><acm-class>D.3</acm-class><journal-ref>Proceedings of 6th Workshop of the ERCIM WG on Constraints
  (Prague, June 2001)</journal-ref><abstract>  In this paper we present the use of Constraint Programming for solving
balanced academic curriculum problems. We discuss the important role that
heuristics play when solving a problem using a constraint-based approach. We
also show how constraint solving techniques allow to very efficiently solve
combinatorial optimization problems that are too hard for integer programming
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110008</id><created>2001-10-02</created><authors><author><keyname>Goldfarb</keyname><forenames>Avi</forenames></author></authors><title>Analyzing Website Choice Using Clickstream Data</title><categories>cs.CY</categories><comments>TPRC conference, 26 pages, 9 tables, 8 figures</comments><acm-class>K.4.4; J.1</acm-class><abstract>  This paper uses clickstream data from Plurimus Corp. (formerly Foveon Corp.)
to analyze user choice of Internet portals. It will show that commonly used
econometric models for examining grocery scanner data can be applied to
clickstream data on advertising-based online markets. Developing a framework to
study consumer choices of free websites is an essential step to better
understanding user behavior on the Internet. The main data for this study is a
clickstream data set consisting of every website visited by 2654 users from
December 27 1999 to March 31 2000. Using this data, I construct several
variables including search success, time spent searching, and whether a website
is an individual's starting page. I also have advertising and media mentions
data. This study helps to increase understanding of user behavior on the
Internet. It explores some key determinants of website choice and simulates
market responses to changes in the online environment. By better understanding
website choice, we can better evaluate the implications of policy decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110009</id><created>2001-10-02</created><authors><author><keyname>Pelletier</keyname><forenames>Olivier</forenames></author><author><keyname>Weimerskirch</keyname><forenames>Andre</forenames></author></authors><title>Algorithmic Self-Assembly of DNA Tiles and its Application to
  Cryptanalysis</title><categories>cs.CR</categories><comments>8 pages, 8 figures, will be submitted to the GECCO-2002 Conference</comments><acm-class>E.3;F.1.1</acm-class><abstract>  The early promises of DNA computing to deliver a massively parallel
architecture well-suited to computationally hard problems have so far been
largely unkept. Indeed, it is probably fair to say that only toy problems have
been addressed experimentally. Recent experimental development on algorithmic
self-assembly using DNA tiles seem to offer the most promising path toward a
potentially useful application of the DNA computing concept. In this paper, we
explore new geometries for algorithmic self-assembly, departing from those
previously described in the literature. This enables us to carry out
mathematical operations like binary multiplication or cyclic convolution
product. We then show how to use the latter operation to implement an attack
against the well-known public-key crypto system NTRU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110010</id><created>2001-10-02</created><authors><author><keyname>Dang</keyname><forenames>Zhe</forenames></author></authors><title>Pushdown Timed Automata: a Binary Reachability Characterization and
  Safety Verification</title><categories>cs.LO</categories><comments>30 pages</comments><acm-class>D.2.4;F.1.1</acm-class><abstract>  We consider pushdown timed automata (PTAs) that are timed automata (with
dense clocks) augmented with a pushdown stack. A configuration of a PTA
includes a control state, dense clock values and a stack word. By using the
pattern technique, we give a decidable characterization of the binary
reachability (i.e., the set of all pairs of configurations such that one can
reach the other) of a PTA. Since a timed automaton can be treated as a PTA
without the pushdown stack, we can show that the binary reachability of a timed
automaton is definable in the additive theory of reals and integers. The
results can be used to verify a class of properties containing linear relations
over both dense variables and unbounded discrete variables. The properties
previously could not be verified using the classic region technique nor
expressed by timed temporal logics for timed automata and CTL$^*$ for pushdown
systems. The results are also extended to other generalizations of timed
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110011</id><created>2001-10-03</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Lueker</keyname><forenames>George</forenames></author></authors><title>The Minimum Expectation Selection Problem</title><categories>cs.DS math.PR</categories><comments>13 pages, 1 figure. Full version of paper presented at 10th Int.
  Conf. Random Structures and Algorithms, Poznan, Poland, August 2001</comments><acm-class>F.2.2; G.1.6</acm-class><journal-ref>Random Structures and Algorithms 21:278-292, 2002</journal-ref><doi>10.1002/rsa.10061</doi><abstract>  We define the min-min expectation selection problem (resp. max-min
expectation selection problem) to be that of selecting k out of n given
discrete probability distributions, to minimize (resp. maximize) the
expectation of the minimum value resulting when independent random variables
are drawn from the selected distributions. We assume each distribution has
finitely many atoms. Let d be the number of distinct values in the support of
the distributions. We show that if d is a constant greater than 2, the min-min
expectation problem is NP-complete but admits a fully polynomial time
approximation scheme. For d an arbitrary integer, it is NP-hard to approximate
the min-min expectation problem with any constant approximation factor. The
max-min expectation problem is polynomially solvable for constant d; we leave
open its complexity for variable d. We also show similar results for binary
selection problems in which we must choose one distribution from each of n
pairs of distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110012</id><created>2001-10-03</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Bartak</keyname><forenames>Roman</forenames></author><author><keyname>Monfroy</keyname><forenames>Eric</forenames></author><author><keyname>Rossi</keyname><forenames>Francesca</forenames></author><author><keyname>Brand</keyname><forenames>Sebastian</forenames></author></authors><title>Proceedings of the 6th Annual Workshop of the ERCIM Working Group on
  Constraints</title><categories>cs.PL</categories><comments>2 invited talks, 17 papers</comments><acm-class>D.3.3</acm-class><abstract>  Homepage of the workshop proceedings, with links to all individually archived
papers
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110013</id><created>2001-10-03</created><authors><author><keyname>Hazelhurst</keyname><forenames>Scott</forenames></author></authors><title>A Proposal for Dynamic Access Lists for TCP/IP Packet Filering</title><categories>cs.NI cs.LO</categories><comments>12 pages. Shortened version appeared in SAICSIT 2001</comments><report-no>TR-Wits-CS-2001-2</report-no><acm-class>F2.2; F4.1; F4.2; I2.4</acm-class><abstract>  The use of IP filtering to improve system security is well established, and
although limited in what it can achieve has proved to be efficient and
effective.
  In the design of a security policy there is always a trade-off between
usability and security. Restricting access means that legitimate use of the
network is prevented; allowing access means illegitimate use may be allowed.
Static access list make finding a balance particularly stark -- we pay the
price of decreased security 100% of the time even if the benefit of increased
usability is only gained 1% of the time.
  Dynamic access lists would allow the rules to change for short periods of
time, and to allow local changes by non-experts. The network administrator can
set basic security guide-lines which allow certain basic services only. All
other services are restricted, but users are able to request temporary
exceptions in order to allow additional access to the network. These exceptions
are granted depending on the privileges of the user.
  This paper covers the following topics: (1) basic introduction to TCP/IP
filtering; (2) semantics for dynamic access lists and; (3) a proposed protocol
for allowing dynamic access; and (4) a method for representing access lists so
that dynamic update and look-up can be done efficiently performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110014</id><created>2001-10-03</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Simons</keyname><forenames>Gary</forenames></author><author><keyname>Huang</keyname><forenames>Chu-Ren</forenames></author></authors><title>The Open Language Archives Community and Asian Language Resources</title><categories>cs.CL cs.DL</categories><comments>8 pages, 2 figures</comments><acm-class>H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5</acm-class><journal-ref>Proceedings of the Workshop on Language Resources in Asia, 6th
  Natural Language Processing Pacific Rim Symposium (NLPRS), Tokyo, November
  2001</journal-ref><abstract>  The Open Language Archives Community (OLAC) is a new project to build a
worldwide system of federated language archives based on the Open Archives
Initiative and the Dublin Core Metadata Initiative. This paper aims to
disseminate the OLAC vision to the language resources community in Asia, and to
show language technologists and linguists how they can document their tools and
data in such a way that others can easily discover them. We describe OLAC and
the OLAC Metadata Set, then discuss two key issues in the Asian context:
language classification and multilingual resource classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110015</id><created>2001-10-03</created><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author></authors><title>Richer Syntactic Dependencies for Structured Language Modeling</title><categories>cs.CL</categories><comments>Proceedings of ASRU 2001, 4 pages</comments><acm-class>I.2.7; G.3</acm-class><abstract>  The paper investigates the use of richer syntactic dependencies in the
structured language model (SLM). We present two simple methods of enriching the
dependencies in the syntactic parse trees used for intializing the SLM. We
evaluate the impact of both methods on the perplexity (PPL) and
word-error-rate(WER, N-best rescoring) performance of the SLM. We show that the
new model achieves an improvement in PPL and WER over the baseline results
reported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)
corpora, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110016</id><created>2001-10-03</created><authors><author><keyname>Gideon</keyname><forenames>Carolyn</forenames></author><author><keyname>Camp</keyname><forenames>L Jean</forenames></author></authors><title>Limits To Certainty in QoS Pricing and Bandwidth</title><categories>cs.CY cs.HC</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-2091</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Advanced services require more reliable bandwidth than currently provided by
the Internet Protocol, even with the reliability enhancements provided by TCP.
More reliable bandwidth will be provided through QoS (quality of service), as
currently discussed widely. Yet QoS has some implications beyond providing
ubiquitous access to advance Internet service, which are of interest from a
policy perspective. In particular, what are the implications for price of
Internet services? Further, how will these changes impact demand and universal
service for the Internet. This paper explores the relationship between
certainty of bandwidth and certainty of price for Internet services over a
statistically shared network and finds that these are mutually exclusive goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110017</id><created>2001-10-04</created><authors><author><keyname>Feld</keyname><forenames>Harold</forenames></author></authors><title>The Increased Need For FCC Merger Review In A Networked World</title><categories>cs.CY</categories><comments>29th TPRC Conference,2001</comments><report-no>TPRC-2001-052</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  Recently, the FCC announced a new standard for review in mergers. Under the
new standard, mass media mergers that comply with existing rules will
automaticly receive approval, while those that do not will receive a more
searching review. Common carrier mergers, however, will continue to receive the
4=part test established in Bell Atlantic/Nynex. The new standard fails to take
into account the complexities of the emerging, converged networked world, and
is essentially obsolete on arrival. Looking to those areas where Congress has
required an additional public interest review of mergers, a pattern emerges.
The emergence of vast, vertically integrated networks of content and conduit
fit the historic pattern of areas requiring piublic interest review and
re-enforce the need for increased, rather than decreased merger review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110018</id><created>2001-10-04</created><updated>2001-10-22</updated><authors><author><keyname>Cannon</keyname><forenames>Robert</forenames></author></authors><title>ENUM: The Collision of Telephony and DNS Policy</title><categories>cs.GL</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-XXX</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  ENUM marks either the convergence or collision of the public telephone
network with the Internet. ENUM is an innovation in the domain name system
(DNS). It starts with numerical domain names that are used to query DNS name
servers. The servers respond with address information found in DNS records.
This can be telephone numbers, email addresses, fax numbers, SIP addresses, or
other information. The concept is to use a single number in order to obtain a
plethora of contact information.
  By convention, the Internet Engineering Task Force (IETF) ENUM Working Group
determined that an ENUM number would be the same numerical string as a
telephone number. In addition, the assignee of an ENUM number would be the
assignee of that telephone number. But ENUM could work with any numerical
string or, in fact, any domain name. The IETF is already working on using E.212
numbers with ENUM. [Abridged]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110019</id><created>2001-10-05</created><authors><author><keyname>Gudkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Johnson</keyname><forenames>Joseph E.</forenames></author></authors><title>New approach for network monitoring and intrusion detection</title><categories>cs.CR</categories><acm-class>K.6.5; E.4; F.1.1; F.1.3</acm-class><abstract>  The approach for a network behavior description in terms of numerical
time-dependant functions of the protocol parameters is suggested. This provides
a basis for application of methods of mathematical and theoretical physics for
information flow analysis on network and for extraction of patterns of typical
network behavior. The information traffic can be described as a trajectory in
multi-dimensional parameter-time space with dimension about 10-12. Based on
this study some algorithms for the proposed intrusion detection system are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110020</id><created>2001-10-08</created><authors><author><keyname>Sarda</keyname><forenames>N. L.</forenames></author></authors><title>Structuring Business Metadata in Data Warehouse Systems for Effective
  Business Support</title><categories>cs.DB</categories><acm-class>H.2.1, H.2.7</acm-class><abstract>  Large organizations today are being served by different types of data
processing and informations systems, ranging from the operational (OLTP)
systems, data warehouse systems, to data mining and business intelligence
applications. It is important to create an integrated repository of what these
systems contain and do in order to use them collectively and effectively. The
repository contains metadata of source systems, data warehouse, and also the
business metadata. Decision support and business analysis require extensive and
in-depth understanding of business entities, tasks, rules and the environment.
The purpose of business metadata is to provide this understanding. Realizing
the importance of metadata, many standardization efforts has been initiated to
define metadata models. In trying to define an integrated metadata and
information systems for a banking application, we discover some important
limitations or inadequacies of the business metadata proposals. They relate to
providing an integrated and flexible inter-operability and navigation between
metadata and data, and to the important issue of systematically handling
temporal characteristics and evolution of the metadata itself.
  In this paper, we study the issue of structuring business metadata so that it
can provide a context for business management and decision support when
integrated with data warehousing. We define temporal object-oriented business
metadata model, and relate it both to the technical metadata and the data
warehouse. We also define ways of accessing and navigating metadata in
conjunction with data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110021</id><created>2001-10-08</created><authors><author><keyname>Burtsev</keyname><forenames>Mikhail S.</forenames></author><author><keyname>Redko</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Gusarev</keyname><forenames>Roman V.</forenames></author></authors><title>Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior</title><categories>cs.NE</categories><comments>9 pages, 5 figures. Full version of poster presentation on ECAL 2001
  (see &quot;Advances in Artificial Life.&quot; J. Kelemen, P. Sosik (Eds.), 6th European
  Conference, ECAL 2001, Prague, Czech Republic, September 10-14, 2001,
  Proceedings, p. 413.)</comments><acm-class>I.2.6; I.2.8; I.2.11</acm-class><abstract>  The process of evolutionary emergence of purposeful adaptive behavior is
investigated by means of computer simulations. The model proposed implies that
there is an evolving population of simple agents, which have two natural needs:
energy and reproduction. Any need is characterized quantitatively by a
corresponding motivation. Motivations determine goal-directed behavior of
agents. The model demonstrates that purposeful behavior does emerge in the
simulated evolutionary processes. Emergence of purposefulness is accompanied by
origin of a simple hierarchy in the control system of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110022</id><created>2001-10-09</created><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Capra</keyname><forenames>Robert</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author></authors><title>Mixed-Initiative Interaction = Mixed Computation</title><categories>cs.PL cs.HC</categories><acm-class>F3.2; H.5.2</acm-class><abstract>  We show that partial evaluation can be usefully viewed as a programming model
for realizing mixed-initiative functionality in interactive applications.
Mixed-initiative interaction between two participants is one where the parties
can take turns at any time to change and steer the flow of interaction. We
concentrate on the facet of mixed-initiative referred to as `unsolicited
reporting' and demonstrate how out-of-turn interactions by users can be modeled
by `jumping ahead' to nested dialogs (via partial evaluation). Our approach
permits the view of dialog management systems in terms of their native support
for staging and simplifying interactions; we characterize three different
voice-based interaction technologies using this viewpoint. In particular, we
show that the built-in form interpretation algorithm (FIA) in the VoiceXML
dialog management architecture is actually a (well disguised) combination of an
interpreter and a partial evaluator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110023</id><created>2001-10-09</created><updated>2005-08-30</updated><authors><author><keyname>Dovier</keyname><forenames>Agostino</forenames></author><author><keyname>Pontelli</keyname><forenames>Enrico</forenames></author><author><keyname>Rossi</keyname><forenames>Gianfranco</forenames></author></authors><title>Set Unification</title><categories>cs.LO cs.AI cs.SC</categories><comments>58 pages, 9 figures, 1 table. To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>F.2.2; F.4.1; I.1.2; I.2.3</acm-class><abstract>  The unification problem in algebras capable of describing sets has been
tackled, directly or indirectly, by many researchers and it finds important
applications in various research areas--e.g., deductive databases, theorem
proving, static analysis, rapid software prototyping. The various solutions
proposed are spread across a large literature. In this paper we provide a
uniform presentation of unification of sets, formalizing it at the level of set
theory. We address the problem of deciding existence of solutions at an
abstract level. This provides also the ability to classify different types of
set unification problems. Unification algorithms are uniformly proposed to
solve the unification problem in each of such classes.
  The algorithms presented are partly drawn from the literature--and properly
revisited and analyzed--and partly novel proposals. In particular, we present a
new goal-driven algorithm for general ACI1 unification and a new simpler
algorithm for general (Ab)(Cl) unification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110024</id><created>2001-10-10</created><authors><author><keyname>Kobara</keyname><forenames>Kazukuni</forenames></author><author><keyname>Imai</keyname><forenames>Hideki</forenames></author></authors><title>Pretty-Simple Password-Authenticated Key-Exchange Protocol</title><categories>cs.CR</categories><acm-class>K.6.5</acm-class><abstract>  We propose pretty simple password-authenticated key-exchange protocol which
is based on the difficulty of solving DDH problem. It has the following
advantages: (1) Both $y_1$ and $y_2$ in our protocol are independent and thus
they can be pre-computed and can be sent independently. This speeds up the
protocol. (2) Clients and servers can use almost the same algorithm. This
reduces the implementation costs without accepting replay attacks and abuse of
entities as oracles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110025</id><created>2001-10-10</created><updated>2005-01-25</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Spakowski</keyname><forenames>Holger</forenames></author></authors><title>Recognizing When Heuristics Can Approximate Minimum Vertex Covers Is
  Complete for Parallel Access to NP</title><categories>cs.CC</categories><comments>16 pages, 2 figures</comments><acm-class>F.1.3; F.2.2</acm-class><abstract>  For both the edge deletion heuristic and the maximum-degree greedy heuristic,
we study the problem of recognizing those graphs for which that heuristic can
approximate the size of a minimum vertex cover within a constant factor of r,
where r is a fixed rational number. Our main results are that these problems
are complete for the class of problems solvable via parallel access to NP. To
achieve these main results, we also show that the restriction of the vertex
cover problem to those graphs for which either of these heuristics can find an
optimal solution remains NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110026</id><created>2001-10-10</created><authors><author><keyname>Lopatenko</keyname><forenames>Andrei</forenames></author></authors><title>Information retrieval in Current Research Information Systems</title><categories>cs.IR cs.DL</categories><comments>8 pages, ontology description included, position paper at the
  Workshop on Knowledge Markup and Semantic Annotation at K-CAP'2001</comments><acm-class>H.3.3; H.3.4; H.3.7</acm-class><abstract>  In this paper we describe the requirements for research information systems
and problems which arise in the development of such system. Here is shown which
problems could be solved by using of knowledge markup technologies. Ontology
for Research Information System offered. Architecture for collecting research
data and providing access to it is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110027</id><created>2001-10-11</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames></author></authors><title>Part-of-Speech Tagging with Two Sequential Transducers</title><categories>cs.CL</categories><comments>7 pages, 4 figures, LaTeX (+ eps)</comments><acm-class>F.1.1; I.2.7</acm-class><journal-ref>Proc. CLIN 2000, pp. 88-96, Tilburg, The Netherlands. November 3</journal-ref><abstract>  We present a method of constructing and using a cascade consisting of a left-
and a right-sequential finite-state transducer (FST), T1 and T2, for
part-of-speech (POS) disambiguation. Compared to an HMM, this FST cascade has
the advantage of significantly higher processing speed, but at the cost of
slightly lower accuracy. Applications such as Information Retrieval, where the
speed can be more important than accuracy, could benefit from this approach.
  In the process of tagging, we first assign every word a unique ambiguity
class c_i that can be looked up in a lexicon encoded by a sequential FST. Every
c_i is denoted by a single symbol, e.g. [ADJ_NOUN], although it represents a
set of alternative tags that a given word can occur with. The sequence of the
c_i of all words of one sentence is the input to our FST cascade. It is mapped
by T1, from left to right, to a sequence of reduced ambiguity classes r_i.
Every r_i is denoted by a single symbol, although it represents a set of
alternative tags. Intuitively, T1 eliminates the less likely tags from c_i,
thus creating r_i. Finally, T2 maps the sequence of r_i, from right to left, to
a sequence of single POS tags t_i. Intuitively, T2 selects the most likely t_i
from every r_i.
  The probabilities of all t_i, r_i, and c_i are used only at compile time, not
at run time. They do not (directly) occur in the FSTs, but are &quot;implicitly
contained&quot; in their structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110028</id><created>2001-10-11</created><authors><author><keyname>Harper</keyname><forenames>Robert</forenames></author><author><keyname>Pfenning</keyname><forenames>Frank</forenames></author></authors><title>On Equivalence and Canonical Forms in the LF Type Theory</title><categories>cs.LO</categories><comments>41 pages</comments><acm-class>F.4.1</acm-class><abstract>  Decidability of definitional equality and conversion of terms into canonical
form play a central role in the meta-theory of a type-theoretic logical
framework. Most studies of definitional equality are based on a confluent,
strongly-normalizing notion of reduction. Coquand has considered a different
approach, directly proving the correctness of a practical equivalance algorithm
based on the shape of terms. Neither approach appears to scale well to richer
languages with unit types or subtyping, and neither directly addresses the
problem of conversion to canonical.
  In this paper we present a new, type-directed equivalence algorithm for the
LF type theory that overcomes the weaknesses of previous approaches. The
algorithm is practical, scales to richer languages, and yields a new notion of
canonical form sufficient for adequate encodings of logical systems. The
algorithm is proved complete by a Kripke-style logical relations argument
similar to that suggested by Coquand. Crucially, both the algorithm itself and
the logical relations rely only on the shapes of types, ignoring dependencies
on terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110029</id><created>2001-10-13</created><updated>2002-01-04</updated><authors><author><keyname>Czarapata</keyname><forenames>P.</forenames><affiliation>FNAL</affiliation></author><author><keyname>Hartill</keyname><forenames>D.</forenames><affiliation>Cornell</affiliation></author><author><keyname>Myers</keyname><forenames>S.</forenames><affiliation>CERN</affiliation></author><author><keyname>Peggs</keyname><forenames>S.</forenames><affiliation>BNL</affiliation></author><author><keyname>Phinney</keyname><forenames>N.</forenames><affiliation>SLAC</affiliation></author><author><keyname>Serio</keyname><forenames>M.</forenames><affiliation>INFN</affiliation></author><author><keyname>Toge</keyname><forenames>N.</forenames><affiliation>KEK</affiliation></author><author><keyname>Willeke</keyname><forenames>F.</forenames><affiliation>DESY</affiliation></author><author><keyname>Zhang</keyname><forenames>C.</forenames><affiliation>IHEP Beijing</affiliation></author></authors><title>How to Commission, Operate and Maintain a Large Future Accelerator
  Complex from Far Remote</title><categories>cs.OH</categories><comments>ICALEPCS 2001 abstract ID No. FRBI001 invited talk submitting author
  F. Willeke 5 pages, 1 figure</comments><acm-class>A.m</acm-class><journal-ref>eConf C011127 (2001) FRBI001</journal-ref><abstract>  A study on future large accelerators [1] has considered a facility, which is
designed, built and operated by a worldwide collaboration of equal partner
institutions, and which is remote from most of these institutions. The full
range of operation was considered including commi-ssioning, machine
development, maintenance, trouble shooting and repair. Experience from existing
accele-rators confirms that most of these activities are already performed
'remotely'. The large high-energy physics ex-periments and astronomy projects,
already involve inter-national collaborations of distant institutions. Based on
this experience, the prospects for a machine operated remotely from far sites
are encouraging. Experts from each laboratory would remain at their home
institution but continue to participate in the operation of the machine after
construction. Experts are required to be on site only during initial
commissioning and for par-ticularly difficult problems. Repairs require an
on-site non-expert maintenance crew. Most of the interventions can be made
without an expert and many of the rest resolved with remote assistance. There
appears to be no technical obstacle to controlling an accelerator from a
distance. The major challenge is to solve the complex management and
communication problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110030</id><created>2001-10-15</created><updated>2002-11-07</updated><authors><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author></authors><title>Dense point sets have sparse Delaunay triangulations</title><categories>cs.CG cs.DM</categories><comments>31 pages, 11 figures. Full version of SODA 2002 paper. Also available
  at http://www.cs.uiuc.edu/~jeffe/pubs/screw.html</comments><acm-class>F.2.2, G.2.m</acm-class><abstract>  The spread of a finite set of points is the ratio between the longest and
shortest pairwise distances. We prove that the Delaunay triangulation of any
set of n points in R^3 with spread D has complexity O(D^3). This bound is tight
in the worst case for all D = O(sqrt{n}). In particular, the Delaunay
triangulation of any dense point set has linear complexity. We also generalize
this upper bound to regular triangulations of k-ply systems of balls, unions of
several dense point sets, and uniform samples of smooth surfaces. On the other
hand, for any n and D=O(n), we construct a regular triangulation of complexity
Omega(nD) whose n vertices have spread D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110031</id><created>2001-10-16</created><authors><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Sen</keyname><forenames>Pranab</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Sundar</forenames></author></authors><title>Depth-3 Arithmetic Circuits for S^2_n(X) and Extensions of the
  Graham-Pollack Theorem</title><categories>cs.DM math.CO</categories><comments>30 pages. 1 figure. A preliminary version appeared in FSTTCS 2000.
  This is the full version of that paper</comments><acm-class>G.2.1;G.2.2</acm-class><abstract>  We consider the problem of computing the second elementary symmetric
polynomial S^2_n(X) using depth-three arithmetic circuits of the form &quot;sum of
products of linear forms&quot;. We consider this problem over several fields and
determine EXACTLY the number of multiplication gates required. The lower bounds
are proved for inhomogeneous circuits where the linear forms are allowed to
have constants; the upper bounds are proved in the homogeneous model. For reals
and rationals, the number of multiplication gates required is exactly n-1; in
most other cases, it is \ceil{n/2}. This problem is related to the
Graham-Pollack theorem in algebraic graph theory. In particular, our results
answer the following question of Babai and Frankl: what is the minimum number
of complete bipartite graphs required to cover each edge of a complete graph an
odd number of times? We show that for infinitely many n, the answer is
\ceil{n/2}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110032</id><created>2001-10-16</created><authors><author><keyname>Grant</keyname><forenames>J.</forenames></author><author><keyname>Minker</keyname><forenames>J.</forenames></author></authors><title>A logic-based approach to data integration</title><categories>cs.DB cs.AI</categories><comments>47 pages, Accepted for publication in the Theory and Practice of
  Logic Programming</comments><acm-class>H.2, I.2.4</acm-class><abstract>  An important aspect of data integration involves answering queries using
various resources rather than by accessing database relations. The process of
transforming a query from the database relations to the resources is often
referred to as query folding or answering queries using views, where the views
are the resources. We present a uniform approach that includes as special cases
much of the previous work on this subject. Our approach is logic-based using
resolution. We deal with integrity constraints, negation, and recursion also
within this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110034</id><created>2001-10-17</created><updated>2003-07-10</updated><authors><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author></authors><title>Inference of termination conditions for numerical loops in Prolog</title><categories>cs.PL cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming. To appear in
  Theory and Practice of Logic Programming</comments><acm-class>D.1.6; D.2.4</acm-class><abstract>  We present a new approach to termination analysis of numerical computations
in logic programs. Traditional approaches fail to analyse them due to non
well-foundedness of the integers. We present a technique that allows overcoming
these difficulties. Our approach is based on transforming a program in a way
that allows integrating and extending techniques originally developed for
analysis of numerical computations in the framework of query-mapping pairs with
the well-known framework of acceptability. Such an integration not only
contributes to the understanding of termination behaviour of numerical
computations, but also allows us to perform a correct analysis of such
computations automatically, by extending previous work on a constraint-based
approach to termination. Finally, we discuss possible extensions of the
technique, including incorporating general term orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110035</id><created>2001-10-17</created><updated>2003-12-24</updated><authors><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author><author><keyname>De Schreye</keyname><forenames>Danny</forenames></author></authors><title>On termination of meta-programs</title><categories>cs.PL cs.LO</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><acm-class>D.1.6; D.2.4</acm-class><abstract>  The term {\em meta-programming} refers to the ability of writing programs
that have other programs as data and exploit their semantics.
  The aim of this paper is presenting a methodology allowing us to perform a
correct termination analysis for a broad class of practical meta-interpreters,
including negation and performing different tasks during the execution. It is
based on combining the power of general orderings, used in proving termination
of term-rewrite systems and programs, and on the well-known acceptability
condition, used in proving termination of logic programs.
  The methodology establishes a relationship between the ordering needed to
prove termination of the interpreted program and the ordering needed to prove
termination of the meta-interpreter together with this interpreted program. If
such a relationship is established, termination of one of those implies
termination of the other one, i.e., the meta-interpreter preserves termination.
  Among the meta-interpreters that are analysed correctly are a proof trees
constructing meta-interpreter, different kinds of tracers and reasoners.
  To appear without appendix in Theory and Practice of Logic Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110036</id><created>2001-10-17</created><authors><author><keyname>Blockeel</keyname><forenames>Hendrik</forenames></author><author><keyname>Struyf</keyname><forenames>Jan</forenames></author></authors><title>Efficient algorithms for decision tree cross-validation</title><categories>cs.LG</categories><comments>9 pages, 6 figures.
  http://www.cs.kuleuven.ac.be/cgi-bin-dtai/publ_info.pl?id=34784</comments><acm-class>I.2.6</acm-class><journal-ref>H. Blockeel and J. Struyf. Efficient algorithms for decision tree
  cross-validation. Proceedings of the Eighteenth International Conference on
  Machine Learning (C. Brodley and A. Danyluk, eds.), Morgan Kaufmann, 2001,
  pp. 11-18</journal-ref><abstract>  Cross-validation is a useful and generally applicable technique often
employed in machine learning, including decision tree induction. An important
disadvantage of straightforward implementation of the technique is its
computational overhead. In this paper we show that, for decision trees, the
computational overhead of cross-validation can be reduced significantly by
integrating the cross-validation with the normal decision tree induction
process. We discuss how existing decision tree algorithms can be adapted to
this aim, and provide an analysis of the speedups these adaptations may yield.
The analysis is supported by experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110037</id><created>2001-10-17</created><authors><author><keyname>Mazur</keyname><forenames>Nancy</forenames><affiliation>Dept. of Computer Science K.U.Leuven</affiliation></author><author><keyname>Ross</keyname><forenames>Peter</forenames><affiliation>Mission Critical</affiliation></author><author><keyname>Janssens</keyname><forenames>Gerda</forenames><affiliation>Dept. of Computer Science K.U.Leuven</affiliation></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames><affiliation>Dept. of Computer Science K.U.Leuven</affiliation></author></authors><title>Practical Aspects for a Working Compile Time Garbage Collection System
  for Mercury</title><categories>cs.PL</categories><comments>15 pages. A version of this paper will appear in Proceeding of the
  Seventeenth International Conference on Logic Programming (ICLP2001)</comments><acm-class>D.3.4;I.2.3</acm-class><abstract>  Compile-time garbage collection (CTGC) is still a very uncommon feature
within compilers. In previous work we have developed a compile-time structure
reuse system for Mercury, a logic programming language. This system indicates
which datastructures can safely be reused at run-time. As preliminary
experiments were promising, we have continued this work and have now a working
and well performing near-to-ship CTGC-system built into the Melbourne Mercury
Compiler (MMC).
  In this paper we present the multiple design decisions leading to this
system, we report the results of using CTGC for a set of benchmarks, including
a real-world program, and finally we discuss further possible improvements.
Benchmarks show substantial memory savings and a noticeable reduction in
execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110038</id><created>2001-10-18</created><authors><author><keyname>Seiferas</keyname><forenames>Joel</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Counting is Easy</title><categories>cs.CC cs.DS</categories><acm-class>E.1, F.1.1., F.2.2, G.2.1, E.2, E.4</acm-class><journal-ref>J. Seiferas and P.M.B. Vitanyi, Counting is easy, J. Assoc. Comp.
  Mach. 35 (1988), pp. 985-1000</journal-ref><abstract>  For any fixed $k$, a remarkably simple single-tape Turing machine can
simulate $k$ independent counters in real time. Informally, a counter is a
storage unit that maintains a single integer (initially 0), incrementing it,
decrementing it, or reporting its sign (positive, negative, or zero) on
command. Any automaton that responds to each successive command as a counter
would is said to simulate a counter. (Only for a sign inquiry is the response
of interest, of course. And zeroness is the only real issue, since a simulator
can readily use zero detection to keep track of positivity and negativity in
finite-state control. In this paper we describe a remarkably simple real-time
simulation, based on just five simple rewriting rules, of any fixed number $k$
of independent counters. On a Turing machine with a single, binary work tape,
the simulation runs in real time, handling an arbitrary counter command at each
step. The space used by the simulation can be held to $(k+\epsilon) \log_2 n$
bits for the first $n$ commands, for any specified $\epsilon &gt; 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110039</id><created>2001-10-18</created><authors><author><keyname>Jiang</keyname><forenames>Tao</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Seiferas</keyname><forenames>Joel</forenames><affiliation>Rochester University</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Two heads are better than two tapes</title><categories>cs.CC</categories><comments>LaTeX, 16 pages. The final journal paper contains minor corrections
  as well as some extra typos, but, also, some clarifying figures. A close copy
  to that can be downloaded from http://www.cwi.nl/~paulv/complexity.html</comments><acm-class>F.2.2, F.1.1</acm-class><journal-ref>T. Jiang, J. Seiferas and P.M.B. Vitanyi, Two heads are better
  than two tapes, J. Assoc. Comp. Mach., 44:2(1997), 237--256</journal-ref><abstract>  We show that a Turing machine with two single-head one-dimensional tapes
cannot recognize the set {x2x'| x \in {0,1}^* and x' is a prefix of x} in real
time, although it can do so with three tapes, two two-dimensional tapes, or one
two-head tape, or in linear time with just one tape. In particular, this
settles the longstanding conjecture that a two-head Turing machine can
recognize more languages in real time if its heads are on the same
one-dimensional tape than if they are on separate one-dimensional tapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110040</id><created>2001-10-18</created><authors><author><keyname>Li</keyname><forenames>Ming</forenames><affiliation>University of Waterloo</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>A New Approach to Formal Language Theory by Kolmogorov Complexity</title><categories>cs.CC</categories><acm-class>F.4.2, F.2, I.2.7</acm-class><journal-ref>M. Li and P.M.B. Vitanyi, A new approach to formal language theory
  by Kolmogorov complexity, SIAM J. Comput., 24:2(1995), 398-410</journal-ref><abstract>  We present a new approach to formal language theory using Kolmogorov
complexity. The main results presented here are an alternative for pumping
lemma(s), a new characterization for regular languages, and a new method to
separate deterministic context-free languages and nondeterministic context-free
languages. The use of the new `incompressibility arguments' is illustrated by
many examples. The approach is also successful at the high end of the Chomsky
hierarchy since one can quantify nonrecursiveness in terms of Kolmogorov
complexity. (This is a preliminary uncorrected version. The final version is
the one published in SIAM J. Comput., 24:2(1995), 398-410.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110041</id><created>2001-10-18</created><authors><author><keyname>Paquet</keyname><forenames>Sebastien</forenames></author></authors><title>Towards Solving the Interdisciplinary Language Barrier Problem</title><categories>cs.CY cs.CL cs.IR</categories><comments>15 pages, 1 figure; prototype implementation is located at
  http://www.iro.umontreal.ca/~paquetse/knoweb/000_INTRODUCTION.html</comments><acm-class>H3.1; I2.4; K3.1</acm-class><abstract>  This work aims to make it easier for a specialist in one field to find and
explore ideas from another field which may be useful in solving a new problem
arising in his practice. It presents a methodology which serves to represent
the relationships that exist between concepts, problems, and solution patterns
from different fields of human activity in the form of a graph. Our approach is
based upon generalization and specialization relationships and problem solving.
It is simple enough to be understood quite easily, and general enough to enable
coherent integration of concepts and problems from virtually any field. We have
built an implementation which uses the World Wide Web as a support to allow
navigation between graph nodes and collaborative development of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110042</id><created>2001-10-18</created><authors><author><keyname>Treese</keyname><forenames>G. Winfield</forenames></author><author><keyname>Stewart</keyname><forenames>Lawrence C.</forenames></author></authors><title>An Architecture for Security and Privacy in Mobile Communications</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><report-no>TPRC-2001-101</report-no><acm-class>K.4.m Miscellaneous</acm-class><abstract>  There is much discussion and debate about how to improve the security and
privacy of mobile communication systems, both voice and data. Most proposals
attempt to provide incremental improvements to systems that are deployed today.
Indeed, only incremental improvements are possible, given the regulatory,
technological, economic, and historical structure of the telecommunications
system. In this paper, we conduct a ``thought experiment'' to redesign the
mobile communications system to provide a high level of security and privacy
for the users of the system. We discuss the important requirements and how a
different architecture might successfully satisfy them. In doing so, we hope to
illuminate the possibilities for secure and private systems, as well as explore
their real limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110043</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110043</id><created>2001-10-21</created><authors><author><keyname>Annam</keyname><forenames>Shireesh Reddy</forenames></author></authors><title>An Overview of Computer security</title><categories>cs.CY cs.NI</categories><comments>11 pages,PDF, Comments about threats to the computer security and
  their protection mechanisms</comments><acm-class>D.4.6;K.4.2</acm-class><abstract>  As more business activities are being automated and an increasing number of
computers are being used to store vital and sensitive information the need for
secure computer systems becomes more apparent. These systems can be achieved
only through systematic design; they cannot be achieved through haphazard
seat-of-the-pants methods.This paper introduces some known threats to the
computer security, categorizes the threats, and analyses protection mechanisms
and techniques for countering the threats. The threats have been classified
more so as definitions and then followed by the classifications of these
threats. Also mentioned are the protection mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110044</id><created>2001-10-22</created><authors><author><keyname>Cohen</keyname><forenames>Sara</forenames></author><author><keyname>Kanza</keyname><forenames>Yaron</forenames></author><author><keyname>Kogan</keyname><forenames>Yakov</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author></authors><title>EquiX--A Search and Query Language for XML</title><categories>cs.DB</categories><comments>This is a preprint of an article accepted for publication in Journal
  of the American Society for Information Science and Technology @ copyright
  2001 John Wiley &amp; Sons, Inc</comments><report-no>CW-322</report-no><acm-class>H.2.5; H.2.3</acm-class><abstract>  EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graph-based
abstract syntax and a formal concrete syntax are presented for EquiX queries.
In addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110046</id><created>2001-10-22</created><authors><author><keyname>Selian</keyname><forenames>Audrey N.</forenames></author></authors><title>From 2G TO 3G - The Evolution of International Cellular Standards</title><categories>cs.CY</categories><comments>29th TPRC Conference, 2001</comments><acm-class>K.4.m Miscellaneous</acm-class><journal-ref>TPRC-2001-102</journal-ref><abstract>  The purpose of this paper is to examine the major factors surrouding and
contributing to the creation (and success) of Europe's 2nd generation 'GSM'
cellular system, and compare and contrast it to key events and recent
developments in 3rd generation 'IMT-2000' systems. The objective is to
ascertain whether lessons from the development of one system can be applied to
the other, and what implications 2G has for the development and assessment of
3G technologies. Among the major themes incorporated into this assessment is
the concept of cooperation, and its role in bringing about the collaboration
and integration necessary to support the success of an international cellular
standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110047</id><created>2001-10-23</created><authors><author><keyname>Heath</keyname><forenames>Lenwood S.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Sederoff</keyname><forenames>Ronald R.</forenames></author><author><keyname>Whetten</keyname><forenames>Ross W.</forenames></author><author><keyname>Chevone</keyname><forenames>Boris I.</forenames></author><author><keyname>Struble</keyname><forenames>Craig A.</forenames></author><author><keyname>Jouenne</keyname><forenames>Vincent Y.</forenames></author><author><keyname>Chen</keyname><forenames>Dawei</forenames></author><author><keyname>van Zyl</keyname><forenames>Leonel</forenames></author><author><keyname>Alscher</keyname><forenames>Ruth G.</forenames></author></authors><title>The Expresso Microarray Experiment Management System: The Functional
  Genomics of Stress Responses in Loblolly Pine</title><categories>cs.OH cs.CE q-bio.GN</categories><acm-class>J.3</acm-class><abstract>  Conception, design, and implementation of cDNA microarray experiments present
a variety of bioinformatics challenges for biologists and computational
scientists. The multiple stages of data acquisition and analysis have motivated
the design of Expresso, a system for microarray experiment management. Salient
aspects of Expresso include support for clone replication and randomized
placement; automatic gridding, extraction of expression data from each spot,
and quality monitoring; flexible methods of combining data from individual
spots into information about clones and functional categories; and the use of
inductive logic programming for higher-level data analysis and mining. The
development of Expresso is occurring in parallel with several generations of
microarray experiments aimed at elucidating genomic responses to drought stress
in loblolly pine seedlings. The current experimental design incorporates 384
pine cDNAs replicated and randomly placed in two specific microarray layouts.
We describe the design of Expresso as well as results of analysis with Expresso
that suggest the importance of molecular chaperones and membrane transport
proteins in mechanisms conferring successful adaptation to long-term drought
stress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110048</id><created>2001-10-23</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author><author><keyname>Karplus</keyname><forenames>Walter</forenames></author><author><keyname>Liu</keyname><forenames>Damon</forenames></author></authors><title>Multivariant Branching Prediction, Reflection, and Retrospection</title><categories>cs.CE cs.DC</categories><comments>16 pages 1 figure</comments><acm-class>F.1.2; F.1.1</acm-class><abstract>  In branching simulation, a novel approach to simulation presented in this
paper, a multiplicity of plausible scenarios are concurrently developed and
implemented. In conventional simulations of complex systems, there arise from
time to time uncertainties as to which of two or more alternatives are more
likely to be pursued by the system being simulated. Under these conditions the
simulationist makes a judicious choice of one of these alternatives and embeds
this choice in the simulation model. By contrast, in the branching approach,
two or more of such alternatives (or branches) are included in the model and
implemented for concurrent computer solution. The theoretical foundations for
branching simulation as a computational process are in the domains of
alternating Turing machines, molecular computing, and E-machines. Branching
simulations constitute the development of diagrams of scenarios representing
significant, alternative flows of events. Logical means for interpretation and
investigation of the branching simulation and prediction are provided by the
logical theories of possible worlds, which have been formalized by the
construction of logical varieties. Under certain conditions, the branching
approach can considerably enhance the efficiency of computer simulations and
provide more complete insights into the interpretation of predictions based on
simulations. As an example, the concepts developed in this paper have been
applied to a simulation task that plays an important role in radiology - the
noninvasive treatment of brain aneurysms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110049</id><created>2001-10-24</created><authors><author><keyname>Harary</keyname><forenames>Frank</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>A Symmetric Strategy in Graph Avoidance Games</title><categories>cs.DM cs.CC</categories><comments>14 pages, 6 figures; for a video of a talk based on a preliminary
  version see
  http://www.msri.org/publications/ln/msri/2000/gametheory/slany/1/index.html
  For related material see http://www.dbai.tuwien.ac.at/proj/ramsey/</comments><report-no>DBAI-TR-2001-42</report-no><acm-class>F.1.3; G.2.1; G.2.2</acm-class><abstract>  In the graph avoidance game two players alternatingly color edges of a graph
G in red and in blue respectively. The player who first creates a monochromatic
subgraph isomorphic to a forbidden graph F loses. A symmetric strategy of the
second player ensures that, independently of the first player's strategy, the
blue and the red subgraph are isomorphic after every round of the game. We
address the class of those graphs G that admit a symmetric strategy for all F
and discuss relevant graph-theoretic and complexity issues. We also show
examples when, though a symmetric strategy on G generally does not exist, it is
still available for a particular F.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110050</id><created>2001-10-24</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames></author></authors><title>What is the minimal set of fragments that achieves maximal parse
  accuracy?</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings ACL'2001, Toulouse, France</journal-ref><abstract>  We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank (a precision of 90.8% and a recall of 90.6%). We
isolate some dependency relations which previous models neglect but which
contribute to higher parse accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110051</id><created>2001-10-24</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames></author></authors><title>Combining semantic and syntactic structure for language modeling</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings ICSLP'2000, Beijing, China</journal-ref><abstract>  Structured language models for speech recognition have been shown to remedy
the weaknesses of n-gram models. All current structured language models are,
however, limited in that they do not take into account dependencies between
non-headwords. We show that non-headword dependencies contribute to
significantly improved word error rate, and that a data-oriented parsing model
trained on semantically and syntactically annotated data can exploit these
dependencies. This paper also contains the first DOP model trained by means of
a maximum likelihood reestimation procedure, which solves some of the
theoretical shortcomings of previous DOP models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110052</id><created>2001-10-25</created><authors><author><keyname>Sarda</keyname><forenames>N. L.</forenames></author><author><keyname>Jain</keyname><forenames>Ankur</forenames></author></authors><title>Mragyati : A System for Keyword-based Searching in Databases</title><categories>cs.DB</categories><acm-class>H.2.4; H.3.3; H.3.5</acm-class><abstract>  The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110053</id><created>2001-10-26</created><authors><author><keyname>Sebastiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Machine Learning in Automated Text Categorization</title><categories>cs.IR cs.LG</categories><comments>Accepted for publication on ACM Computing Surveys</comments><acm-class>H.3.1;H.3.3;I.2.3</acm-class><abstract>  The automated categorization (or classification) of texts into predefined
categories has witnessed a booming interest in the last ten years, due to the
increased availability of documents in digital form and the ensuing need to
organize them. In the research community the dominant approach to this problem
is based on machine learning techniques: a general inductive process
automatically builds a classifier by learning, from a set of preclassified
documents, the characteristics of the categories. The advantages of this
approach over the knowledge engineering approach (consisting in the manual
definition of a classifier by domain experts) are a very good effectiveness,
considerable savings in terms of expert manpower, and straightforward
portability to different domains. This survey discusses the main approaches to
text categorization that fall within the machine learning paradigm. We will
discuss in detail issues pertaining to three different problems, namely
document representation, classifier construction, and classifier evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110054</id><created>2001-10-27</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Hart</keyname><forenames>George W.</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Vertex-Unfoldings of Simplicial Manifolds</title><categories>cs.CG cs.DM</categories><comments>12 pages, 7 figures, 10 references. Significant improvement of arXive
  cs.CG/0107023</comments><report-no>Smith Technical Report 072</report-no><acm-class>F.2.2; G.2.2</acm-class><abstract>  We present an algorithm to unfold any triangulated 2-manifold (in particular,
any simplicial polyhedron) into a non-overlapping, connected planar layout in
linear time. The manifold is cut only along its edges. The resulting layout is
connected, but it may have a disconnected interior; the triangles are connected
at vertices, but not necessarily joined along edges. We extend our algorithm to
establish a similar result for simplicial manifolds of arbitrary dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110055</id><created>2001-10-28</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Analytical solution of transient scalar wave and diffusion problems of
  arbitrary dimensionality and geometry by RBF wavelet series</title><categories>cs.NA cs.CE</categories><comments>Welcome any comments to W. Chen on wenc@ifi.uio.no</comments><acm-class>G1.3; G1.8</acm-class><abstract>  This study applies the RBF wavelet series to the evaluation of analytical
solutions of linear time-dependent wave and diffusion problems of any
dimensionality and geometry. To the best of the author's knowledge, such
analytical solutions have never been achieved before. The RBF wavelets can be
understood an alternative for multidimensional problems to the standard Fourier
series via fundamental and general solutions of partial differential equation.
The present RBF wavelets are infinitely differential, compactly supported,
orthogonal over different scales and very simple. The rigorous mathematical
proof of completeness and convergence is still missing in this study. The
present work may open a new window to numerical solution and theoretical
analysis of many other high-dimensional time-dependent PDE problems under
arbitrary geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110056</id><created>2001-10-29</created><updated>2003-04-07</updated><authors><author><keyname>Ben-Hur</keyname><forenames>Asa</forenames></author><author><keyname>Feinberg</keyname><forenames>Joshua</forenames></author><author><keyname>Fishman</keyname><forenames>Shmuel</forenames></author><author><keyname>Siegelmann</keyname><forenames>Hava T.</forenames></author></authors><title>Probabilistic analysis of a differential equation for linear programming</title><categories>cs.CC cond-mat.stat-mech math-ph math.MP math.OC</categories><comments>1+37 pages, latex, 5 eps figures. Version accepted for publication in
  the Journal of Complexity. Changes made: Presentation reorganized for
  clarity, expanded discussion of measure of complexity in the non-asymptotic
  regime (added a new section)</comments><acm-class>F.1.3, F.2</acm-class><abstract>  In this paper we address the complexity of solving linear programming
problems with a set of differential equations that converge to a fixed point
that represents the optimal solution. Assuming a probabilistic model, where the
inputs are i.i.d. Gaussian variables, we compute the distribution of the
convergence rate to the attracting fixed point. Using the framework of Random
Matrix Theory, we derive a simple expression for this distribution in the
asymptotic limit of large problem size. In this limit, we find that the
distribution of the convergence rate is a scaling function, namely it is a
function of one variable that is a combination of three parameters: the number
of variables, the number of constraints and the convergence rate, rather than a
function of these parameters separately. We also estimate numerically the
distribution of computation times, namely the time required to reach a vicinity
of the attracting fixed point, and find that it is also a scaling function.
Using the problem size dependence of the distribution functions, we derive high
probability bounds on the convergence rates and on the computation times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110057</id><created>2001-10-29</created><authors><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Kokkinaki</keyname><forenames>Vassiliki</forenames></author><author><keyname>Dimitromanolaki</keyname><forenames>Aggeliki</forenames></author><author><keyname>Calder</keyname><forenames>Jo</forenames></author><author><keyname>Oberlander</keyname><forenames>Jon</forenames></author><author><keyname>Not</keyname><forenames>Elena</forenames></author></authors><title>Generating Multilingual Personalized Descriptions of Museum Exhibits -
  The M-PIRO Project</title><categories>cs.CL cs.AI</categories><comments>15 pages. Presented at the 29th Conference on Computer Applications
  and Quantitative Methods in Archaeology, Gotland, Sweden, 2001. A version of
  the paper with higher quality images can be downloaded from:
  http://www.iit.demokritos.gr/~ionandr/caa_paper.pdf</comments><acm-class>I.2.7; H.5.2; H.5.4; I.7.4</acm-class><abstract>  This paper provides an overall presentation of the M-PIRO project. M-PIRO is
developing technology that will allow museums to generate automatically textual
or spoken descriptions of exhibits for collections available over the Web or in
virtual reality environments. The descriptions are generated in several
languages from information in a language-independent database and small
fragments of text, and they can be tailored according to the backgrounds of the
users, their ages, and their previous interaction with the system. An authoring
tool allows museum curators to update the system's database and to control the
language and content of the resulting descriptions. Although the project is
still in progress, a Web-based demonstrator that supports English, Greek and
Italian is already available, and it is used throughout the paper to highlight
the capabilities of the emerging technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110058</id><created>2001-10-29</created><authors><author><keyname>Pan</keyname><forenames>Yi</forenames></author></authors><title>Teaching Parallel Programming Using Both High-Level and Low-Level
  Languages</title><categories>cs.DC</categories><comments>10 pages</comments><acm-class>D.1.3; D.3.2</acm-class><abstract>  We discuss the use of both MPI and OpenMP in the teaching of senior
undergraduate and junior graduate classes in parallel programming. We briefly
introduce the OpenMP standard and discuss why we have chosen to use it in
parallel programming classes. Advantages of using OpenMP over message passing
methods are discussed. We also include a brief enumeration of some of the
drawbacks of using OpenMP and how these drawbacks are being addressed by
supplementing OpenMP with additional MPI codes and projects. Several projects
given in my class are also described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110059</id><created>2001-10-29</created><updated>2002-05-07</updated><authors><author><keyname>Donoso</keyname><forenames>Melody</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Nonorthogonal Polyhedra Built from Rectangles</title><categories>cs.CG cs.DM</categories><comments>19 pages, 20 figures. Revised version makes two corrections: The
  statement of the old Lemma 14 was incorrect. It has been corrected and merged
  with Lemma 13 now. Second, Figure 19 (a skew quadrilateral) was incorrect,
  and is now removed. It played no substantive role in the proofs</comments><report-no>Smith Technical Report 073, Oct. 2001; revised May 2002</report-no><acm-class>F.2.2;G.2.2</acm-class><abstract>  We prove that any polyhedron of genus zero or genus one built out of
rectangular faces must be an orthogonal polyhedron, but that there are
nonorthogonal polyhedra of genus seven all of whose faces are rectangles. This
leads to a resolution of a question posed by Biedl, Lubiw, and Sun [BLS99].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110060</id><created>2001-10-31</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Selected Topics in Asynchronous Automata</title><categories>cs.LO</categories><acm-class>94C05; 94C10</acm-class><journal-ref>Serban E. Vlad, Selected Topics in Asynchronous Automata, Analele
  universitatii din Oradea, Fascicola matematica, Tom VII, 1999</journal-ref><abstract>  The paper is concerned with defining the electrical signals and their models.
The delays are discussed, the asynchronous automata - which are the models of
the asynchronous circuits - and the examples of the clock generator and of the
R-S latch are given. We write the equations of the asynchronous automata, which
combine the pure delay model and the inertial delay model; the simple gate
model and the complex gate model; the fixed, bounded and unbounded delay model.
We give the solutions of these equations, which are written on R-&gt;{0,1}
functions, where R is the time set. The connection between the real time and
the discrete time is discussed. The stability, the fundamental mode of
operation, the combinational automata, the semi-modularity are defined and
characterized. Some connections are suggested with the linear time and the
branching time temporal logic of the propositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110061</id><created>2001-10-31</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>An Asynchronous Automata Approach to the Semantics of Temporal Logic</title><categories>cs.LO</categories><acm-class>94C05; 94C10</acm-class><journal-ref>Serban E. Vlad, An Asynchronous Automata Approach to the Semantics
  of Temporal Logic, the 8-th Symposium of Mathematics and its Applications of
  the 'Politehnica' University, Timisoara, 1999</journal-ref><abstract>  The paper presents the differential equations that characterize an
asynchronous automaton and gives their solution x:R-&gt;{0,1}x...x{0,1}. Remarks
are made on the connection between the continuous time and the discrete time of
the approach. The continuous and the discrete time, the linear and the
branching temporal logics have the semantics depending on x and their formulas
give the properties of the automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110062</id><created>2001-10-31</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>The Delay-Insensitivity, the Hazard-Freedom, the Semi-Modularity and the
  Technical Condition of Good Running of the Discrete Time Asynchronous
  Automata</title><categories>cs.LO</categories><acm-class>94C05; 94C10</acm-class><journal-ref>Serban E. Vlad, The Delay-Insensitivity, the Hazard-Freedom, the
  Semi-Modularity and the Technical Condition of Good Running of the Discrete
  Time Asynchronous Automata, Analele universitatii din Oradea, Fascicola
  matematica, Tom VIII, 2001</journal-ref><abstract>  The paper studies some important properties of the asynchronous (=timed)
automata: the delay-insensitivity, the hazard-freedom, the semi-modularity and
the technical condition of good running. Time is discrete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110063</id><created>2001-10-31</created><updated>2005-04-29</updated><authors><author><keyname>Dang</keyname><forenames>Zhe</forenames></author><author><keyname>Ibarra</keyname><forenames>Oscar</forenames></author></authors><title>The Existence of $\omega$-Chains for Transitive Mixed Linear Relations
  and Its Applications</title><categories>cs.LO</categories><comments>26 pages</comments><acm-class>D.2.4; F.1.1</acm-class><abstract>  We show that it is decidable whether a transitive mixed linear relation has
an $\omega$-chain. Using this result, we study a number of liveness
verification problems for generalized timed automata within a unified
framework. More precisely, we prove that (1) the mixed linear liveness problem
for a timed automaton with dense clocks, reversal-bounded counters, and a free
counter is decidable, and (2) the Presburger liveness problem for a timed
automaton with discrete clocks, reversal-bounded counters, and a pushdown stack
is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110064</id><created>2001-10-31</created><authors><author><keyname>Vlad</keyname><forenames>Serban E.</forenames></author></authors><title>Applications of the Differential Calculus in the Study of the Timed
  Automata: the Inertial Delay Buffer</title><categories>cs.LO</categories><acm-class>94C05; 94C10</acm-class><abstract>  We write the relations that characterize the simpliest timed automaton, the
inertial delay buffer, in two versions: the non-deterministic and the
deterministic one, by making use of the derivatives of the R-&gt;{0,1} functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110065</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110065</id><created>2001-10-31</created><authors><author><keyname>Kasemir</keyname><forenames>K. U.</forenames></author><author><keyname>Dalesio</keyname><forenames>L. R.</forenames></author></authors><title>Interfacing the ControlLogix PLC over Ethernet/IP</title><categories>cs.NI cs.AR</categories><comments>3 pages, 1 figure, 8th International Conference on Accelerator and
  Large Experimental Physics Control Systems (PSN THAP020), San Jose, CA, USA,
  November 27-30</comments><report-no>LA-UR -01-5891</report-no><acm-class>B.4.2</acm-class><journal-ref>eConf C011127:THDT002,2001</journal-ref><abstract>  The Allen-Bradley ControlLogix line of pro-grammable logic controllers (PLCs)
offers several in-terfaces: Ethernet, ControlNet, DeviceNet, RS-232 and others.
The ControlLogix Ethernet interface module 1756-ENET uses EtherNet/IP, the
ControlNet protocol, encapsulated in Ethernet packages, with specific service
codes. A driver for the Experimental Physics and Industrial Control System
(EPICS) has been developed that utilizes this EtherNet/IP protocol for
controllers running the vxWorks RTOS as well as a Win32 and Unix/Linux test
program. Features, per-formance and limitations of this interface are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110066</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110066</id><created>2001-10-31</created><authors><author><keyname>Kasemir</keyname><forenames>K. U.</forenames></author><author><keyname>Dalesio</keyname><forenames>L. R.</forenames></author></authors><title>Overview of the Experimental Physics and Industrial Control System
  (EPICS) Channel Archiver</title><categories>cs.OH</categories><comments>3 pages, 1 figure, 8th International Conference on Accelerator and
  Large Experimental Physics Control Systems (PSN THAP019), San Jose, CA, USA,
  November 27-30</comments><report-no>LA-UR-01-5892</report-no><acm-class>H.3.4</acm-class><journal-ref>eConf C011127:THAP019,2001</journal-ref><abstract>  The Channel Archiver has been operational for more than two years at Los
Alamos National Laboratory and other sites. This paper introduces the available
components (data sampling engine, viewers, scripting interface, HTTP/CGI
integration and data management), presents updated performance measurements and
reviews operational experience with the Channel Archiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0110067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0110067</id><created>2001-10-31</created><authors><author><keyname>Kilin</keyname><forenames>Fedor S.</forenames></author></authors><title>Analysis of Investment Policy in Belarus</title><categories>cs.CE</categories><comments>20 pages, 2 figures, 1 table. email: fedia@dragon.bas-net.by</comments><acm-class>J.1;J.4</acm-class><abstract>  The optimal planning trajectory is analyzed on the basis of the growth model
with effectiveness. The saving per capital value has to be rather high
initially with smooth decrement in the future years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111001</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111001</id><created>2001-10-31</created><authors><author><keyname>Kasemir</keyname><forenames>K. U.</forenames></author><author><keyname>Pieck</keyname><forenames>M.</forenames></author><author><keyname>Dalesio</keyname><forenames>L. R.</forenames></author></authors><title>Integrating LabVIEW into a Distributed Computing Environment</title><categories>cs.OH</categories><comments>3 pages, 2 figures, 8th International Conference on Accelerator and
  Large Experimental Physics Control Systems (PSN THAP032), San Jose, CA, USA,
  November 27-30</comments><report-no>LA-UR-01-5905</report-no><acm-class>J.7</acm-class><journal-ref>eConf C011127:THAP032,2001</journal-ref><abstract>  Being easy to learn and well suited for a self-contained desktop laboratory
setup, many casual programmers prefer to use the National Instruments LabVIEW
environment to develop their logic. An ActiveX interface is presented that
allows integration into a plant-wide distributed environment based on the
Experimental Physics and Industrial Control System (EPICS). This paper
discusses the design decisions and provides performance information, especially
considering requirements for the Spallation Neutron Source (SNS) diagnostics
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111002</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111002</id><created>2001-10-31</created><authors><author><keyname>Kehagias</keyname><forenames>Ath.</forenames></author><author><keyname>Konstantinidou</keyname><forenames>M.</forenames></author></authors><title>L-Fuzzy Valued Inclusion Measure, L-Fuzzy Similarity and L-Fuzzy
  Distance</title><categories>cs.OH</categories><comments>19 pages</comments><acm-class>I.5.1</acm-class><abstract>  The starting point of this paper is the introduction of a new measure of
inclusion of fuzzy set A in fuzzy set B. Previously used inclusion measures
take values in the interval [0,1]; the inclusion measure proposed here takes
values in a Boolean lattice. In other words, inclusion is viewed as an L-fuzzy
valued relation between fuzzy sets. This relation is re exive, antisymmetric
and transitive, i.e. it is a fuzzy order relation; in addition it possesess a
number of properties which various authors have postulated as axiomatically
appropriate for an inclusion measure. We also define an L-fuzzy valued measure
of similarity between fuzzy sets and and an L-fuzzy valued distance function
between fuzzy sets; these possess properties analogous to the ones of
real-valued similarity and distance functions.
  Keywords: Fuzzy Relations, inclusion measure, subsethood, L-fuzzy sets,
similarity, distance, transitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111003</id><created>2001-10-31</created><authors><author><keyname>Punyakanok</keyname><forenames>Vasin</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>The Use of Classifiers in Sequential Inference</title><categories>cs.LG cs.CL</categories><comments>7 pages, 1 figure</comments><acm-class>I.2.6, I.2.7</acm-class><journal-ref>Advances in Neural Information Processing Systems 13</journal-ref><abstract>  We study the problem of combining the outcomes of several different
classifiers in a way that provides a coherent inference that satisfies some
constraints. In particular, we develop two general approaches for an important
subproblem-identifying phrase structure. The first is a Markovian approach that
extends standard HMMs to allow the use of a rich observation structure and of
general classifiers to model state-observation dependencies. The second is an
extension of constraint satisfaction formalisms. We develop efficient
combination algorithms under both models and study them experimentally in the
context of shallow parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111004</id><created>2001-11-01</created><authors><author><keyname>Quock</keyname><forenames>D. E. R.</forenames><affiliation>ANL</affiliation></author><author><keyname>Munson</keyname><forenames>F. H.</forenames><affiliation>ANL</affiliation></author><author><keyname>Eder</keyname><forenames>K. J.</forenames><affiliation>ANL</affiliation></author><author><keyname>Dean</keyname><forenames>S. L.</forenames><affiliation>ANL</affiliation></author></authors><title>The Relational Database Aspects of Argonne's ATLAS Control System</title><categories>cs.DB</categories><comments>ICALEPCS 2001 Conference, PSN WEAP066, 3 pages, 3 figures</comments><report-no>WEAP066</report-no><acm-class>C.3</acm-class><journal-ref>eConf C011127 (2001) WEAP066</journal-ref><abstract>  The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111005</id><created>2001-11-02</created><updated>2002-01-07</updated><authors><author><keyname>Hawkins</keyname><forenames>Jon</forenames></author><author><keyname>Nguyen</keyname><forenames>Haung V.</forenames></author><author><keyname>Howard</keyname><forenames>Reginald B.</forenames></author></authors><title>Automated Real-Time Testing (ARTT) for Embedded Control Systems (ECS)</title><categories>cs.OH cs.SE</categories><comments>6 pages, 8 figures, ICALEPCS 2001, Poster Session</comments><report-no>PSN #78</report-no><acm-class>C.3</acm-class><journal-ref>eConf C011127 (2001) TUAP037</journal-ref><abstract>  Developing real-time automated test systems for embedded control systems has
been a real problem. Some engineers and scientists have used customized
software and hardware as a solution, which can be very expensive and time
consuming to develop. We have discovered how to integrate a suite of
commercially available off-the-shelf software tools and hardware to develop a
scalable test platform that is capable of performing complete black-box testing
for a dual-channel real-time Embedded-PLC-based control system
(www.aps.anl.gov). We will discuss how the Vali/Test Pro testing methodology
was implemented to structure testing for a personnel safety system with large
quantities of requirements and test cases.
  This work was supported by the U.S. Department of Energy, Basic Energy
Sciences, under Contract No. W-31-109-Eng-38.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111006</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111006</id><created>2001-11-05</created><authors><author><keyname>Soliday</keyname><forenames>Robert</forenames><affiliation>APS/ANL</affiliation></author></authors><title>Proliferation of SDDS Support for Various Platforms and Languages</title><categories>cs.DB</categories><comments>3 pages, 2 figures, submitted to ICALEPCS 2001</comments><report-no>THAP031</report-no><acm-class>H.2.8</acm-class><journal-ref>eConfC011127:THAP031,2001</journal-ref><abstract>  Since Self-Describing Data Sets (SDDS) were first introduced, the source code
has been ported to many different operating systems and various languages. SDDS
is now available in C, Tcl, Java, Fortran, and Python. All of these versions
are supported on Solaris, Linux, and Windows. The C version of SDDS is also
supported on VxWorks. With the recent addition of the Java port, SDDS can now
be deployed on virtually any operating system. Due to this proliferation, SDDS
files serve to link not only a collection of C programs, but programs and
scripts in many languages on different operating systems. The platform
independent binary feature of SDDS also facilitates portability among operating
systems. This paper presents an overview of various benefits of SDDS platform
interoperability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111007</id><created>2001-11-05</created><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Rosson</keyname><forenames>Mary Beth</forenames></author><author><keyname>Carroll</keyname><forenames>John M.</forenames></author></authors><title>Explaining Scenarios for Information Personalization</title><categories>cs.HC cs.IR</categories><acm-class>H.3.5; H.4.2; H.5.4; I.2.6; K.8</acm-class><abstract>  Personalization customizes information access. The PIPE (&quot;Personalization is
Partial Evaluation&quot;) modeling methodology represents interaction with an
information space as a program. The program is then specialized to a user's
known interests or information seeking activity by the technique of partial
evaluation. In this paper, we elaborate PIPE by considering requirements
analysis in the personalization lifecycle. We investigate the use of scenarios
as a means of identifying and analyzing personalization requirements. As our
first result, we show how designing a PIPE representation can be cast as a
search within a space of PIPE models, organized along a partial order. This
allows us to view the design of a personalization system, itself, as
specialized interpretation of an information space. We then exploit the
underlying equivalence of explanation-based generalization (EBG) and partial
evaluation to realize high-level goals and needs identified in scenarios; in
particular, we specialize (personalize) an information space based on the
explanation of a user scenario in that information space, just as EBG
specializes a theory based on the explanation of an example in that theory. In
this approach, personalization becomes the transformation of information spaces
to support the explanation of usage scenarios. An example application is
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111008</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111008</id><created>2001-11-06</created><authors><author><keyname>Yu</keyname><forenames>X. J.</forenames></author><author><keyname>Wang</keyname><forenames>Q. P.</forenames></author><author><keyname>Xu</keyname><forenames>P. S.</forenames></author></authors><title>The Control of a Beamline Over Intranet</title><categories>cs.NI</categories><comments>Poster at 8th International Conference on Accelerator and Large
  experimental Physics Control Systems, San Jose, California, USA, November
  2001, 3 pages, pdf, 1 jpeg figure</comments><acm-class>C.2.4;D.1.3</acm-class><journal-ref>eConfC011127:THAP021,2001</journal-ref><abstract>  The machines and beamlines controlled by VME industrial networks are very
popular in accelerator faculties. Recently new software technology, among of
which are Internet/Intranet application, Java language, and distributed
calculating environment, changes the control manner rapidly. A program based on
DCOM is composed to control of a variable included angle spherical grating
monochromator beamline at National Synchrotron Radiation Laboratory (NSRL) in
China. The control computer with a residential DCOM program is connected to
Intranet by LAN, over which the user-end-operating program located in another
computer sends driving beamline units' commands to the control computer. And
also a web page coded in Java, published by the WWW service running in the
control computer, is simply illustrated how to use web browser to query the
states of or to control the beamline units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111009</id><created>2001-11-06</created><authors><author><keyname>Angiulli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>On the complexity of inducing categorical and quantitative association
  rules</title><categories>cs.CC</categories><report-no>ISI-CNR TR 10-2001</report-no><acm-class>F.2.0</acm-class><abstract>  Inducing association rules is one of the central tasks in data mining
applications. Quantitative association rules induced from databases describe
rich and hidden relationships holding within data that can prove useful for
various application purposes (e.g., market basket analysis, customer profiling,
and others). Even though such association rules are quite widely used in
practice, a thorough analysis of the computational complexity of inducing them
is missing. This paper intends to provide a contribution in this setting. To
this end, we first formally define quantitative association rule mining
problems, which entail boolean association rules as a special case, and then
analyze their computational complexities, by considering both the standard
cases, and a some special interesting case, that is, association rule induction
over databases with null values, fixed-size attribute set databases, sparse
databases, fixed threshold problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111010</id><created>2001-11-06</created><authors><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Perri</keyname><forenames>Simona</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>Abduction with Penalization in Logic Programming</title><categories>cs.LO</categories><report-no>Unical Math-Dept TR10-2001</report-no><acm-class>D.1.6</acm-class><abstract>  Abduction, first proposed in the setting of classical logics, has been
studied with growing interest in the logic programming area during the last
years.
  In this paper we study {\em abduction with penalization} in logic
programming. This form of abductive reasoning, which has not been previously
analyzed in logic programming, turns out to represent several relevant
problems, including optimization problems, very naturally. We define a formal
model for abduction with penalization from logic programs, which extends the
abductive framework proposed by Kakas and Mancarella. We show the high
expressiveness of this formalism, by encoding a couple of relevant problems,
including the well-know Traveling Salesman Problem from optimization theory, in
this abductive framework. The resulting encodings are very simple and elegant.
We analyze the complexity of the main decisional problems arising in this
framework. An interesting result in this course is that ``negation comes for
free.'' Indeed, the addition of (even unstratified) negation does not cause any
further increase to the complexity of the abductive reasoning tasks (which
remains the same as for not-free programs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111011</id><created>2001-11-06</created><authors><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author></authors><title>Sintesi di algoritmi con SKY</title><categories>cs.LO</categories><comments>In italian</comments><report-no>Unical Math. Dept. TR 11-2001</report-no><acm-class>D.1.6</acm-class><abstract>  This paper describes the semantics and ideas about SKY, a logic programming
language intended in order to specify algorithmic strategies for the evaluation
of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111012</id><created>2001-11-06</created><authors><author><keyname>Ianni</keyname><forenames>Giovambattista</forenames></author></authors><title>Intelligent Anticipated Exploration of Web Sites</title><categories>cs.AI cs.IR</categories><comments>Accepted for publications on AI Communications</comments><report-no>Unical Math. Dept. TR 08-2001</report-no><acm-class>I.2.11;H.3.3</acm-class><abstract>  In this paper we describe a web search agent, called Global Search Agent
(hereafter GSA for short). GSA integrates and enhances several search
techniques in order to achieve significant improvements in the user-perceived
quality of delivered information as compared to usual web search engines. GSA
features intelligent merging of relevant documents from different search
engines, anticipated selective exploration and evaluation of links from the
current result set, automated derivation of refined queries based on user
relevance feedback. System architecture as well as experimental accounts are
also illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111013</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111013</id><created>2001-11-06</created><authors><author><keyname>Woodruff</keyname><forenames>John P.</forenames></author><author><keyname>Casavant</keyname><forenames>Drew D.</forenames></author><author><keyname>Cline</keyname><forenames>Barry D.</forenames></author><author><keyname>Gorvad</keyname><forenames>Michael R.</forenames></author></authors><title>Quality Control, Testing and Deployment Results in NIF ICCS</title><categories>cs.SE</categories><comments>Submitted to ICALEPCS 2001</comments><acm-class>D.2.9</acm-class><journal-ref>eConfC011127:TUDT001,2001</journal-ref><abstract>  The strategy used to develop the NIF Integrated Computer Control System
(ICCS) calls for incremental cycles of construction and formal test to deliver
a total of 1 million lines of code. Each incremental release takes four to six
months to implement specific functionality and culminates when offline tests
conducted in the ICCS Integration and Test Facility verify functional,
performance, and interface requirements. Tests are then repeated on line to
confirm integrated operation in dedicated laser laboratories or ultimately in
the NIF. Test incidents along with other change requests are recorded and
tracked to closure by the software change control board (SCCB). Annual
independent audits advise management on software process improvements.
Extensive experience has been gained by integrating controls in the prototype
laser preamplifier laboratory. The control system installed in the preamplifier
lab contains five of the ten planned supervisory subsystems and seven of
sixteen planned front-end processors (FEPs). Beam alignment, timing, diagnosis
and laser pulse amplification up to 20 joules was tested through an automated
series of shots. Other laboratories have provided integrated testing of six
additional FEPs. Process measurements including earned-value, product size, and
defect densities provide software project controls and generate confidence that
the control system will be successfully deployed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111014</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111014</id><created>2001-11-07</created><authors><author><keyname>Sekoranja</keyname><forenames>M.</forenames></author><author><keyname>Hunt</keyname><forenames>S.</forenames></author><author><keyname>Luedeke</keyname><forenames>A.</forenames></author></authors><title>Visual DCT - Visual EPICS Database Configuration Tool</title><categories>cs.SE</categories><comments>ICALEPCS 2001, THAP029</comments><acm-class>K.8.1</acm-class><journal-ref>eConfC011127:THAP029,2001</journal-ref><abstract>  Visual DCT is an EPICS configuration tool completely written in Java and
therefore supported in various systems. It was developed to provide features
missing in existing configuration tools as Capfast and GDCT. Visually Visual
DCT resembles GDCT - records can be created, moved and linked, fields and links
can be easily modified. But Visual DCT offers more: using groups, records can
be grouped together in a logical block, which allows a hierarchical design.
Additionally indication of data flow direction using arrows makes the design
easier to understand. Visual DCT has a powerful DB parser, which allows
importing existing DB and DBD files. Output file is also DB file, all comments
and record order is preserved and visual data saved as comment, which allows
DBs to be edited in other tools or manually. Great effort has been taken and
many tricks used to optimize the performance in order to compensate for the
fact that Java is an interpreted language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111015</id><created>2001-11-07</created><authors><author><keyname>Szalay</keyname><forenames>Alexander</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Thakar</keyname><forenames>Ani</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter Z.</forenames></author><author><keyname>Malik</keyname><forenames>Tanu</forenames></author><author><keyname>Raddick</keyname><forenames>Jordan</forenames></author><author><keyname>Stoughton</keyname><forenames>Christopher</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data</title><categories>cs.DL cs.DB</categories><comments>submitted for publication, original at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2001-104</comments><report-no>Microsoft Research TR 2001 104</report-no><acm-class>H.3.5, H.4, J.2, H.2.8</acm-class><abstract>  The SkyServer provides Internet access to the public Sloan Digital Sky Survey
(SDSS) data for both astronomers and for science education. This paper
describes the SkyServer goals and architecture. It also describes our
experience operating the SkyServer on the Internet. The SDSS data is public and
well-documented so it makes a good test platform for research on database
algorithms and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111016</id><created>2001-11-07</created><authors><author><keyname>Fong</keyname><forenames>Kirby W.</forenames></author><author><keyname>Estes</keyname><forenames>Christopher M.</forenames></author><author><keyname>Fisher</keyname><forenames>John M.</forenames></author><author><keyname>Shelton</keyname><forenames>Randy T.</forenames></author></authors><title>Application Software Structure Enables Nif Operations Kirby W. Fong</title><categories>cs.SE</categories><comments>3 pages, 1 figure, ICALEPCS 2001 Conference</comments><report-no>UCRL-JC-143317</report-no><acm-class>D.2.11</acm-class><journal-ref>eConf C011127 (2001) THcT003</journal-ref><abstract>  The NIF Integrated Computer Control System (ICCS) application software uses a
set of service frameworks that assures uniform behavior spanning the front-end
processors (FEPs) and supervisor programs. This uniformity is visible both in
the way each program employs shared services and in the flexibility it affords
for attaching graphical user interfaces (GUIs). Uniformity of structure across
applications is desired for the benefit of programmers who will be maintaining
the many programs that constitute the ICCS. In this paper, the framework
components that have the greatest impact on the application structure are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111017</id><created>2001-11-08</created><authors><author><keyname>Munson</keyname><forenames>F. H.</forenames><affiliation>ANL</affiliation></author><author><keyname>Quock</keyname><forenames>D. E. R.</forenames><affiliation>ANL</affiliation></author><author><keyname>Dean</keyname><forenames>S. L.</forenames><affiliation>ANL</affiliation></author><author><keyname>Eder</keyname><forenames>K. J.</forenames><affiliation>ANL</affiliation></author></authors><title>First Experiences Integrating PC Distributed I/O Into Argonne's ATLAS
  Control System</title><categories>cs.OH</categories><comments>ICALEPCS 2001 Conference, PSN WEAP027, 3 pages, 1 figure</comments><report-no>WEAP027</report-no><acm-class>H.3.4</acm-class><journal-ref>eConf C011127 (2001) TUcT002</journal-ref><abstract>  First Experiences Integrating PC Distributed I/O Into Argonne's ATLAS Control
System The roots of ATLAS (Argonne Tandem-Linac Accelerator System) date back
to the early 1960s. Located at the Argonne National Laboratory, the accelerator
has been designated a National User Facility, which focuses primarily on
heavy-ion nuclear physics. Like the accelerator it services, the control system
has been in a constant state of evolution. The present real-time portion of the
control system is based on the commercial product Vsystem [1]. While Vsystem
has always been capable of distributed I/O processing, the latest offering of
this product provides for the use of relatively inexpensive PC hardware and
software. This paper reviews the status of the ATLAS control system, and
describes first experiences with PC distributed I/O.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111018</id><created>2001-11-08</created><authors><author><keyname>Chu</keyname><forenames>Y.</forenames></author><author><keyname>Baek</keyname><forenames>S.</forenames></author><author><keyname>Yonekawa</keyname><forenames>H.</forenames></author><author><keyname>Chertovskikh</keyname><forenames>A.</forenames></author><author><keyname>Kim</keyname><forenames>M.</forenames></author><author><keyname>Kim</keyname><forenames>J. S.</forenames></author><author><keyname>Park</keyname><forenames>K.</forenames></author><author><keyname>Baang</keyname><forenames>S.</forenames></author><author><keyname>Chang</keyname><forenames>Y.</forenames></author><author><keyname>Kim</keyname><forenames>J. H.</forenames></author><author><keyname>Lee</keyname><forenames>S.</forenames></author><author><keyname>Lim</keyname><forenames>B.</forenames></author><author><keyname>Chung</keyname><forenames>W.</forenames></author><author><keyname>Park</keyname><forenames>H.</forenames></author><author><keyname>Kim</keyname><forenames>K.</forenames></author></authors><title>Data Acquisition and Database Management System for Samsung
  Superconductor Test Facility</title><categories>cs.DB cs.AI</categories><comments>3 pages, 3 figures, ICALEPCS 2001</comments><acm-class>B.1.1</acm-class><journal-ref>eConf C011127 (2001) TUAP018</journal-ref><abstract>  In order to fulfill the test requirement of KSTAR (Korea Superconducting
Tokamak Advanced Research) superconducting magnet system, a large scale
superconducting magnet and conductor test facility, SSTF (Samsung
Superconductor Test Facility), has been constructed at Samsung Advanced
Institute of Technology. The computer system for SSTF DAC (Data Acquisition and
Control) is based on UNIX system and VxWorks is used for the real-time OS of
the VME system. EPICS (Experimental Physics and Industrial Control System) is
used for the communication between IOC server and client. A database program
has been developed for the efficient management of measured data and a Linux
workstation with PENTIUM-4 CPU is used for the database server. In this paper,
the current status of SSTF DAC system, the database management system and
recent test results are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111019</id><created>2001-11-08</created><updated>2001-11-14</updated><authors><author><keyname>Luedeke</keyname><forenames>A.</forenames><affiliation>PSI</affiliation></author></authors><title>Application of digital regulated Power Supplies for Magnet Control at
  the Swiss Light Source</title><categories>cs.SE</categories><comments>3 pages, 4 graphics, ICALEPCS 2001 paper (Poster)</comments><report-no>PSN: TUAP049</report-no><acm-class>B.4.3;B.4.5;B.8;C.0;C.4</acm-class><journal-ref>eConf C011127 (2001) TUAP049</journal-ref><abstract>  The Swiss Light Source (SLS) has in the order of 500 magnet power supplies
(PS) installed, ranging from from 3 A/20 V four-quadrant PS to a 950 A/1000 V
two-quadrant 3 Hz PS. All magnet PS have a local digital controller for a
digital regulation loop and a 5 MHz optical point-to-point link to the VME
level. The PS controller is running a pulse width/pulse repetition regulation
scheme, optional with multiple slave regulation loops. Many internal regulation
parameters and controller diagnostics are readable by the control system.
Industry Pack modules with standard VME carrier cards are used as VME hardware
interface with the high control density of eight links per VME card. The low
level EPICS interface is identical for all 500 magnet PS, including insertion
devices. The digital PS have proven to be very stable and reliable during
commissioning of the light source. All specifications were met for all PS. The
advanced diagnostic for the magnet PS turned out to be very useful not only for
the diagnostic of the PS but also to identify problems on the magnets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111020</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111020</id><created>2001-11-08</created><authors><author><keyname>Boyer</keyname><forenames>C.</forenames></author><author><keyname>Sebag</keyname><forenames>J.</forenames></author><author><keyname>Ellerbroek</keyname><forenames>B.</forenames></author></authors><title>Gemini MCAO Control System</title><categories>cs.OH</categories><comments>3 pages, ICALPECS 2001, San Jose, California</comments><acm-class>C.1.2</acm-class><journal-ref>eConf C011127 (2001) TUAT004</journal-ref><doi>10.1117/12.454790</doi><abstract>  The Gemini Observatory is planning to implement a Multi Conjugate Adaptive
Optics (MCAO) System as a facility instrument for the Gemini-South telescope.
The system will include 5 Laser Guide Stars, 3 Natural Guide Stars, and 3
Deformable mirrors optically conjugated at different altitudes to achieve
near-uniform atmospheric compensation over a 1 arc minute square field of view.
The control of such a system will be split into 3 main functions: the control
of the opto-mechanical assemblies of the whole system (including the Laser, the
Beam Transfer Optics and the Adaptive Optics bench), the control of the
Adaptive Optics System itself at a rate of 800FPS and the control of the safety
system. The control of the Adaptive Optics System is the most critical in terms
of real time performances. The control system will be an EPICS based system. In
this paper, we will describe the requirements for the whole MCAO control
system, preliminary designs for the control of the opto-mechanical devices and
architecture options for the control of the Adaptive Optics system and the
safety system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111021</id><created>2001-11-09</created><updated>2001-12-17</updated><authors><author><keyname>Luedeke</keyname><forenames>A.</forenames><affiliation>PSI</affiliation></author></authors><title>System Integration of High Level Applications during the Commissioning
  of the Swiss Light Source</title><categories>cs.SE</categories><comments>3 pages, 5 figures, ICALEPS 2001 paper (talk)</comments><report-no>PSN: FRBT002</report-no><acm-class>J.2</acm-class><journal-ref>eConf C011127 (2001) FRBT002</journal-ref><abstract>  The commissioning of the Swiss Light Source (SLS) started in Feb. 2000 with
the Linac, continued in May 2000 with the booster synchrotron and by Dec. 2000
first light in the storage ring were produced. The first four beam lines had to
be operational by August 2001. The thorough integration of all subsystems to
the control system and a high level of automation was prerequisite to meet the
tight time schedule. A careful balanced distribution of functionality into high
level and low level applications allowed an optimization of short development
cycles and high reliability of the applications. High level applications were
implemented as CORBA based client/server applications (tcl/tk and Java based
clients, C++ based servers), IDL applications using EZCA, medm/dm2k screens and
tcl/tk applications using CDEV. Low level applications were mainly built as
EPICS process databases, SNL state machines and customized drivers.
Functionality of the high level application was encapsulated and pushed to
lower levels whenever it has proven to be adequate. That enabled to reduce
machine setups to a handful of physical parameters and allow the usage of
standard EPICS tools for display, archiving and processing of complex physical
values. High reliability and reproducibility were achieved with that approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111022</id><created>2001-11-08</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author><author><keyname>Karplus</keyname><forenames>Walter</forenames></author><author><keyname>Liu</keyname><forenames>Damon</forenames></author></authors><title>Distributed Computing for Localized and Multilayer Visualizations</title><categories>cs.DC cs.DS</categories><acm-class>F.1.2; I.1.2; H.3.4; I.4.10; J.3</acm-class><abstract>  The aim of this paper is to develop an approach to visualizations that
benefits from distributed computing. Three schemes of process distribution are
considered: parallel, pipeline, and expanding pipeline computations. Expanding
pipeline structure synthesizes the advantages and traits of both parallel and
pipeline computations. In expanding pipeline computing, a novel approach
presented in this paper, a multiplicity of processes are concurrently developed
in parallel and knotted processor pipelines. The theoretical foundations for
expanding pipeline computing as a computational process are in the domains of
alternating Turing machines, molecular computing, and E-machines. Expanding
pipeline computing constitutes the development of the conventional pipeline
architecture aimed at utilization of implicit parallel structures existing in
algorithms. Such structures appear in various kinds of visualization. Image
deriving and processing is a field that provides diverse opportunities for
utilization of the advantages of distributed computing. The most relevant to
the distributed architecture is stratified visualization with its two cases
based on data localization and layer separation. Visualization is treated here
as a special case of simulation. The conceptual approach to distributed
computing developed in this paper have been applied to visualization in a
computer support system, which is utilized in radiology and namely, for the
noninvasive treatment of brain aneurysms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111023</id><created>2001-11-08</created><updated>2001-12-04</updated><authors><author><keyname>Pokorny</keyname><forenames>M.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Brooks</keyname><forenames>M.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Glendenning</keyname><forenames>B.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Harris</keyname><forenames>G.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Heald</keyname><forenames>R.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Stauffer</keyname><forenames>F.</forenames><affiliation>NRAO</affiliation></author><author><keyname>Pisano</keyname><forenames>J.</forenames><affiliation>NRAO</affiliation></author></authors><title>Distributed Control System for the Test Interferometer of the ALMA
  Project</title><categories>cs.DC physics.ins-det</categories><comments>Submitted to ICALEPCS'01, San Jose, USA, November 2001, (THAT004) 3
  pages, LaTeX</comments><acm-class>J.2</acm-class><journal-ref>eConf C011127 (2001) THAT004</journal-ref><abstract>  The control system (TICS) for the test interferometer being built to support
the development of the Atacama Large Millimeter Array (ALMA)will itself be a
prototype for the final ALMA array, providing a test for the distributed
control system under development. TICS will be based on the ALMA Common
Software (ACS) (developed at the European Southern Observatory), which provides
CORBA-based services and a device management framework for the control
software.
  Simple device controllers will run on single board computers, one of which
(known as an LCU) is located at each antenna; whereas complex, compound device
controllers may run on centrally located computers. In either circumstance,
client programs may obtain direct CORBA references to the devices and their
properties. Monitor and control requests are sent to devices or properties,
which then process and forward the commands to the appropriate hardware devices
as required. Timing requirements are met by tagging commands with (future)
timestamps synchronized to a timing pulse, which is regulated by a central
reference generator, and is distributed to all hardware devices in the array.
Monitoring is provided through a publish/subscribe CORBA-based service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111024</id><created>2001-11-08</created><authors><author><keyname>Ali</keyname><forenames>Mir Farooq</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Shell</keyname><forenames>Eric</forenames></author><author><keyname>Abrams</keyname><forenames>Marc</forenames></author></authors><title>Building Multi-Platform User Interfaces with UIML</title><categories>cs.HC</categories><comments>12 pages</comments><acm-class>H.5.2;H.5.4;I.3.6;D.2.2</acm-class><abstract>  There has been a widespread emergence of computing devices in the past few
years that go beyond the capabilities of traditional desktop computers.
However, users want to use the same kinds of applications and access the same
data and information on these appliances that they can access on their desktop
computers. The user interfaces for these platforms go beyond the traditional
interaction metaphors. It is a challenge to build User Interfaces (UIs) for
these devices of differing capabilities that allow the end users to perform the
same kinds of tasks. The User Interface Markup Language (UIML) is an XML-based
language that allows the canonical description of UIs for different platforms.
We describe the language features of UIML that facilitate the development of
multi-platform UIs. We also describe the key aspects of our approach that makes
UIML succeed where previous approaches failed, namely the division in the
representation of a UI, the use of a generic vocabulary, and an integrated
development environment specifically designed for transformation-based UI
development. Finally we describe the initial details of a multi-step usability
engineering process for building multi-platform UI using UIML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111025</id><created>2001-11-08</created><authors><author><keyname>Ali</keyname><forenames>Mir Farooq</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author><author><keyname>Abrams</keyname><forenames>Marc</forenames></author></authors><title>A Multi-Step Process for Generating Multi-Platform User Interfaces using
  UIML</title><categories>cs.HC</categories><comments>11 pages</comments><acm-class>H.5.2;H.5.4;I.3.6;D.2.2</acm-class><abstract>  There has been a widespread emergence of computing devices in the past few
years that go beyond the capabilities of traditional desktop computers. These
devices have varying input/output characteristics, modalities and interaction
mechanisms. However, users want to use the same kinds of applications and
access the same data and information on these appliances that they can access
on their desktop computers. The user interfaces for these devices and platforms
go beyond the traditional interaction metaphors. It is a challenge to build
User Interfaces (UIs) for these devices of differing capabilities that allow
the end users to perform the same kinds of tasks. The User Interface Markup
Language (UIML) is an XML-based language that allows the canonical description
of UIs for different platforms. We present a multi-step transformation-based
framework for building Multi-Platform User Interfaces using UIML. We describe
the language features of UIML that facilitate the development of multi-platform
UIs, the multi-step process involved in our framework and the transformations
needed to build the UIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111026</id><created>2001-11-09</created><authors><author><keyname>Hill</keyname><forenames>J.</forenames></author><author><keyname>Lange</keyname><forenames>R.</forenames></author></authors><title>Next Generation EPICS Interface to Abstract Data</title><categories>cs.NI cs.DS</categories><comments>Poster at International Conference on Accelerator and Large
  Experimental Physics Control Systems, San Jose, CA, Nov 2001, (THAP014) 3
  pages, PDF</comments><acm-class>c.2.4</acm-class><journal-ref>eConf C011127 (2001) THAP014</journal-ref><abstract>  The set of externally visible properties associated with process variables in
the Experimental Physics and Industrial Control System (EPICS) is predefined in
the EPICS base distribution and is therefore not extensible by plug-compatible
applications. We believe that this approach, while practical for early versions
of the system with a smaller user base, is now severely limiting expansion of
the high-level application tool set for EPICS. To eliminate existing barriers,
we propose a new C++ based interface to abstract containerized data. This paper
describes the new interface, its application to message passing in distributed
systems, its application to direct communication between tightly coupled
programs co-resident in an address space, and its paramount position in an
emerging role for EPICS - the integration of dissimilar systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111027</id><created>2001-11-09</created><updated>2001-12-17</updated><authors><author><keyname>Ishii</keyname><forenames>M.</forenames></author><author><keyname>Fukui</keyname><forenames>T.</forenames></author><author><keyname>Furukawa</keyname><forenames>Y.</forenames></author><author><keyname>Nakatani</keyname><forenames>T.</forenames></author><author><keyname>Ohata</keyname><forenames>T.</forenames></author><author><keyname>Tanaka</keyname><forenames>R.</forenames></author></authors><title>Upgrade of Spring-8 Beamline Network with Vlan Technology Over Gigabit
  Ethernet</title><categories>cs.NI</categories><comments>3 pages, 2 figure, 8th International Conference on Accelerator and
  Large Experimental Physics Control Systems (PSN TUAP056), San Jose, CA, USA,
  November 27-30</comments><acm-class>C.2.1</acm-class><journal-ref>eConf C011127 (2001) TUAP056</journal-ref><abstract>  The beamline network system at SPring-8 consists of three LANs; a BL-LAN for
beamline component control, a BL-USER-LAN for beamline experimental users and
an OA-LAN for the information services. These LANs are interconnected by a
firewall system. Since the network traffic and the number of beamlines have
increased, we upgraded the backbone of BL-USER-LAN from Fast Ethernet to
Gigabit Ethernet. And then, to establish the independency of a beamline and to
raise flexibility of every beamline, we also introduced the IEEE802.1Q Virtual
LAN (VLAN) technology into the BL-USER-LAN. We discuss here a future plan to
build the firewall system with hardware load balancers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111028</id><created>2001-11-09</created><authors><author><keyname>Chaize</keyname><forenames>JM.</forenames></author><author><keyname>Goetz</keyname><forenames>A.</forenames></author><author><keyname>Klotz</keyname><forenames>WD.</forenames></author><author><keyname>Meyer</keyname><forenames>J.</forenames></author><author><keyname>Perez</keyname><forenames>M.</forenames></author><author><keyname>Taurel</keyname><forenames>E.</forenames></author><author><keyname>Verdier</keyname><forenames>P.</forenames></author></authors><title>The ESRF TANGO control system status</title><categories>cs.DC</categories><comments>3 pages</comments><acm-class>C.2.4;D.1.3</acm-class><journal-ref>eConf C011127 (2001) TUAP004</journal-ref><abstract>  TANGO is an object oriented control system toolkit based on CORBA presently
under development at the ESRF. IN this paper, the TANGO philosophy is briefly
presented. All the existing tools developed around TANGO will also be
presented. This include a code genrator, a WEB interface to TANGO objects, an
administration tool and an interface to LabView. Finally, an xample of a TANGO
device server for OPC device is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111029</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111029</id><created>2001-11-09</created><authors><author><keyname>Allison</keyname><forenames>T.</forenames></author><author><keyname>Flood</keyname><forenames>R.</forenames></author></authors><title>Versatile Data Acquisition and Controls for Epics Using Vme-Based Fpgas</title><categories>cs.AR</categories><comments>3 pages, ICALEPCS 2001, T. Allison and R. Foold, Jefferson Lab</comments><acm-class>B.6.0;B.1.0;B.2.0</acm-class><journal-ref>eConf C011127:TUAP053,2001</journal-ref><abstract>  Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111030</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111030</id><created>2001-11-09</created><authors><author><keyname>Dong</keyname><forenames>H.</forenames></author><author><keyname>Flood</keyname><forenames>R.</forenames></author><author><keyname>Hovater</keyname><forenames>C.</forenames></author><author><keyname>Musson</keyname><forenames>J.</forenames></author></authors><title>A Dual Digital Signal Processor VME Board For Instrumentation And
  Control Applications</title><categories>cs.AR</categories><comments>3 PDF pages</comments><acm-class>B.0</acm-class><journal-ref>eConf C011127:THAP049,2001</journal-ref><abstract>  A Dual Digital Signal Processing VME Board was developed for the Continuous
Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at
Jefferson Lab. It is a versatile general-purpose digital signal processing
board using an open architecture, which allows for adaptation to various
applications. The base design uses two independent Texas Instrument (TI)
TMS320C6711, which are 900 MFLOPS floating-point digital signal processors
(DSP). Applications that require a fixed point DSP can be implemented by
replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The
design can be manufactured with a reduced chip set without redesigning the
printed circuit board. For example it can be implemented as a single-channel
DSP with no analog I/O.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111031</id><created>2001-11-09</created><authors><author><keyname>Carey</keyname><forenames>Robert W.</forenames></author><author><keyname>Fong</keyname><forenames>Kirby W.</forenames></author><author><keyname>Sanchez</keyname><forenames>Randy J.</forenames></author><author><keyname>Tappero</keyname><forenames>Joseph D.</forenames></author><author><keyname>Woodruff</keyname><forenames>John P.</forenames></author></authors><title>Large-Scale Corba-Distributed Software Framework for Nif Controls</title><categories>cs.DC</categories><comments>5 pages, 0 figures, ICALEPCS '01</comments><report-no>THAI001</report-no><acm-class>C.2.4</acm-class><journal-ref>eConf C011127 (2001) THAI001</journal-ref><abstract>  The Integrated Computer Control System (ICCS) is based on a scalable software
framework that is distributed over some 325 computers throughout the NIF
facility. The framework provides templates and services at multiple levels of
abstraction for the construction of software applications that communicate via
CORBA (Common Object Request Broker Architecture). Various forms of
object-oriented software design patterns are implemented as templates to be
extended by application software. Developers extend the framework base classes
to model the numerous physical control points, thereby sharing the
functionality defined by the base classes. About 56,000 software objects each
individually addressed through CORBA are to be created in the complete ICCS.
Most objects have a persistent state that is initialized at system start-up and
stored in a database. Additional framework services are provided by centralized
server programs that implement events, alerts, reservations, message logging,
database/file persistence, name services, and process management. The ICCS
software framework approach allows for efficient construction of a software
system that supports a large number of distributed control points representing
a complex control application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111032</id><created>2001-11-09</created><authors><author><keyname>oerter</keyname><forenames>B.</forenames></author><author><keyname>Nelson</keyname><forenames>R.</forenames></author><author><keyname>Shea</keyname><forenames>T.</forenames></author><author><keyname>Sibley</keyname><forenames>C.</forenames></author></authors><title>SNS Timing System</title><categories>cs.AR</categories><acm-class>B.m</acm-class><journal-ref>eConf C011127 (2001) FRAT001</journal-ref><abstract>  This poster describes the timing system being designed for Spallation Neutron
Source being built at Oak Ridge National lab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111033</id><created>2001-11-09</created><authors><author><keyname>Gotz</keyname><forenames>A.</forenames></author><author><keyname>Homs</keyname><forenames>A.</forenames></author><author><keyname>Regad</keyname><forenames>B.</forenames></author><author><keyname>Perez</keyname><forenames>M.</forenames></author><author><keyname>Makijarvi</keyname><forenames>P.</forenames></author><author><keyname>Klotz</keyname><forenames>W. D.</forenames></author></authors><title>Modernising the ESRF control system with GNU/Linux</title><categories>cs.DC</categories><comments>3 pages</comments><report-no>WEAP023</report-no><acm-class>C.3</acm-class><journal-ref>eConf C011127 (2001) WEAP023</journal-ref><abstract>  he ESRF control system is in the process of being modernised. The present
contrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,
Motif and C. The new control system will be based on compact PCI, 100 MHz
Ethernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main
frontend operating system will be GNU/Linux running on Intel/x86 and
Motorola/68k. Linux will also be used on handheld devices for mobile control.
This poster describes how GNU/Linux is being used to modernise the control
system and what problems have been encountered so far
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111034</id><created>2001-11-09</created><authors><author><keyname>Milcinski</keyname><forenames>G.</forenames></author><author><keyname>Plesko</keyname><forenames>M.</forenames></author><author><keyname>Sekoranja</keyname><forenames>M.</forenames></author></authors><title>Experiences with advanced CORBA services</title><categories>cs.PF</categories><comments>Poster THAP005, ICALEPS 2001, 3 pages (ZIP and WORD file)</comments><acm-class>D.m</acm-class><journal-ref>eConf C011127 (2001) THAP005</journal-ref><abstract>  The Common Object Request Broker Architecture (CORBA) is successfully used in
many control systems (CS) for data transfer and device modeling. Communication
rates below 1 millisecond, high reliability, scalability, language independence
and other features make it very attractive. For common types of applications
like error logging, alarm messaging or slow monitoring, one can benefit from
standard CORBA services that are implemented by third parties and save
tremendous amount of developing time. We have started using few CORBA services
on our previous CORBA-based control system for the light source ANKA [1] and
use now several CORBA services for the ALMA Common Software (ACS) [2], the core
of the control system of the Atacama Large Millimeter Array. Our experiences
with the interface repository (IFR), the implementation repository, the naming
service, the property service, telecom log service and the notify service from
different vendors are presented. Performance and scalability benchmarks have
been performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111035</id><created>2001-11-09</created><authors><author><keyname>Straumann</keyname><forenames>Till</forenames></author></authors><title>Open Source Real Time Operating Systems Overview</title><categories>cs.OS</categories><comments>Talk at ICALEPCS 2001 Conference, Nov 2001, San Jose, USA, (WEBT001),
  3 pages, LaTex</comments><acm-class>D.4</acm-class><journal-ref>eConf C011127 (2001) WEBT001</journal-ref><abstract>  Modern control systems applications are often built on top of a real time
operating system (RTOS) which provides the necessary hardware abstraction as
well as scheduling, networking and other services. Several open source RTOS
solutions are publicly available, which is very attractive, both from an
economic (no licensing fees) as well as from a technical (control over the
source code) point of view. This contribution gives an overview of the RTLinux
and RTEMS systems (architecture, development environment, API etc.). Both
systems feature most popular CPUs, several APIs (including Posix), networking,
portability and optional commercial support. Some performance figures are
presented, focusing on interrupt latency and context switching delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111036</id><created>2001-11-12</created><authors><author><keyname>Lange</keyname><forenames>R.</forenames><affiliation>BESSY</affiliation></author><author><keyname>Hill</keyname><forenames>J.</forenames><affiliation>LANL</affiliation></author></authors><title>Data Access - Experiences Implementing an Object Oriented Library on
  Various Platforms</title><categories>cs.SE cs.DS</categories><comments>Poster paper submitted to ICALEPCS'01, San Jose, Nov. 2001, PSN#
  THAP015, 3 pages, PDF (source: M$ Office 2000)</comments><acm-class>D.1.5; D.2.13</acm-class><journal-ref>eConf C011127 (2001) THAP015</journal-ref><abstract>  Data Access will be the next generation data abstraction layer for EPICS. Its
implementation in C++ brought up a number of issues that are related to object
oriented technology's impact on CPU and memory usage.
  What is gained by the new abstract interface? What is the price that has to
be paid for these gains? What compromises seem applicable and affordable?
  This paper discusses tests that have been made about performance and memory
usage as well as the different measures that have been taken to optimize the
situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111037</id><created>2001-11-14</created><updated>2001-11-16</updated><authors><author><keyname>Jussien</keyname><forenames>Narendra</forenames></author><author><keyname>Ouis</keyname><forenames>Samir</forenames></author></authors><title>User-friendly explanations for constraint programming</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus. cs.PL/0111042</comments><acm-class>D.2.6;D.3.3; F.4.1;D.2.5</acm-class><abstract>  In this paper, we introduce a set of tools for providing user-friendly
explanations in an explanation-based constraint programming system. The idea is
to represent the constraints of a problem as an hierarchy (a tree). Users are
then represented as a set of understandable nodes in that tree (a cut).
Classical explanations (sets of system constraints) just need to get projected
on that representation in order to be understandable by any user. We present
here the main interests of this idea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111038</id><created>2001-11-14</created><updated>2001-11-30</updated><authors><author><keyname>Cooper</keyname><forenames>Martin</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author></authors><title>Arc consistency for soft constraints</title><categories>cs.AI cs.CC cs.DS</categories><comments>Figure 3 modified</comments><acm-class>I.2.8;F.4.1;D.3.3</acm-class><abstract>  The notion of arc consistency plays a central role in constraint
satisfaction. It is known that the notion of local consistency can be extended
to constraint optimisation problems defined by soft constraint frameworks based
on an idempotent cost combination operator. This excludes non idempotent
operators such as + which define problems which are very important in practical
applications such as Max-CSP, where the aim is to minimize the number of
violated constraints. In this paper, we show that using a weak additional axiom
satisfied by most existing soft constraints proposals, it is possible to define
a notion of soft arc consistency that extends the classical notion of arc
consistency and this even in the case of non idempotent cost combination
operators. A polynomial time algorithm for enforcing this soft arc consistency
exists and its space and time complexities are identical to that of enforcing
arc consistency in CSPs when the cost combination operator is strictly
monotonic (for example Max-CSP). A directional version of arc consistency is
potentially even stronger than the non-directional version, since it allows non
local propagation of penalties. We demonstrate the utility of directional arc
consistency by showing that it not only solves soft constraint problems on
trees, but that it also implies a form of local optimality, which we call arc
irreducibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111039</id><created>2001-11-14</created><updated>2001-11-19</updated><authors><author><keyname>Hanus</keyname><forenames>Michael</forenames></author><author><keyname>Koj</keyname><forenames>Johannes</forenames></author></authors><title>An Integrated Development Environment for Declarative Multi-Paradigm
  Programming</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus. cs.PL/0111042</comments><acm-class>D.1.1; D.1.3; D.1.6; D.2.6; D.2.5; D.3.4</acm-class><abstract>  In this paper we present CIDER (Curry Integrated Development EnviRonment), an
analysis and programming environment for the declarative multi-paradigm
language Curry. CIDER is a graphical environment to support the development of
Curry programs by providing integrated tools for the analysis and visualization
of programs. CIDER is completely implemented in Curry using libraries for GUI
programming (based on Tcl/Tk) and meta-programming. An important aspect of our
environment is the possible adaptation of the development environment to other
declarative source languages (e.g., Prolog or Haskell) and the extensibility
w.r.t. new analysis methods. To support the latter feature, the lazy evaluation
strategy of the underlying implementation language Curry becomes quite useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111040</id><created>2001-11-15</created><updated>2001-11-16</updated><authors><author><keyname>Bracchi</keyname><forenames>Christiane</forenames></author><author><keyname>Gefflot</keyname><forenames>Christophe</forenames></author><author><keyname>Paulin</keyname><forenames>Frederic</forenames></author></authors><title>Combining Propagation Information and Search Tree Visualization using
  ILOG OPL Studio</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus, cs.PL/0111042</comments><acm-class>D.1.6; D.2.6; D.2.5; F.4.1</acm-class><abstract>  In this paper we give an overview of the current state of the graphical
features provided by ILOG OPL Studio for debugging and performance tuning of
OPL programs or external ILOG Solver based applications. This paper focuses on
combining propagation and search information using the Search Tree view and the
Propagation Spy. A new synthetic view is presented: the Christmas Tree, which
combines the Search Tree view with statistics on the efficiency of the domain
reduction and on the number of the propagation events triggered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111041</id><created>2001-11-15</created><updated>2001-11-27</updated><authors><author><keyname>Ospina</keyname><forenames>Gustavo A.</forenames></author><author><keyname>Charlier</keyname><forenames>Baudouin Le</forenames></author></authors><title>On the Design of a Tool for Supporting the Construction of Logic
  Programs</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), Proceedings of the Eleventh Workshop on Logic
  Programming Environments (WLPE'01), December 1, 2001, Paphos, Cyprus.
  cs.PL/0111042</comments><acm-class>D.1.6;D.2.6</acm-class><abstract>  Environments for systematic construction of logic programs are needed in the
academy as well as in the industry. Such environments should support well
defined construction methods and should be able to be extended and interact
with other programming tools like debuggers and compilers. We present a variant
of the Deville methodology for logic program development, and the design of a
tool for supporting the methodology. Our aim is to facilitate the learning of
logic programming and to set the basis of more sophisticated tools for program
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111042</id><created>2001-11-16</created><updated>2001-11-22</updated><authors><author><keyname>Kusalik</keyname><forenames>Anthony</forenames></author></authors><title>Proceedings of the Eleventh Workshop on Logic Programming Environments
  (WLPE'01)</title><categories>cs.PL cs.SE</categories><comments>8 refereed papers; Anthony Kusalik, editor; 11WLPE, WLPE 2001</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  The Eleventh Workshop on Logic Programming Environments (WLPE'01) was one in
a series of international workshops in the topic area. It was held on December
1, 2001 in Paphos, Cyprus as a post-conference workshop at ICLP 2001. Eight
refereed papers were presented at the conference. A majority of the papers
involved, in some way, constraint logic programming and tools for software
development. Other topics areas addressed include execution visualization,
instructional aids (for learning users), software maintenance (including
debugging), and provisions for new paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111043</id><created>2001-11-16</created><authors><author><keyname>Langevine</keyname><forenames>Ludovic</forenames></author><author><keyname>Deransart</keyname><forenames>Pierre</forenames></author><author><keyname>Ducasse</keyname><forenames>Mireille</forenames></author><author><keyname>Jahier</keyname><forenames>Erwan</forenames></author></authors><title>Prototyping CLP(FD) Tracers: a Trace Model and an Experimental
  Validation Environment</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), Proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus. cs.PL/0111042</comments><acm-class>D.1.6; D.2.6; D.2.5; F.4.1</acm-class><abstract>  Developing and maintaining CLP programs requires visualization and
explanation tools. However, existing tools are built in an ad hoc way.
Therefore porting tools from one platform to another is very difficult. We have
shown in previous work that, from a fine-grained execution trace, a number of
interesting views about logic program executions could be generated by trace
analysis.
  In this article, we propose a trace model for constraint solving by
narrowing. This trace model is the first one proposed for CLP(FD) and does not
pretend to be the ultimate one. We also propose an instrumented
meta-interpreter in order to experiment with the model. Furthermore, we show
that the proposed trace model contains the necessary information to build known
and useful execution views. This work sets the basis for generic execution
analysis of CLP(FD) programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111044</id><created>2001-11-16</created><authors><author><keyname>Peng</keyname><forenames>S.</forenames></author><author><keyname>Lambiase</keyname><forenames>R.</forenames></author><author><keyname>Oerter</keyname><forenames>B.</forenames></author><author><keyname>Smith</keyname><forenames>J.</forenames></author></authors><title>SNS Standard Power Supply Interface</title><categories>cs.OH</categories><comments>PDF File</comments><acm-class>c.3; j7</acm-class><journal-ref>eConf C011127 (2001) THAP052</journal-ref><abstract>  The SNS has developed a standard power supply interface for the approximately
350 magnet power supplies in the SNS accumulator ring, Linac and transport
lines. Power supply manufacturers are providing supplies compatible with the
standard interface. The SNS standard consists of a VME based power supply
controller module (PSC) and a power supply interface unit (PSI) that mounts on
the power supply. Communication between the two is via a pair of multimode
fibers. This PSI/PSC system supports one 16-bit analog reference, four 16-bit
analog readbacks, fifteen digital commands and sixteen digital status bits in a
single fiber-isolated module. The system can send commands to the supplies and
read data from them synchronized to an external signal at up to a 10KHz rate.
The PSC time stamps and stores this data in a circular buffer so historical
data leading up to a fault event can be analyzed. The PSC contains a serial
port so that local testing of hardware can be accomplished with a laptop. This
paper concentrates on the software being provided to control the power supply.
It includes the EPICS driver; software to test hardware and power supplies via
the serial port and VME interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111045</id><created>2001-11-16</created><authors><author><keyname>Lagin</keyname><forenames>L. J.</forenames></author><author><keyname>Bettenhausen</keyname><forenames>R. C.</forenames></author><author><keyname>Carey</keyname><forenames>R. A.</forenames></author><author><keyname>Estes</keyname><forenames>C. M.</forenames></author><author><keyname>Fisher</keyname><forenames>J. M.</forenames></author><author><keyname>Krammen</keyname><forenames>J. E.</forenames></author><author><keyname>Reed</keyname><forenames>R. K.</forenames></author><author><keyname>VanArsdall</keyname><forenames>P. J.</forenames></author><author><keyname>Woodruff</keyname><forenames>J. P.</forenames></author></authors><title>The Overview of the National Ignition Facility Distributed Computer
  Control System</title><categories>cs.SE</categories><comments>submitted to ICALEPCS 2001, TUAP001</comments><acm-class>J.2</acm-class><journal-ref>eConf C011127 (2001) TUAP001</journal-ref><abstract>  The Integrated Computer Control System (ICCS) for the National Ignition
Facility (NIF) is a layered architecture of 300 front-end processors (FEP)
coordinated by supervisor subsystems including automatic beam alignment and
wavefront control, laser and target diagnostics, pulse power, and shot control
timed to 30 ps. FEP computers incorporate either VxWorks on PowerPC or Solaris
on UltraSPARC processors that interface to over 45,000 control points attached
to VME-bus or PCI-bus crates respectively. Typical devices are stepping motors,
transient digitizers, calorimeters, and photodiodes. The front-end layer is
divided into another segment comprised of an additional 14,000 control points
for industrial controls including vacuum, argon, synthetic air, and safety
interlocks implemented with Allen-Bradley programmable logic controllers
(PLCs). The computer network is augmented asynchronous transfer mode (ATM) that
delivers video streams from 500 sensor cameras monitoring the 192 laser beams
to operator workstations. Software is based on an object-oriented framework
using CORBA distribution that incorporates services for archiving, machine
configuration, graphical user interface, monitoring, event logging, scripting,
alert management, and access control. Software coding using a mixed language
environment of Ada95 and Java is one-third complete at over 300 thousand source
lines. Control system installation is currently under way for the first 8
beams, with project completion scheduled for 2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111046</id><created>2001-11-19</created><authors><author><keyname>Ed-Dbali</keyname><forenames>AbdelAli</forenames><affiliation>LIFO - University of Orleans - France</affiliation></author><author><keyname>Deransart</keyname><forenames>Pierre</forenames><affiliation>INRIA Rocquencourt - France</affiliation></author><author><keyname>Bigonha</keyname><forenames>Mariza A. S.</forenames><affiliation>DCC - UFMG - Brazil</affiliation></author><author><keyname>de Siqueira</keyname><forenames>Jose</forenames><affiliation>DCC - UFMG - Brazil</affiliation></author><author><keyname>Bigonha</keyname><forenames>Roberto da S.</forenames><affiliation>DCC - UFMG - Brazil</affiliation></author></authors><title>HyperPro An integrated documentation environment for CLP</title><categories>cs.PL cs.SE</categories><comments>In A. Kusalik (ed), Proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus. cs.PL/0111042</comments><acm-class>D.1.6; D.2.6 (possibly also D.2.5; F.4.1; I.2.3)</acm-class><abstract>  The purpose of this paper is to present some functionalities of the HyperPro
System. HyperPro is a hypertext tool which allows to develop Constraint Logic
Programming (CLP) together with their documentation. The text editing part is
not new and is based on the free software Thot. A HyperPro program is a Thot
document written in a report style. The tool is designed for CLP but it can be
adapted to other programming paradigms as well. Thot offers navigation and
editing facilities and synchronized static document views. HyperPro has new
functionalities such as document exportations, dynamic views (projections),
indexes and version management. Projection is a mechanism for extracting and
exporting relevant pieces of code program or of document according to specific
criteria. Indexes are useful to find the references and occurrences of a
relation in a document, i.e., where its predicate definition is found and where
a relation is used in other programs or document versions and, to translate
hyper-texts links into paper references. It still lack importation facilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111047</id><created>2001-11-17</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Branson</keyname><forenames>Kim</forenames></author><author><keyname>Giddy</keyname><forenames>Jon</forenames></author><author><keyname>Abramson</keyname><forenames>David</forenames></author></authors><title>Virtual Laboratory: Enabling On-Demand Drug Design with the World Wide
  Grid</title><categories>cs.DC</categories><acm-class>J.3</acm-class><abstract>  Computational Grids are emerging as a popular paradigm for solving
large-scale compute and data intensive problems in science, engineering, and
commerce. However, application composition, resource management and scheduling
in these environments is a complex undertaking. In this paper, we illustrate
the creation of a virtual laboratory environment by leveraging existing Grid
technologies to enable molecular modeling for drug design on distributed
resources. It involves screening millions of molecules of chemical compounds
against a protein target, chemical database (CDB) to identify those with
potential use for drug design. We have grid-enabled the molecular docking
process by composing it as a parameter sweep application using the Nimrod-G
tools. We then developed new tools for remote access to molecules in CDB small
molecule database. The Nimrod-G resource broker along with molecule CDB data
broker is used for scheduling and on-demand processing of jobs on distributed
grid resources. The results demonstrate the ease of use and suitability of the
Nimrod-G and virtual laboratory tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111048</id><created>2001-11-17</created><authors><author><keyname>Abramson</keyname><forenames>David</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Giddy</keyname><forenames>Jonathan</forenames></author></authors><title>A Computational Economy for Grid Computing and its Implementation in the
  Nimrod-G Resource Brok</title><categories>cs.DC</categories><acm-class>C.2.4</acm-class><abstract>  Computational Grids, coupling geographically distributed resources such as
PCs, workstations, clusters, and scientific instruments, have emerged as a next
generation computing platform for solving large-scale problems in science,
engineering, and commerce. However, application development, resource
management, and scheduling in these environments continue to be a complex
undertaking. In this article, we discuss our efforts in developing a resource
management system for scheduling computations on resources distributed across
the world with varying quality of service. Our service-oriented grid computing
system called Nimrod-G manages all operations associated with remote execution
including resource discovery, trading, scheduling based on economic principles
and a user defined quality of service requirement. The Nimrod-G resource broker
is implemented by leveraging existing technologies such as Globus, and provides
new services that are essential for constructing industrial-strength Grids. We
discuss results of preliminary experiments on scheduling some parametric
computations using the Nimrod-G resource broker on a world-wide grid testbed
that spans five continents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111049</id><created>2001-11-19</created><authors><author><keyname>Castro</keyname><forenames>Luis F.</forenames></author><author><keyname>Warren</keyname><forenames>David S.</forenames></author></authors><title>An Environment for the Exploration of Non Monotonic Logic Programs</title><categories>cs.PL cs.LO</categories><comments>* In A. Kusalik (ed), Proceedings of the Eleventh International
  Workshop on Logic Programming Environments (WLPE'01), December 1, 2001,
  Paphos, Cyprus. cs.PL/0111042</comments><acm-class>D.1.6; D.2.6</acm-class><abstract>  Stable Model Semantics and Well Founded Semantics have been shown to be very
useful in several applications of non-monotonic reasoning. However, Stable
Models presents a high computational complexity, whereas Well Founded Semantics
is easy to compute and provides an approximation of Stable Models. Efficient
engines exist for both semantics of logic programs. This work presents a
computational integration of two of such systems, namely XSB and SMODELS. The
resulting system is called XNMR, and provides an interactive system for the
exploration of both semantics. Aspects such as modularity can be exploited in
order to ease debugging of large knowledge bases with the usual Prolog
debugging techniques and an interactive environment. Besides, the use of a full
Prolog system as a front-end to a Stable Models engine augments the language
usually accepted by such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111050</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111050</id><created>2001-11-19</created><updated>2003-10-09</updated><authors><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes
  Polynomial Time</title><categories>cs.DS</categories><comments>Revised. Improved statement of main theorem</comments><acm-class>G.1.6</acm-class><abstract>  We introduce the smoothed analysis of algorithms, which is a hybrid of the
worst-case and average-case analysis of algorithms. In smoothed analysis, we
measure the maximum over inputs of the expected performance of an algorithm
under small random perturbations of that input. We measure this performance in
terms of both the input size and the magnitude of the perturbations. We show
that the simplex algorithm has polynomial smoothed complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111051</id><created>2001-11-19</created><authors><author><keyname>Ieong</keyname><forenames>Samuel</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Yiu</keyname><forenames>Siu-Ming</forenames></author></authors><title>Predicting RNA Secondary Structures with Arbitrary Pseudoknots by
  Maximizing the Number of Stacking Pairs</title><categories>cs.CE cs.DS q-bio</categories><comments>A preliminary version of this work appeared in Proceedings of the
  IEEE International Symposium on Bio-Informatics &amp; Biomedical Engineering
  (BIBE 2001), Washington, DC, 2001</comments><acm-class>F2.2; G2.3; J.3</acm-class><abstract>  The paper investigates the computational problem of predicting RNA secondary
structures. The general belief is that allowing pseudoknots makes the problem
hard. Existing polynomial-time algorithms are heuristic algorithms with no
performance guarantee and can only handle limited types of pseudoknots. In this
paper we initiate the study of predicting RNA secondary structures with a
maximum number of stacking pairs while allowing arbitrary pseudoknots. We
obtain two approximation algorithms with worst-case approximation ratios of 1/2
and 1/3 for planar and general secondary structures,respectively. For an RNA
sequence of $n$ bases, the approximation algorithm for planar secondary
structures runs in $O(n^3)$ time while that for the general case runs in linear
time. Furthermore, we prove that allowing pseudoknots makes it NP-hard to
maximize the number of stacking pairs in a planar secondary structure. This
result is in contrast with the recent NP-hard results on psuedoknots which are
based on optimizing some general and complicated energy functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111052</id><created>2001-11-20</created><authors><author><keyname>Vyalyi</keyname><forenames>M. N.</forenames></author></authors><title>A comparison of Zeroes and Ones of a Boolean Polynomial</title><categories>cs.CC</categories><comments>LaTeX2e, 6 pages</comments><acm-class>F.1.3</acm-class><abstract>  In this paper we consider the computational complexity of the following
problem. Let $f$ be a Boolean polynomial. What value of $f$, 0 or 1, is taken
more frequently? The problem is solved in polynomial time for polynomials of
degrees 1,2. The next case of degree 3 appears to be PP-complete under
polynomial reductions in the class of promise problems. The proof is based on
techniques of quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111053</id><created>2001-11-20</created><updated>2006-12-09</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Meaningful Information</title><categories>cs.CC math-ph math.MP math.PR physics.data-an</categories><comments>LaTeX, 12 pages, published in Proc. 13th International Symposium on
  Algorithms and Computation (ISAAC)}, Lecture Notes in Computer Science, Vol
  ???, Springer-Verlag, Berlin, 2002. For background see Gacs-Tromp-Vitanyi
  math.PR/0006233, and especially Vereschagin-Vitanyi cs.CC/0204037. Replaced
  by the final, improved, Journal version</comments><acm-class>E.5; E.4; E.2; H.1.1; F.1.1; F.1.3</acm-class><journal-ref>IEEE Trans. Inform. Th., 52:10(2006), 4617--4626</journal-ref><abstract>  The information in an individual finite object (like a binary string) is
commonly measured by its Kolmogorov complexity. One can divide that information
into two parts: the information accounting for the useful regularity present in
the object and the information accounting for the remaining accidental
information. There can be several ways (model classes) in which the regularity
is expressed. Kolmogorov has proposed the model class of finite sets,
generalized later to computable probability mass functions. The resulting
theory, known as Algorithmic Statistics, analyzes the algorithmic sufficient
statistic when the statistic is restricted to the given model class. However,
the most general way to proceed is perhaps to express the useful information as
a recursive function. The resulting measure has been called the
``sophistication'' of the object. We develop the theory of recursive functions
statistic, the maximum and minimum value, the existence of absolutely
nonstochastic objects (that have maximal sophistication--all the information in
them is meaningful and there is no residual randomness), determine its relation
with the more restricted model classes of finite sets, and computable
probability distributions, in particular with respect to the algorithmic
(Kolmogorov) minimal sufficient statistic, the relation to the halting problem
and further algorithmic properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111054</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111054</id><created>2001-11-20</created><updated>2004-08-05</updated><authors><author><keyname>Li</keyname><forenames>Ming</forenames><affiliation>Univ. of Waterloo and BioInformatics Solutions Inc.</affiliation></author><author><keyname>Chen</keyname><forenames>Xin</forenames><affiliation>Univ. California, Santa Barbara</affiliation></author><author><keyname>Li</keyname><forenames>Xin</forenames><affiliation>Univ. Western Ontario</affiliation></author><author><keyname>Ma</keyname><forenames>Bin</forenames><affiliation>Univ. Western Ontario</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and Univ. of Amsterdam</affiliation></author></authors><title>The similarity metric</title><categories>cs.CC cond-mat.stat-mech cs.CE cs.CV math.CO math.MG math.ST physics.data-an q-bio.GN stat.TH</categories><comments>13 pages, LaTex, 5 figures, Part of this work appeared in Proc. 14th
  ACM-SIAM Symp. Discrete Algorithms, 2003. This is the final, corrected,
  version to appear in IEEE Trans Inform. Th</comments><acm-class>J.3, E.4</acm-class><abstract>  A new class of distances appropriate for measuring similarity relations
between sequences, say one type of similarity per distance, is studied. We
propose a new ``normalized information distance'', based on the noncomputable
notion of Kolmogorov complexity, and show that it is in this class and it
minorizes every computable distance in the class (that is, it is universal in
that it discovers all computable similarities). We demonstrate that it is a
metric and call it the {\em similarity metric}. This theory forms the
foundation for a new practical tool. To evidence generality and robustness we
give two distinctive applications in widely divergent areas using standard
compression programs like gzip and GenCompress. First, we compare whole
mitochondrial genomes and infer their evolutionary history. This results in a
first completely automatic computed whole mitochondrial phylogeny tree.
Secondly, we fully automatically compute the language tree of 52 different
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111055</id><created>2001-11-20</created><updated>2001-12-10</updated><authors><author><keyname>Sichta</keyname><forenames>P.</forenames></author><author><keyname>Dong</keyname><forenames>J.</forenames></author><author><keyname>Oliaro</keyname><forenames>G.</forenames></author><author><keyname>Roney</keyname><forenames>P.</forenames></author></authors><title>Overview of the NSTX Control System</title><categories>cs.OH</categories><comments>3 PDF pages, 8th International Conference on Accelerator and Large
  Experimental Physics Control Systems (PSN TUBT004), San Jose, CA, USA,
  November 27-30</comments><acm-class>C.3;J.7</acm-class><journal-ref>eConf C011127 (2001) TUBT004</journal-ref><abstract>  The National Spherical Torus Experiment (NSTX) is an innovative magnetic
fusion device that was constructed by the Princeton Plasma Physics Laboratory
(PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia
University, and the University of Washington at Seattle. Since achieving first
plasma in 1999, the device has been used for fusion research through an
international collaboration of over twenty institutions. The NSTX is operated
through a collection of control systems that encompass a wide range of
technology, from hardwired relay controls to real-time control systems with
giga-FLOPS of capability. This paper presents a broad introduction to the
control systems used on NSTX, with an emphasis on the computing controls, data
acquisition, and synchronization systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111056</id><created>2001-11-21</created><updated>2002-12-20</updated><authors><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Some Facets of Complexity Theory and Cryptography: A Five-Lectures
  Tutorial</title><categories>cs.CC cs.CR</categories><comments>57 pages, 17 figures, Lecture Notes for the 11th Jyvaskyla Summer
  School</comments><acm-class>E.3; F.1.3; F.2.2</acm-class><journal-ref>ACM Computing Surveys, volume 34, issue 4, pp. 504--549, December
  2002</journal-ref><abstract>  In this tutorial, selected topics of cryptology and of computational
complexity theory are presented. We give a brief overview of the history and
the foundations of classical cryptography, and then move on to modern
public-key cryptography. Particular attention is paid to cryptographic
protocols and the problem of constructing the key components of such protocols
such as one-way functions. A function is one-way if it is easy to compute, but
hard to invert. We discuss the notion of one-way functions both in a
cryptographic and in a complexity-theoretic setting. We also consider
interactive proof systems and present some interesting zero-knowledge
protocols. In a zero-knowledge protocol one party can convince the other party
of knowing some secret information without disclosing any bit of this
information. Motivated by these protocols, we survey some complexity-theoretic
results on interactive proof systems and related complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111057</id><created>2001-11-23</created><authors><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Towards a characterization of the star-free sets of integers</title><categories>cs.CC cs.LO</categories><comments>15 pages, 4 figures</comments><acm-class>F.1.1;F.4.1;F.4.3</acm-class><abstract>  Let U be a numeration system, a set X of integers is U-star-free if the set
made up of the U-representations of the elements in X is a star-free regular
language. Answering a question of A. de Luca and A. Restivo, we obtain a
complete logical characterization of the U-star-free sets of integers for
suitable numeration systems related to a Pisot number and in particular for
integer base systems. For these latter systems, we study as well the problem of
the base dependence. Finally, the case of k-adic systems is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111058</id><created>2001-11-23</created><authors><author><keyname>Kersting</keyname><forenames>Kristian</forenames><affiliation>Institute of Computer Science, Albert-Ludwigs-University Freiburg, Germany</affiliation></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames><affiliation>Institute of Computer Science, Albert-Ludwigs-University Freiburg, Germany</affiliation></author></authors><title>Bayesian Logic Programs</title><categories>cs.AI cs.LO</categories><comments>52 pages</comments><report-no>151</report-no><acm-class>I.2; I.2.3; I.2.4; G.3; F.3.2; F.4.1</acm-class><abstract>  Bayesian networks provide an elegant formalism for representing and reasoning
about uncertainty using probability theory. Theyare a probabilistic extension
of propositional logic and, hence, inherit some of the limitations of
propositional logic, such as the difficulties to represent objects and
relations. We introduce a generalization of Bayesian networks, called Bayesian
logic programs, to overcome these limitations. In order to represent objects
and relations it combines Bayesian networks with definite clause logic by
establishing a one-to-one mapping between ground atoms and random variables. We
show that Bayesian logic programs combine the advantages of both definite
clause logic and Bayesian networks. This includes the separation of
quantitative and qualitative aspects of the model. Furthermore, Bayesian logic
programs generalize both Bayesian networks as well as logic programs. So, many
ideas developed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111059</id><created>2001-11-27</created><authors><author><keyname>Loyer</keyname><forenames>Yann</forenames></author><author><keyname>Spyratos</keyname><forenames>Nicolas</forenames></author><author><keyname>Stamate</keyname><forenames>Daniel</forenames></author></authors><title>Hypotheses Founded Semantics of Logic Programs for Information
  Integration in Multi-Valued Logics</title><categories>cs.LO</categories><comments>27pages, 1 figure</comments><acm-class>I.2.3; I.2.11; H.1.1</acm-class><abstract>  We address the problem of integrating information coming from different
sources. The information consists of facts that a central server collects and
tries to combine using (a) a set of logical rules, i.e. a logic program, and
(b) a hypothesis representing the server's own estimates. In such a setting
incomplete information from a source or contradictory information from
different sources necessitate the use of many-valued logics in which programs
can be evaluated and hypotheses can be tested. To carry out such activities we
propose a formal framework based on bilattices such as Belnap's four-valued
logics. In this framework we work with the class of programs defined by Fitting
and we develop a theory for information integration.
 We also establish an intuitively appealing connection between our hypothesis
testing mechanism on the one hand, and the well-founded semantics and
Kripke-Kleene semantics of Datalog programs with negation, on the other hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111060</id><created>2001-11-28</created><authors><author><keyname>Kwee</keyname><forenames>Ivo</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Gradient-based Reinforcement Planning in Policy-Search Methods</title><categories>cs.AI</categories><comments>This is an extended version of the paper presented at the EWRL 2001
  in Utrecht (The Netherlands)</comments><report-no>14-01</report-no><acm-class>I.2; I.2.6; I.2.8</acm-class><abstract>  We introduce a learning method called ``gradient-based reinforcement
planning'' (GREP). Unlike traditional DP methods that improve their policy
backwards in time, GREP is a gradient-based method that plans ahead and
improves its policy before it actually acts in the environment. We derive
formulas for the exact policy gradient that maximizes the expected future
reward and confirm our ideas with numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111061</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111061</id><created>2001-11-29</created><updated>2001-11-30</updated><authors><author><keyname>Naidenko</keyname><forenames>V.</forenames></author><author><keyname>Orlovich</keyname><forenames>Yu.</forenames></author></authors><title>On a Special Case of the Generalized Neighbourhood Problem</title><categories>cs.DM</categories><comments>13 pages, 3 figures, in Russian</comments><acm-class>G.2.2</acm-class><abstract>  For a given finite class of finite graphs H, a graph G is called a
realization of H if the neighbourhood of its any vertex induces the subgraph
isomorphic to a graph of H. We consider the following problem known as the
Generalized Neighbourhood Problem (GNP): given a finite class of finite graphs
H, does there exist a non-empty graph G that is a realization of H? In fact,
there are two modifications of that problem, namely the finite (the existence
of a finite realization is required) and infinite one (the realization is
required to be infinite). In this paper we show that GNP and its modifications
for all finite classes H of finite graphs are reduced to the same problems with
an additional restriction on H. Namely, the orders of any two graphs of H are
equal and every graph of H has exactly s dominating vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111062</id><created>2001-11-29</created><updated>2004-02-27</updated><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author></authors><title>One-way communication complexity and the Neciporuk lower bound on
  formula size</title><categories>cs.CC quant-ph</categories><comments>32 pages, conference versions at ISAAC '97, Complexity '98, STOC '00</comments><acm-class>F.1.3;F.1.2</acm-class><abstract>  In this paper the Neciporuk method for proving lower bounds on the size of
Boolean formulae is reformulated in terms of one-way communication complexity.
We investigate the scenarios of probabilistic formulae, nondeterministic
formulae, and quantum formulae. In all cases we can use results about one-way
communication complexity to prove lower bounds on formula size. In the latter
two cases we newly develop the employed communication complexity bounds. The
main results regarding formula size are as follows: A polynomial size gap
between probabilistic/quantum and deterministic formulae. A near-quadratic size
gap for nondeterministic formulae with limited access to nondeterministic bits.
A near quadratic lower bound on quantum formula size, as well as a polynomial
separation between the sizes of quantum formulae with and without multiple read
random inputs. The methods for quantum and probabilistic formulae employ a
variant of the Neciporuk bound in terms of the VC-dimension. Regarding
communication complexity we give optimal separations between one-way and
two-way protocols in the cases of limited nondeterministic and quantum
communication, and we show that zero-error quantum one-way communication
complexity asymptotically equals deterministic one-way communication complexity
for total functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111063</id><created>2001-11-30</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>New RBF collocation methods and kernel RBF with applications</title><categories>cs.NA cs.CE</categories><comments>Welcom any comments to wenc@simula.no</comments><acm-class>G1.3, G1.8</acm-class><abstract>  A few novel radial basis function (RBF) discretization schemes for partial
differential equations are developed in this study. For boundary-type methods,
we derive the indirect and direct symmetric boundary knot methods. Based on the
multiple reciprocity principle, the boundary particle method is introduced for
general inhomogeneous problems without using inner nodes. For domain-type
schemes, by using the Green integral we develop a novel Hermite RBF scheme
called the modified Kansa method, which significantly reduces calculation
errors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the
least square RBF collocation scheme. Finally, five types of the kernel RBF are
also briefly presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111064</id><created>2001-11-30</created><authors><author><keyname>Venkataraman</keyname><forenames>Anand</forenames></author></authors><title>A procedure for unsupervised lexicon learning</title><categories>cs.CL</categories><comments>Expanded version of this paper appears in Computational Linguistics
  27(3)</comments><acm-class>I.2.6;I.2.7</acm-class><journal-ref>Proceedings of the eighteenth international conference on machine
  learning, ICML-01, pp.569--576, 2001</journal-ref><abstract>  We describe an incremental unsupervised procedure to learn words from
transcribed continuous speech. The algorithm is based on a conservative and
traditional statistical model, and results of empirical tests show that it is
competitive with other algorithms that have been proposed recently for this
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0111065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0111065</id><created>2001-11-30</created><authors><author><keyname>Venkataraman</keyname><forenames>Anand</forenames></author></authors><title>A Statistical Model for Word Discovery in Transcribed Speech</title><categories>cs.CL</categories><comments>Expanded version of ICML-01 paper (pp.569--576)</comments><acm-class>I.2.6;I.2.7</acm-class><journal-ref>Computational Linguistics, 27(3), pp.352--372, 2001</journal-ref><abstract>  A statistical model for segmentation and word discovery in continuous speech
is presented. An incremental unsupervised learning algorithm to infer word
boundaries based on this model is described. Results of empirical tests showing
that the algorithm is competitive with other models that have been used for
similar tasks are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112001</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112001</id><created>2001-12-02</created><updated>2015-07-30</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ramarathnam</forenames></author></authors><title>An Average Case NP-Complete Graph Coloring Problem</title><categories>cs.CC</categories><comments>15 pages, major revision and simplification from v3; now also proof
  overview added</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NP-complete problems should be hard on some instances but those may be
extremely rare. On generic instances many such problems, especially related to
random graphs, have been proven easy. We show the intractability of random
instances of a graph coloring problem: this graph problem is hard on average
unless all NP problem under all samplable (i.e., generatable in polynomial
time) distributions are easy. Worst case reductions use special gadgets and
typically map instances into a negligible fraction of possible outputs. Ours
must output nearly random graphs and avoid any super-polynomial distortion of
probabilities. This poses significant technical difficulty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112002</id><created>2001-12-03</created><authors><author><keyname>Stewart</keyname><forenames>Iain A.</forenames></author></authors><title>Program schemes with binary write-once arrays and the complexity classes
  they capture</title><categories>cs.LO cs.CC</categories><acm-class>F.4.1; F.1.3; F.1.1</acm-class><abstract>  We study a class of program schemes, NPSB, in which, aside from basic
assignments, non-deterministic guessing and while loops, we have access to
arrays; but where these arrays are binary write-once in that they are
initialized to `zero' and can only ever be set to `one'. We show, amongst other
results, that: NPSB can be realized as a vectorized Lindstrom logic; there are
problems accepted by program schemes of NPSB that are not definable in the
bounded-variable infinitary logic ${\cal L}^\omega_{\infty\omega}$; all
problems accepted by the program schemes of NPSB have a zero-one law; and on
ordered structures, NPSB captures the complexity class $[ L]^[{\scriptsize
NP\normalsize}]$. The class of program schemes NPSB is actually the union of an
infinite hierarchy of classes of program schemes. When we amend the semantics
of our program schemes slightly, we find that the classes of the resulting
hierarchy capture the complexity classes $\Sigma^p_i$ (where $i\geq 1$) of the
Polynomial Hierarchy PH. Finally, we give logical equivalences of the
complexity-theoretic question `Does NP equal PSPACE?' where the logics (and
classes of program schemes) involved define only problems with zero-one laws
(and so do not define some computationally trivial problems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112003</id><created>2001-12-05</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Uchimoto</keyname><forenames>Kiyotaka</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Using a Support-Vector Machine for Japanese-to-English Translation of
  Tense, Aspect, and Modality</title><categories>cs.CL</categories><comments>8 pages. Computation and Language</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>ACL Workshop, the Data-Driven Machine Translation, 2001</journal-ref><abstract>  This paper describes experiments carried out using a variety of
machine-learning methods, including the k-nearest neighborhood method that was
used in a previous study, for the translation of tense, aspect, and modality.
It was found that the support-vector machine method was the most precise of all
the methods tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112004</id><created>2001-12-05</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Ma</keyname><forenames>Qing</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Part of Speech Tagging in Thai Language Using Support Vector Machine</title><categories>cs.CL</categories><comments>8 pages. Computation and Language</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>NLPRS'2001 Workshop, the Second Workshop on Natural Language
  Processing and Neural Networks (NLPNN2001)</journal-ref><abstract>  The elastic-input neuro tagger and hybrid tagger, combined with a neural
network and Brill's error-driven learning, have already been proposed for the
purpose of constructing a practical tagger using as little training data as
possible. When a small Thai corpus is used for training, these taggers have
tagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words
in terms of the part of speech), respectively. In this study, in order to
construct more accurate taggers we developed new tagging methods using three
machine learning methods: the decision-list, maximum entropy, and support
vector machine methods. We then performed tagging experiments by using these
methods. Our results showed that the support vector machine method has the best
precision (96.1%), and that it is capable of improving the accuracy of tagging
in the Thai language. Finally, we theoretically examined all these methods and
discussed how the improvements were achived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112005</id><created>2001-12-05</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Universal Model for Paraphrasing -- Using Transformation Based on a
  Defined Criteria --</title><categories>cs.CL</categories><comments>8 pages. Computation and Language</comments><acm-class>H.3.3; I.2.7</acm-class><journal-ref>NLPRS'2001, Workshop on Automatic Paraphrasing: Theories and
  Applications</journal-ref><abstract>  This paper describes a universal model for paraphrasing that transforms
according to defined criteria. We showed that by using different criteria we
could construct different kinds of paraphrasing systems including one for
answering questions, one for compressing sentences, one for polishing up, and
one for transforming written language to spoken language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112006</identifier>
 <datestamp>2008-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112006</id><created>2001-12-05</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Pfeifer</keyname><forenames>Gerald</forenames></author><author><keyname>Polleres</keyname><forenames>Axel</forenames></author></authors><title>A Logic Programming Approach to Knowledge-State Planning: Semantics and
  Complexity</title><categories>cs.AI cs.LO</categories><comments>48 pages, appeared as a Technical Report at KBS of the Vienna
  University of Technology, see http://www.kr.tuwien.ac.at/research/reports/</comments><acm-class>I.2.4; I.2.8; I.2.3</acm-class><journal-ref>Artificial Intelligence 144:157-211, 2003</journal-ref><doi>10.1016/S0004-3702(02)00367-3</doi><abstract>  We propose a new declarative planning language, called K, which is based on
principles and methods of logic programming. In this language, transitions
between states of knowledge can be described, rather than transitions between
completely described states of the world, which makes the language well-suited
for planning under incomplete knowledge. Furthermore, it enables the use of
default principles in the planning process by supporting negation as failure.
Nonetheless, K also supports the representation of transitions between states
of the world (i.e., states of complete knowledge) as a special case, which
shows that the language is very flexible. As we demonstrate on particular
examples, the use of knowledge states may allow for a natural and compact
problem representation. We then provide a thorough analysis of the
computational complexity of K, and consider different planning problems,
including standard planning and secure planning (also known as conformant
planning) problems. We show that these problems have different complexities
under various restrictions, ranging from NP to NEXPTIME in the propositional
case. Our results form the theoretical basis for the DLV^K system, which
implements the language K on top of the DLV logic programming system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112007</id><created>2001-12-07</created><updated>2002-11-29</updated><authors><author><keyname>Geerts</keyname><forenames>Floris</forenames></author><author><keyname>Goethals</keyname><forenames>Bart</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>A Tight Upper Bound on the Number of Candidate Patterns</title><categories>cs.DB cs.AI</categories><acm-class>H.2.8</acm-class><abstract>  In the context of mining for frequent patterns using the standard levelwise
algorithm, the following question arises: given the current level and the
current set of frequent patterns, what is the maximal number of candidate
patterns that can be generated on the next level? We answer this question by
providing a tight upper bound, derived from a combinatorial result from the
sixties by Kruskal and Katona. Our result is useful to reduce the number of
database scans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112008</id><created>2001-12-07</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author></authors><title>Representation of Uncertainty for Limit Processes</title><categories>cs.AI cs.NA</categories><acm-class>G.1; E.1</acm-class><abstract>  Many mathematical models utilize limit processes. Continuous functions and
the calculus, differential equations and topology, all are based on limits and
continuity. However, when we perform measurements and computations, we can
achieve only approximate results. In some cases, this discrepancy between
theoretical schemes and practical actions changes drastically outcomes of a
research and decision-making resulting in uncertainty of knowledge. In the
paper, a mathematical approach to such kind of uncertainty, which emerges in
computation and measurement, is suggested on the base of the concept of a fuzzy
limit. A mathematical technique is developed for differential models with
uncertainty. To take into account the intrinsic uncertainty of a model, it is
suggested to use fuzzy derivatives instead of conventional derivatives of
functions in this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112009</id><created>2001-12-08</created><authors><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijay</forenames></author></authors><title>DNA Self-Assembly For Constructing 3D Boxes</title><categories>cs.CC cs.CE</categories><comments>15 pages, 3 figures. Extended abstract included in ISAAC 2001
  proceedings</comments><acm-class>F.2.2; F.1.1; J.3</acm-class><journal-ref>Algorithms and Computation, 12th International Symposium, ISAAC
  2001 Proceedings. Springer LNCS 2223 (2001): 429-440</journal-ref><abstract>  We propose a mathematical model of DNA self-assembly using 2D tiles to form
3D nanostructures. This is the first work to combine studies in self-assembly
and nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Our
model is a more precise superset of their Tile Assembly Model that facilitates
building scalable 3D molecules. Under our model, we present algorithms to build
a hollow cube, which is intuitively one of the simplest 3D structures to
construct. We also introduce five basic measures of complexity to analyze these
algorithms. Our model and algorithmic techniques are applicable to more complex
2D and 3D nanostructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112010</id><created>2001-12-10</created><authors><author><keyname>Sgarbas</keyname><forenames>Kyriakos N.</forenames></author><author><keyname>Fakotakis</keyname><forenames>Nikos D.</forenames></author><author><keyname>Kokkinakis</keyname><forenames>George K.</forenames></author></authors><title>A Straightforward Approach to Morphological Analysis and Synthesis</title><categories>cs.CL cs.DS</categories><comments>4(+2) pages, 3 figures, 1 table, 10 references. Keywords: Morphology,
  Directed Acyclic Word Graphs, Lexicon Structures. For related work, see also
  http://slt.wcl.ee.upatras.gr</comments><acm-class>I.2.7; F.2.2; E.1</acm-class><journal-ref>Proc. COMLEX 2000, Workshop on Computational Lexicography and
  Multimedia Dictionaries, pp.31-34, Kato Achaia, Greece, 22-23 September 2000.</journal-ref><abstract>  In this paper we present a lexicon-based approach to the problem of
morphological processing. Full-form words, lemmas and grammatical tags are
interconnected in a DAWG. Thus, the process of analysis/synthesis is reduced to
a search in the graph, which is very fast and can be performed even if several
pieces of information are missing from the input. The contents of the DAWG are
updated using an on-line incremental process. The proposed approach is language
independent and it does not utilize any morphophonetic rules or any other
special linguistic information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112011</id><created>2001-12-10</created><updated>2003-02-05</updated><authors><author><keyname>Goethals</keyname><forenames>Bart</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Interactive Constrained Association Rule Mining</title><categories>cs.DB cs.AI</categories><comments>A preliminary report on this work was presented at the Second
  International Conference on Knowledge Discovery and Data Mining (DaWaK 2000)</comments><acm-class>H.2.8</acm-class><abstract>  We investigate ways to support interactive mining sessions, in the setting of
association rule mining. In such sessions, users specify conditions (queries)
on the associations to be generated. Our approach is a combination of the
integration of querying conditions inside the mining phase, and the incremental
querying of already generated associations. We present several concrete
algorithms and compare their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112012</id><created>2001-12-11</created><updated>2001-12-12</updated><authors><author><keyname>Krob</keyname><forenames>Daniel</forenames></author><author><keyname>Mairesse</keyname><forenames>Jean</forenames></author><author><keyname>Michos</keyname><forenames>Ioannis</forenames></author></authors><title>Computing the average parallelism in trace monoids</title><categories>cs.DM cs.DC</categories><comments>This is an extended version with proofs of D. Krob, J. Mairesse, and
  I. Michos. On the average parallelism in trace monoids. In H. Alt and A.
  Ferreira, editors, {\em Proceedings of STACS'02}, LNCS. Springer-Verlag, 2002</comments><acm-class>G.2.1; G.2.2; F.4.3</acm-class><abstract>  The {\em height} of a trace is the height of the corresponding heap of pieces
in Viennot's representation, or equivalently the number of factors in its
Cartier-Foata decomposition. Let $h(t)$ and $|t|$ stand respectively for the
height and the length of a trace $t$. Roughly speaking, $|t|$ is the
`sequential' execution time and $h(t)$ is the `parallel' execution time. We
prove that the bivariate commutative series $\sum_t x^{h(t)}y^{|t|}$ is
rational, and we give a finite representation of it. We use the rationality to
obtain precise information on the asymptotics of the number of traces of a
given height or length. Then, we study the average height of a trace for
various probability distributions on traces. For the uniform probability
distribution on traces of the same length (resp. of the same height), the
asymptotic average height (resp. length) exists and is an algebraic number. To
illustrate our results and methods, we consider a couple of examples: the free
commutative monoid and the trace monoid whose independence graph is the ladder
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112013</id><created>2001-12-11</created><authors><author><keyname>Brijs</keyname><forenames>Tom</forenames></author><author><keyname>Goethals</keyname><forenames>Bart</forenames></author><author><keyname>Swinnen</keyname><forenames>Gilbert</forenames></author><author><keyname>Vanhoof</keyname><forenames>Koen</forenames></author><author><keyname>Wets</keyname><forenames>Geert</forenames></author></authors><title>A Data Mining Framework for Optimal Product Selection in Retail
  Supermarket Data: The Generalized PROFSET Model</title><categories>cs.DB cs.AI</categories><acm-class>H.2.8</acm-class><abstract>  In recent years, data mining researchers have developed efficient association
rule algorithms for retail market basket analysis. Still, retailers often
complain about how to adopt association rules to optimize concrete retail
marketing-mix decisions. It is in this context that, in a previous paper, the
authors have introduced a product selection model called PROFSET. This model
selects the most interesting products from a product assortment based on their
cross-selling potential given some retailer defined constraints. However this
model suffered from an important deficiency: it could not deal effectively with
supermarket data, and no provisions were taken to include retail category
management principles. Therefore, in this paper, the authors present an
important generalization of the existing model in order to make it suitable for
supermarket data as well, and to enable retailers to add category restrictions
to the model. Experiments on real world data obtained from a Belgian
supermarket chain produce very promising results and demonstrate the
effectiveness of the generalized PROFSET model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112014</identifier>
 <datestamp>2010-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112014</id><created>2001-12-12</created><updated>2010-10-31</updated><authors><author><keyname>Shamir</keyname><forenames>Adi</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>Guaranteeing the diversity of number generators</title><categories>cs.CR cs.CC math.CO</categories><comments>Small updates</comments><acm-class>G.3; G.2.1; D.4.6</acm-class><journal-ref>Information and Computation 171 (2001), 350--363</journal-ref><doi>10.1006/inco.2001.3045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem in using iterative number generators of the form
x_i=f(x_{i-1}) is that they can enter unexpectedly short cycles. This is hard
to analyze when the generator is designed, hard to detect in real time when the
generator is used, and can have devastating cryptanalytic implications. In this
paper we define a measure of security, called_sequence_diversity_, which
generalizes the notion of cycle-length for non-iterative generators. We then
introduce the class of counter assisted generators, and show how to turn any
iterative generator (even a bad one designed or seeded by an adversary) into a
counter assisted generator with a provably high diversity, without reducing the
quality of generators which are already cryptographically strong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112015</id><created>2001-12-12</created><authors><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Rational Competitive Analysis</title><categories>cs.AI</categories><acm-class>I.2.11</acm-class><journal-ref>Proceedings of IJCAI-2001</journal-ref><abstract>  Much work in computer science has adopted competitive analysis as a tool for
decision making under uncertainty. In this work we extend competitive analysis
to the context of multi-agent systems. Unlike classical competitive analysis
where the behavior of an agent's environment is taken to be arbitrary, we
consider the case where an agent's environment consists of other agents. These
agents will usually obey some (minimal) rationality constraints. This leads to
the definition of rational competitive analysis. We introduce the concept of
rational competitive analysis, and initiate the study of competitive analysis
for multi-agent systems. We also discuss the application of rational
competitive analysis to the context of bidding games, as well as to the
classical one-way trading problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112016</id><created>2001-12-13</created><updated>2003-04-14</updated><authors><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>Pseudorandom permutations with the fast forward property</title><categories>cs.CR cs.CC</categories><comments>Paper withdrawal</comments><acm-class>G.3; G.2.1; D.4.6</acm-class><abstract>  This paper has been withdrawn by the author(s), due to the existence of a
much better paper in http://arxiv.org/abs/cs.CR/0207027
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112017</id><created>2001-12-14</created><authors><author><keyname>Dushay</keyname><forenames>Naomi</forenames></author></authors><title>Using Structural Metadata to Localize Experience of Digital Content</title><categories>cs.DL</categories><comments>23 pages including 2 appendices, 8 figures</comments><acm-class>H.3.7</acm-class><abstract>  With the increasing technical sophistication of both information consumers
and providers, there is increasing demand for more meaningful experiences of
digital information. We present a framework that separates digital object
experience, or rendering, from digital object storage and manipulation, so the
rendering can be tailored to particular communities of users. Our framework
also accommodates extensible digital object behaviors and interoperability. The
two key components of our approach are 1) exposing structural metadata
associated with digital objects -- metadata about the labeled access points
within a digital object and 2) information intermediaries called context
brokers that match structural characteristics of digital objects with
mechanisms that produce behaviors. These context brokers allow for localized
rendering of digital information stored externally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112018</id><created>2001-12-15</created><authors><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Fast Context-Free Grammar Parsing Requires Fast Boolean Matrix
  Multiplication</title><categories>cs.CL cs.DS</categories><comments>To appear in Journal of the ACM</comments><acm-class>I.2.7; F.2.2</acm-class><journal-ref>Journal of the ACM 49(1), pp. 1--15, January 2002</journal-ref><abstract>  In 1975, Valiant showed that Boolean matrix multiplication can be used for
parsing context-free grammars (CFGs), yielding the asympotically fastest
(although not practical) CFG parsing algorithm known. We prove a dual result:
any CFG parser with time complexity $O(g n^{3 - \epsilson})$, where $g$ is the
size of the grammar and $n$ is the length of the input string, can be
efficiently converted into an algorithm to multiply $m \times m$ Boolean
matrices in time $O(m^{3 - \epsilon/3})$.
  Given that practical, substantially sub-cubic Boolean matrix multiplication
algorithms have been quite difficult to find, we thus explain why there has
been little progress in developing practical, substantially sub-cubic general
CFG parsers. In proving this result, we also develop a formalization of the
notion of parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112019</identifier>
 <datestamp>2007-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112019</id><created>2001-12-15</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Distribution of Mutual Information</title><categories>cs.AI cs.IT math.IT math.ST stat.TH</categories><comments>8 pages</comments><report-no>IDSIA-13-01</report-no><acm-class>G.3; G.1.2</acm-class><journal-ref>Advances in Neural Information Processing Systems 14 (NIPS-2001)
  pages 399-406</journal-ref><abstract>  The mutual information of two random variables i and j with joint
probabilities t_ij is commonly used in learning Bayesian nets as well as in
many other fields. The chances t_ij are usually estimated by the empirical
sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual
information. To answer questions like &quot;is I(n_ij/n) consistent with zero?&quot; or
&quot;what is the probability that the true mutual information is much larger than
the point estimate?&quot; one has to go beyond the point estimate. In the Bayesian
framework one can answer these questions by utilizing a (second order) prior
distribution p(t) comprising prior information about t. From the prior p(t) one
can compute the posterior p(t|n), from which the distribution p(I|n) of the
mutual information can be calculated. We derive reliable and quickly computable
approximations for p(I|n). We concentrate on the mean, variance, skewness, and
kurtosis, and non-informative priors. For the mean we also give an exact
expression. Numerical issues and the range of validity are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112020</id><created>2001-12-17</created><authors><author><keyname>Matherat</keyname><forenames>Philippe</forenames></author><author><keyname>Jaekel</keyname><forenames>Marc-Thierry</forenames></author></authors><title>Concurrent computing machines and physical space-time</title><categories>cs.DC cs.LO gr-qc</categories><comments>Latex Revtex style, 15 pages, 14 figures</comments><report-no>LPTENS 01/05</report-no><acm-class>F.1.2;B.6.1;D.3.3</acm-class><journal-ref>M.S.C.S., Cambridge University Press, 13 (2003) 771</journal-ref><abstract>  Concrete computing machines, either sequential or concurrent, rely on an
intimate relation between computation and time. We recall the general
characteristic properties of physical time and of present realizations of
computing systems. We emphasize the role of computing interferences, i.e. the
necessity to avoid them in order to give a causal implementation to logical
operations. We compare synchronous and asynchronous systems, and make a brief
survey of some methods used to deal with computing interferences. Using a
graphic representation, we show that synchronous and asynchronous circuits
reflect the same opposition as the Newtonian and relativistic causal structures
for physical space-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112021</id><created>2001-12-20</created><authors><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Spakowski</keyname><forenames>Holger</forenames></author><author><keyname>Vogel</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Exact Complexity of the Winner Problem for Young Elections</title><categories>cs.CC</categories><comments>10 pages</comments><acm-class>F.1.3; F.2.2; J.4</acm-class><abstract>  In 1977, Young proposed a voting scheme that extends the Condorcet Principle
based on the fewest possible number of voters whose removal yields a Condorcet
winner. We prove that both the winner and the ranking problem for Young
elections is complete for the class of problems solvable in polynomial time by
parallel access to NP. Analogous results for Lewis Carroll's 1876 voting scheme
were recently established by Hemaspaandra et al. In contrast, we prove that the
winner and ranking problems in Fishburn's homogeneous variant of Carroll's
voting scheme can be solved efficiently by linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112022</id><created>2001-12-21</created><updated>2001-12-25</updated><authors><author><keyname>Yang</keyname><forenames>Qi Xiao</forenames></author><author><keyname>Yuan</keyname><forenames>Sung Sam</forenames></author><author><keyname>Chun</keyname><forenames>Lu</forenames></author><author><keyname>Zhao</keyname><forenames>Li</forenames></author><author><keyname>Peng</keyname><forenames>Sun</forenames></author></authors><title>Faster Algorithm of String Comparison</title><categories>cs.DS</categories><acm-class>I.1.2</acm-class><abstract>  In many applications, it is necessary to determine the string similarity.
Edit distance[WF74] approach is a classic method to determine Field Similarity.
A well known dynamic programming algorithm [GUS97] is used to calculate edit
distance with the time complexity O(nm). (for worst case, average case and even
best case) Instead of continuing with improving the edit distance approach,
[LL+99] adopted a brand new approach-token-based approach. Its new concept of
token-base-retain the original semantic information, good time complex-O(nm)
(for worst, average and best case) and good experimental performance make it a
milestone paper in this area. Further study indicates that there is still room
for improvement of its Field Similarity algorithm. Our paper is to introduce a
package of substring-based new algorithms to determine Field Similarity.
Combined together, our new algorithms not only achieve higher accuracy but also
gain the time complexity O(knm) (k&lt;0.75) for worst case, O(*n) where &lt;6 for
average case and O(1) for best case. Throughout the paper, we use the approach
of comparative examples to show higher accuracy of our algorithms compared to
the one proposed in [LL+99]. Theoretical analysis, concrete examples and
experimental result show that our algorithms can significantly improve the
accuracy and time complexity of the calculation of Field Similarity. [US97] D.
Guseld. Algorithms on Strings, Trees and Sequences, in Computer Science and
Computational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and
warehousing, In Proceedings of the 10th International Conference on Database
and Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R.
Wagner and M. Fisher, The String to String Correction Problem, JACM 21 pages
168-173, 1974.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112023</id><created>2001-12-21</created><authors><author><keyname>Wocjan</keyname><forenames>Pawel</forenames><affiliation>Universitaet Karlsruhe</affiliation></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames><affiliation>Universitaet Karlsruhe</affiliation></author><author><keyname>Beth</keyname><forenames>Thomas</forenames><affiliation>Universitaet Karlsruhe</affiliation></author></authors><title>Lower Bound on the Chromatic Number by Spectra of Weighted Adjacency
  Matrices</title><categories>cs.DM quant-ph</categories><comments>6 pages</comments><acm-class>G.2.2</acm-class><abstract>  A lower bound on the chromatic number of a graph is derived by majorization
of spectra of weighted adjacency matrices. These matrices are given by Hadamard
products of the adjacency matrix and arbitrary Hermitian matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0112024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0112024</id><created>2001-12-28</created><authors><author><keyname>Feustel</keyname><forenames>B.</forenames></author><author><keyname>Schmidt</keyname><forenames>T. C.</forenames></author></authors><title>Media Objects in Time - A Multimedia Streaming System</title><categories>cs.NI cs.MM</categories><comments>9 pdf pages</comments><acm-class>H.5.4; H.2.4; H.4.3; C.3</acm-class><journal-ref>Computer Networks 37,6 (2001), pp. 729 - 737</journal-ref><abstract>  The widespread availability of networked multimedia potentials embedded in an
infrastructure of qualitative superior kind gives rise to new approaches in the
areas of teleteaching and internet presentation: The distribution of
professionally styled multimedia streams has fallen in the realm of
possibility. This paper presents a prototype - both model and runtime
environment - of a time directed media system treating any kind of
presentational contribution as reusable media object components. The plug-in
free runtime system is based on a database and allows for a flexible support of
static media types as well as for easy extensions by streaming media servers.
The prototypic implementation includes a preliminary Web Authoring platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201001</id><created>2002-01-02</created><authors><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Lower Bounds for Matrix Product</title><categories>cs.CC</categories><comments>Published in the proceedings of the 42nd Annual Symposium on
  Foundations of Computer Science (FOCS) 2001</comments><acm-class>F.1.1; F.2.0</acm-class><journal-ref>Published in the proceedings of the 42nd Annual Symposium on
  Foundations of Computer Science (FOCS) 2001</journal-ref><abstract>  We prove lower bounds on the number of product gates in bilinear and
quadratic circuits that compute the product of two $n \cross n$ matrices over
finite fields. In particular we obtain the following results:
  1. We show that the number of product gates in any bilinear (or quadratic)
circuit that computes the product of two $n \cross n$ matrices over $F_2$ is at
least $3 n^2 - o(n^2)$.
  2. We show that the number of product gates in any bilinear circuit that
computes the product of two $n \cross n$ matrices over $F_p$ is at least $(2.5
+ \frac{1.5}{p^3 -1})n^2 -o(n^2)$.
  These results improve the former results of Bshouty '89 and Blaser '99 who
proved lower bounds of $2.5 n^2 - o(n^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201002</id><created>2002-01-04</created><authors><author><keyname>Sgarbas</keyname><forenames>Kyriakos N.</forenames></author><author><keyname>Fakotakis</keyname><forenames>Nikos D.</forenames></author><author><keyname>Kokkinakis</keyname><forenames>George K.</forenames></author></authors><title>Incremental Construction of Compact Acyclic NFAs</title><categories>cs.DS cs.CL</categories><comments>8(+2) pages, 4 figures, 1 table, 22 references. For related work, see
  also http://slt.wcl.ee.upatras.gr</comments><acm-class>E.1; F.2.2; H.3.1; I.2.7</acm-class><journal-ref>Proc. ACL-2001, 39th Annual Meeting of the Association for
  Computational Linguistics, pp.474-481, Toulouse, France, 6-11 July 2001</journal-ref><abstract>  This paper presents and analyzes an incremental algorithm for the
construction of Acyclic Non-deterministic Finite-state Automata (NFA). Automata
of this type are quite useful in computational linguistics, especially for
storing lexicons. The proposed algorithm produces compact NFAs, i.e. NFAs that
do not contain equivalent states. Unlike Deterministic Finite-state Automata
(DFA), this property is not sufficient to ensure minimality, but still the
resulting NFAs are considerably smaller than the minimal DFAs for the same
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201003</id><created>2002-01-06</created><authors><author><keyname>Bennett</keyname><forenames>Charles H.</forenames></author><author><keyname>Smolin</keyname><forenames>John A.</forenames></author></authors><title>Trust enhancement by multiple random beacons</title><categories>cs.CR</categories><acm-class>G.3;K.6.5</acm-class><abstract>  Random beacons-information sources that broadcast a stream of random digits
unknown by anyone beforehand-are useful for various cryptographic purposes. But
such beacons can be easily and undetectably sabotaged, so that their output is
known beforehand by a dishonest party, who can use this information to defeat
the cryptographic protocols supposedly protected by the beacon. We explore a
strategy to reduce this hazard by combining the outputs from several
noninteracting (eg spacelike-separated) beacons by XORing them together to
produce a single digit stream which is more trustworthy than any individual
beacon, being random and unpredictable if at least one of the contributing
beacons is honest. If the contributing beacons are not spacelike separated, so
that a dishonest beacon can overhear and adapt to earlier outputs of other
beacons, the beacons' trustworthiness can still be enhanced to a lesser extent
by a time sharing strategy. We point out some disadvantages of alternative
trust amplification methods based on one-way hash functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201004</id><created>2002-01-08</created><authors><author><keyname>Mori</keyname><forenames>Tatsuya</forenames></author><author><keyname>Kawahara</keyname><forenames>Ryoichi</forenames></author><author><keyname>Naito</keyname><forenames>Shozo</forenames></author></authors><title>Analysis of Non-Gaussian Nature of Network Traffic</title><categories>cs.NI</categories><comments>4 pages, 4 figures, IEICE General Conference 2002, Tokyo, Japan,
  March 2002</comments><acm-class>C.2.5</acm-class><abstract>  To study mechanisms that cause the non-Gaussian nature of network traffic, we
analyzed IP flow statistics. For greedy flows in particular, we investigated
the hop counts between source and destination nodes, and classified
applications by the port number. We found that the main flows contributing to
the non-Gaussian nature of network traffic were HTTP flows with relatively
small hop counts compared with the average hop counts of all flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201005</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201005</id><created>2002-01-08</created><updated>2002-10-10</updated><authors><author><keyname>Li</keyname><forenames>Ming</forenames><affiliation>Univ. Waterloo</affiliation></author><author><keyname>Tromp</keyname><forenames>John</forenames><affiliation>CWI</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Sharpening Occam's Razor</title><categories>cs.LG cond-mat.dis-nn cs.AI cs.CC math.PR physics.data-an</categories><comments>LaTeX 13 pages; Proc 8th COCOON, LNCS 2387, Springer-Verlag, Berlin,
  2002, 411--419</comments><report-no>CWI Manuscript 1994</report-no><acm-class>F.2, E.4, I.2</acm-class><abstract>  We provide a new representation-independent formulation of Occam's razor
theorem, based on Kolmogorov complexity. This new formulation allows us to:
  (i) Obtain better sample complexity than both length-based and VC-based
versions of Occam's razor theorem, in many applications.
  (ii) Achieve a sharper reverse of Occam's razor theorem than previous work.
  Specifically, we weaken the assumptions made in an earlier publication, and
extend the reverse to superpolynomial running times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201006</id><created>2002-01-09</created><updated>2002-02-04</updated><authors><author><keyname>Buhrman</keyname><forenames>Harry</forenames><affiliation>CWI</affiliation></author><author><keyname>Panconesi</keyname><forenames>Alessandro</forenames><affiliation>Univ. La Sapienza, Rome</affiliation></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames><affiliation>Univ. La Sapienza, Rome</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and Univ. Amsterdam</affiliation></author></authors><title>On the Importance of Having an Identity or, is Consensus really
  Universal?</title><categories>cs.DC cs.CC</categories><comments>LaTeX, 15 pages, In: Distributed Computing Conference (DISC'00),
  Lecture Notes in Computer Science, Vol. 1914, Springer-Verlag, Berlin, 2000,
  134--148</comments><acm-class>F.1.2; C.2.4; B.3.2;B.4.3;D.1.3;D.4.1;D.4.4</acm-class><abstract>  We show that Naming-- the existence of distinct IDs known to all-- is a
hidden but necessary assumption of Herlihy's universality result for Consensus.
We then show in a very precise sense that Naming is harder than Consensus and
bring to the surface some important differences existing between popular shared
memory models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201007</identifier>
 <datestamp>2009-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201007</id><created>2002-01-10</created><updated>2009-10-14</updated><authors><author><keyname>Sharipov</keyname><forenames>Ruslan</forenames></author></authors><title>Algorithm for generating orthogonal matrices with rational elements</title><categories>cs.MS cs.DS</categories><comments>AmSTeX, 7 pages, amsppt style, English wording is improved,
  references are transformed to hyperlinks, the fugure is incorporated into the
  PS and PDF files</comments><acm-class>G.4; K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Special orthogonal matrices with rational elements form the group SO(n,Q),
where Q is the field of rational numbers. A theorem describing the structure of
an arbitrary matrix from this group is proved. This theorem yields an algorithm
for generating such matrices by means of random number routines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201008</id><created>2002-01-11</created><authors><author><keyname>Schmidt</keyname><forenames>Nikita</forenames></author><author><keyname>Patel</keyname><forenames>Ahmed</forenames></author></authors><title>Using Tree Automata and Regular Expressions to Manipulate Hierarchically
  Structured Data</title><categories>cs.CL cs.DS</categories><comments>Submission to JACM for review. 27 single spaced pages</comments><acm-class>E.1; F.4.3; I.7.2; H.3.3</acm-class><abstract>  Information, stored or transmitted in digital form, is often structured.
Individual data records are usually represented as hierarchies of their
elements. Together, records form larger structures. Information processing
applications have to take account of this structuring, which assigns different
semantics to different data elements or records. Big variety of structural
schemata in use today often requires much flexibility from applications--for
example, to process information coming from different sources. To ensure
application interoperability, translators are needed that can convert one
structure into another. This paper puts forward a formal data model aimed at
supporting hierarchical data processing in a simple and flexible way. The model
is based on and extends results of two classical theories, studying finite
string and tree automata. The concept of finite automata and regular languages
is applied to the case of arbitrarily structured tree-like hierarchical data
records, represented as &quot;structured strings.&quot; These automata are compared with
classical string and tree automata; the model is shown to be a superset of the
classical models. Regular grammars and expressions over structured strings are
introduced. Regular expression matching and substitution has been widely used
for efficient unstructured text processing; the model described here brings the
power of this proven technique to applications that deal with information
trees. A simple generic alternative is offered to replace today's specialised
ad-hoc approaches. The model unifies structural and content transformations,
providing applications with a single data type. An example scenario of how to
build applications based on this theory is discussed. Further research
directions are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201009</id><created>2002-01-14</created><authors><author><keyname>Rivin</keyname><forenames>Igor</forenames></author></authors><title>The performance of the batch learner algorithm</title><categories>cs.LG cs.DM</categories><comments>Supercedes a part of cs.LG/0107033</comments><acm-class>I2.6</acm-class><abstract>  We analyze completely the convergence speed of the \emph{batch learning
algorithm}, and compare its speed to that of the memoryless learning algorithm
and of learning with memory. We show that the batch learning algorithm is never
worse than the memoryless learning algorithm (at least asymptotically). Its
performance \emph{vis-a-vis} learning with full memory is less clearcut, and
depends on certain probabilistic assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201010</id><created>2002-01-14</created><authors><author><keyname>Holzman</keyname><forenames>Ron</forenames></author><author><keyname>Kfir-Dahav</keyname><forenames>Noa</forenames></author><author><keyname>Monderer</keyname><forenames>Dov</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Bundling Equilibrium in Combinatorial auctions</title><categories>cs.GT</categories><acm-class>F.2.3; I.2.11</acm-class><abstract>  This paper analyzes individually-rational ex post equilibrium in the VC
(Vickrey-Clarke) combinatorial auctions. If $\Sigma$ is a family of bundles of
goods, the organizer may restrict the participants by requiring them to submit
their bids only for bundles in $\Sigma$. The $\Sigma$-VC combinatorial auctions
(multi-good auctions) obtained in this way are known to be
individually-rational truth-telling mechanisms. In contrast, this paper deals
with non-restricted VC auctions, in which the buyers restrict themselves to
bids on bundles in $\Sigma$, because it is rational for them to do so. That is,
it may be that when the buyers report their valuation of the bundles in
$\Sigma$, they are in an equilibrium. We fully characterize those $\Sigma$ that
induce individually rational equilibrium in every VC auction, and we refer to
the associated equilibrium as a bundling equilibrium. The number of bundles in
$\Sigma$ represents the communication complexity of the equilibrium. A special
case of bundling equilibrium is partition-based equilibrium, in which $\Sigma$
is a field, that is, it is generated by a partition. We analyze the tradeoff
between communication complexity and economic efficiency of bundling
equilibrium, focusing in particular on partition-based equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201011</id><created>2002-01-16</created><authors><author><keyname>King</keyname><forenames>Andy</forenames></author><author><keyname>Lu</keyname><forenames>Lunjin</forenames></author></authors><title>A Backward Analysis for Constraint Logic Programs</title><categories>cs.PL cs.SE</categories><comments>32 pages</comments><acm-class>D.1.6; F.3.2</acm-class><abstract>  One recurring problem in program development is that of understanding how to
re-use code developed by a third party. In the context of (constraint) logic
programming, part of this problem reduces to figuring out how to query a
program. If the logic program does not come with any documentation, then the
programmer is forced to either experiment with queries in an ad hoc fashion or
trace the control-flow of the program (backward) to infer the modes in which a
predicate must be called so as to avoid an instantiation error. This paper
presents an abstract interpretation scheme that automates the latter technique.
The analysis presented in this paper can infer moding properties which if
satisfied by the initial query, come with the guarantee that the program and
query can never generate any moding or instantiation errors. Other applications
of the analysis are discussed. The paper explains how abstract domains with
certain computational properties (they condense) can be used to trace
control-flow backward (right-to-left) to infer useful properties of initial
queries. A correctness argument is presented and an implementation is reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201012</id><created>2002-01-16</created><authors><author><keyname>Howe</keyname><forenames>Jacob M.</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author></authors><title>Efficient Groundness Analysis in Prolog</title><categories>cs.PL</categories><comments>31 pages To appear in Theory and Practice of Logic Programming</comments><acm-class>D.1.6; F.3.2</acm-class><abstract>  Boolean functions can be used to express the groundness of, and trace
grounding dependencies between, program variables in (constraint) logic
programs. In this paper, a variety of issues pertaining to the efficient Prolog
implementation of groundness analysis are investigated, focusing on the domain
of definite Boolean functions, Def. The systematic design of the representation
of an abstract domain is discussed in relation to its impact on the algorithmic
complexity of the domain operations; the most frequently called operations
should be the most lightweight. This methodology is applied to Def, resulting
in a new representation, together with new algorithms for its domain operations
utilising previously unexploited properties of Def -- for instance,
quadratic-time entailment checking. The iteration strategy driving the analysis
is also discussed and a simple, but very effective, optimisation of induced
magic is described. The analysis can be implemented straightforwardly in Prolog
and the use of a non-ground representation results in an efficient, scalable
tool which does not require widening to be invoked, even on the largest
benchmarks. An extensive experimental evaluation is given
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201013</identifier>
 <datestamp>2008-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201013</id><created>2002-01-16</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Pfeifer</keyname><forenames>Gerald</forenames></author></authors><title>Computing Preferred Answer Sets by Meta-Interpretation in Answer Set
  Programming</title><categories>cs.LO cs.AI</categories><comments>34 pages, appeared as a Technical Report at KBS of the Vienna
  University of Technology, see http://www.kr.tuwien.ac.at/research/reports/</comments><acm-class>D.1.6; I.2.3; I.2.4</acm-class><journal-ref>Theory and Practice of Logic Programming 3(4/5):463-498, 2003</journal-ref><doi>10.1017/S1471068403001753</doi><abstract>  Most recently, Answer Set Programming (ASP) is attracting interest as a new
paradigm for problem solving. An important aspect which needs to be supported
is the handling of preferences between rules, for which several approaches have
been presented. In this paper, we consider the problem of implementing
preference handling approaches by means of meta-interpreters in Answer Set
Programming. In particular, we consider the preferred answer set approaches by
Brewka and Eiter, by Delgrande, Schaub and Tompits, and by Wang, Zhou and Lin.
We present suitable meta-interpreters for these semantics using DLV, which is
an efficient engine for ASP. Moreover, we also present a meta-interpreter for
the weakly preferred answer set approach by Brewka and Eiter, which uses the
weak constraint feature of DLV as a tool for expressing and solving an
underlying optimization problem. We also consider advanced meta-interpreters,
which make use of graph-based characterizations and often allow for more
efficient computations. Our approach shows the suitability of ASP in general
and of DLV in particular for fast prototyping. This can be fruitfully exploited
for experimenting with new languages and knowledge-representation formalisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201014</id><created>2002-01-17</created><authors><author><keyname>Caprile</keyname><forenames>Bruno</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author><author><keyname>Merler</keyname><forenames>Stefano</forenames></author></authors><title>The Dynamics of AdaBoost Weights Tells You What's Hard to Classify</title><categories>cs.LG cs.DS</categories><comments>7 pages, LaTeX, 4 figures</comments><report-no>IRST TechRep #010612</report-no><acm-class>I.5.1</acm-class><abstract>  The dynamical evolution of weights in the Adaboost algorithm contains useful
information about the role that the associated data points play in the built of
the Adaboost model. In particular, the dynamics induces a bipartition of the
data set into two (easy/hard) classes. Easy points are ininfluential in the
making of the model, while the varying relevance of hard points can be gauged
in terms of an entropy value associated to their evolution. Smooth
approximations of entropy highlight regions where classification is most
uncertain. Promising results are obtained when methods proposed are applied in
the Optimal Sampling framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201015</id><created>2002-01-17</created><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>On the Significance of Digits in Interval Notation</title><categories>cs.NA</categories><comments>11 pages, LaTeX</comments><report-no>DCS-270-IR</report-no><acm-class>B.4.m; G.1.0; G.4</acm-class><abstract>  To analyse the significance of the digits used for interval bounds, we
clarify the philosophical presuppositions of various interval notations. We use
information theory to determine the information content of the last digit of
the numeral used to denote the interval's bounds. This leads to the notion of
efficiency of a decimal digit: the actual value as percentage of the maximal
value of its information content. By taking this efficiency into account, many
presentations of intervals can be made more readable at the expense of
negligible loss of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201016</id><created>2002-01-18</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A computer scientist looks at game theory</title><categories>cs.GT cs.DC cs.MA</categories><comments>To appear, Games and Economic Behavior. JEL classification numbers:
  D80, D83</comments><acm-class>I.2.11, C.2.4</acm-class><abstract>  I consider issues in distributed computation that should be of relevance to
game theory. In particular, I focus on (a) representing knowledge and
uncertainty, (b) dealing with failures, and (c) specification of mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201017</identifier>
 <datestamp>2012-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201017</id><created>2002-01-18</created><updated>2012-08-22</updated><authors><author><keyname>Leyton-Brown</keyname><forenames>Kevin</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author><author><keyname>Bhat</keyname><forenames>Navin</forenames></author><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author></authors><title>Collusion in Unrepeated, First-Price Auctions with an Uncertain Number
  of Participants</title><categories>cs.GT cs.AI</categories><comments>Updated the version originally submitted to Arxiv in 2002</comments><acm-class>I.2.11; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of whether collusion among bidders (a &quot;bidding
ring&quot;) can be supported in equilibrium of unrepeated first-price auctions.
Unlike previous work on the topic such as that by McAfee and McMillan [1992]
and Marshall and Marx [2007], we do not assume that non-colluding agents have
perfect knowledge about the number of colluding agents whose bids are
suppressed by the bidding ring, and indeed even allow for the existence of
multiple cartels. Furthermore, while we treat the association of bidders with
bidding rings as exogenous, we allow bidders to make strategic decisions about
whether to join bidding rings when invited. We identify a bidding ring protocol
that results in an efficient allocation in Bayes{Nash equilibrium, under which
non-colluding agents bid straightforwardly, and colluding agents join bidding
rings when invited and truthfully declare their valuations to the ring center.
We show that bidding rings benefit ring centers and all agents, both members
and non-members of bidding rings, at the auctioneer's expense. The techniques
we introduce in this paper may also be useful for reasoning about other
problems in which agents have asymmetric information about a setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201018</id><created>2002-01-21</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Bremner</keyname><forenames>David</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Meijer</keyname><forenames>Henk</forenames></author><author><keyname>Sacrist&#xe1;n</keyname><forenames>Vera</forenames></author><author><keyname>Soss</keyname><forenames>Michael</forenames></author></authors><title>Long Proteins with Unique Optimal Foldings in the H-P Model</title><categories>cs.CG q-bio.BM</categories><comments>22 pages, 18 figures</comments><acm-class>G.2; I.3.5</acm-class><abstract>  It is widely accepted that (1) the natural or folded state of proteins is a
global energy minimum, and (2) in most cases proteins fold to a unique state
determined by their amino acid sequence. The H-P (hydrophobic-hydrophilic)
model is a simple combinatorial model designed to answer qualitative questions
about the protein folding process. In this paper we consider a problem
suggested by Brian Hayes in 1998: what proteins in the two-dimensional H-P
model have unique optimal (minimum energy) foldings? In particular, we prove
that there are closed chains of monomers (amino acids) with this property for
all (even) lengths; and that there are open monomer chains with this property
for all lengths divisible by four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201019</id><created>2002-01-22</created><authors><author><keyname>Bazin</keyname><forenames>Pierre-Louis</forenames><affiliation>Brown University</affiliation></author><author><keyname>Boutin</keyname><forenames>Mireille</forenames><affiliation>Brown University</affiliation></author></authors><title>Structure from Motion: Theoretical Foundations of a Novel Approach Using
  Custom Built Invariants</title><categories>cs.CV math.DG</categories><comments>15 pages</comments><acm-class>I.4.8;I.2.10</acm-class><abstract>  We rephrase the problem of 3D reconstruction from images in terms of
intersections of projections of orbits of custom built Lie groups actions. We
then use an algorithmic method based on moving frames &quot;a la Fels-Olver&quot; to
obtain a fundamental set of invariants of these groups actions. The invariants
are used to define a set of equations to be solved by the points of the 3D
object, providing a new technique for recovering 3D structure from motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201020</id><created>2002-01-22</created><authors><author><keyname>Liau</keyname><forenames>Churn-Jung</forenames></author></authors><title>A Modal Logic Framework for Multi-agent Belief Fusion</title><categories>cs.AI cs.LO</categories><comments>LaTex, 29 pages</comments><acm-class>I.2.4;F.4.1</acm-class><abstract>  This paper is aimed at providing a uniform framework for reasoning about
beliefs of multiple agents and their fusion. In the first part of the paper, we
develop logics for reasoning about cautiously merged beliefs of agents with
different degrees of reliability. The logics are obtained by combining the
multi-agent epistemic logic and multi-sources reasoning systems. Every ordering
for the reliability of the agents is represented by a modal operator, so we can
reason with the merged results under different situations. The fusion is
cautious in the sense that if an agent's belief is in conflict with those of
higher priorities, then his belief is completely discarded from the merged
result. We consider two strategies for the cautious merging of beliefs. In the
first one, if inconsistency occurs at some level, then all beliefs at the lower
levels are discarded simultaneously, so it is called level cutting strategy.
For the second one, only the level at which the inconsistency occurs is
skipped, so it is called level skipping strategy. The formal semantics and
axiomatic systems for these two strategies are presented. In the second part,
we extend the logics both syntactically and semantically to cover some more
sophisticated belief fusion and revision operators. While most existing
approaches treat belief fusion operators as meta-level constructs, these
operators are directly incorporated into our object logic language. Thus it is
possible to reason not only with the merged results but also about the fusion
process in our logics. The relationship of our extended logics with the
conditional logics of belief revision is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201021</id><created>2002-01-23</created><authors><author><keyname>Jehiel</keyname><forenames>Philippe</forenames></author><author><keyname>Samet</keyname><forenames>Dov</forenames></author></authors><title>Learning to Play Games in Extensive Form by Valuation</title><categories>cs.LG cs.GT</categories><acm-class>I.2.1; I.2.6; I.2.8</acm-class><abstract>  A valuation for a player in a game in extensive form is an assignment of
numeric values to the players moves. The valuation reflects the desirability
moves. We assume a myopic player, who chooses a move with the highest
valuation. Valuations can also be revised, and hopefully improved, after each
play of the game. Here, a very simple valuation revision is considered, in
which the moves made in a play are assigned the payoff obtained in the play. We
show that by adopting such a learning process a player who has a winning
strategy in a win-lose game can almost surely guarantee a win in a repeated
game. When a player has more than two payoffs, a more elaborate learning
procedure is required. We consider one that associates with each move the
average payoff in the rounds in which this move was made. When all players
adopt this learning procedure, with some perturbations, then, with probability
1, strategies that are close to subgame perfect equilibrium are played after
some time. A single player who adopts this procedure can guarantee only her
individually rational payoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201022</id><created>2002-01-23</created><updated>2003-02-23</updated><authors><author><keyname>Albarede</keyname><forenames>Pierre</forenames></author></authors><title>A theory of experiment</title><categories>cs.AI</categories><comments>19 pages LaTeX article (uses some pstricks) thorough revision 2 see
  also http://pierre.albarede.free.fr</comments><acm-class>I.2.4;I.2.6</acm-class><abstract>  This article aims at clarifying the language and practice of scientific
experiment, mainly by hooking observability on calculability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201023</id><created>2002-01-26</created><authors><author><keyname>Blotz</keyname><forenames>Andree</forenames><affiliation>EADS Deutschland GmbH</affiliation></author><author><keyname>Huber</keyname><forenames>Franz</forenames><affiliation>Validas AG</affiliation></author><author><keyname>Loetzbeyer</keyname><forenames>Heiko</forenames><affiliation>TU Munich</affiliation></author><author><keyname>Pretschner</keyname><forenames>Alexander</forenames><affiliation>TU Munich</affiliation></author><author><keyname>Slotosch</keyname><forenames>Oscar</forenames><affiliation>Validas AG</affiliation></author><author><keyname>Zaengerl</keyname><forenames>Hans-Peter</forenames><affiliation>Validas AG</affiliation></author></authors><title>Model-Based Software Engineering and Ada: Synergy for the Development of
  Safety-Critical Systems</title><categories>cs.SE</categories><comments>16 pages, figures included, paper accepted for ADA Deutschland Tagung
  2002, March 6-8, Jena, GERMANY</comments><acm-class>D.2, K.6.3</acm-class><abstract>  In this paper we outline a software development process for safety-critical
systems that aims at combining some of the specific strengths of model-based
development with those of programming language based development using
safety-critical subsets of Ada. Model-based software development and
model-based test case generation techniques are combined with code generation
techniques and tools providing a transition from model to code both for a
system itself and for its test cases. This allows developers to combine
domain-oriented, model-based techniques with source code based validation
techniques, as required for conformity with standards for the development of
safety-critical software, such as the avionics standard RTCA/DO-178B. We
introduce the AutoFocus and Validator modeling and validation toolset and
sketch its usage for modeling, test case generation, and code generation in a
combined approach, which is further illustrated by a simplified leading edge
aerospace model with built-in fault tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201024</id><created>2002-01-27</created><authors><author><keyname>Hatjimihail</keyname><forenames>Aristides T.</forenames><affiliation>Hellenic Complex Systems Laboratory, Drama, Greece</affiliation></author><author><keyname>Hatjimihail</keyname><forenames>Theophanes T.</forenames><affiliation>Hellenic Complex Systems Laboratory, Drama, Greece</affiliation></author></authors><title>Design of statistical quality control procedures using genetic
  algorithms</title><categories>cs.NE</categories><comments>11 pages, 1 figure</comments><report-no>HCSLTR02</report-no><acm-class>G.1.6</acm-class><journal-ref>LJ Eshelman (ed): Proceedings of the Sixth International
  Conference on Genetic Algorithms. San Francisco: Morgan Kauffman, 1995:551-7</journal-ref><abstract>  In general, we can not use algebraic or enumerative methods to optimize a
quality control (QC) procedure so as to detect the critical random and
systematic analytical errors with stated probabilities, while the probability
for false rejection is minimum. Genetic algorithms (GAs) offer an alternative,
as they do not require knowledge of the objective function to be optimized and
search through large parameter spaces quickly. To explore the application of
GAs in statistical QC, we have developed an interactive GAs based computer
program that designs a novel near optimal QC procedure, given an analytical
process. The program uses the deterministic crowding algorithm. An illustrative
application of the program suggests that it has the potential to design QC
procedures that are significantly better than 45 alternative ones that are used
in the clinical laboratories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201025</id><created>2002-01-29</created><authors><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author><author><keyname>Arms</keyname><forenames>William</forenames></author><author><keyname>Gan</keyname><forenames>Stoney</forenames></author><author><keyname>Hillmann</keyname><forenames>Diane</forenames></author><author><keyname>Ingram</keyname><forenames>Christopher</forenames></author><author><keyname>Krafft</keyname><forenames>Dean</forenames></author><author><keyname>Marisa</keyname><forenames>Richard</forenames></author><author><keyname>Phipps</keyname><forenames>Jon</forenames></author><author><keyname>Saylor</keyname><forenames>John</forenames></author><author><keyname>Terrizzi</keyname><forenames>Carol</forenames></author><author><keyname>Hoehn</keyname><forenames>Walter</forenames></author><author><keyname>Millman</keyname><forenames>David</forenames></author><author><keyname>Allan</keyname><forenames>James</forenames></author><author><keyname>Guzman-Lara</keyname><forenames>Sergio</forenames></author><author><keyname>Kalt</keyname><forenames>Tom</forenames></author></authors><title>Core Services in the Architecture of the National Digital Library for
  Science Education (NSDL)</title><categories>cs.DL</categories><acm-class>D.2.12</acm-class><abstract>  We describe the core components of the architecture for the (NSDL) National
Science, Mathematics, Engineering, and Technology Education Digital Library.
Over time the NSDL will include heterogeneous users, content, and services. To
accommodate this, a design for a technical and organization infrastructure has
been formulated based on the notion of a spectrum of interoperability. This
paper describes the first phase of the interoperability infrastructure
including the metadata repository, search and discovery services, rights
management services, and user interface portal facilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201026</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201026</id><created>2002-01-29</created><authors><author><keyname>McCauley</keyname><forenames>Joseph L.</forenames></author><author><keyname>Gunaratne</keyname><forenames>Gemunu H.</forenames></author></authors><title>An Empirical Model for Volatility of Returns and Option Pricing</title><categories>cs.CE</categories><comments>4 figures; 1 table</comments><acm-class>G.3; J.1</acm-class><doi>10.1016/S0378-4371(03)00589-2</doi><abstract>  In a seminal paper in 1973, Black and Scholes argued how expected
distributions of stock prices can be used to price options. Their model assumed
a directed random motion for the returns and consequently a lognormal
distribution of asset prices after a finite time. We point out two problems
with their formulation. First, we show that the option valuation is not
uniquely determined; in particular, stratergies based on the delta-hedge and
CAMP (Capital Asset Pricing Model) are shown to provide different valuations of
an option. Second, asset returns are known not to be Gaussian distributed.
Empirically, distributions of returns are seen to be much better approximated
by an exponential distribution. This exponential distribution of asset prices
can be used to develop a new pricing model for options that is shown to provide
valuations that agree very well with those used by traders. We show how the
Fokker-Planck formulation of fluctuations (i.e., the dynamics of the
distribution) can be modified to provide an exponential distribution for
returns. We also show how a singular volatility can be used to go smoothly from
exponential to Gaussian returns and thereby illustrate why exponential returns
cannot be reached perturbatively starting from Gaussian ones, and explain how
the theory of 'stochastic volatility' can be obtained from our model by making
a bad approximation. Finally, we show how to calculate put and call prices for
a stretched exponential density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201027</id><created>2002-01-29</created><authors><author><keyname>Fulker</keyname><forenames>David</forenames></author><author><keyname>Janee</keyname><forenames>Greg</forenames></author></authors><title>Components of an NSDL Architecture: Technical Scope and Functional Model</title><categories>cs.DL</categories><acm-class>H.3.7</acm-class><abstract>  We describe work leading toward specification of a technical architecture for
the National Science, Mathematics, Engineering, and Technology Education
Digital Library (NSDL). This includes a technical scope and a functional model,
with some elaboration on the particularly rich set of library services that
NSDL is expected eventually to encompass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201028</id><created>2002-01-30</created><authors><author><keyname>Lencevicius</keyname><forenames>Raimondas</forenames></author><author><keyname>Metz</keyname><forenames>Edu</forenames></author><author><keyname>Ran</keyname><forenames>Alexander</forenames></author></authors><title>Software Validation using Power Profiles</title><categories>cs.SE</categories><comments>6 pages, 5 figures, to be published in the Proceedings of the
  Symposium on Software Engineering at 20th IASTED International
  Multi-Conference on Applied Informatics (AI 2002)</comments><acm-class>D.2.4</acm-class><abstract>  The validation of modern software systems incorporates both functional and
quality requirements. This paper proposes a validation approach for software
quality requirement - its power consumption. This approach validates whether
the software produces the desired results with a minimum expenditure of energy.
We present energy requirements and an approach for their validation using a
power consumption model, test-case specification, software traces, and power
measurements. Three different approaches for power data gathering are
described. The power consumption of mobile phone applications is obtained and
matched against the power consumption model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0201029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0201029</id><created>2002-01-31</created><authors><author><keyname>Andrews</keyname><forenames>James H.</forenames></author></authors><title>The Witness Properties and the Semantics of the Prolog Cut</title><categories>cs.PL</categories><comments>60 pages, 15 figures. Accepted for publication in Theory and Practice
  of Logic Programming</comments><acm-class>D.3.1; D.3.3</acm-class><abstract>  The semantics of the Prolog ``cut'' construct is explored in the context of
some desirable properties of logic programming systems, referred to as the
witness properties. The witness properties concern the operational consistency
of responses to queries. A generalization of Prolog with negation as failure
and cut is described, and shown not to have the witness properties. A
restriction of the system is then described, which preserves the choice and
first-solution behaviour of cut but allows the system to have the witness
properties.
  The notion of cut in the restricted system is more restricted than the Prolog
hard cut, but retains the useful first-solution behaviour of hard cut, not
retained by other proposed cuts such as the ``soft cut''. It is argued that the
restricted system achieves a good compromise between the power and utility of
the Prolog cut and the need for internal consistency in logic programming
systems. The restricted system is given an abstract semantics, which depends on
the witness properties; this semantics suggests that the restricted system has
a deeper connection to logic than simply permitting some computations which are
logical.
  Parts of this paper appeared previously in a different form in the
Proceedings of the 1995 International Logic Programming Symposium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202001</id><created>2002-02-01</created><authors><author><keyname>Arni</keyname><forenames>Faiz</forenames></author><author><keyname>Ong</keyname><forenames>KayLiang</forenames></author><author><keyname>Tsur</keyname><forenames>Shalom</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author><author><keyname>Zaniolo</keyname><forenames>Carlo</forenames></author></authors><title>The Deductive Database System LDL++</title><categories>cs.DB cs.AI</categories><acm-class>D.3.2</acm-class><abstract>  This paper describes the LDL++ system and the research advances that have
enabled its design and development. We begin by discussing the new nonmonotonic
and nondeterministic constructs that extend the functionality of the LDL++
language, while preserving its model-theoretic and fixpoint semantics. Then, we
describe the execution model and the open architecture designed to support
these new constructs and to facilitate the integration with existing DBMSs and
applications. Finally, we describe the lessons learned by using LDL++ on
various tested applications, such as middleware and datamining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202002</id><created>2002-02-03</created><authors><author><keyname>Hayes</keyname><forenames>Ian</forenames></author><author><keyname>Colvin</keyname><forenames>Robert</forenames></author><author><keyname>Hemer</keyname><forenames>David</forenames></author><author><keyname>Strooper</keyname><forenames>Paul</forenames></author><author><keyname>Nickson</keyname><forenames>Ray</forenames></author></authors><title>A Refinement Calculus for Logic Programs</title><categories>cs.SE cs.LO</categories><comments>36 pages, 3 figures. To be published in Theory and Practice of Logic
  Programming (TPLP)</comments><acm-class>F.3.1; D.1.6</acm-class><abstract>  Existing refinement calculi provide frameworks for the stepwise development
of imperative programs from specifications. This paper presents a refinement
calculus for deriving logic programs. The calculus contains a wide-spectrum
logic programming language, including executable constructs such as sequential
conjunction, disjunction, and existential quantification, as well as
specification constructs such as general predicates, assumptions and universal
quantification. A declarative semantics is defined for this wide-spectrum
language based on executions. Executions are partial functions from states to
states, where a state is represented as a set of bindings. The semantics is
used to define the meaning of programs and specifications, including parameters
and recursion. To complete the calculus, a notion of correctness-preserving
refinement over programs in the wide-spectrum language is defined and
refinement laws for developing programs are introduced. The refinement calculus
is illustrated using example derivations and prototype tool support is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202003</id><created>2002-02-04</created><updated>2005-12-15</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Simple Optimal Wait-free Multireader Registers</title><categories>cs.DC</categories><comments>11 pages LaTeX, 1 table, 2 pseudo-programs; previous version
  published in Proc 16th International Symposium on DIStributed Computing (DISC
  2002), Lecture Notes in Computer Science, Vol 2508, Springer-Verlag, Berlin,
  118-132. New version eliminates error in the protocol (merges a split scan
  operation that proved problematic) and defers the formal proof to a planned
  future I/O automaton version</comments><acm-class>F.1.2; C.2.4; B.3.2; B.4.3; D.1.3; D.4.1; D.4.4</acm-class><abstract>  Multireader shared registers are basic objects used as communication medium
in asynchronous concurrent computation. We propose a surprisingly simple and
natural scheme to obtain several wait-free constructions of bounded 1-writer
multireader registers from atomic 1-writer 1-reader registers, that is easier
to prove correct than any previous construction. Our main construction is the
first symmetric pure timestamp one that is optimal with respect to the
worst-case local use of control bits; the other one is optimal with respect to
global use of control bits; both are optimal in time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202004</id><created>2002-02-05</created><updated>2005-10-06</updated><authors><author><keyname>Eisenack</keyname><forenames>K.</forenames></author><author><keyname>Welsch</keyname><forenames>H.</forenames></author><author><keyname>Kropp</keyname><forenames>J. P.</forenames></author></authors><title>A Qualitative Dynamical Modelling Approach to Capital Accumulation in
  Unregulated Fisheries</title><categories>cs.AI cs.CE</categories><comments>24 pages, 6 figures</comments><acm-class>G.1.6; I.2.3; I.2.8; I.6.3; I.6.1; I.6.3</acm-class><doi>10.1016/j.jedc.2005.08.004</doi><abstract>  Capital accumulation has been a major issue in fisheries economics over the
last two decades, whereby the interaction of the fish and capital stocks were
of particular interest. Because bio-economic systems are intrinsically complex,
previous efforts in this field have relied on a variety of simplifying
assumptions. The model presented here relaxes some of these simplifications.
Problems of tractability are surmounted by using the methodology of qualitative
differential equations (QDE). The theory of QDEs takes into account that
scientific knowledge about particular fisheries is usually limited, and
facilitates an analysis of the global dynamics of systems with more than two
ordinary differential equations. The model is able to trace the evolution of
capital and fish stock in good agreement with observed patterns, and shows that
over-capitalization is unavoidable in unregulated fisheries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202005</id><created>2002-02-06</created><authors><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Secure History Preservation Through Timeline Entanglement</title><categories>cs.DC cs.CR cs.DB cs.DS</categories><comments>16 pages, 12 figures, 1 table</comments><acm-class>D.4.6; K.6.5; E.2; C.2.4</acm-class><abstract>  A secure timeline is a tamper-evident historic record of the states through
which a system goes throughout its operational history. Secure timelines can
help us reason about the temporal ordering of system states in a provable
manner. We extend secure timelines to encompass multiple, mutually distrustful
services, using timeline entanglement. Timeline entanglement associates
disparate timelines maintained at independent systems, by linking undeniably
the past of one timeline to the future of another. Timeline entanglement is a
sound method to map a time step in the history of one service onto the timeline
of another, and helps clients of entangled services to get persistent temporal
proofs for services rendered that survive the demise or non-cooperation of the
originating service. In this paper we present the design and implementation of
Timeweave, our service development framework for timeline entanglement based on
two novel disk-based authenticated data structures. We evaluate Timeweave's
performance characteristics and show that it can be efficiently deployed in a
loosely-coupled distributed system of a few hundred services with overhead of
roughly 2-8% of the processing resources of a PC-grade system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202006</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202006</id><created>2002-02-07</created><authors><author><keyname>Ravi</keyname><forenames>D.</forenames></author><author><keyname>Shyamasundar</keyname><forenames>R. K.</forenames></author></authors><title>Approximate Computation of Reach Sets in Hybrid Systems</title><categories>cs.LO cs.SE</categories><acm-class>F3, D2.2</acm-class><abstract>  One of the most important problems in hybrid systems is the {\em reachability
problem}. The reachability problem has been shown to be undecidable even for a
subclass of {\em linear} hybrid systems. In view of this, the main focus in the
area of hybrid systems has been to find {\em effective} semi-decision
procedures for this problem. Such an algorithmic approach involves finding
methods of computation and representation of reach sets of the continuous
variables within a discrete state of a hybrid system. In this paper, after
presenting a brief introduction to hybrid systems and reachability problem, we
propose a computational method for obtaining the reach sets of continuous
variables in a hybrid system. In addition to this, we also describe a new
algorithm to over-approximate with polyhedra the reach sets of the continuous
variables with linear dynamics and polyhedral initial set. We illustrate these
algorithms with typical interesting examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202007</id><created>2002-02-07</created><authors><author><keyname>Nasuto</keyname><forenames>Slawomir J.</forenames></author><author><keyname>Bishop</keyname><forenames>Mark J.</forenames></author></authors><title>Steady State Resource Allocation Analysis of the Stochastic Diffusion
  Search</title><categories>cs.AI cs.NE</categories><comments>25 pages, 7 figures</comments><acm-class>I.2.8; I.2.11; F.2.2; G.3</acm-class><abstract>  This article presents the long-term behaviour analysis of Stochastic
Diffusion Search (SDS), a distributed agent-based system for best-fit pattern
matching. SDS operates by allocating simple agents into different regions of
the search space. Agents independently pose hypotheses about the presence of
the pattern in the search space and its potential distortion. Assuming a
compositional structure of hypotheses about pattern matching agents perform an
inference on the basis of partial evidence from the hypothesised solution.
Agents posing mutually consistent hypotheses about the pattern support each
other and inhibit agents with inconsistent hypotheses. This results in the
emergence of a stable agent population identifying the desired solution.
Positive feedback via diffusion of information between the agents significantly
contributes to the speed with which the solution population is formed. The
formulation of the SDS model in terms of interacting Markov Chains enables its
characterisation in terms of the allocation of agents, or computational
resources. The analysis characterises the stationary probability distribution
of the activity of agents, which leads to the characterisation of the solution
population in terms of its similarity to the target pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202008</id><created>2002-02-07</created><authors><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>CUP: Controlled Update Propagation in Peer-to-Peer Networks</title><categories>cs.NI</categories><comments>15 pages, 6 figures</comments><acm-class>C.2.4</acm-class><abstract>  Recently the problem of indexing and locating content in peer-to-peer
networks has received much attention. Previous work suggests caching index
entries at intermediate nodes that lie on the paths taken by search queries,
but until now there has been little focus on how to maintain these intermediate
caches. This paper proposes CUP, a new comprehensive architecture for
Controlled Update Propagation in peer-to-peer networks. CUP asynchronously
builds caches of index entries while answering search queries. It then
propagates updates of index entries to maintain these caches. Under unfavorable
conditions, when compared with standard caching based on expiration times, CUP
reduces the average miss latency by as much as a factor of three. Under
favorable conditions, CUP can reduce the average miss latency by more than a
factor of ten.
  CUP refreshes intermediate caches, reduces query latency, and reduces network
load by coalescing bursts of queries for the same item. CUP controls and
confines propagation to updates whose cost is likely to be recovered by
subsequent queries. CUP gives peer-to-peer nodes the flexibility to use their
own incentive-based policies to determine when to receive and when to propagate
updates. Finally, the small propagation overhead incurred by CUP is more than
compensated for by its savings in cache misses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202009</id><created>2002-02-11</created><authors><author><keyname>Hoyer</keyname><forenames>Patrik O.</forenames></author></authors><title>Non-negative sparse coding</title><categories>cs.NE cs.CV</categories><acm-class>I.4.2</acm-class><abstract>  Non-negative sparse coding is a method for decomposing multivariate data into
non-negative sparse components. In this paper we briefly describe the
motivation behind this type of data representation and its relation to standard
sparse coding and non-negative matrix factorization. We then give a simple yet
efficient multiplicative algorithm for finding the optimal values of the hidden
components. In addition, we show how the basis vectors can be learned from the
observed data. Simulations demonstrate the effectiveness of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202010</id><created>2002-02-11</created><authors><author><keyname>Drabent</keyname><forenames>W.</forenames></author><author><keyname>Maluszynski</keyname><forenames>J.</forenames></author><author><keyname>Pietrzak</keyname><forenames>P.</forenames></author></authors><title>Using parametric set constraints for locating errors in CLP programs</title><categories>cs.PL</categories><comments>64 pages, To appear in Theory and Practice of Logic Programming</comments><acm-class>D1.6; D2.5; F3.1</acm-class><journal-ref>Theory and Practice of Logic Programming, Vol 2(4&amp;5), 2002, pp
  549-611.</journal-ref><abstract>  This paper introduces a framework of parametric descriptive directional types
for constraint logic programming (CLP). It proposes a method for locating type
errors in CLP programs and presents a prototype debugging tool. The main
technique used is checking correctness of programs w.r.t. type specifications.
The approach is based on a generalization of known methods for proving
correctness of logic programs to the case of parametric specifications.
Set-constraint techniques are used for formulating and checking verification
conditions for (parametric) polymorphic type specifications. The specifications
are expressed in a parametric extension of the formalism of term grammars. The
soundness of the method is proved and the prototype debugging tool supporting
the proposed approach is illustrated on examples.
  The paper is a substantial extension of the previous work by the same authors
concerning monomorphic directional types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202011</id><created>2002-02-12</created><authors><author><keyname>Bremner</keyname><forenames>David</forenames></author><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Ramaswami</keyname><forenames>Suneeta</forenames></author><author><keyname>Sacristan</keyname><forenames>Vera</forenames></author></authors><title>Small Strictly Convex Quadrilateral Meshes of Point Sets</title><categories>cs.CG</categories><comments>25 pages, 23 figures. A preliminary version appeared in ISAAC 2001,
  Christchurch NZ</comments><acm-class>F.2.2</acm-class><abstract>  In this paper, we give upper and lower bounds on the number of Steiner points
required to construct a strictly convex quadrilateral mesh for a planar point
set. In particular, we show that $3{\lfloor\frac{n}{2}\rfloor}$ internal
Steiner points are always sufficient for a convex quadrilateral mesh of $n$
points in the plane. Furthermore, for any given $n\geq 4$, there are point sets
for which $\lceil\frac{n-3}{2}\rceil-1$ Steiner points are necessary for a
convex quadrilateral mesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202012</id><created>2002-02-12</created><authors><author><keyname>Leuschel</keyname><forenames>Michael</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Logic program specialisation through partial deduction: Control issues</title><categories>cs.PL cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>D.1.6; D.1.2; I.2.2; F.4.1; I.2.3</acm-class><abstract>  Program specialisation aims at improving the overall performance of programs
by performing source to source transformations. A common approach within
functional and logic programming, known respectively as partial evaluation and
partial deduction, is to exploit partial knowledge about the input. It is
achieved through a well-automated application of parts of the
Burstall-Darlington unfold/fold transformation framework. The main challenge in
developing systems is to design automatic control that ensures correctness,
efficiency, and termination. This survey and tutorial presents the main
developments in controlling partial deduction over the past 10 years and
analyses their respective merits and shortcomings. It ends with an assessment
of current achievements and sketches some remaining research challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202013</id><created>2002-02-12</created><authors><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Thakar</keyname><forenames>Ani R.</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter Z.</forenames></author><author><keyname>Malik</keyname><forenames>Tanu</forenames></author><author><keyname>Raddick</keyname><forenames>Jordan</forenames></author><author><keyname>Stoughton</keyname><forenames>Christopher</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>The SDSS SkyServer: Public Access to the Sloan Digital Sky Server Data</title><categories>cs.DL cs.DB</categories><comments>12 pages, original word document at
  http://research.microsoft.com/~gray/Papers/MSR_TR_01_104_SkyServer.doc</comments><report-no>MSR TR 01 104</report-no><acm-class>H.3.7; H.3.5;H.2; H.3; H.4; H.5</acm-class><journal-ref>ACM SIGMOD 2002 proceedings</journal-ref><abstract>  The SkyServer provides Internet access to the public Sloan Digi-tal Sky
Survey (SDSS) data for both astronomers and for science education. This paper
describes the SkyServer goals and archi-tecture. It also describes our
experience operating the SkyServer on the Internet. The SDSS data is public and
well-documented so it makes a good test platform for research on database
algorithms and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202014</id><created>2002-02-12</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Szalay</keyname><forenames>Alex S.</forenames></author><author><keyname>Thakar</keyname><forenames>Ani R.</forenames></author><author><keyname>Kunszt</keyname><forenames>Peter Z.</forenames></author><author><keyname>Stoughton</keyname><forenames>Christopher</forenames></author><author><keyname>Slutz</keyname><forenames>Don</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>Data Mining the SDSS SkyServer Database</title><categories>cs.DB cs.DL</categories><comments>40 pages, Original source is at
  http://research.microsoft.com/~gray/Papers/MSR_TR_O2_01_20_queries.doc</comments><report-no>Microsoft Tech Report MSR TR 02 01</report-no><acm-class>H.2.8;H.3.3; H.3.5;h.3.7;H.4.2</acm-class><abstract>  An earlier paper (Szalay et. al. &quot;Designing and Mining MultiTerabyte
Astronomy Archives: The Sloan Digital Sky Survey,&quot; ACM SIGMOD 2000) described
the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty
database queries and twelve data visualization tasks that a good data
management system should support. We built a database and interfaces to support
both the query load and also a website for ad-hoc access. This paper reports on
the database design, describes the data loading pipeline, and reports on the
query implementation and performance. The queries typically translated to a
single SQL statement. Most queries run in less than 20 seconds, allowing
scientists to interactively explore the database. This paper is an in-depth
tour of those queries. Readers should first have studied the companion overview
paper Szalay et. al. &quot;The SDSS SkyServer, Public Access to the Sloan Digital
Sky Server Data&quot; ACM SIGMOND 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202015</id><created>2002-02-15</created><updated>2002-09-12</updated><authors><author><keyname>Lehmann</keyname><forenames>Benny</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Combinatorial Auctions with Decreasing Marginal Utilities</title><categories>cs.GT</categories><comments>To appear in GEB. Preliminary version appeared in EC'01</comments><report-no>Leibniz Center for Research in Computer Science TR-2002-15, April
  2002</report-no><acm-class>J.4</acm-class><journal-ref>Games and Economic Behavior, Vol 55/2 May 2006 pp 270-296</journal-ref><doi>10.1016/j.geb2005.02.006</doi><abstract>  In most of microeconomic theory, consumers are assumed to exhibit decreasing
marginal utilities. This paper considers combinatorial auctions among such
submodular buyers. The valuations of such buyers are placed within a hierarchy
of valuations that exhibit no complementarities, a hierarchy that includes also
OR and XOR combinations of singleton valuations, and valuations satisfying the
gross substitutes property. Those last valuations are shown to form a
zero-measure subset of the submodular valuations that have positive measure.
While we show that the allocation problem among submodular valuations is
NP-hard, we present an efficient greedy 2-approximation algorithm for this case
and generalize it to the case of limited complementarities. No such
approximation algorithm exists in a setting allowing for arbitrary
complementarities. Some results about strategic aspects of combinatorial
auctions among players with decreasing marginal utilities are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202016</id><created>2002-02-15</created><authors><author><keyname>Gonen</keyname><forenames>Rica</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Linear Programming helps solving large multi-unit combinatorial auctions</title><categories>cs.GT cs.AI</categories><report-no>Leibniz Center for Research in Computer Science TR-2001-8</report-no><acm-class>G.1.6;I.2.8</acm-class><abstract>  Previous works suggested the use of Branch and Bound techniques for finding
the optimal allocation in (multi-unit) combinatorial auctions. They remarked
that Linear Programming could provide a good upper-bound to the optimal
allocation, but they went on using lighter and less tight upper-bound
heuristics, on the ground that LP was too time-consuming to be used
repetitively to solve large combinatorial auctions. We present the results of
extensive experiments solving large (multi-unit) combinatorial auctions
generated according to distributions proposed by different researchers. Our
surprising conclusion is that Linear Programming is worth using. Investing
almost all of one's computing time in using LP to bound from above the value of
the optimal solution in order to prune aggressively pays off. We present a way
to save on the number of calls to the LP routine and experimental results
comparing different heuristics for choosing the bid to be considered next.
Those results show that the ordering based on the square root of the size of
the bids that was shown to be theoretically optimal in a previous paper by the
authors performs surprisingly better than others in practice. Choosing to deal
first with the bid with largest coefficient (typically 1) in the optimal
solution of the relaxed LP problem, is also a good choice. The gap between the
lower bound provided by greedy heuristics and the upper bound provided by LP is
typically small and pruning is therefore extensive. For most distributions,
auctions of a few hundred goods among a few thousand bids can be solved in
practice. All experiments were run on a PC under Matlab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202017</id><created>2002-02-15</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>O'Callaghan</keyname><forenames>Liadan Ita</forenames></author><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author></authors><title>Truth Revelation in Approximately Efficient Combinatorial Auctions</title><categories>cs.GT</categories><comments>Submitted to a Journal. A preliminary version appeared in EC'99</comments><report-no>Stanford University CS-TN-99-88</report-no><acm-class>G.1.6;I.2.8;J.4</acm-class><journal-ref>Journal of the ACM Vol. 49, No. 5, September 2002, pp. 577-602</journal-ref><abstract>  Some important classical mechanisms considered in Microeconomics and Game
Theory require the solution of a difficult optimization problem. This is true
of mechanisms for combinatorial auctions, which have in recent years assumed
practical importance, and in particular of the gold standard for combinatorial
auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these
mechanisms - in particular, their truth revelation properties - assumes that
the optimization problems are solved precisely. In reality, these optimization
problems can usually be solved only in an approximate fashion. We investigate
the impact on such mechanisms of replacing exact solutions by approximate ones.
Specifically, we look at a particular greedy optimization method. We show that
the GVA payment scheme does not provide for a truth revealing mechanism. We
introduce another scheme that does guarantee truthfulness for a restricted
class of players. We demonstrate the latter property by identifying natural
properties for combinatorial auctions and showing that, for our restricted
class of players, they imply that truthful strategies are dominant. Those
properties have applicability beyond the specific auction studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202018</id><created>2002-02-15</created><updated>2002-04-15</updated><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Nonmonotonic Logics and Semantics</title><categories>cs.AI cs.LO math.LO</categories><comments>28 pages. Misprint corrected 15/04/02</comments><report-no>Leibniz Center for Research in Computer Science TR-98-6</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Logic and Computation, Vol. 11 No.2, pp.229-256 2001</journal-ref><abstract>  Tarski gave a general semantics for deductive reasoning: a formula a may be
deduced from a set A of formulas iff a holds in all models in which each of the
elements of A holds. A more liberal semantics has been considered: a formula a
may be deduced from a set A of formulas iff a holds in all of the &quot;preferred&quot;
models in which all the elements of A hold. Shoham proposed that the notion of
&quot;preferred&quot; models be defined by a partial ordering on the models of the
underlying language. A more general semantics is described in this paper, based
on a set of natural properties of choice functions. This semantics is here
shown to be equivalent to a semantics based on comparing the relative
&quot;importance&quot; of sets of models, by what amounts to a qualitative probability
measure. The consequence operations defined by the equivalent semantics are
then characterized by a weakening of Tarski's properties in which the
monotonicity requirement is replaced by three weaker conditions. Classical
propositional connectives are characterized by natural introduction-elimination
rules in a nonmonotonic setting. Even in the nonmonotonic setting, one obtains
classical propositional logic, thus showing that monotonicity is not required
to justify classical propositional connectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202019</id><created>2002-02-16</created><authors><author><keyname>Gunther</keyname><forenames>N. J.</forenames></author></authors><title>Hypernets -- Good (G)news for Gnutella</title><categories>cs.PF cs.DC cs.IR cs.NI</categories><comments>12 pages, 3 figures</comments><acm-class>C.2.4;C.4;D.2.11;H.3.5</acm-class><abstract>  Criticism of Gnutella network scalability has rested on the bandwidth
attributes of the original interconnection topology: a Cayley tree. Trees, in
general, are known to have lower aggregate bandwidth than higher dimensional
topologies e.g., hypercubes, meshes and tori. Gnutella was intended to support
thousands to millions of peers. Studies of interconnection topologies in the
literature, however, have focused on hardware implementations which are limited
by cost to a few thousand nodes. Since the Gnutella network is virtual,
hyper-topologies are relatively unfettered by such constraints. We present
performance models for several plausible hyper-topologies and compare their
query throughput up to millions of peers. The virtual hypercube and the virtual
hypertorus are shown to offer near linear scalability subject to the number of
peer TCP/IP connections that can be simultaneously kept open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202020</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202020</id><created>2002-02-17</created><updated>2012-08-24</updated><authors><author><keyname>Kupervasser</keyname><forenames>Oleg</forenames></author><author><keyname>Vardy</keyname><forenames>Alexsander</forenames></author></authors><title>The Mysterious Optimality of Naive Bayes: Estimation of the Probability
  in the System of &quot;Classifiers&quot;</title><categories>cs.CV cs.AI</categories><comments>9 pages,1 figure, all changes in the second version is made by
  Kupervasser only</comments><acm-class>E.5; E.4; E.2; H.1.1; F.1.1; F.1.3</acm-class><journal-ref>Pattern Recognition and Image Analysis, 2014, Vol. 24, No. 1</journal-ref><doi>10.1134/S1054661814010088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayes Classifiers are widely used currently for recognition, identification
and knowledge discovery. The fields of application are, for example, image
processing, medicine, chemistry (QSAR). But by mysterious way the Naive Bayes
Classifier usually gives a very nice and good presentation of a recognition. It
can not be improved considerably by more complex models of Bayes Classifier. We
demonstrate here a very nice and simple proof of the Naive Bayes Classifier
optimality, that can explain this interesting fact.The derivation in the
current paper is based on arXiv:cs/0202020v1
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202021</id><created>2002-02-18</created><authors><author><keyname>Kraus</keyname><forenames>Sarit</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>Magidor</keyname><forenames>Menachem</forenames></author></authors><title>Nonmonotonic Reasoning, Preferential Models and Cumulative Logics</title><categories>cs.AI</categories><comments>Presented at JELIA, June 1988. Some misprints in the Journal paper
  have been corrected</comments><report-no>Leibniz Center for Research in Computer Science TR-88-15</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Artificial Intelligence, Vol. 44 Nos. 1-2 (July 1990)
  pp. 167-207</journal-ref><abstract>  Many systems that exhibit nonmonotonic behavior have been described and
studied already in the literature. The general notion of nonmonotonic
reasoning, though, has almost always been described only negatively, by the
property it does not enjoy, i.e. monotonicity. We study here general patterns
of nonmonotonic reasoning and try to isolate properties that could help us map
the field of nonmonotonic reasoning by reference to positive properties. We
concentrate on a number of families of nonmonotonic consequence relations,
defined in the style of Gentzen. Both proof-theoretic and semantic points of
view are developed in parallel. The former point of view was pioneered by D.
Gabbay, while the latter has been advocated by Y. Shoham in. Five such families
are defined and characterized by representation theorems, relating the two
points of view. One of the families of interest, that of preferential
relations, turns out to have been studied by E. Adams. The &quot;preferential&quot;
models proposed here are a much stronger tool than Adams' probabilistic
semantics. The basic language used in this paper is that of propositional
logic. The extension of our results to first order predicate calculi and the
study of the computational complexity of the decision problems described in
this paper will be treated in another paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202022</id><created>2002-02-18</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>Magidor</keyname><forenames>Menachem</forenames></author></authors><title>What does a conditional knowledge base entail?</title><categories>cs.AI</categories><comments>Preliminary version presented at KR'89. Minor corrections of the
  Journal Version</comments><report-no>Leibniz Center for Research in Computer Science TR-88-16 and
  TR-90-10</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Artificial Intelligence, Vol. 55 no.1 (May 1992) pp.
  1-60. Erratum in Vol. 68 (1994) p. 411</journal-ref><abstract>  This paper presents a logical approach to nonmonotonic reasoning based on the
notion of a nonmonotonic consequence relation. A conditional knowledge base,
consisting of a set of conditional assertions of the type &quot;if ... then ...&quot;,
represents the explicit defeasible knowledge an agent has about the way the
world generally behaves. We look for a plausible definition of the set of all
conditional assertions entailed by a conditional knowledge base. In a previous
paper, S. Kraus and the authors defined and studied &quot;preferential&quot; consequence
relations. They noticed that not all preferential relations could be considered
as reasonable inference procedures. This paper studies a more restricted class
of consequence relations, &quot;rational&quot; relations. It is argued that any
reasonable nonmonotonic inference procedure should define a rational relation.
It is shown that the rational relations are exactly those that may be
represented by a &quot;ranked&quot; preferential model, or by a (non-standard)
probabilistic model. The rational closure of a conditional knowledge base is
defined and shown to provide an attractive answer to the question of the title.
Global properties of this closure operation are proved: it is a cumulative
operation. It is also computationally tractable. This paper assumes the
underlying language is propositional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202023</id><created>2002-02-18</created><updated>2002-02-20</updated><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Expected Qualitative Utility Maximization</title><categories>cs.GT</categories><comments>Small correction in Section 4</comments><report-no>Leibniz Center for Research in Computer Science TR-97-15</report-no><acm-class>I.2.3</acm-class><journal-ref>Games and Economic Behavior, Vol. 35, No. 1-2 (April 2001) pp.
  54-79</journal-ref><abstract>  A model for decision making that generalizes Expected Utility Maximization is
presented. This model, Expected Qualitative Utility Maximization, encompasses
the Maximin criterion. It relaxes both the Independence and the Continuity
postulates. Its main ingredient is the definition of a qualitative order on
nonstandard models of the real numbers and the consideration of nonstandard
utilities. Expected Qualitative Utility Maximization is characterized by an
original weakening of von Neumann-Morgenstern's postulates. Subjective
probabilities may be defined from those weakened postulates, as Anscombe and
Aumann did from the original postulates. Subjective probabilities are numbers,
not matrices as in the Subjective Expected Lexicographic Utility approach. JEL
no.: D81 Keywords: Utility Theory, Non-Standard Utilities, Qualitative Decision
Theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202024</id><created>2002-02-18</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>A note on Darwiche and Pearl</title><categories>cs.AI</categories><comments>A small unpublished remark on a paper by Darwiche and Pearl</comments><acm-class>I.2.3</acm-class><abstract>  It is shown that Darwiche and Pearl's postulates imply an interesting
property, not noticed by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202025</id><created>2002-02-18</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>Magidor</keyname><forenames>Menachem</forenames></author><author><keyname>Schlechta</keyname><forenames>Karl</forenames></author></authors><title>Distance Semantics for Belief Revision</title><categories>cs.AI</categories><comments>Preliminary version presented at TARK '96</comments><report-no>Leibniz Center for Research in Computer Science TR-98-10</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Symbolic Logic, Vol. 66 No.1 (March 2001) pp. 295-317</journal-ref><abstract>  A vast and interesting family of natural semantics for belief revision is
defined. Suppose one is given a distance d between any two models. One may then
define the revision of a theory K by a formula a as the theory defined by the
set of all those models of a that are closest, by d, to the set of models of K.
This family is characterized by a set of rationality postulates that extends
the AGM postulates. The new postulates describe properties of iterated
revisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202026</id><created>2002-02-18</created><authors><author><keyname>Berger</keyname><forenames>Shai</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author><author><keyname>Schlechta</keyname><forenames>Karl</forenames></author></authors><title>Preferred History Semantics for Iterated Updates</title><categories>cs.AI</categories><report-no>Leibniz Center for Research in Computer SCience TR-98-11 (July 1998)</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Logic and Computation, Vol. 9 no. 6 (1999) pp. 817-833</journal-ref><abstract>  We give a semantics to iterated update by a preference relation on possible
developments. An iterated update is a sequence of formulas, giving (incomplete)
information about successive states of the world. A development is a sequence
of models, describing a possible trajectory through time. We assume a principle
of inertia and prefer those developments, which are compatible with the
information, and avoid unnecessary changes. The logical properties of the
updates defined in this way are considered, and a representation result is
proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202027</id><created>2002-02-18</created><authors><author><keyname>Verstak</keyname><forenames>Alex</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Watson</keyname><forenames>Layne T.</forenames></author><author><keyname>He</keyname><forenames>Jian</forenames></author><author><keyname>Shaffer</keyname><forenames>Clifford A.</forenames></author><author><keyname>Bae</keyname><forenames>Kyung Kyoon</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Tranter</keyname><forenames>William H.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>BSML: A Binding Schema Markup Language for Data Interchange in Problem
  Solving Environments (PSEs)</title><categories>cs.CE cs.SE</categories><acm-class>D.2.6; I.2.4</acm-class><abstract>  We describe a binding schema markup language (BSML) for describing data
interchange between scientific codes. Such a facility is an important
constituent of scientific problem solving environments (PSEs). BSML is designed
to integrate with a PSE or application composition system that views model
specification and execution as a problem of managing semistructured data. The
data interchange problem is addressed by three techniques for processing
semistructured data: validation, binding, and conversion. We present BSML and
describe its application to a PSE for wireless communications system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202028</id><created>2002-02-20</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Classes of service under perfect competition and technological change: a
  model for the dynamics of the Internet?</title><categories>cs.GT</categories><comments>Extended abstract in Proceedings of EC'01</comments><report-no>Leibniz Center for Research in Computer Science: TR-2000-42</report-no><acm-class>J.4</acm-class><abstract>  Certain services may be provided in a continuous, one-dimensional, ordered
range of different qualities and a customer requiring a service of quality q
can only be offered a quality superior or equal to q. Only a discrete set of
different qualities will be offered, and a service provider will provide the
same service (of fixed quality b) to all customers requesting qualities of
service inferior or equal to b. Assuming all services (of quality b) are priced
identically, a monopolist will choose the qualities of service and the prices
that maximize profit but, under perfect competition, a service provider will
choose the (inferior) quality of service that can be priced at the lowest
price. Assuming significant economies of scale, two fundamentally different
regimes are possible: either a number of different classes of service are
offered (DC regime), or a unique class of service offers an unbounded quality
of service (UC regime). The DC regime appears in one of two sub-regimes: one,
BDC, in which a finite number of classes is offered, the qualities of service
offered are bounded and requests for high-quality services are not met, or UDC
in which an infinite number of classes of service are offered and every request
is met. The types of the demand curve and of the economies of scale, not the
pace of technological change, determine the regime and the class boundaries.
The price structure in the DC regime obeys very general laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202029</id><created>2002-02-20</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Nonstandard numbers for qualitative decision making</title><categories>cs.GT</categories><comments>14 pages. Presented at TARK'98</comments><report-no>Leibniz Center for Research in Computer Science: TR-97-15</report-no><acm-class>I.2.3</acm-class><journal-ref>Proceedings of the 7th Conference on Theoretical Aspects of
  Reasoning and Knowledge, I. Gilboa ed., Evanston Ill., July 1998, pp. 161-174</journal-ref><abstract>  The consideration of nonstandard models of the real numbers and the
definition of a qualitative ordering on those models provides a generalization
of the principle of maximization of expected utility. It enables the decider to
assign probabilities of different orders of magnitude to different events or to
assign utilities of different orders of magnitude to different outcomes. The
properties of this generalized notion of rationality are studied in the
frameworks proposed by von Neumann and Morgenstern and later by Anscombe and
Aumann. It is characterized by an original weakening of their postulates in two
different situations: nonstandard probabilities and standard utilities on one
hand and standard probabilities and nonstandard utilities on the other hand.
This weakening concerns both Independence and Continuity. It is orthogonal with
the weakening proposed by lexicographic orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202030</id><created>2002-02-20</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Generalized Qualitative Probability: Savage revisited</title><categories>cs.GT cs.AI</categories><comments>Presented at UAI'96</comments><acm-class>I.2.3</acm-class><journal-ref>Twelfth Conference on Uncertainty in Artificial Intelligence, E.
  Horvitz and F. Jensen eds., Morgan Kaufmann, pp. 381-388, Portland, Oregon,
  August 1996</journal-ref><abstract>  Preferences among acts are analyzed in the style of L. Savage, but as
partially ordered. The rationality postulates considered are weaker than
Savage's on three counts. The Sure Thing Principle is derived in this setting.
The postulates are shown to lead to a characterization of generalized
qualitative probability that includes and blends both traditional qualitative
probability and the ranked structures used in logical approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202031</id><created>2002-02-20</created><authors><author><keyname>Freund</keyname><forenames>Michael</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Nonmonotonic inference operations</title><categories>cs.AI</categories><comments>54 pages. A short version appeared in Studia Logica, Vol. 53 no. 2
  (1994) pp. 161-201</comments><report-no>Leibniz Center for Research in Computer Science: TR-92-2</report-no><acm-class>I.2.3</acm-class><journal-ref>Bulletin of the IGPL, Vol. 1 no. 1 (July 1993), pp. 23-68</journal-ref><abstract>  A. Tarski proposed the study of infinitary consequence operations as the
central topic of mathematical logic. He considered monotonicity to be a
property of all such operations. In this paper, we weaken the monotonicity
requirement and consider more general operations, inference operations. These
operations describe the nonmonotonic logics both humans and machines seem to be
using when infering defeasible information from incomplete knowledge. We single
out a number of interesting families of inference operations. This study of
infinitary inference operations is inspired by the results of Kraus, Lehmann
and Magidor on finitary nonmonotonic operations, but this paper is
self-contained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202032</id><created>2002-02-20</created><authors><author><keyname>Gonen</keyname><forenames>Rica</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Optimal Solutions for Multi-Unit Combinatorial Auctions: Branch and
  Bound Heuristics</title><categories>cs.GT cs.AI</categories><comments>Presented at EC'00</comments><acm-class>G.1.6;I.2.8</acm-class><journal-ref>Second ACM Conference on Electronic Commerce (EC'00) Minneapolis,
  Minnesota, October 2000, pp. 13-20</journal-ref><abstract>  Finding optimal solutions for multi-unit combinatorial auctions is a hard
problem and finding approximations to the optimal solution is also hard. We
investigate the use of Branch-and-Bound techniques: they require both a way to
bound from above the value of the best allocation and a good criterion to
decide which bids are to be tried first. Different methods for efficiently
bounding from above the value of the best allocation are considered.
Theoretical original results characterize the best approximation ratio and the
ordering criterion that provides it. We suggest to use this criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202033</id><created>2002-02-20</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>The logical meaning of Expansion</title><categories>cs.AI</categories><comments>9 pages. Unpublished</comments><acm-class>I.2.3</acm-class><abstract>  The Expansion property considered by researchers in Social Choice is shown to
correspond to a logical property of nonmonotonic consequence relations that is
the {\em pure}, i.e., not involving connectives, version of a previously known
weak rationality condition. The assumption that the union of two definable sets
of models is definable is needed for the soundness part of the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202034</id><created>2002-02-20</created><authors><author><keyname>Bienenstock</keyname><forenames>Elie</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Covariance Plasticity and Regulated Criticality</title><categories>cs.NE cs.AI nlin.AO q-bio</categories><comments>35 pages, 8 figures</comments><report-no>Center for Neural Computation, Hebrew University, Jerusalem TR-95-1</report-no><acm-class>I.2.6</acm-class><journal-ref>Advances in Complex Systems, 1(4) (1998) pp. 361-384</journal-ref><abstract>  We propose that a regulation mechanism based on Hebbian covariance plasticity
may cause the brain to operate near criticality. We analyze the effect of such
a regulation on the dynamics of a network with excitatory and inhibitory
neurons and uniform connectivity within and across the two populations. We show
that, under broad conditions, the system converges to a critical state lying at
the common boundary of three regions in parameter space; these correspond to
three modes of behavior: high activity, low activity, oscillation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202035</id><created>2002-02-21</created><authors><author><keyname>Valluri</keyname><forenames>Satyanarayana R</forenames></author><author><keyname>Vadapalli</keyname><forenames>Soujanya</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Sprinkling Selections over Join DAGs for Efficient Query Optimization</title><categories>cs.DB</categories><comments>12 Pages</comments><acm-class>H.2.4</acm-class><abstract>  In optimizing queries, solutions based on AND/OR DAG can generate all
possible join orderings and select placements before searching for optimal
query execution strategy. But as the number of joins and selection conditions
increase, the space and time complexity to generate optimal query plan
increases exponentially. In this paper, we use join graph for a relational
database schema to either pre-compute all possible join orderings that can be
executed and store it as a join DAG or, extract joins in the queries to
incrementally build a history join DAG as and when the queries are executed.
The select conditions in the queries are appropriately placed in the retrieved
join DAG (or, history join DAG) to generate optimal query execution strategy.
We experimentally evaluate our query optimization technique on TPC-D/H query
sets to show their effectiveness over AND/OR DAG query optimization strategy.
Finally, we illustrate how our technique can be used for efficient multiple
query optimization and selection of materialized views in data warehousing
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202036</id><created>2002-02-25</created><authors><author><keyname>Boehler</keyname><forenames>E.</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>E.</forenames></author><author><keyname>Reith</keyname><forenames>Steffen</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Equivalence and Isomorphism for Boolean Constraint Satisfaction</title><categories>cs.CC cs.LO</categories><acm-class>F.4.1; F.2.2</acm-class><abstract>  A Boolean constraint satisfaction instance is a conjunction of constraint
applications, where the allowed constraints are drawn from a fixed set B of
Boolean functions. We consider the problem of determining whether two given
constraint satisfaction instances are equivalent and prove a Dichotomy Theorem
by showing that for all sets C of allowed constraints, this problem is either
polynomial-time solvable or coNP-complete, and we give a simple criterion to
determine which case holds.
  A more general problem addressed in this paper is the isomorphism problem,
the problem of determining whether there exists a renaming of the variables
that makes two given constraint satisfaction instances equivalent in the above
sense. We prove that this problem is coNP-hard if the corresponding equivalence
problem is coNP-hard, and polynomial-time many-one reducible to the graph
isomorphism problem in all other cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202037</id><created>2002-02-25</created><updated>2003-10-13</updated><authors><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author><author><keyname>Vossen</keyname><forenames>Gottfried</forenames></author></authors><title>Towards practical meta-querying</title><categories>cs.DB</categories><comments>Includes a new section &quot;Experimental performance evaluation&quot;</comments><acm-class>H.2.3</acm-class><journal-ref>Information Systems, Volume 30, Issue 4 , June 2005, Pages 317-332</journal-ref><doi>10.1016/j.is.2004.04.001</doi><abstract>  We describe a meta-querying system for databases containing queries in
addition to ordinary data. In the context of such databases, a meta-query is a
query about queries. Representing stored queries in XML, and using the standard
XML manipulation language XSLT as a sublanguage, we show that just a few
features need to be added to SQL to turn it into a fully-fledged meta-query
language. The good news is that these features can be directly supported by
extensible database technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202038</id><created>2002-02-26</created><authors><author><keyname>Jacek</keyname><forenames>Leszczynski</forenames></author><author><keyname>Sebastian</keyname><forenames>Pluta</forenames></author></authors><title>The efficient generation of unstructured control volumes in 2D and 3D</title><categories>cs.CG cs.CE cs.NA math.NA physics.comp-ph</categories><comments>8 pages, 2 figures, Parallel Processing and Applied Mathematics
  (PPAM2001) Conference, under publishing in Lecture Notes in Computer Science,
  Springer Verlag</comments><acm-class>65N20, 68Q20, 68Q22, 76M20</acm-class><journal-ref>Lecture Notes in Computer Science (LNCS), Springer-Verlag, 2328,
  2001, pp. 682-689</journal-ref><abstract>  Many problems in engineering, chemistry and physics require the
representation of solutions in complex geometries. In the paper we deal with a
problem of unstructured mesh generation for the control volume method. We
propose an algorithm which bases on the spheres generation in central points of
the control volumes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0202039</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0202039</id><created>2002-02-28</created><authors><author><keyname>Batagelj</keyname><forenames>V.</forenames></author><author><keyname>Zaver&#x161;nik</keyname><forenames>M.</forenames></author></authors><title>Generalized Cores</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2</acm-class><journal-ref>Advances in Data Analysis and Classification, 2011. Volume 5,
  Number 2, 129-145</journal-ref><abstract>  Cores are, besides connectivity components, one among few concepts that
provides us with efficient decompositions of large graphs and networks.
  In the paper a generalization of the notion of core of a graph based on
vertex property function is presented. It is shown that for the local monotone
vertex property functions the corresponding cores can be determined in $O(m
\max (\Delta, \log n))$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203001</id><created>2002-03-01</created><authors><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author></authors><title>Towards Generic Refactoring</title><categories>cs.PL</categories><acm-class>D.1.1; D.1.2; D.2.1; D.2.3; D.2.13; D.3.1; I.1.1; I.1.2; I.1.3</acm-class><abstract>  We study program refactoring while considering the language or even the
programming paradigm as a parameter. We use typed functional programs, namely
Haskell programs, as the specification medium for a corresponding refactoring
framework. In order to detach ourselves from language syntax, our
specifications adhere to the following style. (I) As for primitive algorithms
for program analysis and transformation, we employ generic function combinators
supporting generic traversal and polymorphic functions refined by ad-hoc cases.
(II) As for the language abstractions involved in refactorings, we design a
dedicated multi-parameter class. This class can be instantiated for
abstractions as present in various languages, e.g., Java, Prolog or Haskell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203002</id><created>2002-03-01</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Another perspective on Default Reasoning</title><categories>cs.AI</categories><comments>Presented at Workshop on Logical Formalizations of Commense Sense,
  Austin (Texas), January 1993</comments><report-no>Leibniz Center for Research in Computer Science TR-92-12, July 1992</report-no><acm-class>I.2.3</acm-class><journal-ref>Annals of Mathematics and Artificial Intelligence, 15(1) (1995)
  pp. 61-82</journal-ref><abstract>  The lexicographic closure of any given finite set D of normal defaults is
defined. A conditional assertion &quot;if a then b&quot; is in this lexicographic closure
if, given the defaults D and the fact a, one would conclude b. The
lexicographic closure is essentially a rational extension of D, and of its
rational closure, defined in a previous paper. It provides a logic of normal
defaults that is different from the one proposed by R. Reiter and that is rich
enough not to require the consideration of non-normal defaults. A large number
of examples are provided to show that the lexicographic closure corresponds to
the basic intuitions behind Reiter's logic of defaults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203003</id><created>2002-03-01</created><authors><author><keyname>Kaluzhny</keyname><forenames>Yuri</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Deductive Nonmonotonic Inference Operations: Antitonic Representations</title><categories>cs.AI</categories><report-no>Leibniz Center for Research in Computer Science TR-94-3, March 1994</report-no><acm-class>I.2.3</acm-class><journal-ref>Journal of Logic and Computation, 5(1) (1995) pp. 111-122</journal-ref><abstract>  We provide a characterization of those nonmonotonic inference operations C
for which C(X) may be described as the set of all logical consequences of X
together with some set of additional assumptions S(X) that depends
anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset
of S(X)). The operations represented are exactly characterized in terms of
properties most of which have been studied in Freund-Lehmann(cs.AI/0202031).
Similar characterizations of right-absorbing and cumulative operations are also
provided. For cumulative operations, our results fit in closely with those of
Freund. We then discuss extending finitary operations to infinitary operations
in a canonical way and discuss co-compactness properties. Our results provide a
satisfactory notion of pseudo-compactness, generalizing to deductive
nonmonotonic operations the notion of compactness for monotonic operations.
They also provide an alternative, more elegant and more general, proof of the
existence of an infinitary deductive extension for any finitary deductive
operation (Theorem 7.9 of Freund-Lehmann).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203004</id><created>2002-03-04</created><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Stereotypical Reasoning: Logical Properties</title><categories>cs.AI</categories><comments>Presented at Fourth Workshop on Logic, Language, Information and
  Computation, Fortaleza (Brasil), August 1997</comments><report-no>Leibniz Center for Research in Computer Science TR-97-10</report-no><acm-class>I.2.3</acm-class><journal-ref>Logic Journal of the Interest Group in Pure and Applied Logics
  (IGPL), 6(1) (1998) pp. 49-58</journal-ref><abstract>  Stereotypical reasoning assumes that the situation at hand is one of a kind
and that it enjoys the properties generally associated with that kind of
situation. It is one of the most basic forms of nonmonotonic reasoning. A
formal model for stereotypical reasoning is proposed and the logical properties
of this form of reasoning are studied. Stereotypical reasoning is shown to be
cumulative under weak assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203005</id><created>2002-03-04</created><updated>2002-03-05</updated><authors><author><keyname>Delgrande</keyname><forenames>J. P.</forenames></author><author><keyname>Schaub</keyname><forenames>T.</forenames></author><author><keyname>Tompits</keyname><forenames>H.</forenames></author></authors><title>A Framework for Compiling Preferences in Logic Programs</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming</comments><acm-class>I.2.3; D.1.6</acm-class><abstract>  We introduce a methodology and framework for expressing general preference
information in logic programming under the answer set semantics. An ordered
logic program is an extended logic program in which rules are named by unique
terms, and in which preferences among rules are given by a set of atoms of form
s &lt; t where s and t are names. An ordered logic program is transformed into a
second, regular, extended logic program wherein the preferences are respected,
in that the answer sets obtained in the transformed program correspond with the
preferred answer sets of the original program. Our approach allows the
specification of dynamic orderings, in which preferences can appear arbitrarily
within a program. Static orderings (in which preferences are external to a
logic program) are a trivial restriction of the general dynamic case. First, we
develop a specific approach to reasoning with preferences, wherein the
preference ordering specifies the order in which rules are to be applied. We
then demonstrate the wide range of applicability of our framework by showing
how other approaches, among them that of Brewka and Eiter, can be captured
within our framework. Since the result of each of these transformations is an
extended logic program, we can make use of existing implementations, such as
dlv and smodels. To this end, we have developed a publicly available compiler
as a front-end for these programming systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203006</id><created>2002-03-04</created><authors><author><keyname>Molina</keyname><forenames>Juan M.</forenames></author><author><keyname>Pimentel</keyname><forenames>Ernesto</forenames></author></authors><title>Composing Programs in a Rewriting Logic for Declarative Programming</title><categories>cs.LO cs.PL</categories><comments>47 pages. A shorter version (33 pages) will appear in the Journal of
  Theory and Practice of Logic Programming</comments><report-no>LCC852</report-no><acm-class>D.3.2;D.3.3;F.3.2;F.3.3;F.4.1</acm-class><abstract>  Constructor-Based Conditional Rewriting Logic is a general framework for
integrating first-order functional and logic programming which gives an
algebraic semantics for non-deterministic functional-logic programs. In the
context of this formalism, we introduce a simple notion of program module as an
open program which can be extended together with several mechanisms to combine
them. These mechanisms are based on a reduced set of operations. However, the
high expressiveness of these operations enable us to model typical constructs
for program modularization like hiding, export/import,
genericity/instantiation, and inheritance in a simple way. We also deal with
the semantic aspects of the proposal by introducing an immediate consequence
operator, and studying several alternative semantics for a program module,
based on this operator, in the line of logic programming: the operator itself,
its least fixpoint (the least model of the module), the set of its
pre-fixpoints (term models of the module), and some other variations in order
to find a compositional and fully abstract semantics wrt the set of operations
and a natural notion of observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203007</id><created>2002-03-04</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Two results for proiritized logic programming</title><categories>cs.AI</categories><comments>20 pages, to be appeared in journal Theory and Practice of Logic
  Programming</comments><acm-class>F.4.1</acm-class><abstract>  Prioritized default reasoning has illustrated its rich expressiveness and
flexibility in knowledge representation and reasoning. However, many important
aspects of prioritized default reasoning have yet to be thoroughly explored. In
this paper, we investigate two properties of prioritized logic programs in the
context of answer set semantics. Specifically, we reveal a close relationship
between mutual defeasibility and uniqueness of the answer set for a prioritized
logic program. We then explore how the splitting technique for extended logic
programs can be extended to prioritized logic programs. We prove splitting
theorems that can be used to simplify the evaluation of a prioritized logic
program under certain conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203008</id><created>2002-03-06</created><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Computational Geometry Column 43</title><categories>cs.CG cs.DM</categories><comments>3 pages, 1 figure</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>SIGACT News, 33(1) Issue 122, Mar. 2002, 58-60</journal-ref><abstract>  The concept of pointed pseudo-triangulations is defined and a few of its
applications described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203009</id><created>2002-03-06</created><authors><author><keyname>Matlin</keyname><forenames>O. S.</forenames></author><author><keyname>Lusk</keyname><forenames>E.</forenames></author><author><keyname>McCune</keyname><forenames>W.</forenames></author></authors><title>SPINning Parallel Systems Software</title><categories>cs.LO cs.DC</categories><comments>19 pages; 8 figures; 3 tables</comments><report-no>ANL/MCS-P921-1201</report-no><acm-class>F.3.1; D.1.3</acm-class><abstract>  We describe our experiences in using SPIN to verify parts of the Multi
Purpose Daemon (MPD) parallel process management system. MPD is a distributed
collection of processes connected by Unix network sockets. MPD is dynamic:
processes and connections among them are created and destroyed as MPD is
initialized, runs user processes, recovers from faults, and terminates. This
dynamic nature is easily expressible in the SPIN/PROMELA framework but poses
performance and scalability challenges. We present here the results of
expressing some of the parallel algorithms of MPD and executing both simulation
and verification runs with SPIN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203010</id><created>2002-03-07</created><authors><author><keyname>Nunes</keyname><forenames>L.</forenames></author><author><keyname>Oliveira</keyname><forenames>E.</forenames></author></authors><title>On Learning by Exchanging Advice</title><categories>cs.LG cs.MA</categories><comments>12 pages, 6 figures, 1 table, accepted in Second Symposium on
  Adaptive Agents and Multi-Agent Systems (AAMAS-II), 2002</comments><acm-class>I.2.6; I.2.11</acm-class><abstract>  One of the main questions concerning learning in Multi-Agent Systems is:
(How) can agents benefit from mutual interaction during the learning process?.
This paper describes the study of an interactive advice-exchange mechanism as a
possible way to improve agents' learning performance. The advice-exchange
technique, discussed here, uses supervised learning (backpropagation), where
reinforcement is not directly coming from the environment but is based on
advice given by peers with better performance score (higher confidence), to
enhance the performance of a heterogeneous group of Learning Agents (LAs). The
LAs are facing similar problems, in an environment where only reinforcement
information is available. Each LA applies a different, well known, learning
technique: Random Walk (hill-climbing), Simulated Annealing, Evolutionary
Algorithms and Q-Learning. The problem used for evaluation is a simplified
traffic-control simulation. Initial results indicate that advice-exchange can
improve learning speed, although bad advice and/or blind reliance can disturb
the learning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203011</id><created>2002-03-08</created><authors><author><keyname>Middleton</keyname><forenames>S. E.</forenames></author><author><keyname>De Roure</keyname><forenames>D. C.</forenames></author><author><keyname>Shadbolt</keyname><forenames>N. R.</forenames></author></authors><title>Capturing Knowledge of User Preferences: ontologies on recommender
  systems</title><categories>cs.LG cs.MA</categories><comments>First international conference on Knowledge Capture 2001, 8 pages</comments><acm-class>I.2.6;I.2.11</acm-class><abstract>  Tools for filtering the World Wide Web exist, but they are hampered by the
difficulty of capturing user preferences in such a dynamic environment. We
explore the acquisition of user profiles by unobtrusive monitoring of browsing
behaviour and application of supervised machine-learning techniques coupled
with an ontological representation to extract user preferences. A multi-class
approach to paper classification is used, allowing the paper topic taxonomy to
be utilised during profile construction. The Quickstep recommender system is
presented and two empirical studies evaluate it in a real work setting,
measuring the effectiveness of using a hierarchical topic ontology compared
with an extendable flat list.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203012</id><created>2002-03-08</created><authors><author><keyname>Middleton</keyname><forenames>Stuart E.</forenames></author></authors><title>Interface agents: A review of the field</title><categories>cs.MA cs.LG</categories><comments>Technical report, 28 pages, aug 2001</comments><report-no>ECSTR-IAM01-001</report-no><acm-class>I.2.11;I.2.6</acm-class><abstract>  This paper reviews the origins of interface agents, discusses challenges that
exist within the interface agent field and presents a survey of current
attempts to find solutions to these challenges. A history of agent systems from
their birth in the 1960's to the current day is described, along with the
issues they try to address. A taxonomy of interface agent systems is presented,
and today's agent systems categorized accordingly. Lastly, an analysis of the
machine learning and user modelling techniques used by today's agents is
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203013</id><created>2002-03-11</created><authors><author><keyname>Maynard-Reid</keyname><forenames>Pedrito</forenames><suffix>II</suffix><affiliation>Miami University</affiliation></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames><affiliation>Hebrew University</affiliation></author></authors><title>Representing and Aggregating Conflicting Beliefs</title><categories>cs.AI cs.LO</categories><comments>19 pages, 5 figures, appears (without proofs) in Proceedings of the
  Seventh International Conference on Principles of Knowledge Representation
  and Reasoning (KR 2000)</comments><acm-class>I.2.4; I.2.11</acm-class><journal-ref>Proceedings of the Seventh International Conference on Principles
  of Knowledge Representation and Reasoning (KR 2000), April 2000, pp. 153-164</journal-ref><abstract>  We consider the two-fold problem of representing collective beliefs and
aggregating these beliefs. We propose modular, transitive relations for
collective beliefs. They allow us to represent conflicting opinions and they
have a clear semantics. We compare them with the quasi-transitive relations
often used in Social Choice. Then, we describe a way to construct the belief
state of an agent informed by a set of sources of varying degrees of
reliability. This construction circumvents Arrow's Impossibility Theorem in a
satisfactory manner. Finally, we give a simple set-theory-based operator for
combining the information of multiple agents. We show that this operator
satisfies the desirable invariants of idempotence, commutativity, and
associativity, and, thus, is well-behaved when iterated, and we describe a
computationally effective way of computing the resulting belief state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203014</id><created>2002-03-11</created><authors><author><keyname>Bush</keyname><forenames>Stephen F.</forenames></author></authors><title>Active Virtual Network Management Prediction: Complexity as a Framework
  for Prediction, Optimization, and Assurance</title><categories>cs.CC cs.NI</categories><acm-class>D.2.8;C.2.0;C.2.1;C.2.2;C.2.3</acm-class><journal-ref>IEEE Computer Society Press, Proceedings of the 2002 DARPA Active
  Networks Conference and Exposition (DANCE 2002), May 29-31, 2002, San
  Francisco, California, USA</journal-ref><abstract>  Research into active networking has provided the incentive to re-visit what
has traditionally been classified as distinct properties and characteristics of
information transfer such as protocol versus service; at a more fundamental
level this paper considers the blending of computation and communication by
means of complexity. The specific service examined in this paper is network
self-prediction enabled by Active Virtual Network Management Prediction.
Computation/communication is analyzed via Kolmogorov Complexity. The result is
a mechanism to understand and improve the performance of active networking and
Active Virtual Network Management Prediction in particular. The Active Virtual
Network Management Prediction mechanism allows information, in various states
of algorithmic and static form, to be transported in the service of prediction
for network management. The results are generally applicable to algorithmic
transmission of information. Kolmogorov Complexity is used and experimentally
validated as a theory describing the relationship among algorithmic
compression, complexity, and prediction accuracy within an active network.
Finally, the paper concludes with a complexity-based framework for Information
Assurance that attempts to take a holistic view of vulnerability analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203015</id><created>2002-03-12</created><authors><author><keyname>Jones</keyname><forenames>Cameron L</forenames></author></authors><title>Towards Experimental Nanosound Using Almost Disjoint Set Theory</title><categories>cs.SD cs.LO</categories><comments>20 pages, 4 figures, 1 table</comments><acm-class>H.5.5</acm-class><abstract>  Music composition using digital audio sequence editors is increasingly
performed in a visual workspace where sound complexes are built from discrete
sound objects, called gestures that are arranged in time and space to generate
a continuous composition. The visual workspace, common to most industry
standard audio loop sequencing software, is premised on the arrangement of
gestures defined with geometric shape properties. Here, one aspect of fractal
set theory was validated using audio-frequency sets to evaluate self-affine
scaling behavior when new sound complexes are built through union and
intersection operations on discrete musical gestures. Results showed that
intersection of two sets revealed lower complexity compared with the union
operator, meaning that the intersection of two sound gestures is an almost
disjoint set, and in accord with formal logic. These results are also discussed
with reference to fuzzy sets, cellular automata, nanotechnology and
self-organization to further explore the link between sequenced notation and
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203016</id><created>2002-03-12</created><authors><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author></authors><title>Dimension in Complexity Classes</title><categories>cs.CC</categories><comments>24 pages</comments><acm-class>F.1.3</acm-class><abstract>  A theory of resource-bounded dimension is developed using gales, which are
natural generalizations of martingales. When the resource bound \Delta (a
parameter of the theory) is unrestricted, the resulting dimension is precisely
the classical Hausdorff dimension (sometimes called fractal dimension). Other
choices of the parameter \Delta yield internal dimension theories in E, E2,
ESPACE, and other complexity classes, and in the class of all decidable
problems. In general, if C is such a class, then every set X of languages has a
dimension in C, which is a real number dim(X|C) in [0,1]. Along with the
elements of this theory, two preliminary applications are presented:
  1. For every real number \alpha in (0,1/2), the set FREQ(&lt;=\alpha),
consisting of all languages that asymptotically contain at most \alpha of all
strings, has dimension H(\alpha) -- the binary entropy of \alpha -- in E and in
E2.
  2. For every real number \alpha in (0,1), the set SIZE(\alpha* (2^n)/n),
consisting of all languages decidable by Boolean circuits of at most
\alpha*(2^n)/n gates, has dimension \alpha in ESPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203017</id><created>2002-03-12</created><authors><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author></authors><title>The Dimensions of Individual Strings and Sequences</title><categories>cs.CC</categories><comments>31 pages</comments><acm-class>F.1.3</acm-class><abstract>  A constructive version of Hausdorff dimension is developed using constructive
supergales, which are betting strategies that generalize the constructive
supermartingales used in the theory of individual random sequences. This
constructive dimension is used to assign every individual (infinite, binary)
sequence S a dimension, which is a real number dim(S) in the interval [0,1].
Sequences that are random (in the sense of Martin-Lof) have dimension 1, while
sequences that are decidable, \Sigma^0_1, or \Pi^0_1 have dimension 0. It is
shown that for every \Delta^0_2-computable real number \alpha in [0,1] there is
a \Delta^0_2 sequence S such that \dim(S) = \alpha.
  A discrete version of constructive dimension is also developed using
termgales, which are supergale-like functions that bet on the terminations of
(finite, binary) strings as well as on their successive bits. This discrete
dimension is used to assign each individual string w a dimension, which is a
nonnegative real number dim(w). The dimension of a sequence is shown to be the
limit infimum of the dimensions of its prefixes.
  The Kolmogorov complexity of a string is proven to be the product of its
length and its dimension. This gives a new characterization of algorithmic
information and a new proof of Mayordomo's recent theorem stating that the
dimension of a sequence is the limit infimum of the average Kolmogorov
complexity of its first n bits.
  Every sequence that is random relative to any computable sequence of
coin-toss biases that converge to a real number \beta in (0,1) is shown to have
dimension \H(\beta), the binary entropy of \beta.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203018</id><created>2002-03-13</created><authors><author><keyname>Buchsbaum</keyname><forenames>Adam L.</forenames></author><author><keyname>Fowler</keyname><forenames>Glenn S.</forenames></author><author><keyname>Giancarlo</keyname><forenames>Raffaele</forenames></author></authors><title>Improving Table Compression with Combinatorial Optimization</title><categories>cs.DS</categories><comments>22 pages, 2 figures, 5 tables, 23 references. Extended abstract
  appears in Proc. 13th ACM-SIAM SODA, pp. 213-222, 2002</comments><acm-class>E.4; F.1.3; F.2.2; G.2.1; H.1.1; H.1.8; H.2.7</acm-class><journal-ref>JACM 50(6):825-851, 2003</journal-ref><abstract>  We study the problem of compressing massive tables within the
partition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which
a table is partitioned by an off-line training procedure into disjoint
intervals of columns, each of which is compressed separately by a standard,
on-line compressor like gzip. We provide a new theory that unifies previous
experimental observations on partitioning and heuristic observations on column
permutation, all of which are used to improve compression rates. Based on the
theory, we devise the first on-line training algorithms for table compression,
which can be applied to individual files, not just continuously operating
sources; and also a new, off-line training algorithm, based on a link to the
asymmetric traveling salesman problem, which improves on prior work by
rearranging columns prior to partitioning. We demonstrate these results
experimentally. On various test files, the on-line algorithms provide 35-55%
improvement over gzip with negligible slowdown; the off-line reordering
provides up to 20% further improvement over partitioning alone. We also show
that a variation of the table compression problem is MAX-SNP hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203019</id><created>2002-03-13</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Murshed</keyname><forenames>Manzur</forenames></author></authors><title>GridSim: A Toolkit for the Modeling and Simulation of Distributed
  Resource Management and Scheduling for Grid Computing</title><categories>cs.DC</categories><acm-class>C5</acm-class><journal-ref>Concurrency and Computation: Practice and Experience, Wiley, May
  2002</journal-ref><abstract>  Clusters, grids, and peer-to-peer (P2P) networks have emerged as popular
paradigms for next generation parallel and distributed computing. The
management of resources and scheduling of applications in such large-scale
distributed systems is a complex undertaking. In order to prove the
effectiveness of resource brokers and associated scheduling algorithms, their
performance needs to be evaluated under different scenarios such as varying
number of resources and users with different requirements. In a grid
environment, it is hard and even impossible to perform scheduler performance
evaluation in a repeatable and controllable manner as resources and users are
distributed across multiple organizations with their own policies. To overcome
this limitation, we have developed a Java-based discrete-event grid simulation
toolkit called GridSim. The toolkit supports modeling and simulation of
heterogeneous grid resources (both time- and space-shared), users and
application models. It provides primitives for creation of application tasks,
mapping of tasks to resources, and their management. To demonstrate suitability
of the GridSim toolkit, we have simulated a Nimrod-G like grid resource broker
and evaluated the performance of deadline and budget constrained cost- and
time-minimization scheduling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203020</id><created>2002-03-13</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Murshed</keyname><forenames>Manzur</forenames></author></authors><title>A Deadline and Budget Constrained Cost-Time Optimisation Algorithm for
  Scheduling Task Farming Applications on Global Grids</title><categories>cs.DC</categories><acm-class>C5</acm-class><journal-ref>Technical Report, Monash University, March 2002</journal-ref><abstract>  Computational Grids and peer-to-peer (P2P) networks enable the sharing,
selection, and aggregation of geographically distributed resources for solving
large-scale problems in science, engineering, and commerce. The management and
composition of resources and services for scheduling applications, however,
becomes a complex undertaking. We have proposed a computational economy
framework for regulating the supply and demand for resources and allocating
them for applications based on the users quality of services requirements. The
framework requires economy driven deadline and budget constrained (DBC)
scheduling algorithms for allocating resources to application jobs in such a
way that the users requirements are met. In this paper, we propose a new
scheduling algorithm, called DBC cost-time optimisation, which extends the DBC
cost-optimisation algorithm to optimise for time, keeping the cost of
computation at the minimum. The superiority of this new scheduling algorithm,
in achieving lower job completion time, is demonstrated by simulating the
World-Wide Grid and scheduling task-farming applications for different deadline
and budget scenarios using both this new and the cost optimisation scheduling
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203021</id><created>2002-03-17</created><authors><author><keyname>Goldman</keyname><forenames>Claudia V.</forenames></author><author><keyname>Gang</keyname><forenames>Dan</forenames></author><author><keyname>Rosenschein</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>NetNeg: A Connectionist-Agent Integrated System for Representing Musical
  Knowledge</title><categories>cs.AI cs.MA</categories><comments>21 pages, 3 figures, Preliminary versions presented at International
  Computer Music Conference, pp. 133-140, Hong Kong (China), August 1996 and
  AAAI Spring Symposium, 1999</comments><acm-class>I.2.6; J.5</acm-class><journal-ref>Annals of Mathematics and Artificial Intelligence, 25 (1999) pp.
  69-90</journal-ref><abstract>  The system presented here shows the feasibility of modeling the knowledge
involved in a complex musical activity by integrating sub-symbolic and symbolic
processes. This research focuses on the question of whether there is any
advantage in integrating a neural network together with a distributed
artificial intelligence approach within the music domain. The primary purpose
of our work is to design a model that describes the different aspects a user
might be interested in considering when involved in a musical activity. The
approach we suggest in this work enables the musician to encode his knowledge,
intuitions, and aesthetic taste into different modules. The system captures
these aspects by computing and applying three distinct functions: rules, fuzzy
concepts, and learning.
  As a case study, we began experimenting with first species two-part
counterpoint melodies. We have developed a hybrid system composed of a
connectionist module and an agent-based module to combine the sub-symbolic and
symbolic levels to achieve this task. The technique presented here to represent
musical knowledge constitutes a new approach for composing polyphonic music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203022</id><created>2002-03-18</created><authors><author><keyname>Howe</keyname><forenames>Jacob M.</forenames></author><author><keyname>King</keyname><forenames>Andy</forenames></author></authors><title>Three Optimisations for Sharing</title><categories>cs.PL</categories><comments>To appear in Theiry and Practice of Logic Programming</comments><acm-class>D.1.6; f.3.2</acm-class><abstract>  In order to improve precision and efficiency sharing analysis should track
both freeness and linearity. The abstract unification algorithms for these
combined domains are suboptimal, hence there is scope for improving precision.
This paper proposes three optimisations for tracing sharing in combination with
freeness and linearity. A novel connection between equations and sharing
abstractions is used to establish correctness of these optimisations even in
the presence of rational trees. A method for pruning intermediate sharing
abstractions to improve efficiency is also proposed. The optimisations are
lightweight and therefore some, if not all, of these optimisations will be of
interest to the implementor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203023</id><created>2002-03-19</created><authors><author><keyname>Lyback</keyname><forenames>David</forenames></author><author><keyname>Boman</keyname><forenames>Magnus</forenames></author></authors><title>Agent trade servers in financial exchange systems</title><categories>cs.CE</categories><comments>11 pages, 1 figure</comments><acm-class>I.2.11; J.4; K.4.4</acm-class><abstract>  New services based on the best-effort paradigm could complement the current
deterministic services of an electronic financial exchange. Four crucial
aspects of such systems would benefit from a hybrid stance: proper use of
processing resources, bandwidth management, fault tolerance, and exception
handling. We argue that a more refined view on Quality-of-Service control for
exchange systems, in which the principal ambition of upholding a fair and
orderly marketplace is left uncompromised, would benefit all interested
parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203024</id><created>2002-03-20</created><authors><author><keyname>Chakrabarti</keyname><forenames>Soumen</forenames></author><author><keyname>Joshi</keyname><forenames>Mukul M.</forenames></author><author><keyname>Punera</keyname><forenames>Kunal</forenames></author><author><keyname>Pennock</keyname><forenames>David M.</forenames></author></authors><title>The structure of broad topics on the Web</title><categories>cs.IR cs.DL</categories><comments>PDF, HTML, LaTeX source, all images</comments><acm-class>H.5.4; H.5.3; H.1.0</acm-class><abstract>  The Web graph is a giant social network whose properties have been measured
and modeled extensively in recent years. Most such studies concentrate on the
graph structure alone, and do not consider textual properties of the nodes.
Consequently, Web communities have been characterized purely in terms of graph
structure and not on page content. We propose that a topic taxonomy such as
Yahoo! or the Open Directory provides a useful framework for understanding the
structure of content-based clusters and communities. In particular, using a
topic taxonomy and an automatic classifier, we can measure the background
distribution of broad topics on the Web, and analyze the capability of recent
random walk algorithms to draw samples which follow such distributions. In
addition, we can measure the probability that a page about one broad topic will
link to another broad topic. Extending this experiment, we can measure how
quickly topic context is lost while walking randomly on the Web graph.
Estimates of this topic mixing distance may explain why a global PageRank is
still meaningful in the context of broad queries. In general, our measurements
may prove valuable in the design of community-specific crawlers and link-based
ranking systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203025</id><created>2002-03-20</created><authors><author><keyname>Bremner</keyname><forenames>David</forenames></author><author><keyname>Golynski</keyname><forenames>Alexander</forenames></author></authors><title>Sufficiently Fat Polyhedra are not 2-castable</title><categories>cs.CG</categories><comments>5 pages, 1 figure</comments><acm-class>F.2.2; J.6</acm-class><abstract>  In this note we consider the problem of manufacturing a convex polyhedral
object via casting. We consider a generalization of the sand casting process
where the object is manufactured by gluing together two identical faces of
parts cast with a single piece mold. In this model we show that the class of
convex polyhedra which can be enclosed between two concentric spheres of the
ratio of their radii less than 1.07 cannot be manufactured using only two cast
parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203026</id><created>2002-03-22</created><authors><author><keyname>Doran</keyname><forenames>Chris</forenames></author><author><keyname>Lasenby</keyname><forenames>Anthony</forenames></author><author><keyname>Lasenby</keyname><forenames>Joan</forenames></author></authors><title>Conformal Geometry, Euclidean Space and Geometric Algebra</title><categories>cs.CG cs.GR math.MG</categories><comments>Proceedings, &quot;Uncertainty in Geometric Computations&quot;, Sheffield 2001</comments><acm-class>I.3.5;I.3.6</acm-class><abstract>  Projective geometry provides the preferred framework for most implementations
of Euclidean space in graphics applications. Translations and rotations are
both linear transformations in projective geometry, which helps when it comes
to programming complicated geometrical operations. But there is a fundamental
weakness in this approach - the Euclidean distance between points is not
handled in a straightforward manner. Here we discuss a solution to this
problem, based on conformal geometry. The language of geometric algebra is best
suited to exploiting this geometry, as it handles the interior and exterior
products in a single, unified framework. A number of applications are
discussed, including a compact formula for reflecting a line off a general
spherical surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203027</id><created>2002-03-26</created><authors><author><keyname>Zheng</keyname><forenames>Qingguo</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Ma</keyname><forenames>Shilong</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author></authors><title>The Algorithms of Updating Sequential Patterns</title><categories>cs.DB cs.AI</categories><comments>12 pages, 4 figures</comments><acm-class>H.2.8</acm-class><journal-ref>The Second SIAM Data mining2002: workshop HPDM</journal-ref><abstract>  Because the data being mined in the temporal database will evolve with time,
many researchers have focused on the incremental mining of frequent sequences
in temporal database. In this paper, we propose an algorithm called IUS, using
the frequent and negative border sequences in the original database for
incremental sequence mining. To deal with the case where some data need to be
updated from the original database, we present an algorithm called DUS to
maintain sequential patterns in the updated database. We also define the
negative border sequence threshold: Min_nbd_supp to control the number of
sequences in the negative border.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203028</id><created>2002-03-27</created><updated>2003-01-26</updated><authors><author><keyname>Zheng</keyname><forenames>Qingguo</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Ma</keyname><forenames>Shilong</forenames></author></authors><title>When to Update the sequential patterns of stream data?</title><categories>cs.DB cs.AI</categories><comments>12 pages, 5 figures</comments><report-no>NLSDE_01_2002_3_27</report-no><acm-class>H.2.8</acm-class><abstract>  In this paper, we first define a difference measure between the old and new
sequential patterns of stream data, which is proved to be a distance. Then we
propose an experimental method, called TPD (Tradeoff between Performance and
Difference), to decide when to update the sequential patterns of stream data by
making a tradeoff between the performance of increasingly updating algorithms
and the difference of sequential patterns. The experiments for the incremental
updating algorithm IUS on two data sets show that generally, as the size of
incremental windows grows, the values of the speedup and the values of the
difference will decrease and increase respectively. It is also shown
experimentally that the incremental ratio determined by the TPD method does not
monotonically increase or decrease but changes in a range between 20 and 30
percentage for the IUS algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203029</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203029</id><created>2002-03-27</created><updated>2013-05-13</updated><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Forbidden Information</title><categories>cs.CC</categories><comments>8 pages, minor changes</comments><acm-class>F.1</acm-class><journal-ref>JACM, 60(2), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goedel Incompleteness Theorem leaves open a way around it, vaguely perceived
for a long time but not clearly identified. (Thus, Goedel believed informal
arguments can answer any math question.) Closing this loophole does not seem
obvious and involves Kolmogorov complexity. (This is unrelated to, well studied
before, complexity quantifications of the usual Goedel effects.) I consider
extensions U of the universal partial recursive predicate (or, say, Peano
Arithmetic). I prove that any U either leaves an n-bit input (statement)
unresolved or contains nearly all information about the n-bit prefix of any
r.e. real r (which is n bits for some r). I argue that creating significant
information about a SPECIFIC math sequence is impossible regardless of the
methods used. Similar problems and answers apply to other unsolvability results
for tasks allowing multiple solutions, e.g. non-recursive tilings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0203030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0203030</id><created>2002-03-28</created><updated>2002-03-28</updated><authors><author><keyname>Andrews</keyname><forenames>Matthew</forenames></author><author><keyname>Fernandez</keyname><forenames>Antonio</forenames></author><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Zhang</keyname><forenames>Lisa</forenames></author></authors><title>Source Routing and Scheduling in Packet Networks</title><categories>cs.NI cs.DC</categories><comments>A preliminary version of this paper appeared in the Proceedings of
  the 42th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2001</comments><acm-class>C.2.1; C.2.6; F.2.2</acm-class><abstract>  We study {\em routing} and {\em scheduling} in packet-switched networks. We
assume an adversary that controls the injection time, source, and destination
for each packet injected. A set of paths for these packets is {\em admissible}
if no link in the network is overloaded. We present the first on-line routing
algorithm that finds a set of admissible paths whenever this is feasible. Our
algorithm calculates a path for each packet as soon as it is injected at its
source using a simple shortest path computation. The length of a link reflects
its current congestion. We also show how our algorithm can be implemented under
today's Internet routing paradigms.
  When the paths are known (either given by the adversary or computed as above)
our goal is to schedule the packets along the given paths so that the packets
experience small end-to-end delays. The best previous delay bounds for
deterministic and distributed scheduling protocols were exponential in the path
length. In this paper we present the first deterministic and distributed
scheduling protocol that guarantees a polynomial end-to-end delay for every
packet.
  Finally, we discuss the effects of combining routing with scheduling. We
first show that some unstable scheduling protocols remain unstable no matter
how the paths are chosen. However, the freedom to choose paths can make a
difference. For example, we show that a ring with parallel links is stable for
all greedy scheduling protocols if paths are chosen intelligently, whereas this
is not the case if the adversary specifies the paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204001</identifier>
 <datestamp>2011-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204001</id><created>2002-03-29</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Joseph</forenames></author></authors><title>A steady state model for graph power laws</title><categories>cs.DM cond-mat.dis-nn cs.SI</categories><comments>8 pages, 5 figures</comments><acm-class>G.2.2; G.3</acm-class><abstract>  Power law distribution seems to be an important characteristic of web graphs.
Several existing web graph models generate power law graphs by adding new
vertices and non-uniform edge connectivities to existing graphs. Researchers
have conjectured that preferential connectivity and incremental growth are both
required for the power law distribution. In this paper, we propose a different
web graph model with power law distribution that does not require incremental
growth. We also provide a comparison of our model with several others in their
ability to predict web graph clustering behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204002</id><created>2002-03-30</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Verrill</keyname><forenames>Helena A.</forenames></author></authors><title>Coin-Moving Puzzles</title><categories>cs.DM cs.CG cs.GT</categories><comments>25 pages, 33 figures. To appear in the book More Games of No Chance
  edited by Richard Nowakowski and published by MSRI</comments><acm-class>G.2.1; G.2.2; F.2.2; I.3.5</acm-class><abstract>  We introduce a new family of one-player games, involving the movement of
coins from one configuration to another. Moves are restricted so that a coin
can be placed only in a position that is adjacent to at least two other coins.
The goal of this paper is to specify exactly which of these games are solvable.
By introducing the notion of a constant number of extra coins, we give tight
theorems characterizing solvable puzzles on the square grid and
equilateral-triangle grid. These existence results are supplemented by
polynomial-time algorithms for finding a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204003</id><created>2002-04-02</created><authors><author><keyname>Levin</keyname><forenames>David N.</forenames><affiliation>U. of Chicago</affiliation></author></authors><title>Blind Normalization of Speech From Different Channels and Speakers</title><categories>cs.CL</categories><comments>4 pages, 2 figures</comments><acm-class>I.2.7</acm-class><abstract>  This paper describes representations of time-dependent signals that are
invariant under any invertible time-independent transformation of the signal
time series. Such a representation is created by rescaling the signal in a
non-linear dynamic manner that is determined by recently encountered signal
levels. This technique may make it possible to normalize signals that are
related by channel-dependent and speaker-dependent transformations, without
having to characterize the form of the signal transformations, which remain
unknown. The technique is illustrated by applying it to the time-dependent
spectra of speech that has been filtered to simulate the effects of different
channels. The experimental results show that the rescaled speech
representations are largely normalized (i.e., channel-independent), despite the
channel-dependence of the raw (unrescaled) speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204004</id><created>2002-04-03</created><authors><author><keyname>Ma</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Lee</keyname><forenames>Haejoong</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Maeda</keyname><forenames>Kazuaki</forenames></author></authors><title>Models and Tools for Collaborative Annotation</title><categories>cs.CL cs.SD</categories><comments>8 pages, 6 figures</comments><acm-class>H.2.4; H.5.3; H.5.5; I.2.7</acm-class><journal-ref>Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</journal-ref><abstract>  The Annotation Graph Toolkit (AGTK) is a collection of software which
facilitates development of linguistic annotation tools. AGTK provides a
database interface which allows applications to use a database server for
persistent storage. This paper discusses various modes of collaborative
annotation and how they can be supported with tools built using AGTK and its
database interface. We describe the relational database schema and API, and
describe a version of the TableTrans tool which supports collaborative
annotation. The remainder of the paper discusses a high-level query language
for annotation graphs, along with optimizations, in support of expressive and
efficient access to the annotations held on a large central server. The paper
demonstrates that it is straightforward to support a variety of different
levels of collaborative annotation with existing AGTK-based tools, with a
minimum of additional programming effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204005</id><created>2002-04-03</created><authors><author><keyname>Maeda</keyname><forenames>Kazuaki</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Lee</keyname><forenames>Haejoong</forenames></author></authors><title>Creating Annotation Tools with the Annotation Graph Toolkit</title><categories>cs.CL cs.SD</categories><comments>8 pages, 12 figures</comments><acm-class>D.2.13; H.5.5; I.2.7</acm-class><journal-ref>Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</journal-ref><abstract>  The Annotation Graph Toolkit is a collection of software supporting the
development of annotation tools based on the annotation graph model. The
toolkit includes application programming interfaces for manipulating annotation
graph data and for importing data from other formats. There are interfaces for
the scripting languages Tcl and Python, a database interface, specialized
graphical user interfaces for a variety of annotation tasks, and several sample
applications. This paper describes all the toolkit components for the benefit
of would-be application developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204006</id><created>2002-04-03</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Maeda</keyname><forenames>Kazuaki</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Lee</keyname><forenames>Haejoong</forenames></author><author><keyname>Randall</keyname><forenames>Beth</forenames></author><author><keyname>Zayat</keyname><forenames>Salim</forenames></author></authors><title>TableTrans, MultiTrans, InterTrans and TreeTrans: Diverse Tools Built on
  the Annotation Graph Toolkit</title><categories>cs.CL cs.SD</categories><comments>7 pages, 7 figures</comments><acm-class>D.2.13; H.5.5; I.2.7</acm-class><journal-ref>Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</journal-ref><abstract>  Four diverse tools built on the Annotation Graph Toolkit are described. Each
tool associates linguistic codes and structures with time-series data. All are
based on the same software library and tool architecture. TableTrans is for
observational coding, using a spreadsheet whose rows are aligned to a signal.
MultiTrans is for transcribing multi-party communicative interactions recorded
using multi-channel signals. InterTrans is for creating interlinear text
aligned to audio. TreeTrans is for creating and manipulating syntactic trees.
This work demonstrates that the development of diverse tools and re-use of
software components is greatly facilitated by a common high-level application
programming interface for representing the data and managing input/output,
together with a common architecture for managing the interaction of multiple
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204007</id><created>2002-04-03</created><authors><author><keyname>Cotton</keyname><forenames>Scott</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>An Integrated Framework for Treebanks and Multilayer Annotations</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the Third International Conference on Language
  Resources and Evaluation, Paris: European Language Resources Association,
  2002</journal-ref><abstract>  Treebank formats and associated software tools are proliferating rapidly,
with little consideration for interoperability. We survey a wide variety of
treebank structures and operations, and show how they can be mapped onto the
annotation graph model, and leading to an integrated framework encompassing
tree and non-tree annotations alike. This development opens up new
possibilities for managing and exploiting multilayer annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204008</id><created>2002-04-04</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>The tip-of-the-tongue phenomenon: Irrelevant neural network localization
  or disruption of its interneuron links ?</title><categories>cs.CL cs.AI q-bio.NC q-bio.QM</categories><comments>Proceedings of the VIII All-Russian conference &quot;Neurocomputers and
  their application - 2002&quot; with international participation, held in Moscow on
  21-22 March 2002, 5 pages and 1 Figure</comments><acm-class>H.3.3; I.2.7; J.4</acm-class><abstract>  On the base of recently proposed three-stage quantitative neural network
model of the tip-of-the-tongue (TOT) phenomenon a possibility to occur of TOT
states coursed by neural network interneuron links' disruption has been
studied. Using a numerical example it was found that TOTs coursed by interneron
links' disruption are in (1.5 + - 0.3)x1000 times less probable then those
coursed by irrelevant (incomplete) neural network localization. It was shown
that delayed TOT states' etiology cannot be related to neural network
interneuron links' disruption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204009</id><created>2002-04-04</created><updated>2002-04-26</updated><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author></authors><title>New Results on Monotone Dualization and Generating Hypergraph
  Transversals</title><categories>cs.DS cs.CC</categories><comments>Removed some minor errors. A shorter version of this paper appears
  in: Proceedings of the 34th ACM Symposium on Theory of Computing (STOC-02),
  May 19-21, 2002, Montreal, Quebec, Canada</comments><report-no>INFSYS RR-1843-02-05, Institut f. Informationssysteme, TU Wien,
  April 2002</report-no><acm-class>F.2.2; F.1.3; G.2.1; G.2.2</acm-class><abstract>  We consider the problem of dualizing a monotone CNF (equivalently, computing
all minimal transversals of a hypergraph), whose associated decision problem is
a prominent open problem in NP-completeness. We present a number of new
polynomial time resp. output-polynomial time results for significant cases,
which largely advance the tractability frontier and improve on previous
results. Furthermore, we show that duality of two monotone CNFs can be
disproved with limited nondeterminism. More precisely, this is feasible in
polynomial time with O(chi(n) * log n) suitably guessed bits, where chi(n) is
given by \chi(n)^chi(n) = n; note that chi(n) = o(log n). This result sheds new
light on the complexity of this important problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204010</id><created>2002-04-05</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jerzy</forenames></author></authors><title>On the Computational Complexity of Consistent Query Answers</title><categories>cs.DB</categories><comments>9 pages</comments><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  We consider here the problem of obtaining reliable, consistent information
from inconsistent databases -- databases that do not have to satisfy given
integrity constraints. We use the notion of consistent query answer -- a query
answer which is true in every (minimal) repair of the database. We provide a
complete classification of the computational complexity of consistent answers
to first-order queries w.r.t. functional dependencies and denial constraints.
We show how the complexity depends on the {\em type} of the constraints
considered, their {\em number}, and the {\em size} of the query. We obtain
several new PTIME cases, using new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204011</id><created>2002-04-08</created><authors><author><keyname>Das</keyname><forenames>Abhimanyu</forenames></author><author><keyname>Dutta</keyname><forenames>Deboyjoti</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Fair Stateless Aggregate Traffic Marking using Active Queue Management
  Techniques</title><categories>cs.NI</categories><comments>7 pages</comments><acm-class>C.2.6</acm-class><abstract>  In heterogeneous networks such as today's Internet, the differentiated
services architecture promises to provide QoS guarantees through scalable
service differentiation. Traffic marking is an important component of this
framework. In this paper, we propose two new aggregate markers that are
stateless, scalable and fair. We leverage stateless Active Queue Management
(AQM) algorithms to enable fair and efficient token distribution among
individual flows of an aggregate. The first marker, Probabilistic Aggregate
Marker (PAM), uses the Token Bucket burst size to probabilistically mark
incoming packets to ensure TCP-friendly and proportionally fair marking. The
second marker, Stateless Aggregate Fair Marker (F-SAM) approximates fair
queueing techniques to isolate flows while marking packets of the aggregate. It
distributes tokens evenly among the flows without maintaining per-flow state.
Our simulation results show that our marking strategies show upto 30%
improvement over other commonly used markers while marking flow aggregates.
These improvements are in terms of better average throughput and fairness
indices, in scenarios containing heterogeneous traffic consisting of TCP (both
long lived elephants and short lived mice) and misbehaving UDP flows. As a
bonus, F-SAM helps the mice to win the war against elephants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204012</id><created>2002-04-08</created><authors><author><keyname>Middleton</keyname><forenames>Stuart E.</forenames></author><author><keyname>Alani</keyname><forenames>Harith</forenames></author><author><keyname>De Roure</keyname><forenames>David C.</forenames></author></authors><title>Exploiting Synergy Between Ontologies and Recommender Systems</title><categories>cs.LG cs.MA</categories><comments>Semantic web conference, WWW2002, 10 pages</comments><acm-class>K.3.2;I.2.11</acm-class><abstract>  Recommender systems learn about user preferences over time, automatically
finding things of similar interest. This reduces the burden of creating
explicit queries. Recommender systems do, however, suffer from cold-start
problems where no initial information is available early on upon which to base
recommendations. Semantic knowledge structures, such as ontologies, can provide
valuable domain knowledge and user information. However, acquiring such
knowledge and keeping it up to date is not a trivial task and user interests
are particularly difficult to acquire and maintain. This paper investigates the
synergy between a web-based research paper recommender system and an ontology
containing information automatically extracted from departmental databases
available on the web. The ontology is used to address the recommender systems
cold-start problem. The recommender system addresses the ontology's
interest-acquisition problem. An empirical evaluation of this approach is
conducted and the performance of the integrated systems measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204013</id><created>2002-04-08</created><updated>2002-11-01</updated><authors><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author></authors><title>The Sketch of a Polymorphic Symphony</title><categories>cs.PL</categories><acm-class>D.1.1; D.3.3; I.1.3</acm-class><abstract>  In previous work, we have introduced functional strategies, that is,
first-class generic functions that can traverse into terms of any type while
mixing uniform and type-specific behaviour. In the present paper, we give a
detailed description of one particular Haskell-based model of functional
strategies. This model is characterised as follows. Firstly, we employ
first-class polymorphism as a form of second-order polymorphism as for the mere
types of functional strategies. Secondly, we use an encoding scheme of run-time
type case for mixing uniform and type-specific behaviour. Thirdly, we base all
traversal on a fundamental combinator for folding over constructor
applications.
  Using this model, we capture common strategic traversal schemes in a highly
parameterised style. We study two original forms of parameterisation. Firstly,
we design parameters for the specific control-flow, data-flow and traversal
characteristics of more concrete traversal schemes. Secondly, we use
overloading to postpone commitment to a specific type scheme of traversal. The
resulting portfolio of traversal schemes can be regarded as a challenging
benchmark for setups for typed generic programming.
  The way we develop the model and the suite of traversal schemes, it becomes
clear that parameterised + typed strategic programming is best viewed as a
potent combination of certain bits of parametric, intensional, polytypic, and
ad-hoc polymorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204014</id><created>2002-04-09</created><authors><author><keyname>Monge</keyname><forenames>R. Asensio</forenames><affiliation>U. of Oviedo</affiliation></author><author><keyname>Marco</keyname><forenames>F. Sanchis</forenames><affiliation>U.P. of Madrid</affiliation></author><author><keyname>Cervigon</keyname><forenames>F. Torre</forenames><affiliation>U. of Oviedo</affiliation></author></authors><title>An Assessment of the Consistency for Software Measurement Methods</title><categories>cs.SE</categories><comments>7 pages, 0 figures</comments><acm-class>D.4.8</acm-class><abstract>  Consistency, defined as the requirement that a series of measurements of the
same project carried out by different raters using the same method should
produce similar results, is one of the most important aspects to be taken into
account in the measurement methods of the software. In spite of this, there is
a widespread view that many measurement methods introduce an undesirable amount
of subjectivity in the measurement process. This perception has made several
organizations develop revisions of the standard methods whose main aim is to
improve their consistency by introducing some suitable modifications of those
aspects which are believed to introduce a greater degree of subjectivity.Each
revision of a method must be empirically evaluated to determine to what extent
is the aim of improving its consistency achieved. In this article we will
define an homogeneous statistic intended to describe the consistency level of a
method, and we will develop the statistical analysis which should be carried
out in order to conclude whether or not a measurement method is more consistent
than other one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204015</id><created>2002-04-09</created><authors><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author><author><keyname>Visser</keyname><forenames>Joost</forenames></author></authors><title>Design Patterns for Functional Strategic Programming</title><categories>cs.PL</categories><acm-class>D.1.1; D.2.3; D.2.10</acm-class><abstract>  In previous work, we introduced the fundamentals and a supporting combinator
library for \emph{strategic programming}. This an idiom for generic programming
based on the notion of a \emph{functional strategy}: a first-class generic
function that cannot only be applied to terms of any type, but which also
allows generic traversal into subterms and can be customized with type-specific
behaviour.
  This paper seeks to provide practicing functional programmers with pragmatic
guidance in crafting their own strategic programs. We present the fundamentals
and the support from a user's perspective, and we initiate a catalogue of
\emph{strategy design patterns}. These design patterns aim at consolidating
strategic programming expertise in accessible form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204016</id><created>2002-04-09</created><authors><author><keyname>Giacobazzi</keyname><forenames>R.</forenames></author><author><keyname>Ranzato</keyname><forenames>F.</forenames></author><author><keyname>Scozzari</keyname><forenames>F.</forenames></author></authors><title>Making Abstract Domains Condensing</title><categories>cs.PL cs.LO</categories><comments>20 pages</comments><acm-class>D.3.1; D.3.2; F.3.2</acm-class><abstract>  In this paper we show that reversible analysis of logic languages by abstract
interpretation can be performed without loss of precision by systematically
refining abstract domains. The idea is to include semantic structures into
abstract domains in such a way that the refined abstract domain becomes rich
enough to allow approximate bottom-up and top-down semantics to agree. These
domains are known as condensing abstract domains. Substantially, an abstract
domain is condensing if goal-driven and goal-independent analyses agree, namely
no loss of precision is introduced by approximating queries in a
goal-independent analysis. We prove that condensation is an abstract domain
property and that the problem of making an abstract domain condensing boils
down to the problem of making the domain complete with respect to unification.
In a general abstract interpretation setting we show that when concrete domains
and operations give rise to quantales, i.e. models of propositional linear
logic, objects in a complete refined abstract domain can be explicitly
characterized by linear logic-based formulations. This is the case for abstract
domains for logic program analysis approximating computed answer substitutions
where unification plays the role of multiplicative conjunction in a quantale of
idempotent substitutions. Condensing abstract domains can therefore be
systematically derived by minimally extending any, generally non-condensing
domain, by a simple domain refinement operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204017</id><created>2002-04-09</created><updated>2002-07-30</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Fleischer</keyname><forenames>Rudolf</forenames></author></authors><title>Solitaire Clobber</title><categories>cs.DM cs.CG cs.GT</categories><comments>14 pages. v2 fixes small typo</comments><report-no>HKUST-TCSC-2002-05</report-no><acm-class>G.2.1; G.2.2; F.2.2; I.3.5</acm-class><abstract>  Clobber is a new two-player board game. In this paper, we introduce the
one-player variant Solitaire Clobber where the goal is to remove as many stones
as possible from the board by alternating white and black moves. We show that a
checkerboard configuration on a single row (or single column) can be reduced to
about n/4 stones. For boards with at least two rows and two columns, we show
that a checkerboard configuration can be reduced to a single stone if and only
if the number of stones is not a multiple of three, and otherwise it can be
reduced to two stones. We also show that in general it is NP-complete to decide
whether an arbitrary Clobber configuration can be reduced to a single stone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204018</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204018</id><created>2002-04-09</created><updated>2003-02-24</updated><authors><author><keyname>Kort</keyname><forenames>Jan</forenames></author><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author></authors><title>A Framework for Datatype Transformation</title><categories>cs.PL</categories><comments>Minor revision; now accepted at LDTA 2003</comments><acm-class>D.1.1; D.2.3; D.2.6; D.2.7; D.3.4</acm-class><abstract>  We study one dimension in program evolution, namely the evolution of the
datatype declarations in a program. To this end, a suite of basic
transformation operators is designed. We cover structure-preserving
refactorings, but also structure-extending and -reducing adaptations. Both the
object programs that are subject to datatype transformations, and the meta
programs that encode datatype transformations are functional programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204019</id><created>2002-04-09</created><authors><author><keyname>Akcoglu</keyname><forenames>Karhan</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author></authors><title>Fast Universalization of Investment Strategies with Provably Good
  Relative Returns</title><categories>cs.CE cs.DS</categories><comments>23 Pages</comments><acm-class>F.2; G.3; I.2.6</acm-class><abstract>  A universalization of a parameterized investment strategy is an online
algorithm whose average daily performance approaches that of the strategy
operating with the optimal parameters determined offline in hindsight. We
present a general framework for universalizing investment strategies and
discuss conditions under which investment strategies are universalizable. We
present examples of common investment strategies that fit into our framework.
The examples include both trading strategies that decide positions in
individual stocks, and portfolio strategies that allocate wealth among multiple
stocks. This work extends Cover's universal portfolio work. We also discuss the
runtime efficiency of universalization algorithms. While a straightforward
implementation of our algorithms runs in time exponential in the number of
parameters, we show that the efficient universal portfolio computation
technique of Kalai and Vempala involving the sampling of log-concave functions
can be generalized to other classes of investment strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204020</id><created>2002-04-10</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author><author><keyname>Simons</keyname><forenames>Gary</forenames></author></authors><title>Seven Dimensions of Portability for Language Documentation and
  Description</title><categories>cs.CL cs.DL</categories><comments>8 pages</comments><acm-class>H.3.7; I.2.7; J.5</acm-class><journal-ref>Proceedings of the Workshop on Portability Issues in Human
  Language Technologies, Third International Conference on Language Resources
  and Evaluation, Paris: European Language Resources Association, 2002</journal-ref><abstract>  The process of documenting and describing the world's languages is undergoing
radical transformation with the rapid uptake of new digital technologies for
capture, storage, annotation and dissemination. However, uncritical adoption of
new tools and technologies is leading to resources that are difficult to reuse
and which are less portable than the conventional printed resources they
replace. We begin by reviewing current uses of software tools and digital
technologies for language documentation and description. This sheds light on
how digital language documentation and description are created and managed,
leading to an analysis of seven portability problems under the following
headings: content, format, discovery, access, citation, preservation and
rights. After characterizing each problem we provide a series of value
statements, and this provides the framework for a broad range of best practice
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204021</id><created>2002-04-10</created><authors><author><keyname>Dornseif</keyname><forenames>Maximillian</forenames></author><author><keyname>Schumann</keyname><forenames>Kay</forenames></author><author><keyname>Klein</keyname><forenames>Christian</forenames></author></authors><title>Factual and Legal Risks regarding wireless Computer Networks</title><categories>cs.CY cs.CR</categories><acm-class>K.4.1;K.4.2; K.5.0;K.6.5</acm-class><journal-ref>DuD - Datenschutz und Datensicherheit, 4/2002, S. 226ff, Vieweg,
  ISSN 0724-4371</journal-ref><abstract>  The IEEE 802.11b wireless ethernet standart has several serious security
flaws. This paper describes this flaws, surveys wireless networks in the
Cologne/Bonn area to get an assessment of the security configurations of
fielded networks and analizes the legal protections provided to wireless
ethernet operators by german law. We conclude that wireless ethernets without
additional security measures are not usable for any transmissions which are not
meant for a public audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204022</id><created>2002-04-10</created><authors><author><keyname>Cieri</keyname><forenames>Christopher</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure
  for Interdisciplinary Education, Research and Development</title><categories>cs.CL</categories><comments>8 pages, 6 figures</comments><acm-class>H.2.4; H.5.3; H.5.5; I.2.7</acm-class><journal-ref>Proceedings of ACL Workshop on Sharing Tools and Resources for
  Research and Education, Toulouse, July 2001, pp 23-30</journal-ref><abstract>  Annotation graphs and annotation servers offer infrastructure to support the
analysis of human language resources in the form of time-series data such as
text, audio and video. This paper outlines areas of common need among empirical
linguists and computational linguists. After reviewing examples of data and
tools used or under development for each of several areas, it proposes a common
framework for future tool development, data annotation and resource sharing
based upon annotation graphs and servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204023</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204023</id><created>2002-04-10</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Computational Phonology</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7; J.5</acm-class><journal-ref>Oxford International Encyclopedia of Linguistics, 2nd Edition,
  2002</journal-ref><abstract>  Phonology, as it is practiced, is deeply computational. Phonological analysis
is data-intensive and the resulting models are nothing other than specialized
data structures and algorithms. In the past, phonological computation -
managing data and developing analyses - was done manually with pencil and
paper. Increasingly, with the proliferation of affordable computers, IPA fonts
and drawing software, phonologists are seeking to move their computation work
online. Computational Phonology provides the theoretical and technological
framework for this migration, building on methodologies and tools from
computational linguistics. This piece consists of an apology for computational
phonology, a history, and an overview of current research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204024</id><created>2002-04-10</created><updated>2003-05-29</updated><authors><author><keyname>Barvinok</keyname><forenames>Alexander</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Johnson</keyname><forenames>David S.</forenames></author><author><keyname>Tamir</keyname><forenames>Arie</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author><author><keyname>Woodroofe</keyname><forenames>Russ</forenames></author></authors><title>The Geometric Maximum Traveling Salesman Problem</title><categories>cs.DS cs.CC</categories><comments>24 pages, 6 figures; revised to appear in Journal of the ACM.
  (clarified some minor points, fixed typos)</comments><acm-class>F.2.2</acm-class><journal-ref>Journal of the ACM, 50 (5) 2003, 641-664.</journal-ref><abstract>  We consider the traveling salesman problem when the cities are points in R^d
for some fixed d and distances are computed according to geometric distances,
determined by some norm. We show that for any polyhedral norm, the problem of
finding a tour of maximum length can be solved in polynomial time. If
arithmetic operations are assumed to take unit time, our algorithms run in time
O(n^{f-2} log n), where f is the number of facets of the polyhedron determining
the polyhedral norm. Thus for example we have O(n^2 log n) algorithms for the
cases of points in the plane under the Rectilinear and Sup norms. This is in
contrast to the fact that finding a minimum length tour in each case is
NP-hard. Our approach can be extended to the more general case of quasi-norms
with not necessarily symmetric unit ball, where we get a complexity of
O(n^{2f-2} log n).
  For the special case of two-dimensional metrics with f=4 (which includes the
Rectilinear and Sup norms), we present a simple algorithm with O(n) running
time. The algorithm does not use any indirect addressing, so its running time
remains valid even in comparison based models in which sorting requires Omega(n
\log n) time. The basic mechanism of the algorithm provides some intuition on
why polyhedral norms allow fast algorithms.
  Complementing the results on simplicity for polyhedral norms, we prove that
for the case of Euclidean distances in R^d for d&gt;2, the Maximum TSP is NP-hard.
This sheds new light on the well-studied difficulties of Euclidean distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204025</id><created>2002-04-11</created><authors><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Phonology</title><categories>cs.CL</categories><comments>27 pages</comments><acm-class>I.2.7; J.5</acm-class><journal-ref>In Ruslan Mitkov (ed) (2002). Oxford Handbook of Computational
  Linguistics</journal-ref><abstract>  Phonology is the systematic study of the sounds used in language, their
internal structure, and their composition into syllables, words and phrases.
Computational phonology is the application of formal and computational
techniques to the representation and processing of phonological information.
This chapter will present the fundamentals of descriptive phonology along with
a brief overview of computational phonology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204026</id><created>2002-04-11</created><authors><author><keyname>Cassidy</keyname><forenames>Steve</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>Querying Databases of Annotated Speech</title><categories>cs.CL cs.DB</categories><comments>9 pages, 4 figures</comments><acm-class>H.2.3; H.2.4; H.5.5; I.2.7; J.5</acm-class><journal-ref>Database Technologies: Proceedings of the Eleventh Australasian
  Database Conference, pp. 12-20, IEEE Computer Society, 2000</journal-ref><abstract>  Annotated speech corpora are databases consisting of signal data along with
time-aligned symbolic `transcriptions'. Such databases are typically
multidimensional, heterogeneous and dynamic. These properties present a number
of tough challenges for representation and query. The temporal nature of the
data adds an additional layer of complexity. This paper presents and harmonises
two independent efforts to model annotated speech databases, one at Macquarie
University and one at the University of Pennsylvania. Various query languages
are described, along with illustrative applications to a variety of analytical
problems. The research reported here forms a part of several ongoing projects
to develop platform-independent open-source tools for creating, browsing,
searching, querying and transforming linguistic databases, and to disseminate
large linguistic databases over the internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204027</id><created>2002-04-11</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Integrating selectional preferences in WordNet</title><categories>cs.CL</categories><comments>9 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of First International WordNet Conference. Mysore
  (India). 2002</journal-ref><abstract>  Selectional preference learning methods have usually focused on word-to-class
relations, e.g., a verb selects as its subject a given nominal class. This
paper extends previous statistical models to class-to-class preferences, and
presents a model that learns selectional preferences for classes of verbs,
together with an algorithm to integrate the learned preferences in WordNet. The
theoretical motivation is twofold: different senses of a verb may have
different preferences, and classes of verbs may share preferences. On the
practical side, class-to-class selectional preferences can be learned from
untagged corpora (the same as word-to-class), they provide selectional
preferences for less frequent word senses via inheritance, and more important,
they allow for easy integration in WordNet. The model is trained on
subject-verb and object-verb relationships extracted from a small corpus
disambiguated with WordNet senses. Examples are provided illustrating that the
theoretical motivations are well founded, and showing that the approach is
feasible. Experimental results on a word sense disambiguation task are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204028</id><created>2002-04-12</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author></authors><title>Decision Lists for English and Basque</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the SENSEVAL-2 Workshop. In conjunction with
  ACL'2001/EACL'2001. Toulouse</journal-ref><abstract>  In this paper we describe the systems we developed for the English (lexical
and all-words) and Basque tasks. They were all supervised systems based on
Yarowsky's Decision Lists. We used Semcor for training in the English all-words
task. We defined different feature sets for each language. For Basque, in order
to extract all the information from the text, we defined features that have not
been used before in the literature, using a morphological analyzer. We also
implemented systems that selected automatically good features and were able to
obtain a prefixed precision (85%) at the cost of coverage. The systems that
used all the features were identified as BCU-ehu-dlist-all and the systems that
selected some features as BCU-ehu-dlist-best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204029</id><created>2002-04-12</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Garcia</keyname><forenames>Elena</forenames></author><author><keyname>Lersundi</keyname><forenames>Mikel</forenames></author><author><keyname>Martinez</keyname><forenames>David</forenames></author><author><keyname>Pociello</keyname><forenames>Eli</forenames></author></authors><title>The Basque task: did systems perform in the upperbound?</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the SENSEVAL-2 Workshop. In conjunction with
  ACL'2001/EACL'2001. Toulouse</journal-ref><abstract>  In this paper we describe the Senseval 2 Basque lexical-sample task. The task
comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal
Hiztegia, the main Basque dictionary. Most examples were taken from the
Egunkaria newspaper. The method used to hand-tag the examples produced low
inter-tagger agreement (75%) before arbitration. The four competing systems
attained results well above the most frequent baseline and the best system
scored 75% precision at 100% coverage. The paper includes an analysis of the
tagging procedure used, as well as the performance of the competing systems. In
particular, we argue that inter-tagger agreement is not a real upperbound for
the Basque WSD task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204030</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204030</id><created>2002-04-12</created><updated>2002-08-21</updated><authors><author><keyname>Ward</keyname><forenames>David J.</forenames></author><author><keyname>MacKay</keyname><forenames>David J. C.</forenames></author></authors><title>Fast Hands-free Writing by Gaze Direction</title><categories>cs.HC cs.AI</categories><comments>3 pages. Final version</comments><acm-class>H.5.2;K.4.2;H.1.1;H.5.1;I.2.7</acm-class><journal-ref>Nature 418, 2002 p. 838 (22nd August 2002) www.nature.com</journal-ref><doi>10.1038/418838a</doi><abstract>  We describe a method for text entry based on inverse arithmetic coding that
relies on gaze direction and which is faster and more accurate than using an
on-screen keyboard.
  These benefits are derived from two innovations: the writing task is matched
to the capabilities of the eye, and a language model is used to make
predictable words and phrases easier to write.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204031</id><created>2002-04-12</created><authors><author><keyname>Shen</keyname><forenames>Yi-Dong</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author><author><keyname>Yuan</keyname><forenames>Li-Yan</forenames></author><author><keyname>Shen</keyname><forenames>Samuel S. P.</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author></authors><title>A Dynamic Approach to Characterizing Termination of General Logic
  Programs</title><categories>cs.LO cs.PL</categories><comments>To appear in ACM TOCL</comments><acm-class>D.1.6;D.1.2;F.4.1</acm-class><journal-ref>ACM Transactions on Computational Logic 4(4):417-430, 2003</journal-ref><abstract>  We present a new characterization of termination of general logic programs.
Most existing termination analysis approaches rely on some static information
about the structure of the source code of a logic program, such as modes/types,
norms/level mappings, models/interargument relations, and the like. We propose
a dynamic approach which employs some key dynamic features of an infinite
(generalized) SLDNF-derivation, such as repetition of selected subgoals and
recursive increase in term size. We also introduce a new formulation of
SLDNF-trees, called generalized SLDNF-trees. Generalized SLDNF-trees deal with
negative subgoals in the same way as Prolog and exist for any general logic
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204032</id><created>2002-04-14</created><authors><author><keyname>Freund</keyname><forenames>Michael</forenames></author><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Belief Revision and Rational Inference</title><categories>cs.AI</categories><comments>25 pages</comments><report-no>Leibniz Center for Research in Computer Science, Hebrew University:
  TR-94-16, July 1994</report-no><acm-class>I.2.3</acm-class><abstract>  The (extended) AGM postulates for belief revision seem to deal with the
revision of a given theory K by an arbitrary formula, but not to constrain the
revisions of two different theories by the same formula. A new postulate is
proposed and compared with other similar postulates that have been proposed in
the literature. The AGM revisions that satisfy this new postulate stand in
one-to-one correspondence with the rational, consistency-preserving relations.
This correspondence is described explicitly. Two viewpoints on iterative
revisions are distinguished and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204033</id><created>2002-04-15</created><authors><author><keyname>Kiwiel</keyname><forenames>Krzysztof C.</forenames></author></authors><title>Randomized selection revisited</title><categories>cs.DS</categories><comments>25 pages, LaTeX 2e</comments><report-no>PMMO-02-01</report-no><acm-class>F.2.2;G3</acm-class><abstract>  We show that several versions of Floyd and Rivest's algorithm Select for
finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+o(n)$ comparisons on average and with high probability. This
rectifies the analysis of Floyd and Rivest, and extends it to the case of
nondistinct elements. Our computational results confirm that Select may be the
best algorithm in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204034</id><created>2002-04-15</created><authors><author><keyname>Kiniry</keyname><forenames>Joseph R.</forenames></author></authors><title>Monitoring and Debugging Concurrent and Distributed Object-Oriented
  Systems</title><categories>cs.SE</categories><comments>12 pages, 3 figures, submitted to OOPSLA '02</comments><acm-class>D.2; D.2.5</acm-class><abstract>  A major part of debugging, testing, and analyzing a complex software system
is understanding what is happening within the system at run-time. Some
developers advocate running within a debugger to better understand the system
at this level. Others embed logging statements, even in the form of hard-coded
calls to print functions, throughout the code. These techniques are all
general, rough forms of what we call system monitoring, and, while they have
limited usefulness in simple, sequential systems, they are nearly useless in
complex, concurrent ones. We propose a set of new mechanisms, collectively
known as a monitoring system, for understanding such complex systems, and we
describe an example implementation of such a system, called IDebug, for the
Java programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204035</id><created>2002-04-15</created><authors><author><keyname>Kiniry</keyname><forenames>Joseph R.</forenames></author></authors><title>Semantic Properties for Lightweight Specification in Knowledgeable
  Development Environments</title><categories>cs.SE</categories><comments>10 pages, submitted to FSE-10</comments><acm-class>D.1.0; D.2; D.3.1; D.3.2; D.3.4; F.3.1; F.4.1; F.4.3</acm-class><abstract>  Semantic properties are domain-specific specification constructs used to
augment an existing language with richer semantics. These properties are taken
advantage of in system analysis, design, implementation, testing, and
maintenance through the use of documentation and source-code transformation
tools. Semantic properties are themselves specified at two levels: loosely with
precise natural language, and formally within the problem domain. The
refinement relationships between these specification levels, as well as between
a semantic property's use and its realization in program code via tools, is
specified with a new formal method for reuse called kind theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204036</id><created>2002-04-15</created><authors><author><keyname>Kiniry</keyname><forenames>Joseph R.</forenames></author></authors><title>Semantic Component Composition</title><categories>cs.SE</categories><comments>9 pages, submitted to GCSE/SAIG '02</comments><acm-class>D.1.0; D.2; D.3.1; F.3.1; F.4.1; F.4.3</acm-class><abstract>  Building complex software systems necessitates the use of component-based
architectures. In theory, of the set of components needed for a design, only
some small portion of them are &quot;custom&quot;; the rest are reused or refactored
existing pieces of software. Unfortunately, this is an idealized situation.
Just because two components should work together does not mean that they will
work together.
  The &quot;glue&quot; that holds components together is not just technology. The
contracts that bind complex systems together implicitly define more than their
explicit type. These &quot;conceptual contracts&quot; describe essential aspects of
extra-system semantics: e.g., object models, type systems, data representation,
interface action semantics, legal and contractual obligations, and more.
  Designers and developers spend inordinate amounts of time technologically
duct-taping systems to fulfill these conceptual contracts because system-wide
semantics have not been rigorously characterized or codified. This paper
describes a formal characterization of the problem and discusses an initial
implementation of the resulting theoretical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204037</id><created>2002-04-16</created><updated>2004-08-05</updated><authors><author><keyname>Vereshchagin</keyname><forenames>Nikolai</forenames><affiliation>Moscow State University</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Kolmogorov's Structure Functions and Model Selection</title><categories>cs.CC math.PR physics.data-an</categories><comments>25 pages LaTeX, 5 figures. In part in Proc 47th IEEE FOCS; this final
  version (more explanations, cosmetic modifications) to appear in IEEE Trans
  Inform Th</comments><acm-class>E.5, E.4, E.2, H.1.1, F.1.1, F.1.3</acm-class><abstract>  In 1974 Kolmogorov proposed a non-probabilistic approach to statistics and
model selection. Let data be finite binary strings and models be finite sets of
binary strings. Consider model classes consisting of models of given maximal
(Kolmogorov) complexity. The ``structure function'' of the given data expresses
the relation between the complexity level constraint on a model class and the
least log-cardinality of a model in the class containing the data. We show that
the structure function determines all stochastic properties of the data: for
every constrained model class it determines the individual best-fitting model
in the class irrespective of whether the ``true'' model is in the model class
considered or not. In this setting, this happens {\em with certainty}, rather
than with high probability as is in the classical case. We precisely quantify
the goodness-of-fit of an individual model with respect to individual data. We
show that--within the obvious constraints--every graph is realized by the
structure function of some data. We determine the (un)computability properties
of the various functions contemplated and of the ``algorithmic minimal
sufficient statistic.''
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204038</id><created>2002-04-16</created><authors><author><keyname>Lewak</keyname><forenames>Jerzy</forenames></author></authors><title>Technology For Information Engineering (TIE): A New Way of Storing,
  Retrieving and Analyzing Information</title><categories>cs.DB cs.IR</categories><comments>32 pages. Introduces the theoretical foundation for associative
  information</comments><acm-class>E.0</acm-class><abstract>  The theoretical foundations of a new model and paradigm (called TIE) for data
storage and access are introduced. Associations between data elements are
stored in a single Matrix table, which is usually kept entirely in RAM for
quick access. The model ties together a very intuitive &quot;guided&quot; GUI to the
Matrix structure, allowing extremely easy complex searches through the data.
Although it is an &quot;Associative Model&quot; in that it stores the data associations
separately from the data itself, in contrast to other implementations of that
model TIE guides the user to only the available information ensuring that every
search is always fruitful. Very many diverse applications of the technology are
reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204039</id><created>2002-04-16</created><authors><author><keyname>Bloom</keyname><forenames>B.</forenames><affiliation>Cornell -&gt; IBM</affiliation></author><author><keyname>Fokkink</keyname><forenames>W. J.</forenames><affiliation>CWI</affiliation></author><author><keyname>van Glabbeek</keyname><forenames>R. J.</forenames><affiliation>Stanford</affiliation></author></authors><title>Precongruence Formats for Decorated Trace Semantics</title><categories>cs.LO</categories><comments>48 pages. More info at
  http://theory.stanford.edu/~rvg/abstracts.html#48</comments><acm-class>D.3.1; F.1.2; F.3.2</acm-class><abstract>  This paper explores the connection between semantic equivalences and
preorders for concrete sequential processes, represented by means of labelled
transition systems, and formats of transition system specifications using
Plotkin's structural approach. For several preorders in the linear time -
branching time spectrum a format is given, as general as possible, such that
this preorder is a precongruence for all operators specifiable in that format.
The formats are derived using the modal characterizations of the corresponding
preorders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204040</id><created>2002-04-17</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Self-Optimizing and Pareto-Optimal Policies in General Environments
  based on Bayes-Mixtures</title><categories>cs.AI cs.LG math.OC math.PR</categories><comments>15 pages</comments><report-no>IDSIA-04-02</report-no><acm-class>I.2</acm-class><journal-ref>Proceedings of the 15th Annual Conference on Computational
  Learning Theory (COLT-2002) 364-379</journal-ref><abstract>  The problem of making sequential decisions in unknown probabilistic
environments is studied. In cycle $t$ action $y_t$ results in perception $x_t$
and reward $r_t$, where all quantities in general may depend on the complete
history. The perception $x_t$ and reward $r_t$ are sampled from the (reactive)
environmental probability distribution $\mu$. This very general setting
includes, but is not limited to, (partial observable, k-th order) Markov
decision processes. Sequential decision theory tells us how to act in order to
maximize the total expected reward, called value, if $\mu$ is known.
Reinforcement learning is usually used if $\mu$ is unknown. In the Bayesian
approach one defines a mixture distribution $\xi$ as a weighted sum of
distributions $\nu\in\M$, where $\M$ is any class of distributions including
the true environment $\mu$. We show that the Bayes-optimal policy $p^\xi$ based
on the mixture $\xi$ is self-optimizing in the sense that the average value
converges asymptotically for all $\mu\in\M$ to the optimal value achieved by
the (infeasible) Bayes-optimal policy $p^\mu$ which knows $\mu$ in advance. We
show that the necessary condition that $\M$ admits self-optimizing policies at
all, is also sufficient. No other structural assumptions are made on $\M$. As
an example application, we discuss ergodic Markov decision processes, which
allow for self-optimizing policies. Furthermore, we show that $p^\xi$ is
Pareto-optimal in the sense that there is no other policy yielding higher or
equal value in {\em all} environments $\nu\in\M$ and a strictly higher value in
at least one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204041</id><created>2002-04-18</created><authors><author><keyname>Eaves</keyname><forenames>Walter</forenames></author></authors><title>Trust Brokerage Systems for the Internet</title><categories>cs.CR cs.GT cs.NE</categories><comments>Doctoral Thesis. 279 pages, 7 appendices, 18 tables, 44 figures</comments><acm-class>C.2.4; F.4.1; G.2.1; G.3; H.2.4; H.2.7; H.3.5; H.5.3; J.3; K.4.1;
  K.6.5</acm-class><abstract>  This thesis addresses the problem of providing trusted individuals with
confidential information about other individuals, in particular, granting
access to databases of personal records using the World-Wide Web. It proposes
an access rights management system for distributed databases which aims to
create and implement organisation structures based on the wishes of the owners
and of demands of the users of the databases. The dissertation describes how
current software components could be used to implement this system; it
re-examines the theory of collective choice to develop mechanisms for
generating hierarchies of authorities; it analyses organisational processes for
stability and develops a means of measuring the similarity of their
hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204042</id><created>2002-04-19</created><authors><author><keyname>Soss</keyname><forenames>Michael</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Overmars</keyname><forenames>Mark</forenames></author></authors><title>Preprocessing Chains for Fast Dihedral Rotations Is Hard or Even
  Impossible</title><categories>cs.CG</categories><comments>11 pages, 9 figures, see also
  http://www.cs.uiuc.edu/~jeffe/pubs/dihedral.html</comments><acm-class>F.2.2</acm-class><abstract>  We examine a computational geometric problem concerning the structure of
polymers. We model a polymer as a polygonal chain in three dimensions. Each
edge splits the polymer into two subchains, and a dihedral rotation rotates one
of these chains rigidly about this edge. The problem is to determine, given a
chain, an edge, and an angle of rotation, if the motion can be performed
without causing the chain to self-intersect. An Omega(n log n) lower bound on
the time complexity of this problem is known.
  We prove that preprocessing a chain of n edges and answering n dihedral
rotation queries is 3SUM-hard, giving strong evidence that solving n queries
requires Omega(n^2) time in the worst case. For dynamic queries, which also
modify the chain if the requested dihedral rotation is feasible, we show that
answering n queries is by itself 3SUM-hard, suggesting that sublinear query
time is impossible after any amount of preprocessing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204043</id><created>2002-04-20</created><authors><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author></authors><title>Learning from Scarce Experience</title><categories>cs.AI cs.LG cs.NE cs.RO</categories><comments>8 pages 4 figures</comments><acm-class>I.2; I.2.8; I.2.11; I.2.6; G.1.6</acm-class><abstract>  Searching the space of policies directly for the optimal policy has been one
popular method for solving partially observable reinforcement learning
problems. Typically, with each change of the target policy, its value is
estimated from the results of following that very policy. This requires a large
number of interactions with the environment as different polices are
considered. We present a family of algorithms based on likelihood ratio
estimation that use data gathered when executing one policy (or collection of
policies) to estimate the value of a different policy. The algorithms combine
estimation and optimization stages. The former utilizes experience to build a
non-parametric representation of an optimized function. The latter performs
optimization on this estimate. We show positive empirical results and provide
the sample complexity bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204044</id><created>2002-04-20</created><authors><author><keyname>Sanchez</keyname><forenames>Javier Nicolas</forenames></author><author><keyname>Milstein</keyname><forenames>Adam</forenames></author><author><keyname>Williamson</keyname><forenames>Evan</forenames></author></authors><title>Robust Global Localization Using Clustered Particle Filtering</title><categories>cs.RO cs.AI</categories><comments>6 pages. Proceedings of AAAI-2002 (in press)</comments><acm-class>I.2.9</acm-class><abstract>  Global mobile robot localization is the problem of determining a robot's pose
in an environment, using sensor data, when the starting position is unknown. A
family of probabilistic algorithms known as Monte Carlo Localization (MCL) is
currently among the most popular methods for solving this problem. MCL
algorithms represent a robot's belief by a set of weighted samples, which
approximate the posterior probability of where the robot is located by using a
Bayesian formulation of the localization problem. This article presents an
extension to the MCL algorithm, which addresses its problems when localizing in
highly symmetrical environments; a situation where MCL is often unable to
correctly track equally probable poses for the robot. The problem arises from
the fact that sample sets in MCL often become impoverished, when samples are
generated according to their posterior likelihood. Our approach incorporates
the idea of clusters of samples and modifies the proposal distribution
considering the probability mass of those clusters. Experimental results are
presented that show that this new extension to the MCL algorithm successfully
localizes in symmetric environments where ordinary MCL often fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204045</id><created>2002-04-22</created><authors><author><keyname>Ignjatovic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Sharma</keyname><forenames>Arun</forenames></author></authors><title>Some applications of logic to feasibility in higher types</title><categories>cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  In this paper we demonstrate that the class of basic feasible functionals has
recursion theoretic properties which naturally generalize the corresponding
properties of the class of feasible functions. We also improve the Kapron -
Cook result on mashine representation of basic feasible functionals. Our proofs
are based on essential applications of logic. We introduce a weak fragment of
second order arithmetic with second order variables ranging over functions from
N into N which suitably characterizes basic feasible functionals, and show that
it is a useful tool for investigating the properties of basic feasible
functionals. In particular, we provide an example how one can extract feasible
&quot;programs&quot; from mathematical proofs which use non-feasible functionals (like
second order polynomials).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204046</id><created>2002-04-22</created><authors><author><keyname>Fagin</keyname><forenames>Ron</forenames></author><author><keyname>Lotem</keyname><forenames>Amnon</forenames></author><author><keyname>Naor</keyname><forenames>Moni</forenames></author></authors><title>Optimal Aggregation Algorithms for Middleware</title><categories>cs.DB cs.DS</categories><comments>41 pages. Preliminary version appeared in ACM PODS 2001, pp. 102-113</comments><acm-class>H.2.4; F.2.2</acm-class><abstract>  Let D be a database of N objects where each object has m fields. The objects
are given in m sorted lists (where the ith list is sorted according to the ith
field). Our goal is to find the top k objects according to a monotone
aggregation function t, while minimizing access to the lists. The problem
arises in several contexts. In particular Fagin (JCSS 1999) considered it for
the purpose of aggregating information in a multimedia database system.
  We are interested in instance optimality, i.e. that our algorithm will be as
good as any other (correct) algorithm on any instance. We provide and analyze
several instance optimal algorithms for the task, with various access costs and
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204047</id><created>2002-04-22</created><updated>2002-04-22</updated><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Bailey-Kellogg</keyname><forenames>Chris</forenames></author></authors><title>Sampling Strategies for Mining in Data-Scarce Domains</title><categories>cs.CE cs.AI</categories><acm-class>D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8</acm-class><abstract>  Data mining has traditionally focused on the task of drawing inferences from
large datasets. However, many scientific and engineering domains, such as fluid
dynamics and aircraft design, are characterized by scarce data, due to the
expense and complexity of associated experiments and simulations. In such
data-scarce domains, it is advantageous to focus the data collection effort on
only those regions deemed most important to support a particular data mining
objective. This paper describes a mechanism that interleaves bottom-up data
mining, to uncover multi-level structures in spatial data, with top-down
sampling, to clarify difficult decisions in the mining process. The mechanism
exploits relevant physical properties, such as continuity, correspondence, and
locality, in a unified framework. This leads to effective mining and sampling
decisions that are explainable in terms of domain knowledge and data
characteristics. This approach is demonstrated in two diverse applications --
mining pockets in spatial data, and qualitative determination of Jordan forms
of matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204048</id><created>2002-04-22</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Economic-based Distributed Resource Management and Scheduling for Grid
  Computing</title><categories>cs.DC</categories><report-no>PhD Thesis, April 2002</report-no><acm-class>C.1.3, C.1.4, C.2.4</acm-class><journal-ref>Monash University, Melbourne, Australia, April 2002</journal-ref><abstract>  Computational Grids, emerging as an infrastructure for next generation
computing, enable the sharing, selection, and aggregation of geographically
distributed resources for solving large-scale problems in science, engineering,
and commerce. As the resources in the Grid are heterogeneous and geographically
distributed with varying availability and a variety of usage and cost policies
for diverse users at different times and, priorities as well as goals that vary
with time. The management of resources and application scheduling in such a
large and distributed environment is a complex task. This thesis proposes a
distributed computational economy as an effective metaphor for the management
of resources and application scheduling. It proposes an architectural framework
that supports resource trading and quality of services based scheduling. It
enables the regulation of supply and demand for resources and provides an
incentive for resource owners for participating in the Grid and motives the
users to trade-off between the deadline, budget, and the required level of
quality of service. The thesis demonstrates the capability of economic-based
systems for peer-to-peer distributed computing by developing users'
quality-of-service requirements driven scheduling strategies and algorithms. It
demonstrates their effectiveness by performing scheduling experiments on the
World-Wide Grid for solving parameter sweep applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204049</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204049</id><created>2002-04-24</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author></authors><title>Memory-Based Shallow Parsing</title><categories>cs.CL</categories><report-no>jmlr-2002-tks</report-no><acm-class>I.2.7</acm-class><journal-ref>Journal of Machine Learning Research, volume 2 (March), 2002, pp.
  559-594</journal-ref><abstract>  We present memory-based learning approaches to shallow parsing and apply
these to five tasks: base noun phrase identification, arbitrary base phrase
recognition, clause detection, noun phrase parsing and full parsing. We use
feature selection techniques and system combination methods for improving the
performance of the memory-based learner. Our approach is evaluated on standard
data sets and the results are compared with that of other systems. This reveals
that our approach works well for base phrase identification while its
application towards recognizing embedded structures leaves some room for
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204050</id><created>2002-04-25</created><authors><author><keyname>Efrat</keyname><forenames>Alon</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author></authors><title>Computing Homotopic Shortest Paths Efficiently</title><categories>cs.CG</categories><comments>12 pages, 11 figures</comments><acm-class>I.3.5;F.2.2</acm-class><abstract>  This paper addresses the problem of finding shortest paths homotopic to a
given disjoint set of paths that wind amongst point obstacles in the plane. We
present a faster algorithm than previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204051</id><created>2002-04-26</created><authors><author><keyname>Boman</keyname><forenames>Magnus</forenames></author><author><keyname>Johansson</keyname><forenames>Stefan</forenames></author><author><keyname>Lyback</keyname><forenames>David</forenames></author></authors><title>Parrondo Strategies for Artificial Traders</title><categories>cs.CE</categories><comments>10 pages, 4 figures</comments><acm-class>I.2.11; J.4; K.4.4</acm-class><journal-ref>Intelligent Agent Technology; Zhong, Liu, Ohsuga, Bradshaw (eds);
  150-159; World Scientific, 2001</journal-ref><abstract>  On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204052</id><created>2002-04-26</created><authors><author><keyname>Wocjan</keyname><forenames>Pawel</forenames><affiliation>Universitaet Karlsruhe</affiliation></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames><affiliation>Universitaet Karlsruhe</affiliation></author><author><keyname>Beth</keyname><forenames>Thomas</forenames><affiliation>Universitaet Karlsruhe</affiliation></author></authors><title>Required sample size for learning sparse Bayesian networks with many
  variables</title><categories>cs.LG math.PR</categories><comments>9 pages</comments><acm-class>I.2.6</acm-class><abstract>  Learning joint probability distributions on n random variables requires
exponential sample size in the generic case. Here we consider the case that a
temporal (or causal) order of the variables is known and that the (unknown)
graph of causal dependencies has bounded in-degree Delta. Then the joint
measure is uniquely determined by the probabilities of all (2 Delta+1)-tuples.
Upper bounds on the sample size required for estimating their probabilities can
be given in terms of the VC-dimension of the set of corresponding cylinder
sets. The sample size grows less than linearly with n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204053</id><created>2002-04-26</created><authors><author><keyname>Bailey-Kellogg</keyname><forenames>Chris</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Qualitative Analysis of Correspondence for Experimental Algorithmics</title><categories>cs.AI cs.CE</categories><comments>11 pages</comments><acm-class>D.2.6; G.1.2; G.1.3; G.3; I.2.10; I.5; H.2.8</acm-class><abstract>  Correspondence identifies relationships among objects via similarities among
their components; it is ubiquitous in the analysis of spatial datasets,
including images, weather maps, and computational simulations. This paper
develops a novel multi-level mechanism for qualitative analysis of
correspondence. Operators leverage domain knowledge to establish
correspondence, evaluate implications for model selection, and leverage
identified weaknesses to focus additional data collection. The utility of the
mechanism is demonstrated in two applications from experimental algorithmics --
matrix spectral portrait analysis and graphical assessment of Jordan forms of
matrices. Results show that the mechanism efficiently samples computational
experiments and successfully uncovers high-level problem properties. It
overcomes noise and data sparsity by leveraging domain knowledge to detect
mutually reinforcing interpretations of spatial data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204054</id><created>2002-04-26</created><authors><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Navigating the Small World Web by Textual Cues</title><categories>cs.IR cs.NI</categories><acm-class>H.3.1; H.3.3; H.3.4; H.3.5</acm-class><abstract>  Can a Web crawler efficiently locate an unknown relevant page? While this
question is receiving much empirical attention due to its considerable
commercial value in the search engine community
[Cho98,Chakrabarti99,Menczer00,Menczer01], theoretical efforts to bound the
performance of focused navigation have only exploited the link structure of the
Web graph, neglecting other features [Kleinberg01,Adamic01,Kim02]. Here I
investigate the connection between linkage and a content-induced topology of
Web pages, suggesting that efficient paths can be discovered by decentralized
navigation algorithms based on textual cues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204055</id><created>2002-04-29</created><authors><author><keyname>Zheng</keyname><forenames>Qingguo</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Lv</keyname><forenames>Weifeng</forenames></author><author><keyname>Ma</keyname><forenames>Shilong</forenames></author></authors><title>Intelligent Search of Correlated Alarms for GSM Networks with
  Model-based Constraints</title><categories>cs.NI cs.AI</categories><comments>8 pages, 7 figures</comments><acm-class>C.2.4</acm-class><journal-ref>the 9th IEEE International Conference on
  Telecommunications,June,2002, Beijing,China</journal-ref><abstract>  In order to control the process of data mining and focus on the things of
interest to us, many kinds of constraints have been added into the algorithms
of data mining. However, discovering the correlated alarms in the alarm
database needs deep domain constraints. Because the correlated alarms greatly
depend on the logical and physical architecture of networks. Thus we use the
network model as the constraints of algorithms, including Scope constraint,
Inter-correlated constraint and Intra-correlated constraint, in our proposed
algorithm called SMC (Search with Model-based Constraints). The experiments
show that the SMC algorithm with Inter-correlated or Intra-correlated
constraint is about two times faster than the algorithm with no constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0204056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0204056</id><created>2002-04-29</created><authors><author><keyname>Boman</keyname><forenames>Magnus</forenames></author><author><keyname>Bylund</keyname><forenames>Markus</forenames></author><author><keyname>Espinoza</keyname><forenames>Fredrik</forenames></author><author><keyname>Danielson</keyname><forenames>Mats</forenames></author><author><keyname>Lyback</keyname><forenames>David</forenames></author></authors><title>Trading Agents for Roaming Users</title><categories>cs.CE</categories><comments>5 pages, 1 figure</comments><acm-class>I.2.11; K.8</acm-class><abstract>  Some roaming users need services to manipulate autonomous processes. Trading
agents running on agent trade servers are used as a case in point. We present a
solution that provides the agent owners with means to upkeeping their desktop
environment, and maintaining their agent trade server processes, via a
briefcase service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205001</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205001</id><created>2002-05-02</created><updated>2002-06-11</updated><authors><author><keyname>Burchard</keyname><forenames>A.</forenames></author><author><keyname>Liebeherr</keyname><forenames>J.</forenames></author><author><keyname>Patek</keyname><forenames>S. D.</forenames></author></authors><title>A Calculus for End-to-end Statistical Service Guarantees</title><categories>cs.NI</categories><comments>21 pages, 2 figures. Revisions in Theorem 5 and Section 4</comments><report-no>University of Virginia CS-2001-19 (2nd revised version)</report-no><acm-class>G.3, C.m</acm-class><journal-ref>IEEE Transactions on Information Theory 52:4105-4114 (2006)</journal-ref><abstract>  The deterministic network calculus offers an elegant framework for
determining delays and backlog in a network with deterministic service
guarantees to individual traffic flows. This paper addresses the problem of
extending the network calculus to a probabilistic framework with statistical
service guarantees. Here, the key difficulty relates to expressing, in a
statistical setting, an end-to-end (network) service curve as a concatenation
of per-node service curves. The notion of an effective service curve is
developed as a probabilistic bound on the service received by an individual
flow. It is shown that per-node effective service curves can be concatenated to
yield a network effective service curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205002</id><created>2002-05-02</created><authors><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>A Polynomial Description of the Rijndael Advanced Encryption Standard</title><categories>cs.CR math.AC math.RA</categories><comments>12 pages, LaTeX</comments><acm-class>E.3</acm-class><abstract>  The paper gives a polynomial description of the Rijndael Advanced Encryption
Standard recently adopted by the National Institute of Standards and
Technology. Special attention is given to the structure of the S-Box.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205003</id><created>2002-05-03</created><authors><author><keyname>Buss</keyname><forenames>Samuel R.</forenames></author><author><keyname>Kechris</keyname><forenames>Alexander S.</forenames></author><author><keyname>Pillay</keyname><forenames>Anand</forenames></author><author><keyname>Shore</keyname><forenames>Richard A.</forenames></author></authors><title>The prospects for mathematical logic in the twenty-first century</title><categories>cs.LO</categories><comments>Association for Symbolic Logic</comments><acm-class>A.1;F.0;I.2.0</acm-class><journal-ref>Bulletin of Symbolic Logic 7 (2001) 169-196</journal-ref><abstract>  The four authors present their speculations about the future developments of
mathematical logic in the twenty-first century. The areas of recursion theory,
proof theory and logic for computer science, model theory, and set theory are
discussed independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205004</id><created>2002-05-03</created><authors><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author><author><keyname>Mukherjee</keyname><forenames>Joy</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Weaves: A Novel Direct Code Execution Interface for Parallel High
  Performance Scientific Codes</title><categories>cs.DC cs.PF</categories><acm-class>D.4.1; I.6</acm-class><abstract>  Scientific codes are increasingly being used in compositional settings,
especially problem solving environments (PSEs). Typical compositional modeling
frameworks require significant buy-in, in the form of commitment to a
particular style of programming (e.g., distributed object components). While
this solution is feasible for newer generations of component-based scientific
codes, large legacy code bases present a veritable software engineering
nightmare. We introduce Weaves a novel framework that enables modeling,
composition, direct code execution, performance characterization, adaptation,
and control of unmodified high performance scientific codes. Weaves is an
efficient generalized framework for parallel compositional modeling that is a
proper superset of the threads and processes models of programming. In this
paper, our focus is on the transparent code execution interface enabled by
Weaves. We identify design constraints, their impact on implementation
alternatives, configuration scenarios, and present results from a prototype
implementation on Intel x86 architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205005</id><created>2002-05-04</created><updated>2004-11-11</updated><authors><author><keyname>Hearn</keyname><forenames>Robert A.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author></authors><title>PSPACE-Completeness of Sliding-Block Puzzles and Other Problems through
  the Nondeterministic Constraint Logic Model of Computation</title><categories>cs.CC cs.GT</categories><comments>19 pages, 16 figures. Version 4 adds token formulation and minor
  corrections. To appear in Theoretical Computer Science</comments><acm-class>F.1; F.2.2</acm-class><abstract>  We present a nondeterministic model of computation based on reversing edge
directions in weighted directed graphs with minimum in-flow constraints on
vertices. Deciding whether this simple graph model can be manipulated in order
to reverse the direction of a particular edge is shown to be PSPACE-complete by
a reduction from Quantified Boolean Formulas. We prove this result in a variety
of special cases including planar graphs and highly restricted vertex
configurations, some of which correspond to a kind of passive constraint logic.
Our framework is inspired by (and indeed a generalization of) the ``Generalized
Rush Hour Logic'' developed by Flake and Baum.
  We illustrate the importance of our model of computation by giving simple
reductions to show that several motion-planning problems are PSPACE-hard. Our
main result along these lines is that classic unrestricted sliding-block
puzzles are PSPACE-hard, even if the pieces are restricted to be all dominoes
(1x2 blocks) and the goal is simply to move a particular piece. No prior
complexity results were known about these puzzles. This result can be seen as a
strengthening of the existing result that the restricted Rush Hour puzzles are
PSPACE-complete, of which we also give a simpler proof. Finally, we strengthen
the existing result that the pushing-blocks puzzle Sokoban is PSPACE-complete,
by showing that it is PSPACE-complete even if no barriers are allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205006</id><created>2002-05-08</created><authors><author><keyname>Baroni</keyname><forenames>Marco</forenames></author><author><keyname>Matiasek</keyname><forenames>Johannes</forenames></author><author><keyname>Trost</keyname><forenames>Harald</forenames></author></authors><title>Unsupervised discovery of morphologically related words based on
  orthographic and semantic similarity</title><categories>cs.CL</categories><comments>10 pages, to appear in the Proceedings of the Morphology/Phonology
  Learning Workshop of ACL-02</comments><acm-class>I.2.7</acm-class><abstract>  We present an algorithm that takes an unannotated corpus as its input, and
returns a ranked list of probable morphologically related pairs as its output.
The algorithm tries to discover morphologically related pairs by looking for
pairs that are both orthographically and semantically similar, where
orthographic similarity is measured in terms of minimum edit distance, and
semantic similarity is measured in terms of mutual information. The procedure
does not rely on a morpheme concatenation model, nor on distributional
properties of word substrings (such as affix frequency). Experiments with
German and English input give encouraging results, both in terms of precision
(proportion of good pairs found at various cutoff points of the ranked list),
and in terms of a qualitative analysis of the types of morphological patterns
discovered by the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205007</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205007</id><created>2002-05-09</created><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>On-Line Paging against Adversarially Biased Random Inputs</title><categories>cs.DS cs.CC</categories><comments>Conference version appeared in SODA '98 as &quot;Bounding the Diffuse
  Adversary&quot;</comments><acm-class>F.2.0; F.1.3</acm-class><journal-ref>J. Algorithms 37(1) pp 218-235 (2000)</journal-ref><doi>10.1006/jagm.2000.1099</doi><abstract>  In evaluating an algorithm, worst-case analysis can be overly pessimistic.
Average-case analysis can be overly optimistic. An intermediate approach is to
show that an algorithm does well on a broad class of input distributions.
Koutsoupias and Papadimitriou recently analyzed the least-recently-used (LRU)
paging strategy in this manner, analyzing its performance on an input sequence
generated by a so-called diffuse adversary -- one that must choose each request
probabilitistically so that no page is chosen with probability more than some
fixed epsilon&gt;0. They showed that LRU achieves the optimal competitive ratio
(for deterministic on-line algorithms), but they didn't determine the actual
ratio.
  In this paper we estimate the optimal ratios within roughly a factor of two
for both deterministic strategies (e.g. least-recently-used and
first-in-first-out) and randomized strategies. Around the threshold epsilon ~
1/k (where k is the cache size), the optimal ratios are both Theta(ln k). Below
the threshold the ratios tend rapidly to O(1). Above the threshold the ratio is
unchanged for randomized strategies but tends rapidly to Theta(k) for
deterministic ones.
  We also give an alternate proof of the optimality of LRU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205008</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205008</id><created>2002-05-09</created><authors><author><keyname>Aslam</keyname><forenames>Javed</forenames></author><author><keyname>Rasala</keyname><forenames>April</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>Improved Bicriteria Existence Theorems for Scheduling</title><categories>cs.DS cs.CC</categories><acm-class>F.2.0; F.1.3</acm-class><journal-ref>ACM-SIAM Symposium on Discrete Algorithms, pp. 846-847 (1999)</journal-ref><abstract>  Two common objectives for evaluating a schedule are the makespan, or schedule
length, and the average completion time. This short note gives improved bounds
on the existence of schedules that simultaneously optimize both criteria. In
particular, for any rho&gt; 0, there exists a schedule of makespan at most 1+rho
times the minimum, with average completion time at most (1-e)^rho times the
minimum. The proof uses an infininite-dimensional linear program to generalize
and strengthen a previous analysis by Cliff Stein and Joel Wein (1997).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205009</id><created>2002-05-10</created><authors><author><keyname>Ando</keyname><forenames>Rie Kubota</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Mostly-Unsupervised Statistical Segmentation of Japanese Kanji Sequences</title><categories>cs.CL</categories><comments>22 pages. To appear in Natural Language Engineering</comments><acm-class>I.2.7</acm-class><journal-ref>Natural Language Engineering 9 (2), pp. 127--149, 2003</journal-ref><doi>10.1017/S1351324902002954</doi><abstract>  Given the lack of word delimiters in written Japanese, word segmentation is
generally considered a crucial first step in processing Japanese texts. Typical
Japanese segmentation algorithms rely either on a lexicon and syntactic
analysis or on pre-segmented data; but these are labor-intensive, and the
lexico-syntactic techniques are vulnerable to the unknown word problem. In
contrast, we introduce a novel, more robust statistical method utilizing
unsegmented training data. Despite its simplicity, the algorithm yields
performance on long kanji sequences comparable to and sometimes surpassing that
of state-of-the-art morphological analyzers over a variety of error metrics.
The algorithm also outperforms another mostly-unsupervised statistical
algorithm previously proposed for Chinese.
  Additionally, we present a two-level annotation scheme for Japanese to
incorporate multiple segmentation granularities, and introduce two novel
evaluation metrics, both based on the notion of a compatible bracket, that can
account for multiple granularities simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205010</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205010</id><created>2002-05-09</created><authors><author><keyname>Matias</keyname><forenames>Yossi</forenames></author><author><keyname>Vitter</keyname><forenames>Jeff</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>Approximate Data Structures with Applications</title><categories>cs.DS cs.CC</categories><acm-class>F.2.0; F.1.3</acm-class><journal-ref>ACM-SIAM Symposium on Discrete Algorithms, pp. 187-194 (1994)</journal-ref><abstract>  This paper explores the notion of approximate data structures, which return
approximately correct answers to queries, but run faster than their exact
counterparts. The paper describes approximate variants of the van Emde Boas
data structure, which support the same dynamic operations as the standard van
Emde Boas data structure (min, max, successor, predecessor, and existence
queries, as well as insertion and deletion), except that answers to queries are
approximate. The variants support all operations in constant time provided the
performance guarantee is 1+1/polylog(n), and in O(loglogn) time provided the
performance guarantee is 1+1/polynomial(n), for n elements in the data
structure.
  Applications described include Prim's minimum-spanning-tree algorithm,
Dijkstra's single-source shortest paths algorithm, and an on-line variant of
Graham's convex hull algorithm. To obtain output which approximates the desired
output with the performance guarantee tending to 1, Prim's algorithm requires
only linear time, Dijkstra's algorithm requires O(mloglogn) time, and the
on-line variant of Graham's algorithm requires constant amortized time per
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205011</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205011</id><created>2002-05-09</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Raghavachari</keyname><forenames>Balaji</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>On Strongly Connected Digraphs with Bounded Cycle Length</title><categories>cs.DS cs.CC</categories><acm-class>F.2.0; F.1.3</acm-class><journal-ref>Discrete Applied Mathematics 69(3):281-289 (1996)</journal-ref><doi>10.1016/0166-218X(95)00105-Z</doi><abstract>  The MEG (minimum equivalent graph) problem is, given a directed graph, to
find a small subset of the edges that maintains all reachability relations
between nodes. The problem is NP-hard. This paper gives a proof that, for
graphs where each directed cycle has at most three edges, the MEG problem is
equivalent to maximum bipartite matching, and therefore solvable in polynomial
time. This leads to an improvement in the performance guarantee of the
previously best approximation algorithm for the general problem in
``Approximating the Minimum Equivalent Digraph'' (1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205012</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205012</id><created>2002-05-09</created><authors><author><keyname>Kenyon</keyname><forenames>Claire</forenames></author><author><keyname>Schabanel</keyname><forenames>Nicolas</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>Polynomial-Time Approximation Scheme for Data Broadcast</title><categories>cs.DS cs.CC</categories><acm-class>F.2.0, F.1.3</acm-class><doi>10.1145/335305.335398</doi><abstract>  The data broadcast problem is to find a schedule for broadcasting a given set
of messages over multiple channels. The goal is to minimize the cost of the
broadcast plus the expected response time to clients who periodically and
probabilistically tune in to wait for particular messages.
  The problem models disseminating data to clients in asymmetric communication
environments, where there is a much larger capacity from the information source
to the clients than in the reverse direction. Examples include satellites,
cable TV, internet broadcast, and mobile phones. Such environments favor the
``push-based'' model where the server broadcasts (pushes) its information on
the communication medium and multiple clients simultaneously retrieve the
specific information of individual interest.
  This paper presents the first polynomial-time approximation scheme (PTAS) for
data broadcast with O(1) channels and when each message has arbitrary
probability, unit length and bounded cost. The best previous polynomial-time
approximation algorithm for this case has a performance ratio of 9/8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205013</id><created>2002-05-11</created><authors><author><keyname>Lonc</keyname><forenames>Zbigniew</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Computing stable models: worst-case performance estimates</title><categories>cs.LO cs.AI</categories><comments>Paper published in the Proceedings of the International Conference on
  Logic Programming, ICLP-2002</comments><acm-class>D.1.6;I.2.4</acm-class><abstract>  We study algorithms for computing stable models of propositional logic
programs and derive estimates on their worst-case performance that are
asymptotically better than the trivial bound of O(m 2^n), where m is the size
of an input program and n is the number of its atoms. For instance, for
programs, whose clauses consist of at most two literals (counting the head) we
design an algorithm to compute stable models that works in time O(m\times
1.44225^n). We present similar results for several broader classes of programs,
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205014</id><created>2002-05-11</created><authors><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Marek</keyname><forenames>Victor W.</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Ultimate approximations in nonmonotonic knowledge representation systems</title><categories>cs.AI</categories><comments>This paper was published in Principles of Knowledge Representation
  and Reasoning, Proceedings of the Eighth International Conference (KR2002)</comments><acm-class>I.2.4</acm-class><abstract>  We study fixpoints of operators on lattices. To this end we introduce the
notion of an approximation of an operator. We order approximations by means of
a precision ordering. We show that each lattice operator O has a unique most
precise or ultimate approximation. We demonstrate that fixpoints of this
ultimate approximation provide useful insights into fixpoints of the operator
O.
  We apply our theory to logic programming and introduce the ultimate
Kripke-Kleene, well-founded and stable semantics. We show that the ultimate
Kripke-Kleene and well-founded semantics are more precise then their standard
counterparts We argue that ultimate semantics for logic programming have
attractive epistemological properties and that, while in general they are
computationally more complex than the standard semantics, for many classes of
theories, their complexity is no worse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205015</id><created>2002-05-12</created><authors><author><keyname>Farber</keyname><forenames>Michael</forenames></author></authors><title>Instabilities of Robot Motion</title><categories>cs.RO cs.CG math.AT</categories><comments>26 pages, 5 figures</comments><acm-class>I.2.9; I.3.5</acm-class><abstract>  Instabilities of robot motion are caused by topological reasons. In this
paper we find a relation between the topological properties of a configuration
space (the structure of its cohomology algebra) and the character of
instabilities, which are unavoidable in any motion planning algorithm. More
specifically, let $X$ denote the space of all admissible configurations of a
mechanical system. A {\it motion planner} is given by a splitting $X\times X =
F_1\cup F_2\cup ... \cup F_k$ (where $F_1, ..., F_k$ are pairwise disjoint
ENRs, see below) and by continuous maps $s_j: F_j \to PX,$ such that $E\circ
s_j =1_{F_j}$. Here $PX$ denotes the space of all continuous paths in $X$
(admissible motions of the system) and $E: PX\to X\times X$ denotes the map
which assigns to a path the pair of its initial -- end points. Any motion
planner determines an algorithm of motion planning for the system. In this
paper we apply methods of algebraic topology to study the minimal number of
sets $F_j$ in any motion planner in $X$. We also introduce a new notion of {\it
order of instability} of a motion planner; it describes the number of
essentially distinct motions which may occur as a result of small perturbations
of the input data. We find the minimal order of instability, which may have
motion planners on a given configuration space $X$. We study a number of
specific problems: motion of a rigid body in $\R^3$, a robot arm, motion in
$\R^3$ in the presence of obstacles, and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205016</id><created>2002-05-13</created><authors><author><keyname>Han</keyname><forenames>Jing</forenames></author><author><keyname>Liu</keyname><forenames>Jiming</forenames></author><author><keyname>Cai</keyname><forenames>Qingsheng</forenames></author></authors><title>From Alife Agents to a Kingdom of N Queens</title><categories>cs.AI cs.DS cs.MA</categories><comments>13 pages, 9 figures, in 1999 international conference of Intelligent
  Agent Technology. Nominated for the best paper award</comments><acm-class>I.2.8;I.2.11;G.2.1</acm-class><journal-ref>in Jiming Liu and Ning Zhong (Eds.), Intelligent Agent Technology:
  Systems, Methodologies, and Tools, page 110-120, The World Scientific
  Publishing Co. Pte, Ltd., Nov. 1999</journal-ref><abstract>  This paper presents a new approach to solving N-queen problems, which
involves a model of distributed autonomous agents with artificial life (ALife)
and a method of representing N-queen constraints in an agent environment. The
distributed agents locally interact with their living environment, i.e., a
chessboard, and execute their reactive behaviors by applying their behavioral
rules for randomized motion, least-conflict position searching, and cooperating
with other agents etc. The agent-based N-queen problem solving system evolves
through selection and contest according to the rule of Survival of the Fittest,
in which some agents will die or be eaten if their moving strategies are less
efficient than others. The experimental results have shown that this system is
capable of solving large-scale N-queen problems. This paper also provides a
model of ALife agents for solving general CSPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205017</id><created>2002-05-13</created><authors><author><keyname>Petasis</keyname><forenames>Georgios</forenames></author><author><keyname>Karkaletsis</keyname><forenames>Vangelis</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Androutsopoulos</keyname><forenames>Ion</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Constantine D.</forenames></author></authors><title>Ellogon: A New Text Engineering Platform</title><categories>cs.CL</categories><comments>7 pages, 9 figures. Will be presented to the Third International
  Conference on Language Resources and Evaluation - LREC 2002</comments><acm-class>I.2.7</acm-class><abstract>  This paper presents Ellogon, a multi-lingual, cross-platform, general-purpose
text engineering environment. Ellogon was designed in order to aid both
researchers in natural language processing, as well as companies that produce
language engineering systems for the end-user. Ellogon provides a powerful
TIPSTER-based infrastructure for managing, storing and exchanging textual data,
embedding and managing text processing components as well as visualising
textual data and their associated linguistic information. Among its key
features are full Unicode support, an extensive multi-lingual graphical user
interface, its modular architecture and the reduced hardware requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205018</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205018</id><created>2002-05-14</created><updated>2002-07-28</updated><authors><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author></authors><title>Typed Generic Traversal With Term Rewriting Strategies</title><categories>cs.PL</categories><comments>85 pages, submitted for publication to the Journal of Logic and
  Algebraic Programming</comments><acm-class>D.1.1; D.1.2; D.3.1; D.3.3; F.4.2; I.1 .3; I.2.2</acm-class><abstract>  A typed model of strategic term rewriting is developed. The key innovation is
that generic traversal is covered. To this end, we define a typed rewriting
calculus S'_{gamma}. The calculus employs a many-sorted type system extended by
designated generic strategy types gamma. We consider two generic strategy
types, namely the types of type-preserving and type-unifying strategies.
S'_{gamma} offers traversal combinators to construct traversals or schemes
thereof from many-sorted and generic strategies. The traversal combinators
model different forms of one-step traversal, that is, they process the
immediate subterms of a given term without anticipating any scheme of recursion
into terms. To inhabit generic types, we need to add a fundamental combinator
to lift a many-sorted strategy $s$ to a generic type gamma. This step is called
strategy extension. The semantics of the corresponding combinator states that s
is only applied if the type of the term at hand fits, otherwise the extended
strategy fails. This approach dictates that the semantics of strategy
application must be type-dependent to a certain extent. Typed strategic term
rewriting with coverage of generic term traversal is a simple but expressive
model of generic programming. It has applications in program transformation and
program analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205019</id><created>2002-05-14</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Distance function wavelets - Part I: Helmholtz and convection-diffusion
  transforms and series</title><categories>cs.CE cs.NA</categories><comments>Welcome any comments to wenc@simula.no</comments><acm-class>G.1</acm-class><abstract>  This report aims to present my research updates on distance function wavelets
(DFW) based on the fundamental solutions and the general solutions of the
Helmholtz, modified Helmholtz, and convection-diffusion equations, which
include the isotropic Helmholtz-Fourier (HF) transform and series, the
Helmholtz-Laplace (HL) transform, and the anisotropic convection-diffusion
wavelets and ridgelets. The latter is set to handle discontinuous and track
data problems. The edge effect of the HF series is addressed. Alternative
existence conditions for the DFW transforms are proposed and discussed. To
simplify and streamline the expression of the HF and HL transforms, a new
dimension-dependent function notation is introduced. The HF series is also used
to evaluate the analytical solutions of linear diffusion problems of arbitrary
dimensionality and geometry. The weakness of this report is lacking of rigorous
mathematical analysis due to the author's limited mathematical knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205020</id><created>2002-05-14</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>A quasi-RBF technique for numerical discretization of PDE's</title><categories>cs.CE cs.CG</categories><comments>Comments to wenc@simula.no</comments><acm-class>G1.3, G1.8</acm-class><abstract>  Atkinson developed a strategy which splits solution of a PDE system into
homogeneous and particular solutions, where the former have to satisfy the
boundary and governing equation, while the latter only need to satisfy the
governing equation without concerning geometry. Since the particular solution
can be solved irrespective of boundary shape, we can use a readily available
fast Fourier or orthogonal polynomial technique O(NlogN) to evaluate it in a
regular box or sphere surrounding physical domain. The distinction of this
study is that we approximate homogeneous solution with nonsingular general
solution RBF as in the boundary knot method. The collocation method using
general solution RBF has very high accuracy and spectral convergent speed and
is a simple, truly meshfree approach for any complicated geometry. More
importantly, the use of nonsingular general solution avoids the controversial
artificial boundary in the method of fundamental solution due to the
singularity of fundamental solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205021</id><created>2002-05-14</created><authors><author><keyname>Waananen</keyname><forenames>A.</forenames></author><author><keyname>Ellert</keyname><forenames>M.</forenames></author><author><keyname>Konstantinov</keyname><forenames>A.</forenames></author><author><keyname>Konya</keyname><forenames>B.</forenames></author><author><keyname>Smirnova</keyname><forenames>O.</forenames></author></authors><title>An Overview of a Grid Architecture for Scientific Computing</title><categories>cs.DC</categories><acm-class>C.2.4;C.5;J.2</acm-class><abstract>  This document gives an overview of a Grid testbed architecture proposal for
the NorduGrid project. The aim of the project is to establish an inter-Nordic
testbed facility for implementation of wide area computing and data handling.
The architecture is supposed to define a Grid system suitable for solving data
intensive problems at the Large Hadron Collider at CERN. We present the various
architecture components needed for such a system. After that we go on to give a
description of the dynamics by showing the task flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205022</id><created>2002-05-14</created><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>The Traits of the Personable</title><categories>cs.AI cs.IR</categories><acm-class>H.3.5; H.4.2; H.5.4; I.2.6; K.8</acm-class><abstract>  Information personalization is fertile ground for application of AI
techniques. In this article I relate personalization to the ability to capture
partial information in an information-seeking interaction. The specific focus
is on personalizing interactions at web sites. Using ideas from partial
evaluation and explanation-based generalization, I present a modeling
methodology for reasoning about personalization. This approach helps identify
seven tiers of `personable traits' in web sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205023</id><created>2002-05-14</created><updated>2002-05-14</updated><authors><author><keyname>Ellert</keyname><forenames>M.</forenames></author><author><keyname>Konstantinov</keyname><forenames>A.</forenames></author><author><keyname>Konya</keyname><forenames>B.</forenames></author><author><keyname>Smirnova</keyname><forenames>O.</forenames></author><author><keyname>Waananen</keyname><forenames>A.</forenames></author></authors><title>Performance evaluation of the GridFTP within the NorduGrid project</title><categories>cs.DC</categories><acm-class>C.2.2;C.2.4;C.5</acm-class><abstract>  This report presents results of the tests measuring the performance of
multi-threaded file transfers, using the GridFTP implementation of the Globus
project over the NorduGrid network resources. Point to point WAN tests, carried
out between the sites of Copenhagen, Lund, Oslo and Uppsala, are described. It
was found that multiple threaded download via the high performance GridFTP
protocol can significantly improve file transfer performance, and can serve as
a reliable data
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205024</id><created>2002-05-15</created><authors><author><keyname>Vitchev</keyname><forenames>Evgueniy</forenames></author></authors><title>A (non)static 0-order statistical model and its implementation for
  compressing virtually uncompressible data</title><categories>cs.DS cs.DM cs.GL</categories><comments>7 pages, 1 figure</comments><acm-class>E.4; G.3; H.1.1</acm-class><abstract>  We give an implementation of a statistical model, which can be successfully
applied for compressing of a sequence of binary digits with behavior close to
random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205025</id><created>2002-05-16</created><authors><author><keyname>van Zaanen</keyname><forenames>Menno M.</forenames></author></authors><title>Bootstrapping Structure into Language: Alignment-Based Learning</title><categories>cs.LG cs.CL</categories><comments>148 pages</comments><acm-class>I.2; I.2.6; I.2.7</acm-class><abstract>  This thesis introduces a new unsupervised learning framework, called
Alignment-Based Learning, which is based on the alignment of sentences and
Harris's (1951) notion of substitutability. Instances of the framework can be
applied to an untagged, unstructured corpus of natural language sentences,
resulting in a labelled, bracketed version of that corpus.
  Firstly, the framework aligns all sentences in the corpus in pairs, resulting
in a partition of the sentences consisting of parts of the sentences that are
equal in both sentences and parts that are unequal. Unequal parts of sentences
can be seen as being substitutable for each other, since substituting one
unequal part for the other results in another valid sentence. The unequal parts
of the sentences are thus considered to be possible (possibly overlapping)
constituents, called hypotheses.
  Secondly, the selection learning phase considers all hypotheses found by the
alignment learning phase and selects the best of these. The hypotheses are
selected based on the order in which they were found, or based on a
probabilistic function.
  The framework can be extended with a grammar extraction phase. This extended
framework is called parseABL. Instead of returning a structured version of the
unstructured input corpus, like the ABL system, this system also returns a
stochastic context-free or tree substitution grammar.
  Different instances of the framework have been tested on the English ATIS
corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the
interesting results, apart from the encouraging numerical results, is that all
instances can (and do) learn recursive structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205026</id><created>2002-05-17</created><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames><affiliation>Harvard University</affiliation></author></authors><title>Monads for natural language semantics</title><categories>cs.CL cs.PL</categories><comments>14 pages</comments><acm-class>I.2.7; D.3.1; F.3.2</acm-class><journal-ref>Proceedings of the 2001 European Summer School in Logic, Language
  and Information student session, ed. Kristina Striegnitz, 285-298</journal-ref><abstract>  Accounts of semantic phenomena often involve extending types of meanings and
revising composition rules at the same time. The concept of monads allows many
such accounts -- for intensionality, variable binding, quantification and focus
-- to be stated uniformly and compositionally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205027</id><created>2002-05-17</created><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames><affiliation>Harvard University</affiliation></author></authors><title>A variable-free dynamic semantics</title><categories>cs.CL</categories><comments>6 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 13th Amsterdam Colloquium, ed. Robert van Rooy
  and Martin Stokhof, 204-209 (2001)</journal-ref><abstract>  I propose a variable-free treatment of dynamic semantics. By &quot;dynamic
semantics&quot; I mean analyses of donkey sentences (&quot;Every farmer who owns a donkey
beats it&quot;) and other binding and anaphora phenomena in natural language where
meanings of constituents are updates to information states, for instance as
proposed by Groenendijk and Stokhof. By &quot;variable-free&quot; I mean denotational
semantics in which functional combinators replace variable indices and
assignment functions, for instance as advocated by Jacobson.
  The new theory presented here achieves a compositional treatment of dynamic
anaphora that does not involve assignment functions, and separates the
combinatorics of variable-free semantics from the particular linguistic
phenomena it treats. Integrating variable-free semantics and dynamic semantics
gives rise to interactions that make new empirical predictions, for example
&quot;donkey weak crossover&quot; effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205028</id><created>2002-05-17</created><authors><author><keyname>Loper</keyname><forenames>Edward</forenames></author><author><keyname>Bird</keyname><forenames>Steven</forenames></author></authors><title>NLTK: The Natural Language Toolkit</title><categories>cs.CL</categories><comments>8 pages, 1 figure, Proceedings of the ACL Workshop on Effective Tools
  and Methodologies for Teaching Natural Language Processing and Computational
  Linguistics, Philadelphia, July 2002, Association for Computational
  Linguistics</comments><acm-class>D.2.6; I.2.7; J.5; K.3.2</acm-class><abstract>  NLTK, the Natural Language Toolkit, is a suite of open source program
modules, tutorials and problem sets, providing ready-to-use computational
linguistics courseware. NLTK covers symbolic and statistical natural language
processing, and is interfaced to annotated corpora. Students augment and
replace existing components, learn structured programming by example, and
manipulate sophisticated models from the outset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205029</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205029</id><created>2002-05-17</created><authors><author><keyname>Zhang</keyname><forenames>Qin</forenames></author><author><keyname>Danskin</keyname><forenames>John</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>A Codebook Generation Algorithm for Document Image Compression</title><categories>cs.DS</categories><acm-class>F.2.0, E.4, I.4.2</acm-class><doi>10.1109/DCC.1997.582053</doi><abstract>  Pattern-matching-based document-compression systems (e.g. for faxing) rely on
finding a small set of patterns that can be used to represent all of the ink in
the document. Finding an optimal set of patterns is NP-hard; previous
compression schemes have resorted to heuristics. This paper describes an
extension of the cross-entropy approach, used previously for measuring pattern
similarity, to this problem. This approach reduces the problem to a k-medians
problem, for which the paper gives a new algorithm with a provably good
performance guarantee. In comparison to previous heuristics (First Fit, with
and without generalized Lloyd's/k-means postprocessing steps), the new
algorithm generates a better codebook, resulting in an overall improvement in
compression performance of almost 17%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205030</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205030</id><created>2002-05-17</created><updated>2005-05-28</updated><authors><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Approximation Algorithms for Covering/Packing Integer Programs</title><categories>cs.DS cs.DM</categories><comments>Preliminary version appeared in IEEE Symposium on Foundations of
  Computer Science (2001). To appear in Journal of Computer and System Sciences</comments><acm-class>F.2.2; G.1.6; G.2.1</acm-class><journal-ref>Journal of Computer and System Sciences 71(4):495-505(2005)</journal-ref><doi>10.1016/j.jcss.2005.05.002</doi><abstract>  Given matrices A and B and vectors a, b, c and d, all with non-negative
entries, we consider the problem of computing min {c.x: x in Z^n_+, Ax &gt; a, Bx
&lt; b, x &lt; d}. We give a bicriteria-approximation algorithm that, given epsilon
in (0, 1], finds a solution of cost O(ln(m)/epsilon^2) times optimal, meeting
the covering constraints (Ax &gt; a) and multiplicity constraints (x &lt; d), and
satisfying Bx &lt; (1 + epsilon)b + beta, where beta is the vector of row sums
beta_i = sum_j B_ij. Here m denotes the number of rows of A.
  This gives an O(ln m)-approximation algorithm for CIP -- minimum-cost
covering integer programs with multiplicity constraints, i.e., the special case
when there are no packing constraints Bx &lt; b. The previous best approximation
ratio has been O(ln(max_j sum_i A_ij)) since 1982. CIP contains the set cover
problem as a special case, so O(ln m)-approximation is the best possible unless
P=NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205031</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205031</id><created>2002-05-17</created><authors><author><keyname>Lovasz</keyname><forenames>Laszlo</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Lecture Notes on Evasiveness of Graph Properties</title><categories>cs.CC</categories><comments>Tech. Rep. CS-TR-317-91, Computer Science Dept., Princeton University</comments><report-no>Princeton University CS-TR-317-91</report-no><acm-class>F.1.3, F.2.0</acm-class><abstract>  This report presents notes from the first eight lectures of the class Many
Models of Complexity taught by Laszlo Lovasz at Princeton University in the
fall of 1990. The topic is evasiveness of graph properties: given a graph
property, how many edges of the graph an algorithm must check in the worst case
before it knows whether the property holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205032</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205032</id><created>2002-05-17</created><updated>2002-08-30</updated><authors><author><keyname>Garg</keyname><forenames>Naveen</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>On-Line End-to-End Congestion Control</title><categories>cs.DS cs.CC cs.NI</categories><comments>Proceedings IEEE Symp. Foundations of Computer Science, 2002</comments><acm-class>F.1.2, F.2.0; C.2.6; G.1.6</acm-class><journal-ref>The 43rd Annual IEEE Symposium on Foundations of Computer Science,
  303-310 (2002)</journal-ref><doi>10.1109/SFCS.2002.1181953</doi><abstract>  Congestion control in the current Internet is accomplished mainly by TCP/IP.
To understand the macroscopic network behavior that results from TCP/IP and
similar end-to-end protocols, one main analytic technique is to show that the
the protocol maximizes some global objective function of the network traffic.
Here we analyze a particular end-to-end, MIMD (multiplicative-increase,
multiplicative-decrease) protocol. We show that if all users of the network use
the protocol, and all connections last for at least logarithmically many
rounds, then the total weighted throughput (value of all packets received) is
near the maximum possible. Our analysis includes round-trip-times, and (in
contrast to most previous analyses) gives explicit convergence rates, allows
connections to start and stop, and allows capacities to change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205033</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205033</id><created>2002-05-17</created><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>On-Line File Caching</title><categories>cs.DS cs.CC cs.NI</categories><comments>ACM-SIAM Symposium on Discrete Algorithms (1998)</comments><acm-class>F.1.2, F.2.0, C.2.0</acm-class><journal-ref>Algorithmica 33:371-383 (2002)</journal-ref><doi>10.1007/s00453-001-0124-5</doi><abstract>  In the on-line file-caching problem problem, the input is a sequence of
requests for files, given on-line (one at a time). Each file has a non-negative
size and a non-negative retrieval cost. The problem is to decide which files to
keep in a fixed-size cache so as to minimize the sum of the retrieval costs for
files that are not in the cache when requested. The problem arises in web
caching by browsers and by proxies. This paper describes a natural
generalization of LRU called Landlord and gives an analysis showing that it has
an optimal performance guarantee (among deterministic on-line algorithms).
  The paper also gives an analysis of the algorithm in a so-called ``loosely''
competitive model, showing that on a ``typical'' cache size, either the
performance guarantee is O(1) or the total retrieval cost is insignificant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205034</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205034</id><created>2002-05-17</created><authors><author><keyname>Lupton</keyname><forenames>Robert</forenames></author><author><keyname>Maley</keyname><forenames>Miller</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>Data-Collection for the Sloan Digital Sky Survey: a Network-Flow
  Heuristic</title><categories>cs.DS cs.CE</categories><comments>proceedings version appeared in ACM-SIAM Symposium on Discrete
  Algorithms (1998)</comments><acm-class>F.2.1; G.1.6; J.2</acm-class><journal-ref>Journal of Algorithms 27(2):339-356 (1998)</journal-ref><doi>10.1006/jagm.1997.0922</doi><abstract>  The goal of the Sloan Digital Sky Survey is ``to map in detail one-quarter of
the entire sky, determining the positions and absolute brightnesses of more
than 100 million celestial objects''. The survey will be performed by taking
``snapshots'' through a large telescope. Each snapshot can capture up to 600
objects from a small circle of the sky. This paper describes the design and
implementation of the algorithm that is being used to determine the snapshots
so as to minimize their number. The problem is NP-hard in general; the
algorithm described is a heuristic, based on Lagriangian-relaxation and
min-cost network flow. It gets within 5-15% of a naive lower bound, whereas
using a ``uniform'' cover only gets within 25-35%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205035</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205035</id><created>2002-05-17</created><authors><author><keyname>Lipton</keyname><forenames>Richard</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Simple Strategies for Large Zero-Sum Games with Applications to
  Complexity Theory</title><categories>cs.CC cs.DM</categories><acm-class>F.1.3; G.2.1</acm-class><journal-ref>ACM Symposium on Theory of Computing (1994)</journal-ref><doi>10.1145/195058.195447</doi><abstract>  Von Neumann's Min-Max Theorem guarantees that each player of a zero-sum
matrix game has an optimal mixed strategy. This paper gives an elementary proof
that each player has a near-optimal mixed strategy that chooses uniformly at
random from a multiset of pure strategies of size logarithmic in the number of
pure strategies available to the opponent.
  For exponentially large games, for which even representing an optimal mixed
strategy can require exponential space, it follows that there are near-optimal,
linear-size strategies. These strategies are easy to play and serve as small
witnesses to the approximate value of the game.
  As a corollary, it follows that every language has small ``hard'' multisets
of inputs certifying that small circuits can't decide the language. For
example, if SAT does not have polynomial-size circuits, then, for each n and c,
there is a set of n^(O(c)) Boolean formulae of size n such that no circuit of
size n^c (or algorithm running in time n^c) classifies more than two-thirds of
the formulae succesfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205036</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205036</id><created>2002-05-18</created><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Randomized Rounding without Solving the Linear Program</title><categories>cs.DS cs.DM</categories><acm-class>F.1.3; G.1.6; G.2.1</acm-class><journal-ref>ACM-SIAM Symposium on Discrete Algorithms (1995)</journal-ref><abstract>  Randomized rounding is a standard method, based on the probabilistic method,
for designing combinatorial approximation algorithms. In Raghavan's seminal
paper introducing the method (1988), he writes: &quot;The time taken to solve the
linear program relaxations of the integer programs dominates the net running
time theoretically (and, most likely, in practice as well).&quot;
  This paper explores how this bottleneck can be avoided for randomized
rounding algorithms for packing and covering problems (linear programs, or
mixed integer linear programs, having no negative coefficients). The resulting
algorithms are greedy algorithms, and are faster and simpler to implement than
standard randomized-rounding algorithms.
  This approach can also be used to understand Lagrangian-relaxation algorithms
for packing/covering linear programs: such algorithms can be viewed as as
(derandomized) randomized-rounding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205037</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205037</id><created>2002-05-18</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Vishkin</keyname><forenames>Uzi</forenames></author><author><keyname>Young</keyname><forenames>Neal</forenames></author></authors><title>A Primal-Dual Parallel Approximation Technique Applied to Weighted Set
  and Vertex Cover</title><categories>cs.DS cs.DC</categories><comments>conference version appeared in IPCO'93</comments><acm-class>F.2.0; G.1.6; C.2.4</acm-class><journal-ref>Journal of Algorithms 17(2):280-289 (1994)</journal-ref><doi>10.1006/jagm.1994.1036</doi><abstract>  The paper describes a simple deterministic parallel/distributed
(2+epsilon)-approximation algorithm for the minimum-weight vertex-cover problem
and its dual (edge/element packing).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205038</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205038</id><created>2002-05-18</created><authors><author><keyname>Fiat</keyname><forenames>Amos</forenames></author><author><keyname>Karp</keyname><forenames>Richard</forenames></author><author><keyname>Luby</keyname><forenames>Mike</forenames></author><author><keyname>McGeoch</keyname><forenames>Lyle</forenames></author><author><keyname>Sleator</keyname><forenames>Daniel</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Competitive Paging Algorithms</title><categories>cs.DS cs.NI</categories><acm-class>F.2.0; F.1.2; C.0</acm-class><journal-ref>Journal of Algorithms 12:685-699 (1991)</journal-ref><doi>10.1016/0196-6774(91)90041-V</doi><abstract>  The paging problem is that of deciding which pages to keep in a memory of k
pages in order to minimize the number of page faults. This paper introduces the
marking algorithm, a simple randomized on-line algorithm for the paging
problem, and gives a proof that its performance guarantee (competitive ratio)
is O(log k). In contrast, no deterministic on-line algorithm can have a
performance guarantee better than k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205039</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205039</id><created>2002-05-18</created><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Sequential and Parallel Algorithms for Mixed Packing and Covering</title><categories>cs.DS</categories><acm-class>F.2.1, G.1.6</acm-class><doi>10.1109/SFCS.2001.959930</doi><abstract>  Mixed packing and covering problems are problems that can be formulated as
linear programs using only non-negative coefficients. Examples include
multicommodity network flow, the Held-Karp lower bound on TSP, fractional
relaxations of set cover, bin-packing, knapsack, scheduling problems,
minimum-weight triangulation, etc. This paper gives approximation algorithms
for the general class of problems. The sequential algorithm is a simple greedy
algorithm that can be implemented to find an epsilon-approximate solution in
O(epsilon^-2 log m) linear-time iterations. The parallel algorithm does
comparable work but finishes in polylogarithmic time.
  The results generalize previous work on pure packing and covering (the
special case when the constraints are all &quot;less-than&quot; or all &quot;greater-than&quot;) by
Michael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205040</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205040</id><created>2002-05-18</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Raghavachari</keyname><forenames>Balaji</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Approximating the Minimum Equivalent Digraph</title><categories>cs.DS cs.DM</categories><comments>conference version in ACM-SIAM Symposium on Discrete Algorithms
  (1994)</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>SIAM J. Computing 24(4):859-872 (1995)</journal-ref><doi>10.1137/S0097539793256685</doi><abstract>  The MEG (minimum equivalent graph) problem is, given a directed graph, to
find a small subset of the edges that maintains all reachability relations
between nodes. The problem is NP-hard. This paper gives an approximation
algorithm with performance guarantee of pi^2/6 ~ 1.64. The algorithm and its
analysis are based on the simple idea of contracting long cycles. (This result
is strengthened slightly in ``On strongly connected digraphs with bounded cycle
length'' (1996).) The analysis applies directly to 2-Exchange, a simple ``local
improvement'' algorithm, showing that its performance guarantee is 1.75.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205041</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205041</id><created>2002-05-18</created><authors><author><keyname>Young</keyname><forenames>Neal</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert</forenames></author><author><keyname>Orlin</keyname><forenames>James</forenames></author></authors><title>Faster Parametric Shortest Path and Minimum Balance Algorithms</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.2.2; G.1.6</acm-class><journal-ref>Networks 21(2):205-221 (1991)</journal-ref><doi>10.1002/net.3230210206</doi><abstract>  The parametric shortest path problem is to find the shortest paths in graph
where the edge costs are of the form w_ij+lambda where each w_ij is constant
and lambda is a parameter that varies. The problem is to find shortest path
trees for every possible value of lambda.
  The minimum-balance problem is to find a ``weighting'' of the vertices so
that adjusting the edge costs by the vertex weights yields a graph in which,
for every cut, the minimum weight of any edge crossing the cut in one direction
equals the minimum weight of any edge crossing the cut in the other direction.
  The paper presents fast algorithms for both problems. The algorithms run in
O(nm+n^2 log n) time. The paper also describes empirical studies of the
algorithms on random graphs, suggesting that the expected time for finding a
minimum-mean cycle (an important special case of both problems) is O(n log(n) +
m).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205042</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205042</id><created>2002-05-18</created><authors><author><keyname>Hakimi</keyname><forenames>S. L.</forenames></author><author><keyname>Schmeichel</keyname><forenames>E.</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Orienting Graphs to Optimize Reachability</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Information Processing Letters 63:229-235 (1997)</journal-ref><doi>10.1016/S0020-0190(97)00129-4</doi><abstract>  The paper focuses on two problems: (i) how to orient the edges of an
undirected graph in order to maximize the number of ordered vertex pairs (x,y)
such that there is a directed path from x to y, and (ii) how to orient the
edges so as to minimize the number of such pairs. The paper describes a
quadratic-time algorithm for the first problem, and a proof that the second
problem is NP-hard to approximate within some constant 1+epsilon &gt; 1. The
latter proof also shows that the second problem is equivalent to
``comparability graph completion''; neither problem was previously known to be
NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205043</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205043</id><created>2002-05-18</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Raghavachari</keyname><forenames>Balaji</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Low-Degree Spanning Trees of Small Weight</title><categories>cs.DS cs.DM</categories><comments>conference version in Symposium on Theory of Computing (1994)</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>SIAM J. Computing 25(2):355-368 (1996)</journal-ref><doi>10.1137/S0097539794264585</doi><abstract>  The degree-d spanning tree problem asks for a minimum-weight spanning tree in
which the degree of each vertex is at most d. When d=2 the problem is TSP, and
in this case, the well-known Christofides algorithm provides a
1.5-approximation algorithm (assuming the edge weights satisfy the triangle
inequality).
  In 1984, Christos Papadimitriou and Umesh Vazirani posed the challenge of
finding an algorithm with performance guarantee less than 2 for Euclidean
graphs (points in R^n) and d &gt; 2. This paper gives the first answer to that
challenge, presenting an algorithm to compute a degree-3 spanning tree of cost
at most 5/3 times the MST. For points in the plane, the ratio improves to 3/2
and the algorithm can also find a degree-4 spanning tree of cost at most 5/4
times the MST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205044</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205044</id><created>2002-05-18</created><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>The K-Server Dual and Loose Competitiveness for Paging</title><categories>cs.DS cs.NI</categories><comments>conference version: &quot;On-Line Caching as Cache Size Varies&quot;, SODA
  (1991)</comments><acm-class>F.1.2; F.2.0; C.2.0</acm-class><journal-ref>Algorithmica 11(6):525-541 (1994)</journal-ref><doi>10.1007/BF01189992</doi><abstract>  This paper has two results. The first is based on the surprising observation
that the well-known ``least-recently-used'' paging algorithm and the
``balance'' algorithm for weighted caching are linear-programming primal-dual
algorithms. This observation leads to a strategy (called ``Greedy-Dual'') that
generalizes them both and has an optimal performance guarantee for weighted
caching.
  For the second result, the paper presents empirical studies of paging
algorithms, documenting that in practice, on ``typical'' cache sizes and
sequences, the performance of paging strategies are much better than their
worst-case analyses in the standard model suggest. The paper then presents
theoretical results that support and explain this. For example: on any input
sequence, with almost all cache sizes, either the performance guarantee of
least-recently-used is O(log k) or the fault rate (in an absolute sense) is
insignificant.
  Both of these results are strengthened and generalized in``On-line File
Caching'' (1998).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205045</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205045</id><created>2002-05-18</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Raghavachari</keyname><forenames>Balaji</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Balancing Minimum Spanning and Shortest Path Trees</title><categories>cs.DS cs.DM</categories><comments>conference version: ACM-SIAM Symposium on Discrete Algorithms (1993)</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Algorithmica 14(4):305-322 (1995)</journal-ref><doi>10.1007/BF01294129</doi><abstract>  This paper give a simple linear-time algorithm that, given a weighted
digraph, finds a spanning tree that simultaneously approximates a shortest-path
tree and a minimum spanning tree. The algorithm provides a continuous
trade-off: given the two trees and epsilon &gt; 0, the algorithm returns a
spanning tree in which the distance between any vertex and the root of the
shortest-path tree is at most 1+epsilon times the shortest-path distance, and
yet the total weight of the tree is at most 1+2/epsilon times the weight of a
minimum spanning tree. This is the best tradeoff possible. The paper also
describes a fast parallel implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205046</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205046</id><created>2002-05-19</created><updated>2015-11-19</updated><authors><author><keyname>Klein</keyname><forenames>Phil</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>On the Number of Iterations for Dantzig-Wolfe Optimization and
  Packing-Covering Approximation Algorithms</title><categories>cs.DS cs.CC</categories><acm-class>F.2.1; G.1.6</acm-class><journal-ref>LNCS 1610 (IPCO): 320-327 (1999); SIAM Journal on Computing
  44(4):1154-1172(2015)</journal-ref><doi>10.1007/3-540-48777-8_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a lower bound on the iteration complexity of a natural class of
Lagrangean-relaxation algorithms for approximately solving packing/covering
linear programs. We show that, given an input with $m$ random 0/1-constraints
on $n$ variables, with high probability, any such algorithm requires
$\Omega(\rho \log(m)/\epsilon^2)$ iterations to compute a
$(1+\epsilon)$-approximate solution, where $\rho$ is the width of the input.
The bound is tight for a range of the parameters $(m,n,\rho,\epsilon)$.
  The algorithms in the class include Dantzig-Wolfe decomposition, Benders'
decomposition, Lagrangean relaxation as developed by Held and Karp [1971] for
lower-bounding TSP, and many others (e.g. by Plotkin, Shmoys, and Tardos [1988]
and Grigoriadis and Khachiyan [1996]). To prove the bound, we use a discrepancy
argument to show an analogous lower bound on the support size of
$(1+\epsilon)$-approximate mixed strategies for random two-player zero-sum
0/1-matrix games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205047</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205047</id><created>2002-05-18</created><updated>2005-04-08</updated><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>K-Medians, Facility Location, and the Chernoff-Wald Bound</title><categories>cs.DS cs.DM</categories><acm-class>F.2.1; G.1.6; G.2.2; G.3</acm-class><journal-ref>ACM-SIAM Symposium on Discrete Algorithms (2000)</journal-ref><abstract>  The paper gives approximation algorithms for the k-medians and
facility-location problems (both NP-hard). For k-medians, the algorithm returns
a solution using at most ln(n+n/epsilon)k medians and having cost at most
(1+epsilon) times the cost of the best solution that uses at most k medians.
Here epsilon &gt; 0 is an input to the algorithm. In comparison, the best previous
algorithm (Jyh-Han Lin and Jeff Vitter, 1992) had a (1+1/epsilon)ln(n) term
instead of the ln(n+n/epsilon) term in the performance guarantee. For facility
location, the algorithm returns a solution of cost at most d+ln(n) k, provided
there exists a solution of cost d+k where d is the assignment cost and k is the
facility cost. In comparison, the best previous algorithm (Dorit Hochbaum,
1982) returned a solution of cost at most ln(n)(d+k). For both problems, the
algorithms currently provide the best performance guarantee known for the
general (non-metric) problems.
  The paper also introduces a new probabilistic bound (called &quot;Chernoff-Wald
bound&quot;) for bounding the expectation of the maximum of a collection of sums of
random variables, when each sum contains a random number of terms. The bound is
used to analyze the randomized rounding scheme that underlies the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205048</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205048</id><created>2002-05-18</created><updated>2012-04-23</updated><authors><author><keyname>Golin</keyname><forenames>Mordecai</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Huffman Coding with Letter Costs: A Linear-Time Approximation Scheme</title><categories>cs.DS</categories><acm-class>F.2.0; E.4; I.4.2</acm-class><journal-ref>SIAM Journal on Computing 41(3):684-713(2012)</journal-ref><doi>10.1137/100794092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial-time approximation scheme for the generalization of
Huffman Coding in which codeword letters have non-uniform costs (as in Morse
code, where the dash is twice as long as the dot). The algorithm computes a
(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is
the input size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205049</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205049</id><created>2002-05-18</created><authors><author><keyname>Golin</keyname><forenames>Mordecai</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Prefix Codes: Equiprobable Words, Unequal Letter Costs</title><categories>cs.DS</categories><comments>proceedings version in ICALP (1994)</comments><acm-class>F.2.0; E.4; I.4.2</acm-class><journal-ref>SIAM J. Computing 25(6):1281-1304 (1996)</journal-ref><doi>10.1137/S0097539794268388</doi><abstract>  Describes a near-linear-time algorithm for a variant of Huffman coding, in
which the letters may have non-uniform lengths (as in Morse code), but with the
restriction that each word to be encoded has equal probability. [See also
``Huffman Coding with Unequal Letter Costs'' (2002).]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205050</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205050</id><created>2002-05-18</created><authors><author><keyname>Fekete</keyname><forenames>S.</forenames></author><author><keyname>Khuller</keyname><forenames>S.</forenames></author><author><keyname>Klemmstein</keyname><forenames>M.</forenames></author><author><keyname>Raghavachari</keyname><forenames>B.</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>A Network-Flow Technique for Finding Low-Weight Bounded-Degree Spanning
  Trees</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Journal of Algorithms 24(2):310-324 (1997)</journal-ref><doi>10.1006/jagm.1997.0862</doi><abstract>  The problem considered is the following. Given a graph with edge weights
satisfying the triangle inequality, and a degree bound for each vertex, compute
a low-weight spanning tree such that the degree of each vertex is at most its
specified bound. The problem is NP-hard (it generalizes Traveling Salesman
(TSP)). This paper describes a network-flow heuristic for modifying a given
tree T to meet the constraints. Choosing T to be a minimum spanning tree (MST)
yields approximation algorithms with performance guarantee less than 2 for the
problem on geometric graphs with L_p-norms. The paper also describes a
Euclidean graph whose minimum TSP costs twice the MST, disproving a conjecture
made in ``Low-Degree Spanning Trees of Small Weight'' (1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205051</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205051</id><created>2002-05-19</created><updated>2003-09-15</updated><authors><author><keyname>Karger</keyname><forenames>David</forenames></author><author><keyname>Klein</keyname><forenames>Phil</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Rounding Algorithms for a Geometric Embedding of Minimum Multiway Cut</title><categories>cs.DS cs.DM</categories><comments>Conference version in ACM Symposium on Theory of Computing (1999). To
  appear in Mathematics of Operations Research</comments><acm-class>F.2.0; G.1.6; G.2.2</acm-class><journal-ref>Mathematics of Operations Research 29(3):436-461(2004)</journal-ref><doi>10.1287/moor.1030.0086</doi><abstract>  The multiway-cut problem is, given a weighted graph and k &gt;= 2 terminal
nodes, to find a minimum-weight set of edges whose removal separates all the
terminals. The problem is NP-hard, and even NP-hard to approximate within
1+delta for some small delta &gt; 0.
  Calinescu, Karloff, and Rabani (1998) gave an algorithm with performance
guarantee 3/2-1/k, based on a geometric relaxation of the problem. In this
paper, we give improved randomized rounding schemes for their relaxation,
yielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation
algorithm in general.
  Our approach hinges on the observation that the problem of designing a
randomized rounding scheme for a geometric relaxation is itself a linear
programming problem. The paper explores computational solutions to this
problem, and gives a proof that for a general class of geometric relaxations,
there are always randomized rounding schemes that match the integrality gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205052</id><created>2002-05-19</created><authors><author><keyname>Alagar</keyname><forenames>Vasu</forenames></author><author><keyname>Laemmel</keyname><forenames>Ralf</forenames></author></authors><title>Three-Tiered Specification of Micro-Architectures</title><categories>cs.SE cs.PL</categories><acm-class>D.2.4; D.2.10; D.2.11; D.2.13</acm-class><abstract>  A three-tiered specification approach is developed to formally specify
collections of collaborating objects, say micro-architectures. (i) The
structural properties to be maintained in the collaboration are specified in
the lowest tier. (ii) The behaviour of the object methods in the classes is
specified in the middle tier. (iii) The interaction of the objects in the
micro-architecture is specified in the third tier. The specification approach
is based on Larch and accompanying notations and tools. The approach enables
the unambiguous and complete specification of reusable collections of
collaborating objects. The layered, formal approach is compared to other
approaches including the mainstream UML approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205053</id><created>2002-05-20</created><authors><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Grinter</keyname><forenames>Rebecca E.</forenames></author><author><keyname>Hurst</keyname><forenames>Amy</forenames></author><author><keyname>Szymanski</keyname><forenames>Margaret H.</forenames></author><author><keyname>Thornton</keyname><forenames>James D.</forenames></author><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author></authors><title>Sotto Voce: Exploring the Interplay of Conversation and Mobile Audio
  Spaces</title><categories>cs.HC cs.SD</categories><comments>8 pages</comments><acm-class>H.5.1; H.5.2; H.5.3; H.5.5; J.5; K.3.1</acm-class><journal-ref>Proc. ACM SIGCHI Conference on Human Factors in Computing Systems,
  Minneapolis, MN, April 2002, 431-438. ACM Press.</journal-ref><doi>10.1145/503376.503454</doi><abstract>  In addition to providing information to individual visitors, electronic
guidebooks have the potential to facilitate social interaction between visitors
and their companions. However, many systems impede visitor interaction. By
contrast, our electronic guidebook, Sotto Voce, has social interaction as a
primary design goal. The system enables visitors to share audio information -
specifically, they can hear each other's guidebook activity using a
technologically mediated audio eavesdropping mechanism. We conducted a study of
visitors using Sotto Voce while touring a historic house. The results indicate
that visitors are able to use the system effectively, both as a conversational
resource and as an information appliance. More surprisingly, our results
suggest that the technologically mediated audio often cohered the visitors'
conversation and activity to a far greater degree than audio delivered through
the open air.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205054</id><created>2002-05-20</created><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author><author><keyname>Grinter</keyname><forenames>Rebecca E.</forenames></author><author><keyname>Hurst</keyname><forenames>Amy</forenames></author><author><keyname>Szymanski</keyname><forenames>Margaret H.</forenames></author><author><keyname>Thornton</keyname><forenames>James D.</forenames></author></authors><title>Eavesdropping on Electronic Guidebooks: Observing Learning Resources in
  Shared Listening Environments</title><categories>cs.HC</categories><comments>8 pages</comments><acm-class>H.5.1; H.5.2; H.5.3; H.5.5; J.5; K.3.1</acm-class><journal-ref>In David Bearman and Jennifer Trant (eds.), Museums and the Web
  2002: Selected Papers. (Proc. 6th International Conference on Museums and the
  Web, Boston, MA, April 2002.) Pittsburgh, PA: Archives &amp; Museum Informatics,
  2002, 21-30</journal-ref><abstract>  We describe an electronic guidebook, Sotto Voce, that enables visitors to
share audio information by eavesdropping on each other's guidebook activity. We
have conducted three studies of visitors using electronic guidebooks in a
historic house: one study with open air audio played through speakers and two
studies with eavesdropped audio. An analysis of visitor interaction in these
studies suggests that eavesdropped audio provides more social and interactive
learning resources than open air audio played through speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205055</id><created>2002-05-20</created><authors><author><keyname>Woodruff</keyname><forenames>Allison</forenames></author><author><keyname>Szymanski</keyname><forenames>Margaret H.</forenames></author><author><keyname>Grinter</keyname><forenames>Rebecca E.</forenames></author><author><keyname>Aoki</keyname><forenames>Paul M.</forenames></author></authors><title>Practical Strategies for Integrating a Conversation Analyst in an
  Iterative Design Process</title><categories>cs.HC</categories><comments>11 pages</comments><acm-class>H.5.2; H.5.3</acm-class><journal-ref>Proc. ACM Conf. on Designing Interactive Systems, London, UK, June
  2002, 19-28. ACM Press.</journal-ref><doi>10.1145/778712.778748</doi><abstract>  We present a case study of an iterative design process that includes a
conversation analyst. We discuss potential benefits of conversation analysis
for design, and we describe our strategies for integrating the conversation
analyst in the design process. Since the analyst on our team had no previous
exposure to design or engineering, and none of the other members of our team
had any experience with conversation analysis, we needed to build a foundation
for our interaction. One of our key strategies was to pair the conversation
analyst with a designer in a highly interactive collaboration. Our tactics have
been effective on our project, leading to valuable results that we believe we
could not have obtained using another method. We hope that this paper can serve
as a practical guide to those interested in establishing a productive and
efficient working relationship between a conversation analyst and the other
members of a design team.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205056</id><created>2002-05-21</created><authors><author><keyname>Fellows</keyname><forenames>Michael R.</forenames></author><author><keyname>Gramm</keyname><forenames>Jens</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Parameterized Intractability of Motif Search Problems</title><categories>cs.CC</categories><comments>21 pages, 5 figures; an extended abstract of this paper was presented
  at the 19th International Symposium on Theoretical Aspects of Computer
  Science (STACS 2002), Springer-Verlag, LNCS 2285, pages 262--273, held in
  Juan-Les-Pins, France, March~14--16, 2002</comments><report-no>WSI-2002-2</report-no><acm-class>F.2;F.2.2;J.3</acm-class><abstract>  We show that Closest Substring, one of the most important problems in the
field of biological sequence analysis, is W[1]-hard when parameterized by the
number k of input strings (and remains so, even over a binary alphabet). This
problem is therefore unlikely to be solvable in time O(f(k)\cdot n^{c}) for any
function f of k and constant c independent of k. The problem can therefore be
expected to be intractable, in any practical sense, for k&gt;=3. Our result
supports the intuition that Closest Substring is computationally much harder
than the special case of Closest String, although both problems are
NP-complete. We also prove W[1]-hardness for other parameterizations in the
case of unbounded alphabet size. Our W[1]-hardness result for Closest Substring
generalizes to Consensus Patterns, a problem of similar significance in
computational biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205057</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205057</id><created>2002-05-21</created><authors><author><keyname>Creutz</keyname><forenames>Mathias</forenames></author><author><keyname>Lagus</keyname><forenames>Krista</forenames></author></authors><title>Unsupervised Discovery of Morphemes</title><categories>cs.CL</categories><comments>10 pages, to appear in Proceedings of Morphological and Phonological
  Learning Workshop of ACL'02</comments><acm-class>I.2.7</acm-class><abstract>  We present two methods for unsupervised segmentation of words into
morpheme-like units. The model utilized is especially suited for languages with
a rich morphology, such as Finnish. The first method is based on the Minimum
Description Length (MDL) principle and works online. In the second method,
Maximum Likelihood (ML) optimization is used. The quality of the segmentations
is measured using an evaluation method that compares the segmentations produced
to an existing morphological analysis. Experiments on both Finnish and English
corpora show that the presented methods perform well compared to a current
state-of-the-art system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205058</id><created>2002-05-21</created><authors><author><keyname>Novikov</keyname><forenames>Alexei</forenames></author></authors><title>Content Distribution in Unicast Replica Meshes</title><categories>cs.NI</categories><comments>14 pages, 8 figures, submitted to ACM Transactions on Internet
  Technology</comments><acm-class>H.3.3;H.3.4;H.3.5</acm-class><abstract>  We propose centralized algorithm of data distribution in the unicast p2p
network. Good example of such networks are meshes of WWW and FTP mirrors.
Simulation of data propogation for different network topologies is performed
and it is shown that proposed method performs up to 200% better then common
apporaches
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205059</id><created>2002-05-22</created><updated>2003-07-29</updated><authors><author><keyname>Perugini</keyname><forenames>Saverio</forenames></author><author><keyname>Goncalves</keyname><forenames>Marcos Andre</forenames></author><author><keyname>Fox</keyname><forenames>Edward A.</forenames></author></authors><title>A Connection-Centric Survey of Recommender Systems Research</title><categories>cs.IR cs.HC</categories><comments>Based on the comments from reviewers, we have made modifications to
  our article, including the following: Shifted the focus of the survey
  completely to recommender system research rather than recommendation and
  personalization and subsequently changed the title to &quot;A Connection-Centric
  Survey of Recommender Systems Research.&quot; Now only cite the most seminal works
  in this area and as a result have reduced the references significantly from
  over 200 to 120</comments><acm-class>A.1;H.1.0;H.1.2;H.3.0;H.3.3;H.3.4;H.3.5;H.4.2;H.5.2;H.5.4</acm-class><abstract>  Recommender systems attempt to reduce information overload and retain
customers by selecting a subset of items from a universal set based on user
preferences. While research in recommender systems grew out of information
retrieval and filtering, the topic has steadily advanced into a legitimate and
challenging research area of its own. Recommender systems have traditionally
been studied from a content-based filtering vs. collaborative design
perspective. Recommendations, however, are not delivered within a vacuum, but
rather cast within an informal community of users and social context.
Therefore, ultimately all recommender systems make connections among people and
thus should be surveyed from such a perspective. This viewpoint is
under-emphasized in the recommender systems literature. We therefore take a
connection-oriented viewpoint toward recommender systems research. We posit
that recommendation has an inherently social element and is ultimately intended
to connect people either directly as a result of explicit user modeling or
indirectly through the discovery of relationships implicit in extant data.
Thus, recommender systems are characterized by how they model users to bring
people together: explicitly or implicitly. Finally, user modeling and the
connection-centric viewpoint raise broadening and social issues--such as
evaluation, targeting, and privacy and trust--which we also briefly address.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205060</id><created>2002-05-23</created><authors><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Optimizing Queries Using a Meta-level Database</title><categories>cs.DB</categories><comments>26 pages, 5 figures</comments><acm-class>H.2.1, H.2.3, D.1.5, D.3.3</acm-class><abstract>  Graph simulation (using graph schemata or data guides) has been successfully
proposed as a technique for adding structure to semistructured data. Design
patterns for description (such as meta-classes and homomorphisms between schema
layers), which are prominent in the object-oriented programming community,
constitute a generalization of this graph simulation approach.
  In this paper, we show description applicable to a wide range of data models
that have some notion of object (-identity), and propose to turn it into a data
model primitive much like, say, inheritance. We argue that such an extension
fills a practical need in contemporary data management. Then, we present
algebraic techniques for query optimization (using the notions of described and
description queries). Finally, in the semistructured setting, we discuss the
pruning of regular path queries (with nested conditions) using description
meta-data. In this context, our notion of meta-data extends graph schemata and
data guides by meta-level values, allowing to boost query performance and to
reduce the redundancy of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205061</id><created>2002-05-23</created><authors><author><keyname>Gutowski</keyname><forenames>Marek W.</forenames></author></authors><title>Aging, double helix and small world property in genetic algorithms</title><categories>cs.NE cs.DS physics.data-an</categories><comments>Submitted to the workshop on evolutionary algorithms, Krakow
  (Cracow), Poland, Sept. 30, 2002, 6 pages, no figures, LaTeX 2.09 requires
  kaeog.sty (included)</comments><acm-class>F.2.1; G.1.6; I.1.2</acm-class><abstract>  Over a quarter of century after the invention of genetic algorithms and
miriads of their modifications, as well as successful implementations, we are
still lacking many essential details of thorough analysis of it's inner
working. One of such fundamental questions is: how many generations do we need
to solve the optimization problem? This paper tries to answer this question,
albeit in a fuzzy way, making use of the double helix concept. As a byproduct
we gain better understanding of the ways, in which the genetic algorithm may be
fine tuned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205062</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205062</id><created>2002-05-23</created><authors><author><keyname>Frumkin</keyname><forenames>Michael</forenames></author><author><keyname>Van der Wijngaart</keyname><forenames>Rob F.</forenames></author></authors><title>Minimizing Cache Misses in Scientific Computing Using Isoperimetric
  Bodies</title><categories>cs.PF</categories><comments>27 pages, 10 figures</comments><acm-class>C.4; B.8</acm-class><abstract>  A number of known techniques for improving cache performance in scientific
computations involve the reordering of the iteration space. Some of these
reorderings can be considered coverings of the iteration space with sets having
small surface-to-volume ratios. Use of such sets may reduce the number of cache
misses in computations of local operators having the iteration space as their
domain. First, we derive lower bounds on cache misses that any algorithm must
suffer while computing a local operator on a grid. Then, we explore coverings
of iteration spaces of structured and unstructured discretization grid
operators which allow us to approach these lower bounds. For structured grids
we introduce a covering by successive minima tiles based on the interference
lattice of the grid. We show that the covering has a small surface-to-volume
ratio and present a computer experiment showing actual reduction of the cache
misses achieved by using these tiles. For planar unstructured grids we show
existence of a covering which reduces the number of cache misses to the level
of that of structured grids. Next, we introduce a class of multidimensional
grids, called starry grids in this paper. These grids represent an abstraction
of unstructured grids used in, for example, molecular simulations and the
solution of partial differential equations. We show that starry grids can be
covered by sets having a low surface-to-volume ratio and, hence have the same
cache efficiency as structured grids. Finally, we present a triangulation of a
three-dimensional cube that has the property that any local operator on the
corresponding grid must incur a significantly larger number of cache misses
than a similar operator on a structured grid of the same size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205063</id><created>2002-05-24</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Distance function wavelets - Part II: Extended results and conjectures</title><categories>cs.CE cs.CG</categories><comments>Welcome any comments to wenc@simula.no</comments><acm-class>G1.8, G1.9</acm-class><abstract>  Report II is concerned with the extended results of distance function
wavelets (DFW). The fractional DFW transforms are first addressed relating to
the fractal geometry and fractional derivative, and then, the discrete
Helmholtz-Fourier transform is briefly presented. The Green second identity may
be an alternative devise in developing the theoretical framework of the DFW
transform and series. The kernel solutions of the Winkler plate equation and
the Burger's equation are used to create the DFW transforms and series. Most
interestingly, it is found that the translation invariant monomial solutions of
the high-order Laplace equations can be used to make very simple harmonic
polynomial DFW series. In most cases of this study, solid mathematical analysis
is missing and results are obtained intuitively in the conjecture status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205064</id><created>2002-05-24</created><updated>2002-09-09</updated><authors><author><keyname>Sauerbier</keyname><forenames>Charles</forenames></author></authors><title>A polynomial time (heuristic) SAT algorithm</title><categories>cs.CC</categories><comments>27 pages, 4 figures. Addendum for hole found respective a path
  inconsistency</comments><report-no>S3E-2002-01</report-no><acm-class>F.2.2;F.1.1</acm-class><abstract>  A hole has been found in the algorithm as given, where an eleventh hour
change admits a path inconsistency. The inconsistency arises due to an improper
closure of a path to a cycle against a root not supportive of the path. The
mathematical basis of the algorithm remains supportive of the intended
solution, and a revision of the algorithm is underway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205065</id><created>2002-05-25</created><authors><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Bootstrapping Lexical Choice via Multiple-Sequence Alignment</title><categories>cs.CL</categories><comments>8 pages; to appear in the proceedings of EMNLP-2002</comments><acm-class>1.2.7</acm-class><abstract>  An important component of any generation system is the mapping dictionary, a
lexicon of elementary semantic expressions and corresponding natural language
realizations. Typically, labor-intensive knowledge-based methods are used to
construct the dictionary. We instead propose to acquire it automatically via a
novel multiple-pass algorithm employing multiple-sequence alignment, a
technique commonly used in bioinformatics. Crucially, our method leverages
latent information contained in multi-parallel corpora -- datasets that supply
several verbalizations of the corresponding semantics rather than just one.
  We used our techniques to generate natural language versions of
computer-generated mathematical proofs, with good results on both a
per-component and overall-output basis. For example, in evaluations involving a
dozen human judges, our system produced output whose readability and
faithfulness to the semantic input rivaled that of a traditional generation
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205066</id><created>2002-05-26</created><authors><author><keyname>Hudson</keyname><forenames>Benoit</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Effectiveness of Preference Elicitation in Combinatorial Auctions</title><categories>cs.GT cs.MA</categories><comments>20 pages, 7 figures</comments><report-no>CMU-CS-02-124</report-no><acm-class>I.2.11</acm-class><abstract>  Combinatorial auctions where agents can bid on bundles of items are desirable
because they allow the agents to express complementarity and substitutability
between the items. However, expressing one's preferences can require bidding on
all bundles. Selective incremental preference elicitation by the auctioneer was
recently proposed to address this problem (Conen &amp; Sandholm 2001), but the idea
was not evaluated. In this paper we show, experimentally and theoretically,
that automated elicitation provides a drastic benefit. In all of the
elicitation schemes under study, as the number of items for sale increases, the
amount of information elicited is a vanishing fraction of the information
collected in traditional ``direct revelation mechanisms'' where bidders reveal
all their valuation information. Most of the elicitation schemes also maintain
the benefit as the number of agents increases. We develop more effective
elicitation policies for existing query types. We also present a new query type
that takes the incremental nature of elicitation to a new level by allowing
agents to give approximate answers that are refined only on an as-needed basis.
In the process, we present methods for evaluating different types of
elicitation policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205067</id><created>2002-05-27</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames></author></authors><title>Evaluating the Effectiveness of Ensembles of Decision Trees in
  Disambiguating Senseval Lexical Samples</title><categories>cs.CL</categories><comments>Appears in the Proceedings of the ACL-02 Workshop on Word Sense
  Disambiguation: Recent Successes and Future Directions, July 11, 2002,
  Philadelphia, PA</comments><acm-class>I.2.7</acm-class><abstract>  This paper presents an evaluation of an ensemble--based system that
participated in the English and Spanish lexical sample tasks of Senseval-2. The
system combines decision trees of unigrams, bigrams, and co--occurrences into a
single classifier. The analysis is extended to include the Senseval-1 data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205068</id><created>2002-05-27</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames></author></authors><title>Assessing System Agreement and Instance Difficulty in the Lexical Sample
  Tasks of Senseval-2</title><categories>cs.CL</categories><comments>Appears in the Proceedings of the ACL-02 Workshop on Word Sense
  Disambiguation: Recent Successes and Future Directions, July 11, 2002,
  Philadelphia, PA</comments><acm-class>I.2.7</acm-class><abstract>  This paper presents a comparative evaluation among the systems that
participated in the Spanish and English lexical sample tasks of Senseval-2. The
focus is on pairwise comparisons among systems to assess the degree to which
they agree, and on measuring the difficulty of the test instances included in
these tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205069</id><created>2002-05-27</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames></author></authors><title>Machine Learning with Lexical Features: The Duluth Approach to
  Senseval-2</title><categories>cs.CL</categories><comments>Appears in the Proceedings of SENSEVAL-2: Second International
  Workshop on Evaluating Word Sense Disambiguation Systems July 5-6, 2001,
  Toulouse, France</comments><acm-class>I.2.7</acm-class><abstract>  This paper describes the sixteen Duluth entries in the Senseval-2 comparative
exercise among word sense disambiguation systems. There were eight pairs of
Duluth systems entered in the Spanish and English lexical sample tasks. These
are all based on standard machine learning algorithms that induce classifiers
from sense-tagged training text where the context in which ambiguous words
occur are represented by simple lexical features. These are highly portable,
robust methods that can serve as a foundation for more tailored approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205070</id><created>2002-05-27</created><authors><author><keyname>Pang</keyname><forenames>Bo</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Vaithyanathan</keyname><forenames>Shivakumar</forenames></author></authors><title>Thumbs up? Sentiment Classification using Machine Learning Techniques</title><categories>cs.CL cs.LG</categories><comments>To appear in EMNLP-2002</comments><acm-class>I.2.7; I.2.6</acm-class><abstract>  We consider the problem of classifying documents not by topic, but by overall
sentiment, e.g., determining whether a review is positive or negative. Using
movie reviews as data, we find that standard machine learning techniques
definitively outperform human-produced baselines. However, the three machine
learning methods we employed (Naive Bayes, maximum entropy classification, and
support vector machines) do not perform as well on sentiment classification as
on traditional topic-based categorization. We conclude by examining factors
that make the sentiment classification problem more challenging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205071</id><created>2002-05-28</created><authors><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Brody</keyname><forenames>Tim</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author><author><keyname>Carr</keyname><forenames>Les</forenames></author><author><keyname>Maly</keyname><forenames>Kurt</forenames></author><author><keyname>Zubair</keyname><forenames>Mohammad</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>A Scalable Architecture for Harvest-Based Digital Libraries - The
  ODU/Southampton Experiments</title><categories>cs.DL cs.IR</categories><comments>20 pages, 6 figures</comments><acm-class>H.3.7</acm-class><abstract>  This paper discusses the requirements of current and emerging applications
based on the Open Archives Initiative (OAI) and emphasizes the need for a
common infrastructure to support them. Inspired by HTTP proxy, cache, gateway
and web service concepts, a design for a scalable and reliable infrastructure
that aims at satisfying these requirements is presented. Moreover it is shown
how various applications can exploit the services included in the proposed
infrastructure. The paper concludes by discussing the current status of several
prototype implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205072</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205072</id><created>2002-05-29</created><authors><author><keyname>Neuvel</keyname><forenames>Sylvain</forenames></author><author><keyname>Fulop</keyname><forenames>Sean A.</forenames></author></authors><title>Unsupervised Learning of Morphology without Morphemes</title><categories>cs.CL cs.LG</categories><comments>10 pages, to appear in Proceedings of the Workshop on Morphological
  and Phonological Learning 2002, ACL Publications</comments><acm-class>I.2.6</acm-class><abstract>  The first morphological learner based upon the theory of Whole Word
Morphology Ford et al. (1997) is outlined, and preliminary evaluation results
are presented. The program, Whole Word Morphologizer, takes a POS-tagged
lexicon as input, induces morphological relationships without attempting to
discover or identify morphemes, and is then able to generate new words beyond
the learning sample. The accuracy (precision) of the generated new words is as
high as 80% using the pure Whole Word theory, and 92% after a post-hoc
adjustment is added to the routine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205073</id><created>2002-05-28</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Vote Elicitation: Complexity and Strategy-Proofness</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>Proceedings of the 18th National Conference on Artificial
  Intelligence (AAAI-02), Edmonton, Canada, 2002</journal-ref><abstract>  Preference elicitation is a central problem in AI, and has received
significant attention in single-agent settings. It is also a key problem in
multiagent systems, but has received little attention here so far. In this
setting, the agents may have different preferences that often must be
aggregated using voting. This leads to interesting issues because what, if any,
information should be elicited from an agent depends on what other agents have
revealed about their preferences so far.
  In this paper we study effective elicitation, and its impediments, for the
most common voting protocols. It turns out that in the Single Transferable Vote
protocol, even knowing when to terminate elicitation is mathcal NP-complete,
while this is easy for all the other protocols under study. Even for these
protocols, determining how to elicit effectively is NP-complete, even with
perfect suspicions about how the agents will vote. The exception is the
Plurality protocol where such effective elicitation is easy.
  We also show that elicitation introduces additional opportunities for
strategic manipulation by the voters. We demonstrate how to curtail the space
of elicitation schemes so that no such additional strategic issues arise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205074</id><created>2002-05-28</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Complexity Results about Nash Equilibria</title><categories>cs.GT cs.CC cs.MA</categories><report-no>CMU-CS-02-135</report-no><acm-class>I.2.11</acm-class><journal-ref>In Proceedings of the 18th International Joint Conference on
  Artificial Intelligence (IJCAI-03), Acapulco, Mexico, 2003</journal-ref><abstract>  Noncooperative game theory provides a normative framework for analyzing
strategic interactions. However, for the toolbox to be operational, the
solutions it defines will have to be computed. In this paper, we provide a
single reduction that 1) demonstrates NP-hardness of determining whether Nash
equilibria with certain natural properties exist, and 2) demonstrates the
#P-hardness of counting Nash equilibria (or connected sets of Nash equilibria).
We also show that 3) determining whether a pure-strategy Bayes-Nash equilibrium
exists is NP-hard, and that 4) determining whether a pure-strategy Nash
equilibrium exists in a stochastic (Markov) game is PSPACE-hard even if the
game is invisible (this remains NP-hard if the game is finite). All of our
hardness results hold even if there are only two players and the game is
symmetric.
  Keywords: Nash equilibrium; game theory; computational complexity;
noncooperative game theory; normal form game; stochastic game; Markov game;
Bayes-Nash equilibrium; multiagent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205075</id><created>2002-05-28</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Complexity of Mechanism Design</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11</acm-class><journal-ref>Proceedings of the 18th Annual Conference on Uncertainty in
  Artificial Intelligence (UAI-02), Edmonton, Canada, 2002</journal-ref><abstract>  The aggregation of conflicting preferences is a central problem in multiagent
systems. The key difficulty is that the agents may report their preferences
insincerely. Mechanism design is the art of designing the rules of the game so
that the agents are motivated to report their preferences truthfully and a
(socially) desirable outcome is chosen. We propose an approach where a
mechanism is automatically created for the preference aggregation setting at
hand. This has several advantages, but the downside is that the mechanism
design optimization problem needs to be solved anew each time. Focusing on
settings where side payments are not possible, we show that the mechanism
design problem is NP-complete for deterministic mechanisms. This holds both for
dominant-strategy implementation and for Bayes-Nash implementation. We then
show that if we allow randomized mechanisms, the mechanism design problem
becomes tractable. In other words, the coordinator can tackle the computational
complexity introduced by its uncertainty about the agents' preferences by
making the agents face additional uncertainty. This comes at no loss, and in
some cases at a gain, in the (social) objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205076</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205076</id><created>2002-05-29</created><authors><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Complexity of Manipulating Elections with Few Candidates</title><categories>cs.GT</categories><comments>In AAAI-02 plenary session</comments><acm-class>I.2.11</acm-class><journal-ref>Proceedings of the 18th National Conference on Artificial
  Intelligence (AAAI-02), Edmonton, Canada, 2002</journal-ref><abstract>  In multiagent settings where the agents have different preferences,
preference aggregation is a central issue. Voting is a general method for
preference aggregation, but seminal results have shown that all general voting
protocols are manipulable. One could try to avoid manipulation by using voting
protocols where determining a beneficial manipulation is hard. Especially among
computational agents, it is reasonable to measure this hardness by
computational complexity. Some earlier work has been done in this area, but it
was assumed that the number of voters and candidates is unbounded. We derive
hardness results for practical multiagent settings where the number of
candidates is small but the number of voters can be large. We show that with
complete information about the others' votes, individual manipulation is easy,
and coalitional manipulation is easy with unweighted voters. However,
constructive coalitional manipulation with weighted voters is intractable for
all of the voting protocols under study, except for the nonrandomized Cup.
Destructive manipulation tends to be easier. Randomizing over instantiations of
the protocols (such as schedules of the Cup protocol) can be used to make
manipulation hard. Finally, we show that under weak assumptions, if weighted
coalitional manipulation with complete information about the others' votes is
hard in some voting protocol, then individual and unweighted manipulation is
hard when there is uncertainty about the others' votes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205077</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205077</id><created>2002-05-30</created><authors><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Raghavachari</keyname><forenames>Balaji</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Designing Multi-Commodity Flow Trees</title><categories>cs.DS cs.DM</categories><comments>Conference version in WADS'93</comments><acm-class>F.2.2; G.2.2</acm-class><journal-ref>Information Processing Letters 50:49-55 (1994)</journal-ref><doi>10.1016/0020-0190(94)90044-2</doi><abstract>  The traditional multi-commodity flow problem assumes a given flow network in
which multiple commodities are to be maximally routed in response to given
demands. This paper considers the multi-commodity flow network-design problem:
given a set of multi-commodity flow demands, find a network subject to certain
constraints such that the commodities can be maximally routed.
  This paper focuses on the case when the network is required to be a tree. The
main result is an approximation algorithm for the case when the tree is
required to be of constant degree. The algorithm reduces the problem to the
minimum-weight balanced-separator problem; the performance guarantee of the
algorithm is within a factor of 4 of the performance guarantee of the
balanced-separator procedure. If Leighton and Rao's balanced-separator
procedure is used, the performance guarantee is O(log n). This improves the
O(log^2 n) approximation factor that is trivial to obtain by a direct
application of the balanced-separator method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205078</id><created>2002-05-30</created><authors><author><keyname>Wos</keyname><forenames>Larry</forenames></author></authors><title>A Spectrum of Applications of Automated Reasoning</title><categories>cs.AI cs.LO</categories><comments>13 pages</comments><report-no>ANL/MCS-P923-0102</report-no><acm-class>I.2.3; F.4.1</acm-class><abstract>  The likelihood of an automated reasoning program being of substantial
assistance for a wide spectrum of applications rests with the nature of the
options and parameters it offers on which to base needed strategies and
methodologies. This article focuses on such a spectrum, featuring W. McCune's
program OTTER, discussing widely varied successes in answering open questions,
and touching on some of the strategies and methodologies that played a key
role. The applications include finding a first proof, discovering single
axioms, locating improved axiom systems, and simplifying existing proofs. The
last application is directly pertinent to the recently found (by R. Thiele)
Hilbert's twenty-fourth problem--which is extremely amenable to attack with the
appropriate automated reasoning program--a problem concerned with proof
simplification. The methodologies include those for seeking shorter proofs and
for finding proofs that avoid unwanted lemmas or classes of term, a specific
option for seeking proofs with smaller equational or formula complexity, and a
different option to address the variable richness of a proof. The type of proof
one obtains with the use of OTTER is Hilbert-style axiomatic, including details
that permit one sometimes to gain new insights. We include questions still open
and challenges that merit consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205079</id><created>2002-05-31</created><updated>2002-08-02</updated><authors><author><keyname>Lehmann</keyname><forenames>Daniel</forenames></author></authors><title>Connectives in Quantum and other Cumulative Logics</title><categories>cs.AI math.LO</categories><comments>21 pages</comments><report-no>TR-2002-28, Leibniz Center for Research in Computer Science, Hebrew
  University, revised August 2002</report-no><acm-class>I.2.3; F.4.1</acm-class><abstract>  Cumulative logics are studied in an abstract setting, i.e., without
connectives, very much in the spirit of Makinson's early work. A powerful
representation theorem characterizes those logics by choice functions that
satisfy a weakening of Sen's property alpha, in the spirit of the author's
&quot;Nonmonotonic Logics and Semantics&quot; (JLC). The representation results obtained
are surprisingly smooth: in the completeness part the choice function may be
defined on any set of worlds, not only definable sets and no
definability-preservation property is required in the soundness part. For
abstract cumulative logics, proper conjunction and negation may be defined.
Contrary to the situation studied in &quot;Nonmonotonic Logics and Semantics&quot; no
proper disjunction seems to be definable in general. The cumulative relations
of KLM that satisfy some weakening of the consistency preservation property all
define cumulative logics with a proper negation. Quantum Logics, as defined by
Engesser and Gabbay are such cumulative logics but the negation defined by
orthogonal complement does not provide a proper negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0205080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0205080</id><created>2002-05-31</created><updated>2002-06-01</updated><authors><author><keyname>Marko</keyname><forenames>M.</forenames></author><author><keyname>Porter</keyname><forenames>M. A.</forenames></author><author><keyname>Probst</keyname><forenames>A.</forenames></author><author><keyname>Gershenson</keyname><forenames>C.</forenames></author><author><keyname>Das</keyname><forenames>A.</forenames></author></authors><title>Transforming the World Wide Web into a Complexity-Based Semantic Network</title><categories>cs.NI cs.IR</categories><comments>6 pages, a manuscript for the ICCS 2002</comments><acm-class>I.2.4</acm-class><abstract>  The aim of this paper is to introduce the idea of the Semantic Web to the
Complexity community and set a basic ground for a project resulting in creation
of Internet-based semantic network of Complexity-related information providers.
Implementation of the Semantic Web technology would be of mutual benefit to
both the participants and users and will confirm self-referencing power of the
community to apply the products of its own research to itself. We first explain
the logic of the transition and discuss important notions associated with the
Semantic Web technology. We then present a brief outline of the project
milestones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206001</id><created>2002-06-01</created><authors><author><keyname>Das</keyname><forenames>A.</forenames></author><author><keyname>Marko</keyname><forenames>M.</forenames></author><author><keyname>Probst</keyname><forenames>A.</forenames></author><author><keyname>Porter</keyname><forenames>M. A.</forenames></author><author><keyname>Gershenson</keyname><forenames>C.</forenames></author></authors><title>Neural Net Model for Featured Word Extraction</title><categories>cs.NE cs.NI</categories><acm-class>I.2.6</acm-class><abstract>  Search engines perform the task of retrieving information related to the
user-supplied query words. This task has two parts; one is finding &quot;featured
words&quot; which describe an article best and the other is finding a match among
these words to user-defined search terms. There are two main independent
approaches to achieve this task. The first one, using the concepts of
semantics, has been implemented partially. For more details see another paper
of Marko et al., 2002. The second approach is reported in this paper. It is a
theoretical model based on using Neural Network (NN). Instead of using keywords
or reading from the first few lines from papers/articles, the present model
gives emphasis on extracting &quot;featured words&quot; from an article. Obviously we
propose to exclude prepositions, articles and so on, that is, English words
like &quot;of, the, are, so, therefore, &quot; etc. from such a list. A neural model is
taken with its nodes pre-assigned energies. Whenever a match is found with
featured words and userdefined search words, the node is fired and jumps to a
higher energy. This firing continues until the model attains a steady energy
level and total energy is now calculated. Clearly, higher match will generate
higher energy; so on the basis of total energy, a ranking is done to the
article indicating degree of relevance to the user's interest. Another
important feature of the proposed model is incorporating a semantic module to
refine the search words; like finding association among search words, etc. In
this manner, information retrieval can be improved markedly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206002</id><created>2002-06-01</created><authors><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Guoy</keyname><forenames>Damrong</forenames></author><author><keyname>Sullivan</keyname><forenames>John M.</forenames></author><author><keyname>&#xdc;ng&#xf6;r</keyname><forenames>Alper</forenames></author></authors><title>Building Space-Time Meshes over Arbitrary Spatial Domains</title><categories>cs.CG</categories><comments>12 pages, 14 figures; see also
  http://www.cs.uiuc.edu/~jeffe/pubs/slowpitch.html</comments><acm-class>F.2.2</acm-class><abstract>  We present an algorithm to construct meshes suitable for space-time
discontinuous Galerkin finite-element methods. Our method generalizes and
improves the `Tent Pitcher' algorithm of \&quot;Ung\&quot;or and Sheffer. Given an
arbitrary simplicially meshed domain M of any dimension and a time interval
[0,T], our algorithm builds a simplicial mesh of the space-time domain Mx[0,T],
in constant time per element. Our algorithm avoids the limitations of previous
methods by carefully adapting the durations of space-time elements to the local
quality and feature size of the underlying space mesh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206003</id><created>2002-06-03</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Handling Defeasibilities in Action Domains</title><categories>cs.AI</categories><comments>49 pages, 1 figure, to be appeared in journal Theory and Practice
  Logic Programming</comments><acm-class>F.4.1</acm-class><abstract>  Representing defeasibility is an important issue in common sense reasoning.
In reasoning about action and change, this issue becomes more difficult because
domain and action related defeasible information may conflict with general
inertia rules. Furthermore, different types of defeasible information may also
interfere with each other during the reasoning. In this paper, we develop a
prioritized logic programming approach to handle defeasibilities in reasoning
about action. In particular, we propose three action languages {\cal AT}^{0},
{\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in
action domains named defeasible constraints, defeasible observations and
actions with defeasible and abnormal effects respectively. Each language with a
higher superscript can be viewed as an extension of the language with a lower
superscript. These action languages inherit the simple syntax of {\cal A}
language but their semantics is developed in terms of transition systems where
transition functions are defined based on prioritized logic programs. By
illustrating various examples, we show that our approach eventually provides a
powerful mechanism to handle various defeasibilities in temporal prediction and
postdiction. We also investigate semantic properties of these three action
languages and characterize classes of action domains that present more
desirable solutions in reasoning about action within the underlying action
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206004</id><created>2002-06-03</created><authors><author><keyname>Calders</keyname><forenames>Toon</forenames></author><author><keyname>Goethals</keyname><forenames>Bart</forenames></author></authors><title>Mining All Non-Derivable Frequent Itemsets</title><categories>cs.DB cs.AI</categories><comments>3 figures</comments><acm-class>H.2.8</acm-class><abstract>  Recent studies on frequent itemset mining algorithms resulted in significant
performance improvements. However, if the minimal support threshold is set too
low, or the data is highly correlated, the number of frequent itemsets itself
can be prohibitively large. To overcome this problem, recently several
proposals have been made to construct a concise representation of the frequent
itemsets, instead of mining all frequent itemsets. The main goal of this paper
is to identify redundancies in the set of all frequent itemsets and to exploit
these redundancies in order to reduce the result of a mining operation. We
present deduction rules to derive tight bounds on the support of candidate
itemsets. We show how the deduction rules allow for constructing a minimal
representation for all frequent itemsets. We also present connections between
our proposal and recent proposals for concise representations and we give the
results of experiments on real-life datasets that show the effectiveness of the
deduction rules. In fact, the experiments even show that in many cases, first
mining the concise representation, and then creating the frequent itemsets from
this representation outperforms existing frequent set mining algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206005</id><created>2002-06-03</created><authors><author><keyname>de Jongh</keyname><forenames>Dick</forenames></author><author><keyname>Hendriks</keyname><forenames>Lex</forenames></author></authors><title>Characterization of Strongly Equivalent Logic Programs in Intermediate
  Logics</title><categories>cs.LO</categories><comments>Under consideration for publication in Theory and Practice of Logic
  Programming</comments><acm-class>D.1.6</acm-class><abstract>  The non-classical, nonmonotonic inference relation associated with the answer
set semantics for logic programs gives rise to a relationship of 'strong
equivalence' between logical programs that can be verified in 3-valued Goedel
logic, G3, the strongest non-classical intermediate propositional logic
(Lifschitz, Pearce and Valverde, 2001). In this paper we will show that KC (the
logic obtained by adding axiom ~A v ~~A to intuitionistic logic), is the
weakest intermediate logic for which strongly equivalent logic programs, in a
language allowing negations, are logically equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206006</identifier>
 <datestamp>2008-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206006</id><created>2002-06-03</created><authors><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Robust Feature Selection by Mutual Information Distributions</title><categories>cs.AI cs.LG</categories><comments>8 two-column pages</comments><report-no>IDSIA-08-02</report-no><acm-class>I.2</acm-class><journal-ref>Proc. 14th International Conference on Uncertainty in Artificial
  Intelligence (UAI 2002) pages 577-584</journal-ref><abstract>  Mutual information is widely used in artificial intelligence, in a
descriptive way, to measure the stochastic dependence of discrete random
variables. In order to address questions such as the reliability of the
empirical value, one must consider sample-to-population inferential approaches.
This paper deals with the distribution of mutual information, as obtained in a
Bayesian framework by a second-order Dirichlet prior distribution. The exact
analytical expression for the mean and an analytical approximation of the
variance are reported. Asymptotic approximations of the distribution are
proposed. The results are applied to the problem of selecting features for
incremental learning and classification of the naive Bayes classifier. A fast,
newly defined method is shown to outperform the traditional approach based on
empirical mutual information on a number of real data sets. Finally, a
theoretical development is reported that allows one to efficiently extend the
above methods to incomplete samples in an easy and effective way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206007</id><created>2002-06-04</created><authors><author><keyname>Kan</keyname><forenames>Min-Yen</forenames></author><author><keyname>Klavans</keyname><forenames>Judith L.</forenames></author><author><keyname>McKeown</keyname><forenames>Kathleen R.</forenames></author></authors><title>Using the Annotated Bibliography as a Resource for Indicative
  Summarization</title><categories>cs.CL cs.DL</categories><comments>8 pages, 3 figures</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of LREC 2002, Las Palmas, Spain. pp. 1746-1752</journal-ref><abstract>  We report on a language resource consisting of 2000 annotated bibliography
entries, which is being analyzed as part of our research on indicative document
summarization. We show how annotated bibliographies cover certain aspects of
summarization that have not been well-covered by other summary corpora, and
motivate why they constitute an important form to study for information
retrieval. We detail our methodology for collecting the corpus, and overview
our document feature markup that we introduced to facilitate summary analysis.
We present the characteristics of the corpus, methods of collection, and show
its use in finding the distribution of types of information included in
indicative summaries and their relative ordering within the summaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206008</id><created>2002-06-03</created><authors><author><keyname>Gopych</keyname><forenames>Petro M.</forenames></author></authors><title>Computer modeling of feelings and emotions: a quantitative neural
  network model of the feeling-of-knowing</title><categories>cs.AI cs.NE q-bio.NC q-bio.QM</categories><comments>A report presented at the IVth Kharkiv International Psychology
  Conference dedicated to 70th anniversary of psychology studies at Kharkiv
  University &quot;Psychology in Contemporary World :Theory and Practice&quot; held in
  Kharkiv, Ukraine, April 18-19, 2002, 5 pages, 2 Figures</comments><acm-class>I.2.0; J.4</acm-class><journal-ref>Kharkiv University Bulletin, Series Psychology, 2002, no.550(part
  1), p.54-58</journal-ref><abstract>  The first quantitative neural network model of feelings and emotions is
proposed on the base of available data on their neuroscience and evolutionary
biology nature, and on a neural network human memory model which admits
distinct description of conscious and unconscious mental processes in a time
dependent manner. As an example, proposed model is applied to quantitative
description of the feeling of knowing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206009</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206009</id><created>2002-06-04</created><updated>2002-06-06</updated><authors><author><keyname>Felkel</keyname><forenames>Petr</forenames></author><author><keyname>Bruckwschwaiger</keyname><forenames>Mario</forenames></author><author><keyname>Wegenkittl</keyname><forenames>Rainer</forenames></author></authors><title>Implementation and complexity of the watershed-from-markers algorithm
  computed as a minimal cost forest</title><categories>cs.DS cs.CG</categories><comments>v1: 10 pages, 6 figures, 7 tables EUROGRAPHICS conference,
  Manchester, UK, 2001. v2: 12 pages, reformated for letter, corrected IFT to
  &quot;Image Foresting Tranform&quot;</comments><acm-class>I.3.5 [Computational Geometry and Object Modeling]</acm-class><journal-ref>Computer Graphics Forum, Vol 20, No 3, 2001, Conference Issue,
  pages C-26 - C-35, ISSN 0167-7075</journal-ref><abstract>  The watershed algorithm belongs to classical algorithms in mathematical
morphology. Lotufo et al. published a principle of the watershed computation by
means of an Image Foresting Transform (IFT), which computes a shortest path
forest from given markers. The algorithm itself was described for a 2D case
(image) without a detailed discussion of its computation and memory demands for
real datasets. As IFT cleverly solves the problem of plateaus and as it gives
precise results when thin objects have to be segmented, it is obvious to use
this algorithm for 3D datasets taking in mind the minimizing of a higher memory
consumption for the 3D case without loosing low asymptotical time complexity of
O(m+C) (and also the real computation speed). The main goal of this paper is an
implementation of the IFT algorithm with a priority queue with buckets and
careful tuning of this implementation to reach as minimal memory consumption as
possible.
  The paper presents five possible modifications and methods of implementation
of the IFT algorithm. All presented implementations keep the time complexity of
the standard priority queue with buckets but the best one minimizes the costly
memory allocation and needs only 19-45% of memory for typical 3D medical
imaging datasets. Memory saving was reached by an IFT algorithm simplification,
which stores more elements in temporary structures but these elements are
simpler and thus need less memory.
  The best presented modification allows segmentation of large 3D medical
datasets (up to 512x512x680 voxels) with 12-or 16-bits per voxel on currently
available PC based workstations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206010</id><created>2002-06-06</created><updated>2002-07-14</updated><authors><author><keyname>Liberti</keyname><forenames>Leo</forenames></author></authors><title>Performance Comparison of Function Evaluation Methods</title><categories>cs.SC cs.NA</categories><comments>Main conclusion of this paper is wrong due to a bug in the test code;
  see &quot;Important warning&quot; at the beginning of the paper</comments><acm-class>I.1.3</acm-class><abstract>  We perform a comparison of the performance and efficiency of four different
function evaluation methods: black-box functions, binary trees, $n$-ary trees
and string parsing. The test consists in evaluating 8 different functions of
two variables $x,y$ over 5000 floating point values of the pair $(x,y)$. The
outcome of the test indicates that the $n$-ary tree representation of algebraic
expressions is the fastest method, closely followed by black-box function
method, then by binary trees and lastly by string parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206011</id><created>2002-06-07</created><authors><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author><author><keyname>Redner</keyname><forenames>S.</forenames></author></authors><title>A Statistical Physics Perspective on Web Growth</title><categories>cs.NI cond-mat.stat-mech cs.GL</categories><comments>20 pages; solicited mini-review to appear in aspecial issue of
  Computer Networks and ISDN Systems; submitted to the journal April 1, 2002</comments><acm-class>C.2.6; E.1; G.2.2</acm-class><journal-ref>Computer Networks 39, 261-276 (2002)</journal-ref><abstract>  Approaches from statistical physics are applied to investigate the structure
of network models whose growth rules mimic aspects of the evolution of the
world-wide web. We first determine the degree distribution of a growing network
in which nodes are introduced one at a time and attach to an earlier node of
degree k with rate A_ksim k^gamma. Very different behaviors arise for gamma&lt;1,
gamma=1, and gamma&gt;1. We also analyze the degree distribution of a
heterogeneous network, the joint age-degree distribution, the correlation
between degrees of neighboring nodes, as well as global network properties. An
extension to directed networks is then presented. By tuning model parameters to
reasonable values, we obtain distinct power-law forms for the in-degree and
out-degree distributions with exponents that are in good agreement with current
data for the web. Finally, a general growth process with independent
introduction of nodes and links is investigated. This leads to independently
growing sub-networks that may coalesce with other sub-networks. General results
for both the size distribution of sub-networks and the degree distribution are
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206012</id><created>2002-06-07</created><updated>2002-07-17</updated><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author></authors><title>Fast Deterministic Consensus in a Noisy Environment</title><categories>cs.DS cs.DC</categories><comments>Typographical errors fixed</comments><acm-class>F.2.2</acm-class><abstract>  It is well known that the consensus problem cannot be solved
deterministically in an asynchronous environment, but that randomized solutions
are possible. We propose a new model, called noisy scheduling, in which an
adversarial schedule is perturbed randomly, and show that in this model
randomness in the environment can substitute for randomness in the algorithm.
In particular, we show that a simplified, deterministic version of Chandra's
wait-free shared-memory consensus algorithm (PODC, 1996, pp. 166-175) solves
consensus in time at most logarithmic in the number of active processes. The
proof of termination is based on showing that a race between independent
delayed renewal processes produces a winner quickly. In addition, we show that
the protocol finishes in constant time using quantum and priority-based
scheduling on a uniprocessor, suggesting that it is robust against the choice
of model over a wide range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206013</id><created>2002-06-08</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>High-order fundamental and general solutions of convection-diffusion
  equation and their applications with boundary particle method</title><categories>cs.CE cs.CG</categories><comments>To appear in Int. J. of Engng. Anal. Bound Elem very soon. The
  present version has a little difference from the journal version. Welcome to
  contact wenc@simula.no</comments><acm-class>G1.3, G1.8</acm-class><abstract>  In this study, we presented the high-order fundamental solutions and general
solutions of convection-diffusion equation. To demonstrate their efficacy, we
applied the high-order general solutions to the boundary particle method (BPM)
for the solution of some inhomogeneous convection-diffusion problems, where the
BPM is a new truly boundary-only meshfree collocation method based on multiple
reciprocity principle. For the sake of completeness, the BPM is also briefly
described here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206014</id><created>2002-06-09</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>A Method for Open-Vocabulary Speech-Driven Text Retrieval</title><categories>cs.CL</categories><comments>Proceedings of the 2002 Conference on Empirical Methods in Natural
  Language Processing (To appear)</comments><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>Proceedings of the 2002 Conference on Empirical Methods in Natural
  Language Processing (EMNLP-2002), pp.188-195, July. 2002</journal-ref><abstract>  While recent retrieval techniques do not limit the number of index terms,
out-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at
retrieving information with spoken queries, we fill the gap between speech
recognition and text retrieval in terms of the vocabulary size. Given a spoken
query, we generate a transcription and detect OOV words through speech
recognition. We then correspond detected OOV words to terms indexed in a target
collection to complete the transcription, and search the collection for
documents relevant to the completed transcription. We show the effectiveness of
our method by way of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206015</id><created>2002-06-09</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Japanese/English Cross-Language Information Retrieval: Exploration of
  Query Translation and Transliteration</title><categories>cs.CL</categories><acm-class>H.3.3; I.2.7</acm-class><journal-ref>Computers and the Humanities, Vol.35, No.4, pp.389-420, 2001</journal-ref><abstract>  Cross-language information retrieval (CLIR), where queries and documents are
in different languages, has of late become one of the major topics within the
information retrieval community. This paper proposes a Japanese/English CLIR
system, where we combine a query translation and retrieval modules. We
currently target the retrieval of technical documents, and therefore the
performance of our system is highly dependent on the quality of the translation
of technical terms. However, the technical term translation is still
problematic in that technical terms are often compound words, and thus new
terms are progressively created by combining existing base words. In addition,
Japanese often represents loanwords based on its special phonogram.
Consequently, existing dictionaries find it difficult to achieve sufficient
coverage. To counter the first problem, we produce a Japanese/English
dictionary for base words, and translate compound words on a word-by-word
basis. We also use a probabilistic method to resolve translation ambiguity. For
the second problem, we use a transliteration method, which corresponds words
unlisted in the base word dictionary to their phonetic equivalents in the
target language. We evaluate our system using a test collection for CLIR, and
show that both the compound word translation and transliteration methods
improve the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206016</id><created>2002-06-10</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Distance function wavelets - Part III: &quot;Exotic&quot; transforms and series</title><categories>cs.CE cs.CG</categories><comments>Welcome comments to wenc@simula.no</comments><acm-class>G1.8, G1.9</acm-class><abstract>  Part III of the reports consists of various unconventional distance function
wavelets (DFW). The dimension and the order of partial differential equation
(PDE) are first used as a substitute of the scale parameter in the DFW
transforms and series, especially with the space and time-space potential
problems. It is noted that the recursive multiple reciprocity formulation is
the DFW series. The Green second identity is used to avoid the singularity of
the zero-order fundamental solution in creating the DFW series. The fundamental
solutions of various composite PDEs are found very flexible and efficient to
handle a borad range of problems. We also discuss the underlying connections
between the crucial concepts of dimension, scale and the order of PDE through
the analysis of dissipative acoustic wave propagation. The shape parameter of
the potential problems is also employed as the &quot;scale parameter&quot; to create the
non-orthogonal DFW. This paper also briefly discusses and conjectures the DFW
correspondences of a variety of coordinate variable transforms and series.
Practically important, the anisotropic and inhomogeneous DFW's are developed by
using the geodesic distance variable. The DFW and the related basis functions
are also used in making the kernel distance sigmoidal functions, which are
potentially useful in the artificial neural network and machine learning. As or
even worse than the preceding two reports, this study scarifies mathematical
rigor and in turn unfetter imagination. Most results are intuitively obtained
without rigorous analysis. Follow-up research is still under way. The paper is
intended to inspire more research into this promising area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206017</id><created>2002-06-10</created><authors><author><keyname>Ma</keyname><forenames>Shilong</forenames></author><author><keyname>Sui</keyname><forenames>Yuefei</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>The Prioritized Inductive Logic Programs</title><categories>cs.AI cs.LG</categories><comments>9 pages. Welcome any comments to kexu@nlsde.buaa.edu.cn</comments><acm-class>I.2.3; I.2.6</acm-class><abstract>  The limit behavior of inductive logic programs has not been explored, but
when considering incremental or online inductive learning algorithms which
usually run ongoingly, such behavior of the programs should be taken into
account. An example is given to show that some inductive learning algorithm may
not be correct in the long run if the limit behavior is not considered. An
inductive logic program is convergent if given an increasing sequence of
example sets, the program produces a corresponding sequence of the Horn logic
programs which has the set-theoretic limit, and is limit-correct if the limit
of the produced sequence of the Horn logic programs is correct with respect to
the limit of the sequence of the example sets. It is shown that the GOLEM
system is not limit-correct. Finally, a limit-correct inductive logic system,
called the prioritized GOLEM system, is proposed as a solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206018</id><created>2002-06-11</created><updated>2002-09-09</updated><authors><author><keyname>Duncan</keyname><forenames>C. A.</forenames></author><author><keyname>Efrat</keyname><forenames>A.</forenames></author><author><keyname>Erten</keyname><forenames>C.</forenames></author><author><keyname>Kobourov</keyname><forenames>S.</forenames></author><author><keyname>Mitchell</keyname><forenames>J. S. B.</forenames></author></authors><title>On Simultaneous Graph Embedding</title><categories>cs.CG</categories><comments>12 pages, 4 figures</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  We consider the problem of simultaneous embedding of planar graphs. There are
two variants of this problem, one in which the mapping between the vertices of
the two graphs is given and another where the mapping is not given. In
particular, we show that without mapping, any number of outerplanar graphs can
be embedded simultaneously on an $O(n)\times O(n)$ grid, and an outerplanar and
general planar graph can be embedded simultaneously on an $O(n^2)\times O(n^3)$
grid. If the mapping is given, we show how to embed two paths on an $n \times
n$ grid, a caterpillar and a path on an $n \times 2n$ grid, or two caterpillar
graphs on an $O(n^2)\times O(n^3)$ grid. We also show that 5 paths, or 3
caterpillars, or two general planar graphs cannot be simultaneously embedded
given the mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206019</id><created>2002-06-11</created><authors><author><keyname>Erten</keyname><forenames>C.</forenames></author><author><keyname>Kobourov</keyname><forenames>S. G.</forenames></author></authors><title>Simultaneous Embedding of a Planar Graph and Its Dual on the Grid</title><categories>cs.CG</categories><comments>14 pages, 8 figures</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  Traditional representations of graphs and their duals suggest the requirement
that the dual vertices be placed inside their corresponding primal faces, and
the edges of the dual graph cross only their corresponding primal edges. We
consider the problem of simultaneously embedding a planar graph and its dual
into a small integer grid such that the edges are drawn as straight-line
segments and the only crossings are between primal-dual pairs of edges. We
provide a linear-time algorithm that simultaneously embeds a 3-connected planar
graph and its dual on a (2n-2) by (2n-2) integer grid, where n is the total
number of vertices in the graph and its dual. Furthermore our embedding
algorithm satisfies the two natural requirements mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206020</id><created>2002-06-13</created><authors><author><keyname>Gudkov</keyname><forenames>Vladimir</forenames></author><author><keyname>Johnson</keyname><forenames>Joseph E.</forenames></author></authors><title>Multidimensional Network Monitoring for Intrusion Detection</title><categories>cs.CR</categories><comments>Talk at the International Conference on Complex Systems, June 9-14,
  2002, Nashua, NH</comments><acm-class>C.2.0.;C.2.3.;C.2.5</acm-class><abstract>  An approach for real-time network monitoring in terms of numerical
time-dependant functions of protocol parameters is suggested. Applying complex
systems theory for information f{l}ow analysis of networks, the information
traffic is described as a trajectory in multi-dimensional parameter-time space
with about 10-12 dimensions. The network traffic description is synthesized by
applying methods of theoretical physics and complex systems theory, to provide
a robust approach for network monitoring that detects known intrusions, and
supports developing real systems for detection of unknown intrusions. The
methods of data analysis and pattern recognition presented are the basis of a
technology study for an automatic intrusion detection system that detects the
attack in the reconnaissance stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206021</id><created>2002-06-14</created><authors><author><keyname>Monge</keyname><forenames>R. Asensio</forenames><affiliation>U. of Oviedo</affiliation></author><author><keyname>Marco</keyname><forenames>F. Sanchis</forenames><affiliation>U.P. of Madrid</affiliation></author><author><keyname>Cervigon</keyname><forenames>F. Torre</forenames><affiliation>U. of Oviedo</affiliation></author></authors><title>The analysis of the IFPUG method sensitivity</title><categories>cs.SE</categories><comments>17 pages, 4 figures</comments><acm-class>D.4.8</acm-class><abstract>  J. Albrecht`s Function Point Analysis (FPA) is a method to determine the
functional size of software products. An organization called International
Function Point Users Group (IPFUG), considers the FPA as a standard in the
software functional size measurement. The Albrechts method is followed by IPFUG
method which includes some modifications in order to improve it. A limitation
of the method refers to the fact that FPA is not sensitive enough to
differentiate the functional size in small enhancements. That affects the
productivity analysis, where the software product functional size is required.
To provide more power to the functional size measurement, A. Abran, M. Maya and
H. Nguyeckim have proposed some modifications to improve it. The IPFUG v 4.1
method which includes these modifications is named IFPUG v 4.1 extended. In
this work we set the conditions to delimiting granular from non granular
functions and we calculate the static calibration and sensitivity graphs for
the measurements of a set of projects with a high percentage of granular
functions, all of then measured with the IFPUG v 4.1 method and the IFPUG v 4.1
extended. Finally, we introduce a statistic analysis in order to determine
whether significant differences exist between both methods or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206022</id><created>2002-06-14</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>The Fastest and Shortest Algorithm for All Well-Defined Problems</title><categories>cs.CC cs.LO</categories><comments>12 pages, 1 figure</comments><report-no>IDSIA-16-00</report-no><acm-class>F.2.3</acm-class><journal-ref>International Journal of Foundations of Computer Science, Vol.13,
  No.3, June 2002, 431-443</journal-ref><abstract>  An algorithm $M$ is described that solves any well-defined problem $p$ as
quickly as the fastest algorithm computing a solution to $p$, save for a factor
of 5 and low-order additive terms. $M$ optimally distributes resources between
the execution of provably correct $p$-solving programs and an enumeration of
all proofs, including relevant proofs of program correctness and of time bounds
on program runtimes. $M$ avoids Blum's speed-up theorem by ignoring programs
without correctness proof. $M$ has broader applicability and can be faster than
Levin's universal search, the fastest method for inverting functions save for a
large multiplicative constant. An extension of Kolmogorov complexity and two
novel natural measures of function complexity are used to show that the most
efficient program computing some function $f$ is also among the shortest
programs provably computing $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206023</id><created>2002-06-15</created><authors><author><keyname>Goethals</keyname><forenames>Bart</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Relational Association Rules: getting WARMeR</title><categories>cs.DB cs.AI</categories><acm-class>H.2.8</acm-class><abstract>  In recent years, the problem of association rule mining in transactional data
has been well studied. We propose to extend the discovery of classical
association rules to the discovery of association rules of conjunctive queries
in arbitrary relational data, inspired by the WARMR algorithm, developed by
Dehaspe and Toivonen, that discovers association rules over a limited set of
conjunctive queries. Conjunctive query evaluation in relational databases is
well understood, but still poses some great challenges when approached from a
discovery viewpoint in which patterns are generated and evaluated with respect
to some well defined search space and pruning operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206024</id><created>2002-06-15</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author><author><keyname>Dani</keyname><forenames>Anita</forenames></author></authors><title>Sierpinski Gaskets for Logic Functions Representation</title><categories>cs.LO cs.DM</categories><comments>7 pages, 8 figures, 3 tables, experiments</comments><acm-class>B.6.3</acm-class><journal-ref>ISMVL 2002 Proceedinds</journal-ref><abstract>  This paper introduces a new approach to represent logic functions in the form
of Sierpinski Gaskets. The structure of the gasket allows to manipulate with
the corresponding logic expression using recursive essence of fractals. Thus,
the Sierpinski gasket's pattern has myriad useful properties which can enhance
practical features of other graphic representations like decision diagrams. We
have covered possible applications of Sierpinski gaskets in logic design and
justified our assumptions in logic function minimization (both Boolean and
multiple-valued cases). The experimental results on benchmarks with advances in
the novel structure are considered as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206025</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206025</id><created>2002-06-17</created><authors><author><keyname>Kehagias</keyname><forenames>Ath.</forenames></author></authors><title>The Lattice of Fuzzy Intervals and Sufficient Conditions for its
  Distributivity</title><categories>cs.OH</categories><acm-class>I5.1</acm-class><abstract>  Given a reference lattice, we define fuzzy intervals to be the fuzzy sets
such that their p-cuts are crisp closed intervals. We show that: given a
complete reference lattice, the collection of its fuzzy intervals is a complete
lattice. Furthermore we show that: if the reference lattice is completely
distributive then the lattice of its fuzzy intervals is distributive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206026</id><created>2002-06-18</created><authors><author><keyname>Schuler</keyname><forenames>William</forenames></author></authors><title>Interleaved semantic interpretation in environment-based parsing</title><categories>cs.CL cs.HC</categories><acm-class>I.2.7; H.2.5</acm-class><journal-ref>Proceedings of the 19th International Conference on Computational
  Linguistics (COLING 2002)</journal-ref><abstract>  This paper extends a polynomial-time parsing algorithm that resolves
structural ambiguity in input to a speech-based user interface by calculating
and comparing the denotations of rival constituents, given some model of the
interfaced application environment (Schuler 2001). The algorithm is extended to
incorporate a full set of logical operators, including quantifiers and
conjunctions, into this calculation without increasing the complexity of the
overall algorithm beyond polynomial time, both in terms of the length of the
input and the number of entities in the environment model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206027</id><created>2002-06-18</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Behaviour-based Knowledge Systems: An Epigenetic Path from Behaviour to
  Knowledge</title><categories>cs.AI cs.AR cs.NE</categories><comments>7 pages, 3 figures. To be published in Proceedings of 2nd Workshop on
  Epigenetic Robotics. Edinburgh, Aug. 10-11 2002</comments><acm-class>I.2.0, I.2.6</acm-class><abstract>  In this paper we expose the theoretical background underlying our current
research. This consists in the development of behaviour-based knowledge
systems, for closing the gaps between behaviour-based and knowledge-based
systems, and also between the understandings of the phenomena they model. We
expose the requirements and stages for developing behaviour-based knowledge
systems and discuss their limits. We believe that these are necessary
conditions for the development of higher order cognitive capacities, in
artificial and natural cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206028</id><created>2002-06-19</created><updated>2002-07-31</updated><authors><author><keyname>Eiden</keyname><forenames>Wolfgang</forenames></author></authors><title>Knowledge management for enterprises (Wissensmanagement fuer Unternehmen)</title><categories>cs.IR cs.AI</categories><comments>published in January 2000, 22 pages, 13 figures, german</comments><acm-class>H.3.0; I.2.4</acm-class><abstract>  Although knowledge is one of the most valuable resource of enterprises and an
important production and competition factor, this intellectual potential is
often used (or maintained) only inadequate by the enterprises. Therefore, in a
globalised and growing market the optimal usage of existing knowledge
represents a key factor for enterprises of the future. Here, knowledge
management systems should engage facilitating. Because geographically far
distributed establishments cause, however, a distributed system, this paper
should uncover the spectrum connected with it and present a possible basic
approach which is based on ontologies and modern, platform independent
technologies. Last but not least this attempt, as well as general questions of
the knowledge management, are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206029</id><created>2002-06-20</created><authors><author><keyname>Lin</keyname><forenames>Alice J.</forenames></author></authors><title>Computer-Generated Photorealistic Hair</title><categories>cs.GR</categories><comments>7 pages, 7 figures</comments><acm-class>I.3.3</acm-class><abstract>  This paper presents an efficient method for generating and rendering
photorealistic hair in two dimensional pictures. The method consists of three
major steps. Simulating an artist drawing is used to design the rough hair
shape. A convolution based filter is then used to generate photorealistic hair
patches. A refine procedure is finally used to blend the boundaries of the
patches with surrounding areas. This method can be used to create all types of
photorealistic human hair (head hair, facial hair and body hair). It is also
suitable for fur and grass generation. Applications of this method include:
hairstyle designing/editing, damaged hair image restoration, human hair
animation, virtual makeover of a human, and landscape creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206030</id><created>2002-06-20</created><authors><author><keyname>Seki</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero
  Pronoun Detection and Resolution</title><categories>cs.CL</categories><comments>Proceedings of the 19th International Conference on Computational
  Linguistics (To appear)</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the 19th International Conference on Computational
  Linguistics (COLING 2002), pp.911-917, Aug. 2002</journal-ref><abstract>  This paper proposes a method to analyze Japanese anaphora, in which zero
pronouns (omitted obligatory cases) are used to refer to preceding entities
(antecedents). Unlike the case of general coreference resolution, zero pronouns
have to be detected prior to resolution because they are not expressed in
discourse. Our method integrates two probability parameters to perform zero
pronoun detection and resolution in a single framework. The first parameter
quantifies the degree to which a given case is a zero pronoun. The second
parameter quantifies the degree to which a given entity is the antecedent for a
detected zero pronoun. To compute these parameters efficiently, we use corpora
with/without annotations of anaphoric relations. We show the effectiveness of
our method by way of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206031</id><created>2002-06-20</created><authors><author><keyname>Tarasov</keyname><forenames>S.</forenames></author></authors><title>A sufficient condition for global invertibility of Lipschitz mapping</title><categories>cs.NA</categories><comments>LATeX2e, 3 pages, MSC-class: 26B10</comments><acm-class>G.1.1; G.1.2</acm-class><abstract>  We show that S.Vavasis' sufficient condition for global invertibility of a
polynomial mapping can be easily generalized to the case of a general Lipschitz
mapping. Keywords: Invertibility conditions, generalized Jacobian, nonsmooth
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206032</id><created>2002-06-21</created><authors><author><keyname>Parisse</keyname><forenames>Bernard</forenames></author></authors><title>A correct proof of the heuristic GCD algorithm</title><categories>cs.SC</categories><acm-class>G.4</acm-class><abstract>  In this note, we fill a gap in the proof of the heuristic GCD in the
multivariate case made by Char, Geddes and Gonnet (JSC 1989) and give some
additionnal information on this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206033</id><created>2002-06-24</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Falmagne</keyname><forenames>Jean-Claude</forenames></author></authors><title>Algorithms for Media</title><categories>cs.DS</categories><comments>12 pages</comments><acm-class>F.2.2</acm-class><abstract>  Falmagne recently introduced the concept of a medium, a combinatorial object
encompassing hyperplane arrangements, topological orderings, acyclic
orientations, and many other familiar structures. We find efficient solutions
for several algorithmic problems on media: finding short reset sequences,
shortest paths, testing whether a medium has a closed orientation, and listing
the states of a medium given a black-box description.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206034</id><created>2002-06-24</created><authors><author><keyname>Fukui</keyname><forenames>Masatoshi</forenames></author><author><keyname>Higuchi</keyname><forenames>Shigeto</forenames></author><author><keyname>Nakatani</keyname><forenames>Youichi</forenames></author><author><keyname>Tanaka</keyname><forenames>Masao</forenames></author><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Applying a Hybrid Query Translation Method to Japanese/English
  Cross-Language Patent Retrieval</title><categories>cs.CL</categories><acm-class>H.3.3; I.2.7</acm-class><journal-ref>ACM SIGIR 2000 Workshop on Patent Retrieval, July, 2000</journal-ref><abstract>  This paper applies an existing query translation method to cross-language
patent retrieval. In our method, multiple dictionaries are used to derive all
possible translations for an input query, and collocational statistics are used
to resolve translation ambiguity. We used Japanese/English parallel patent
abstracts to perform comparative experiments, where our method outperformed a
simple dictionary-based query translation method, and achieved 76% of
monolingual retrieval in terms of average precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206035</id><created>2002-06-24</created><authors><author><keyname>Higuchi</keyname><forenames>Shigeto</forenames></author><author><keyname>Fukui</keyname><forenames>Masatoshi</forenames></author><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>PRIME: A System for Multi-lingual Patent Retrieval</title><categories>cs.CL</categories><acm-class>H.3.3; I.2.7</acm-class><journal-ref>Proceedings of MT Summit VIII, pp.163-167, Sep. 2001</journal-ref><abstract>  Given the growing number of patents filed in multiple countries, users are
interested in retrieving patents across languages. We propose a multi-lingual
patent retrieval system, which translates a user query into the target
language, searches a multilingual database for patents relevant to the query,
and improves the browsing efficiency by way of machine translation and
clustering. Our system also extracts new translations from patent families
consisting of comparable patents, to enhance the translation dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206036</id><created>2002-06-24</created><authors><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Language Modeling for Multi-Domain Speech-Driven Text Retrieval</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>IEEE Automatic Speech Recognition and Understanding Workshop, Dec.
  2001</journal-ref><abstract>  We report experimental results associated with speech-driven text retrieval,
which facilitates retrieving information in multiple domains with spoken
queries. Since users speak contents related to a target collection, we produce
language models used for speech recognition based on the target collection, so
as to improve both the recognition and retrieval accuracy. Experiments using
existing test collections combined with dictated queries showed the
effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206037</id><created>2002-06-24</created><authors><author><keyname>Fujii</keyname><forenames>Atsushi</forenames></author><author><keyname>Itou</keyname><forenames>Katunobu</forenames></author><author><keyname>Ishikawa</keyname><forenames>Tetsuya</forenames></author></authors><title>Speech-Driven Text Retrieval: Using Target IR Collections for
  Statistical Language Model Adaptation in Speech Recognition</title><categories>cs.CL</categories><acm-class>I.2.7; H.3.3; H.5.1</acm-class><journal-ref>Anni R. Coden and Eric W. Brown and Savitha Srinivasan (Eds.),
  Information Retrieval Techniques for Speech Applications (LNCS 2273),
  pp.94-104, Springer, 2002</journal-ref><abstract>  Speech recognition has of late become a practical technology for real world
applications. Aiming at speech-driven text retrieval, which facilitates
retrieving information with spoken queries, we propose a method to integrate
speech recognition and retrieval methods. Since users speak contents related to
a target collection, we adapt statistical language models used for speech
recognition based on the target collection, so as to improve both the
recognition and retrieval accuracy. Experiments using existing test collections
combined with dictated queries showed the effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206038</id><created>2002-06-24</created><authors><author><keyname>Karonis</keyname><forenames>N. T.</forenames></author><author><keyname>de Supinski</keyname><forenames>B.</forenames></author><author><keyname>Foster</keyname><forenames>I.</forenames></author><author><keyname>Gropp</keyname><forenames>W.</forenames></author><author><keyname>Lusk</keyname><forenames>E.</forenames></author></authors><title>A Multilevel Approach to Topology-Aware Collective Operations in
  Computational Grids</title><categories>cs.DC</categories><comments>16 pages, 8 figures</comments><report-no>Preprint ANL/MCS-P948-0402</report-no><acm-class>D.1.3</acm-class><abstract>  The efficient implementation of collective communiction operations has
received much attention. Initial efforts produced &quot;optimal&quot; trees based on
network communication models that assumed equal point-to-point latencies
between any two processes. This assumption is violated in most practical
settings, however, particularly in heterogeneous systems such as clusters of
SMPs and wide-area &quot;computational Grids,&quot; with the result that collective
operations perform suboptimally. In response, more recent work has focused on
creating topology-aware trees for collective operations that minimize
communication across slower channels (e.g., a wide-area network). While these
efforts have significant communication benefits, they all limit their view of
the network to only two layers. We present a strategy based upon a multilayer
view of the network. By creating multilevel topology-aware trees we take
advantage of communication cost differences at every level in the network. We
used this strategy to implement topology-aware versions of several MPI
collective operations in MPICH-G2, the Globus Toolkit[tm]-enabled version of
the popular MPICH implementation of the MPI standard. Using information about
topology provided by MPICH-G2, we construct these multilevel topology-aware
trees automatically during execution. We present results demonstrating the
advantages of our multilevel approach by comparing it to the default
(topology-unaware) implementation provided by MPICH and a topology-aware
two-layer implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206039</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206039</id><created>2002-06-25</created><authors><author><keyname>Kehagias</keyname><forenames>Ath.</forenames></author></authors><title>Hidden Markov model segmentation of hydrological and enviromental time
  series</title><categories>cs.CE cs.NA math.NA nlin.CD physics.data-an</categories><acm-class>G.3; I.5</acm-class><abstract>  Motivated by Hubert's segmentation procedure we discuss the application of
hidden Markov models (HMM) to the segmentation of hydrological and enviromental
time series. We use a HMM algorithm which segments time series of several
hundred terms in a few seconds and is computationally feasible for even longer
time series. The segmentation algorithm computes the Maximum Likelihood
segmentation by use of an expectation / maximization iteration. We rigorously
prove algorithm convergence and use numerical experiments, involving
temperature and river discharge time series, to show that the algorithm usually
converges to the globally optimal segmentation. The relation of the proposed
algorithm to Hubert's segmentation procedure is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206040</id><created>2002-06-25</created><updated>2002-06-25</updated><authors><author><keyname>Karonis</keyname><forenames>N. T.</forenames></author><author><keyname>Toonen</keyname><forenames>B.</forenames></author><author><keyname>Foster</keyname><forenames>I.</forenames></author></authors><title>MPICH-G2: A Grid-Enabled Implementation of the Message Passing Interface</title><categories>cs.DC</categories><comments>20 pages, 8 figures</comments><report-no>Preprint ANL/MCS-P942-0402</report-no><acm-class>D.1.3</acm-class><abstract>  Application development for distributed computing &quot;Grids&quot; can benefit from
tools that variously hide or enable application-level management of critical
aspects of the heterogeneous environment. As part of an investigation of these
issues, we have developed MPICH-G2, a Grid-enabled implementation of the
Message Passing Interface (MPI) that allows a user to run MPI programs across
multiple computers, at the same or different sites, using the same commands
that would be used on a parallel computer. This library extends the Argonne
MPICH implementation of MPI to use services provided by the Globus Toolkit for
authentication, authorization, resource allocation, executable staging, and
I/O, as well as for process creation, monitoring, and control. Various
performance-critical operations, including startup and collective operations,
are configured to exploit network topology information. The library also
exploits MPI constructs for performance management; for example, the MPI
communicator construct is used for application-level discovery of, and
adaptation to, both network topology and network quality-of-service mechanisms.
We describe the MPICH-G2 design and implementation, present performance
results, and review application experiences, including record-setting
distributed simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0206041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0206041</id><created>2002-06-26</created><updated>2003-02-19</updated><authors><author><keyname>Laaksolahti</keyname><forenames>Jarmo</forenames></author><author><keyname>Boman</keyname><forenames>Magnus</forenames></author></authors><title>Anticipatory Guidance of Plot</title><categories>cs.AI</categories><comments>19 pages, 5 figures</comments><acm-class>I.2.11; I.6.3; I.6.5</acm-class><abstract>  An anticipatory system for guiding plot development in interactive narratives
is described. The executable model is a finite automaton that provides the
implemented system with a look-ahead. The identification of undesirable future
states in the model is used to guide the player, in a transparent manner. In
this way, too radical twists of the plot can be avoided. Since the player
participates in the development of the plot, such guidance can have many forms,
depending on the environment of the player, on the behavior of the other
players, and on the means of player interaction. We present a design method for
interactive narratives which produces designs suitable for the implementation
of anticipatory mechanisms. Use of the method is illustrated by application to
our interactive computer game Kaktus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207001</id><created>2002-07-01</created><authors><author><keyname>Fussell</keyname><forenames>Ronald M.</forenames></author></authors><title>National Infrastructure Contingencies: Survey of Wireless Technology
  Support</title><categories>cs.DC cs.CE</categories><comments>19 pages</comments><acm-class>C.2.5</acm-class><abstract>  In modern society, the flow of information has become the lifeblood of
commerce and social interaction. This movement of data supports most aspects of
the United States economy in particular, as well as, serving as the vehicle
upon which governmental agencies react to social conditions. In addition, it is
understood that the continuance of efficient and reliable data communications
during times of national or regional disaster remains a priority in the United
States. The coordination of emergency response and area revitalization /
rehabilitation efforts between local, state, and federal emergency response is
increasingly necessary as agencies strive to work more seamlessly between the
affected organizations. Additionally, international support is often made
available to react to such adverse conditions as wildfire suppression scenarios
and therefore require the efficient management of workforce and associated
logistics support.
  It is through the examination of the issues related to un-tethered data
transmission during infrastructure contingencies that responders may best
tailor a unified approach to the rapid recovery after disasters occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207002</id><created>2002-07-01</created><authors><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Goldsmith</keyname><forenames>John</forenames></author></authors><title>Using eigenvectors of the bigram graph to infer morpheme identity</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><abstract>  This paper describes the results of some experiments exploring statistical
methods to infer syntactic behavior of words and morphemes from a raw corpus in
an unsupervised fashion. It shares certain points in common with Brown et al
(1992) and work that has grown out of that: it employs statistical techniques
to analyze syntactic behavior based on what words occur adjacent to a given
word. However, we use an eigenvector decomposition of a nearest-neighbor graph
to produce a two-dimensional rendering of the words of a corpus in which words
of the same syntactic category tend to form neighborhoods. We exploit this
technique for extending the value of automatic learning of morphology. In
particular, we look at the suffixes derived from a corpus by unsupervised
learning of morphology, and we ask which of these suffixes have a consistent
syntactic function (e.g., in English, -tion is primarily a mark of nouns, but
-s marks both noun plurals and 3rd person present on verbs), and we determine
that this method works well for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207003</id><created>2002-07-02</created><authors><author><keyname>Senda</keyname><forenames>Yasuko</forenames><affiliation>Central Research Institute of Electric Power Industry, Japan</affiliation></author><author><keyname>Sinohara</keyname><forenames>Yasusi</forenames><affiliation>Central Research Institute of Electric Power Industry, Japan</affiliation></author></authors><title>Analysis of Titles and Readers For Title Generation Centered on the
  Readers</title><categories>cs.CL</categories><comments>7 pages with 3 figures, uses colacl.sty, acl.bst, colaclsub.sty,
  narrowcaption.sty and narrowitem.sty</comments><acm-class>I.2.7</acm-class><journal-ref>COLING'2002(The 19TH International Conference on Computational
  Linguistics)</journal-ref><abstract>  The title of a document has two roles, to give a compact summary and to lead
the reader to read the document. Conventional title generation focuses on
finding key expressions from the author's wording in the document to give a
compact summary and pays little attention to the reader's interest. To make the
title play its second role properly, it is indispensable to clarify the content
(``what to say'') and wording (``how to say'') of titles that are effective to
attract the target reader's interest. In this article, we first identify
typical content and wording of titles aimed at general readers in a comparative
study between titles of technical papers and headlines rewritten for
newspapers. Next, we describe the results of a questionnaire survey on the
effects of the content and wording of titles on the reader's interest. The
survey of general and knowledgeable readers shows both common and different
tendencies in interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207004</id><created>2002-07-02</created><authors><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Optimally cutting a surface into a disk</title><categories>cs.CG cs.DS cs.GR</categories><comments>24 pages, 6 figures; full version of SOCG 2002 paper; see also
  http://www.cs.uiuc.edu/~jeffe/pubs/schema.html</comments><acm-class>F.2.2; I.3.5; G.2.m</acm-class><abstract>  We consider the problem of cutting a set of edges on a polyhedral manifold
surface, possibly with boundary, to obtain a single topological disk,
minimizing either the total number of cut edges or their total length. We show
that this problem is NP-hard, even for manifolds without boundary and for
punctured spheres. We also describe an algorithm with running time n^{O(g+k)},
where n is the combinatorial complexity, g is the genus, and k is the number of
boundary components of the input surface. Finally, we describe a greedy
algorithm that outputs a O(log^2 g)-approximation of the minimum cut graph in
O(g^2 n log n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207005</id><created>2002-07-03</created><authors><author><keyname>Siegel</keyname><forenames>Melanie</forenames></author><author><keyname>Bender</keyname><forenames>Emily M.</forenames></author></authors><title>Efficient Deep Processing of Japanese</title><categories>cs.CL</categories><comments>Proceedings of the 19th International Conference on Computational
  Linguistics</comments><acm-class>I.2.7</acm-class><abstract>  We present a broad coverage Japanese grammar written in the HPSG formalism
with MRS semantics. The grammar is created for use in real world applications,
such that robustness and performance issues play an important role. It is
connected to a POS tagging and word segmentation tool. This grammar is being
developed in a multilingual context, requiring MRS structures that are easily
comparable across languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207006</id><created>2002-07-03</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Orthonormal RBF wavelet and ridgelet-like series and transforms for
  high-dimensional problems</title><categories>cs.SC</categories><acm-class>G1.2, G1.8</acm-class><journal-ref>Int. J. Nonlinear Sci. &amp; Numer. Simulation, 2(2), 155-160, 2001</journal-ref><abstract>  This paper developed a systematic strategy establishing RBF on the wavelet
analysis, which includes continuous and discrete RBF orthonormal wavelet
transforms respectively in terms of singular fundamental solutions and
nonsingular general solutions of differential operators. In particular, the
harmonic Bessel RBF transforms were presented for high-dimensional data
processing. It was also found that the kernel functions of convection-diffusion
operator are feasible to construct some stable ridgelet-like RBF transforms. We
presented time-space RBF transforms based on non-singular solution and
fundamental solution of time-dependent differential operators. The present
methodology was further extended to analysis of some known RBFs such as the MQ,
Gaussian and pre-wavelet kernel RBFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207007</id><created>2002-07-03</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author><author><keyname>Al-Hakeem</keyname><forenames>Nawar</forenames></author></authors><title>Evolutionary Circuit Design: Information Theory Perspective on Signal
  Propagation</title><categories>cs.OH</categories><comments>5 pages, 3 figures, 2 tables, ISSPIT'2001</comments><acm-class>B.1.2</acm-class><journal-ref>ISSPIT'2001</journal-ref><abstract>  This paper presents case-study results on the application of information
theoretic approach to gate-level evolutionary circuit design. We introduce
information measures to provide better estimates of synthesis criteria of
digital circuits. For example, the analysis of signal propagation during
evolving gate-level synthesis can be improved by using information theoretic
measures that will make it possible to find the most effective geometry and
therefore predict the cost of the final design solution. The problem is
considered from the information engine point of view. That is, the process of
evolutionary gate-level circuit design is presented via such measures as
entropy, logical work and information vitality. Some examples of geometry
driven synthesis are provided to prove the above idea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207008</id><created>2002-07-03</created><authors><author><keyname>de Boer</keyname><forenames>F. S.</forenames></author><author><keyname>Hindriks</keyname><forenames>K. V.</forenames></author><author><keyname>van der Hoek</keyname><forenames>W.</forenames></author><author><keyname>Meyer</keyname><forenames>J. -J. Ch.</forenames></author></authors><title>Agent Programming with Declarative Goals</title><categories>cs.AI cs.PL</categories><acm-class>F.3.1;F.3.2;I.2.5;I.2.4</acm-class><abstract>  A long and lasting problem in agent research has been to close the gap
between agent logics and agent programming frameworks. The main reason for this
problem of establishing a link between agent logics and agent programming
frameworks is identified and explained by the fact that agent programming
frameworks have not incorporated the concept of a `declarative goal'. Instead,
such frameworks have focused mainly on plans or `goals-to-do' instead of the
end goals to be realised which are also called `goals-to-be'. In this paper, a
new programming language called GOAL is introduced which incorporates such
declarative goals. The notion of a `commitment strategy' - one of the main
theoretical insights due to agent logics, which explains the relation between
beliefs and goals - is used to construct a computational semantics for GOAL.
Finally, a proof theory for proving properties of GOAL agents is introduced.
Thus, we offer a complete theory of agent programming in the sense that our
theory provides both for a programming framework and a programming logic for
such agents. An example program is proven correct by using this programming
logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207009</id><created>2002-07-03</created><authors><author><keyname>Grolmusz</keyname><forenames>Vince</forenames></author></authors><title>Computing Elementary Symmetric Polynomials with a Sublinear Number of
  Multiplications</title><categories>cs.CC cs.DM cs.DS</categories><comments>10 pages</comments><acm-class>F1.3, F2.1</acm-class><abstract>  Elementary symmetric polynomials $S_n^k$ are used as a benchmark for the
bounded-depth arithmetic circuit model of computation. In this work we prove
that $S_n^k$ modulo composite numbers $m=p_1p_2$ can be computed with much
fewer multiplications than over any field, if the coefficients of monomials
$x_{i_1}x_{i_2}... x_{i_k}$ are allowed to be 1 either mod $p_1$ or mod $p_2$
but not necessarily both. More exactly, we prove that for any constant $k$ such
a representation of $S_n^k$ can be computed modulo $p_1p_2$ using only
$\exp(O(\sqrt{\log n}\log\log n))$ multiplications on the most restricted
depth-3 arithmetic circuits, for $\min({p_1,p_2})&gt;k!$. Moreover, the number of
multiplications remain sublinear while $k=O(\log\log n).$ In contrast, the
well-known Graham-Pollack bound yields an $n-1$ lower bound for the number of
multiplications even for the exact computation (not the representation) of
$S_n^2$. Our results generalize for other non-prime power composite moduli as
well. The proof uses the famous BBR-polynomial of Barrington, Beigel and
Rudich.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207010</id><created>2002-07-03</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Symmetric boundary knot method</title><categories>cs.CE cs.CG</categories><acm-class>G1.8, G1.9</acm-class><journal-ref>Engng. Anal. Bound. Elem., 26(6), 489-494, 2002</journal-ref><abstract>  The boundary knot method (BKM) is a recent boundary-type radial basis
function (RBF) collocation scheme for general PDEs. Like the method of
fundamental solution (MFS), the RBF is employed to approximate the
inhomogeneous terms via the dual reciprocity principle. Unlike the MFS, the
method uses a nonsingular general solution instead of a singular fundamental
solution to evaluate the homogeneous solution so as to circumvent the
controversial artificial boundary outside the physical domain. The BKM is
meshfree, superconvergent, integration free, very easy to learn and program.
The original BKM, however, loses symmetricity in the presense of mixed
boundary. In this study, by analogy with Hermite RBF interpolation, we
developed a symmetric BKM scheme. The accuracy and efficiency of the symmetric
BKM are also numerically validated in some 2D and 3D Helmholtz and diffusion
reaction problems under complicated geometries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207011</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author><author><keyname>Al-Hakeem</keyname><forenames>Nawar</forenames></author></authors><title>Improving Web Database Access Using Decision Diagrams</title><categories>cs.LO cs.DB</categories><comments>7 pages, 7 figures, 3 tables, AICCSA'01</comments><acm-class>E.2</acm-class><abstract>  In some areas of management and commerce, especially in Electronic commerce
(E-commerce), that are accelerated by advances in Web technologies, it is
essential to support the decision making process using formal methods. Among
the problems of E-commerce applications: reducing the time of data access so
that huge databases can be searched quickly; decreasing the cost of database
design ... etc. We present the application of Decision Diagrams design using
Information Theory approach to improve database access speeds. We show that
such utilization provides systematic and visual ways of applying Decision
Making methods to simplify complex Web engineering problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207012</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author></authors><title>Synthesis of Low-Power Digital Circuits Derived from Binary Decision
  Diagrams</title><categories>cs.AR</categories><comments>4 pages, 3 figures, 1 table, ECCTD'01</comments><acm-class>B.6.3</acm-class><journal-ref>ECCTD 2001</journal-ref><abstract>  This paper introduces a novel method for synthesizing digital circuits
derived from Binary Decision Diagrams (BDDs) that can yield to reduction in
power dissipation. The power reduction is achieved by decreasing the switching
activity in a circuit while paying close attention to information measures as
an optimization criterion. We first present the technique of efficient
BDD-based computation of information measures which are used to guide the power
optimization procedures. Using this technique, we have developed an algorithm
of BDD reordering which leads to reducing the power consumption of the circuits
derived from BDDs. Results produced by the synthesis on the ISCAS benchmark
circuits are very encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207013</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author></authors><title>A Compact Graph Model of Handwritten Images: Integration into
  Authentification and Recognition</title><categories>cs.HC cs.DS</categories><comments>9 pages, 6 figures, 1 tables, SSPR'02</comments><acm-class>G.2.2</acm-class><journal-ref>SSPR 2002</journal-ref><abstract>  A novel algorithm for creating a mathematical model of curved shapes is
introduced. The core of the algorithm is based on building a graph
representation of the contoured image, which occupies less storage space than
produced by raster compression techniques. Different advanced applications of
the mathematical model are discussed: recognition of handwritten characters and
verification of handwritten text and signatures for authentification purposes.
Reducing the storage requirements due to the efficient mathematical model
results in faster retrieval and processing times. The experimental outcomes in
compression of contoured images and recognition of handwritten numerals are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207014</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author><author><keyname>Al-Hakeem</keyname><forenames>Nawar</forenames></author></authors><title>On the Information Engine of Circuit Design</title><categories>cs.AR</categories><comments>4 pages, 1 figure, 2 tables, MWSCAS'02</comments><acm-class>B.6.1</acm-class><journal-ref>MWSCAS 2002</journal-ref><abstract>  This paper addresses a new approach to find a spectrum of information
measures for the process of digital circuit synthesis. We consider the problem
from the information engine point of view. The circuit synthesis as a whole and
different steps of the design process (an example of decision diagram is given)
are presented via such measurements as entropy, logical work and information
vitality. We also introduce new information measures to provide better
estimates of synthesis criteria. We show that the basic properties of
information engine, such as the conservation law of information flow and the
equilibrium law of information can be formulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207015</id><created>2002-07-04</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>Tanaka</keyname><forenames>M.</forenames></author></authors><title>New advances in dual reciprocity and boundary-only RBF methods</title><categories>cs.CE cs.CG</categories><report-no>Proc. of BEM technique confer., Vol. 10, 17-22, Tokyo, Japan, 2000</report-no><acm-class>G1.3, G1.8</acm-class><abstract>  This paper made some significant advances in the dual reciprocity and
boundary-only RBF techniques. The proposed boundary knot method (BKM) is
different from the standard boundary element method in a number of important
aspects. Namely, it is truly meshless, exponential convergence,
integration-free (of course, no singular integration), boundary-only for
general problems, and leads to symmetric matrix under certain conditions (able
to be extended to general cases after further modified). The BKM also avoids
the artificial boundary in the method of fundamental solution. An amazing
finding is that the BKM can formulate linear modeling equations for nonlinear
partial differential systems with linear boundary conditions. This merit makes
it circumvent all perplexing issues in the iteration solution of nonlinear
equations. On the other hand, by analogy with Green's second identity, this
paper also presents a general solution RBF (GSR) methodology to construct
efficient RBFs in the dual reciprocity and domain-type RBF collocation methods.
The GSR approach first establishes an explicit relationship between the BEM and
RBF itself on the ground of the weighted residual principle. This paper also
discusses the RBF convergence and stability problems within the framework of
integral equation theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207016</id><created>2002-07-04</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>Tanaka</keyname><forenames>M.</forenames></author></authors><title>Relationship between boundary integral equation and radial basis
  function</title><categories>cs.CE cs.CG</categories><report-no>JASCOME 57th BEM Confer., 30 Sept. 2000</report-no><acm-class>G1.8, G1.9</acm-class><abstract>  This paper aims to survey our recent work relating to the radial basis
function (RBF) from some new views of points. In the first part, we established
the RBF on numerical integration analysis based on an intrinsic relationship
between the Green's boundary integral representation and RBF. It is found that
the kernel function of integral equation is important to create efficient RBF.
The fundamental solution RBF (FS-RBF) was presented as a novel strategy
constructing operator-dependent RBF. We proposed a conjecture formula featuring
the dimension affect on error bound to show the independent-dimension merit of
the RBF techniques. We also discussed wavelet RBF, localized RBF schemes, and
the influence of node placement on the RBF solution accuracy. The
centrosymmetric matrix structure of the RBF interpolation matrix under
symmetric node placing is proved.
  The second part of this paper is concerned with the boundary knot method
(BKM), a new boundary-only, meshless, spectral convergent, integration-free RBF
collocation technique. The BKM was tested to the Helmholtz, Laplace, linear and
nonlinear convection-diffusion problems. In particular, we introduced the
response knot-dependent nonsingular general solution to calculate
varying-parameter and nonlinear steady convection-diffusion problems very
efficiently. By comparing with the multiple dual reciprocity method, we
discussed the completeness issue of the BKM.
  Finally, the nonsingular solutions for some known differential operators were
given in appendix. Also we expanded the RBF concepts by introducing time-space
RBF for transient problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207017</id><created>2002-07-04</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>Tanaka</keyname><forenames>M.</forenames></author></authors><title>New Insights in Boundary-only and Domain-type RBF Methods</title><categories>cs.CE cs.CG</categories><acm-class>G1.3, G1.8</acm-class><journal-ref>J. Nonlinear Sci. &amp; Numer. Simulation, 1(3), 145-151, 2000</journal-ref><abstract>  This paper has made some significant advances in the boundary-only and
domain-type RBF techniques. The proposed boundary knot method (BKM) is
different from the standard boundary element method in a number of important
aspects. Namely, it is truly meshless, exponential convergence,
integration-free (of course, no singular integration), boundary-only for
general problems, and leads to symmetric matrix under certain conditions (able
to be extended to general cases after further modified). The BKM also avoids
the artificial boundary in the method of fundamental solution. An amazing
finding is that the BKM can formulate linear modeling equations for nonlinear
partial differential systems with linear boundary conditions. This merit makes
it circumvent all perplexing issues in the iteration solution of nonlinear
equations. On the other hand, by analogy with Green's second identity, we also
presents a general solution RBF (GSR) methodology to construct efficient RBFs
in the domain-type RBF collocation method and dual reciprocity method. The GSR
approach first establishes an explicit relationship between the BEM and RBF
itself on the ground of the potential theory. This paper also discusses some
essential issues relating to the RBF computing, which include time-space RBFs,
direct and indirect RBF schemes, finite RBF method, and the application of
multipole and wavelet to the RBF solution of the PDEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207018</id><created>2002-07-04</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Definitions of distance function in radial basis function approach</title><categories>cs.CE cs.CG</categories><acm-class>G1.3, G1.8</acm-class><abstract>  Very few studies involve how to construct the efficient RBFs by means of
problem features. Recently the present author presented general solution RBF
(GS-RBF) methodology to create operator-dependent RBFs successfully [1]. On the
other hand, the normal radial basis function (RBF) is defined via Euclidean
space distance function or the geodesic distance [2]. This purpose of this note
is to redefine distance function in conjunction with problem features, which
include problem-dependent and time-space distance function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207019</identifier>
 <datestamp>2007-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207019</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author></authors><title>Information Measures in Detecting and Recognizing Symmetries</title><categories>cs.OH</categories><comments>4 pages, 2 figures, 2 tables, MWSCAS'01</comments><acm-class>B.6.3</acm-class><journal-ref>MWSCAS 2002</journal-ref><abstract>  This paper presents a method to detect and recognize symmetries in Boolean
functions. The idea is to use information theoretic measures of Boolean
functions to detect sub-space of possible symmetric variables. Coupled with the
new techniques of efficient estimations of information measures on Binary
Decision Diagrams (BDDs) we obtain promised results in symmetries detection for
large-scale functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207020</identifier>
 <datestamp>2007-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207020</id><created>2002-07-04</created><authors><author><keyname>Popel</keyname><forenames>Denis V.</forenames></author></authors><title>Towards Efficient Calculation of Information Measures for Reordering of
  Binary Decision Diagrams</title><categories>cs.OH</categories><comments>4 pages, 3 figures, 2 tables, SCS'01</comments><acm-class>B.6.3</acm-class><journal-ref>SCS 2001</journal-ref><abstract>  This paper introduces new technique for efficient calculation of different
Shannon information measures which operates Binary Decision Diagrams (BDDs). We
offer an algorithm of BDD reordering which demonstrates the improvement of the
obtaining outcomes over the existing reordering approaches. The technique and
the reordering algorithm have been implemented, and the results on circuits'
benchmarks are analyzed. We point out that the results are quite promising, the
algorithm is very fast, and it is easy to implement. Finally, we show that our
approach to BDD reordering can yield to reduction in the power dissipation for
the circuits derived from BDDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207021</id><created>2002-07-07</created><authors><author><keyname>Bonatti</keyname><forenames>Piero A.</forenames></author></authors><title>Abduction, ASP and Open Logic Programs</title><categories>cs.AI</categories><comments>7 pages, NMR'02 Workshop</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  Open logic programs and open entailment have been recently proposed as an
abstract framework for the verification of incomplete specifications based upon
normal logic programs and the stable model semantics. There are obvious
analogies between open predicates and abducible predicates. However, despite
superficial similarities, there are features of open programs that have no
immediate counterpart in the framework of abduction and viceversa. Similarly,
open programs cannot be immediately simulated with answer set programming
(ASP). In this paper we start a thorough investigation of the relationships
between open inference, abduction and ASP. We shall prove that open programs
generalize the other two frameworks. The generalized framework suggests
interesting extensions of abduction under the generalized stable model
semantics. In some cases, we will be able to reduce open inference to abduction
and ASP, thereby estimating its computational complexity. At the same time, the
aforementioned reduction opens the way to new applications of abduction and
ASP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207022</id><created>2002-07-07</created><authors><author><keyname>Dastani</keyname><forenames>Mehdi</forenames></author><author><keyname>van der Torre</keyname><forenames>Leendert</forenames></author></authors><title>What is a Joint Goal? Games with Beliefs and Defeasible Desires</title><categories>cs.MA cs.GT</categories><comments>8 pages</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of NMR02, Toulouse, 2002</journal-ref><abstract>  In this paper we introduce a qualitative decision and game theory based on
belief (B) and desire (D) rules. We show that a group of agents acts as if it
is maximizing achieved joint goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207023</id><created>2002-07-07</created><updated>2005-08-29</updated><authors><author><keyname>Son</keyname><forenames>Tran Cao</forenames></author><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Tran</keyname><forenames>Nam</forenames></author><author><keyname>McIlraith</keyname><forenames>Sheila</forenames></author></authors><title>Domain-Dependent Knowledge in Answer Set Planning</title><categories>cs.AI</categories><comments>70 pages, accepted for publication, TOCL Version with all proofs</comments><acm-class>I.2.4; I.2.3; I.2.8</acm-class><abstract>  In this paper we consider three different kinds of domain-dependent control
knowledge (temporal, procedural and HTN-based) that are useful in planning. Our
approach is declarative and relies on the language of logic programming with
answer set semantics (AnsProlog*). AnsProlog* is designed to plan without
control knowledge. We show how temporal, procedural and HTN-based control
knowledge can be incorporated into AnsProlog* by the modular addition of a
small number of domain-dependent rules, without the need to modify the planner.
We formally prove the correctness of our planner, both in the absence and
presence of the control knowledge. Finally, we perform some initial
experimentation that demonstrates the potential reduction in planning time that
can be achieved when procedural domain knowledge is used to solve planning
problems with large plan length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207024</id><created>2002-07-08</created><authors><author><keyname>Dunne</keyname><forenames>Paul E.</forenames></author></authors><title>On Concise Encodings of Preferred Extensions</title><categories>cs.AI cs.CC cs.DS</categories><comments>Proc. 9th Worskhop on Non-monotonic reasoning, Toulouse, 2002
  (NMR'02) (7 pages)</comments><report-no>Dept. of Comp. Sci., Univ. of Liverpool, Tech. Report ULCS-02-003</report-no><acm-class>F.2.m; I.2.3; I.2.m</acm-class><abstract>  Much work on argument systems has focussed on preferred extensions which
define the maximal collectively defensible subsets. Identification and
enumeration of these subsets is (under the usual assumptions) computationally
demanding. We consider approaches to deciding if a subset S is a preferred
extension which query a representations encoding all such extensions, so that
the computational effort is invested once only (for the initial enumeration)
rather than for each separate query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207025</id><created>2002-07-08</created><authors><author><keyname>Cayrol</keyname><forenames>C.</forenames></author><author><keyname>Doutre</keyname><forenames>S.</forenames></author><author><keyname>Lagasquie-Schiex</keyname><forenames>M. -C.</forenames></author><author><keyname>Mengin</keyname><forenames>J.</forenames></author></authors><title>&quot;Minimal defence&quot;: a refinement of the preferred semantics for
  argumentation frameworks</title><categories>cs.AI</categories><comments>8 pages, 3 figures</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of the 9th International Workshop on Non-Monotonic
  Reasoning, 2002, pp. 408-415</journal-ref><abstract>  Dung's abstract framework for argumentation enables a study of the
interactions between arguments based solely on an ``attack'' binary relation on
the set of arguments. Various ways to solve conflicts between contradictory
pieces of information have been proposed in the context of argumentation,
nonmonotonic reasoning or logic programming, and can be captured by appropriate
semantics within Dung's framework. A common feature of these semantics is that
one can always maximize in some sense the set of acceptable arguments. We
propose in this paper to extend Dung's framework in order to allow for the
representation of what we call ``restricted'' arguments: these arguments should
only be used if absolutely necessary, that is, in order to support other
arguments that would otherwise be defeated. We modify Dung's preferred
semantics accordingly: a set of arguments becomes acceptable only if it
contains a minimum of restricted arguments, for a maximum of unrestricted
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207026</id><created>2002-07-08</created><updated>2002-11-04</updated><authors><author><keyname>Goldwasser</keyname><forenames>Michael H.</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author></authors><title>Linear-Time Algorithms for Computing Maximum-Density Sequence Segments
  with Bioinformatics Applications</title><categories>cs.DS cs.DM</categories><comments>23 pages, 13 figures. A significant portion of these results appeared
  under the title, &quot;Fast Algorithms for Finding Maximum-Density Segments of a
  Sequence with Applications to Bioinformatics,&quot; in Proceedings of the Second
  Workshop on Algorithms in Bioinformatics (WABI), volume 2452 of Lecture Notes
  in Computer Science (Springer-Verlag, Berlin), R. Guigo and D. Gusfield
  editors, 2002, pp. 157--171</comments><acm-class>F.2.2; J.3; G.2</acm-class><journal-ref>Journal of Computer and System Sciences, 70(2):128-144, 2005</journal-ref><doi>10.1016/j.jcss.2004.08.001</doi><abstract>  We study an abstract optimization problem arising from biomolecular sequence
analysis. For a sequence A of pairs (a_i,w_i) for i = 1,..,n and w_i&gt;0, a
segment A(i,j) is a consecutive subsequence of A starting with index i and
ending with index j. The width of A(i,j) is w(i,j) = sum_{i &lt;= k &lt;= j} w_k, and
the density is (sum_{i&lt;= k &lt;= j} a_k)/ w(i,j). The maximum-density segment
problem takes A and two values L and U as input and asks for a segment of A
with the largest possible density among those of width at least L and at most
U. When U is unbounded, we provide a relatively simple, O(n)-time algorithm,
improving upon the O(n \log L)-time algorithm by Lin, Jiang and Chao. When both
L and U are specified, there are no previous nontrivial results. We solve the
problem in O(n) time if w_i=1 for all i, and more generally in
O(n+n\log(U-L+1)) time when w_i&gt;=1 for all i.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207027</identifier>
 <datestamp>2010-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207027</id><created>2002-07-08</created><updated>2010-10-31</updated><authors><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>Permutation graphs, fast forward permutations, and sampling the cycle
  structure of a permutation</title><categories>cs.CR cs.CC math.CO math.PR</categories><comments>Corrected a small error</comments><acm-class>F.2.2; G.2.2; G.3</acm-class><journal-ref>Journal of Algorithms 47 (2003), 104--121</journal-ref><doi>10.1016/S0196-6774(03)00017-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A permutation P on {1,..,N} is a_fast_forward_permutation_ if for each m the
computational complexity of evaluating P^m(x)$ is small independently of m and
x. Naor and Reingold constructed fast forward pseudorandom cycluses and
involutions. By studying the evolution of permutation graphs, we prove that the
number of queries needed to distinguish a random cyclus from a random
permutation on {1,..,N} is Theta(N) if one does not use queries of the form
P^m(x), but is only Theta(1) if one is allowed to make such queries.
  We construct fast forward permutations which are indistinguishable from
random permutations even when queries of the form P^m(x) are allowed. This is
done by introducing an efficient method to sample the cycle structure of a
random permutation, which in turn solves an open problem of Naor and Reingold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207028</id><created>2002-07-08</created><authors><author><keyname>Jain</keyname><forenames>Kamal</forenames></author><author><keyname>Mahdian</keyname><forenames>Mohammad</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Saberi</keyname><forenames>Amin</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>Greedy Facility Location Algorithms Analyzed using Dual Fitting with
  Factor-Revealing LP</title><categories>cs.DS cs.GT</categories><comments>28 pages, 2 figures, 4 tables, abstract appeared in STOC 2002
  Montreal</comments><acm-class>F.2.2;G.2.1;G.2.2</acm-class><abstract>  In this paper, we will formalize the method of dual fitting and the idea of
factor-revealing LP. This combination is used to design and analyze two greedy
algorithms for the metric uncapacitated facility location problem. Their
approximation factors are 1.861 and 1.61, with running times of O(mlog m) and
O(n^3), respectively, where n is the total number of vertices and m is the
number of edges in the underlying complete bipartite graph between cities and
facilities. The algorithms are used to improve recent results for several
variants of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207029</id><created>2002-07-09</created><updated>2002-07-14</updated><authors><author><keyname>Bochman</keyname><forenames>Alexander</forenames></author></authors><title>Two Representations for Iterative Non-prioritized Change</title><categories>cs.AI</categories><comments>7 pages,Proceedings NMR'02, references added</comments><acm-class>I.2.3</acm-class><abstract>  We address a general representation problem for belief change, and describe
two interrelated representations for iterative non-prioritized change: a
logical representation in terms of persistent epistemic states, and a
constructive representation in terms of flocks of bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207030</id><created>2002-07-09</created><updated>2002-07-14</updated><authors><author><keyname>Bochman</keyname><forenames>Alexander</forenames></author></authors><title>Collective Argumentation</title><categories>cs.AI</categories><comments>8 pages, Proceedings NMR'02, references added</comments><acm-class>I.2.3</acm-class><abstract>  An extension of an abstract argumentation framework, called collective
argumentation, is introduced in which the attack relation is defined directly
among sets of arguments. The extension turns out to be suitable, in particular,
for representing semantics of disjunctive logic programs. Two special kinds of
collective argumentation are considered in which the opponents can share their
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207031</id><created>2002-07-09</created><authors><author><keyname>Prakken</keyname><forenames>Henry</forenames></author></authors><title>Intuitions and the modelling of defeasible reasoning: some case studies</title><categories>cs.AI cs.LO</categories><comments>Proceedings of the 9th International Workshop on Non-Monotonic
  Reasoning (NMR'2002), Toulouse, France, April 19-21, 2002</comments><acm-class>I.2.3</acm-class><abstract>  The purpose of this paper is to address some criticisms recently raised by
John Horty in two articles against the validity of two commonly accepted
defeasible reasoning patterns, viz. reinstatement and floating conclusions. I
shall argue that Horty's counterexamples, although they significantly raise our
understanding of these reasoning patterns, do not show their invalidity. Some
of them reflect patterns which, if made explicit in the formalisation, avoid
the unwanted inference without having to give up the criticised inference
principles. Other examples seem to involve hidden assumptions about the
specific problem which, if made explicit, are nothing but extra information
that defeat the defeasible inference. These considerations will be put in a
wider perspective by reflecting on the nature of defeasible reasoning
principles as principles of justified acceptance rather than `real' logical
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207032</id><created>2002-07-09</created><authors><author><keyname>Cabalar</keyname><forenames>Pedro</forenames></author></authors><title>Alternative Characterizations for Strong Equivalence of Logic Programs</title><categories>cs.AI cs.LO</categories><comments>15 pages, Workshop on Non-monotonic Reasoning (NMR'02), Toulouse, 2002</comments><acm-class>I.2.3</acm-class><abstract>  In this work we present additional results related to the property of strong
equivalence of logic programs. This property asserts that two programs share
the same set of stable models, even under the addition of new rules. As shown
in a recent work by Lifschitz, Pearce and Valverde, strong equivalence can be
simply reduced to equivalence in the logic of Here-and-There (HT). In this
paper we provide two alternatives respectively based on classical logic and
3-valued logic. The former is applicable to general rules, but not for nested
expressions, whereas the latter is applicable for nested expressions but, when
moving to an unrestricted syntax, it generally yields different results from
HT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207033</id><created>2002-07-09</created><authors><author><keyname>Chen</keyname><forenames>W</forenames></author><author><keyname>Wang</keyname><forenames>Xinwei</forenames></author><author><keyname>Yu</keyname><forenames>Yongxi</forenames></author></authors><title>Reducing the Computational Requirements of the Differential Quadrature
  Method</title><categories>cs.CE cs.CG</categories><acm-class>G1.3, G1.8</acm-class><journal-ref>Numerical Methods for Partial Differential Equations, 12, 565-577,
  1996</journal-ref><abstract>  This paper shows that the weighting coefficient matrices of the differential
quadrature method (DQM) are centrosymmetric or skew-centrosymmetric if the grid
spacings are symmetric irrespective of whether they are equal or unequal. A new
skew centrosymmetric matrix is also discussed. The application of the
properties of centrosymmetric and skew centrosymmetric matrix can reduce the
computational effort of the DQM for calculations of the inverse, determinant,
eigenvectors and eigenvalues by 75%. This computational advantage are also
demonstrated via several numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207034</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207034</id><created>2002-07-09</created><authors><author><keyname>Chen</keyname><forenames>W</forenames></author><author><keyname>He</keyname><forenames>Weixing</forenames></author><author><keyname>Zhong</keyname><forenames>Tingxiu</forenames></author></authors><title>A Note on the DQ Analysis of Anisotropic Plates</title><categories>cs.SC</categories><acm-class>G1.3, G1.8</acm-class><journal-ref>J. of Sound &amp; Vibration, 204(1), 180-182, 1997</journal-ref><doi>10.1006/jsvi.1996.0895</doi><abstract>  Recently, Bert, Wang and Striz [1, 2] applied the differential quadrature
(DQ) and harmonic differential quadrature (HDQ) methods to analyze static and
dynamic behaviors of anisotropic plates. Their studies showed that the methods
were conceptually simple and computationally efficient in comparison to other
numerical techniques. Based on some recent work by the present author [3, 4],
the purpose of this note is to further simplify the formulation effort and
improve computing efficiency in applying the DQ and HDQ methods for these
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207035</id><created>2002-07-09</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>Zhong</keyname><forenames>Tingxiu</forenames></author></authors><title>A Lyapunov Formulation for Efficient Solution of the Poisson and
  Convection-Diffusion Equations by the Differential Quadrature Method</title><categories>cs.CE cs.CG</categories><acm-class>G1.8, G1.2</acm-class><journal-ref>J. of Computational Physics, 139, 1-7, 1998</journal-ref><abstract>  Civan and Sliepcevich [1, 2] suggested that special matrix solver should be
developed to further reduce the computing effort in applying the differential
quadrature (DQ) method for the Poisson and convection-diffusion equations.
Therefore, the purpose of the present communication is to introduce and apply
the Lyapunov formulation which can be solved much more efficiently than the
Gaussian elimination method. Civan and Sliepcevich [2] first presented DQ
approximate formulas in polynomial form for partial derivatives in
tow-dimensional variable domain. For simplifying formulation effort, Chen et
al. [3] proposed the compact matrix form of these DQ approximate formulas. In
this study, by using these matrix approximate formulas, the DQ formulations for
the Poisson and convection-diffusion equations can be expressed as the Lyapunov
algebraic matrix equation. The formulation effort is simplified, and a simple
and explicit matrix formulation is obtained. A variety of fast algorithms in
the solution of the Lyapunov equation [4-6] can be successfully applied in the
DQ analysis of these two-dimensional problems, and, thus, the computing effort
can be greatly reduced. Finally, we also point out that the present reduction
technique can be easily extended to the three-dimensional cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207036</id><created>2002-07-09</created><authors><author><keyname>Allen</keyname><forenames>N.</forenames></author><author><keyname>Terriberry</keyname><forenames>T.</forenames></author></authors><title>System Description for a Scalable, Fault-Tolerant, Distributed Garbage
  Collector</title><categories>cs.DC</categories><comments>47 pages, LaTeX</comments><acm-class>C.2.4; D.4.5</acm-class><abstract>  We describe an efficient and fault-tolerant algorithm for distributed cyclic
garbage collection. The algorithm imposes few requirements on the local
machines and allows for flexibility in the choice of local collector and
distributed acyclic garbage collector to use with it. We have emphasized
reducing the number and size of network messages without sacrificing the
promptness of collection throughout the algorithm. Our proposed collector is a
variant of back tracing to avoid extensive synchronization between machines. We
have added an explicit forward tracing stage to the standard back tracing stage
and designed a tuned heuristic to reduce the total amount of work done by the
collector. Of particular note is the development of fault-tolerant cooperation
between traces and a heuristic that aggressively reduces the set of suspect
objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207037</id><created>2002-07-09</created><authors><author><keyname>Chopra</keyname><forenames>Samir</forenames></author><author><keyname>Heidema</keyname><forenames>Johannes</forenames></author><author><keyname>Meyer</keyname><forenames>Thomas</forenames></author></authors><title>Some logics of belief and disbelief</title><categories>cs.AI cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  The introduction of explicit notions of rejection, or disbelief, into logics
for knowledge representation can be justified in a number of ways. Motivations
range from the need for versions of negation weaker than classical negation, to
the explicit recording of classic belief contraction operations in the area of
belief change, and the additional levels of expressivity obtained from an
extended version of belief change which includes disbelief contraction. In this
paper we present four logics of disbelief which address some or all of these
intuitions. Soundness and completeness results are supplied and the logics are
compared with respect to applicability and utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207038</id><created>2002-07-09</created><authors><author><keyname>Chopra</keyname><forenames>Samir</forenames></author><author><keyname>Ghose</keyname><forenames>Aditya</forenames></author><author><keyname>Meyer</keyname><forenames>Thomas</forenames></author></authors><title>Iterated revision and the axiom of recovery: a unified treatment via
  epistemic states</title><categories>cs.AI cs.LO</categories><acm-class>I.2.3</acm-class><abstract>  The axiom of recovery, while capturing a central intuition regarding belief
change, has been the source of much controversy. We argue briefly against
putative counterexamples to the axiom--while agreeing that some of their
insight deserves to be preserved--and present additional recovery-like axioms
in a framework that uses epistemic states, which encode preferences, as the
object of revisions. This provides a framework in which iterated revision
becomes possible and makes explicit the connection between iterated belief
change and the axiom of recovery. We provide a representation theorem that
connects the semantic conditions that we impose on iterated revision and the
additional syntactical properties mentioned. We also show some interesting
similarities between our framework and that of Darwiche-Pearl. In particular,
we show that the intuitions underlying the controversial (C2) postulate are
captured by the recovery axiom and our recovery-like postulates (the latter can
be seen as weakenings of (C2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207039</id><created>2002-07-10</created><authors><author><keyname>Tanaka</keyname><forenames>Masataka</forenames></author><author><keyname>Chen</keyname><forenames>W</forenames></author></authors><title>Dual reciprocity BEM and dynamic programming filter for inverse
  elastodynamic problems</title><categories>cs.CE cs.CG</categories><acm-class>G1.8, G1.2</acm-class><journal-ref>Transactions of the Japan Society for Computational Engineering
  and Science, 2, 20000003, 2000</journal-ref><abstract>  This paper presents the first coupling application of the dual reciprocity
BEM (DRBEM) and dynamic programming filter to inverse elastodynamic problem.
The DRBEM is the only BEM method, which does not require domain discretization
for general linear and nonlinear dynamic problems. Since the size of numerical
discretization system has a great effect on the computing effort of recursive
or iterative calculations of inverse analysis, the intrinsic boundary-only
merit of the DRBEM causes a considerable computational saving. On the other
hand, the strengths of the dynamic programming filter lie in its mathematical
simplicity, easy to program and great flexibility in the type, number and
locations of measurements and unknown inputs. The combination of these two
techniques is therefore very attractive for the solution of practical inverse
problems. In this study, the spatial and temporal partial derivatives of the
governing equation are respectively discretized first by the DRBEM and the
precise integration method, and then, by using dynamic programming with
regularization, dynamic load is estimated based on noisy measurements of
velocity and displacement at very few locations. Numerical experiments involved
with the periodic and Heaviside impact load are conducted to demonstrate the
applicability, efficiency and simplicity of this strategy. The affect of noise
level, regularization parameter, and measurement types on the estimation is
also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207040</id><created>2002-07-10</created><authors><author><keyname>Schweimeier</keyname><forenames>Ralf</forenames></author><author><keyname>Schroeder</keyname><forenames>Michael</forenames></author></authors><title>Well-Founded Argumentation Semantics for Extended Logic Programming</title><categories>cs.LO cs.AI</categories><comments>Workshop on Non-Monotonic Reasoning NMR'02, Special Session on
  Argument, Dialogue, and Decision</comments><acm-class>D.1.6.; F.3.2.; I.2.3.; I.2.4</acm-class><abstract>  This paper defines an argumentation semantics for extended logic programming
and shows its equivalence to the well-founded semantics with explicit negation.
We set up a general framework in which we extensively compare this semantics to
other argumentation semantics, including those of Dung, and Prakken and Sartor.
We present a general dialectical proof theory for these argumentation
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207041</id><created>2002-07-10</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>RBF-based meshless boundary knot method and boundary particle method</title><categories>cs.CE cs.CG</categories><report-no>Proc. of the China Congress on Computational Mechanics's 2001, pp.
  319-326, Guangzhou, China, Dec. 2001</report-no><acm-class>G1.3, G1.8</acm-class><abstract>  This paper is concerned with the two new boundary-type radial basis function
collocation schemes, boundary knot method (BKM) and boundary particle method
(BPM). The BKM is developed based on the dual reciprocity theorem, while the
BPM employs the multiple reciprocity technique. Unlike the method of
fundamental solution, the wto methods use the nonsingular general solutions
instead of singular fundamental solution to circumvent the controversial
artificial boundary outside physical domain. Compared with the boundary element
method, both the BKM and BPM are meshfree, superconvergent, meshfree,
integration free, symmetric, and mathematically simple collocation techniques
for general PDEs. In particular, the BPM does not require any inner nodes for
inhomogeneous problems. In this study, the accuracy and efficiency of the two
methods are numerically demonstrated to some 2D, 3D Helmholtz and
convection-diffusion problems under complicated geometries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207042</id><created>2002-07-11</created><authors><author><keyname>Brewka</keyname><forenames>Gerhard</forenames></author></authors><title>Logic Programming with Ordered Disjunction</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><abstract>  Logic programs with ordered disjunction (LPODs) combine ideas underlying
Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming.
Logic programming under answer set semantics is extended with a new connective
called ordered disjunction. The new connective allows us to represent
alternative, ranked options for problem solutions in the heads of rules: A
\times B intuitively means: if possible A, but if A is not possible then at
least B. The semantics of logic programs with ordered disjunction is based on a
preference relation on answer sets. LPODs are useful for applications in design
and configuration and can serve as a basis for qualitative decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207043</id><created>2002-07-11</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>Tanaka</keyname><forenames>M.</forenames></author></authors><title>A meshless, integration-free, and boundary-only RBF technique</title><categories>cs.CE cs.CG</categories><acm-class>G1.3, G1.8</acm-class><journal-ref>Computers and Mathematics with Applications, 43, 379-391, 2002</journal-ref><abstract>  Based on the radial basis function (RBF), non-singular general solution and
dual reciprocity method (DRM), this paper presents an inherently meshless,
integration-free, boundary-only RBF collocation techniques for numerical
solution of various partial differential equation systems. The basic ideas
behind this methodology are very mathematically simple. In this study, the RBFs
are employed to approximate the inhomogeneous terms via the DRM, while
non-singular general solution leads to a boundary-only RBF formulation for
homogenous solution. The present scheme is named as the boundary knot method
(BKM) to differentiate it from the other numerical techniques. In particular,
due to the use of nonsingular general solutions rather than singular
fundamental solutions, the BKM is different from the method of fundamental
solution in that the former does no require the artificial boundary and results
in the symmetric system equations under certain conditions. The efficiency and
utility of this new technique are validated through a number of typical
numerical examples. Completeness concern of the BKM due to the only use of
non-singular part of complete fundamental solution is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207044</id><created>2002-07-11</created><updated>2002-07-17</updated><authors><author><keyname>Neumerkel</keyname><forenames>Ulrich</forenames></author><author><keyname>Kral</keyname><forenames>Stefan</forenames></author></authors><title>Declarative program development in Prolog with GUPU</title><categories>cs.SE</categories><comments>10 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/html/cs/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  We present GUPU, a side-effect free environment specialized for programming
courses. It seamlessly guides and supports students during all phases of
program development, covering specification, implementation, and program
debugging. GUPU features several innovations in this area. The specification
phase is supported by reference implementations augmented with diagnostic
facilities. During implementation, immediate feedback from test cases and from
visualization tools helps the programmer's program understanding. A set of
slicing techniques narrows down programming errors. The whole process is guided
by a marking system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207045</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207045</id><created>2002-07-11</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author><author><keyname>Marquis</keyname><forenames>Pierre</forenames></author></authors><title>Compilation of Propositional Weighted Bases</title><categories>cs.AI</categories><comments>Proceedings of the Ninth International Workshop on Non-Monotonic
  Reasoning (NMR'02), Toulouse, 2002 (6-14)</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  In this paper, we investigate the extent to which knowledge compilation can
be used to improve inference from propositional weighted bases. We present a
general notion of compilation of a weighted base that is parametrized by any
equivalence--preserving compilation function. Both negative and positive
results are presented. On the one hand, complexity results are identified,
showing that the inference problem from a compiled weighted base is as
difficult as in the general case, when the prime implicates, Horn cover or
renamable Horn cover classes are targeted. On the other hand, we show that the
inference problem becomes tractable whenever DNNF-compilations are used and
clausal queries are considered. Moreover, we show that the set of all preferred
models of a DNNF-compilation of a weighted base can be computed in time
polynomial in the output size. Finally, we sketch how our results can be used
in model-based diagnosis in order to compute the most probable diagnoses of a
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207046</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207046</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Ouis</keyname><forenames>Samir</forenames></author><author><keyname>Jussien</keyname><forenames>Narendra</forenames></author><author><keyname>Boizumault</keyname><forenames>Patrice</forenames></author></authors><title>COINS: a constraint-based interactive solving system</title><categories>cs.SE</categories><comments>15 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  This paper describes the COINS (COnstraint-based INteractive Solving) system:
a conflict-based constraint solver. It helps understanding inconsistencies,
simulates constraint additions and/or retractions (without any propagation),
determines if a given constraint belongs to a conflict and provides diagnosis
tools (e.g. why variable v cannot take value val). COINS also uses
user-friendly representation of conflicts and explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207047</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207047</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Agren</keyname><forenames>Magnus</forenames></author><author><keyname>Szeredi</keyname><forenames>Tamas</forenames></author><author><keyname>Beldiceanu</keyname><forenames>Nicolas</forenames></author><author><keyname>Carlsson</keyname><forenames>Mats</forenames></author></authors><title>Tracing and Explaining Execution of CLP(FD) Programs</title><categories>cs.SE</categories><comments>16 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  Previous work in the area of tracing CLP(FD) programs mainly focuses on
providing information about control of execution and domain modification. In
this paper, we present a trace structure that provides information about
additional important aspects. We incorporate explanations in the trace
structure, i.e. reasons for why certain solver actions occur. Furthermore, we
come up with a format for describing the execution of the filtering algorithms
of global constraints. Some new ideas about the design of the trace are also
presented. For example, we have modeled our trace as a nested block structure
in order to achieve a hierarchical view. Also, new ways about how to represent
and identify different entities such as constraints and domain variables are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207048</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207048</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Fages</keyname><forenames>Francois</forenames></author></authors><title>CLPGUI: a generic graphical user interface for constraint logic
  programming over finite domains</title><categories>cs.SE</categories><comments>16 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  CLPGUI is a graphical user interface for visualizing and interacting with
constraint logic programs over finite domains. In CLPGUI, the user can control
the execution of a CLP program through several views of constraints, of finite
domain variables and of the search tree. CLPGUI is intended to be used both for
teaching purposes, and for debugging and improving complex programs of
realworld scale. It is based on a client-server architecture for connecting the
CLP process to a Java-based GUI process. Communication by message passing
provides an open architecture which facilitates the reuse of graphical
components and the porting to different constraint programming systems.
Arbitrary constraints and goals can be posted incrementally from the GUI. We
propose several dynamic 2D and 3D visualizations of the search tree and of the
evolution of finite domain variables. We argue that the 3D representation of
search trees proposed in this paper provides the most appropriate visualization
of large search trees. We describe the current implementation of the
annotations and of the interactive execution model in GNU-Prolog, and report
some evaluation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207049</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207049</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Vaucheret</keyname><forenames>Claudio</forenames></author><author><keyname>Bueno</keyname><forenames>Francisco</forenames></author></authors><title>More Precise Yet Efficient Type Inference for Logic Programs</title><categories>cs.SE</categories><comments>14 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  Type analyses of logic programs which aim at inferring the types of the
program being analyzed are presented in a unified abstract interpretation-based
framework. This covers most classical abstract interpretation-based type
analyzers for logic programs, built on either top-down or bottom-up
interpretation of the program. In this setting, we discuss the widening
operator, arguably a crucial one. We present a new widening which is more
precise than those previously proposed. Practical results with our analysis
domain are also presented, showing that it also allows for efficient analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207050</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207050</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Lesaint</keyname><forenames>Willy</forenames></author></authors><title>Value withdrawal explanations: a theoretical tool for programming
  environments</title><categories>cs.SE</categories><comments>14 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  Constraint logic programming combines declarativity and efficiency thanks to
constraint solvers implemented for specific domains. Value withdrawal
explanations have been efficiently used in several constraints programming
environments but there does not exist any formalization of them. This paper is
an attempt to fill this lack. Furthermore, we hope that this theoretical tool
could help to validate some programming environments. A value withdrawal
explanation is a tree describing the withdrawal of a value during a domain
reduction by local consistency notions and labeling. Domain reduction is
formalized by a search tree using two kinds of operators: operators for local
consistency notions and operators for labeling. These operators are defined by
sets of rules. Proof trees are built with respect to these rules. For each
removed value, there exists such a proof tree which is the withdrawal
explanation of this value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207051</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207051</id><created>2002-07-11</created><updated>2002-07-11</updated><authors><author><keyname>Angelopoulos</keyname><forenames>Nicos</forenames></author></authors><title>Exporting Prolog source code</title><categories>cs.SE</categories><comments>8 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  In this paper we present a simple source code configuration tool. ExLibris
operates on libraries and can be used to extract from local libraries all code
relevant to a particular project. Our approach is not designed to address
problems arising in code production lines, but rather, to support the needs of
individual or small teams of researchers who wish to communicate their Prolog
programs. In the process, we also wish to accommodate and encourage the writing
of reusable code. Moreover, we support and propose ways of dealing with issues
arising in the development of code that can be run on a variety of like-minded
Prolog systems. With consideration to these aims we have made the following
decisions: (i) support file-based source development, (ii) require minimal
program transformation, (iii) target simplicity of usage, and (iv) introduce
minimum number of new primitives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207052</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207052</id><created>2002-07-11</created><authors><author><keyname>Tessier</keyname><forenames>Alexandre</forenames></author></authors><title>Proceedings of the 12th International Workshop on Logic Programming
  Environments</title><categories>cs.SE</categories><comments>10 refereed papers, 123 pages; Alexandre Tessier, editor; WLPE 2002</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  The twelfth Workshop on Logic Programming Environments, WLPE 2002, is one in
a series of international workshops held in the topic area. The workshops
facilitate the exchange ideas and results among researchers and system
developers on all aspects of environments for logic programming. Relevant
topics for these workshops include user interfaces, human engineering,
execution visualization, development tools, providing for new paradigms, and
interfacing to language system tools and external systems. This twelfth
workshop held in Copenhaguen. It follows the successful eleventh Workshop on
Logic Programming Environments held in Cyprus in December, 2001.
  WLPE 2002 features ten presentations. The presentations involve, in some way,
constraint logic programming, object-oriented programming and abstract
interpretation. Topics areas addressed include tools for software development,
execution visualization, software maintenance, instructional aids.
  This workshop was a post-conference workshop at ICLP 2002.
  Alexandre Tessier, Program Chair, WLPE 2002, June 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207053</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207053</id><created>2002-07-11</created><authors><author><keyname>Wielemaker</keyname><forenames>Jan</forenames></author><author><keyname>Anjewierden</keyname><forenames>Anjo</forenames></author></authors><title>An Architecture for Making Object-Oriented Systems Available from Prolog</title><categories>cs.SE</categories><comments>14 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  It is next to impossible to develop real-life applications in just pure
Prolog. With XPCE we realised a mechanism for integrating Prolog with an
external object-oriented system that turns this OO system into a natural
extension to Prolog. We describe the design and how it can be applied to other
external OO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207054</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207054</id><created>2002-07-11</created><authors><author><keyname>Falkman</keyname><forenames>Goran</forenames></author><author><keyname>Torgersson</keyname><forenames>Olof</forenames></author></authors><title>Enhancing Usefulness of Declarative Programming Frameworks through
  Complete Integration</title><categories>cs.SE</categories><comments>11 pages; Alexandre Tessier, editor; WLPE 2002,
  http://xxx.lanl.gov/abs/cs.SE/0207052</comments><acm-class>D.1.6; D.2.5; D.2.6; F.4.1; I.2.3</acm-class><abstract>  The Gisela framework for declarative programming was developed with the
specific aim of providing a tool that would be useful for knowledge
representation and reasoning within real-world applications. To achieve this, a
complete integration into an object-oriented application development
environment was used. The framework and methodology developed provide two
alternative application programming interfaces (APIs): Programming using
objects or programming using a traditional equational declarative style. In
addition to providing complete integration, Gisela also allows extensions and
modifications due to the general computation model and well-defined APIs. We
give a brief overview of the declarative model underlying Gisela and we present
the methodology proposed for building applications together with some real
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207055</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207055</id><created>2002-07-11</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author></authors><title>The Rise and Fall of the Church-Turing Thesis</title><categories>cs.CC cs.AI</categories><acm-class>F.1.1; F.1.2; F.2.0; F.4.1; I.1.2; I.2.0</acm-class><abstract>  The essay consists of three parts. In the first part, it is explained how
theory of algorithms and computations evaluates the contemporary situation with
computers and global networks. In the second part, it is demonstrated what new
perspectives this theory opens through its new direction that is called theory
of super-recursive algorithms. These algorithms have much higher computing
power than conventional algorithmic schemes. In the third part, we explicate
how realization of what this theory suggests might influence life of people in
future. It is demonstrated that now the theory is far ahead computing practice
and practice has to catch up with the theory. We conclude with a comparison of
different approaches to the development of information technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207056</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207056</id><created>2002-07-13</created><authors><author><keyname>Kakas</keyname><forenames>Antonis</forenames></author><author><keyname>Michael</keyname><forenames>Loizos</forenames></author></authors><title>Modeling Complex Domains of Actions and Change</title><categories>cs.AI</categories><comments>9 pages, 3 figures, to download the E-RES system and a full
  representation of the Zoo Scenario World, visit
  http://www.cs.ucy.ac.cy/~pslogic/</comments><acm-class>I.2.3</acm-class><abstract>  This paper studies the problem of modeling complex domains of actions and
change within high-level action description languages. We investigate two main
issues of concern: (a) can we represent complex domains that capture together
different problems such as ramifications, non-determinism and concurrency of
actions, at a high-level, close to the given natural ontology of the problem
domain and (b) what features of such a representation can affect, and how, its
computational behaviour. The paper describes the main problems faced in this
representation task and presents the results of an empirical study, carried out
through a series of controlled experiments, to analyze the computational
performance of reasoning in these representations. The experiments compare
different representations obtained, for example, by changing the basic ontology
of the domain or by varying the degree of use of indirect effect laws through
domain constraints. This study has helped to expose the main sources of
computational difficulty in the reasoning and suggest some methodological
guidelines for representing complex domains. Although our work has been carried
out within one particular high-level description language, we believe that the
results, especially those that relate to the problems of representation, are
independent of the specific modeling language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207057</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207057</id><created>2002-07-14</created><updated>2003-04-04</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>Physical Traces: Quantum vs. Classical Information Processing</title><categories>cs.CG cs.LO math.CT quant-ph</categories><comments>The paper is written for a computer science journal, requires some
  knowledge on category theory but provides some basics on quantum theory</comments><acm-class>F.1, F.2</acm-class><journal-ref>Electronic Notes in Theoretical Computer Science 69 (2003)</journal-ref><abstract>  Within the Geometry of Interaction (GoI) paradigm, we present a setting that
enables qualitative differences between classical and quantum processes to be
explored. The key construction is the physical interpretation/realization of
the traced monoidal categories of finite-dimensional vector spaces with tensor
product as monoidal structure and of finite sets and relations with Cartesian
product as monoidal structure, both of them providing a so-called wave-style
GoI. The developments in this paper reveal that envisioning state update due to
quantum measurement as a process provides a powerful tool for developing
high-level approaches to quantum information processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207058</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207058</id><created>2002-07-14</created><updated>2002-07-18</updated><authors><author><keyname>Leidner</keyname><forenames>Jochen L.</forenames></author></authors><title>Question Answering over Unstructured Data without Domain Restrictions</title><categories>cs.CL cs.IR</categories><comments>8 pages, 6 figures, 5 tables. To appear in Proc. TaCoS'02, Potsdam,
  Germany</comments><acm-class>I.2.7; H.3.1</acm-class><abstract>  Information needs are naturally represented as questions. Automatic
Natural-Language Question Answering (NLQA) has only recently become a practical
task on a larger scale and without domain constraints.
  This paper gives a brief introduction to the field, its history and the
impact of systematic evaluation competitions.
  It is then demonstrated that an NLQA system for English can be built and
evaluated in a very short time using off-the-shelf parsers and thesauri. The
system is based on Robust Minimal Recursion Semantics (RMRS) and is portable
with respect to the parser used as a frontend. It applies atomic term
unification supported by question classification and WordNet lookup for
semantic similarity matching of parsed question representation and free text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207059</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207059</id><created>2002-07-15</created><authors><author><keyname>Bench-Capon</keyname><forenames>T.</forenames></author></authors><title>Value Based Argumentation Frameworks</title><categories>cs.AI</categories><acm-class>I.2.3</acm-class><abstract>  This paper introduces the notion of value-based argumentation frameworks, an
extension of the standard argumentation frameworks proposed by Dung, which are
able toshow how rational decision is possible in cases where arguments derive
their force from the social values their acceptance would promote.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207060</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207060</id><created>2002-07-15</created><authors><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Wang</keyname><forenames>Kewen</forenames></author></authors><title>Preferred well-founded semantics for logic programming by alternating
  fixpoints: Preliminary report</title><categories>cs.AI</categories><comments>Proceedings of the Workshop on Preferences in Artificial Intelligence
  and Constraint In: Proceedings of the Workshop on Non-Monotonic Reasoning
  (NMR'2002)</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  We analyze the problem of defining well-founded semantics for ordered logic
programs within a general framework based on alternating fixpoint theory. We
start by showing that generalizations of existing answer set approaches to
preference are too weak in the setting of well-founded semantics. We then
specify some informal yet intuitive criteria and propose a semantical framework
for preference handling that is more suitable for defining well-founded
semantics for ordered logic programs. The suitability of the new approach is
convinced by the fact that many attractive properties are satisfied by our
semantics. In particular, our semantics is still correct with respect to
various existing answer sets semantics while it successfully overcomes the
weakness of their generalization to well-founded semantics. Finally, we
indicate how an existing preferred well-founded semantics can be captured
within our semantical framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207061</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207061</id><created>2002-07-15</created><updated>2006-11-14</updated><authors><author><keyname>Buchsbaum</keyname><forenames>Adam L.</forenames></author><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Rogers</keyname><forenames>Anne</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author><author><keyname>Westbrook</keyname><forenames>Jeffery R.</forenames></author></authors><title>Linear-Time Pointer-Machine Algorithms for Path-Evaluation Problems on
  Trees and Graphs</title><categories>cs.DS</categories><comments>41 pages; 10 figures; 1 table; 65 references. This work is partially
  covered by the extended abstracts, ``Linear-Time Pointer-Machine Algorithms
  for Least Common Ancestors, MST Verification, and Dominators,'' Proc. 30th
  ACM Symp. on Theory of Computing, pp. 279-888, 1998, and ``Finding Dominators
  Revisited,'' Proc. 15th ACM-SIAM Symp. on Discrete Algorithms, pp. 862-871,
  2004</comments><acm-class>D.3.4; E.1; F.1.1; F.2.2; G.2.2</acm-class><abstract>  We present algorithms that run in linear time on pointer machines for a
collection of problems, each of which either directly or indirectly requires
the evaluation of a function defined on paths in a tree. These problems
previously had linear-time algorithms but only for random-access machines
(RAMs); the best pointer-machine algorithms were super-linear by an
inverse-Ackermann-function factor. Our algorithms are also simpler, in some
cases substantially, than the previous linear-time RAM algorithms. Our
improvements come primarily from three new ideas: a refined analysis of path
compression that gives a linear bound if the compressions favor certain nodes,
a pointer-based radix sort as a replacement for table-based methods, and a more
careful partitioning of a tree into easily managed parts. Our algorithms
compute nearest common ancestors off-line, verify and construct minimum
spanning trees, do interval analysis on a flowgraph, find the dominators of a
flowgraph, and build the component tree of a weighted tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207062</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207062</id><created>2002-07-15</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>Some addenda on distance function wavelets</title><categories>cs.NA cs.CE</categories><acm-class>G1.8, G1.2</acm-class><abstract>  This report will add some supplements to the recently finished report series
on the distance function wavelets (DFW). First, we define the general distance
in terms of the Riesz potential, and then, the distance function Abel wavelets
are derived via the fractional integral and Laplacian. Second, the DFW Weyl
transform is found to be a shifted Laplace potential DFW. The DFW Radon
transform is also presented. Third, we present a conjecture on truncation error
formula of the multiple reciprocity Laplace DFW series and discuss its error
distributions in terms of node density distributions. Forth, we point out that
the Hermite distance function interpolation can be used to replace overlapping
in the domain decomposition in order to produce sparse matrix. Fifth, the shape
parameter is explained as a virtual extra axis contribution in terms of the
MQ-type Possion kernel. The report is concluded with some remarks on a range of
other issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207063</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207063</id><created>2002-07-15</created><authors><author><keyname>Spielman</keyname><forenames>Dan A.</forenames></author><author><keyname>Teng</keyname><forenames>Shang-hua</forenames></author><author><keyname>Ungor</keyname><forenames>Alper</forenames></author></authors><title>Parallel Delaunay Refinement: Algorithms and Analyses</title><categories>cs.CG</categories><comments>12 pages (short version); 2 figures; see also
  http://www.cs.duke.edu/~ungor/abstracts/parallelDelRef.html</comments><acm-class>F.2.2</acm-class><abstract>  In this paper, we analyze the complexity of natural parallelizations of
Delaunay refinement methods for mesh generation. The parallelizations employ a
simple strategy: at each iteration, they choose a set of ``independent'' points
to insert into the domain, and then update the Delaunay triangulation. We show
that such a set of independent points can be constructed efficiently in
parallel and that the number of iterations needed is $O(\log^2(L/s))$, where
$L$ is the diameter of the domain, and $s$ is the smallest edge in the output
mesh. In addition, we show that the insertion of each independent set of points
can be realized sequentially by Ruppert's method in two dimensions and
Shewchuk's in three dimensions. Therefore, our parallel Delaunay refinement
methods provide the same element quality and mesh size guarantees as the
sequential algorithms in both two and three dimensions. For quasi-uniform
meshes, such as those produced by Chew's method, we show that the number of
iterations can be reduced to $O(\log(L/s))$. To the best of our knowledge,
these are the first provably polylog$(L/s)$ parallel time Delaunay meshing
algorithms that generate well-shaped meshes of size optimal to within a
constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207064</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207064</id><created>2002-07-16</created><authors><author><keyname>Amir</keyname><forenames>Eyal</forenames></author></authors><title>Interpolation Theorems for Nonmonotonic Reasoning Systems</title><categories>cs.AI cs.LO</categories><comments>NMR'02</comments><acm-class>I.2.3;I.2.4;F.4.1</acm-class><abstract>  Craig's interpolation theorem (Craig 1957) is an important theorem known for
propositional logic and first-order logic. It says that if a logical formula
$\beta$ logically follows from a formula $\alpha$, then there is a formula
$\gamma$, including only symbols that appear in both $\alpha,\beta$, such that
$\beta$ logically follows from $\gamma$ and $\gamma$ logically follows from
$\alpha$. Such theorems are important and useful for understanding those logics
in which they hold as well as for speeding up reasoning with theories in those
logics. In this paper we present interpolation theorems in this spirit for
three nonmonotonic systems: circumscription, default logic and logic programs
with the stable models semantics (a.k.a. answer set semantics). These results
give us better understanding of those logics, especially in contrast to their
nonmonotonic characteristics. They suggest that some \emph{monotonicity}
principle holds despite the failure of classic monotonicity for these logics.
Also, they sometimes allow us to use methods for the decomposition of reasoning
for these systems, possibly increasing their applicability and tractability.
Finally, they allow us to build structured representations that use those
logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207065</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207065</id><created>2002-07-16</created><authors><author><keyname>Berzati</keyname><forenames>Dritan</forenames></author><author><keyname>Anrig</keyname><forenames>Bernhard</forenames></author><author><keyname>Kohlas</keyname><forenames>Juerg</forenames></author></authors><title>Embedding Default Logic in Propositional Argumentation Systems</title><categories>cs.AI</categories><comments>9 pages</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  In this paper we present a transformation of finite propositional default
theories into so-called propositional argumentation systems. This
transformation allows to characterize all notions of Reiter's default logic in
the framework of argumentation systems. As a consequence, computing extensions,
or determining wether a given formula belongs to one extension or all
extensions can be answered without leaving the field of classical propositional
logic. The transformation proposed is linear in the number of defaults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207066</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207066</id><created>2002-07-16</created><authors><author><keyname>Alber</keyname><forenames>Jochen</forenames><affiliation>Universitaet Tuebingen Germany</affiliation></author><author><keyname>Fellows</keyname><forenames>Michael R.</forenames><affiliation>University of Newcastle Australia</affiliation></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames><affiliation>Universitaet Tuebingen Germany</affiliation></author></authors><title>Polynomial Time Data Reduction for Dominating Set</title><categories>cs.DS</categories><comments>25 pages, 4 figures (using 8 files), extended abstract entitled
  &quot;Efficient Data Reduction for Dominating Set: A Linear Problem Kernel for the
  Planar Case&quot; appeared in the Proceedings of the 8th SWAT 2002, LNCS 2368,
  pages 150-159, Springer-Verlag, 2002</comments><acm-class>F.2.2; G.2.1; G.2.2</acm-class><abstract>  Dealing with the NP-complete Dominating Set problem on undirected graphs, we
demonstrate the power of data reduction by preprocessing from a theoretical as
well as a practical side. In particular, we prove that Dominating Set
restricted to planar graphs has a so-called problem kernel of linear size,
achieved by two simple and easy to implement reduction rules. Moreover, having
implemented our reduction rules, first experiments indicate the impressive
practical potential of these rules. Thus, this work seems to open up a new and
prospective way how to cope with one of the most important problems in graph
theory and combinatorial optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207067</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207067</id><created>2002-07-17</created><authors><author><keyname>Verheij</keyname><forenames>Bart</forenames></author></authors><title>On the existence and multiplicity of extensions in dialectical
  argumentation</title><categories>cs.AI</categories><comments>10 pages; 9th International Workshop on Non-Monotonic Reasoning
  (NMR'2002)</comments><acm-class>I.2.3; I.2.4</acm-class><journal-ref>Verheij, Bart (2002). On the existence and the multiplicity of
  extensions in dialectical argumentation. Proceedings of the 9th International
  Workshop on Non-Monotonic Reasoning (NMR'2002) (eds. S. Benferhat and E.
  Giunchiglia), pp. 416-425. Toulouse</journal-ref><abstract>  In the present paper, the existence and multiplicity problems of extensions
are addressed. The focus is on extension of the stable type. The main result of
the paper is an elegant characterization of the existence and multiplicity of
extensions in terms of the notion of dialectical justification, a close cousin
of the notion of admissibility. The characterization is given in the context of
the particular logic for dialectical argumentation DEFLOG. The results are of
direct relevance for several well-established models of defeasible reasoning
(like default logic, logic programming and argumentation frameworks), since
elsewhere dialectical argumentation has been shown to have close formal
connections with these models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207068</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207068</id><created>2002-07-17</created><authors><author><keyname>Korovin</keyname><forenames>Konstantin</forenames></author><author><keyname>Voronkov</keyname><forenames>Andrei</forenames></author></authors><title>Knuth-Bendix constraint solving is NP-complete</title><categories>cs.LO</categories><comments>27 pages</comments><acm-class>F.4.1</acm-class><abstract>  We show the NP-completeness of the existential theory of term algebras with
the Knuth-Bendix order by giving a nondeterministic polynomial-time algorithm
for solving Knuth-Bendix ordering constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207069</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207069</id><created>2002-07-17</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Small Large-Scale Wireless Networks: Mobility-Assisted Resource Discovery</title><categories>cs.NI</categories><comments>12 pages, 12 figures</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  In this study, the concept of small worlds is investigated in the context of
large-scale wireless ad hoc and sensor networks. Wireless networks are spatial
graphs that are usually much more clustered than random networks and have much
higher path length characteristics. We observe that by adding only few random
links, path length of wireless networks can be reduced drastically without
affecting clustering. What is even more interesting is that such links need not
be formed randomly but may be confined to a limited number of hops between the
connected nodes. This has an important practical implication, as now we can
introduce a distributed algorithm in large-scale wireless networks, based on
what we call contacts, to improve the performance of resource discovery in such
networks, without resorting to global flooding. We propose new contact-based
protocols for adding logical short cuts in wireless networks efficiently. The
new protocols take advantage of mobility in order to increase reachability of
the search. We study the performance of our proposed contact-based
architecture, and clarify the context in which large-scale wireless networks
can be turned into small world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207070</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207070</id><created>2002-07-18</created><updated>2003-07-25</updated><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames><affiliation>Harvard University</affiliation></author></authors><title>A continuation semantics of interrogatives that accounts for Baker's
  ambiguity</title><categories>cs.CL cs.PL</categories><comments>20 pages; typo fixed</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of SALT XII: Semantics and Linguistic Theory, ed.
  Brendan Jackson, 246-265 (2002)</journal-ref><abstract>  Wh-phrases in English can appear both raised and in-situ. However, only
in-situ wh-phrases can take semantic scope beyond the immediately enclosing
clause. I present a denotational semantics of interrogatives that naturally
accounts for these two properties. It neither invokes movement or economy, nor
posits lexical ambiguity between raised and in-situ occurrences of the same
wh-phrase. My analysis is based on the concept of continuations. It uses a
novel type system for higher-order continuations to handle wide-scope
wh-phrases while remaining strictly compositional. This treatment sheds light
on the combinatorics of interrogatives as well as other kinds of so-called
A'-movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207071</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207071</id><created>2002-07-19</created><authors><author><keyname>Pearce</keyname><forenames>David</forenames></author><author><keyname>Sarsakov</keyname><forenames>Vladimir</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author><author><keyname>Tompits</keyname><forenames>Hans</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>A Polynomial Translation of Logic Programs with Nested Expressions into
  Disjunctive Logic Programs: Preliminary Report</title><categories>cs.AI cs.LO</categories><comments>10 pages; published in Proceedings of the 9th International Workshop
  on Non-Monotonic Reasoning</comments><acm-class>I.2.4</acm-class><abstract>  Nested logic programs have recently been introduced in order to allow for
arbitrarily nested formulas in the heads and the bodies of logic program rules
under the answer sets semantics. Nested expressions can be formed using
conjunction, disjunction, as well as the negation as failure operator in an
unrestricted fashion. This provides a very flexible and compact framework for
knowledge representation and reasoning. Previous results show that nested logic
programs can be transformed into standard (unnested) disjunctive logic programs
in an elementary way, applying the negation as failure operator to body
literals only. This is of great practical relevance since it allows us to
evaluate nested logic programs by means of off-the-shelf disjunctive logic
programming systems, like DLV. However, it turns out that this straightforward
transformation results in an exponential blow-up in the worst-case, despite the
fact that complexity results indicate that there is a polynomial translation
among both formalisms. In this paper, we take up this challenge and provide a
polynomial translation of logic programs with nested expressions into
disjunctive logic programs. Moreover, we show that this translation is modular
and (strongly) faithful. We have implemented both the straightforward as well
as our advanced transformation; the resulting compiler serves as a front-end to
DLV and is publicly available on the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207072</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207072</id><created>2002-07-20</created><authors><author><keyname>Cadoli</keyname><forenames>Marco</forenames></author><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author></authors><title>Complexity of Nested Circumscription and Nested Abnormality Theories</title><categories>cs.AI cs.CC cs.LO</categories><comments>A preliminary abstract of this paper appeared in Proc. Seventeenth
  International Joint Conference on Artificial Intelligence (IJCAI-01), pages
  169--174. Morgan Kaufmann, 2001</comments><report-no>INFSYS RR-1843-02-10, Institut f. Informationssysteme, TU Vienna,
  2002</report-no><acm-class>I.2.3; I.2.4; F4.1; F.2.2</acm-class><abstract>  The need for a circumscriptive formalism that allows for simple yet elegant
modular problem representation has led Lifschitz (AIJ, 1995) to introduce
nested abnormality theories (NATs) as a tool for modular knowledge
representation, tailored for applying circumscription to minimize exceptional
circumstances. Abstracting from this particular objective, we propose L_{CIRC},
which is an extension of generic propositional circumscription by allowing
propositional combinations and nesting of circumscriptive theories. As shown,
NATs are naturally embedded into this language, and are in fact of equal
expressive capability. We then analyze the complexity of L_{CIRC} and NATs, and
in particular the effect of nesting. The latter is found to be a source of
complexity, which climbs the Polynomial Hierarchy as the nesting depth
increases and reaches PSPACE-completeness in the general case. We also identify
meaningful syntactic fragments of NATs which have lower complexity. In
particular, we show that the generalization of Horn circumscription in the NAT
framework remains CONP-complete, and that Horn NATs without fixed letters can
be efficiently transformed into an equivalent Horn CNF, which implies
polynomial solvability of principal reasoning tasks. Finally, we also study
extensions of NATs and briefly address the complexity in the first-order case.
Our results give insight into the ``cost'' of using L_{CIRC} (resp. NATs) as a
host language for expressing other formalisms such as action theories,
narratives, or spatial theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207073</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207073</id><created>2002-07-20</created><authors><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author></authors><title>Reinforcing Reachable Routes</title><categories>cs.NI cs.AI</categories><acm-class>C.2.2; I.2.6</acm-class><abstract>  This paper studies the evaluation of routing algorithms from the perspective
of reachability routing, where the goal is to determine all paths between a
sender and a receiver. Reachability routing is becoming relevant with the
changing dynamics of the Internet and the emergence of low-bandwidth
wireless/ad-hoc networks. We make the case for reinforcement learning as the
framework of choice to realize reachability routing, within the confines of the
current Internet infrastructure. The setting of the reinforcement learning
problem offers several advantages, including loop resolution, multi-path
forwarding capability, cost-sensitive routing, and minimizing state overhead,
while maintaining the incremental spirit of current backbone routing
algorithms. We identify research issues in reinforcement learning applied to
the reachability routing problem to achieve a fluid and robust backbone routing
framework. The paper is targeted toward practitioners seeking to implement a
reachability routing algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207074</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207074</id><created>2002-07-21</created><authors><author><keyname>Goldin</keyname><forenames>Dina</forenames><affiliation>U. of Connecticut</affiliation></author><author><keyname>Wegner</keyname><forenames>Peter</forenames><affiliation>Brown U.</affiliation></author></authors><title>Paraconsistency of Interactive Computation</title><categories>cs.LO</categories><comments>10 pages, no figures. Originally published in proc. PCL 2002, a FLoC
  workshop; eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu
  Waragai (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; F.1.2; I.2.0; I.2.11</acm-class><abstract>  The goal of computational logic is to allow us to model computation as well
as to reason about it. We argue that a computational logic must be able to
model interactive computation. We show that first-order logic cannot model
interactive computation due to the incompleteness of interaction. We show that
interactive computation is necessarily paraconsistent, able to model both a
fact and its negation, due to the role of the world (environment) in
determining the course of the computation. We conclude that paraconsistency is
a necessary property for a logic that can model interactive computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207075</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207075</id><created>2002-07-21</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic
  Logic and Probabilistic Logic under Coherence</title><categories>cs.AI</categories><comments>10 pages; in Proceedings of the 9th International Workshop on
  Non-Monotonic Reasoning (NMR-2002), Special Session on Uncertainty Frameworks
  in Nonmonotonic Reasoning, pages 265-274, Toulouse, France, April 2002</comments><acm-class>I.2.3; I.2.4</acm-class><abstract>  Recently, it has been shown that probabilistic entailment under coherence is
weaker than model-theoretic probabilistic entailment. Moreover, probabilistic
entailment under coherence is a generalization of default entailment in System
P. In this paper, we continue this line of research by presenting probabilistic
generalizations of more sophisticated notions of classical default entailment
that lie between model-theoretic probabilistic entailment and probabilistic
entailment under coherence. That is, the new formalisms properly generalize
their counterparts in classical default reasoning, they are weaker than
model-theoretic probabilistic entailment, and they are stronger than
probabilistic entailment under coherence. The new formalisms are useful
especially for handling probabilistic inconsistencies related to conditioning
on zero events. They can also be applied for probabilistic belief revision.
More generally, in the same spirit as a similar previous paper, this paper
sheds light on exciting new formalisms for probabilistic reasoning beyond the
well-known standard ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207076</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207076</id><created>2002-07-22</created><authors><author><keyname>Bertino</keyname><forenames>Elisa</forenames></author><author><keyname>Catania</keyname><forenames>Barbara</forenames></author><author><keyname>Perlasca</keyname><forenames>Paolo</forenames></author></authors><title>Introducing Dynamic Behavior in Amalgamated Knowledge Bases</title><categories>cs.PL cs.DB cs.LO</categories><comments>Other Keywords: Deductive databases; Heterogeneous databases; Active
  rules; Updates</comments><acm-class>D.1.6 Logic Programming; H.2.5 Heterogeneous Databases; F.4.1
  Mathematical Logic, Logic and constraint programming</acm-class><abstract>  The problem of integrating knowledge from multiple and heterogeneous sources
is a fundamental issue in current information systems. In order to cope with
this problem, the concept of mediator has been introduced as a software
component providing intermediate services, linking data resources and
application programs, and making transparent the heterogeneity of the
underlying systems. In designing a mediator architecture, we believe that an
important aspect is the definition of a formal framework by which one is able
to model integration according to a declarative style. To this purpose, the use
of a logical approach seems very promising. Another important aspect is the
ability to model both static integration aspects, concerning query execution,
and dynamic ones, concerning data updates and their propagation among the
various data sources. Unfortunately, as far as we know, no formal proposals for
logically modeling mediator architectures both from a static and dynamic point
of view have already been developed. In this paper, we extend the framework for
amalgamated knowledge bases, presented by Subrahmanian, to deal with dynamic
aspects. The language we propose is based on the Active U-Datalog language, and
extends it with annotated logic and amalgamation concepts. We model the sources
of information and the mediator (also called supervisor) as Active U-Datalog
deductive databases, thus modeling queries, transactions, and active rules,
interpreted according to the PARK semantics. By using active rules, the system
can efficiently perform update propagation among different databases. The
result is a logical environment, integrating active and deductive rules, to
perform queries and update propagation in an heterogeneous mediated framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207077</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207077</id><created>2002-07-22</created><authors><author><keyname>Sherwani</keyname><forenames>Jahanzeb</forenames></author><author><keyname>Ali</keyname><forenames>Nosheen</forenames></author><author><keyname>Lotia</keyname><forenames>Nausheen</forenames></author><author><keyname>Hayat</keyname><forenames>Zahra</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Libra: An Economy driven Job Scheduling System for Clusters</title><categories>cs.DC cs.DS</categories><comments>13 pages</comments><report-no>Technical Report, July 2002, Dept. of Computer Science and Software
  Engineering, The University of Melbourne</report-no><acm-class>* C.1.4</acm-class><abstract>  Clusters of computers have emerged as mainstream parallel and distributed
platforms for high-performance, high-throughput and high-availability
computing. To enable effective resource management on clusters, numerous
cluster managements systems and schedulers have been designed. However, their
focus has essentially been on maximizing CPU performance, but not on improving
the value of utility delivered to the user and quality of services. This paper
presents a new computational economy driven scheduling system called Libra,
which has been designed to support allocation of resources based on the users?
quality of service (QoS) requirements. It is intended to work as an add-on to
the existing queuing and resource management system. The first version has been
implemented as a plugin scheduler to the PBS (Portable Batch System) system.
The scheduler offers market-based economy driven service for managing batch
jobs on clusters by scheduling CPU time according to user utility as determined
by their budget and deadline rather than system performance considerations. The
Libra scheduler ensures that both these constraints are met within an O(n)
run-time. The Libra scheduler has been simulated using the GridSim toolkit to
carry out a detailed performance analysis. Results show that the deadline and
budget based proportional resource allocation strategy improves the utility of
the system and user satisfaction as compared to system-centric scheduling
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207078</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207078</id><created>2002-07-23</created><authors><author><keyname>Benczur</keyname><forenames>Andras</forenames></author><author><keyname>Karger</keyname><forenames>David R.</forenames></author></authors><title>Randomized Approximation Schemes for Cuts and Flows in Capacitated
  Graphs</title><categories>cs.DS cs.DM</categories><comments>Draft journal version combining conference publications in STOC '96
  and SODA '98</comments><acm-class>F.2.2; G.2.1;G.2.2</acm-class><abstract>  We improve on random sampling techniques for approximately solving problems
that involve cuts and flows in graphs. We give a near-linear-time construction
that transforms any graph on n vertices into an O(n\log n)-edge graph on the
same vertices whose cuts have approximately the same value as the original
graph's. In this new graph, for example, we can run the O(m^{3/2})-time maximum
flow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2})
time. This corresponds to a (1+epsilon)-times minimum s--t cut in the original
graph. In a similar way, we can approximate a sparsest cut to within O(log n)
in O(n^2) time using a previous O(mn)-time algorithm. A related approach leads
to a randomized divide and conquer algorithm producing an approximately maximum
flow in O(m sqrt{n}) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207079</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207079</id><created>2002-07-23</created><updated>2002-11-14</updated><authors><author><keyname>Grigoriev</keyname><forenames>D.</forenames></author><author><keyname>Ponomarenko</keyname><forenames>I.</forenames></author></authors><title>On non-abelian homomorphic public-key cryptosystems</title><categories>cs.CR</categories><comments>15 pages, LaTeX</comments><acm-class>E.3</acm-class><abstract>  An important problem of modern cryptography concerns secret public-key
computations in algebraic structures. We construct homomorphic cryptosystems
being (secret) epimorphisms f:G --&gt; H, where G, H are (publically known) groups
and H is finite. A letter of a message to be encrypted is an element h element
of H, while its encryption g element of G is such that f(g)=h. A homomorphic
cryptosystem allows one to perform computations (operating in a group G) with
encrypted information (without knowing the original message over H).
  In this paper certain homomorphic cryptosystems are constructed for the first
time for non-abelian groups H (earlier, homomorphic cryptosystems were known
only in the Abelian case). In fact, we present such a system for any solvable
(fixed) group H.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207080</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207080</id><created>2002-07-23</created><authors><author><keyname>Grigoriev</keyname><forenames>D.</forenames></author></authors><title>Public-key cryptography and invariant theory</title><categories>cs.CR</categories><comments>10 pages, LaTeX</comments><acm-class>E.3</acm-class><abstract>  Public-key cryptosystems are suggested based on invariants of groups. We give
also an overview of the known cryptosystems which involve groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207081</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207081</id><created>2002-07-24</created><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Moebius-Invariant Natural Neighbor Interpolation</title><categories>cs.CG</categories><comments>2 pages, 1 figure</comments><acm-class>F.2.2</acm-class><abstract>  We propose an interpolation method that is invariant under Moebius
transformations; that is, interpolation followed by transformation gives the
same result as transformation followed by interpolation. The method uses
natural (Delaunay) neighbors, but weights neighbors according to angles formed
by Delaunay circles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207082</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207082</id><created>2002-07-24</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Dynamic Generators of Topologically Embedded Graphs</title><categories>cs.DS</categories><comments>13 pages, 2 figures</comments><acm-class>F.2.2</acm-class><abstract>  We provide a data structure for maintaining an embedding of a graph on a
surface (represented combinatorially by a permutation of edges around each
vertex) and computing generators of the fundamental group of the surface, in
amortized time O(log n + log g(log log g)^3) per update on a surface of genus
g; we can also test orientability of the surface in the same time, and maintain
the minimum and maximum spanning tree of the graph in time O(log n + log^4 g)
per update. Our data structure allows edge insertion and deletion as well as
the dual operations; these operations may implicitly change the genus of the
embedding surface. We apply similar ideas to improve the constant factor in a
separator theorem for low-genus graphs, and to find in linear time a
tree-decomposition of low-genus low-diameter graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207083</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207083</id><created>2002-07-24</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr.</suffix></author><author><keyname>Teng</keyname><forenames>Choh Man</forenames></author></authors><title>Evaluating Defaults</title><categories>cs.AI</categories><comments>8 pages</comments><acm-class>I.2.4</acm-class><abstract>  We seek to find normative criteria of adequacy for nonmonotonic logic similar
to the criterion of validity for deductive logic. Rather than stipulating that
the conclusion of an inference be true in all models in which the premises are
true, we require that the conclusion of a nonmonotonic inference be true in
``almost all'' models of a certain sort in which the premises are true. This
``certain sort'' specification picks out the models that are relevant to the
inference, taking into account factors such as specificity and vagueness, and
previous inferences. The frequencies characterizing the relevant models reflect
known frequencies in our actual world. The criteria of adequacy for a default
inference can be extended by thresholding to criteria of adequacy for an
extension. We show that this avoids the implausibilities that might otherwise
result from the chaining of default inferences. The model proportions, when
construed in terms of frequencies, provide a verifiable grounding of default
rules, and can become the basis for generating default rules from statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207084</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207084</id><created>2002-07-25</created><authors><author><keyname>Besnard</keyname><forenames>Philippe</forenames><affiliation>Institut f&#xfc;r Informatik, Universit&#xe4;t Potsdam</affiliation></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames><affiliation>Institut f&#xfc;r Informatik, Universit&#xe4;t Potsdam</affiliation></author><author><keyname>Tompits</keyname><forenames>Hans</forenames><affiliation>Institut f&#xfc;r Informationssysteme, Technische Universit&#xe4;t Wien</affiliation></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames><affiliation>Institut f&#xfc;r Informationssysteme, Technische Universit&#xe4;t Wien</affiliation></author></authors><title>Paraconsistent Reasoning via Quantified Boolean Formulas,I: Axiomatising
  Signed Systems</title><categories>cs.LO cs.CC</categories><comments>15 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; F.1.3; I.2.3; I.2.4</acm-class><abstract>  Signed systems were introduced as a general, syntax-independent framework for
paraconsistent reasoning, that is, non-trivialised reasoning from inconsistent
information. In this paper, we show how the family of corresponding
paraconsistent consequence relations can be axiomatised by means of quantified
Boolean formulas. This approach has several benefits. First, it furnishes an
axiomatic specification of paraconsistent reasoning within the framework of
signed systems. Second, this axiomatisation allows us to identify upper bounds
for the complexity of the different signed consequence relations. We strengthen
these upper bounds by providing strict complexity results for the considered
reasoning tasks. Finally, we obtain an implementation of different forms of
paraconsistent reasoning by appeal to the existing system QUIP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207085</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207085</id><created>2002-07-25</created><authors><author><keyname>Arieli</keyname><forenames>Ofer</forenames><affiliation>The Academic College of Tel-Aviv, Israel</affiliation></author><author><keyname>Denecker</keyname><forenames>Marc</forenames><affiliation>The Catholic University of Leuven, Belgium</affiliation></author><author><keyname>Van Nuffelen</keyname><forenames>Bert</forenames><affiliation>The Catholic University of Leuven, Belgium</affiliation></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames><affiliation>The Catholic University of Leuven, Belgium</affiliation></author></authors><title>Repairing Inconsistent Databases: A Model-Theoretic Approach and
  Abductive Reasoning</title><categories>cs.LO cs.DB</categories><comments>15 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; H.2.7; I.2.3</acm-class><abstract>  In this paper we consider two points of views to the problem of coherent
integration of distributed data. First we give a pure model-theoretic analysis
of the possible ways to `repair' a database. We do so by characterizing the
possibilities to `recover' consistent data from an inconsistent database in
terms of those models of the database that exhibit as minimal inconsistent
information as reasonably possible. Then we introduce an abductive application
to restore the consistency of a given database. This application is based on an
abductive solver (A-system) that implements an SLDNFA-resolution procedure, and
computes a list of data-facts that should be inserted to the database or
retracted from it in order to keep the database consistent. The two approaches
for coherent data integration are related by soundness and completeness
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207086</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207086</id><created>2002-07-25</created><authors><author><keyname>Maher</keyname><forenames>Michael J.</forenames><affiliation>Loyola University, Chicago</affiliation></author></authors><title>A Model-Theoretic Semantics for Defeasible Logic</title><categories>cs.LO</categories><comments>14 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><abstract>  Defeasible logic is an efficient logic for defeasible reasoning. It is
defined through a proof theory and, until now, has had no model theory. In this
paper a model-theoretic semantics is given for defeasible logic. The logic is
sound and complete with respect to the semantics. We also briefly outline how
this approach extends to a wide range of defeasible logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207087</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207087</id><created>2002-07-25</created><authors><author><keyname>Zhang</keyname><forenames>Guo-Qiang</forenames><affiliation>Case Western Reserve University</affiliation></author></authors><title>Axiomatic Aspects of Default Inference</title><categories>cs.LO</categories><comments>16 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; I.2.3</acm-class><abstract>  This paper studies axioms for nonmonotonic consequences from a
semantics-based point of view, focusing on a class of mathematical structures
for reasoning about partial information without a predefined syntax/logic. This
structure is called a default structure. We study axioms for the nonmonotonic
consequence relation derived from extensions as in Reiter's default logic,
using skeptical reasoning, but extensions are now used for the construction of
possible worlds in a default information structure.
  In previous work we showed that skeptical reasoning arising from
default-extensions obeys a well-behaved set of axioms including the axiom of
cautious cut. We show here that, remarkably, the converse is also true: any
consequence relation obeying this set of axioms can be represented as one
constructed from skeptical reasoning. We provide representation theorems to
relate axioms for nonmonotonic consequence relation and properties about
extensions, and provide one-to-one correspondence between nonmonotonic systems
which satisfies the law of cautious monotony and default structures with unique
extensions. Our results give a theoretical justification for a set of basic
rules governing the update of nonmonotonic knowledge bases, demonstrating the
derivation of them from the more concrete and primitive construction of
extensions. It is also striking to note that proofs of the representation
theorems show that only shallow extensions are necessary, in the sense that the
number of iterations needed to achieve an extension is at most three. All of
these developments are made possible by taking a more liberal view of
consistency: consistency is a user defined predicate, satisfying some basic
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207088</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207088</id><created>2002-07-25</created><updated>2003-12-26</updated><authors><author><keyname>Villadsen</keyname><forenames>J&#xf8;rgen</forenames></author></authors><title>A Paraconsistent Higher Order Logic</title><categories>cs.LO cs.AI</categories><comments>Originally in the proceedings of PCL 2002, editors Hendrik Decker,
  Joergen Villadsen, Toshiharu Waragai (http://floc02.diku.dk/PCL/). Corrected</comments><acm-class>F.4.1; I.2.4; I.2.1</acm-class><abstract>  Classical logic predicts that everything (thus nothing useful at all) follows
from inconsistency. A paraconsistent logic is a logic where an inconsistency
does not lead to such an explosion, and since in practice consistency is
difficult to achieve there are many potential applications of paraconsistent
logics in knowledge-based systems, logical semantics of natural language, etc.
Higher order logics have the advantages of being expressive and with several
automated theorem provers available. Also the type system can be helpful. We
present a concise description of a paraconsistent higher order logic with
countable infinite indeterminacy, where each basic formula can get its own
indeterminate truth value (or as we prefer: truth code). The meaning of the
logical operators is new and rather different from traditional many-valued
logics as well as from logics based on bilattices. The adequacy of the logic is
examined by a case study in the domain of medicine. Thus we try to build a
bridge between the HOL and MVL communities. A sequent calculus is proposed
based on recent work by Muskens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207089</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207089</id><created>2002-07-25</created><authors><author><keyname>Ma&#x142;uszy&#x144;ski</keyname><forenames>Jan</forenames><affiliation>Link&#xf6;ping University, Sweden</affiliation></author><author><keyname>Vit&#xf3;ria</keyname><forenames>Aida</forenames><affiliation>Link&#xf6;ping University, Sweden</affiliation></author></authors><title>Defining Rough Sets by Extended Logic Programs</title><categories>cs.LO cs.PL</categories><comments>10 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; I.2.3; I.2.4; D.1.6</acm-class><abstract>  We show how definite extended logic programs can be used for defining and
reasoning with rough sets. Moreover, a rough-set-specific query language is
presented and an answering algorithm is outlined. Thus, we not only show a
possible application of a paraconsistent logic to the field of rough sets as we
also establish a link between rough set theory and logic programming, making
possible transfer of expertise between both fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207090</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207090</id><created>2002-07-25</created><authors><author><keyname>Batens</keyname><forenames>Diderik</forenames><affiliation>Universiteit Gent, Belgium</affiliation></author></authors><title>On a Partial Decision Method for Dynamic Proofs</title><categories>cs.LO</categories><comments>18 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; I.2.3; I.2.4</acm-class><abstract>  This paper concerns a goal directed proof procedure for the propositional
fragment of the adaptive logic ACLuN1. At the propositional level, it forms an
algorithm for final derivability. If extended to the predicative level, it
provides a criterion for final derivability. This is essential in view of the
absence of a positive test. The procedure may be generalized to all flat
adaptive logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207091</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207091</id><created>2002-07-25</created><authors><author><keyname>Bry</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>University of Munich, Germany</affiliation></author></authors><title>An Almost Classical Logic for Logic Programming and Nonmonotonic
  Reasoning</title><categories>cs.LO</categories><comments>16 pages. Originally published in proc. PCL 2002, a FLoC workshop;
  eds. Hendrik Decker, Dina Goldin, Jorgen Villadsen, Toshiharu Waragai
  (http://floc02.diku.dk/PCL/)</comments><acm-class>F.4.1; I.2.3; I.2.4; D.3.1</acm-class><abstract>  The model theory of a first-order logic called N^4 is introduced. N^4 does
not eliminate double negations, as classical logic does, but instead reduces
fourfold negations. N^4 is very close to classical logic: N^4 has two truth
values; implications in N^4 are material, like in classical logic; and negation
distributes over compound formulas in N^4 as it does in classical logic.
Results suggest that the semantics of normal logic programs is conveniently
formalized in N^4: Classical logic Herbrand interpretations generalize
straightforwardly to N^4; the classical minimal Herbrand model of a positive
logic program coincides with its unique minimal N^4 Herbrand model; the stable
models of a normal logic program and its so-called complete minimal N^4
Herbrand models coincide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207092</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207092</id><created>2002-07-25</created><authors><author><keyname>Fuks</keyname><forenames>Henryk</forenames></author><author><keyname>Lawniczak</keyname><forenames>Anna T.</forenames></author><author><keyname>Volkov</keyname><forenames>Stanislav</forenames></author></authors><title>Packet delay in models of data networks</title><categories>cs.NI nlin.CG</categories><comments>19 pages, 4 figures</comments><acm-class>I.6.5;G3</acm-class><journal-ref>ACM Transactions on Modelling and Simulations, vol. 11, pp.
  233--250 (2001)</journal-ref><abstract>  We investigate individual packet delay in a model of data networks with
table-free, partial table and full table routing. We present analytical
estimation for the average packet delay in a network with small partial routing
table. Dependence of the delay on the size of the network and on the size of
the partial routing table is examined numerically. Consequences for network
scalability are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207093</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207093</id><created>2002-07-26</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author></authors><title>Preference Queries</title><categories>cs.DB</categories><comments>34 pages</comments><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207094</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207094</id><created>2002-07-26</created><authors><author><keyname>Arenas</keyname><forenames>Marcelo</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author></authors><title>Answer Sets for Consistent Query Answering in Inconsistent Databases</title><categories>cs.DB</categories><comments>34 pages</comments><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207095</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207095</id><created>2002-07-29</created><updated>2003-08-27</updated><authors><author><keyname>Hesselink</keyname><forenames>Wim H.</forenames></author></authors><title>Eternity variables to prove simulation of specifications</title><categories>cs.DC cs.LO</categories><comments>28 pages, to appear in ACM-TOCL</comments><acm-class>F.1.1;F.3.1</acm-class><journal-ref>ACM Trans. on Computational Logic 6 (2005) 175-201.</journal-ref><abstract>  Simulations of specifications are introduced as a unification and
generalization of refinement mappings, history variables, forward simulations,
prophecy variables, and backward simulations. A specification implements
another specification if and only if there is a simulation from the first one
to the second one that satisfies a certain condition. By adding stutterings,
the formalism allows that the concrete behaviours take more (or possibly less)
steps than the abstract ones.
  Eternity variables are introduced as a more powerful alternative for prophecy
variables and backward simulations. This formalism is semantically complete:
every simulation that preserves quiescence is a composition of a forward
simulation, an extension with eternity variables, and a refinement mapping.
This result does not need finite invisible nondeterminism and machine closure
as in the Abadi-Lamport Theorem. Internal continuity is weakened to
preservation of quiescence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207096</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207096</id><created>2002-07-29</created><authors><author><keyname>Ching</keyname><forenames>Avery</forenames></author><author><keyname>Choudhary</keyname><forenames>Alok</forenames></author><author><keyname>Liao</keyname><forenames>Wei-keng</forenames></author><author><keyname>Ross</keyname><forenames>Rob</forenames></author><author><keyname>Gropp</keyname><forenames>William</forenames></author></authors><title>Noncontiguous I/O through PVFS</title><categories>cs.DC</categories><comments>10 pages, 17 figures</comments><report-no>ANL/MCS-P970-0702</report-no><acm-class>D.4.3</acm-class><abstract>  With the tremendous advances in processor and memory technology, I/O has
risen to become the bottleneck in high-performance computing for many
applications. The development of parallel file systems has helped to ease the
performance gap, but I/O still remains an area needing significant performance
improvement. Research has found that noncontiguous I/O access patterns in
scientific applications combined with current file system methods to perform
these accesses lead to unacceptable performance for large data sets. To enhance
performance of noncontiguous I/O we have created list I/O, a native version of
noncontiguous I/O. We have used the Parallel Virtual File System (PVFS) to
implement our ideas. Our research and experimentation shows that list I/O
outperforms current noncontiguous I/O access methods in most I/O situations and
can substantially enhance the performance of real-world scientific
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0207097</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0207097</id><created>2002-07-31</created><updated>2002-12-23</updated><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Optimal Ordered Problem Solver</title><categories>cs.AI cs.CC cs.LG</categories><comments>43 pages, 2 figures, short version at NIPS 2002 (added 1 figure and
  references; streamlined presentation)</comments><report-no>IDSIA-12-02</report-no><acm-class>I.2.2;I.2.6;I.2.8</acm-class><journal-ref>Machine Learning, 54, 211-254, 2004.</journal-ref><abstract>  We present a novel, general, optimally fast, incremental way of searching for
a universal algorithm that solves each task in a sequence of tasks. The Optimal
Ordered Problem Solver (OOPS) continually organizes and exploits previously
found solutions to earlier tasks, efficiently searching not only the space of
domain-specific algorithms, but also the space of search algorithms.
Essentially we extend the principles of optimal nonincremental universal search
to build an incremental universal learner that is able to improve itself
through experience. In illustrative experiments, our self-improver becomes the
first general system that learns to solve all n disk Towers of Hanoi tasks
(solution size 2^n-1) for n up to 30, profiting from previously solved, simpler
tasks involving samples of a simple context free language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208001</id><created>2002-08-01</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Classification of Random Boolean Networks</title><categories>cs.CC cs.DM math.DS nlin.CG</categories><comments>8 pages, 11 figures, 5 tables. To be published in Standish, Abbass
  and Bedau (eds.) Artificial Life VIII</comments><acm-class>F.1.1, B.6.1, C.1.3</acm-class><abstract>  We provide the first classification of different types of Random Boolean
Networks (RBNs). We study the differences of RBNs depending on the degree of
synchronicity and determinism of their updating scheme. For doing so, we first
define three new types of RBNs. We note some similarities and differences
between different types of RBNs with the aid of a public software laboratory we
developed. Particularly, we find that the point attractors are independent of
the updating scheme, and that RBNs are more different depending on their
determinism or non-determinism rather than depending on their synchronicity or
asynchronicity. We also show a way of mapping non-synchronous deterministic
RBNs into synchronous RBNs. Our results are important for justifying the use of
specific types of RBNs for modelling natural phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208002</id><created>2002-08-02</created><authors><author><keyname>Lavrenov</keyname><forenames>A.</forenames></author></authors><title>Theoretical limit of the compression for the information</title><categories>cs.CR</categories><comments>5 pages, report at the IMK's conference &quot;Science and pedagogics on a
  boundary 21 centuries&quot;, in Russian</comments><acm-class>D.4.6; E.3</acm-class><abstract>  The pit recording of file, the coefficient of compression are introduced. The
theoretical limit of the information compression as minimal coefficient of
compression for the given length of alphabet are found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208003</id><created>2002-08-02</created><updated>2002-08-05</updated><authors><author><keyname>Lavrenov</keyname><forenames>A.</forenames></author></authors><title>MV2-algorithm's clones</title><categories>cs.CR</categories><comments>5 pages, report at the IMK's conference &quot;Science and pedagogics on a
  boundary 21 centuries&quot;, in Russian</comments><acm-class>D.4.6; E.3</acm-class><abstract>  The clones of MV2 algorithm for any radix are discussed. The three various
examples of ones are represented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208004</id><created>2002-08-03</created><authors><author><keyname>Klein</keyname><forenames>Philip N.</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author><author><keyname>Netzer</keyname><forenames>Rob H. B.</forenames></author></authors><title>Detecting Race Conditions in Parallel Programs that Use Semaphores</title><categories>cs.DS cs.DC</categories><comments>24 pages, 12 figures, preliminary versions appeared in WADS 93 and
  ESA 96</comments><acm-class>F.2.2; G.2.2; D.1.3; D.4.1; E.1</acm-class><journal-ref>Algorithmica, 35(4):321-345, 2003</journal-ref><doi>10.1007/s00453-002-1004-3</doi><abstract>  We address the problem of detecting race conditions in programs that use
semaphores for synchronization. Netzer and Miller showed that it is NP-complete
to detect race conditions in programs that use many semaphores. We show in this
paper that it remains NP-complete even if only two semaphores are used in the
parallel programs.
  For the tractable case, i.e., using only one semaphore, we give two
algorithms for detecting race conditions from the trace of executing a parallel
program on p processors, where n semaphore operations are executed. The first
algorithm determines in O(n) time whether a race condition exists between any
two given operations. The second algorithm runs in O(np log n) time and outputs
a compact representation from which one can determine in O(1) time whether a
race condition exists between any two given operations. The second algorithm is
near-optimal in that the running time is only O(log n) times the time required
simply to write down the output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208005</id><created>2002-08-05</created><authors><author><keyname>Hillenbrand</keyname><forenames>Ulrich</forenames></author><author><keyname>Hirzinger</keyname><forenames>Gerd</forenames></author></authors><title>Probabilistic Search for Object Segmentation and Recognition</title><categories>cs.CV</categories><comments>18 pages, 5 figures</comments><acm-class>I.2.10; I.4.6; I.4.7; I.4.8; I.5.4</acm-class><journal-ref>Proceedings ECCV 2002, Lecture Notes in Computer Science Vol.
  2352, pp. 791-806</journal-ref><abstract>  The problem of searching for a model-based scene interpretation is analyzed
within a probabilistic framework. Object models are formulated as generative
models for range data of the scene. A new statistical criterion, the truncated
object probability, is introduced to infer an optimal sequence of object
hypotheses to be evaluated for their match to the data. The truncated
probability is partly determined by prior knowledge of the objects and partly
learned from data. Some experiments on sequence quality and object segmentation
and recognition from stereo data are presented. The article recovers classic
concepts from object recognition (grouping, geometric hashing, alignment) from
the probabilistic perspective and adds insight into the optimal ordering of
object hypotheses for evaluation. Moreover, it introduces point-relation
densities, a key component of the truncated probability, as statistical models
of local surface shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208006</id><created>2002-08-05</created><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author></authors><title>Rectangle Size Bounds and Threshold Covers in Communication Complexity</title><categories>cs.CC</categories><comments>34 pages</comments><acm-class>F.1.3;F.1.2.2</acm-class><abstract>  We investigate the power of the most important lower bound technique in
randomized communication complexity, which is based on an evaluation of the
maximal size of approximately monochromatic rectangles, minimized over all
distributions on the inputs. While it is known that the 0-error version of this
bound is polynomially tight for deterministic communication, nothing in this
direction is known for constant error and randomized communication complexity.
We first study a one-sided version of this bound and obtain that its value lies
between the MA- and AM-complexities of the considered function. Hence the lower
bound actually works for a (communication complexity) class between MA cap
co-MA and AM cap co-AM. We also show that the MA-complexity of the disjointness
problem is Omega(sqrt(n)). Following this we consider the conjecture that the
lower bound method is polynomially tight for randomized communication
complexity. First we disprove a distributional version of this conjecture. Then
we give a combinatorial characterization of the value of the lower bound
method, in which the optimization over all distributions is absent. This
characterization is done by what we call a uniform threshold cover. We also
study relaxations of this notion, namely approximate majority covers and
majority covers, and compare these three notions in power, exhibiting
exponential separations. Each of these covers captures a lower bound method
previously used for randomized communication complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208007</id><created>2002-08-06</created><authors><author><keyname>Kulesza</keyname><forenames>Kamil</forenames></author><author><keyname>Kotulski</keyname><forenames>Zbigniew</forenames></author></authors><title>On the graph coloring check-digit scheme with applications to verifiable
  secret sharing</title><categories>cs.CR cs.DM math.CO</categories><acm-class>D.4.6; E.4</acm-class><abstract>  In the paper we apply graph vertex coloring for verification of secret
shares. We start from showing how to convert any graph into the number and vice
versa. Next, theoretical result concerning properties of n-colorable graphs is
stated and proven. From this result we derive graph coloring check-digit
scheme. Feasibility of proposed scheme increases with the size of the number,
which digits are checked and overall probability of errors. The check-digit
scheme is used to build shares verification method that does not require
cooperation of the third party. It allows implementing verification structure
different from the access structure. It does not depend on particular secret
sharing method. It can be used as long as the secret shares can be represented
by numbers or graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208008</id><created>2002-08-06</created><authors><author><keyname>Bistarelli</keyname><forenames>S.</forenames><affiliation>Istituto di Informatica e Telematica, C.N.R., Pisa, Italy</affiliation></author><author><keyname>Montanari</keyname><forenames>U.</forenames><affiliation>Dipartimento di Informatica, Universita di Pisa, Italy</affiliation></author><author><keyname>Rossi</keyname><forenames>F.</forenames><affiliation>Dipartimento di Matematica Pura ed Applicata, Universita di Padova, Italy</affiliation></author></authors><title>Soft Concurrent Constraint Programming</title><categories>cs.PL cs.AI</categories><comments>25 pages, 4 figures, submitted to the ACM Transactions on
  Computational Logic (TOCL), zipped files</comments><acm-class>D.1.3; D.3.1; D.3.2; D.3.3; F.3.2</acm-class><abstract>  Soft constraints extend classical constraints to represent multiple
consistency levels, and thus provide a way to express preferences, fuzziness,
and uncertainty. While there are many soft constraint solving formalisms, even
distributed ones, by now there seems to be no concurrent programming framework
where soft constraints can be handled. In this paper we show how the classical
concurrent constraint (cc) programming framework can work with soft
constraints, and we also propose an extension of cc languages which can use
soft constraints to prune and direct the search for a solution. We believe that
this new programming paradigm, called soft cc (scc), can be also very useful in
many web-related scenarios. In fact, the language level allows web agents to
express their interaction and negotiation protocols, and also to post their
requests in terms of preferences, and the underlying soft constraint solver can
find an agreement among the agents even if their requests are incompatible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208009</id><created>2002-08-07</created><authors><author><keyname>Leuschel</keyname><forenames>Michael</forenames></author><author><keyname>Joergensen</keyname><forenames>Jesper</forenames></author><author><keyname>Vanhoof</keyname><forenames>Wim</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Offline Specialisation in Prolog Using a Hand-Written Compiler Generator</title><categories>cs.PL cs.AI</categories><comments>52 pages, to appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>D.1.6; D.1.2; I.2.2; F.4.1; I.2.3</acm-class><abstract>  The so called ``cogen approach'' to program specialisation, writing a
compiler generator instead of a specialiser, has been used with considerable
success in partial evaluation of both functional and imperative languages. This
paper demonstrates that the cogen approach is also applicable to the
specialisation of logic programs (also called partial deduction) and leads to
effective specialisers. Moreover, using good binding-time annotations, the
speed-ups of the specialised programs are comparable to the speed-ups obtained
with online specialisers. The paper first develops a generic approach to
offline partial deduction and then a specific offline partial deduction method,
leading to the offline system LIX for pure logic programs. While this is a
usable specialiser by itself, it is used to develop the cogen system LOGEN.
Given a program, a specification of what inputs will be static, and an
annotation specifying which calls should be unfolded, LOGEN generates a
specialised specialiser for the program at hand. Running this specialiser with
particular values for the static inputs results in the specialised program.
While this requires two steps instead of one, the efficiency of the
specialisation process is improved in situations where the same program is
specialised multiple times. The paper also presents and evaluates an automatic
binding-time analysis that is able to derive the annotations. While the derived
annotations are still suboptimal compared to hand-crafted ones, they enable
non-expert users to use the LOGEN system in a fully automated way. Finally,
LOGEN is extended so as to directly support a large part of Prolog's
declarative and non-declarative features and so as to be able to perform so
called mixline specialisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208010</id><created>2002-08-07</created><authors><author><keyname>Barclay</keyname><forenames>Tom</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Strand</keyname><forenames>Eric</forenames></author><author><keyname>Ekblad</keyname><forenames>Steve</forenames></author><author><keyname>Richter</keyname><forenames>Jeffrey</forenames></author></authors><title>TerraService.NET: An Introduction to Web Services</title><categories>cs.DL cs.DB</categories><comments>original at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-53</comments><report-no>MSR-TR-2002-53</report-no><acm-class>H.2; H.3; H.4;H.5</acm-class><abstract>  This article explores the design and construction of a geo-spatial Internet
web service application from the host web site perspective and from the
perspective of an application using the web service. The TerraService.NET web
service was added to the popular TerraServer database and web site with no
major structural changes to the database. The article discusses web service
design, implementation, and deployment concepts and design guidelines. Web
services enable applications that aggregate and interact with information and
resources from Internet-scale distributed servers. The article presents the
design of two USDA applications that interoperate with database and web service
resources in Fort Collins Colorado and the TerraService web service located in
Tukwila Washington.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208011</id><created>2002-08-07</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Chong</keyname><forenames>Wyman</forenames></author><author><keyname>Barclay</keyname><forenames>Tom</forenames></author><author><keyname>Szalay</keyname><forenames>Alex</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>TeraScale SneakerNet: Using Inexpensive Disks for Backup, Archiving, and
  Data Exchange</title><categories>cs.NI cs.DC</categories><comments>original at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-54</comments><report-no>MSR-TR-2002-54</report-no><acm-class>C.2.0;C.2.4; C.4.4;H.3;K.6</acm-class><abstract>  Large datasets are most economically trnsmitted via parcel post given the
current economics of wide-area networking. This article describes how the Sloan
Digital Sky Survey ships terabyte scale datasets both within the US and to
Europe and Asia. We 3GT storage bricks (Ghz processor, GB ram, GbpsEthernet, TB
disk) for about 2k$ each. These bricks act as database servers on the LAN. They
are loaded at one site and read at the second site. The paper describes the
bricks, their economics, and some software issues that they raise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208012</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208012</id><created>2002-08-07</created><authors><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Thakar</keyname><forenames>Ani R.</forenames></author><author><keyname>Stoughton</keyname><forenames>Christopher</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>Online Scientific Data Curation, Publication, and Archiving</title><categories>cs.DL</categories><comments>original at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-74</comments><report-no>MSR-TR-2002-74</report-no><acm-class>H.3.7;I.7.4;J.2;J.3;J.7</acm-class><doi>10.1117/12.461524</doi><abstract>  Science projects are data publishers. The scale and complexity of current and
future science data changes the nature of the publication process. Publication
is becoming a major project component. At a minimum, a project must preserve
the ephemeral data it gathers. Derived data can be reconstructed from metadata,
but metadata is ephemeral. Longer term, a project should expect some archive to
preserve the data. We observe that pub-lished scientific data needs to be
available forever ? this gives rise to the data pyramid of versions and to data
inflation where the derived data volumes explode. As an example, this article
describes the Sloan Digital Sky Survey (SDSS) strategies for data publication,
data access, curation, and preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208013</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208013</id><created>2002-08-07</created><authors><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>vandenBerg</keyname><forenames>Jan</forenames></author></authors><title>Petabyte Scale Data Mining: Dream or Reality?</title><categories>cs.DB cs.CE</categories><comments>originals at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-84</comments><report-no>MSR-TR-2002-84</report-no><acm-class>H.2.8;J.2</acm-class><journal-ref>SIPE Astronmy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</journal-ref><doi>10.1117/12.461427</doi><abstract>  Science is becoming very data intensive1. Today's astronomy datasets with
tens of millions of galaxies already present substantial challenges for data
mining. In less than 10 years the catalogs are expected to grow to billions of
objects, and image archives will reach Petabytes. Imagine having a 100GB
database in 1996, when disk scanning speeds were 30MB/s, and database tools
were immature. Such a task today is trivial, almost manageable with a laptop.
We think that the issue of a PB database will be very similar in six years. In
this paper we scale our current experiments in data archiving and analysis on
the Sloan Digital Sky Survey2,3 data six years into the future. We analyze
these projections and look at the requirements of performing data mining on
such data sets. We conclude that the task scales rather well: we could do the
job today, although it would be expensive. There do not seem to be any
show-stoppers that would prevent us from storing and using a Petabyte dataset
six years from today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208014</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208014</id><created>2002-08-07</created><authors><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Budavari</keyname><forenames>Tamas</forenames></author><author><keyname>Malika</keyname><forenames>Tanu</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Thakara</keyname><forenames>Ani</forenames></author></authors><title>Web Services for the Virtual Observatory</title><categories>cs.DC cs.DL</categories><comments>original documents at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-85</comments><report-no>MSR-TR-2002-85</report-no><acm-class>C.23.1;C.2.4;D.2.12;D.4.7;H.2;H.3;H.4;J.2;J.3</acm-class><journal-ref>SIPE Astronomy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</journal-ref><doi>10.1117/12.463947</doi><abstract>  Web Services form a new, emerging paradigm to handle distributed access to
resources over the Internet. There are platform independent standards (SOAP,
WSDL), which make the developers? task considerably easier. This article
discusses how web services could be used in the context of the Virtual
Observatory. We envisage a multi-layer architecture, with interoperating
services. A well-designed lower layer consisting of simple, standard services
implemented by most data providers will go a long way towards establishing a
modular architecture. More complex applications can be built upon this core
layer. We present two prototype applications, the SdssCutout and the SkyQuery
as examples of this layered architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208015</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208015</id><created>2002-08-07</created><authors><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author><author><keyname>Budavari</keyname><forenames>Tamas</forenames></author><author><keyname>Connolly</keyname><forenames>Andrew</forenames></author><author><keyname>Gray</keyname><forenames>Jim</forenames></author><author><keyname>Matsubara</keyname><forenames>Takahiko</forenames></author><author><keyname>Pope</keyname><forenames>Adrian</forenames></author><author><keyname>Szapudi</keyname><forenames>Istvan</forenames></author></authors><title>Spatial Clustering of Galaxies in Large Datasets</title><categories>cs.DB cs.DS</categories><comments>original documents at
  http://research.microsoft.com/scripts/pubs/view.asp?TR_ID=MSR-TR-2002-86</comments><report-no>TR_ID=MSR-TR-2002-86</report-no><acm-class>G.3;H.2.8; J.2</acm-class><journal-ref>SIPE Astronomy Telescopes and Instruments, 22-28 August 2002,
  Waikoloa, Hawaii</journal-ref><doi>10.1117/12.476761</doi><abstract>  Datasets with tens of millions of galaxies present new challenges for the
analysis of spatial clustering. We have built a framework that integrates a
database of object catalogs, tools for creating masks of bad regions, and a
fast (NlogN) correlation code. This system has enabled unprecedented efficiency
in carrying out the analysis of galaxy clustering in the SDSS catalog. A
similar approach is used to compute the three-dimensional spatial clustering of
galaxies on very large scales. We describe our strategy to estimate the effect
of photometric errors using a database. We discuss our efforts as an early
example of data-intensive science. While it would have been possible to get
these results without the framework we describe, it will be infeasible to
perform these computations on the future huge datasets without using this
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208016</id><created>2002-08-08</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>A note on fractional derivative modeling of broadband
  frequency-dependent absorption: Model III</title><categories>cs.CE cs.CC</categories><report-no>Simula Research Laboratory Report, April 2002</report-no><acm-class>G1.2, G1.8</acm-class><abstract>  By far, the fractional derivative model is mainly related to the modelling of
complicated solid viscoelastic material. In this study, we try to build the
fractional derivative PDE model for broadband ultrasound propagation through
human tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208017</id><created>2002-08-08</created><authors><author><keyname>Moinard</keyname><forenames>Yves</forenames></author></authors><title>Linking Makinson and Kraus-Lehmann-Magidor preferential entailments</title><categories>cs.AI</categories><comments>Proceedings of the 9th Int. Workshop on Non-Monotonic Reasoning
  (NMR'2002), Toulouse, France, April 19-21, 2002. Also, paper with the same
  Title at ECAI 2002 (15th European Conf. on A.I.)</comments><acm-class>I.2.3; F.4.1</acm-class><abstract>  About ten years ago, various notions of preferential entailment have been
introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM),
one of the main competitor being a more general version defined by Makinson
(MAK). These two versions have already been compared, but it is time to revisit
these comparisons. Here are our three main results: (1) These two notions are
equivalent, provided that we restrict our attention, as done in KLM, to the
cases where the entailment respects logical equivalence (on the left and on the
right). (2) A serious simplification of the description of the fundamental
cases in which MAK is equivalent to KLM, including a natural passage in both
ways. (3) The two previous results are given for preferential entailments more
general than considered in some of the original texts, but they apply also to
the original definitions and, for this particular case also, the models can be
simplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208018</identifier>
 <datestamp>2009-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208018</id><created>2002-08-12</created><updated>2002-09-11</updated><authors><author><keyname>Sauerbier</keyname><forenames>C.</forenames></author></authors><title>Does P = NP?</title><categories>cs.CC</categories><comments>withdrawn. It was a rediculously stupid notion</comments><acm-class>F.2.2;F.1.1</acm-class><abstract>  This paper considers the question of P = NP in context of the polynomial time
SAT algorithm. It posits proposition dependent on existence of conjectured
problem that even where the algorithm is shown to solve SAT in polynomial time
it remains theoretically possible for there to yet exist a
non-deterministically polynomial (NP) problem for which the algorithm does not
provide a polynomial (P) time solution. The paper leaves open as subject of
continuing research the question of existence of instance of conjectured
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208019</id><created>2002-08-12</created><authors><author><keyname>Birukou</keyname><forenames>Mikalai</forenames></author></authors><title>Knowledge Representation</title><categories>cs.AI</categories><acm-class>I.2.0</acm-class><abstract>  This work analyses main features that should be present in knowledge
representation. It suggests a model for representation and a way to implement
this model in software. Representation takes care of both low-level sensor
information and high-level concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208020</id><created>2002-08-12</created><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Isahara</keyname><forenames>Hitoshi</forenames></author></authors><title>Using the DIFF Command for Natural Language Processing</title><categories>cs.CL</categories><comments>10 pages. Computation and Language. This paper is the rough English
  translation of our Japanese papar</comments><acm-class>H.3.3; I.2.7</acm-class><abstract>  Diff is a software program that detects differences between two data sets and
is useful in natural language processing. This paper shows several examples of
the application of diff. They include the detection of differences between two
different datasets, extraction of rewriting rules, merging of two different
datasets, and the optimal matching of two different data sets. Since diff comes
with any standard UNIX system, it is readily available and very easy to use.
Our studies showed that diff is a practical tool for research into natural
language processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208021</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208021</id><created>2002-08-14</created><authors><author><keyname>Kohring</keyname><forenames>G. A.</forenames></author></authors><title>Implicit Simulations using Messaging Protocols</title><categories>cs.DC</categories><comments>14 pages, 3 figures</comments><acm-class>D.1.3</acm-class><journal-ref>Int. J. Mod. Phys. C: Computers and Physics, vol. 14 , pp. 203-214
  (2003).</journal-ref><doi>10.1142/S012918310300436X</doi><abstract>  A novel algorithm for performing parallel, distributed computer simulations
on the Internet using IP control messages is introduced. The algorithm employs
carefully constructed ICMP packets which enable the required computations to be
completed as part of the standard IP communication protocol. After providing a
detailed description of the algorithm, experimental applications in the areas
of stochastic neural networks and deterministic cellular automata are
discussed. As an example of the algorithms potential power, a simulation of a
deterministic cellular automaton involving 10^5 Internet connected devices was
performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208022</id><created>2002-08-14</created><authors><author><keyname>Kovalerchuk</keyname><forenames>B.</forenames></author><author><keyname>Vityaev</keyname><forenames>E.</forenames></author><author><keyname>Yusupov</keyname><forenames>H.</forenames></author></authors><title>Symbolic Methodology in Numeric Data Mining: Relational Techniques for
  Financial Applications</title><categories>cs.CE</categories><comments>20 pages, 1 figure, 16 tables</comments><acm-class>I.2.6</acm-class><abstract>  Currently statistical and artificial neural network methods dominate in
financial data mining. Alternative relational (symbolic) data mining methods
have shown their effectiveness in robotics, drug design and other applications.
Traditionally symbolic methods prevail in the areas with significant
non-numeric (symbolic) knowledge, such as relative location in robot
navigation. At first glance, stock market forecast looks as a pure numeric area
irrelevant to symbolic methods. One of our major goals is to show that
financial time series can benefit significantly from relational data mining
based on symbolic methods. The paper overviews relational data mining
methodology and develops this techniques for financial data mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208023</id><created>2002-08-16</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author><author><keyname>Estrin</keyname><forenames>Deborah</forenames></author></authors><title>The STRESS Method for Boundary-point Performance Analysis of End-to-end
  Multicast Timer-Suppression Mechanisms</title><categories>cs.NI cs.GT</categories><comments>24 pages, 10 figures, IEEE/ACM Transactions on Networking (ToN) [To
  appear]</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  Evaluation of Internet protocols usually uses random scenarios or scenarios
based on designers' intuition. Such approach may be useful for average-case
analysis but does not cover boundary-point (worst or best-case) scenarios. To
synthesize boundary-point scenarios a more systematic approach is needed.In
this paper, we present a method for automatic synthesis of worst and best case
scenarios for protocol boundary-point evaluation.
  Our method uses a fault-oriented test generation (FOTG) algorithm for
searching the protocol and system state space to synthesize these scenarios.
The algorithm is based on a global finite state machine (FSM) model. We extend
the algorithm with timing semantics to handle end-to-end delays and address
performance criteria. We introduce the notion of a virtual LAN to represent
delays of the underlying multicast distribution tree. The algorithms used in
our method utilize implicit backward search using branch and bound techniques
and start from given target events. This aims to reduce the search complexity
drastically. As a case study, we use our method to evaluate variants of the
timer suppression mechanism, used in various multicast protocols, with respect
to two performance criteria: overhead of response messages and response time.
Simulation results for reliable multicast protocols show that our method
provides a scalable way for synthesizing worst-case scenarios automatically.
Results obtained using stress scenarios differ dramatically from those obtained
through average-case analyses. We hope for our method to serve as a model for
applying systematic scenario generation to other multicast protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208024</id><created>2002-08-16</created><authors><author><keyname>Garg</keyname><forenames>Saurabh</forenames></author><author><keyname>Pamu</keyname><forenames>Priyatham</forenames></author><author><keyname>Nahata</keyname><forenames>Nitin</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Contact-Based Architecture for Resource Discovery (CARD) in Large Scale
  MANets</title><categories>cs.NI</categories><comments>10 pages, 15 figures</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  In this paper we propose a novel architecture, CARD, for resource discovery
in large scale Mobile Ad hoc Networks (MANets) which, may scale up to thousands
of nodes and may span wide geographical regions. Unlike previously proposed
schemes, our architecture avoids expensive mechanisms such as global flooding
as well as complex coordination between nodes to form a hierarchy. CARD is also
independent of any external source of information such as GPS. In our
architecture nodes within a limited number of hops from each node form the
neighborhood of that node. Resources within the neighborhood can be readily
accessed with the help of a proactive scheme within the neighborhood. For
accessing resources beyond the neighborhood, each node also maintains a few
distant nodes called contacts. Contacts help in creating a small world in the
network and provide an efficient way to query for resources beyond the
neighborhood. As the number of contacts of a node increases, the network view
(reachability) of the node increases. Paths to contacts are validated
periodically to adapt to mobility. We present mechanisms for contact selection
and maintenance that attempt to increase reachability while minimizing
overhead. Our simulation results show a clear trade-off between increase in
reachability on one hand, and contact selection and maintenance overhead on the
other. Our results suggest that CARD can be configured to provide a desirable
reachability distribution for different network sizes. Comparisons with other
schemes for resource discovery, such as flooding and bordercasting, show our
architecture to be much more efficient and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208025</id><created>2002-08-16</created><authors><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author><author><keyname>Jaseemuddin</keyname><forenames>Muhammad</forenames></author><author><keyname>Bhaskara</keyname><forenames>Ganesha</forenames></author></authors><title>Efficient Micro-Mobility using Intra-domain Multicast-based Mechanisms
  (M&amp;M)</title><categories>cs.NI</categories><comments>12 pages, 11 figures</comments><acm-class>C.2.1; C.2.2</acm-class><abstract>  One of the most important metrics in the design of IP mobility protocols is
the handover performance. The current Mobile IP (MIP) standard has been shown
to exhibit poor handover performance. Most other work attempts to modify MIP to
slightly improve its efficiency, while others propose complex techniques to
replace MIP. Rather than taking these approaches, we instead propose a new
architecture for providing efficient and smooth handover, while being able to
co-exist and inter-operate with other technologies. Specifically, we propose an
intra-domain multicast-based mobility architecture, where a visiting mobile is
assigned a multicast address to use while moving within a domain. Efficient
handover is achieved using standard multicast join/prune mechanisms. Two
approaches are proposed and contrasted. The first introduces the concept
proxy-based mobility, while the other uses algorithmic mapping to obtain the
multicast address of visiting mobiles. We show that the algorithmic mapping
approach has several advantages over the proxy approach, and provide mechanisms
to support it. Network simulation (using NS-2) is used to evaluate our scheme
and compare it to other routing-based micro-mobility schemes - CIP and HAWAII.
The proactive handover results show that both M&amp;M and CIP shows low handoff
delay and packet reordering depth as compared to HAWAII. The reason for M&amp;M's
comparable performance with CIP is that both use bi-cast in proactive handover.
The M&amp;M, however, handles multiple border routers in a domain, where CIP fails.
We also provide a handover algorithm leveraging the proactive path setup
capability of M&amp;M, which is expected to outperform CIP in case of reactive
handover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208026</id><created>2002-08-18</created><authors><author><keyname>Sauerbier</keyname><forenames>Charles</forenames></author></authors><title>Mathematical basis for polySAT implication operator</title><categories>cs.CC cs.LO</categories><comments>7 pages, 7 figures. Keywords: algorithms, complexity, computation
  theory, satisfiability, group theory, field theory, set theory</comments><report-no>S3E-2002-03</report-no><acm-class>F.2.2;F.1.1</acm-class><abstract>  The mathematical basis motivating the &quot;implication operator&quot; of the polySAT
algorithm and its function is examined. Such is not undertaken with onerous
rigor of symbolic mathematics; a more intuitive visual appeal being employed to
present some of the mathematical premises underlying function of the
implication operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208027</id><created>2002-08-19</created><authors><author><keyname>Steinke</keyname><forenames>Robert C.</forenames></author><author><keyname>Nutt</keyname><forenames>Gary J.</forenames></author></authors><title>A Unified Theory of Shared Memory Consistency</title><categories>cs.DC</categories><acm-class>D.1.3;F.1.2</acm-class><abstract>  Memory consistency models have been developed to specify what values may be
returned by a read given that, in a distributed system, memory operations may
only be partially ordered. Before this work, consistency models were defined
independently. Each model followed a set of rules which was separate from the
rules of every other model. In our work we have defined a set of four
consistency properties. Any subset of the four properties yields a set of rules
which constitute a consistency model. Every consistency model previously
described in the literature can be defined based on our four properties.
Therefore, we present these properties as a unfied theory of shared memory
consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208028</id><created>2002-08-19</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>A logical reconstruction of SPKI</title><categories>cs.CR cs.LO</categories><comments>This is an updated version of a paper that appears in the Proceedings
  of the 14th IEEE Computer Security Foundations Workshop. It will appear in a
  special issue of the Journal of Computer Security devoted to papers from that
  conference</comments><acm-class>D.4.6; F.4.1</acm-class><abstract>  SPKI/SDSI is a proposed public key infrastructure standard that incorporates
the SDSI public key infrastructure. SDSI's key innovation was the use of local
names. We previously introduced a Logic of Local Name Containment that has a
clear semantics and was shown to completely characterize SDSI name resolution.
Here we show how our earlier approach can be extended to deal with a number of
key features of SPKI, including revocation, expiry dates, and tuple reduction.
We show that these extensions add relatively little complexity to the logic. In
particular, we do not need a nonmonotonic logic to capture revocation. We then
use our semantics to examine SPKI's tuple reduction rules. Our analysis
highlights places where SPKI's informal description of tuple reduction is
somewhat vague, and shows that extra reduction rules are necessary in order to
capture general information about binding and authorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208029</id><created>2002-08-20</created><authors><author><keyname>Van Roy</keyname><forenames>Peter</forenames></author><author><keyname>Brand</keyname><forenames>Per</forenames></author><author><keyname>Duchier</keyname><forenames>Denys</forenames></author><author><keyname>Haridi</keyname><forenames>Seif</forenames></author><author><keyname>Henz</keyname><forenames>Martin</forenames></author><author><keyname>Schulte</keyname><forenames>Christian</forenames></author></authors><title>Logic programming in the context of multiparadigm programming: the Oz
  experience</title><categories>cs.PL</categories><comments>48 pages, to appear in the journal &quot;Theory and Practice of Logic
  Programming&quot;</comments><acm-class>D.1.6; D.3.2; D.3.3; F.3.3</acm-class><abstract>  Oz is a multiparadigm language that supports logic programming as one of its
major paradigms. A multiparadigm language is designed to support different
programming paradigms (logic, functional, constraint, object-oriented,
sequential, concurrent, etc.) with equal ease. This article has two goals: to
give a tutorial of logic programming in Oz and to show how logic programming
fits naturally into the wider context of multiparadigm programming. Our
experience shows that there are two classes of problems, which we call
algorithmic and search problems, for which logic programming can help formulate
practical solutions. Algorithmic problems have known efficient algorithms.
Search problems do not have known efficient algorithms but can be solved with
search. The Oz support for logic programming targets these two problem classes
specifically, using the concepts needed for each. This is in contrast to the
Prolog approach, which targets both classes with one set of concepts, which
results in less than optimal support for each class. To explain the essential
difference between algorithmic and search programs, we define the Oz execution
model. This model subsumes both concurrent logic programming
(committed-choice-style) and search-based logic programming (Prolog-style).
Instead of Horn clause syntax, Oz has a simple, fully compositional,
higher-order syntax that accommodates the abilities of the language. We
conclude with lessons learned from this work, a brief history of Oz, and many
entry points into the Oz literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208030</id><created>2002-08-20</created><authors><author><keyname>Chen</keyname><forenames>W</forenames></author></authors><title>A direct time-domain FEM modeling of broadband frequency-dependent
  absorption with the presence of matrix fractional power: Model I</title><categories>cs.CE cs.CG</categories><report-no>Simula Research Laboratory Report, April 2002</report-no><acm-class>G1.8, G1.9</acm-class><abstract>  The frequency-dependent attenuation of broadband acoustics is often
confronted in many different areas. However, the related time domain simulation
is rarely found in literature due to enormous technical difficulty. The
currently popular relaxation models with the presence of convolution operation
require some material parameters which are not readily available. In this
study, three reports are contributed to address broadband ultrasound
frequency-dependent absorptions using the readily available empirical
parameters. This report is the first in series concerned with developing a
direct time domain FEM formulation. The next two reports are about the
frequency decomposition model and the fractional derivative model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208031</id><created>2002-08-20</created><authors><author><keyname>Andreica</keyname><forenames>Alina</forenames></author></authors><title>Parameterized Type Definitions in Mathematica: Methods and Advantages</title><categories>cs.SC</categories><comments>14 pages</comments><acm-class>I.1.4, I.1.2</acm-class><abstract>  The theme of symbolic computation in algebraic categories has become of
utmost importance in the last decade since it enables the automatic modeling of
modern algebra theories. On this theoretical background, the present paper
reveals the utility of the parameterized categorical approach by deriving a
multivariate polynomial category (over various coefficient domains), which is
used by our Mathematica implementation of Buchberger's algorithms for
determining the Groebner basis. These implementations are designed according to
domain and category parameterization principles underlining their advantages:
operation protection, inheritance, generality, easy extendibility. In
particular, such an extension of Mathematica, a widely used symbolic
computation system, with a new type system has a certain practical importance.
The approach we propose for Mathematica is inspired from D. Gruntz and M.
Monagan's work in Gauss, for Maple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208032</id><created>2002-08-20</created><updated>2002-11-19</updated><authors><author><keyname>Apt</keyname><forenames>K. R.</forenames></author><author><keyname>Vermeulen</keyname><forenames>C. F. M.</forenames></author></authors><title>First-order Logic as a Constraint Programming Language</title><categories>cs.LO</categories><comments>17 pages. v2: improved version corrected reference to Turing (instead
  of Tarski)</comments><acm-class>F3.2, D3.1</acm-class><journal-ref>&quot;Logic for Programming, Artificial Intelligence and Reasoning&quot;,
  Proceedings of the 9th International Conference LPAR2002, Tbilisi, Georgia;
  Editors: A. Voronkov and M. Baaz; Springer Verlag LNAI2514; pages 19-35;
  October 2002</journal-ref><abstract>  We provide a denotational semantics for first-order logic that captures the
two-level view of the computation process typical for constraint programming.
At one level we have the usual program execution. At the other level an
automatic maintenance of the constraint store takes place. We prove that the
resulting semantics is sound with respect to the truth definition. By
instantiating it by specific forms of constraint management policies we obtain
several sound evaluation policies of first-order formulas. This semantics can
also be used a basis for sound implementation of constraint maintenance in
presence of block declarations and conditionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208033</id><created>2002-08-20</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>Complete Axiomatizations for Reasoning About Knowledge and Time</title><categories>cs.LO cs.AI</categories><comments>To appear, SIAM Journal on Computing</comments><acm-class>F.4.1, I.2.4</acm-class><abstract>  Sound and complete axiomatizations are provided for a number of different
logics involving modalities for knowledge and time. These logics arise from
different choices for various parameters. All the logics considered involve the
discrete time linear temporal logic operators `next' and `until' and an
operator for the knowledge of each of a number of agents. Both the single agent
and multiple agent cases are studied: in some instances of the latter there is
also an operator for the common knowledge of the group of all agents. Four
different semantic properties of agents are considered: whether they have a
unique initial state, whether they operate synchronously, whether they have
perfect recall, and whether they learn. The property of no learning is
essentially dual to perfect recall. Not all settings of these parameters lead
to recursively axiomatizable logics, but sound and complete axiomatizations are
presented for all the ones that do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208034</id><created>2002-08-20</created><updated>2005-11-19</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causes and Explanations: A Structural-Model Approach. Part II:
  Explanations</title><categories>cs.AI</categories><comments>Part I of the paper (on causes) is also on the arxiv. The two papers
  originally were posted as one submission. The conference version of the paper
  appears in IJCAI '01. This paper will appear in the British Journal for
  Philosophy of Science</comments><acm-class>I.2.4</acm-class><abstract>  We propose new definitions of (causal) explanation, using structural
equations to model counterfactuals. The definition is based on the notion of
actual cause, as defined and motivated in a companion paper. Essentially, an
explanation is a fact that is not known for certain but, if found to be true,
would constitute an actual cause of the fact to be explained, regardless of the
agent's initial uncertainty. We show that the definition handles well a number
of problematic examples from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208035</id><created>2002-08-21</created><authors><author><keyname>Popescu-Belis</keyname><forenames>Andrei</forenames></author><author><keyname>Robba</keyname><forenames>Isabelle</forenames></author></authors><title>Evaluation of Coreference Rules on Complex Narrative Texts</title><categories>cs.CL</categories><comments>9 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of DAARC2 (Discourse Anaphora and Anaphor Resolution
  Colloquium), Lancaster, UK, 1998, p.178-185</journal-ref><abstract>  This article studies the problem of assessing relevance to each of the rules
of a reference resolution system. The reference solver described here stems
from a formal model of reference and is integrated in a reference processing
workbench. Evaluation of the reference resolution is essential, as it enables
differential evaluation of individual rules. Numerical values of these measures
are given, and discussed, for simple selection rules and other processing
rules; such measures are then studied for numerical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208036</id><created>2002-08-21</created><authors><author><keyname>Popescu-Belis</keyname><forenames>Andrei</forenames></author><author><keyname>Robba</keyname><forenames>Isabelle</forenames></author></authors><title>Three New Methods for Evaluating Reference Resolution</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the LREC'98 Workshop on Linguistic Coreference,
  Madrid, Spain, 1998</journal-ref><abstract>  Reference resolution on extended texts (several thousand references) cannot
be evaluated manually. An evaluation algorithm has been proposed for the MUC
tests, using equivalence classes for the coreference relation. However, we show
here that this algorithm is too indulgent, yielding good scores even for poor
resolution strategies. We elaborate on the same formalism to propose two new
evaluation algorithms, comparing them first with the MUC algorithm and giving
then results on a variety of examples. A third algorithm using only
distributional comparison of equivalence classes is finally described; it
assesses the relative importance of the recall vs. precision errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208037</id><created>2002-08-21</created><authors><author><keyname>Popescu-Belis</keyname><forenames>Andrei</forenames></author><author><keyname>Robba</keyname><forenames>Isabelle</forenames></author></authors><title>Cooperation between Pronoun and Reference Resolution for Unrestricted
  Texts</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of the ACL'97 Workshop on Operational Factors in
  Practical, Robust Anaphora Resolution for Unrestricted Texts, Madrid, Spain,
  1998, p.94-99</journal-ref><abstract>  Anaphora resolution is envisaged in this paper as part of the reference
resolution process. A general open architecture is proposed, which can be
particularized and configured in order to simulate some classic anaphora
resolution methods. With the aim of improving pronoun resolution, the system
takes advantage of elementary cues about characters of the text, which are
represented through a particular data structure. In its most robust
configuration, the system uses only a general lexicon, a local morpho-syntactic
parser and a dictionary of synonyms. A short comparative corpus analysis shows
that narrative texts are the most suitable for testing such a system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208038</id><created>2002-08-21</created><authors><author><keyname>Popescu-Belis</keyname><forenames>Andrei</forenames></author><author><keyname>Robba</keyname><forenames>Isabelle</forenames></author><author><keyname>Sabah</keyname><forenames>Gerard</forenames></author></authors><title>Reference Resolution Beyond Coreference: a Conceptual Frame and its
  Application</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Proceedings of COLING-ACL'98, Montreal, Canada, 1998, p.1046-1052</journal-ref><abstract>  A model for reference use in communication is proposed, from a
representationist point of view. Both the sender and the receiver of a message
handle representations of their common environment, including mental
representations of objects. Reference resolution by a computer is viewed as the
construction of object representations using referring expressions from the
discourse, whereas often only coreference links between such expressions are
looked for. Differences between these two approaches are discussed. The model
has been implemented with elementary rules, and tested on complex narrative
texts (hundreds to thousands of referring expressions). The results support the
mental representations paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208039</id><created>2002-08-23</created><authors><author><keyname>Anderson</keyname><forenames>Elizabeth</forenames></author><author><keyname>Atkinson</keyname><forenames>Robert</forenames></author><author><keyname>Buckley-Geer</keyname><forenames>Elizabeth</forenames></author><author><keyname>Crego</keyname><forenames>Cynthia</forenames></author><author><keyname>Giacchetti</keyname><forenames>Lisa</forenames></author><author><keyname>Hanson</keyname><forenames>Stephen</forenames></author><author><keyname>Ritchie</keyname><forenames>David</forenames></author><author><keyname>Slisz</keyname><forenames>Jean</forenames></author><author><keyname>Tompson</keyname><forenames>Sara</forenames></author><author><keyname>Wolbers</keyname><forenames>Stephen</forenames></author></authors><title>A Virtual Library of Technical Publications</title><categories>cs.DL</categories><comments>Presented at 6th International World Wide Web Conference, Santa
  Clara, CA, 7-12 Apr 1997 and at Inforum'97, Oak Ridge TN, 6-8 May 1997</comments><report-no>FERMILAB-TM-2004</report-no><acm-class>H.3.7</acm-class><abstract>  Through a collaborative effort, the Fermilab Information Resources Department
and Computing Division have created a &quot;virtual library&quot; of technical
publications that provides public access to electronic full-text documents.
This paper will discuss the vision, planning and milestones of the project, as
well as the hardware, software and interdepartmental cooperation components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208040</id><created>2002-08-25</created><authors><author><keyname>Verstak</keyname><forenames>Alex</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Bae</keyname><forenames>Kyung Kyoon</forenames></author><author><keyname>Tranter</keyname><forenames>William H.</forenames></author><author><keyname>Watson</keyname><forenames>Layne T.</forenames></author><author><keyname>He</keyname><forenames>Jian</forenames></author><author><keyname>Shaffer</keyname><forenames>Clifford A.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Using Hierarchical Data Mining to Characterize Performance of Wireless
  System Configurations</title><categories>cs.CE</categories><acm-class>I.6.4</acm-class><abstract>  This paper presents a statistical framework for assessing wireless systems
performance using hierarchical data mining techniques. We consider WCDMA
(wideband code division multiple access) systems with two-branch STTD (space
time transmit diversity) and 1/2 rate convolutional coding (forward error
correction codes). Monte Carlo simulation estimates the bit error probability
(BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A
performance database of simulation runs is collected over a targeted space of
system configurations. This database is then mined to obtain regions of the
configuration space that exhibit acceptable average performance. The shape of
the mined regions illustrates the joint influence of configuration parameters
on system performance. The role of data mining in this application is to
provide explainable and statistically valid design conclusions. The research
issue is to define statistically meaningful aggregation of data in a manner
that permits efficient and effective data mining algorithms. We achieve a good
compromise between these goals and help establish the applicability of data
mining for characterizing wireless systems performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208041</id><created>2002-08-26</created><authors><author><keyname>Desmedt</keyname><forenames>Yvo</forenames></author><author><keyname>Wang</keyname><forenames>Yongge</forenames></author></authors><title>Perfectly Secure Message Transmission Revisited</title><categories>cs.CR cs.CC</categories><acm-class>C.2.2; E.1; E.3; E.4; F.1; F.2.3; H.1.1; H.2.0</acm-class><abstract>  Achieving secure communications in networks has been one of the most
important problems in information technology. Dolev, Dwork, Waarts, and Yung
have studied secure message transmission in one-way or two-way channels. They
only consider the case when all channels are two-way or all channels are
one-way. Goldreich, Goldwasser, and Linial, Franklin and Yung, Franklin and
Wright, and Wang and Desmedt have studied secure communication and secure
computation in multi-recipient (multicast) models. In a ``multicast channel''
(such as Ethernet), one processor can send the same message--simultaneously and
privately--to a fixed subset of processors. In this paper, we shall study
necessary and sufficient conditions for achieving secure communications against
active adversaries in mixed one-way and two-way channels. We also discuss
multicast channels and neighbor network channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208042</id><created>2002-08-28</created><authors><author><keyname>de Boer</keyname><forenames>F. S.</forenames></author><author><keyname>Gabbrielli</keyname><forenames>M.</forenames></author><author><keyname>Meo</keyname><forenames>M. C.</forenames></author></authors><title>Proving correctness of Timed Concurrent Constraint Programs</title><categories>cs.LO cs.PL</categories><acm-class>F.3.1;D.3.1;D.3.2</acm-class><abstract>  A temporal logic is presented for reasoning about the correctness of timed
concurrent constraint programs. The logic is based on modalities which allow
one to specify what a process produces as a reaction to what its environment
inputs. These modalities provide an assumption/commitment style of
specification which allows a sound and complete compositional axiomatization of
the reactive behavior of timed concurrent constraint programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208043</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208043</id><created>2002-08-29</created><authors><author><keyname>Hitchcock</keyname><forenames>John M.</forenames></author></authors><title>Gales Suffice for Constructive Dimension</title><categories>cs.CC</categories><comments>4 pages</comments><acm-class>F.1.3</acm-class><abstract>  Supergales, generalizations of supermartingales, have been used by Lutz
(2002) to define the constructive dimensions of individual binary sequences.
Here it is shown that gales, the corresponding generalizations of martingales,
can be equivalently used to define constructive dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0208044</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0208044</id><created>2002-08-29</created><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames></author></authors><title>Gales and supergales are equivalent for defining constructive Hausdorff
  dimension</title><categories>cs.CC</categories><comments>7 pages, no figures</comments><acm-class>F.1.3</acm-class><abstract>  We show that for a wide range of probability measures, constructive gales are
interchangable with constructive supergales for defining constructive Hausdorff
dimension, thus generalizing a previous independent result of Hitchcock
(cs.CC/0208043) and partially answering an open question of Lutz
(cs.CC/0203017).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209001</id><created>2002-09-01</created><authors><author><keyname>Kim</keyname><forenames>Gene</forenames></author><author><keyname>Kim</keyname><forenames>MyungHo</forenames></author></authors><title>A Novel Statistical Diagnosis of Clinical Data</title><categories>cs.CE cs.CC</categories><comments>20 pages, 4 figures</comments><acm-class>I.1.2; H.1.1; I.5.0</acm-class><abstract>  In this paper, we present a diagnosis method of diseases from clinical data.
The data are routine test such as urine test, hematology, chemistries etc.
Though those tests have been done for people who check in medical institutes,
how each item of the data interacts each other and which combination of them
cause a disease are neither understood nor studied well. Here we attack the
practically important problem by putting the data into mathematical setup and
applying support vector machine. Finally we present simulation results for
fatty liver, gastritis etc and discuss about their implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209002</id><created>2002-09-02</created><authors><author><keyname>Vaillant</keyname><forenames>Pascal</forenames><affiliation>ENST, Paris</affiliation></author></authors><title>A Chart-Parsing Algorithm for Efficient Semantic Analysis</title><categories>cs.CL</categories><comments>7 pages, 1 figure, LaTeX 2e using COLACL and EPSF packages.
  Proceedings of the 19th International Conference on Computational Linguistics
  (COLING 2002), Taipei, Republic of China (Taiwan), 24 Aug. - 1 Sept. 2002</comments><acm-class>I.2.7; F.2.2</acm-class><journal-ref>COLING 2002, Proceedings of the main Conference; The Association
  for Computational Linguistics and Chinese Language Processing; vol. 2, p.
  1044-1050</journal-ref><abstract>  In some contexts, well-formed natural language cannot be expected as input to
information or communication systems. In these contexts, the use of
grammar-independent input (sequences of uninflected semantic units like e.g.
language-independent icons) can be an answer to the users' needs. A semantic
analysis can be performed, based on lexical semantic knowledge: it is
equivalent to a dependency analysis with no syntactic or morphological clues.
However, this requires that an intelligent system should be able to interpret
this input with reasonable accuracy and in reasonable time. Here we propose a
method allowing a purely semantic-based analysis of sequences of semantic
units. It uses an algorithm inspired by the idea of ``chart parsing'' known in
Natural Language Processing, which stores intermediate parsing results in order
to bring the calculation time down. In comparison with using declarative logic
programming - where the calculation time, left to a prolog engine, is
hyperexponential -, this method brings the calculation time down to a
polynomial time, where the order depends on the valency of the predicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209003</id><created>2002-09-03</created><authors><author><keyname>Pustejovsky</keyname><forenames>J.</forenames></author><author><keyname>Rumshisky</keyname><forenames>A.</forenames></author><author><keyname>Castano</keyname><forenames>J.</forenames></author></authors><title>Rerendering Semantic Ontologies: Automatic Extensions to UMLS through
  Corpus Analytics</title><categories>cs.CL</categories><comments>8 pages</comments><acm-class>I.2.7; J.3</acm-class><journal-ref>LREC 2002 Workshop on Ontologies and Lexical Knowledge Bases</journal-ref><abstract>  In this paper, we discuss the utility and deficiencies of existing ontology
resources for a number of language processing applications. We describe a
technique for increasing the semantic type coverage of a specific ontology, the
National Library of Medicine's UMLS, with the use of robust finite state
methods used in conjunction with large-scale corpus analytics of the domain
corpus. We call this technique &quot;semantic rerendering&quot; of the ontology. This
research has been done in the context of Medstract, a joint Brandeis-Tufts
effort aimed at developing tools for analyzing biomedical language (i.e.,
Medline), as well as creating targeted databases of bio-entities, biological
relations, and pathway data for biological researchers. Motivating the current
research is the need to have robust and reliable semantic typing of syntactic
elements in the Medline corpus, in order to improve the overall performance of
the information extraction applications mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209004</id><created>2002-09-02</created><authors><author><keyname>Mori</keyname><forenames>Tatsuya</forenames></author><author><keyname>Kawahara</keyname><forenames>Ryoichi</forenames></author><author><keyname>Naito</keyname><forenames>Shozo</forenames></author></authors><title>Analysis of Non-Gaussian Nature of Network Traffic and its Implication
  on Network Performance</title><categories>cs.NI</categories><comments>11 pages, 12 figures</comments><acm-class>C.2.5</acm-class><abstract>  We analyzed the non-Gaussian nature of network traffic using some Internet
traffic data. We found that (1) the non-Gaussian nature degrades network
performance, (2) it is caused by `greedy flows' that exist with non-negligible
probability, and (3) a large majority of `greedy flows' are TCP flows having
relatively small hop counts, which correspond to small round-trip times. We
conclude that in a network hat has greedy flows with non-negligible
probability, a traffic controlling scheme or bandwidth design that considers
non-Gaussian nature is essential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209005</id><created>2002-09-04</created><authors><author><keyname>Catalani</keyname><forenames>Mario</forenames></author></authors><title>Sampling from a couple of positively correlated binomial variables</title><categories>cs.DM</categories><report-no>DECON0902</report-no><acm-class>G.3</acm-class><abstract>  We know that the marginals in a multinomial distribution are binomial
variates exhibiting a negative correlation. But we can construct two linear
combinations of such marginals in such a way to obtain a positive correlation.
We discuss the restrictions that are to be imposed on the parameters of the
given marginals to accomplish such a result. Next we discuss the regression
function, showing that it is a linear function but not homoscedastic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209006</id><created>2002-09-03</created><updated>2004-07-27</updated><authors><author><keyname>Chow</keyname><forenames>Timothy Y.</forenames></author><author><keyname>Chudak</keyname><forenames>Fabian</forenames></author><author><keyname>Ffrench</keyname><forenames>Anthony M.</forenames></author></authors><title>Fast optical layer mesh protection using pre-cross-connected trails</title><categories>cs.NI</categories><comments>Article has appeared in IEEE/ACM Trans. Networking</comments><acm-class>C.2.1</acm-class><journal-ref>IEEE/ACM Trans. Networking 12 (3) 2004: 539-548</journal-ref><abstract>  Conventional optical networks are based on SONET rings, but since rings are
known to use bandwidth inefficiently, there has been much research into shared
mesh protection, which promises significant bandwidth savings. Unfortunately,
most shared mesh protection schemes cannot guarantee that failed traffic will
be restored within the 50 ms timeframe that SONET standards specify. A notable
exception is the p-cycle scheme of Grover and Stamatelakis. We argue, however,
that p-cycles have certain limitations, e.g., there is no easy way to adapt
p-cycles to a path-based protection scheme, and p-cycles seem more suited to
static traffic than to dynamic traffic. In this paper we show that the key to
fast restoration times is not a ring-like topology per se, but rather the
ability to pre-cross-connect protection paths. This leads to the concept of a
pre-cross-connected trail or PXT, which is a structure that is more flexible
than rings and that adapts readily to both path-based and link-based schemes
and to both static and dynamic traffic. The PXT protection scheme achieves fast
restoration speeds, and our simulations, which have been carefully chosen using
ideas from experimental design theory, show that the bandwidth efficiency of
the PXT protection scheme is comparable to that of conventional shared mesh
protection schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209007</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209007</id><created>2002-09-03</created><updated>2003-03-28</updated><authors><author><keyname>Buzer</keyname><forenames>Lilian</forenames></author></authors><title>A Survey and a New Competitive Method for the Planar min-# Problem</title><categories>cs.CG</categories><comments>preprint withdrawn</comments><acm-class>G.1.2; I.3.5; F.2.2</acm-class><abstract>  We survey most of the different types of approximation algorithms which
minimize the number of output vertices. We present their main qualities and
their inherent drawbacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209008</id><created>2002-09-04</created><authors><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Cate</keyname><forenames>Balder D. ten</forenames><affiliation>Universiteit van Amsterdam</affiliation></author></authors><title>The partition semantics of questions, syntactically</title><categories>cs.CL cs.AI cs.LO</categories><comments>14 pages</comments><acm-class>F.4.1; I.2.3; I.2.7</acm-class><journal-ref>Proceedings of the 2002 European Summer School in Logic, Language
  and Information student session, ed. Malvina Nissim</journal-ref><abstract>  Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically
attractive theory of the semantics of natural language questions, commonly
referred to as the partition theory. Two central notions in this theory are
entailment between questions and answerhood. For example, the question &quot;Who is
going to the party?&quot; entails the question &quot;Is John going to the party?&quot;, and
&quot;John is going to the party&quot; counts as an answer to both. Groenendijk and
Stokhof define these two notions in terms of partitions of a set of possible
worlds.
  We provide a syntactic characterization of entailment between questions and
answerhood . We show that answers are, in some sense, exactly those formulas
that are built up from instances of the question. This result lets us compare
the partition theory with other approaches to interrogation -- both linguistic
analyses, such as Hamblin's and Karttunen's semantics, and computational
systems, such as Prolog. Our comparison separates a notion of answerhood into
three aspects: equivalence (when two questions or answers are interchangeable),
atomic answers (what instances of a question count as answers), and compound
answers (how answers compose).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209009</id><created>2002-09-04</created><authors><author><keyname>Cate</keyname><forenames>Balder D. ten</forenames><affiliation>Universiteit van Amsterdam</affiliation></author><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames><affiliation>Harvard University</affiliation></author></authors><title>Question answering: from partitions to Prolog</title><categories>cs.CL cs.AI cs.LO</categories><comments>15 pages</comments><acm-class>F.4.1; I.2.3; I.2.7</acm-class><journal-ref>Proceedings of TABLEAUX 2002: Automated Reasoning with Analytic
  Tableaux and Related Methods, ed. Uwe Egly and Christian G. Fermueller,
  Lecture Notes in Artificial Intelligence 2381, 251-265; also in Proceedings
  of NLULP 2002, ed. Shuly Wintner</journal-ref><abstract>  We implement Groenendijk and Stokhof's partition semantics of questions in a
simple question answering algorithm. The algorithm is sound, complete, and
based on tableau theorem proving. The algorithm relies on a syntactic
characterization of answerhood: Any answer to a question is equivalent to some
formula built up only from instances of the question. We prove this
characterization by translating the logic of interrogation to classical
predicate logic and applying Craig's interpolation theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209010</id><created>2002-09-05</created><authors><author><keyname>Sang</keyname><forenames>Erik F. Tjong Kim</forenames></author></authors><title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named
  Entity Recognition</title><categories>cs.CL</categories><comments>4 pages</comments><acm-class>I.2.7</acm-class><journal-ref>Dan Roth and Antal van den Bosch (eds.), Proceedings of
  CoNLL-2002, Taipei, Taiwan, 2002, pp. 155-158</journal-ref><abstract>  We describe the CoNLL-2002 shared task: language-independent named entity
recognition. We give background information on the data sets and the evaluation
method, present a general overview of the systems that have taken part in the
task and discuss their performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209011</id><created>2002-09-05</created><authors><author><keyname>Haas</keyname><forenames>Zygmunt</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Li</keyname><forenames>Erran L.</forenames></author></authors><title>Gossip Based Ad-Hoc Routing</title><categories>cs.NI</categories><comments>10 pages</comments><acm-class>C.2.2</acm-class><journal-ref>IEEE INFOCOM, June 2002</journal-ref><abstract>  Many ad hoc routing protocols are based on some variant of flooding. Despite
various optimizations, many routing messages are propagated unnecessarily. We
propose a gossiping-based approach, where each node forwards a message with
some probability, to reduce the overhead of the routing protocols. Gossiping
exhibits bimodal behavior in sufficiently large networks: in some executions,
the gossip dies out quickly and hardly any node gets the message; in the
remaining executions, a substantial fraction of the nodes gets the message. The
fraction of executions in which most nodes get the message depends on the
gossiping probability and the topology of the network. In the networks we have
considered, using gossiping probability between 0.6 and 0.8 suffices to ensure
that almost every node gets the message in almost every execution. For large
networks, this simple gossiping protocol uses up to 35% fewer messages than
flooding, with improved performance. Gossiping can also be combined with
various optimizations of flooding to yield further benefits. Simulations show
that adding gossiping to AODV results in significant performance improvement,
even in networks as small as 150 nodes. We expect that the improvement should
be even more significant in larger networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209012</id><created>2002-09-05</created><authors><author><keyname>Li</keyname><forenames>Erran L.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Bahl</keyname><forenames>Paramvir</forenames></author><author><keyname>Wang</keyname><forenames>Yi-Min</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Analysis of a Cone-Based Distributed Topology Control Algorithm for
  Wireless Multi-hop Networks</title><categories>cs.NI</categories><comments>10 pages</comments><acm-class>C.2.1;C.2.2</acm-class><journal-ref>ACM PODC, 2001</journal-ref><abstract>  The topology of a wireless multi-hop network can be controlled by varying the
transmission power at each node. In this paper, we give a detailed analysis of
a cone-based distributed topology control algorithm. This algorithm, introduced
in [16], does not assume that nodes have GPS information available; rather it
depends only on directional information. Roughly speaking, the basic idea of
the algorithm is that a node $u$ transmits with the minimum power
$p_{u,\alpha}$ required to ensure that in every cone of degree $\alpha$ around
$u$, there is some node that $u$ can reach with power $p_{u,\alpha}$. We show
that taking $\alpha = 5\pi/6$ is a necessary and sufficient condition to
guarantee that network connectivity is preserved. More precisely, if there is a
path from $s$ to $t$ when every node communicates at maximum power, then, if
$\alpha &lt;= 5\pi/6$, there is still a path in the smallest symmetric graph
$G_\alpha$ containing all edges $(u,v)$ such that $u$ can communicate with $v$
using power $p_{u,\alpha}$. On the other hand, if $\alpha &gt; 5\pi/6$,
connectivity is not necessarily preserved. We also propose a set of
optimizations that further reduce power consumption and prove that they retain
network connectivity. Dynamic reconfiguration in the presence of failures and
mobility is also discussed. Simulation results are presented to demonstrate the
effectiveness of the algorithm and the optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209013</id><created>2002-09-05</created><authors><author><keyname>Li</keyname><forenames>Erran L.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Minimum-Energy Mobile Wireless Networks Revisited</title><categories>cs.NI</categories><comments>6 pages</comments><acm-class>C.2.1</acm-class><journal-ref>IEEE ICC, 2001</journal-ref><abstract>  We propose a protocol that, given a communication network, computes a
subnetwork such that, for every pair $(u,v)$ of nodes connected in the original
network, there is a minimum-energy path between $u$ and $v$ in the subnetwork
(where a minimum-energy path is one that allows messages to be transmitted with
a minimum use of energy). The network computed by our protocol is in general a
subnetwork of the one computed by the protocol given in [13]. Moreover, our
protocol is computationally simpler. We demonstrate the performance
improvements obtained by using the subnetwork computed by our protocol through
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209014</id><created>2002-09-06</created><authors><author><keyname>Aspnes</keyname><forenames>James</forenames></author></authors><title>Randomized protocols for asynchronous consensus</title><categories>cs.DS cs.DC</categories><comments>29 pages; survey paper written for PODC 20th anniversary issue of
  Distributed Computing</comments><acm-class>F.2.2; F.1.2</acm-class><abstract>  The famous Fischer, Lynch, and Paterson impossibility proof shows that it is
impossible to solve the consensus problem in a natural model of an asynchronous
distributed system if even a single process can fail. Since its publication,
two decades of work on fault-tolerant asynchronous consensus algorithms have
evaded this impossibility result by using extended models that provide (a)
randomization, (b) additional timing assumptions, (c) failure detectors, or (d)
stronger synchronization mechanisms than are available in the basic model.
Concentrating on the first of these approaches, we illustrate the history and
structure of randomized asynchronous consensus protocols by giving detailed
descriptions of several such protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209015</identifier>
 <datestamp>2009-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209015</id><created>2002-09-10</created><updated>2002-09-11</updated><authors><author><keyname>Sauerbier</keyname><forenames>C.</forenames></author></authors><title>Does NP not equal P?</title><categories>cs.CC</categories><comments>withdrawn. It was a rediculously absurd notion</comments><acm-class>F.2.2;F.1.1</acm-class><abstract>  Stephen Cook posited SAT is NP-Complete in 1971. If SAT is NP-Complete then,
as is generally accepted, any polynomial solution of it must also present a
polynomial solution of all NP decision problems. It is here argued, however,
that NP is not of necessity equivalent to P where it is shown that SAT is
contained in P. This due to a paradox, of nature addressed by both Godel and
Russell, in regards to the P-NP system in total.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209016</id><created>2002-09-10</created><authors><author><keyname>Albert</keyname><forenames>M. H.</forenames></author><author><keyname>Atkinson</keyname><forenames>M. D.</forenames></author></authors><title>Sorting with a forklift</title><categories>cs.DM cs.DS math.CO</categories><comments>24 pages, 2 figures</comments><acm-class>G.2.1</acm-class><abstract>  A fork stack is a generalised stack which allows pushes and pops of several
items at a time. We consider the problem of determining which input streams can
be sorted using a single forkstack, or dually, which permutations of a fixed
input stream can be produced using a single forkstack. An algorithm is given to
solve the sorting problem and the minimal unsortable sequences are found. The
results are extended to fork stacks where there are bounds on how many items
can be pushed and popped at one time. In this context we also establish how to
enumerate the collection of sortable sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209017</id><created>2002-09-15</created><authors><author><keyname>Tucci</keyname><forenames>Michele</forenames><affiliation>U. of Rome &quot;La Sapienza&quot;</affiliation></author></authors><title>Evolution and Gravitation: a Computer Simulation of a Non-Walrasian
  Equilibrium Model</title><categories>cs.CY</categories><comments>PDF, 33 pages</comments><acm-class>I.6.3, J.4</acm-class><abstract>  The paper contains a computer simulation concerning a basic non-Walrasian
equilibrium system, following the Edmond Malinvaud &quot;short side&quot; approach, as
far as the price adjustment is concerned, and the sequential Hicksian &quot;weeks&quot;
structure with regard of the temporal characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209018</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209018</id><created>2002-09-16</created><updated>2002-09-25</updated><authors><author><keyname>Golovkins</keyname><forenames>Marats</forenames></author><author><keyname>Kravtsev</keyname><forenames>Maksim</forenames></author></authors><title>Probabilistic Reversible Automata and Quantum Automata</title><categories>cs.CC cs.FL quant-ph</categories><comments>COCOON 2002, extended version of the paper, 22 pages</comments><acm-class>F.1.1; F.4.3</acm-class><journal-ref>Lecture Notes in Computer Science, 2002, Vol. 2387, pp. 574-583</journal-ref><abstract>  To study relationship between quantum finite automata and probabilistic
finite automata, we introduce a notion of probabilistic reversible automata
(PRA, or doubly stochastic automata). We find that there is a strong
relationship between different possible models of PRA and corresponding models
of quantum finite automata. We also propose a classification of reversible
finite 1-way automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209019</id><created>2002-09-16</created><authors><author><keyname>Eiter</keyname><forenames>T.</forenames></author><author><keyname>Fink</keyname><forenames>M.</forenames></author><author><keyname>Sabbatini</keyname><forenames>G.</forenames></author><author><keyname>Tompits</keyname><forenames>H.</forenames></author></authors><title>Reasoning about Evolving Nonmonotonic Knowledge Bases</title><categories>cs.AI</categories><comments>47 pages.A preliminary version appeared in: Proc. 8th International
  Conference on Logic for Programming, Artificial Intelligence and Reasoning
  (LPAR 2001), R. Nieuwenhuis and A. Voronkov (eds), pp. 407--421, LNCS 2250,
  Springer 2001</comments><report-no>INFSYS RR-1843-02-11</report-no><acm-class>I.2.3; I.2.4; F.4.1</acm-class><abstract>  Recently, several approaches to updating knowledge bases modeled as extended
logic programs have been introduced, ranging from basic methods to incorporate
(sequences of) sets of rules into a logic program, to more elaborate methods
which use an update policy for specifying how updates must be incorporated. In
this paper, we introduce a framework for reasoning about evolving knowledge
bases, which are represented as extended logic programs and maintained by an
update policy. We first describe a formal model which captures various update
approaches, and we define a logical language for expressing properties of
evolving knowledge bases. We then investigate semantical and computational
properties of our framework, where we focus on properties of knowledge states
with respect to the canonical reasoning task of whether a given formula holds
on a given evolving knowledge base. In particular, we present finitary
characterizations of the evolution for certain classes of framework instances,
which can be exploited for obtaining decidability results. In more detail, we
characterize the complexity of reasoning for some meaningful classes of
evolving knowledge bases, ranging from polynomial to double exponential space
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209020</id><created>2002-09-18</created><authors><author><keyname>Chen</keyname><forenames>W.</forenames></author></authors><title>A new definition of the fractional Laplacian</title><categories>cs.NA cs.CE</categories><comments>This study is carred out with the ongoing project of &quot;mathematical
  and numerical modelling of medical ultasound wave propagation&quot; sponsored by
  the Simula Research Laboratory</comments><acm-class>G.1.8; G.1.9</acm-class><abstract>  It is noted that the standard definition of the fractional Laplacian leads to
a hyper-singular convolution integral and is also obscure about how to
implement the boundary conditions. This purpose of this note is to introduce a
new definition of the fractional Laplacian to overcome these major drawbacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209021</id><created>2002-09-19</created><authors><author><keyname>Prekop</keyname><forenames>Paul</forenames></author><author><keyname>Burnett</keyname><forenames>Mark</forenames></author></authors><title>Activities, Context and Ubiquitous Computing</title><categories>cs.IR</categories><acm-class>F.M; H1; H4</acm-class><journal-ref>Computer Communications 26 (2003) 1168-1176</journal-ref><abstract>  Context and context-awareness provides computing environments with the
ability to usefully adapt the services or information they provide. It is the
ability to implicitly sense and automatically derive the user needs that
separates context-aware applications from traditionally designed applications,
and this makes them more attentive, responsive, and aware of their user's
identity, and their user's environment. This paper argues that context-aware
applications capable of supporting complex, cognitive activities can be built
from a model of context called Activity-Centric Context. A conceptual model of
Activity-Centric context is presented. The model is illustrated via a detailed
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209022</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209022</id><created>2002-09-19</created><updated>2002-09-23</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>A Comparison of Different Cognitive Paradigms Using Simple Animats in a
  Virtual Laboratory, with Implications to the Notion of Cognition</title><categories>cs.AI</categories><comments>MSc Thesis, University of Sussex. pdf available from
  http://www.cogs.susx.ac.uk/users/carlos</comments><acm-class>I.2.0</acm-class><abstract>  In this thesis I present a virtual laboratory which implements five different
models for controlling animats: a rule-based system, a behaviour-based system,
a concept-based system, a neural network, and a Braitenberg architecture.
Through different experiments, I compare the performance of the models and
conclude that there is no &quot;best&quot; model, since different models are better for
different things in different contexts.
  The models I chose, although quite simple, represent different approaches for
studying cognition. Using the results as an empirical philosophical aid,
  I note that there is no &quot;best&quot; approach for studying cognition, since
different approaches have all advantages and disadvantages, because they study
different aspects of cognition from different contexts. This has implications
for current debates on &quot;proper&quot; approaches for cognition: all approaches are a
bit proper, but none will be &quot;proper enough&quot;. I draw remarks on the notion of
cognition abstracting from all the approaches used to study it, and propose a
simple classification for different types of cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209023</id><created>2002-09-20</created><authors><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Practical Load Balancing for Content Requests in Peer-to-Peer Networks</title><categories>cs.NI cs.DC</categories><comments>23 pages, 38 figures</comments><acm-class>C.2.4</acm-class><abstract>  This paper studies the problem of load-balancing the demand for content in a
peer-to-peer network across heterogeneous peer nodes that hold replicas of the
content. Previous decentralized load balancing techniques in distributed
systems base their decisions on periodic updates containing information about
load or available capacity observed at the serving entities. We show that these
techniques do not work well in the peer-to-peer context; either they do not
address peer node heterogeneity, or they suffer from significant load
oscillations. We propose a new decentralized algorithm, Max-Cap, based on the
maximum inherent capacities of the replica nodes and show that unlike previous
algorithms, it is not tied to the timeliness or frequency of updates. Yet,
Max-Cap can handle the heterogeneity of a peer-to-peer environment without
suffering from load oscillations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209024</id><created>2002-09-21</created><authors><author><keyname>Karbowski</keyname><forenames>Andrzej</forenames></author></authors><title>Errors in Low and Lapsley's article &quot;Optimization Flow Control, I: Basic
  Algorithm and Convergence&quot;</title><categories>cs.NI cs.DC</categories><comments>The mathematical errors in one of the most cited network publication
  are presented</comments><acm-class>C.2.3; G.1.6</acm-class><abstract>  In the note two errors in Low and Lapsley's article &quot;Optimization Flow
Control, I: Basic Algorithm and Convergence&quot;, &quot;IEEE/ACM Transactions on
Networking&quot;, 7(6), pp. 861-874, 1999, are shown. Because of these errors the
proofs of both theorems presented in the article are incomplete and some
assessments are wrong.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209025</id><created>2002-09-21</created><authors><author><keyname>Karbowski</keyname><forenames>Andrzej</forenames></author></authors><title>Correction to Low and Lapsley's article &quot;Optimization Flow Control, I:
  Basic Algorithm and Convergence&quot;</title><categories>cs.NI cs.DC</categories><comments>The corrections to errors in one of the most important articles on
  network management are presented</comments><acm-class>C.2.3,;G.1.6</acm-class><abstract>  In the note an error in Low and Lapsley's article (&quot;Optimization Flow
Control, I: Basic Algorithm and Convergence&quot;, IEEE/ACM Transactions on
Networking, 7(6), pp. 861-874, 1999) is pointed out. Because of this error the
proof of the Theorem 2 presented in the article is incomplete and some
assessments are wrong. In the second part of the note the author proposes a
correction to this proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209026</id><created>2002-09-23</created><authors><author><keyname>Rowlands</keyname><forenames>Peter</forenames></author><author><keyname>Diaz</keyname><forenames>Bernard</forenames></author></authors><title>A universal alphabet and rewrite system</title><categories>cs.OH</categories><comments>15 pages</comments><acm-class>I.0</acm-class><abstract>  We present two ways in which an infinite universal alphabet may be generated
using a novel rewrite system that conserves zero (a special character of the
alphabet and the symbol for that character) at every step. The recursive method
delivers the entire alphabet in one step when invoked with the zero character
as the initial subset alphabet. The iterative method with the same start
delivers characters that act as ciphers for properties that the developing
subset alphabet contains. These properties emerge in an arbitrary sequence and
there are an infinite number of ways they may be selected. The subset alphabets
in addition to having mathematical interpretation as algebra can also be
constrained to emerge in a minimal way which then has application as a
foundational physical system. Each subset alphabet may itself be the basis of a
rewrite system where rules that operate on symbols (representing characters) or
collections of symbols manipulate the specific properties in a dynamic way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209027</id><created>2002-09-23</created><authors><author><keyname>Yaneff</keyname><forenames>A. G.</forenames></author></authors><title>Remarks on d-Dimensional TSP Optimal Tour Length Behaviour</title><categories>cs.CG</categories><comments>8 pages, 5 charts</comments><acm-class>F.2.2., G.2.2</acm-class><abstract>  The well-known $O(n^{1-1/d})$ behaviour of the optimal tour length for TSP in
d-dimensional Cartesian space causes breaches of the triangle inequality. Other
practical inadequacies of this model are discussed, including its use as basis
for approximation of the TSP optimal tour length or bounds derivations, which I
attempt to remedy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209028</id><created>2002-09-25</created><authors><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author></authors><title>Mapping the Gnutella Network: Properties of Large-Scale Peer-to-Peer
  Systems and Implications for System Design</title><categories>cs.DC cond-mat.stat-mech cs.NI</categories><acm-class>C.2.4</acm-class><journal-ref>IEEE Internet Computing Journal (special issue on peer-to-peer
  networking), vol. 6(1) 2002</journal-ref><abstract>  Despite recent excitement generated by the peer-to-peer (P2P) paradigm and
the surprisingly rapid deployment of some P2P applications, there are few
quantitative evaluations of P2P systems behavior. The open architecture,
achieved scale, and self-organizing structure of the Gnutella network make it
an interesting P2P architecture to study. Like most other P2P applications,
Gnutella builds, at the application level, a virtual network with its own
routing mechanisms. The topology of this virtual network and the routing
mechanisms used have a significant influence on application properties such as
performance, reliability, and scalability. We have built a &quot;crawler&quot; to extract
the topology of Gnutella's application level network. In this paper we analyze
the topology graph and evaluate generated network traffic. Our two major
findings are that: (1) although Gnutella is not a pure power-law network, its
current configuration has the benefits and drawbacks of a power-law structure,
and (2) the Gnutella virtual network topology does not match well the
underlying Internet topology, hence leading to ineffective use of the physical
networking infrastructure. These findings guide us to propose changes to the
Gnutella protocol and implementations that may bring significant performance
and scalability improvements. We believe that our findings as well as our
measurement and analysis techniques have broad applicability to P2P systems and
provide unique insights into P2P system design tradeoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209029</id><created>2002-09-25</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>A generalization of Amdahl's law and relative conditions of parallelism</title><categories>cs.DC cs.PF</categories><comments>11 pages, 4 figures</comments><acm-class>F.1.2; D.2.8</acm-class><abstract>  In this work I present a generalization of Amdahl's law on the limits of a
parallel implementation with many processors. In particular I establish some
mathematical relations involving the number of processors and the dimension of
the treated problem, and with these conditions I define, on the ground of the
reachable speedup, some classes of parallelism for the implementations. I also
derive a condition for obtaining superlinear speedup. The used mathematical
technics are those of differential calculus. I describe some examples from
classical problems offered by the specialized literature on the subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209030</id><created>2002-09-26</created><authors><author><keyname>Boettcher</keyname><forenames>Stefan</forenames><affiliation>Emory U</affiliation></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames><affiliation>Los Alamos</affiliation></author></authors><title>Extremal Optimization: an Evolutionary Local-Search Algorithm</title><categories>cs.NE cs.AI</categories><comments>Latex, 17 pages, to appear in the {\it Proceedings of the 8th INFORMS
  Computing Society Conference,} (2003)</comments><acm-class>I.2.8</acm-class><abstract>  A recently introduced general-purpose heuristic for finding high-quality
solutions for many hard optimization problems is reviewed. The method is
inspired by recent progress in understanding far-from-equilibrium phenomena in
terms of {\em self-organized criticality,} a concept introduced to describe
emergent complexity in physical systems. This method, called {\em extremal
optimization,} successively replaces the value of extremely undesirable
variables in a sub-optimal solution with new, random ones. Large,
avalanche-like fluctuations in the cost function self-organize from this
dynamics, effectively scaling barriers to explore local optima in distant
neighborhoods of the configuration space while eliminating the need to tune
parameters. Drawing upon models used to simulate the dynamics of granular
media, evolution, or geology, extremal optimization complements approximation
methods inspired by equilibrium statistical physics, such as {\em simulated
annealing}. It may be but one example of applying new insights into {\em
non-equilibrium phenomena} systematically to hard optimization problems. This
method is widely applicable and so far has proved competitive with -- and even
superior to -- more elaborate general-purpose heuristics on testbeds of
constrained optimization problems with up to $10^5$ variables, such as
bipartitioning, coloring, and satisfiability. Analysis of a suitable model
predicts the only free parameter of the method in accordance with all
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209031</id><created>2002-09-26</created><authors><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author><author><keyname>Foster</keyname><forenames>Ian</forenames></author></authors><title>Locating Data in (Small-World?) Peer-to-Peer Scientific Collaborations</title><categories>cs.DC cond-mat</categories><comments>8 pages</comments><acm-class>C.2.4</acm-class><journal-ref>1st International Workshop on Peer-to-Peer Systems IPTPS 2002</journal-ref><abstract>  Data-sharing scientific collaborations have particular characteristics,
potentially different from the current peer-to-peer environments. In this paper
we advocate the benefits of exploiting emergent patterns in self-configuring
networks specialized for scientific data-sharing collaborations. We speculate
that a peer-to-peer scientific collaboration network will exhibit small-world
topology, as do a large number of social networks for which the same pattern
has been documented. We propose a solution for locating data in decentralized,
scientific, data-sharing environments that exploits the small-worlds topology.
The research challenge we raise is: what protocols should be used to allow a
self-configuring peer-to-peer network to form small worlds similar to the way
in which the humans that use the network do in their social interactions?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209032</identifier>
 <datestamp>2007-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209032</id><created>2002-09-27</created><updated>2004-03-31</updated><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Complexity Results on DPLL and Resolution</title><categories>cs.LO cs.CC</categories><acm-class>F.2.2; F.1.3; F.4.1; I.2.3</acm-class><doi>10.1145/1119439.1119442</doi><abstract>  DPLL and resolution are two popular methods for solving the problem of
propositional satisfiability. Rather than algorithms, they are families of
algorithms, as their behavior depend on some choices they face during
execution: DPLL depends on the choice of the literal to branch on; resolution
depends on the choice of the pair of clauses to resolve at each step. The
complexity of making the optimal choice is analyzed in this paper. Extending
previous results, we prove that choosing the optimal literal to branch on in
DPLL is Delta[log]^2-hard, and becomes NP^PP-hard if branching is only allowed
on a subset of variables. Optimal choice in regular resolution is both NP-hard
and CoNP-hard. The problem of determining the size of the optimal proofs is
also analyzed: it is CoNP-hard for DPLL, and Delta[log]^2-hard if a conjecture
we make is true. This problem is CoNP-hard for regular resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209033</id><created>2002-09-30</created><updated>2003-03-11</updated><authors><author><keyname>Baptiste</keyname><forenames>Philippe</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Jawor</keyname><forenames>Wojciech</forenames></author><author><keyname>Vakhania</keyname><forenames>Nodari</forenames></author></authors><title>Preemptive Scheduling of Equal-Length Jobs to Maximize Weighted
  Throughput</title><categories>cs.DS</categories><comments>gained one author and lost one degree in the complexity</comments><acm-class>F.2.2</acm-class><abstract>  We study the problem of computing a preemptive schedule of equal-length jobs
with given release times, deadlines and weights. Our goal is to maximize the
weighted throughput, which is the total weight of completed jobs. In Graham's
notation this problem is described as (1 | r_j;p_j=p;pmtn | sum w_j U_j). We
provide an O(n^4)-time algorithm for this problem, improving the previous bound
of O(n^{10}) by Baptiste.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0209034</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0209034</id><created>2002-09-30</created><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author></authors><title>An Algorithmic Study of Manufacturing Paperclips and Other Folded
  Structures</title><categories>cs.CG</categories><comments>28 pages, 14 figures, Latex, to appear in Computational Geometry -
  Theory and Applications</comments><acm-class>F.2.2; I.3.5</acm-class><journal-ref>Computational Geometry: Theory and Applications, 25 (2003),
  117-138.</journal-ref><abstract>  We study algorithmic aspects of bending wires and sheet metal into a
specified structure. Problems of this type are closely related to the question
of deciding whether a simple non-self-intersecting wire structure (a
carpenter's ruler) can be straightened, a problem that was open for several
years and has only recently been solved in the affirmative.
  If we impose some of the constraints that are imposed by the manufacturing
process, we obtain quite different results. In particular, we study the variant
of the carpenter's ruler problem in which there is a restriction that only one
joint can be modified at a time. For a linkage that does not self-intersect or
self-touch, the recent results of Connelly et al. and Streinu imply that it can
always be straightened, modifying one joint at a time. However, we show that
for a linkage with even a single vertex degeneracy, it becomes NP-hard to
decide if it can be straightened while altering only one joint at a time. If we
add the restriction that each joint can be altered at most once, we show that
the problem is NP-complete even without vertex degeneracies.
  In the special case, arising in wire forming manufacturing, that each joint
can be altered at most once, and must be done sequentially from one or both
ends of the linkage, we give an efficient algorithm to determine if a linkage
can be straightened.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210001</id><created>2002-09-30</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Edsger Wybe Dijkstra (1930 -- 2002): A Portrait of a Genius</title><categories>cs.GL</categories><comments>10 pages. To appear in Formal Aspects of Computing</comments><acm-class>A.0</acm-class><abstract>  We discuss the scientific contributions of Edsger Wybe Dijkstra, his opinions
and his legacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210002</id><created>2002-10-01</created><authors><author><keyname>Buyya</keyname><forenames>Alexander Barmouta Rajkumar</forenames></author></authors><title>GridBank: A Grid Accounting Services Architecture (GASA) for Distributed
  Systems Sharing and Integration</title><categories>cs.DC</categories><comments>12 pages</comments><acm-class>C.2.4</acm-class><abstract>  Computational Grids are emerging as new infrastructure for Internet-based
parallel and distributed computing. They enable the sharing, exchange,
discovery, and aggregation of resources distributed across multiple
administrative domains, organizations and enterprises. To accomplish this,
Grids need infrastructure that supports various services: security, uniform
access, resource management, scheduling, application composition, computational
economy, and accountability. Many Grid projects have developed technologies
that provide many of these services with an exception of accountability. To
overcome this limitation, we propose a new infrastructure called Grid Bank that
provides services for accounting. This paper presents requirements of Grid
accountability and different models within which it can operate and proposes
Grid Bank Services Architecture that meets them. The paper highlights
implementation issues with detailed discussion on format for various
records/database that the GridBank need to maintain. It also presents protocols
for interaction between GridBank and various components within Grid computing
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210003</id><created>2002-10-01</created><updated>2002-10-02</updated><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Noy</keyname><forenames>Marc</forenames></author><author><keyname>Sacristan</keyname><forenames>Vera</forenames></author><author><keyname>Sethia</keyname><forenames>Saurabh</forenames></author></authors><title>On the Reflexivity of Point Sets</title><categories>cs.CG cs.DS</categories><comments>28 pages, 16 figures, Latex, short version to appear in Discrete &amp;
  Computational Geometry -- The Goodman-Pollack Festschrift (2002),
  Springer-Verlag</comments><acm-class>F.2.2; I.3.5</acm-class><abstract>  We introduce a new measure for planar point sets S that captures a
combinatorial distance that S is from being a convex set: The reflexivity
rho(S) of S is given by the smallest number of reflex vertices in a simple
polygonalization of S. We prove various combinatorial bounds and provide
efficient algorithms to compute reflexivity, both exactly (in special cases)
and approximately (in general). Our study considers also some closely related
quantities, such as the convex cover number kappa_c(S) of a planar point set,
which is the smallest number of convex chains that cover S, and the convex
partition number kappa_p(S), which is given by the smallest number of convex
chains with pairwise-disjoint convex hulls that cover S. We have proved that it
is NP-complete to determine the convex cover or the convex partition number and
have given logarithmic-approximation algorithms for determining each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210004</id><created>2002-10-03</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Lagrue</keyname><forenames>Sylvain</forenames></author><author><keyname>Papini</keyname><forenames>Odile</forenames></author></authors><title>Revising Partially Ordered Beliefs</title><categories>cs.AI</categories><comments>figures made with the pstricks latex packages</comments><acm-class>I.2.3;I.2.4</acm-class><journal-ref>Proc. of the 9th Workshop on Non-monotonic Reasoning (NMR'2002),
  pp. 142--149</journal-ref><abstract>  This paper deals with the revision of partially ordered beliefs. It proposes
a semantic representation of epistemic states by partial pre-orders on
interpretations and a syntactic representation by partially ordered belief
bases. Two revision operations, the revision stemming from the history of
observations and the possibilistic revision, defined when the epistemic state
is represented by a total pre-order, are generalized, at a semantic level, to
the case of a partial pre-order on interpretations, and at a syntactic level,
to the case of a partially ordered belief base. The equivalence between the two
representations is shown for the two revision operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210005</id><created>2002-10-07</created><authors><author><keyname>Chen</keyname><forenames>W</forenames></author></authors><title>Positive time fractional derivative</title><categories>cs.CE</categories><comments>Welcome any comments to wenc@simula.no</comments><report-no>Simula Research Laboratory Report, Sept. 2002</report-no><acm-class>G.1.8; G.1.9</acm-class><abstract>  In mathematical modeling of the non-squared frequency-dependent diffusions,
also known as the anomalous diffusions, it is desirable to have a positive real
Fourier transform for the time derivative of arbitrary fractional or odd
integer order. The Fourier transform of the fractional time derivative in the
Riemann-Liouville and Caputo senses, however, involves a complex power function
of the fractional order. In this study, a positive time derivative of
fractional or odd integer order is introduced to respect the positivity in
modeling the anomalous diffusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210006</id><created>2002-10-09</created><updated>2003-06-20</updated><authors><author><keyname>Andersson</keyname><forenames>Arne</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Dynamic Ordered Sets with Exponential Search Trees</title><categories>cs.DS</categories><comments>Revision corrects some typoes and state things better for
  applications in subsequent papers</comments><acm-class>E.1;F.2.2;G.2.2</acm-class><abstract>  We introduce exponential search trees as a novel technique for converting
static polynomial space search structures for ordered sets into fully-dynamic
linear space data structures.
  This leads to an optimal bound of O(sqrt(log n/loglog n)) for searching and
updating a dynamic set of n integer keys in linear space. Here searching an
integer y means finding the maximum key in the set which is smaller than or
equal to y. This problem is equivalent to the standard text book problem of
maintaining an ordered set (see, e.g., Cormen, Leiserson, Rivest, and Stein:
Introduction to Algorithms, 2nd ed., MIT Press, 2001).
  The best previous deterministic linear space bound was O(log n/loglog n) due
Fredman and Willard from STOC 1990. No better deterministic search bound was
known using polynomial space.
  We also get the following worst-case linear space trade-offs between the
number n, the word length w, and the maximal key U &lt; 2^w: O(min{loglog n+log
n/log w, (loglog n)(loglog U)/(logloglog U)}). These trade-offs are, however,
not likely to be optimal.
  Our results are generalized to finger searching and string searching,
providing optimal results for both in terms of n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210007</identifier>
 <datestamp>2007-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210007</id><created>2002-10-09</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author><author><keyname>Schaerf</keyname><forenames>Marco</forenames></author></authors><title>Compilability of Abduction</title><categories>cs.AI cs.CC</categories><acm-class>F.4.1; F.1.3</acm-class><doi>10.1145/1182613.1182615</doi><abstract>  Abduction is one of the most important forms of reasoning; it has been
successfully applied to several practical problems such as diagnosis. In this
paper we investigate whether the computational complexity of abduction can be
reduced by an appropriate use of preprocessing. This is motivated by the fact
that part of the data of the problem (namely, the set of all possible
assumptions and the theory relating assumptions and manifestations) are often
known before the rest of the problem. In this paper, we show some complexity
results about abduction when compilation is allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210008</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210008</id><created>2002-10-11</created><authors><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Rapaport</keyname><forenames>Ivan</forenames></author><author><keyname>Theyssier</keyname><forenames>Guillaume</forenames></author></authors><title>Cellular automata and communication complexity</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><abstract>  The model of cellular automata is fascinating because very simple local rules
can generate complex global behaviors. The relationship between local and
global function is subject of many studies. We tackle this question by using
results on communication complexity theory and, as a by-product, we provide
(yet another) classification of cellular automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210009</id><created>2002-10-11</created><authors><author><keyname>Makatchev</keyname><forenames>Maxim</forenames></author></authors><title>On the Cell-based Complexity of Recognition of Bounded Configurations by
  Finite Dynamic Cellular Automata</title><categories>cs.CC cs.CV</categories><comments>11 pages, 1 figure</comments><acm-class>F.1.3; F.2.2; F.2.3; I.4.3; I.5.1; I.5.4; I.5.5</acm-class><abstract>  This paper studies complexity of recognition of classes of bounded
configurations by a generalization of conventional cellular automata (CA) --
finite dynamic cellular automata (FDCA). Inspired by the CA-based models of
biological and computer vision, this study attempts to derive the properties of
a complexity measure and of the classes of input configurations that make it
beneficial to realize the recognition via a two-layered automaton as compared
to a one-layered automaton. A formalized model of an image pattern recognition
task is utilized to demonstrate that the derived conditions can be satisfied
for a non-empty set of practical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210010</id><created>2002-10-13</created><authors><author><keyname>Sarshar</keyname><forenames>Nima</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani</forenames></author></authors><title>A Random Structure for Optimum Cache Size Distributed hash table (DHT)
  Peer-to-Peer design</title><categories>cs.NI cs.DC</categories><comments>13 pages, 2 figures, preprint version</comments><acm-class>H.3.3</acm-class><abstract>  We propose a new and easily-realizable distributed hash table (DHT)
peer-to-peer structure, incorporating a random caching strategy that allows for
{\em polylogarithmic search time} while having only a {\em constant cache}
size. We also show that a very large class of deterministic caching strategies,
which covers almost all previously proposed DHT systems, can not achieve
polylog search time with constant cache size. In general, the new scheme is the
first known DHT structure with the following highly-desired properties: (a)
Random caching strategy with constant cache size; (b) Average search time of
$O(log^{2}(N))$; (c) Guaranteed search time of $O(log^{3}(N))$; (d) Truly local
cache dynamics with constant overhead for node deletions and additions; (e)
Self-organization from any initial network state towards the desired structure;
and (f) Allows a seamless means for various trade-offs, e.g., search speed or
anonymity at the expense of larger cache size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210011</id><created>2002-10-14</created><authors><author><keyname>Ignjatovic</keyname><forenames>Aleksandar</forenames></author></authors><title>A Note on Induction Schemas in Bounded Arithmetic</title><categories>cs.LO cs.CC</categories><acm-class>F.4.1</acm-class><abstract>  As is well known, Buss' theory of bounded arithmetic $S^{1}_{2}$ proves
$\Sigma_{0}^{b}(\Sigma_{1}^{b})-LIND$; however, we show that Allen's
$D_{2}^{1}$ does not prove $\Sigma_{0}^{b}(\Sigma_{1}^{b})-LLIND$ unless $P =
NC$. We also give some interesting alternative axiomatisations of $S^{1}_{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210012</id><created>2002-10-14</created><authors><author><keyname>Konovalov</keyname><forenames>Igor B.</forenames></author></authors><title>Selection of future events from a time series in relation to estimations
  of forecasting uncertainty</title><categories>cs.NE</categories><acm-class>J.2; J.4</acm-class><abstract>  A new general procedure for a priori selection of more predictable events
from a time series of observed variable is proposed. The procedure is
applicable to time series which contains different types of events that feature
significantly different predictability, or, in other words, to heteroskedastic
time series. A priori selection of future events in accordance to expected
uncertainty of their forecasts may be helpful for making practical decisions.
The procedure first implies creation of two neural network based forecasting
models, one of which is aimed at prediction of conditional mean and other -
conditional dispersion, and then elaboration of the rule for future event
selection into groups of more and less predictable events. The method is
demonstrated and tested by the example of the computer generated time series,
and then applied to the real world time series, Dow Jones Industrial Average
index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210013</id><created>2002-10-14</created><updated>2002-10-14</updated><authors><author><keyname>Csirik</keyname><forenames>Janos</forenames></author><author><keyname>Johnson</keyname><forenames>David S.</forenames></author><author><keyname>Kenyon</keyname><forenames>Claire</forenames></author><author><keyname>Orlin</keyname><forenames>James B.</forenames></author><author><keyname>Shor</keyname><forenames>Peter W.</forenames></author><author><keyname>Weber</keyname><forenames>Richard R.</forenames></author></authors><title>On the Sum-of-Squares Algorithm for Bin Packing</title><categories>cs.DS</categories><comments>72 pages</comments><acm-class>F.2.2 G.3</acm-class><abstract>  In this paper we present a theoretical analysis of the deterministic on-line
{\em Sum of Squares} algorithm ($SS$) for bin packing introduced and studied
experimentally in \cite{CJK99}, along with several new variants. $SS$ is
applicable to any instance of bin packing in which the bin capacity $B$ and
item sizes $s(a)$ are integral (or can be scaled to be so), and runs in time
$O(nB)$. It performs remarkably well from an average case point of view: For
any discrete distribution in which the optimal expected waste is sublinear,
$SS$ also has sublinear expected waste. For any discrete distribution where the
optimal expected waste is bounded, $SS$ has expected waste at most $O(\log n)$.
In addition, we discuss several interesting variants on $SS$, including a
randomized $O(nB\log B)$-time on-line algorithm $SS^*$, based on $SS$, whose
expected behavior is essentially optimal for all discrete distributions.
Algorithm $SS^*$ also depends on a new linear-programming-based
pseudopolynomial-time algorithm for solving the NP-hard problem of determining,
given a discrete distribution $F$, just what is the growth rate for the optimal
expected waste. This article is a greatly expanded version of the conference
paper \cite{sumsq2000}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210014</id><created>2002-10-16</created><authors><author><keyname>Kirilov</keyname><forenames>A. S.</forenames></author></authors><title>Current state of the Sonix -- the IBR-2 instrument control software and
  plans for future developments</title><categories>cs.HC</categories><comments>Invited talk at NOBUGS2002 Conference, NIST, Gaithersburg, MD NOBUGS
  abstract identifier NOBUGS2002/015 5 pages, pdf, 2 figures</comments><acm-class>D.2.2</acm-class><abstract>  The Sonix is the main control software for the IBR-2 instruments. This is a
modular configurable and flexible system created using the Varman (real time
database) and the X11/OS9 graphical package in the OS-9 environment. In the
last few years we were mostly focused on making this system more reliable and
user friendly. Because the VME hardware and software upgrade is rather
expensive we would like to replace existing VME + OS9 control computers with
the PC+Windows XP ones in the future. This could be done with the help of
VME-PCI adapters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210015</id><created>2002-10-16</created><updated>2002-10-23</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>New Developments in Interval Arithmetic and Their Implications for
  Floating-Point Standardization</title><categories>cs.NA</categories><comments>12 pages; 3 tables</comments><report-no>DCS-273-IR</report-no><acm-class>B.2.4; G.1.0</acm-class><abstract>  We consider the prospect of a processor that can perform interval arithmetic
at the same speed as conventional floating-point arithmetic. This makes it
possible for all arithmetic to be performed with the superior security of
interval methods without any penalty in speed. In such a situation the IEEE
floating-point standard needs to be compared with a version of floating-point
arithmetic that is ideal for the purpose of interval arithmetic. Such a
comparison requires a succinct and complete exposition of interval arithmetic
according to its recent developments. We present such an exposition in this
paper. We conclude that the directed roundings toward the infinities and the
definition of division by the signed zeros are valuable features of the
standard. Because the operations of interval arithmetic are always defined,
exceptions do not arise. As a result neither Nans nor exceptions are needed. Of
the status flags, only the inexact flag may be useful. Denormalized numbers
seem to have no use for interval arithmetic; in the use of interval
constraints, they are a handicap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210016</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210016</id><created>2002-10-17</created><updated>2003-05-04</updated><authors><author><keyname>Liao</keyname><forenames>Chien-Chih</forenames></author><author><keyname>Lu</keyname><forenames>Hsueh-I</forenames></author><author><keyname>Yen</keyname><forenames>Hsu-Chun</forenames></author></authors><title>Compact Floor-Planning via Orderly Spanning Trees</title><categories>cs.DS cs.CG</categories><comments>13 pages, 5 figures, An early version of this work was presented at
  9th International Symposium on Graph Drawing (GD 2001), Vienna, Austria,
  September 2001. Accepted to Journal of Algorithms, 2003</comments><acm-class>F.2.2; E.1; G.2.2; B.7.2</acm-class><journal-ref>Journal of Algorithms, 48(2):441-451, 2003</journal-ref><doi>10.1016/S0196-6774(03)00057-9</doi><abstract>  Floor-planning is a fundamental step in VLSI chip design. Based upon the
concept of orderly spanning trees, we present a simple O(n)-time algorithm to
construct a floor-plan for any n-node plane triangulation. In comparison with
previous floor-planning algorithms in the literature, our solution is not only
simpler in the algorithm itself, but also produces floor-plans which require
fewer module types. An equally important aspect of our new algorithm lies in
its ability to fit the floor-plan area in a rectangle of size (n-1)x(2n+1)/3.
Lower bounds on the worst-case area for floor-planning any plane triangulation
are also provided in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210017</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210017</id><created>2002-10-17</created><authors><author><keyname>Gunther</keyname><forenames>Neil J.</forenames></author></authors><title>A New Interpretation of Amdahl's Law and Geometric Scalability</title><categories>cs.DC cs.PF</categories><comments>New interpretation of queueing bounds</comments><report-no>PDC-TR190402</report-no><acm-class>B.8; C.4; C.5.5; D.4.8; F.1.2</acm-class><abstract>  The multiprocessor effect refers to the loss of computing cycles due to
processing overhead. Amdahl's law and the Multiprocessing Factor (MPF) are two
scaling models used in industry and academia for estimating multiprocessor
capacity in the presence of this multiprocessor effect. Both models express
different laws of diminishing returns. Amdahl's law identifies diminishing
processor capacity with a fixed degree of serialization in the workload, while
the MPF model treats it as a constant geometric ratio. The utility of both
models for performance evaluation stems from the presence of a single parameter
that can be determined easily from a small set of benchmark measurements. This
utility, however, is marred by a dilemma. The two models produce different
results, especially for large processor configurations that are so important
for today's applications. The question naturally arises: Which of these two
models is the correct one to use? Ignoring this question merely reduces
capacity prediction to arbitrary curve-fitting. Removing the dilemma requires a
dynamical interpretation of these scaling models. We present a physical
interpretation based on queueing theory and show that Amdahl's law corresponds
to synchronous queueing in a bus model while the MPF model belongs to a Coxian
server model. The latter exhibits unphysical effects such as sublinear response
times hence, we caution against its use for large multiprocessor
configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210018</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210018</id><created>2002-10-18</created><authors><author><keyname>Worlton</keyname><forenames>T. G.</forenames></author><author><keyname>Chatterjee</keyname><forenames>A.</forenames></author><author><keyname>Hammonds</keyname><forenames>J. P.</forenames></author><author><keyname>Peterson</keyname><forenames>P. F.</forenames></author><author><keyname>Mikkelson</keyname><forenames>D. J.</forenames></author><author><keyname>Mikkelson</keyname><forenames>R. L.</forenames></author></authors><title>User software for the next generation</title><categories>cs.GR cs.CE</categories><comments>Invited talk at NOBUGS2002 Conference, NIST, Gaithersburg, MD NOBUGS
  abstract identifier NOBUGS2002/023 6 PAGES, pdf</comments><acm-class>J2;I3.6;I3.3</acm-class><abstract>  New generations of neutron scattering sources and instrumentation are
providing challenges in data handling for user software. Time-of-Flight
instruments used at pulsed sources typically produce hundreds or thousands of
channels of data for each detector segment. New instruments are being designed
with thousands to hundreds of thousands of detector segments. High intensity
neutron sources make possible parametric studies and texture studies which
further increase data handling requirements. The Integrated Spectral Analysis
Workbench (ISAW) software developed at Argonne handles large numbers of spectra
simultaneously while providing operations to reduce, sort, combine and export
the data. It includes viewers to inspect the data in detail in real time. ISAW
uses existing software components and packages where feasible and takes
advantage of the excellent support for user interface design and network
communication in Java. The included scripting language simplifies repetitive
operations for analyzing many files related to a given experiment. Recent
additions to ISAW include a contour view, a time-slice table view, routines for
finding and fitting peaks in data, and support for data from other facilities
using the NeXus format. In this paper, I give an overview of features and
planned improvements of ISAW. Details of some of the improvements are covered
in other presentations at this conference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210019</id><created>2002-10-19</created><authors><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>A Historic Name-Trail Service</title><categories>cs.NI cs.DC</categories><comments>13 Pages</comments><acm-class>C.2.3; C.2.5</acm-class><abstract>  People change the identifiers through which they are reachable online as they
change jobs or residences or Internet service providers. This kind of personal
mobility makes reaching people online error-prone. As people move, they do not
always know who or what has cached their now obsolete identifiers so as to
inform them of the move. Use of these old identifiers can cause delivery
failure of important messages, or worse, may cause delivery of messages to
unintended recipients. For example, a sensitive email message sent to my now
obsolete work address at a former place of employment may reach my unfriendly
former boss instead of me.
  In this paper we describe HINTS, a historic name-trail service. This service
provides a persistent way to name willing participants online using today's
transient online identifiers. HINTS accomplishes this by connecting together
the names a person uses along with the times during which those names were
valid for the person, thus giving people control over the historic use of their
names. A correspondent who wishes to reach a mobile person can use an obsolete
online name for that person, qualified with a time at which the online name was
successfully used; HINTS resolves this historic name to a current valid online
identifier for the intended recipient, if that recipient has chosen to leave a
name trail in HINTS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210020</id><created>2002-10-21</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Hohenberger</keyname><forenames>Susan</forenames></author><author><keyname>Liben-Nowell</keyname><forenames>David</forenames></author></authors><title>Tetris is Hard, Even to Approximate</title><categories>cs.CC cs.CG cs.DM</categories><comments>56 pages, 11 figures</comments><report-no>MIT-LCS-TR-865</report-no><acm-class>F.1.3; F.2.2; G.2.1; K.8.0</acm-class><abstract>  In the popular computer game of Tetris, the player is given a sequence of
tetromino pieces and must pack them into a rectangular gameboard initially
occupied by a given configuration of filled squares; any completely filled row
of the gameboard is cleared and all pieces above it drop by one row. We prove
that in the offline version of Tetris, it is NP-complete to maximize the number
of cleared rows, maximize the number of tetrises (quadruples of rows
simultaneously filled and cleared), minimize the maximum height of an occupied
square, or maximize the number of pieces placed before the game ends. We
furthermore show the extreme inapproximability of the first and last of these
objectives to within a factor of p^(1-epsilon), when given a sequence of p
pieces, and the inapproximability of the third objective to within a factor of
(2 - epsilon), for any epsilon&gt;0. Our results hold under several variations on
the rules of Tetris, including different models of rotation, limitations on
player agility, and restricted piece sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210021</id><created>2002-10-21</created><authors><author><keyname>Hunter</keyname><forenames>Jane</forenames></author></authors><title>Reconciling MPEG-7 and MPEG-21 Semantics through a Common Event-Aware
  Metadata Model</title><categories>cs.MM cs.DL</categories><acm-class>H.2.1;H.3.1;H.3.7;H.5.1</acm-class><abstract>  The &quot;event&quot; concept appears repeatedly when developing metadata models for
the description and management of multimedia content. During the typical life
cycle of multimedia content, events occur at many different levels - from the
events which happen during content creation (directing, acting, camera panning
and zooming) to the events which happen to the physical form (acquisition,
relocation, damage of film or video) to the digital conversion, reformatting,
editing and repackaging events, to the events which are depicted in the actual
content (political, news, sporting) to the usage, ownership and copyright
agreement events and even the metadata attribution events. Support is required
within both MPEG-7 and MPEG-21 for the clear and unambiguous description of all
of these event types which may occur at widely different levels of nesting and
granularity. In this paper we first describe an event-aware model (the ABC
model) which is capable of modeling and yet clearly differentiating between all
of these, often recursive and overlapping events. We then illustrate how this
model can be used as the foundation to facilitate semantic interoperability
between MPEG-7 and MPEG-21. By expressing the semantics of both MPEG-7 and
MPEG-21 metadata terms in RDF Schema (and some DAML+OIL extensions) and
attaching the MPEG-7 and MPEG-21 class and property hierarchies to the
appropriate top-level classes and properties of the ABC model, we are
essentially able to define a single distributed machine-understandable
ontology, which will enable interoperability of data and services across the
entire multimedia content delivery chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210022</id><created>2002-10-25</created><updated>2004-03-18</updated><authors><author><keyname>Aehlig</keyname><forenames>Klaus</forenames></author><author><keyname>Johannsen</keyname><forenames>Jan</forenames></author></authors><title>An Elementary Fragment of Second-Order Lambda Calculus</title><categories>cs.LO</categories><comments>16 pages; corrections</comments><acm-class>F.4.1; F.2.2</acm-class><abstract>  A fragment of second-order lambda calculus (System F) is defined that
characterizes the elementary recursive functions. Type quantification is
restricted to be non-interleaved and stratified, i.e., the types are assigned
levels, and a quantified variable can only be instantiated by a type of smaller
level, with a slightly liberalized treatment of the level zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210023</id><created>2002-10-25</created><authors><author><keyname>Porter</keyname><forenames>Timothy</forenames></author></authors><title>Geometric Aspects of Multiagent Systems</title><categories>cs.MA cs.AI</categories><comments>14 pages, 1 eps figure, prepared for GETCO2002</comments><acm-class>I.2.11 Distributed Artificial Intelligence</acm-class><abstract>  Recent advances in Multiagent Systems (MAS) and Epistemic Logic within
Distributed Systems Theory, have used various combinatorial structures that
model both the geometry of the systems and the Kripke model structure of models
for the logic. Examining one of the simpler versions of these models,
interpreted systems, and the related Kripke semantics of the logic $S5_n$ (an
epistemic logic with $n$-agents), the similarities with the geometric /
homotopy theoretic structure of groupoid atlases is striking. These latter
objects arise in problems within algebraic K-theory, an area of algebra linked
to the study of decomposition and normal form theorems in linear algebra. They
have a natural well structured notion of path and constructions of path
objects, etc., that yield a rich homotopy theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210024</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210024</id><created>2002-10-26</created><authors><author><keyname>Arkin</keyname><forenames>Esther M.</forenames></author><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Skiena</keyname><forenames>Steven S.</forenames></author></authors><title>The Lazy Bureaucrat Scheduling Problem</title><categories>cs.DS cs.DM</categories><comments>19 pages, 2 figures, Latex. To appear, Information and Computation</comments><acm-class>F.2.2; I.2.8</acm-class><abstract>  We introduce a new class of scheduling problems in which the optimization is
performed by the worker (single ``machine'') who performs the tasks. A typical
worker's objective is to minimize the amount of work he does (he is ``lazy''),
or more generally, to schedule as inefficiently (in some sense) as possible.
The worker is subject to the constraint that he must be busy when there is work
that he can do; we make this notion precise both in the preemptive and
nonpreemptive settings. The resulting class of ``perverse'' scheduling
problems, which we denote ``Lazy Bureaucrat Problems,'' gives rise to a rich
set of new questions that explore the distinction between maximization and
minimization in computing optimal schedules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210025</id><created>2002-10-28</created><updated>2002-11-26</updated><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Shalizi</keyname><forenames>Kristina Lisa</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>An Algorithm for Pattern Discovery in Time Series</title><categories>cs.LG cs.CL</categories><comments>26 pages, 5 figures; 5 tables;
  http://www.santafe.edu/projects/CompMech Added discussion of algorithm
  parameters; improved treatment of convergence and time complexity; added
  comparison to older methods</comments><report-no>SFI Working Paper 02-10-060</report-no><acm-class>I.2.6;H.1.1;E.4</acm-class><abstract>  We present a new algorithm for discovering patterns in time series and other
sequential data. We exhibit a reliable procedure for building the minimal set
of hidden, Markovian states that is statistically capable of producing the
behavior exhibited in the data -- the underlying process's causal states.
Unlike conventional methods for fitting hidden Markov models (HMMs) to data,
our algorithm makes no assumptions about the process's causal architecture (the
number of hidden states and their transition structure), but rather infers it
from the data. It starts with assumptions of minimal structure and introduces
complexity only when the data demand it. Moreover, the causal states it infers
have important predictive optimality properties that conventional HMM states
lack. We introduce the algorithm, review the theory behind it, prove its
asymptotic reliability, use large deviation theory to estimate its rate of
convergence, and compare it to other algorithms which also construct HMMs from
data. We also illustrate its behavior on an example process, and report
selected numerical results from an implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210026</id><created>2002-10-29</created><authors><author><keyname>Alvarez</keyname><forenames>Gonzalo</forenames></author><author><keyname>Petrovic</keyname><forenames>Slobodan</forenames></author></authors><title>Encoding a Taxonomy of Web Attacks with Different-Length Vectors</title><categories>cs.CR cs.AI</categories><comments>22 pages, 1 figure, latex format</comments><acm-class>H.3.3;H.3.5</acm-class><journal-ref>Computers and Security 22, 435-449, 2003</journal-ref><abstract>  Web attacks, i.e. attacks exclusively using the HTTP protocol, are rapidly
becoming one of the fundamental threats for information systems connected to
the Internet. When the attacks suffered by web servers through the years are
analyzed, it is observed that most of them are very similar, using a reduced
number of attacking techniques. It is generally agreed that classification can
help designers and programmers to better understand attacks and build more
secure applications. As an effort in this direction, a new taxonomy of web
attacks is proposed in this paper, with the objective of obtaining a
practically useful reference framework for security applications. The use of
the taxonomy is illustrated by means of multiplatform real world web attack
examples. Along with this taxonomy, important features of each attack category
are discussed. A suitable semantic-dependent web attack encoding scheme is
defined that uses different-length vectors. Possible applications are
described, which might benefit from this taxonomy and encoding scheme, such as
intrusion detection systems and application firewalls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210027</id><created>2002-10-29</created><updated>2003-11-12</updated><authors><author><keyname>Hitzler</keyname><forenames>Pascal</forenames><affiliation>Artificial Intelligence Institute, Dresden University of Technology, Germany</affiliation></author><author><keyname>Wendt</keyname><forenames>Matthias</forenames><affiliation>Artificial Intelligence Institute, Dresden University of Technology, Germany</affiliation></author></authors><title>A uniform approach to logic programming semantics</title><categories>cs.AI cs.LO</categories><comments>29 pages. To appear in Theory and Practice of Logic Programming</comments><report-no>WV-02-14</report-no><acm-class>I.2.4; D.1.6; F.4.1</acm-class><abstract>  Part of the theory of logic programming and nonmonotonic reasoning concerns
the study of fixed-point semantics for these paradigms. Several different
semantics have been proposed during the last two decades, and some have been
more successful and acknowledged than others. The rationales behind those
various semantics have been manifold, depending on one's point of view, which
may be that of a programmer or inspired by commonsense reasoning, and
consequently the constructions which lead to these semantics are technically
very diverse, and the exact relationships between them have not yet been fully
understood. In this paper, we present a conceptually new method, based on level
mappings, which allows to provide uniform characterizations of different
semantics for logic programs. We will display our approach by giving new and
uniform characterizations of some of the major semantics, more particular of
the least model semantics for definite programs, of the Fitting semantics, and
of the well-founded semantics. A novel characterization of the weakly perfect
model semantics will also be provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210028</id><created>2002-10-29</created><updated>2002-10-30</updated><authors><author><keyname>Cohen</keyname><forenames>Sara</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>Sagiv</keyname><forenames>Yehoshua</forenames></author></authors><title>Equivalences Among Aggregate Queries with Negation</title><categories>cs.DB cs.LO</categories><comments>28 pages</comments><acm-class>F.4.1;H.2.3;H.2.4</acm-class><abstract>  Query equivalence is investigated for disjunctive aggregate queries with
negated subgoals, constants and comparisons. A full characterization of
equivalence is given for the aggregation functions count, max, sum, prod,
toptwo and parity. A related problem is that of determining, for a given
natural number N, whether two given queries are equivalent over all databases
with at most N constants. We call this problem bounded equivalence. A complete
characterization of decidability of bounded equivalence is given. In
particular, it is shown that this problem is decidable for all the above
aggregation functions as well as for count distinct and average. For
quasilinear queries (i.e., queries where predicates that occur positively are
not repeated) it is shown that equivalence can be decided in polynomial time
for the aggregation functions count, max, sum, parity, prod, toptwo and
average. A similar result holds for count distinct provided that a few
additional conditions hold. The results are couched in terms of abstract
characteristics of aggregation functions, and new proof techniques are used.
Finally, the results above also imply that equivalence, under bag-set
semantics, is decidable for non-aggregate queries with negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210029</id><created>2002-10-29</created><authors><author><keyname>Marcondes</keyname><forenames>Carlos H.</forenames></author><author><keyname>Sayao</keyname><forenames>Luis Fernando</forenames></author></authors><title>Integration and interoperability accessing electronic information
  resources in science and technology: the proposal of Brazilian Digital
  Library</title><categories>cs.DL</categories><comments>9 pages, 2 figure. Paper presented to the International Congress on
  Enterprise Information Systems/Workshop on NDDL - New Developments in Digital
  Libraries, April, 2002, Ciudad Real, Spain. Proceeedings. Ciudad Real,Spain:
  ICEIS Press 2002. p.104-115</comments><acm-class>H.3.7</acm-class><abstract>  This paper describes technological and methodological options to achieve
interoperability in accessing electronic information resources, available in
Internet, in the scope of Brazilian Digital Library in Science and Technology
Project - BDL, developed by Brazilian Institute for Scientific and Technical
Information - IBICT. It stresses the impact of the Web in the publishing and
communication processes in science and technology and also in the information
systems and libraries. The work points out the two major objectives of the BDL
Project: facilitates electronic publishing of different full text materials
such as theses, journal articles, conference papers,grey literature - by
Brazilian scientific community, so amplifying their nationally and
internationally visibility; and achieving, through a unified gateway, thus
avoiding a user to navigate and query across different information resources
individually. The work explains technological options and standards that will
assure interoperability in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210030</id><created>2002-10-30</created><authors><author><keyname>Suykens</keyname><forenames>J. A. K.</forenames></author><author><keyname>Vandewalle</keyname><forenames>J.</forenames></author><author><keyname>De Moor</keyname><forenames>B.</forenames></author></authors><title>Intelligence and Cooperative Search by Coupled Local Minimizers</title><categories>cs.AI cs.MA cs.NE</categories><comments>25 pages, 10 figures</comments><acm-class>F.1.1</acm-class><journal-ref>Int. J. Bifurcation and Chaos, Vol.11, No.8, pp.2133-2144, 2001</journal-ref><abstract>  We show how coupling of local optimization processes can lead to better
solutions than multi-start local optimization consisting of independent runs.
This is achieved by minimizing the average energy cost of the ensemble, subject
to synchronization constraints between the state vectors of the individual
local minimizers. From an augmented Lagrangian which incorporates the
synchronization constraints both as soft and hard constraints, a network is
derived wherein the local minimizers interact and exchange information through
the synchronization constraints. From the viewpoint of neural networks, the
array can be considered as a Lagrange programming network for continuous
optimization and as a cellular neural network (CNN). The penalty weights
associated with the soft state synchronization constraints follow from the
solution to a linear program. This expresses that the energy cost of the
ensemble should maximally decrease. In this way successful local minimizers can
implicitly impose their state to the others through a mechanism of master-slave
dynamics resulting into a cooperative search mechanism. Improved information
spreading within the ensemble is obtained by applying the concept of
small-world networks. This work suggests, in an interdisciplinary context, the
importance of information exchange and state synchronization within ensembles,
towards issues as evolution, collective behaviour, optimality and intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0210031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0210031</id><created>2002-10-30</created><authors><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author></authors><title>The Weaves Reconfigurable Programming Framework</title><categories>cs.PL cs.OS</categories><comments>To be submitted to ACM TOCS</comments><acm-class>D.2.11 D.2.12 D.1.3 D.3.2 D.3.4</acm-class><abstract>  This research proposes a language independent intra-process framework for
object based composition of unmodified code modules. Intuitively, the two major
programming models, threads and processes, can be considered as extremes along
a sharing axis. Multiple threads through a process share all global state,
whereas instances of a process (or independent processes) share no global
state. Weaves provide the generalized framework that allows arbitrary
(selective) sharing of state between multiple control flows through a process.
The Weaves framework supports multiple independent components in a single
process, with flexible state sharing and scheduling, all of which is achieved
without requiring any modification to existing code bases. Furthermore, the
framework allows dynamic instantiation of code modules and control flows
through them. In effect, weaves create intra-process modules (similar to
objects in OOP) from code written in any language. The Weaves paradigm allows
objects to be arbitrarily shared, it is a true superset of both processes as
well as threads, with code sharing and fast context switching time similar to
threads. Weaves does not require any special support from either the language
or application code, practically any code can be weaved. Weaves also include
support for fast automatic checkpointing and recovery with no application
support. This paper presents the elements of the Weaves framework and results
from our implementation that works by reverse-analyzing source-code independent
ELF object files. The current implementation has been validated over Sweep3D, a
benchmark for 3D discrete ordinates neutron transport [Koch et al., 1992], and
a user-level port of the Linux 2.4 family kernel TCP/IP protocol stack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211001</identifier>
 <datestamp>2011-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211001</id><created>2002-11-01</created><updated>2011-03-08</updated><authors><author><keyname>Greenberg</keyname><forenames>Ronald I.</forenames></author></authors><title>Fast and Simple Computation of All Longest Common Subsequences</title><categories>cs.DS</categories><comments>LaTeX 8 pages, 4 figures, corrected typos (especially in pseudocode
  in Figure 4)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that a simple algorithm produces the {\em
all-prefixes-LCSs-graph} in $O(mn)$ time for two input sequences of size $m$
and $n$. Given any prefix $p$ of the first input sequence and any prefix $q$ of
the second input sequence, all longest common subsequences (LCSs) of $p$ and
$q$ can be generated in time proportional to the output size, once the
all-prefixes-LCSs-graph has been constructed. The problem can be solved in the
context of generating all the distinct character strings that represent an LCS
or in the context of generating all ways of embedding an LCS in the two input
strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211002</id><created>2002-11-01</created><updated>2005-03-08</updated><authors><author><keyname>Pauly</keyname><forenames>Marc</forenames></author></authors><title>Programming and Verifying Subgame Perfect Mechanisms</title><categories>cs.LO cs.GT</categories><comments>26 pages, 3 figures A section has been added which applies the
  calculus to an auction mechanism</comments><acm-class>F.3.1; F.3.2; F.4.1; I.2.11</acm-class><abstract>  An extension of the WHILE-language is developed for programming
game-theoretic mechanisms involving multiple agents. Examples of such
mechanisms include auctions, voting procedures, and negotiation protocols. A
structured operational semantics is provided in terms of extensive games of
almost perfect information. Hoare-style partial correctness assertions are
proposed to reason about the correctness of these mechanisms, where correctness
is interpreted as the existence of a subgame perfect equilibrium. Using an
extensional approach to pre- and postconditions, we show that an extension of
Hoare's original calculus is sound and complete for reasoning about subgame
perfect equilibria in game-theoretic mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211003</id><created>2002-11-01</created><authors><author><keyname>Madden</keyname><forenames>Michael G.</forenames></author></authors><title>Evaluation of the Performance of the Markov Blanket Bayesian Classifier
  Algorithm</title><categories>cs.LG</categories><comments>9 pages: Technical Report No. NUIG-IT-011002, Department of
  Information Technology, National University of Ireland, Galway (2002)</comments><report-no>NUIG-IT-011002</report-no><acm-class>I.2.6</acm-class><abstract>  The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for
construction of probabilistic classifiers. This paper presents an empirical
comparison of the MBBC algorithm with three other Bayesian classifiers: Naive
Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these
are implemented using the K2 framework of Cooper and Herskovits. The
classifiers are compared in terms of their performance (using simple accuracy
measures and ROC curves) and speed, on a range of standard benchmark data sets.
It is concluded that MBBC is competitive in terms of speed and accuracy with
the other algorithms considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211004</identifier>
 <datestamp>2008-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211004</id><created>2002-11-04</created><updated>2003-09-10</updated><authors><author><keyname>Leone</keyname><forenames>Nicola</forenames></author><author><keyname>Pfeifer</keyname><forenames>Gerald</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Perri</keyname><forenames>Simona</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>The DLV System for Knowledge Representation and Reasoning</title><categories>cs.AI cs.LO cs.PL</categories><comments>56 pages, 9 figures, 6 tables</comments><acm-class>I.2.3; I.2.4; D.3.1</acm-class><journal-ref>ACM Transactions on Computational Logic 7(3):499-562, 2006</journal-ref><doi>10.1145/1149114.1149117</doi><abstract>  This paper presents the DLV system, which is widely considered the
state-of-the-art implementation of disjunctive logic programming, and addresses
several aspects. As for problem solving, we provide a formal definition of its
kernel language, function-free disjunctive logic programs (also known as
disjunctive datalog), extended by weak constraints, which are a powerful tool
to express optimization problems. We then illustrate the usage of DLV as a tool
for knowledge representation and reasoning, describing a new declarative
programming methodology which allows one to encode complex problems (up to
$\Delta^P_3$-complete problems) in a declarative fashion. On the foundational
side, we provide a detailed analysis of the computational complexity of the
language of DLV, and by deriving new complexity results we chart a complete
picture of the complexity of this language and important fragments thereof.
  Furthermore, we illustrate the general architecture of the DLV system which
has been influenced by these results. As for applications, we overview
application front-ends which have been developed on top of DLV to solve
specific knowledge representation tasks, and we briefly describe the main
international projects investigating the potential of the system for industrial
exploitation. Finally, we report about thorough experimentation and
benchmarking, which has been carried out to assess the efficiency of the
system. The experimental results confirm the solidity of DLV and highlight its
potential for emerging application areas like knowledge management and
information integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211005</id><created>2002-11-05</created><authors><author><keyname>Kettebekov</keyname><forenames>Sanshzar</forenames></author><author><keyname>Yeasin</keyname><forenames>Mohammed</forenames></author><author><keyname>Sharma</keyname><forenames>Rajeev</forenames></author></authors><title>Prosody Based Co-analysis for Continuous Recognition of Coverbal
  Gestures</title><categories>cs.CV cs.HC</categories><comments>Alternative see:
  http://vision.cse.psu.edu/kettebek/academ/publications.htm</comments><acm-class>H.5.2; I.5.4</acm-class><journal-ref>S. Kettebekov, M. Yeasin, and R. Sharma, &quot;Prosody Based
  Co-analysis for Continuous Recognition of Coverbal Gestures,&quot; presented at
  International Conference on Multimodal Interfaces (ICMI'02), Pittsburgh, USA,
  2002</journal-ref><abstract>  Although speech and gesture recognition has been studied extensively, all the
successful attempts of combining them in the unified framework were
semantically motivated, e.g., keyword-gesture cooccurrence. Such formulations
inherited the complexity of natural language processing. This paper presents a
Bayesian formulation that uses a phenomenon of gesture and speech articulation
for improving accuracy of automatic recognition of continuous coverbal
gestures. The prosodic features from the speech signal were coanalyzed with the
visual signal to learn the prior probability of co-occurrence of the prominent
spoken segments with the particular kinematical phases of gestures. It was
found that the above co-analysis helps in detecting and disambiguating visually
small gestures, which subsequently improves the rate of continuous gesture
recognition. The efficacy of the proposed approach was demonstrated on a large
database collected from the weather channel broadcast. This formulation opens
new avenues for bottom-up frameworks of multimodal integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211006</id><created>2002-11-07</created><authors><author><keyname>Akaho</keyname><forenames>Shotaro</forenames><affiliation>AIST Neuroscience Research Institute</affiliation></author></authors><title>Maximing the Margin in the Input Space</title><categories>cs.AI cs.LG</categories><comments>19 pages, 5 figures, NIPS workshop</comments><acm-class>I.2.6; I.5.1</acm-class><abstract>  We propose a novel criterion for support vector machine learning: maximizing
the margin in the input space, not in the feature (Hilbert) space. This
criterion is a discriminative version of the principal curve proposed by Hastie
et al. The criterion is appropriate in particular when the input space is
already a well-designed feature space with rather small dimensionality. The
definition of the margin is generalized in order to represent prior knowledge.
The derived algorithm consists of two alternating steps to estimate the dual
parameters. Firstly, the parameters are initialized by the original SVM. Then
one set of parameters is updated by Newton-like procedure, and the other set is
updated by solving a quadratic programming problem. The algorithm converges in
a few steps to a local optimum under mild conditions and it preserves the
sparsity of support vectors. Although the complexity to calculate temporal
variables increases the complexity to solve the quadratic programming problem
for each step does not change. It is also shown that the original SVM can be
seen as a special case. We further derive a simplified algorithm which enables
us to use the existing code for the original SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211007</id><created>2002-11-07</created><authors><author><keyname>Tsuda</keyname><forenames>Koji</forenames><affiliation>AIST</affiliation></author><author><keyname>Akaho</keyname><forenames>Shotaro</forenames><affiliation>AIST</affiliation></author><author><keyname>Asai</keyname><forenames>Kiyoshi</forenames><affiliation>AIST</affiliation></author></authors><title>Approximating Incomplete Kernel Matrices by the em Algorithm</title><categories>cs.LG</categories><comments>17 pages, 4 figures</comments><acm-class>I2.6; I5.2</acm-class><abstract>  In biological data, it is often the case that observed data are available
only for a subset of samples. When a kernel matrix is derived from such data,
we have to leave the entries for unavailable samples as missing. In this paper,
we make use of a parametric model of kernel matrices, and estimate missing
entries by fitting the model to existing entries. The parametric model is
created as a set of spectral variants of a complete kernel matrix derived from
another information source. For model fitting, we adopt the em algorithm based
on the information geometry of positive definite matrices. We will report
promising results on bacteria clustering experiments using two marker
sequences: 16S and gyrB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211008</id><created>2002-11-09</created><updated>2002-11-15</updated><authors><author><keyname>Eliashberg</keyname><forenames>Victor</forenames></author></authors><title>Can the whole brain be simpler than its &quot;parts&quot;?</title><categories>cs.AI</categories><comments>No figures</comments><report-no>AER0-02-10</report-no><acm-class>I.2.0</acm-class><abstract>  This is the first in a series of connected papers discussing the problem of a
dynamically reconfigurable universal learning neurocomputer that could serve as
a computational model for the whole human brain. The whole series is entitled
&quot;The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal
Learning Neurocomputer.&quot; (For more information visit the website
www.brain0.com.) This introductory paper is concerned with general methodology.
Its main goal is to explain why it is critically important for both neural
modeling and cognitive modeling to pay much attention to the basic requirements
of the whole brain as a complex computing system. The author argues that it can
be easier to develop an adequate computational model for the whole
&quot;unprogrammed&quot; (untrained) human brain than to find adequate formal
representations of some nontrivial parts of brain's performance. (In the same
way as, for example, it is easier to describe the behavior of a complex
analytical function than the behavior of its real and/or imaginary part.) The
&quot;curse of dimensionality&quot; that plagues purely phenomenological (&quot;brainless&quot;)
cognitive theories is a natural penalty for an attempt to represent
insufficiently large parts of brain's performance in a state space of
insufficiently high dimensionality. A &quot;partial&quot; modeler encounters &quot;Catch 22.&quot;
An attempt to simplify a cognitive problem by artificially reducing its
dimensionality makes the problem more difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211009</id><created>2002-11-11</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author><author><keyname>Sung</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Yiu</keyname><forenames>Siu-Ming</forenames></author></authors><title>Improved Phylogeny Comparisons: Non-Shared Edges Nearest Neighbor
  Interchanges, and Subtree Transfers</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><abstract>  The number of the non-shared edges of two phylogenies is a basic measure of
the dissimilarity between the phylogenies. The non-shared edges are also the
building block for approximating a more sophisticated metric called the nearest
neighbor interchange (NNI) distance. In this paper, we give the first
subquadratic-time algorithm for finding the non-shared edges, which are then
used to speed up the existing approximating algorithm for the NNI distance from
$O(n^2)$ time to $O(n \log n)$ time. Another popular distance metric for
phylogenies is the subtree transfer (STT) distance. Previous work on computing
the STT distance considered degree-3 trees only. We give an approximation
algorithm for the STT distance for degree-$d$ trees with arbitrary $d$ and with
generalized STT operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211010</id><created>2002-11-11</created><updated>2004-07-28</updated><authors><author><keyname>Alstrup</keyname><forenames>Stephen</forenames></author><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Farach-Colton</keyname><forenames>Martin</forenames></author><author><keyname>Rauhe</keyname><forenames>Theis</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Efficient Tree Layout in a Multilevel Memory Hierarchy</title><categories>cs.DS</categories><comments>18 pages. Version 2 adds faster dynamic programs. Preliminary version
  appeared in European Symposium on Algorithms, 2002</comments><acm-class>E.1; F.2.2</acm-class><abstract>  We consider the problem of laying out a tree with fixed parent/child
structure in hierarchical memory. The goal is to minimize the expected number
of block transfers performed during a search along a root-to-leaf path, subject
to a given probability distribution on the leaves. This problem was previously
considered by Gil and Itai, who developed optimal but slow algorithms when the
block-transfer size B is known. We present faster but approximate algorithms
for the same problem; the fastest such algorithm runs in linear time and
produces a solution that is within an additive constant of optimal.
  In addition, we show how to extend any approximately optimal algorithm to the
cache-oblivious setting in which the block-transfer size is unknown to the
algorithm. The query performance of the cache-oblivious layout is within a
constant factor of the query performance of the optimal known-block-size
layout. Computing the cache-oblivious layout requires only logarithmically many
calls to the layout algorithm for known block size; in particular, the
cache-oblivious layout can be computed in O(N lg N) time, where N is the number
of nodes.
  Finally, we analyze two greedy strategies, and show that they have a
performance ratio between Omega(lg B / lg lg B) and O(lg B) when compared to
the optimal layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211011</id><created>2002-11-12</created><authors><author><keyname>Dezani-Ciancaglini</keyname><forenames>M.</forenames></author><author><keyname>Lusin</keyname><forenames>S.</forenames></author></authors><title>Intersection Types and Lambda Theories</title><categories>cs.LO</categories><acm-class>F.3.2</acm-class><abstract>  We illustrate the use of intersection types as a semantic tool for showing
properties of the lattice of lambda theories. Relying on the notion of easy
intersection type theory we successfully build a filter model in which the
interpretation of an arbitrary simple easy term is any filter which can be
described in an uniform way by a predicate. This allows us to prove the
consistency of a well-know lambda theory: this consistency has interesting
consequences on the algebraic structure of the lattice of lambda theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211012</id><created>2002-11-12</created><updated>2005-04-10</updated><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Phase Transitions and all that</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><abstract>  The paper (as posted originally) contains several errors. It has been
subsequently split into two papers, the corrected (and accepted for
publication) versions appear in the archive as papers cs.CC/0503082 and
cs.DM/0503083.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211013</id><created>2002-11-12</created><authors><author><keyname>Kolakowska</keyname><forenames>A.</forenames></author><author><keyname>Novotny</keyname><forenames>M. A.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Algorithmic scalability in globally constrained conservative parallel
  discrete event simulations of asynchronous systems</title><categories>cs.DC cond-mat.stat-mech cs.DS physics.comp-ph</categories><comments>14 pages, 11 figures</comments><acm-class>D.1.3; F.1.1; F.2.0; G.3</acm-class><journal-ref>Phys. Rev. E 67, 046703 (2003)</journal-ref><doi>10.1103/PhysRevE.67.046703</doi><abstract>  We consider parallel simulations for asynchronous systems employing L
processing elements that are arranged on a ring. Processors communicate only
among the nearest neighbors and advance their local simulated time only if it
is guaranteed that this does not violate causality. In simulations with no
constraints, in the infinite L-limit the utilization scales (Korniss et al, PRL
84, 2000); but, the width of the virtual time horizon diverges (i.e., the
measurement phase of the algorithm does not scale). In this work, we introduce
a moving window global constraint, which modifies the algorithm so that the
measurement phase scales as well. We present results of systematic studies in
which the system size (i.e., L and the volume load per processor) as well as
the constraint are varied. The constraint eliminates the extreme fluctuations
in the virtual time horizon, provides a bound on its width, and controls the
average progress rate. The width of the window constraint can serve as a tuning
parameter that, for a given volume load per processor, could be adjusted to
optimize the utilization so as to maximize the efficiency. This result may find
numerous applications in modeling the evolution of general spatially extended
short-range interacting systems with asynchronous dynamics, including dynamic
Monte Carlo studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211014</id><created>2002-11-13</created><authors><author><keyname>Wos</keyname><forenames>Larry</forenames></author><author><keyname>Ulrich</keyname><forenames>Dolph</forenames></author><author><keyname>Fitelson</keyname><forenames>Branden</forenames></author></authors><title>Vanquishing the XCB Question: The Methodology Discovery of the Last
  Shortest Single Axiom for the Equivalential Calculus</title><categories>cs.LO cs.AI</categories><comments>21 pages, no figures</comments><report-no>Preprint ANL/MCS-P971-0702</report-no><acm-class>F.4.1; I.2.3</acm-class><abstract>  With the inclusion of an effective methodology, this article answers in
detail a question that, for a quarter of a century, remained open despite
intense study by various researchers. Is the formula XCB =
e(x,e(e(e(x,y),e(z,y)),z)) a single axiom for the classical equivalential
calculus when the rules of inference consist of detachment (modus ponens) and
substitution? Where the function e represents equivalence, this calculus can be
axiomatized quite naturally with the formulas e(x,x), e(e(x,y),e(y,x)), and
e(e(x,y),e(e(y,z),e(x,z))), which correspond to reflexivity, symmetry, and
transitivity, respectively. (We note that e(x,x) is dependent on the other two
axioms.) Heretofore, thirteen shortest single axioms for classical equivalence
of length eleven had been discovered, and XCB was the only remaining formula of
that length whose status was undetermined. To show that XCB is indeed such a
single axiom, we focus on the rule of condensed detachment, a rule that
captures detachment together with an appropriately general, but restricted,
form of substitution. The proof we present in this paper consists of
twenty-five applications of condensed detachment, completing with the deduction
of transitivity followed by a deduction of symmetry. We also discuss some
factors that may explain in part why XCB resisted relinquishing its treasure
for so long. Our approach relied on diverse strategies applied by the automated
reasoning program OTTER. Thus ends the search for shortest single axioms for
the equivalential calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211015</id><created>2002-11-13</created><authors><author><keyname>Wos</keyname><forenames>Larry</forenames></author><author><keyname>Ulrich</keyname><forenames>Dolph</forenames></author><author><keyname>Fitelson</keyname><forenames>Branden</forenames></author></authors><title>XCB, the Last of the Shortest Single Axioms for the Classical
  Equivalential Calculus</title><categories>cs.LO cs.AI</categories><comments>6 pages, no figures</comments><report-no>ANL/MCS-P966-0602</report-no><acm-class>F.4.1; I.2.3</acm-class><abstract>  It has long been an open question whether the formula XCB = EpEEEpqErqr is,
with the rules of substitution and detachment, a single axiom for the classical
equivalential calculus. This paper answers that question affirmatively, thus
completing a search for all such eleven-symbol single axioms that began seventy
years ago.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211016</identifier>
 <datestamp>2007-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211016</id><created>2002-11-14</created><updated>2007-12-06</updated><authors><author><keyname>Ratschan</keyname><forenames>Stefan</forenames></author></authors><title>Efficient Solving of Quantified Inequality Constraints over the Real
  Numbers</title><categories>cs.LO cs.NA</categories><acm-class>F.4.1; G.1.0; I.2.3</acm-class><abstract>  Let a quantified inequality constraint over the reals be a formula in the
first-order predicate language over the structure of the real numbers, where
the allowed predicate symbols are $\leq$ and $&lt;$. Solving such constraints is
an undecidable problem when allowing function symbols such $\sin$ or $\cos$. In
the paper we give an algorithm that terminates with a solution for all, except
for very special, pathological inputs. We ensure the practical efficiency of
this algorithm by employing constraint programming techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211017</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211017</id><created>2002-11-14</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames></author></authors><title>Probabilistic Parsing Strategies</title><categories>cs.CL</categories><comments>36 pages, 1 figure</comments><acm-class>F.4.3; I.2.7</acm-class><abstract>  We present new results on the relation between purely symbolic context-free
parsing strategies and their probabilistic counter-parts. Such parsing
strategies are seen as constructions of push-down devices from grammars. We
show that preservation of probability distribution is possible under two
conditions, viz. the correct-prefix property and the property of strong
predictiveness. These results generalize existing results in the literature
that were obtained by considering parsing strategies in isolation. From our
general results we also derive negative results on so-called generalized LR
parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211018</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211018</id><created>2002-11-14</created><updated>2005-10-13</updated><authors><author><keyname>Pestov</keyname><forenames>Vladimir</forenames></author><author><keyname>Stojmirovic</keyname><forenames>Aleksandar</forenames></author></authors><title>Indexing schemes for similarity search: an illustrated paradigm</title><categories>cs.DS</categories><comments>19 pages, LaTeX with 8 figures, prepared using Fundamenta
  Informaticae style file</comments><acm-class>H.3.3</acm-class><journal-ref>Fundamenta Informaticae Vol. 70 (2006), No. 4, 367-385</journal-ref><abstract>  We suggest a variation of the Hellerstein--Koutsoupias--Papadimitriou
indexability model for datasets equipped with a similarity measure, with the
aim of better understanding the structure of indexing schemes for
similarity-based search and the geometry of similarity workloads. This in
particular provides a unified approach to a great variety of schemes used to
index into metric spaces and facilitates their transfer to more general
similarity measures such as quasi-metrics. We discuss links between performance
of indexing schemes and high-dimensional geometry. The concepts and results are
illustrated on a very large concrete dataset of peptide fragments equipped with
a biologically significant similarity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211019</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211019</id><created>2002-11-15</created><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Brand</keyname><forenames>Sebastian</forenames></author></authors><title>Schedulers for Rule-based Constraint Programming</title><categories>cs.DS cs.PL</categories><comments>8 pages. To appear in Proc. ACM Symposium on Applied Computing (SAC)
  2003</comments><acm-class>I.2.2; I.2.3; D.3.3; D.3.4</acm-class><abstract>  We study here schedulers for a class of rules that naturally arise in the
context of rule-based constraint programming. We systematically derive a
scheduler for them from a generic iteration algorithm of Apt [2000]. We apply
this study to so-called membership rules of Apt and Monfroy [2001]. This leads
to an implementation that yields for these rules a considerably better
performance than their execution as standard CHR rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211020</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211020</id><created>2002-11-15</created><updated>2003-10-07</updated><authors><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Monadic Datalog and the Expressive Power of Languages for Web
  Information Extraction</title><categories>cs.DB</categories><comments>40 pages, 3 figures, journal version of PODS 2002 paper, to appear in
  JACM</comments><acm-class>F.1.1; F.4.1; F.4.3; H.2.3; I.7.2</acm-class><abstract>  Research on information extraction from Web pages (wrapping) has seen much
activity recently (particularly systems implementations), but little work has
been done on formally studying the expressiveness of the formalisms proposed or
on the theoretical foundations of wrapping. In this paper, we first study
monadic datalog over trees as a wrapping language. We show that this simple
language is equivalent to monadic second order logic (MSO) in its ability to
specify wrappers. We believe that MSO has the right expressiveness required for
Web information extraction and propose MSO as a yardstick for evaluating and
comparing wrappers. Along the way, several other results on the complexity of
query evaluation and query containment for monadic datalog over trees are
established, and a simple normal form for this language is presented. Using the
above results, we subsequently study the kernel fragment Elog$^-$ of the Elog
wrapping language used in the Lixto system (a visual wrapper generator).
Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,
programs in this language can be entirely visually specified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211021</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211021</id><created>2002-11-18</created><authors><author><keyname>Metcalfe</keyname><forenames>G.</forenames></author><author><keyname>Olivetti</keyname><forenames>N.</forenames></author><author><keyname>Gabbay</keyname><forenames>D.</forenames></author></authors><title>Sequent and Hypersequent Calculi for Abelian and Lukasiewicz Logics</title><categories>cs.LO</categories><comments>35 pages, 1 figure</comments><acm-class>F.4.1;I.2.3</acm-class><abstract>  We present two embeddings of infinite-valued Lukasiewicz logic L into Meyer
and Slaney's abelian logic A, the logic of lattice-ordered abelian groups. We
give new analytic proof systems for A and use the embeddings to derive
corresponding systems for L. These include: hypersequent calculi for A and L
and terminating versions of these calculi; labelled single sequent calculi for
A and L of complexity co-NP; unlabelled single sequent calculi for A and L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211022</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211022</id><created>2002-11-19</created><authors><author><keyname>Schweikardt</keyname><forenames>Nicole</forenames></author></authors><title>Arithmetic, First-Order Logic, and Counting Quantifiers</title><categories>cs.LO</categories><comments>39 pages, 5 figures</comments><acm-class>F.4.1</acm-class><abstract>  This paper gives a thorough overview of what is known about first-order logic
with counting quantifiers and with arithmetic predicates. As a main theorem we
show that Presburger arithmetic is closed under unary counting quantifiers.
Precisely, this means that for every first-order formula phi(y,z_1,...,z_k)
over the signature {&lt;,+} there is a first-order formula psi(x,z_1,...,z_k)
which expresses over the structure &lt;Nat,&lt;,+&gt; (respectively, over initial
segments of this structure) that the variable x is interpreted exactly by the
number of possible interpretations of the variable y for which the formula
phi(y,z_1,...,z_k) is satisfied. Applying this theorem, we obtain an easy proof
of Ruhl's result that reachability (and similarly, connectivity) in finite
graphs is not expressible in first-order logic with unary counting quantifiers
and addition. Furthermore, the above result on Presburger arithmetic helps to
show the failure of a particular version of the Crane Beach conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211023</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211023</id><created>2002-11-19</created><authors><author><keyname>Malik</keyname><forenames>Tanu</forenames></author><author><keyname>Szalay</keyname><forenames>Alex S.</forenames></author><author><keyname>Budavari</keyname><forenames>Tamas</forenames></author><author><keyname>Thakar</keyname><forenames>Ani R.</forenames></author></authors><title>SkyQuery: A WebService Approach to Federate Databases</title><categories>cs.DB cs.CE</categories><comments>9 pages, 3 Figures, To Appear in CIDR'03, Also at
  http://www.skyquery.net</comments><acm-class>J.2 ;H.3.5;H.2.8 ;H.2.5</acm-class><abstract>  Traditional science searched for new objects and phenomena that led to
discoveries. Tomorrow's science will combine together the large pool of
information in scientific archives and make discoveries. Scienthists are
currently keen to federate together the existing scientific databases. The
major challenge in building a federation of these autonomous and heterogeneous
databases is system integration. Ineffective integration will result in defunct
federations and under utilized scientific data.
  Astronomy, in particular, has many autonomous archives spread over the
Internet. It is now seeking to federate these, with minimal effort, into a
Virtual Observatory that will solve complex distributed computing tasks such as
answering federated spatial join queries.
  In this paper, we present SkyQuery, a successful prototype of an evolving
federation of astronomy archives. It interoperates using the emerging Web
services standard. We describe the SkyQuery architecture and show how it
efficiently evaluates a probabilistic federated spatial join query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211024</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211024</id><created>2002-11-20</created><authors><author><keyname>Giuli</keyname><forenames>TJ</forenames></author><author><keyname>Baker</keyname><forenames>Mary</forenames></author></authors><title>Narses: A Scalable Flow-Based Network Simulator</title><categories>cs.PF cs.NI</categories><comments>6 pages, 4 figures</comments><acm-class>I.6.8; I.6.5; I.6.3; C.2.2</acm-class><abstract>  Most popular, modern network simulators, such as ns, are targeted towards
simulating low-level protocol details. These existing simulators are not
intended for simulating large distributed applications with many hosts and many
concurrent connections over long periods of simulated time. We introduce a new
simulator, Narses, targeted towards large distributed applications. The goal of
Narses is to simulate and validate large applications efficiently using network
models of varying levels of detail. We introduce several simplifying
assumptions that allow our simulator to scale to the needs of large distributed
applications while maintaining a reasonable degree of accuracy. Initial results
show up to a 45 times speedup while consuming 28% of the memory used by ns.
Narses maintains a reasonable degree of accuracy -- within 8% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211025</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211025</id><created>2002-11-20</created><updated>2003-01-05</updated><authors><author><keyname>Athreya</keyname><forenames>Krishna B.</forenames></author><author><keyname>Hitchcock</keyname><forenames>John M.</forenames></author><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Mayordomo</keyname><forenames>Elvira</forenames></author></authors><title>Effective Strong Dimension, Algorithmic Information, and Computational
  Complexity</title><categories>cs.CC</categories><comments>35 pages</comments><acm-class>F.1.3</acm-class><abstract>  The two most important notions of fractal dimension are {\it Hausdorff
dimension}, developed by Hausdorff (1919), and {\it packing dimension},
developed by Tricot (1982).
  Lutz (2000) has recently proven a simple characterization of Hausdorff
dimension in terms of {\it gales}, which are betting strategies that generalize
martingales. Imposing various computability and complexity constraints on these
gales produces a spectrum of effective versions of Hausdorff dimension.
  In this paper we show that packing dimension can also be characterized in
terms of gales. Moreover, even though the usual definition of packing dimension
is considerably more complex than that of Hausdorff dimension, our gale
characterization of packing dimension is an exact dual of -- and every bit as
simple as -- the gale characterization of Hausdorff dimension.
  Effectivizing our gale characterization of packing dimension produces a
variety of {\it effective strong dimensions}, which are exact duals of the
effective dimensions mentioned above.
  We develop the basic properties of effective strong dimensions and prove a
number of results relating them to fundamental aspects of randomness,
Kolmogorov complexity, prediction, Boolean circuit-size complexity,
polynomial-time degrees, and data compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211026</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211026</id><created>2002-11-21</created><updated>2003-01-25</updated><authors><author><keyname>Yaneff</keyname><forenames>A. G.</forenames></author></authors><title>How long is a Proof? - A short note</title><categories>cs.CC</categories><comments>Withdrawn</comments><acm-class>F2.0</acm-class><abstract>  Withdrawn. Silly notion and out of context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211027</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211027</id><created>2002-11-21</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Adaptive Development of Koncepts in Virtual Animats: Insights into the
  Development of Knowledge</title><categories>cs.AI</categories><comments>15 pages, COGS Adaptive Systems Essay</comments><acm-class>I.2.6</acm-class><abstract>  As a part of our effort for studying the evolution and development of
cognition, we present results derived from synthetic experimentations in a
virtual laboratory where animats develop koncepts adaptively and ground their
meaning through action. We introduce the term &quot;koncept&quot; to avoid confusions and
ambiguity derived from the wide use of the word &quot;concept&quot;. We present the
models which our animats use for abstracting koncepts from perceptions,
plastically adapt koncepts, and associate koncepts with actions. On a more
philosophical vein, we suggest that knowledge is a property of a cognitive
system, not an element, and therefore observer-dependent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211028</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211028</id><created>2002-11-21</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Gonzalez</keyname><forenames>Pedro Pablo</forenames></author><author><keyname>Negrete</keyname><forenames>Jose</forenames></author></authors><title>Thinking Adaptive: Towards a Behaviours Virtual Laboratory</title><categories>cs.AI cs.MA</categories><comments>9 pages, SAB 2000 Proceedings Supplement. Paris, France</comments><acm-class>I.2.0, I.6.7</acm-class><abstract>  In this paper we name some of the advantages of virtual laboratories; and
propose that a Behaviours Virtual Laboratory should be useful for both
biologists and AI researchers, offering a new perspective for understanding
adaptive behaviour. We present our development of a Behaviours Virtual
Laboratory, which at this stage is focused in action selection, and show some
experiments to illustrate the properties of our proposal, which can be accessed
via Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211029</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211029</id><created>2002-11-22</created><authors><author><keyname>Perez</keyname><forenames>Pedro Pablo Gonzalez</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Garcia</keyname><forenames>Maura Cardenas</forenames></author><author><keyname>Otero</keyname><forenames>Jaime Lagunez</forenames></author></authors><title>Modelling intracellular signalling networks using behaviour-based
  systems and the blackboard architecture</title><categories>cs.MA q-bio.CB</categories><comments>7 pages, Proceedings of the International Conference: Mathematics and
  Computers in Biology and Chemistry (MCBC 2000), Montego Bay, Jamaica</comments><acm-class>J.3</acm-class><abstract>  This paper proposes to model the intracellular signalling networks using a
fusion of behaviour-based systems and the blackboard architecture. In virtue of
this fusion, the model developed by us, which has been named Cellulat, allows
to take account two essential aspects of the intracellular signalling networks:
(1) the cognitive capabilities of certain types of networks' components and (2)
the high level of spatial organization of these networks. A simple example of
modelling of Ca2+ signalling pathways using Cellulat is presented here. An
intracellular signalling virtual laboratory is being developed from Cellulat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211030</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211030</id><created>2002-11-22</created><authors><author><keyname>Perez</keyname><forenames>Pedro Pablo Gonzalez</forenames></author><author><keyname>Garcia</keyname><forenames>Maura Cardenas</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Lagunez-Otero</keyname><forenames>Jaime</forenames></author></authors><title>Integration of Computational Techniques for the Modelling of Signal
  Transduction</title><categories>cs.MA q-bio.CB</categories><comments>12 pages, In N.E. Mastorakis and L.A. Pecorelli-Peres (Eds.) Advances
  in Systems Science: Measurement, Circuits and Control. WSES Press</comments><acm-class>J.3</acm-class><abstract>  A cell can be seen as an adaptive autonomous agent or as a society of
adaptive autonomous agents, where each can exhibit a particular behaviour
depending on its cognitive capabilities. We present an intracellular signalling
model obtained by integrating several computational techniques into an
agent-based paradigm. Cellulat, the model, takes into account two essential
aspects of the intracellular signalling networks: cognitive capacities and a
spatial organization. Exemplifying the functionality of the system by modelling
the EGFR signalling pathway, we discuss the methodology as well as the purposes
of an intracellular signalling virtual laboratory, presently under development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211031</identifier>
 <datestamp>2007-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211031</id><created>2002-11-22</created><authors><author><keyname>Liberatore</keyname><forenames>Paolo</forenames></author></authors><title>Redundancy in Logic I: CNF Propositional Formulae</title><categories>cs.AI cs.CC</categories><comments>Extended and revised version of a paper that has been presented at
  ECAI 2002</comments><acm-class>I.2.4, F.1.3</acm-class><doi>10.1016/j.artint.2004.11.002</doi><abstract>  A knowledge base is redundant if it contains parts that can be inferred from
the rest of it. We study the problem of checking whether a CNF formula (a set
of clauses) is redundant, that is, it contains clauses that can be derived from
the other ones. Any CNF formula can be made irredundant by deleting some of its
clauses: what results is an irredundant equivalent subset (I.E.S.) We study the
complexity of some related problems: verification, checking existence of a
I.E.S. with a given size, checking necessary and possible presence of clauses
in I.E.S.'s, and uniqueness. We also consider the problem of redundancy with
different definitions of equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211032</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211032</id><created>2002-11-25</created><updated>2003-01-25</updated><authors><author><keyname>Yaneff</keyname><forenames>A. G.</forenames></author></authors><title>Solution Bounds for a Hypothetical Polynomial Time Aproximation
  Algorithm for the TSP</title><categories>cs.CC</categories><comments>2 pages</comments><acm-class>F.2.2; G.2.2</acm-class><abstract>  Bounds for the optimal tour length for a hypothetical TSP algorithm are
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211033</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211033</id><created>2002-11-25</created><authors><author><keyname>East</keyname><forenames>Deborah</forenames></author><author><keyname>Truszczynski</keyname><forenames>Miroslaw</forenames></author></authors><title>Propositional satisfiability in declarative programming</title><categories>cs.LO cs.AI</categories><comments>34 pages, 4 tables; extended version of papers that appeared in
  Proceedings of AAAI-2000 and Proceedings of KI-2001</comments><acm-class>F.4.1;I.2.3;D.1.6</acm-class><abstract>  Answer-set programming (ASP) paradigm is a way of using logic to solve search
problems. Given a search problem, to solve it one designs a theory in the logic
so that models of this theory represent problem solutions. To compute a
solution to a problem one needs to compute a model of the corresponding theory.
Several answer-set programming formalisms have been developed on the basis of
logic programming with the semantics of stable models. In this paper we show
that also the logic of predicate calculus gives rise to effective
implementations of the ASP paradigm, similar in spirit to logic programming
with stable model semantics and with a similar scope of applicability.
Specifically, we propose two logics based on predicate calculus as formalisms
for encoding search problems. We show that the expressive power of these logics
is given by the class NP-search. We demonstrate how to use them in programming
and develop computational tools for model finding. In the case of one of the
logics our techniques reduce the problem to that of propositional
satisfiability and allow one to use off-the-shelf satisfiability solvers. The
language of the other logic has more complex syntax and provides explicit means
to model some high-level constraints. For theories in this logic, we designed
our own solver that takes advantage of the expanded syntax. We present
experimental results demonstrating computational effectiveness of the overall
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211035</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211035</id><created>2002-11-25</created><authors><author><keyname>Chauvet</keyname><forenames>Jean-Marie</forenames></author></authors><title>Monadic Style Control Constructs for Inference Systems</title><categories>cs.AI cs.PL</categories><comments>25 pages</comments><report-no>DD-2002-1</report-no><acm-class>68Q55</acm-class><abstract>  Recent advances in programming languages study and design have established a
standard way of grounding computational systems representation in category
theory. These formal results led to a better understanding of issues of control
and side-effects in functional and imperative languages. Another benefit is a
better way of modelling computational effects in logical frameworks. With this
analogy in mind, we embark on an investigation of inference systems based on
considering inference behaviour as a form of computation. We delineate a
categorical formalisation of control constructs in inference systems. This
representation emphasises the parallel between the modular articulation of the
categorical building blocks (triples) used to account for the inference
architecture and the modular composition of cognitive processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211036</id><created>2002-11-26</created><authors><author><keyname>Dubois</keyname><forenames>Olivier</forenames></author><author><keyname>Boufkhad</keyname><forenames>Yacine</forenames></author><author><keyname>Mandler</keyname><forenames>Jacques</forenames></author></authors><title>Typical random 3-SAT formulae and the satisfiability threshold</title><categories>cs.DM cs.CC</categories><acm-class>G.2.1</acm-class><abstract>  We present a new structural (or syntatic) approach for estimating the
satisfiability threshold of random 3-SAT formulae. We show its efficiency in
obtaining a jump from the previous upper bounds, lowering them to 4.506. The
method combines well with other techniques, and also applies to other problems,
such as the 3-colourability of random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211037</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211037</id><created>2002-11-26</created><authors><author><keyname>Shao</keyname><forenames>Zhiyan</forenames></author><author><keyname>Capra</keyname><forenames>Robert</forenames></author><author><keyname>Perez-Quinones</keyname><forenames>Manuel A.</forenames></author></authors><title>Annotations for HTML to VoiceXML Transcoding: Producing Voice WebPages
  with Usability in Mind</title><categories>cs.HC</categories><comments>8 pages</comments><acm-class>H.1.2, H.5.2</acm-class><abstract>  Web pages contain a large variety of information, but are largely designed
for use by graphical web browsers. Mobile access to web-based information often
requires presenting HTML web pages using channels that are limited in their
graphical capabilities such as small-screens or audio-only interfaces. Content
transcoding and annotations have been explored as methods for intelligently
presenting HTML documents. Much of this work has focused on transcoding for
small-screen devices such as are found on PDAs and cell phones. Here, we focus
on the use of annotations and transcoding for presenting HTML content through a
voice user interface instantiated in VoiceXML. This transcoded voice interface
is designed with an assumption that it will not be used for extended web
browsing by voice, but rather to quickly gain directed access to information on
web pages. We have found repeated structures that are common in the
presentation of data on web pages that are well suited for voice presentation
and navigation. In this paper, we describe these structures and their use in an
annotation system we have implemented that produces a VoiceXML interface to
information originally embedded in HTML documents. We describe the transcoding
process used to translate HTML into VoiceXML, including transcoding features we
have designed to lead to highly usable VoiceXML code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211038</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211038</id><created>2002-11-27</created><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Gonzalez</keyname><forenames>Pedro Pablo</forenames></author></authors><title>Dynamic Adjustment of the Motivation Degree in an Action Selection
  Mechanism</title><categories>cs.AI</categories><comments>7 pages, Proceedings of ISA '2000. Wollongong, Australia</comments><acm-class>I.2.6</acm-class><abstract>  This paper presents a model for dynamic adjustment of the motivation degree,
using a reinforcement learning approach, in an action selection mechanism
previously developed by the authors. The learning takes place in the
modification of a parameter of the model of combination of internal and
external stimuli. Experiments that show the claimed properties are presented,
using a VR simulation developed for such purposes. The importance of adaptation
by learning in action selection is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211039</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211039</id><created>2002-11-27</created><authors><author><keyname>Garcia</keyname><forenames>Carlos Gershenson</forenames></author><author><keyname>Perez</keyname><forenames>Pedro Pablo Gonzalez</forenames></author><author><keyname>Martinez</keyname><forenames>Jose Negrete</forenames></author></authors><title>Action Selection Properties in a Software Simulated Agent</title><categories>cs.AI</categories><comments>12 pages, in MICAI 2000: Advances in Artificial Intelligence. Lecture
  Notes in Artificial Intelligence 1793, pp. 634-648. Springer-Verlag</comments><acm-class>I.2.9</acm-class><journal-ref># MICAI 2000: Advances in Artificial Intelligence. Lecture Notes
  in Artificial Intelligence 1793, pp. 634-648. Springer-Verlag</journal-ref><abstract>  This article analyses the properties of the Internal Behaviour network, an
action selection mechanism previously proposed by the authors, with the aid of
a simulation developed for such ends. A brief review of the Internal Behaviour
network is followed by the explanation of the implementation of the simulation.
Then, experiments are presented and discussed analysing the properties of the
action selection in the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211040</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211040</id><created>2002-11-27</created><authors><author><keyname>Perez</keyname><forenames>Pedro Pablo Gonzalez</forenames></author><author><keyname>Martinez</keyname><forenames>Jose Negrete</forenames></author><author><keyname>Garcia</keyname><forenames>Ariel Barreiro</forenames></author><author><keyname>Garcia</keyname><forenames>Carlos Gershenson</forenames></author></authors><title>A Model for Combination of External and Internal Stimuli in the Action
  Selection of an Autonomous Agent</title><categories>cs.AI</categories><comments>13 pages, in MICAI 2000: Advances in Artificial Intelligence. Lecture
  Notes in Artificial Intelligence 1793, pp. 621-633. Springer-Verlag</comments><acm-class>I.2.9</acm-class><journal-ref>MICAI 2000: Advances in Artificial Intelligence. Lecture Notes in
  Artificial Intelligence 1793, pp. 621-633. Springer-Verlag</journal-ref><abstract>  This paper proposes a model for combination of external and internal stimuli
for the action selection in an autonomous agent, based in an action selection
mechanism previously proposed by the authors. This combination model includes
additive and multiplicative elements, which allows to incorporate new
properties, which enhance the action selection. A given parameter a, which is
part of the proposed model, allows to regulate the degree of dependence of the
observed external behaviour from the internal states of the entity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211041</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211041</id><created>2002-11-28</created><authors><author><keyname>Averin</keyname><forenames>A. V.</forenames><affiliation>NSI, Moscow</affiliation></author><author><keyname>Vassilevskaya</keyname><forenames>L. A.</forenames><affiliation>DESY, Hamburg</affiliation></author></authors><title>An Approach to Automatic Indexing of Scientific Publications in High
  Energy Physics for Database SPIRES HEP</title><categories>cs.IR cs.DL</categories><comments>23 pages, 4 figures</comments><report-no>DESY L-02-02 (November 2002)</report-no><acm-class>H.3.1; H.3.2; H.3.6; H.3.7</acm-class><abstract>  We introduce an approach to automatic indexing of e-prints based on a
pattern-matching technique making extensive use of an Associative Patterns
Dictionary (APD), developed by us. Entries in the APD consist of natural
language phrases with the same semantic interpretation as a set of keywords
from a controlled vocabulary. The method also allows to recognize within
e-prints formulae written in TeX notations that might also appear as keywords.
We present an automatic indexing system, AUTEX, which we have applied to
keyword index e-prints in selected areas in high energy physics (HEP) making
use of the DESY-HEPI thesaurus as a controlled vocabulary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0211042</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0211042</id><created>2002-11-28</created><authors><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Schwind</keyname><forenames>Camilla</forenames></author></authors><title>Database Repairs and Analytic Tableaux</title><categories>cs.DB cs.LO</categories><comments>Extended version of paper appeared in Proc. FOIKS02. Submitted by
  invitation to AMAI journal. Uses packages: llncs.cls, amssymb.sty,
  parsetree.sty. 31 pages</comments><acm-class>H2; F4; I2</acm-class><abstract>  In this article, we characterize in terms of analytic tableaux the repairs of
inconsistent relational databases, that is databases that do not satisfy a
given set of integrity constraints. For this purpose we provide closing and
opening criteria for branches in tableaux that are built for database instances
and their integrity constraints. We use the tableaux based characterization as
a basis for consistent query answering, that is for retrieving from the
database answers to queries that are consistent wrt the integrity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212001</id><created>2002-12-03</created><authors><author><keyname>Fekete</keyname><forenames>Sandor P.</forenames></author><author><keyname>Fleischer</keyname><forenames>Rudolf</forenames></author><author><keyname>Fraenkel</keyname><forenames>Aviezri</forenames></author><author><keyname>Schmitt</keyname><forenames>Matthias</forenames></author></authors><title>Traveling Salesmen in the Presence of Competition</title><categories>cs.CC</categories><comments>23 pages, 9 figures, Latex, to appear in Theoretical Computer Science</comments><acm-class>F.2.2</acm-class><journal-ref>Theoretical Computer Science, 313 (2004), 377-392.</journal-ref><abstract>  We propose the ``Competing Salesmen Problem'' (CSP), a 2-player competitive
version of the classical Traveling Salesman Problem. This problem arises when
considering two competing salesmen instead of just one. The concern for a
shortest tour is replaced by the necessity to reach any of the customers before
the opponent does. In particular, we consider the situation where players take
turns, moving along one edge at a time within a graph G=(V,E). The set of
customers is given by a subset V_C V of the vertices. At any given time, both
players know of their opponent's position. A player wins if he is able to reach
a majority of the vertices in V_C before the opponent does. We prove that the
CSP is PSPACE-complete, even if the graph is bipartite, and both players start
at distance 2 from each other. We show that the starting player may lose the
game, even if both players start from the same vertex. For bipartite graphs, we
show that the starting player always can avoid a loss. We also show that the
second player can avoid to lose by more than one customer, when play takes
place on a graph that is a tree T, and V_C consists of leaves of T. For the
case where T is a star and V_C consists of n leaves of T, we give a simple and
fast strategy which is optimal for both players. If V_C consists not only of
leaves, the situation is more involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212002</id><created>2002-12-04</created><updated>2006-04-04</updated><authors><author><keyname>Braunstein</keyname><forenames>A.</forenames></author><author><keyname>Mezard</keyname><forenames>M.</forenames></author><author><keyname>Zecchina</keyname><forenames>R.</forenames></author></authors><title>Survey propagation: an algorithm for satisfiability</title><categories>cs.CC cond-mat.stat-mech</categories><comments>19 pages, 6 figure</comments><acm-class>G.3</acm-class><journal-ref>Random Structures and Algorithms 27, 201-226 (2005)</journal-ref><abstract>  We study the satisfiability of randomly generated formulas formed by $M$
clauses of exactly $K$ literals over $N$ Boolean variables. For a given value
of $N$ the problem is known to be most difficult with $\alpha=M/N$ close to the
experimental threshold $\alpha_c$ separating the region where almost all
formulas are SAT from the region where all formulas are UNSAT. Recent results
from a statistical physics analysis suggest that the difficulty is related to
the existence of a clustering phenomenon of the solutions when $\alpha$ is
close to (but smaller than) $\alpha_c$. We introduce a new type of message
passing algorithm which allows to find efficiently a satisfiable assignment of
the variables in the difficult region. This algorithm is iterative and composed
of two main parts. The first is a message-passing procedure which generalizes
the usual methods like Sum-Product or Belief Propagation: it passes messages
that are surveys over clusters of the ordinary messages. The second part uses
the detailed probabilistic information obtained from the surveys in order to
fix variables and simplify the problem. Eventually, the simplified problem that
remains is solved by a conventional heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212003</id><created>2002-12-04</created><authors><author><keyname>Banerjee</keyname><forenames>Anindya</forenames><affiliation>Kansas State University</affiliation></author><author><keyname>Naumann</keyname><forenames>David A.</forenames><affiliation>Stevens Institute of Technology</affiliation></author></authors><title>Ownership Confinement Ensures Representation Independence for
  Object-Oriented Programs</title><categories>cs.PL</categories><comments>88 pages, 13 figures</comments><acm-class>D.3.3; F.3.1</acm-class><abstract>  Dedicated to the memory of Edsger W.Dijkstra.
  Representation independence or relational parametricity formally
characterizes the encapsulation provided by language constructs for data
abstraction and justifies reasoning by simulation. Representation independence
has been shown for a variety of languages and constructs but not for shared
references to mutable state; indeed it fails in general for such languages.
This paper formulates representation independence for classes, in an
imperative, object-oriented language with pointers, subclassing and dynamic
dispatch, class oriented visibility control, recursive types and methods, and a
simple form of module. An instance of a class is considered to implement an
abstraction using private fields and so-called representation objects.
Encapsulation of representation objects is expressed by a restriction, called
confinement, on aliasing. Representation independence is proved for programs
satisfying the confinement condition. A static analysis is given for
confinement that accepts common designs such as the observer and factory
patterns. The formalization takes into account not only the usual interface
between a client and a class that provides an abstraction but also the
interface (often called ``protected'') between the class and its subclasses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212004</id><created>2002-12-05</created><authors><author><keyname>Chomicki</keyname><forenames>Jan</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jerzy</forenames></author></authors><title>Minimal-Change Integrity Maintenance Using Tuple Deletions</title><categories>cs.DB</categories><acm-class>H.2.3; F.4.1; I.2.3</acm-class><abstract>  We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212005</id><created>2002-12-05</created><authors><author><keyname>Regnier</keyname><forenames>Laurent</forenames></author><author><keyname>Urzyczyn</keyname><forenames>Pawel</forenames></author></authors><title>Retractions of Types with Many Atoms</title><categories>cs.LO</categories><comments>First International Workshop on Isomorphisms of Types Toulouse,
  France, 8-9 november 2002</comments><acm-class>F.4.1</acm-class><abstract>  We define a sound and complete proof system for affine beta-eta-retractions
in simple types built over many atoms, and we state simple necessary conditions
for arbitrary beta-eta-retractions in simple and polymorphic types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212006</id><created>2002-12-06</created><authors><author><keyname>Argentini</keyname><forenames>Gianluca</forenames></author></authors><title>Use of openMosix for parallel I/O balancing on storage in Linux cluster</title><categories>cs.DC cs.DB</categories><comments>Linux clustering, conference in CINECA, 28 november 2002; 2 figures,
  2 tables</comments><acm-class>C.1.2; C.2.4; H.3.3</acm-class><abstract>  In this paper I present some experiences made in the matter of I/O for Linux
Clustering. In particular is illustrated the use of the package openMosix, a
balancer of workload for processes running in a cluster of nodes. I describe
some tests for balancing the load of I/O storage massive processes in a cluster
with four components. This work is been written for the proceedings of the
workshop Linux cluster: the openMosix approach held at CINECA, Bologna, Italy,
on 28 november 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212007</id><created>2002-12-06</created><authors><author><keyname>Bern</keyname><forenames>Marshall</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Optimized Color Gamuts for Tiled Displays</title><categories>cs.CG cs.GR</categories><comments>10 pages, 4 figures</comments><acm-class>F.2.2</acm-class><abstract>  We consider the problem of finding a large color space that can be generated
by all units in multi-projector tiled display systems. Viewing the problem
geometrically as one of finding a large parallelepiped within the intersection
of multiple parallelepipeds, and using colorimetric principles to define a
volume-based objective function for comparing feasible solutions, we develop an
algorithm for finding the optimal gamut in time O(n^3), where n denotes the
number of projectors in the system. We also discuss more efficient quasiconvex
programming algorithms for alternative objective functions based on maximizing
the quality of the color space extrema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212008</id><created>2002-12-07</created><authors><author><keyname>Zhang</keyname><forenames>Zhenyue</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent
  Space Alignment</title><categories>cs.LG cs.AI</categories><acm-class>I.5.1;I5.3</acm-class><abstract>  Nonlinear manifold learning from unorganized data points is a very
challenging unsupervised learning and data visualization problem with a great
variety of applications. In this paper we present a new algorithm for manifold
learning and nonlinear dimension reduction. Based on a set of unorganized data
points sampled with noise from the manifold, we represent the local geometry of
the manifold using tangent spaces learned by fitting an affine subspace in a
neighborhood of each data point. Those tangent spaces are aligned to give the
internal global coordinates of the data points with respect to the underlying
manifold by way of a partial eigendecomposition of the neighborhood connection
matrix. We present a careful error analysis of our algorithm and show that the
reconstruction errors are of second-order accuracy. We illustrate our algorithm
using curves and surfaces both in
 2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images
with various pose and lighting conditions. We also address several theoretical
and algorithmic issues for further research and improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212009</id><created>2002-12-07</created><authors><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author></authors><title>On the survey-propagation equations for the random K-satisfiability
  problem</title><categories>cs.CC cond-mat.dis-nn</categories><comments>13 pages, 3 figures</comments><acm-class>G.3, G.2.1</acm-class><abstract>  In this note we study the existence of a solution to the survey-propagation
equations for the random K-satisfiability problem for a given instance. We
conjecture that when the number of variables goes to infinity, the solution of
these equations for a given instance can be approximated by the solution of the
corresponding equations on an infinite tree. We conjecture (and we bring
numerical evidence) that the survey-propagation equations on the infinite tree
have an unique solution in the suitable range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212010</id><created>2002-12-07</created><authors><author><keyname>Smith</keyname><forenames>Arnold</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Turney</keyname><forenames>Peter</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Ewaschuk</keyname><forenames>Robert</forenames><affiliation>University of Waterloo</affiliation></author></authors><title>JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space</title><categories>cs.NE cs.CE</categories><comments>26 pages, issued 2002, Java code available at
  http://purl.org/net/johnnyvon/</comments><report-no>NRC-44953</report-no><acm-class>I.6.3; I.6.8; J.2; J.3</acm-class><abstract>  JohnnyVon is an implementation of self-replicating automata in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary &quot;seed&quot; pattern
is put in a &quot;soup&quot; of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212011</id><created>2002-12-08</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Mining the Web for Lexical Knowledge to Improve Keyphrase Extraction:
  Learning from Labeled and Unlabeled Data</title><categories>cs.LG cs.IR</categories><comments>36 pages, issued 2002</comments><report-no>NRC-44947</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><abstract>  Keyphrases are useful for a variety of purposes, including summarizing,
indexing, labeling, categorizing, clustering, highlighting, browsing, and
searching. The task of automatic keyphrase extraction is to select keyphrases
from within the text of a given document. Automatic keyphrase extraction makes
it feasible to generate keyphrases for the huge number of documents that do not
have manually assigned keyphrases. Good performance on this task has been
obtained by approaching it as a supervised learning problem. An input document
is treated as a set of candidate phrases that must be classified as either
keyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase,
the most important features (attributes) appear to be the frequency and
location of the candidate phrase in the document. Recent work has demonstrated
that it is also useful to know the frequency of the candidate phrase as a
manually assigned keyphrase for other documents in the same domain as the given
document (e.g., the domain of computer science). Unfortunately, this
keyphrase-frequency feature is domain-specific (the learning process must be
repeated for each new domain) and training-intensive (good performance requires
a relatively large number of training documents in the given domain, with
manually assigned keyphrases). The aim of the work described here is to remove
these limitations. In this paper, I introduce new features that are derived by
mining lexical knowledge from a very large collection of unlabeled data,
consisting of approximately 350 million Web pages without manually assigned
keyphrases. I present experiments that show that the new features result in
improved keyphrase extraction, although they are neither domain-specific nor
training-intensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212012</id><created>2002-12-08</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames><affiliation>Stowe Research</affiliation></author></authors><title>Unsupervised Learning of Semantic Orientation from a
  Hundred-Billion-Word Corpus</title><categories>cs.LG cs.IR</categories><comments>11 pages, issued 2002</comments><report-no>NRC-44929</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><abstract>  The evaluative character of a word is called its semantic orientation. A
positive semantic orientation implies desirability (e.g., &quot;honest&quot;, &quot;intrepid&quot;)
and a negative semantic orientation implies undesirability (e.g., &quot;disturbing&quot;,
&quot;superfluous&quot;). This paper introduces a simple algorithm for unsupervised
learning of semantic orientation from extremely large corpora. The method
involves issuing queries to a Web search engine and using pointwise mutual
information to analyse the results. The algorithm is empirically evaluated
using a training corpus of approximately one hundred billion words -- the
subset of the Web that is indexed by the chosen search engine. Tested with
3,596 words (1,614 positive and 1,982 negative), the algorithm attains an
accuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, and
verbs. The accuracy is comparable with the results achieved by Hatzivassiloglou
and McKeown (1997), using a complex four-stage supervised learning algorithm
that is restricted to determining the semantic orientation of adjectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212013</id><created>2002-12-08</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Learning to Extract Keyphrases from Text</title><categories>cs.LG cs.IR</categories><comments>45 pages, issued 1999</comments><report-no>NRC-41622</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><abstract>  Many academic journals ask their authors to provide a list of about five to
fifteen key words, to appear on the first page of each article. Since these key
words are often phrases of two or more words, we prefer to call them
keyphrases. There is a surprisingly wide variety of tasks for which keyphrases
are useful, as we discuss in this paper. Recent commercial software, such as
Microsoft's Word 97 and Verity's Search 97, includes algorithms that
automatically extract keyphrases from documents. In this paper, we approach the
problem of automatically extracting keyphrases from text as a supervised
learning task. We treat a document as a set of phrases, which the learning
algorithm must learn to classify as positive or negative examples of
keyphrases. Our first set of experiments applies the C4.5 decision tree
induction algorithm to this learning task. The second set of experiments
applies the GenEx algorithm to the task. We developed the GenEx algorithm
specifically for this task. The third set of experiments examines the
performance of GenEx on the task of metadata generation, relative to the
performance of Microsoft's Word 97. The fourth and final set of experiments
investigates the performance of GenEx on the task of highlighting, relative to
Verity's Search 97. The experimental results support the claim that a
specialized learning algorithm (GenEx) can generate better keyphrases than a
general-purpose learning algorithm (C4.5) and the non-learning algorithms that
are used in commercial software (Word 97 and Search 97).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212014</id><created>2002-12-08</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Extraction of Keyphrases from Text: Evaluation of Four Algorithms</title><categories>cs.LG cs.IR</categories><comments>31 pages, issued 1997</comments><report-no>NRC-41550</report-no><acm-class>H.3.1; H.3.3; I.2.6; I.2.7</acm-class><abstract>  This report presents an empirical evaluation of four algorithms for
automatically extracting keywords and keyphrases from documents. The four
algorithms are compared using five different collections of documents. For each
document, we have a target set of keyphrases, which were generated by hand. The
target keyphrases were generated for human readers; they were not tailored for
any of the four keyphrase extraction algorithms. Each of the algorithms was
evaluated by the degree to which the algorithm's keyphrases matched the
manually generated keyphrases. The four algorithms were (1) the AutoSummarize
feature in Microsoft's Word 97, (2) an algorithm based on Eric Brill's
part-of-speech tagger, (3) the Summarize feature in Verity's Search 97, and (4)
NRC's Extractor algorithm. For all five document collections, NRC's Extractor
yields the best match with the manually generated keyphrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cs/0212015</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cs/0212015</id><created>2002-12-09</created><authors><author><keyname>Turney</keyname><forenames>Peter D.</forenames><affiliation>National Research Council of Canada</affiliation></author></authors><title>Answering Subcognitive Turing Test Questions: A Reply to French</title><categories>cs.CL</categories><comments>15 pages</comments><report-no>NRC-44898</report-no><acm-class>I.2.7</acm-class><journal-ref>Journal of Experimental and Theoretical Artificial Intelligence,
  (2001), 13 (4), 409-419</journal-ref><abstract>  Robert French has argued that a disembodied computer is incapable of passing
a Turing Test that includes subcognitive questions. Subcognitive questions are
designed to probe the network of cultural and perceptual associations that
humans naturally develop as we live, embodied and embedded in the world. In
this paper, I show how it is possible for a disembodied computer to answer
subcognitive questions appropriately, contrary to French's claim. My approach
to answering subcognitive questions is to use statistical information extracted
from a very large collection of text. In particular, I show how it is possible
to answer a sample of subcognitive questions taken from French, by issuing
queries to a search engine that indexes about 350 million Web pages. This
simple algorithm may shed light on the nature of human (sub-) cognition, but
the scope of this paper is limited to demonstrating that French is mistaken: a
disembodied computer can answer subcognitive questions.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="95000" completeListSize="102538">1122234|96001</resumptionToken>
</ListRecords>
</OAI-PMH>
