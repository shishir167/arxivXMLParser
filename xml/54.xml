<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:03:34Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|53001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5930</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5930</id><created>2013-11-22</created><authors><author><keyname>Hager</keyname><forenames>William</forenames></author><author><keyname>Hungerford</keyname><forenames>James</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author></authors><title>A Continuous Refinement Strategy for the Multilevel Computation of
  Vertex Separators</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vertex Separator Problem (VSP) on a graph is the problem of finding the
smallest collection of vertices whose removal separates the graph into two
disjoint subsets of roughly equal size. Recently, Hager and Hungerford [1]
developed a continuous bilinear programming formulation of the VSP. In this
paper, we reinforce the bilinear programming approach with a multilevel scheme
for learning the structure of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5932</identifier>
 <datestamp>2015-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5932</id><created>2013-11-22</created><authors><author><keyname>Cui</keyname><forenames>Ai-Xiang</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Strong ties promote the epidemic prevalence in
  susceptible-infected-susceptible spreading dynamics</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 6 figures, and 1 Table. arXiv admin note: substantial text
  overlap with arXiv:1204.0100</comments><journal-ref>PLoS ONE 9(12): e113457 (2014)</journal-ref><doi>10.1371/journal.pone.0113457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding spreading dynamics will benefit society as a whole in better
preventing and controlling diseases, as well as facilitating the socially
responsible information while depressing destructive rumors. In network-based
spreading dynamics, edges with different weights may play far different roles:
a friend from afar usually brings novel stories, and an intimate relationship
is highly risky for a flu epidemic. In this article, we propose a weighted
susceptible-infected-susceptible model on complex networks, where the weight of
an edge is defined by the topological proximity of the two associated nodes.
Each infected individual is allowed to select limited number of neighbors to
contact, and a tunable parameter is introduced to control the preference to
contact through high-weight or low-weight edges. Experimental results on six
real networks show that the epidemic prevalence can be largely promoted when
strong ties are favored in the spreading process. By comparing with two
statistical null models respectively with randomized topology and randomly
redistributed weights, we show that the distribution pattern of weights, rather
than the topology, mainly contributes to the experimental observations. Further
analysis suggests that the weight-weight correlation strongly affects the
results: high-weight edges are more significant in keeping high epidemic
prevalence when the weight-weight correlation is present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5933</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5933</id><created>2013-11-22</created><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>Network Strategies in the Voter Model</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>13 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple voter model with two competing parties. In particular, we
represent the case of political elections, where people can choose to support
one of the two competitors or to remain neutral. People interact in a social
network and their opinion depends on those of their neighbors. Therefore,
people may change opinion over time, i.e., they can support one competitor or
none. The two competitors try to gain the people's consensus by interacting
with their neighbors and also with other people. In particular, competitors
define temporal connections, following a strategy, to interact with people they
do not know, i.e., with all the people that are not their neighbors. We analyze
the proposed model to investigate which network strategies are more
advantageous, for the competitors, in order to gain the popular consensus. As
result, we found that the best network strategy depends on the topology of the
social network. Finally, we investigate how the charisma of competitors affects
the outcomes of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5934</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5934</id><created>2013-11-22</created><updated>2014-06-11</updated><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Elwes</keyname><forenames>Richard</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andy</forenames></author></authors><title>Tipping Points in Schelling Segregation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the earliest agent-based economical models, Schelling's spacial
proximity model illustrated how global segregation can emerge, often unwanted,
from the actions of agents of two races acting in accordance with their
individual local preferences. Here a 1-dimensional unperturbed variant of the
model is studied, which is additionally open in the sense that agents may enter
and exit the model. Following the authors' previous work in [1] and that of
Brandt, Immorlica, Kamath, and Kleinberg in [2], rigorous results are
established, whose statements are asymptotic in both the model and
neighbourhood sizes.
  The current model's openness allows one race or the other to take over almost
everywhere in a measure-theoretic sense. Tipping points are identified between
the two regions of takeover and the region of staticity, in terms of the
parameters of the model. In a significant generalization from previous work,
the parameters comprise the initial proportions of the two races, along with
independent values of the tolerance for each race.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5935</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5935</id><created>2013-11-22</created><updated>2014-04-02</updated><authors><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>The Simplex Algorithm is NP-mighty</title><categories>cs.DM cs.CC cs.DS math.CO</categories><msc-class>90C05, 05C21</msc-class><acm-class>F.2.2; G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to classify the power of algorithms by the complexity of the
problems that they can be used to solve. Instead of restricting to the problem
a particular algorithm was designed to solve explicitly, however, we include
problems that, with polynomial overhead, can be solved 'implicitly' during the
algorithm's execution. For example, we allow to solve a decision problem by
suitably transforming the input, executing the algorithm, and observing whether
a specific bit in its internal configuration ever switches during the
execution. We show that the Simplex Method, the Network Simplex Method (both
with Dantzig's original pivot rule), and the Successive Shortest Path Algorithm
are NP-mighty, that is, each of these algorithms can be used to solve any
problem in NP. This result casts a more favorable light on these algorithms'
exponential worst-case running times. Furthermore, as a consequence of our
approach, we obtain several novel hardness results. For example, for a given
input to the Simplex Algorithm, deciding whether a given variable ever enters
the basis during the algorithm's execution and determining the number of
iterations needed are both NP-hard problems. Finally, we close a long-standing
open problem in the area of network flows over time by showing that earliest
arrival flows are NP-hard to obtain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5947</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5947</id><created>2013-11-22</created><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Suter</keyname><forenames>David</forenames></author></authors><title>Fast Training of Effective Multi-class Boosting Using Coordinate Descent
  Optimization</title><categories>cs.CV cs.LG stat.CO</categories><comments>Appeared in Proc. Asian Conf. Computer Vision 2012. Code can be
  downloaded at http://goo.gl/WluhrQ</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wepresentanovelcolumngenerationbasedboostingmethod for multi-class
classification. Our multi-class boosting is formulated in a single optimization
problem as in Shen and Hao (2011). Different from most existing multi-class
boosting methods, which use the same set of weak learners for all the classes,
we train class specified weak learners (i.e., each class has a different set of
weak learners). We show that using separate weak learner sets for each class
leads to fast convergence, without introducing additional computational
overhead in the training procedure. To further make the training more efficient
and scalable, we also propose a fast co- ordinate descent method for solving
the optimization problem at each boosting iteration. The proposed coordinate
descent method is conceptually simple and easy to implement in that it is a
closed-form solution for each coordinate update. Experimental results on a
variety of datasets show that, compared to a range of existing multi-class
boosting meth- ods, the proposed method has much faster convergence rate and
better generalization performance in most cases. We also empirically show that
the proposed fast coordinate descent algorithm needs less training time than
the MultiBoost algorithm in Shen and Hao (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5949</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5949</id><created>2013-11-22</created><authors><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Kumbhare</keyname><forenames>Alok</forenames></author><author><keyname>Wickramaarachchi</keyname><forenames>Charith</forenames></author><author><keyname>Nagarkar</keyname><forenames>Soonil</forenames></author><author><keyname>Ravi</keyname><forenames>Santosh</forenames></author><author><keyname>Raghavendra</keyname><forenames>Cauligi</forenames></author><author><keyname>Prasanna</keyname><forenames>Viktor</forenames></author></authors><title>GoFFish: A Sub-Graph Centric Framework for Large-Scale Graph Analytics</title><categories>cs.DC</categories><comments>Under review by a conference, 2014</comments><journal-ref>Lecture Notes in Computer Science (Euro-Par 2014) vol. 8632 (2014)
  pp. 451-462</journal-ref><doi>10.1007/978-3-319-09873-9_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale graph processing is a major research area for Big Data
exploration. Vertex centric programming models like Pregel are gaining traction
due to their simple abstraction that allows for scalable execution on
distributed systems naturally. However, there are limitations to this approach
which cause vertex centric algorithms to under-perform due to poor compute to
communication overhead ratio and slow convergence of iterative superstep. In
this paper we introduce GoFFish a scalable sub-graph centric framework
co-designed with a distributed persistent graph storage for large scale graph
analytics on commodity clusters. We introduce a sub-graph centric programming
abstraction that combines the scalability of a vertex centric approach with the
flexibility of shared memory sub-graph computation. We map Connected
Components, SSSP and PageRank algorithms to this model to illustrate its
flexibility. Further, we empirically analyze GoFFish using several real world
graphs and demonstrate its significant performance improvement, orders of
magnitude in some cases, compared to Apache Giraph, the leading open source
vertex centric implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5966</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5966</id><created>2013-11-23</created><updated>2014-04-29</updated><authors><author><keyname>Wang</keyname><forenames>Zihe</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author></authors><title>Optimal mechanisms with simple menus</title><categories>cs.GT</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider optimal mechanism design for the case with one buyer and two
items. The buyer's valuations towards the two items are independent and
additive. In this setting, optimal mechanism is unknown for general valuation
distributions. We obtain two categories of structural results that shed light
on the optimal mechanisms.
  The first category of results state that, under certain mild condition, the
optimal mechanism has a monotone menu. In other words, in the menu that
represents the optimal mechanism, as payment increases, the allocation
probabilities for both items increase simultaneously. Applying this theorem, we
derive a version of revenue monotonicity theorem that states stochastically
superior distributions yield more revenue. Moreover, our theorem subsumes a
previous result regarding sufficient conditions under which bundling is
optimal. The second category of results state that, under certain conditions,
the optimal mechanisms have few menu items. Our first result in this category
says, for certain distributions, the optimal menu contains at most 4 items. The
condition admits power (including uniform) density functions. Based on a
similar proof of this result, we are able to obtain a wide class of
distributions where bundling is optimal. Our second result in this category
works for a weaker condition, under which the optimal menu contains at most 6
items. This condition includes exponential density functions. Our last result
in this category works for unit-demand setting. It states that, for uniform
distributions, the optimal menu contains at most 5 items. All these results are
in sharp contrast to Hart and Nisan's recent result that finite-sized menu
cannot guarantee any positive fraction of optimal revenue for correlated
valuation distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5978</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5978</id><created>2013-11-23</created><authors><author><keyname>Lee</keyname><forenames>Pei</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author><author><keyname>Milios</keyname><forenames>Evangelos E.</forenames></author></authors><title>Event Evolution Tracking from Streaming Social Posts</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Online social post streams such as Twitter timelines and forum discussions
have emerged as important channels for information dissemination. They are
noisy, informal, and surge quickly. Real life events, which may happen and
evolve every minute, are perceived and circulated in post streams by social
users. Intuitively, an event can be viewed as a dense cluster of posts with a
life cycle sharing the same descriptive words. There are many previous works on
event detection from social streams. However, there has been surprisingly
little work on tracking the evolution patterns of events, e.g., birth/death,
growth/decay, merge/split, which we address in this paper. To define a tracking
scope, we use a sliding time window, where old posts disappear and new posts
appear at each moment. Following that, we model a social post stream as an
evolving network, where each social post is a node, and edges between posts are
constructed when the post similarity is above a threshold. We propose a
framework which summarizes the information in the stream within the current
time window as a ``sketch graph'' composed of ``core'' posts. We develop
incremental update algorithms to handle highly dynamic social streams and track
event evolution patterns in real time. Moreover, we visualize events as word
clouds to aid human perception. Our evaluation on a real data set consisting of
5.2 million posts demonstrates that our method can effectively track event
dynamics in the whole life cycle from very large volumes of social streams on
the fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5989</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5989</id><created>2013-11-23</created><updated>2014-02-07</updated><authors><author><keyname>Avonds</keyname><forenames>Yurrit</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Van Huffel</keyname><forenames>Sabine</forenames></author></authors><title>Robust Cosparse Greedy Signal Reconstruction for Compressive Sensing
  with Multiplicative and Additive Noise</title><categories>cs.IT cs.DS math.IT stat.AP</categories><comments>This paper has been withdrawn by the author due to errors (missed
  \gamma in the 2nd term on the right) in equation 10, equation 11, and
  equation 12, which leads to further error in Algorithm 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy algorithms are popular in compressive sensing for their high
computational efficiency. But the performance of current greedy algorithms can
be degenerated seriously by noise (both multiplicative noise and additive
noise). A robust version of greedy cosparse greedy algorithm (greedy analysis
pursuit) is presented in this paper. Comparing with previous methods, The
proposed robust greedy analysis pursuit algorithm is based on an optimization
model which allows both multiplicative noise and additive noise in the data
fitting constraint. Besides, a new stopping criterion that is derived. The new
algorithm is applied to compressive sensing of ECG signals. Numerical
experiments based on real-life ECG signals demonstrate the performance
improvement of the proposed greedy algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.5998</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.5998</id><created>2013-11-23</created><authors><author><keyname>Li</keyname><forenames>Yunpeng</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Deng</keyname><forenames>Yong</forenames></author></authors><title>A brief network analysis of Artificial Intelligence publication</title><categories>cs.AI cs.DL</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an illustration to the history of Artificial
Intelligence(AI) with a statistical analysis of publish since 1940. We
collected and mined through the IEEE publish data base to analysis the
geological and chronological variance of the activeness of research in AI. The
connections between different institutes are showed. The result shows that the
leading community of AI research are mainly in the USA, China, the Europe and
Japan. The key institutes, authors and the research hotspots are revealed. It
is found that the research institutes in the fields like Data Mining, Computer
Vision, Pattern Recognition and some other fields of Machine Learning are quite
consistent, implying a strong interaction between the community of each field.
It is also showed that the research of Electronic Engineering and Industrial or
Commercial applications are very active in California. Japan is also publishing
a lot of papers in robotics. Due to the limitation of data source, the result
might be overly influenced by the number of published articles, which is to our
best improved by applying network keynode analysis on the research community
instead of merely count the number of publish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6001</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6001</id><created>2013-11-23</created><authors><author><keyname>Bethu</keyname><forenames>Srikanth</forenames></author><author><keyname>Kumar</keyname><forenames>K Kanthi</forenames></author><author><keyname>Ahmed</keyname><forenames>MD Asrar</forenames></author><author><keyname>Soujanya</keyname><forenames>S.</forenames></author></authors><title>Comparison analysis in Multicast Authentication based on Batch Signature
  (MABS) in Network Security</title><categories>cs.CR</categories><comments>6 pages, 11 figures. IJIP 2013</comments><journal-ref>IJIP VOLUME 8 ISSUE 1 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional block-based multicast authentication schemes overlook the
heterogeneity of receivers by letting the sender choose the block size, divide
a multicast stream into blocks, associate each block with a signature, and
spread the e?ect of the signature across all the packets in the block through
hash graphs or coding algorithms. The correlation among packets makes them
vulnerable to packet loss, which is inherent in the Internet and wireless
networks. Moreover, the lack of Denial of Service (DoS) resilience renders most
of them vulnerable to packet injection in hostile environments. In this paper,
we propose a novel multicast authentication protocol, namely MABS, including
two schemes. The basic scheme (MABS-B) eliminates the correlation among packets
and thus provides the perfect resilience to packet loss, and it is also e?cient
in terms of latency, computation, and communication overhead due to an e?cient
cryptographic primitive called batch signature, which supports the
authentication of any number of packets simultaneously.so we discuss their
comparisons and performance evaluation of Packet Loss, Comparisons over Lossy
Channels, Comparisons of Signature Schemes, computationational overheads etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6002</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6002</id><created>2013-11-23</created><updated>2014-12-01</updated><authors><author><keyname>Balkova</keyname><forenames>Lubomira</forenames></author><author><keyname>Bucci</keyname><forenames>Michelangelo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author><author><keyname>Hladky</keyname><forenames>Jiri</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author></authors><title>Aperiodic pseudorandom number generators based on infinite words</title><categories>math.CO cs.DM cs.FL</categories><comments>updated and extended version; 24 pages</comments><msc-class>68R15</msc-class><acm-class>G.3; F.4.3; G.2.1; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study how certain families of aperiodic infinite words can
be used to produce aperiodic pseudorandom number generators (PRNGs) with good
statistical behavior. We introduce the \emph{well distributed occurrences}
(WELLDOC) combinatorial property for infinite words, which guarantees absence
of the lattice structure defect in related pseudorandom number generators. An
infinite word $u$ on a $d$-ary alphabet has the WELLDOC property if, for each
factor $w$ of $u$, positive integer $m$, and vector $\mathbf v\in\mathbb
Z_{m}^{d}$, there is an occurrence of $w$ such that the Parikh vector of the
prefix of $u$ preceding such occurrence is congruent to $\mathbf v$ modulo $m$.
(The Parikh vector of a finite word $v$ over an alphabet $\mathcal A$ has its
$i$-th component equal to the number of occurrences of the $i$-th letter of
$\mathcal A$ in $v$.) We prove that Sturmian words, and more generally
Arnoux-Rauzy words and some morphic images of them, have the WELLDOC property.
Using the TestU01 and PractRand statistical tests, we moreover show that not
only the lattice structure is absent, but also other important properties of
PRNGs are improved when linear congruential generators are combined using
infinite words having the WELLDOC property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6005</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6005</id><created>2013-11-23</created><authors><author><keyname>Ahourai</keyname><forenames>Fereidoun</forenames></author><author><keyname>Huang</keyname><forenames>Irvin</forenames></author><author><keyname>Faruque</keyname><forenames>Mohammad Abdullah Al</forenames></author></authors><title>Modeling and Simulation of the EV Charging in a Residential Distribution
  Power Grid</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There are numerous advantages of using Electric Vehicles (EVs) as an
alternative method of transportation. However, an increase in EV usage in the
existing residential distribution grid poses problems such as overloading the
existing infrastructure. In this paper, we have modeled and simulated a
residential distribution grid in GridLAB-D (an open-source software tool used
to model, simulate, and analyze power distribution systems) to illustrate the
problems associated with a higher EV market penetration rates in the
residential domain. Power grid upgrades or control algorithms at the
transformer level are required to overcome issues such as transformer
overloading. We demonstrate the method of coordinating EV charging in a
residential distribution grid so as to overcome the overloading problem without
any upgrades in the distribution grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6007</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6007</id><created>2013-11-23</created><authors><author><keyname>Bajaj</keyname><forenames>Nikunj</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Happy</keyname><forenames>S L</forenames></author></authors><title>Dynamic Model of Facial Expression Recognition based on Eigen-face
  Approach</title><categories>cs.CV</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Emotions are best way of communicating information; and sometimes it carry
more information than words. Recently, there has been a huge interest in
automatic recognition of human emotion because of its wide spread application
in security, surveillance, marketing, advertisement, and human-computer
interaction. To communicate with a computer in a natural way, it will be
desirable to use more natural modes of human communication based on voice,
gestures and facial expressions. In this paper, a holistic approach for facial
expression recognition is proposed which captures the variation in facial
features in temporal domain and classifies the sequence of images in different
emotions. The proposed method uses Haar-like features to detect face in an
image. The dimensionality of the eigenspace is reduced using Principal
Component Analysis (PCA). By projecting the subsequent face images into
principal eigen directions, the variation pattern of the obtained weight vector
is modeled to classify it into different emotions. Owing to the variations of
expressions for different people and its intensity, a person specific method
for emotion recognition is followed. Using the gray scale images of the frontal
face, the system is able to classify four basic emotions such as happiness,
sadness, surprise, and anger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6009</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6009</id><created>2013-11-23</created><authors><author><keyname>Chung</keyname><forenames>Ching-Yen</forenames></author><author><keyname>Chynoweth</keyname><forenames>Joshua</forenames></author><author><keyname>Qiu</keyname><forenames>Charlie</forenames></author><author><keyname>Chu</keyname><forenames>Chi-Cheng</forenames></author><author><keyname>Gadh</keyname><forenames>Rajit</forenames></author></authors><title>Design of Fast Response Smart Electric Vehicle Charging Infrastructure</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The response time of the smart electrical vehicle (EV) charging
infrastructure is the key index of the system performance. The traffic between
the smart EV charging station and the control center dominates the response
time of the smart charging stations. To accelerate the response of the smart EV
charging station, there is a need for a technology that collects the
information locally and relays it to the control center periodically. To reduce
the traffic between the smart EV charger and the control center, a Power
Information Collector (PIC), capable of collecting all the meters power
information in the charging station, is proposed and implemented in this paper.
The response time is further reduced by pushing the power information to the
control center. Thus, a fast response smart EV charging infrastructure is
achieved to handle the shortage of energy in the local grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6010</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6010</id><created>2013-11-23</created><authors><author><keyname>Hamano</keyname><forenames>Fumio</forenames></author></authors><title>Derivative of Rotation Matrix Direct Matrix Derivation of Well Known
  Formula</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In motion Kinematics, it is well-known that the time derivative of a
3x3rotation matrix equals a skew-symmetric matrix multiplied by the rotation
matrix where the skew symmetric matrix is a linear (matrix valued) function of
the angular velocity and the rotation matrix represents the rotating motion of
a frame with respect to a reference frame. The equation is widely used in
engineering, e.g., robotics, control, air/spacecraft modeling, etc. However,
the derivations found in the literature are indirect. Motivated by the fact
that the set of 3x3rotation matrices, i.e., SO(3), is a Lie group, forming a
smooth (differentiable) manifold, we describe the infinitesimal increment of
the rotation matrix in terms of rotation matrices and show that the above
equation immediately follows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6012</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6012</id><created>2013-11-23</created><authors><author><keyname>Hsu</keyname><forenames>Tai-Ran</forenames></author></authors><title>On a Flywheel-Based Regenerative Braking System for Regenerative Energy
  Recovery</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a unique flywheel-based regenerative energy recovery,
storage and release system developed at the author's laboratory. It can recover
and store regenerative energy produced by braking a motion generator with
intermittent rotary velocity such as the rotor of a wind turbogenerator subject
to intermittent intake wind and the axels of electric and hybrid gas-electric
vehicles during frequent coasting and braking. Releasing of the stored
regenerative energy in the flywheel is converted to electricity by the attached
alternator. A proof-of-concept prototype called the SJSU-RBS was designed,
built and tested by author's students with able assistance of a technical staff
in his school.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6013</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6013</id><created>2013-11-23</created><authors><author><keyname>Bruno</keyname><forenames>R.</forenames></author><author><keyname>Nurchis</keyname><forenames>M.</forenames></author></authors><title>Efficient Data Collection in Multimedia Vehicular Sensing Platforms</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicles provide an ideal platform for urban sensing applications, as they
can be equipped with all kinds of sensing devices that can continuously monitor
the environment around the travelling vehicle. In this work we are particularly
concerned with the use of vehicles as building blocks of a multimedia mobile
sensor system able to capture camera snapshots of the streets to support
traffic monitoring and urban surveillance tasks. However, cameras are high
data-rate sensors while wireless infrastructures used for vehicular
communications may face performance constraints. Thus, data redundancy
mitigation is of paramount importance in such systems. To address this issue in
this paper we exploit sub-modular optimisation techniques to design efficient
and robust data collection schemes for multimedia vehicular sensor networks. We
also explore an alternative approach for data collection that operates on
longer time scales and relies only on localised decisions rather than
centralised computations. We use network simulations with realistic vehicular
mobility patterns to verify the performance gains of our proposed schemes
compared to a baseline solution that ignores data redundancy. Simulation
results show that our data collection techniques can ensure a more accurate
coverage of the road network while significantly reducing the amount of
transferred data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6015</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6015</id><created>2013-11-23</created><authors><author><keyname>Hsu</keyname><forenames>Tai-Ran</forenames></author></authors><title>On the Sustainability of Electrical Vehicles</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many perceive electric vehicles (EVs) to be eco-environmentally sustainable
because they are free of emissions of toxic and greenhouse gases to the
environment. However, few have questioned the sustainability of the electric
power required to drive these vehicles. This paper presents an in-depth study
that indicates that massive infusion of EVs to our society in a short time span
will likely create a colossal demand for additional electric power generation
much beyond what the US electric power generating industry can provide with its
current generating capacity. Additionally, such demand would result in much
adverse environmental consequences if the current technology of electric power
generation by predominant fossil fuels continues. Other rarely accounted facts
on environmental impacts by EVs are the substantial electric energy required to
produce batteries that drive EVs, and the negative consequences relating to the
recycling of spent batteries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6020</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6020</id><created>2013-11-23</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Shen</keyname><forenames>Weiming</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Security versus Reliability Analysis of Opportunistic Relaying</title><categories>cs.IT cs.CR math.IT</categories><comments>9 pages. IEEE Transactions on Vehicular Technology, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical-layer security is emerging as a promising paradigm of securing
wireless communications against eavesdropping between legitimate users, when
the main link spanning from source to destination has better propagation
conditions than the wiretap link from source to eavesdropper. In this paper, we
identify and analyze the tradeoffs between the security and reliability of
wireless communications in the presence of eavesdropping attacks. Typically,
the reliability of the main link can be improved by increasing the source's
transmit power (or decreasing its date rate) to reduce the outage probability,
which unfortunately increases the risk that an eavesdropper succeeds in
intercepting the source message through the wiretap link, since the outage
probability of the wiretap link also decreases when a higher transmit power (or
lower date rate) is used. We characterize the security-reliability tradeoffs
(SRT) of conventional direct transmission from source to destination in the
presence of an eavesdropper, where the security and reliability are quantified
in terms of the intercept probability by an eavesdropper and the outage
probability experienced at the destination, respectively. In order to improve
the SRT, we then propose opportunistic relay selection (ORS) and quantify the
attainable SRT improvement upon increasing the number of relays. It is shown
that given the maximum tolerable intercept probability, the outage probability
of our ORS scheme approaches zero for $N \to \infty$, where $N$ is the number
of relays. Conversely, given the maximum tolerable outage probability, the
intercept probability of our ORS scheme tends to zero for $N \to \infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6023</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6023</id><created>2013-11-23</created><authors><author><keyname>Tsang</keyname><forenames>Chit-Sang</forenames></author></authors><title>Third Order Intermodulation Power Estimation for N Sinusoidal Channels</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper analysis is given to find the third order intermodulation power
given sinusoids are fed into a nonlinear device. A simple expression of the
third order intermodulation power is given for the case that the center
frequencies of the input sinusoids are equally spaced. Further, if the powers
of the signals are equal, the expression becomes a closed form expression. The
analysis will be helpful for communication system engineering in estimating the
adjacent channel interference due to nonlinearity. Numerical results are
presented for various values of (number of input channels). Though the analysis
assumes the input signals to be sinusoids without phase modulation, the third
order intermodulation power estimate serves as a good estimate for link budget
computation purpose. For the case that the center frequencies of the input
sinusoids are not spaced equally, the analysis can still highly likely be
applied if we insert pseudo channels in between the real channels so that all
(real and pseudo) channels are spaced equally (or approximately equally for
approximation). In this case, the pseudo channel powers are set to zero so that
the interference powers due to the pseudo channels will not be included in the
analysis. In other words, the analysis is highly likely applicable without the
constraint of the input channel center frequencies being equally likely.
Simulations are also provided for the case that the input sinusoids are QPSK
modulated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6024</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6024</id><created>2013-11-23</created><authors><author><keyname>Friggstad</keyname><forenames>Zachary</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Approximation Algorithms for Regret-Bounded Vehicle Routing and
  Applications to Distance-Constrained Vehicle Routing</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.1.6; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider vehicle-routing problems (VRPs) that incorporate the notion of
{\em regret} of a client, which is a measure of the waiting time of a client
relative to its shortest-path distance from the depot. Formally, we consider
both the additive and multiplicative versions of, what we call, the {\em
regret-bounded vehicle routing problem} (RVRP). In these problems, we are given
an undirected complete graph $G=(\{r\}\cup V,E)$ on $n$ nodes with a
distinguished root (depot) node $r$, edge costs $\{c_{uv}\}$ that form a
metric, and a regret bound $R$. Given a path $P$ rooted at $r$ and a node $v\in
P$, let $c_P(v)$ be the distance from $r$ to $v$ along $P$. The goal is to find
the fewest number of paths rooted at $r$ that cover all the nodes so that for
every node $v$ covered by (say) path $P$: (i) its additive regret
$c_P(v)-c_{rv}$, with respect to $P$ is at most $R$ in {\em additive-RVRP}; or
(ii) its multiplicative regret, $c_P(c)/c_{rv}$, with respect to $P$ is at most
$R$ in {\em multiplicative-RVRP}.
  Our main result is the {\em first} constant-factor approximation algorithm
for additive-RVRP by devising rounding techniques for a natural {\em
configuration-style LP}. This is a substantial improvement over the
previous-best $O(\log n)$-approximation. Additive-RVRP turns out be a rather
central vehicle-routing problem, whose study reveals insights into a variety of
other regret-related problems as well as the classical {\em
distance-constrained VRP} ({DVRP}). We obtain approximation ratios of
$O\bigl(\log(\frac{R}{R-1})\bigr)$ for multiplicative-RVRP, and
$O\bigl(\min\bigl\{\mathit{OPT},\frac{\log D}{\log\log D}\bigr\}\bigr)$ for
DVRP with distance bound $D$ via reductions to additive-RVRP; the latter
improves upon the previous-best approximation for DVRP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6026</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6026</id><created>2013-11-23</created><authors><author><keyname>Yee</keyname><forenames>Raymond K.</forenames></author><author><keyname>Hsu</keyname><forenames>Tai-Ran</forenames></author><author><keyname>Le</keyname><forenames>Thuy T.</forenames></author></authors><title>Research and innovative design of a zeroemissions vehicle by
  multidisciplinary student teams in multi-years</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a unique learning and research experience for students
from mechanical and electrical engineering majors in a course on senior design
projects involving research and development, design and production of a
proof-of-concept electric vehicle, the ZEM (Zero EMissions) vehicle. The ZEM
vehicle combined positive aspects and latest technologies in electric vehicle
design,solar-electric power conversions, and ergonomic human power into one
affordable and environmentally sustainable vehicle for urban transportation.
The 43 mechanical and 10 electrical engineering majors plus 7 students from
business participated in this multidisciplinary project spanned over two
academic years. The students involved in this multiyear endeavor gained
valuable experiences in real-world working environment with multifunctional and
multi-year sub-groups. The success of this new attempt in conducting senior
design projects classes have set a model for faculty members in the authors'
university in conducting similar courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6033</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6033</id><created>2013-11-23</created><authors><author><keyname>Vigan</keyname><forenames>Ivo</forenames></author></authors><title>Packing and Covering a Polygon with Geodesic Disks</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a polygon $P$, for two points $s$ and $t$ contained in the polygon,
their \emph{geodesic distance} is the length of the shortest $st$-path within
$P$. A \emph{geodesic disk} of radius $r$ centered at a point $v \in P$ is the
set of points in $P$ whose geodesic distance to $v$ is at most $r$. We present
a polynomial time $2$-approximation algorithm for finding a densest geodesic
unit disk packing in $P$. Allowing arbitrary radii but constraining the number
of disks to be $k$, we present a $4$-approximation algorithm for finding a
packing in $P$ with $k$ geodesic disks whose minimum radius is maximized. We
then turn our focus on \emph{coverings} of $P$ and present a $2$-approximation
algorithm for covering $P$ with $k$ geodesic disks whose maximal radius is
minimized. Furthermore, we show that all these problems are $\mathsf{NP}$-hard
in polygons with holes. Lastly, we present a polynomial time exact algorithm
which covers a polygon with two geodesic disks of minimum maximal radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6041</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6041</id><created>2013-11-23</created><updated>2013-12-01</updated><authors><author><keyname>Serafino</keyname><forenames>Loris</forenames></author></authors><title>No Free Lunch Theorem and Bayesian probability theory: two sides of the
  same coin. Some implications for black-box optimization and metaheuristics</title><categories>cs.LG</categories><comments>Multiple changes throughout the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Challenging optimization problems, which elude acceptable solution via
conventional calculus methods, arise commonly in different areas of industrial
design and practice. Hard optimization problems are those who manifest the
following behavior: a) high number of independent input variables; b) very
complex or irregular multi-modal fitness; c) computational expensive fitness
evaluation. This paper will focus on some theoretical issues that have strong
implications for practice. I will stress how an interpretation of the No Free
Lunch theorem leads naturally to a general Bayesian optimization framework. The
choice of a prior over the space of functions is a critical and inevitable step
in every black-box optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6045</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6045</id><created>2013-11-23</created><authors><author><keyname>El-Abbadi</keyname><forenames>Nidhal</forenames></author><author><keyname>Khdhair</keyname><forenames>Ahmed Nidhal</forenames></author><author><keyname>Al-Nasrawi</keyname><forenames>Adel</forenames></author></authors><title>Build Electronic Arabic Lexicon</title><categories>cs.CL</categories><comments>4 pages</comments><journal-ref>The International Arab Journal of Information Technology, Vol. 8,
  No. 2, April 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many known Arabic lexicons organized on different ways, each of
them has a different number of Arabic words according to its organization way.
This paper has used mathematical relations to count a number of Arabic words,
which proofs the number of Arabic words presented by Al Farahidy. The paper
also presents new way to build an electronic Arabic lexicon by using a hash
function that converts each word (as input) to correspond a unique integer
number (as output), these integer numbers will be used as an index to a lexicon
entry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6048</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6048</id><created>2013-11-23</created><authors><author><keyname>Dong</keyname><forenames>Jingming</forenames></author><author><keyname>Balzer</keyname><forenames>Jonathan</forenames></author><author><keyname>Davis</keyname><forenames>Damek</forenames></author><author><keyname>Hernandez</keyname><forenames>Joshua</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>On the Design and Analysis of Multiple View Descriptors</title><categories>cs.CV</categories><report-no>UCLA CSD TR130024, Nov. 8, 2013</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an extension of popular descriptors based on gradient orientation
histograms (HOG, computed in a single image) to multiple views. It hinges on
interpreting HOG as a conditional density in the space of sampled images, where
the effects of nuisance factors such as viewpoint and illumination are
marginalized. However, such marginalization is performed with respect to a very
coarse approximation of the underlying distribution. Our extension leverages on
the fact that multiple views of the same scene allow separating intrinsic from
nuisance variability, and thus afford better marginalization of the latter. The
result is a descriptor that has the same complexity of single-view HOG, and can
be compared in the same manner, but exploits multiple views to better trade off
insensitivity to nuisance variability with specificity to intrinsic
variability. We also introduce a novel multi-view wide-baseline matching
dataset, consisting of a mixture of real and synthetic objects with ground
truthed camera motion and dense three-dimensional geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6049</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6049</id><created>2013-11-23</created><authors><author><keyname>Abbadi</keyname><forenames>Nidhal K. El</forenames></author><author><keyname>Dahir</keyname><forenames>Nazar</forenames></author><author><keyname>Alkareem</keyname><forenames>Zaid Abd</forenames></author></authors><title>Skin Texture Recognition Using Neural Networks</title><categories>cs.CV</categories><comments>4 pages, 6 figures, conference ACIT 2008, Tunisia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skin recognition is used in many applications ranging from algorithms for
face detection, hand gesture analysis, and to objectionable image filtering. In
this work a skin recognition system was developed and tested. While many skin
segmentation algorithms relay on skin color, our work relies on both skin color
and texture features (features derives from the GLCM) to give a better and more
efficient recognition accuracy of skin textures. We used feed forward neural
networks to classify input textures images to be skin or non skin textures. The
system gave very encouraging results during the neural network generalization
face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6054</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6054</id><created>2013-11-23</created><authors><author><keyname>Qaffou</keyname><forenames>Issam</forenames></author><author><keyname>Sadgal</keyname><forenames>Mohamed</forenames></author><author><keyname>Elfazziki</keyname><forenames>Abdelaziz</forenames></author></authors><title>Q-learning optimization in a multi-agents system for image segmentation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To know which operators to apply and in which order, as well as attributing
good values to their parameters is a challenge for users of computer vision.
This paper proposes a solution to this problem as a multi-agent system modeled
according to the Vowel approach and using the Q-learning algorithm to optimize
its choice. An implementation is given to test and validate this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6057</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6057</id><created>2013-11-23</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author></authors><title>Games and Full Completeness for Multiplicative Linear Logic</title><categories>cs.LO</categories><comments>45 pages, 5 figures</comments><journal-ref>Journal of Symbolic Logic (1994), volume 59 no. 2, pages 543-574</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a game semantics for Linear Logic, in which formulas denote games
and proofs denote winning strategies. We show that our semantics yields a
categorical model of Linear Logic and prove full completeness for
Multiplicative Linear Logic with the MIX rule: every winning strategy is the
denotation of a unique cut-free proof net. A key role is played by the notion
of {\em history-free} strategy; strong connections are made between
history-free strategies and the Geometry of Interaction. Our semantics
incorporates a natural notion of polarity, leading to a refined treatment of
the additives. We make comparisons with related work by Joyal, Blass et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6062</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6062</id><created>2013-11-23</created><authors><author><keyname>Casado</keyname><forenames>A.</forenames></author><author><keyname>Guerra</keyname><forenames>S.</forenames></author><author><keyname>Pl&#xe1;cido</keyname><forenames>J.</forenames></author></authors><title>Wigner function description of entanglement swapping using parametric
  down conversion: the role of vacuum fluctuations in teleportation</title><categories>quant-ph cs.IT math.IT</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the Wigner formalism of quantum optics to study the role of the
zeropoint field fluctuations in entanglement swapping produced via parametric
down conversion. It is shown that the generation of mode entanglement between
two initially non interacting photons is related to the quadruple correlation
properties of the electromagnetic field, through the stochastic properties of
the vacuum. The relationship between the process of transferring entanglement
and the different zeropoint inputs at the nonlinear crystal and the Bell-state
analyser is emphasized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6063</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6063</id><created>2013-11-23</created><updated>2014-10-13</updated><authors><author><keyname>Yu</keyname><forenames>Sheng</forenames></author><author><keyname>Cai</keyname><forenames>Tianxi</forenames></author></authors><title>A Short Introduction to NILE</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we briefly introduce the Narrative Information Linear
Extraction (NILE) system, a natural language processing library for clinical
narratives. NILE is an experiment of our ideas on efficient and effective
medical language processing. We introduce the overall design of NILE and its
major components, and show the performance of it in real projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6079</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6079</id><created>2013-11-23</created><updated>2014-03-05</updated><authors><author><keyname>Shaban</keyname><forenames>Amirreza</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Najibi</keyname><forenames>Mahyar</forenames></author></authors><title>Local Similarities, Global Coding: An Algorithm for Feature Coding and
  its Applications</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data coding as a building block of several image processing algorithms has
been received great attention recently. Indeed, the importance of the locality
assumption in coding approaches is studied in numerous works and several
methods are proposed based on this concept. We probe this assumption and claim
that taking the similarity between a data point and a more global set of anchor
points does not necessarily weaken the coding method as long as the underlying
structure of the anchor points are taken into account. Based on this fact, we
propose to capture this underlying structure by assuming a random walker over
the anchor points. We show that our method is a fast approximate learning
algorithm based on the diffusion map kernel. The experiments on various
datasets show that making different state-of-the-art coding algorithms aware of
this structure boosts them in different learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6091</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6091</id><created>2013-11-24</created><updated>2014-03-05</updated><authors><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>A Primal-Dual Method for Training Recurrent Neural Networks Constrained
  by the Echo-State Property</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an architecture of a recurrent neural network (RNN) with a
fully-connected deep neural network (DNN) as its feature extractor. The RNN is
equipped with both causal temporal prediction and non-causal look-ahead, via
auto-regression (AR) and moving-average (MA), respectively. The focus of this
paper is a primal-dual training method that formulates the learning of the RNN
as a formal optimization problem with an inequality constraint that provides a
sufficient condition for the stability of the network dynamics. Experimental
results demonstrate the effectiveness of this new method, which achieves 18.86%
phone recognition error on the TIMIT benchmark for the core test set. The
result approaches the best result of 17.7%, which was obtained by using RNN
with long short-term memory (LSTM). The results also show that the proposed
primal-dual training method produces lower recognition errors than the popular
RNN methods developed earlier based on the carefully tuned threshold parameter
that heuristically prevents the gradient from exploding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6092</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6092</id><created>2013-11-24</created><authors><author><keyname>Nuzzo</keyname><forenames>Pierluigi</forenames></author><author><keyname>Finn</keyname><forenames>John</forenames></author><author><keyname>Mozumdar</keyname><forenames>Mohammad</forenames></author><author><keyname>Sangiovanni-Vincentelli</keyname><forenames>Alberto</forenames></author></authors><title>Platform-Based Design Methodology and Modeling for Aircraft Electric
  Power Systems</title><categories>cs.SY cs.SE</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In an aircraft electric power system (EPS), a supervisory control unit must
actuate a set of switches to distribute power from generators to loads, while
satisfying safety, reliability and real-time performance requirements. To
reduce expensive re-design steps in current design methodologies, such a
control problem is generally addressed based on minor incremental changes on
top of consolidated solutions, since it is difficult to estimate the impact of
earlier design decisions on the final implementation. In this paper, we
introduce a methodology for the design space exploration and virtual
prototyping of EPS supervisory control protocols, following the platform-based
design (PBD) paradigm. Moreover, we describe the modeling infrastructure that
supports the methodology. In PBD, design space exploration is carried out as a
sequence of refinement steps from the initial specification towards a final
implementation, by mapping higher-level behavioral models into a set of library
components at a lower level of abstraction. In our flow, the system
specification is captured using SysML requirement and structure diagrams.
State-machine diagrams enable verification of the control protocol at a high
level of abstraction, while lowerlevel hybrid models, implemented in Simulink,
are used to verify properties related to physical quantities, such as time,
voltage and current values. The effectiveness of our approach is illustrated on
a prototype EPS control protocol design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6093</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6093</id><created>2013-11-24</created><updated>2015-11-01</updated><authors><author><keyname>Mishra</keyname><forenames>Pushkar</forenames></author></authors><title>A New Algorithm for Updating and Querying Sub-arrays of Multidimensional
  Arrays</title><categories>cs.DS</categories><comments>14 Pages, 3 Figures, 1 Table</comments><doi>10.13140/RG.2.1.2394.2485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a $d$-dimensional array $A$, an \textit{update} operation adds a given
constant $C$ to each element within a continuous sub-array of $A$. A
\textit{query} operation computes the sum of all the elements within a
continuous sub-array of $A$. The one-dimensional update and query handling
problem has been studied intensively and is usually solved using segment trees
with \textit{lazy propagation} technique. In this paper, we present a new
algorithm incorporating Binary Indexed Trees and Inclusion-Exclusion Principle
to accomplish the same task. We extend the algorithm to update and query
sub-matrices of matrices (two-dimensional array). Finally, we propose a general
form of the algorithm for $d$-dimensions which achieves
$\mathcal{O}(2^d*\log^{d}n)$ time complexity for both updates and queries. This
is an improvement over the previously known algorithms which utilize
hierarchical data structures like quadtrees and octrees and have a worst-case
time complexity of $\Omega(n^{d-1})$ per update/query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6094</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6094</id><created>2013-11-24</created><authors><author><keyname>Maasoumy</keyname><forenames>Mehdi</forenames></author><author><keyname>Ortiz</keyname><forenames>Jorge</forenames></author><author><keyname>Culler</keyname><forenames>David</forenames></author><author><keyname>Sangiovanni-Vincentelli</keyname><forenames>Alberto</forenames></author></authors><title>Flexibility of Commercial Building HVAC Fan as Ancillary Service for
  Smart Grid</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we model energy use in commercial buildings using empirical
data captured through sMAP, a campus building data portal at UC Berkeley. We
conduct at-scale experiments in a newly constructed building on campus. By
modulating the supply duct static pressure (SDSP) for the main supply air duct,
we induce a response on the main supply fan and determine how much ancillary
power flexibility can be provided by a typical commercial building. We show
that the consequent intermittent fluctuations in the air mass flow into the
building does not influence the building climate in a human-noticeable way. We
estimate that at least 4 GW of regulation reserve is readily available only
through commercial buildings in the US. Based on predictions this value will
reach to 5.6 GW in 2035. We also show how thermal slack can be leveraged to
provide an ancillary service to deal with transient frequency fluctuations in
the grid. We consider a simplified model of the grid power system with time
varying demand and generation and present a simple control scheme to direct the
ancillary service power flow from buildings to improve on the classical
automatic generation control (AGC)-based approach. Simulation results are
provided to show the effectiveness of the proposed methodology for enhancing
grid frequency regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6107</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6107</id><created>2013-11-24</created><updated>2014-05-11</updated><authors><author><keyname>Luo</keyname><forenames>Biao</forenames></author><author><keyname>Wu</keyname><forenames>Huai-Ning</forenames></author><author><keyname>Huang</keyname><forenames>Tingwen</forenames></author></authors><title>Off-policy reinforcement learning for $ H_\infty $ control design</title><categories>cs.SY cs.LG math.OC stat.ML</categories><comments>Accepted by IEEE Transactions on Cybernetics. IEEE Transactions on
  Cybernetics, Online Available, 2014</comments><doi>10.1109/TCYB.2014.2319577</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $H_\infty$ control design problem is considered for nonlinear systems
with unknown internal system model. It is known that the nonlinear $ H_\infty $
control problem can be transformed into solving the so-called
Hamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial
differential equation that is generally impossible to be solved analytically.
Even worse, model-based approaches cannot be used for approximately solving HJI
equation, when the accurate system model is unavailable or costly to obtain in
practice. To overcome these difficulties, an off-policy reinforcement leaning
(RL) method is introduced to learn the solution of HJI equation from real
system data instead of mathematical system model, and its convergence is
proved. In the off-policy RL method, the system data can be generated with
arbitrary policies rather than the evaluating policy, which is extremely
important and promising for practical systems. For implementation purpose, a
neural network (NN) based actor-critic structure is employed and a least-square
NN weight update algorithm is derived based on the method of weighted
residuals. Finally, the developed NN-based off-policy RL method is tested on a
linear F16 aircraft plant, and further applied to a rotational/translational
actuator system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6125</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6125</id><created>2013-11-24</created><updated>2014-01-20</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author><author><keyname>Malacaria</keyname><forenames>Pasquale</forenames></author></authors><title>Full Abstraction for PCF</title><categories>cs.LO</categories><comments>50 pages</comments><journal-ref>Information and Computation vol. 163 no. 2 (2000), pages 409-470</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intensional model for the programming language PCF is described, in which
the types of PCF are interpreted by games, and the terms by certain
&quot;history-free&quot; strategies. This model is shown to capture definability in PCF.
More precisely, every compact strategy in the model is definable in a certain
simple extension of PCF. We then introduce an intrinsic preorder on strategies,
and show that it satisfies some striking properties, such that the intrinsic
preorder on function types coincides with the pointwise preorder. We then
obtain an order-extensional fully abstract model of PCF by quotienting the
intensional model by the intrinsic preorder. This is the first
syntax-independent description of the fully abstract model for PCF. (Hyland and
Ong have obtained very similar results by a somewhat different route,
independently and at the same time).
  We then consider the effective version of our model, and prove a Universality
Theorem: every element of the effective extensional model is definable in PCF.
Equivalently, every recursive strategy is definable up to observational
equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6126</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6126</id><created>2013-11-24</created><authors><author><keyname>Oliveira</keyname><forenames>Leonardo I. L.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author></authors><title>An energy function and its application to the periodic behavior of
  k-reversible processes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the graph dynamical systems known as k-reversible processes. In
such processes, each vertex in the graph has one of two possible states at each
discrete time step. Each vertex changes its state between the current time and
the next if and only if it currently has at least k neighbors in a state
different than its own. For such processes, we present a monotonic function
similar to the decreasing energy functions used to study threshold networks.
Using this new function, we show an alternative proof for the maximum period
length in a k-reversible process and provide better upper bounds on the
transient length in both the general case and the case of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6145</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6145</id><created>2013-11-24</created><authors><author><keyname>Gmehlich</keyname><forenames>Rainer</forenames></author><author><keyname>Grau</keyname><forenames>Katrin</forenames></author><author><keyname>Iliasov</keyname><forenames>Alexei</forenames></author><author><keyname>Jackson</keyname><forenames>Michael</forenames></author><author><keyname>Loesch</keyname><forenames>Felix</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author></authors><title>Towards a Formalism-Based Toolkit for Automotive Applications</title><categories>cs.SE</categories><journal-ref>1st FME Workshop on Formal Methods in Software Engineering
  (FormaliSE), May 25, 2013, San Francisco, CA, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of a number of projects has been shown to be significantly
improved by the use of a formalism. However, there remains an open issue: to
what extent can a development process based on a singular formal notation and
method succeed. The majority of approaches demonstrate a low level of
flexibility by attempting to use a single notation to express all of the
different aspects encountered in software development. Often, these approaches
leave a number of scalability issues open. We prefer a more eclectic approach.
In our experience, the use of a formalism-based toolkit with adequate notations
for each development phase is a viable solution. Following this principle, any
specific notation is used only where and when it is really suitable and not
necessarily over the entire software lifecycle. The approach explored in this
article is perhaps slowly emerging in practice - we hope to accelerate its
adoption. However, the major challenge is still finding the best way to
instantiate it for each specific application scenario. In this work, we
describe a development process and method for automotive applications which
consists of five phases. The process recognizes the need for having adequate
(and tailored) notations (Problem Frames, Requirements State Machine Language,
and Event-B) for each development phase as well as direct traceability between
the documents produced during each phase. This allows for a stepwise
verification/validation of the system under development. The ideas for the
formal development method have evolved over two significant case studies
carried out in the DEPLOY project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6146</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6146</id><created>2013-11-24</created><authors><author><keyname>Zhou</keyname><forenames>Qunzhi</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Prasanna</keyname><forenames>Viktor</forenames></author></authors><title>On Using Complex Event Processing for Dynamic Demand Response
  Optimization in Microgrid</title><categories>cs.DC cs.NI</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Demand-side load reduction is a key benefit of Smart Grids. However, existing
demand response optimization (DR) programs fail to effectively leverage the
near-realtime information available from smart meters and Building Area
Networks to respond dynamically to changing energy use profiles. We investigate
the use of semantic Complex Event Processing (CEP) patterns to model and detect
dynamic situations in a campus microgrid to facilitate adaptive DR. Our focus
is on demand-side management rather than supply-side constraints. Continuous
data from information sources like smart meters and building sensors are
abstracted as event streams. Event patterns for situations that assist with DR
are detected from them. Specifically, we offer a taxonomy of event patterns
that can guide operators to define situations of interest and we illustrate its
efficacy for DR by applying these patterns on realtime events in the USC Campus
microgrid using our CEP framework
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6149</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6149</id><created>2013-11-24</created><authors><author><keyname>Benmerzoug</keyname><forenames>Djamel</forenames></author></authors><title>Agent Approach in Support of Enterprise Application Integration</title><categories>cs.MA</categories><comments>7 pages, Journal paper</comments><journal-ref>International Journal of Computer Science and Telecommunications
  [Volume 4, Issue 1, January 2013]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present approach highlights the synergies between application integration
and interaction protocols. Since both fields have advanced in different
directions, a number of important technical problems can be addressed by their
proper synthesis. In our previous work, we proposed a methodological approach
based on Interaction Protocols for Enterprise Applica tion Integration (EAI).
This approach permits to specify MAS (Multi-Agent System) interaction
protocols, verify their behavior and use them to integrate multiple business
applications. The result of the proposed approach is a validated interaction
protocol. Based on this protocol, we define in this paper, an agent- based
architecture for the EAI. It includes all the concepts nec- essary to support
communication and coordination mechanisms such as inter-agent and agent-Web
services communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6163</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6163</id><created>2013-11-24</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author></authors><title>Analytical Studies of Quasi Steady-State Model in Power System Long-Term
  Stability Analysis</title><categories>cs.SY</categories><comments>This paper has been accepted by IEEE Transactions on Circuits and
  Systems I</comments><doi>10.1109/TCSI.2013.2284171</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a theoretical foundation for the Quasi Steady-State (QSS)
model in power system long-term stability analysis is developed. Sufficient
conditions under which the QSS model gives accurate approximations of the
long-term stability model in terms of trajectory and !-limit set are derived.
These sufficient conditions provide some physical insights regarding the reason
for the failure of the QSS model. Additionally, several numerical examples are
presented to illustrate the analytical results derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6165</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6165</id><created>2013-11-24</created><updated>2015-03-30</updated><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Liu</keyname><forenames>Xingjian</forenames></author></authors><title>Automated identification and characterization of parcels (AICP) with
  OpenStreetMap and Points of Interest</title><categories>cs.CY cs.DB</categories><comments>26 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Against the paucity of urban parcels in China, this paper proposes a method
to automatically identify and characterize parcels (AICP) with OpenStreetMap
(OSM) and Points of Interest (POI) data. Parcels are the basic spatial units
for fine-scale urban modeling, urban studies, as well as spatial planning.
Conventional ways of identification and characterization of parcels rely on
remote sensing and field surveys, which are labor intensive and
resource-consuming. Poorly developed digital infrastructure, limited resources,
and institutional barriers have all hampered the gathering and application of
parcel data in developing countries. Against this backdrop, we employ OSM road
networks to identify parcel geometries and POI data to infer parcel
characteristics. A vector-based CA model is adopted to select urban parcels.
The method is applied to the entire state of China and identifies 82,645 urban
parcels in 297 cities. Notwithstanding all the caveats of open and/or
crowd-sourced data, our approach could produce reasonably good approximation of
parcels identified from conventional methods, thus having the potential to
become a useful supplement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6178</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6178</id><created>2013-11-24</created><authors><author><keyname>Poostindouz</keyname><forenames>Alireza</forenames></author><author><keyname>Aghajan</keyname><forenames>Adel</forenames></author></authors><title>Minimum Delay Huffman Code in Backward Decoding Procedure</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For some applications where the speed of decoding and the fault tolerance are
important, like in video storing, one of the successful answers is Fix-Free
Codes. These codes have been applied in some standards like H.263+ and MPEG-4.
The cost of using fix-free codes is to increase the redundancy of the code
which means the increase in the amount of bits we need to represent any peace
of information. Thus we investigated the use of Huffman Codes with low and
negligible backward decoding delay. We showed that for almost all cases there
is always a Minimum Delay Huffman Code for a given length vector. The average
delay of this code for anti-uniform sources is calculated, that is in agreement
with the simulations, and it is shown that this delay is one bit for large
alphabet sources. Also an algorithm is proposed which will find the minimum
delay code with a good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6183</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6183</id><created>2013-11-24</created><authors><author><keyname>Marandi</keyname><forenames>Parisa Jalili</forenames></author><author><keyname>Bezerra</keyname><forenames>Carlos Eduardo</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author></authors><title>Rethinking State-Machine Replication for Parallelism</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-machine replication, a fundamental approach to designing fault-tolerant
services, requires commands to be executed in the same order by all replicas.
Moreover, command execution must be deterministic: each replica must produce
the same output upon executing the same sequence of commands. These
requirements usually result in single-threaded replicas, which hinders service
performance. This paper introduces Parallel State-Machine Replication (P-SMR),
a new approach to parallelism in state-machine replication. P-SMR scales better
than previous proposals since no component plays a centralizing role in the
execution of independent commands---those that can be executed concurrently, as
defined by the service. The paper introduces P-SMR, describes a &quot;commodified
architecture&quot; to implement it, and compares its performance to other proposals
using a key-value store and a networked file system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6184</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6184</id><created>2013-11-24</created><updated>2014-05-09</updated><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author></authors><title>Bounding the Test Log-Likelihood of Generative Models</title><categories>cs.LG</categories><comments>10 pages, 1 figure, 2 tables. International Conference on Learning
  Representations (ICLR'2014, conference track)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several interesting generative learning algorithms involve a complex
probability distribution over many random variables, involving intractable
normalization constants or latent variable normalization. Some of them may even
not have an analytic expression for the unnormalized probability function and
no tractable approximation. This makes it difficult to estimate the quality of
these models, once they have been trained, or to monitor their quality (e.g.
for early stopping) while training. A previously proposed method is based on
constructing a non-parametric density estimator of the model's probability
function from samples generated by the model. We revisit this idea, propose a
more efficient estimator, and prove that it provides a lower bound on the true
test log-likelihood, and an unbiased estimator as the number of generated
samples goes to infinity, although one that incorporates the effect of poor
mixing. We further propose a biased variant of the estimator that can be used
reliably with a finite number of samples for the purpose of model comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6192</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6192</id><created>2013-11-24</created><updated>2013-12-26</updated><authors><author><keyname>Shigeta</keyname><forenames>Manami</forenames></author><author><keyname>Amano</keyname><forenames>Kazuyuki</forenames></author></authors><title>Ordered Biclique Partitions and Communication Complexity Problems</title><categories>cs.CC cs.DM math.CO</categories><comments>8 pages; the version submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ordered biclique partition of the complete graph $K_n$ on $n$ vertices is
a collection of bicliques (i.e., complete bipartite graphs) such that (i) every
edge of $K_n$ is covered by at least one and at most two bicliques in the
collection, and (ii) if an edge $e$ is covered by two bicliques then each
endpoint of $e$ is in the first class in one of these bicliques and in the
second class in other one. In this note, we give an explicit construction of
such a collection of size $n^{1/2+o(1)}$, which improves the $O(n^{2/3})$ bound
shown in the previous work [Disc. Appl. Math., 2014].
  As the immediate consequences of this result, we show (i) a construction of
$n \times n$ 0/1 matrices of rank $n^{1/2+o(1)}$ which have a fooling set of
size $n$, i.e., the gap between rank and fooling set size can be at least
almost quadratic, and (ii) an improved lower bound $(2-o(1)) \log N$ on the
nondeterministic communication complexity of the clique vs. independent set
problem, which matches the best known lower bound on the deterministic version
of the problem shown by Kushilevitz, Linial and Ostrovsky [Combinatorica,
1999].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6199</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6199</id><created>2013-11-24</created><authors><author><keyname>Yeh</keyname><forenames>Hen-Geul</forenames></author><author><keyname>Doan</keyname><forenames>Son H.</forenames></author></authors><title>Battery Placement on Performance of VAR Controls</title><categories>cs.SY</categories><comments>Proceedings of Green Energy and Systems Conference 2013, November 25,
  Long Beach, CA, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Battery's role in the development of smart grid is gaining greater attention
as an energy storage device that can be integrated with a Photovoltaic (PV)
cell in the distribution circuit. As more PVs are connected to the system, real
power injection to the distribution can cause fluctuation in the voltage. Due
to the rapid fluctuation of the voltage, a more advanced volt-ampere reactive
(VAR) power control scheme on a fast time scale is used to minimize the voltage
deviation on the distribution. Employing both global and local dynamic VAR
control schemes in our previous work, we show the effects of battery placement
on the performance of VAR controls in the example of a single branch radial
distribution circuit. Simulations verify that having battery placement at the
rear in the distribution circuit can provide smaller voltage variations and
higher energy savings than front battery placement when used with dynamic VAR
control algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6204</identifier>
 <datestamp>2014-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6204</id><created>2013-11-24</created><updated>2014-07-23</updated><authors><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Approximating Hereditary Discrepancy via Small Width Ellipsoids</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Discrepancy of a hypergraph is the minimum attainable value, over
two-colorings of its vertices, of the maximum absolute imbalance of any
hyperedge. The Hereditary Discrepancy of a hypergraph, defined as the maximum
discrepancy of a restriction of the hypergraph to a subset of its vertices, is
a measure of its complexity. Lovasz, Spencer and Vesztergombi (1986) related
the natural extension of this quantity to matrices to rounding algorithms for
linear programs, and gave a determinant based lower bound on the hereditary
discrepancy. Matousek (2011) showed that this bound is tight up to a
polylogarithmic factor, leaving open the question of actually computing this
bound. Recent work by Nikolov, Talwar and Zhang (2013) showed a polynomial time
$\tilde{O}(\log^3 n)$-approximation to hereditary discrepancy, as a by-product
of their work in differential privacy. In this paper, we give a direct simple
$O(\log^{3/2} n)$-approximation algorithm for this problem. We show that up to
this approximation factor, the hereditary discrepancy of a matrix $A$ is
characterized by the optimal value of simple geometric convex program that
seeks to minimize the largest $\ell_{\infty}$ norm of any point in a ellipsoid
containing the columns of $A$. This characterization promises to be a useful
tool in discrepancy theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6208</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6208</id><created>2013-11-24</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Kim</keyname><forenames>Na-Rae</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Channel and Noise Models for Nonlinear Molecular Communication Systems</title><categories>cs.ET</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a tabletop molecular communication platform has been developed for
transmitting short text messages across a room. The end-to-end system impulse
response for this platform does not follow previously published theoretical
works because of imperfect receiver, transmitter, and turbulent flows.
Moreover, it is observed that this platform resembles a nonlinear system, which
makes the rich body of theoretical work that has been developed by
communication engineers not applicable to this platform. In this work, we first
introduce corrections to the previous theoretical models of the end-to-end
system impulse response based on the observed data from experimentation. Using
the corrected impulse response models, we then formulate the nonlinearity of
the system as noise and show that through simplifying assumptions it can be
represented as Gaussian noise. Through formulating the system's nonlinearity as
the output a linear system corrupted by noise, the rich toolbox of mathematical
models of communication systems, most of which are based on linearity
assumption, can be applied to this platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6209</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6209</id><created>2013-11-25</created><updated>2015-09-16</updated><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author></authors><title>Distributed Computation of Large-scale Graph Problems</title><categories>cs.DC cs.DS</categories><comments>In Proceedings of SODA 2015</comments><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the increasing need for fast distributed processing of
large-scale graphs such as the Web graph and various social networks, we study
a message-passing distributed computing model for graph processing and present
lower bounds and algorithms for several graph problems. This work is inspired
by recent large-scale graph processing systems (e.g., Pregel and Giraph) which
are designed based on the message-passing model of distributed computing.
  Our model consists of a point-to-point communication network of $k$ machines
interconnected by bandwidth-restricted links. Communicating data between the
machines is the costly operation (as opposed to local computation). The network
is used to process an arbitrary $n$-node input graph (typically $n \gg k &gt; 1$)
that is randomly partitioned among the $k$ machines (a common implementation in
many real world systems). Our goal is to study fundamental complexity bounds
for solving graph problems in this model.
  We present techniques for obtaining lower bounds on the distributed time
complexity. Our lower bounds develop and use new bounds in random-partition
communication complexity. We first show a lower bound of $\Omega(n/k)$ rounds
for computing a spanning tree (ST) of the input graph. This result also implies
the same bound for other fundamental problems such as computing a minimum
spanning tree (MST). We also show an $\Omega(n/k^2)$ lower bound for
connectivity, ST verification and other related problems.
  We give algorithms for various fundamental graph problems in our model. We
show that problems such as PageRank, MST, connectivity, and graph covering can
be solved in $\tilde{O}(n/k)$ time, whereas for shortest paths, we present
algorithms that run in $\tilde{O}(n/\sqrt{k})$ time (for $(1+\epsilon)$-factor
approx.) and in $\tilde{O}(n/k)$ time (for $O(\log n)$-factor approx.)
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6211</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6211</id><created>2013-11-25</created><authors><author><keyname>Lou</keyname><forenames>Qi</forenames></author><author><keyname>Raich</keyname><forenames>Raviv</forenames></author><author><keyname>Briggs</keyname><forenames>Forrest</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author></authors><title>Novelty Detection Under Multi-Instance Multi-Label Framework</title><categories>cs.LG</categories><doi>10.1109/MLSP.2013.6661985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novelty detection plays an important role in machine learning and signal
processing. This paper studies novelty detection in a new setting where the
data object is represented as a bag of instances and associated with multiple
class labels, referred to as multi-instance multi-label (MIML) learning.
Contrary to the common assumption in MIML that each instance in a bag belongs
to one of the known classes, in novelty detection, we focus on the scenario
where bags may contain novel-class instances. The goal is to determine, for any
given instance in a new bag, whether it belongs to a known class or a novel
class. Detecting novelty in the MIML setting captures many real-world phenomena
and has many potential applications. For example, in a collection of tagged
images, the tag may only cover a subset of objects existing in the images.
Discovering an object whose class has not been previously tagged can be useful
for the purpose of soliciting a label for the new object class. To address this
novel problem, we present a discriminative framework for detecting new class
instances. Experiments demonstrate the effectiveness of our proposed method,
and reveal that the presence of unlabeled novel instances in training bags is
helpful to the detection of such instances in testing stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6215</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6215</id><created>2013-11-25</created><authors><author><keyname>Wolff</keyname><forenames>Val&#xe9;ry</forenames><affiliation>LaMCoS</affiliation></author><author><keyname>Dinh</keyname><forenames>Tin Tran</forenames><affiliation>LaMCoS</affiliation></author><author><keyname>Raynaud</keyname><forenames>Stephane</forenames><affiliation>INSA Lyon</affiliation></author></authors><title>Using virtual parts to optimize the metrology process</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Tehnomus journal 19, 1 (2012) 9-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the measurement process, there are many parameters affecting the
measurement results: the influence of the probe system, material stiffness of
measured workpiece, the calibration of the probe with a reference sphere, the
thermal effects. We want to obtain the limits of a measurement methodology to
be able to validate a result. The study is applied to a simple part. We observe
the dispersion of the position of different drilled holes (XYZ values in a
coordinate system) when we change the quality of the part and the method of
calculation. We use the Design of Experiment (Taguchi method) to realize our
study. We study the influence of the part quality on a measurement results. We
consider two parameters to define the part quality (flatness and
perpendicularity). We will also study the influence of different methods of
calculation to determine the coordinate system. We can use two options in
Metrolog XG software (tangent plane with or without orientation constraint).
The originality of this paper is that we present a method for the design of
experiment that uses CATIA (CAD system) to generate the measured parts. In this
way we can realize a design of experiment with a largest number of experimental
results. This is a positive point for a statistical analysis. We are also free
to define the parts we want to study without manufacturing difficulties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6221</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6221</id><created>2013-11-25</created><authors><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>Kettunen</keyname><forenames>Petri</forenames></author><author><keyname>Pagels</keyname><forenames>Max</forenames></author><author><keyname>Partanen</keyname><forenames>Jari</forenames></author></authors><title>The Effects of GQM+Strategies on Organizational Alignment</title><categories>cs.SE</categories><comments>15 pages. Proceedings of the DASMA Software Metric Congress (MetriKon
  2013): Magdeburger Schriften zum Empirischen Software Engineering, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing role of software for developing products and services requires
that organizations align their software-related activities with high-level
business goals. In practice, this alignment is very difficult and only little
systematic support is available. GQM+Strategies is a method that aims at
aligning organizational goals, strategies, and measurements at all levels of an
organization in a seamless way. This article describes a case study of applying
GQM+Strategies in a globally op- erating industrial R&amp;D organization developing
special-purpose device products for B2B customers. The study analyzes how
GQM+Strategies has helped clarify and harmonize the goal set of the
organization. Results of the study indicate improved alignment and integration
of different goals. In addition, the method helped to make the initially
informal goal-setting more transparent and consequently enabled revising it
while new, more important goals were discovered and comprehended. Moreover,
several elements affecting the achievement of goals as well as impediments were
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6224</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6224</id><created>2013-11-25</created><authors><author><keyname>Basili</keyname><forenames>Victor R.</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>Lindvall</keyname><forenames>Mikael</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Regardie</keyname><forenames>Myrna</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author><author><keyname>Seaman</keyname><forenames>Carolyn</forenames></author><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author></authors><title>Linking Software Development and Business Strategy Through Measurement</title><categories>cs.SE</categories><comments>16 pages. The final version of this paper is available at
  http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5445168</comments><journal-ref>IEEE Computer 43, no. 4 (2010): 57-65</journal-ref><doi>10.1109/MC.2010.108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of today's products and services are software-based. Organizations that
develop software want to maintain and improve their competitiveness by
controlling software-related risks. To do this, they need to align their
business goals with software development strategies and translate them into
quantitative project management. There is also an increasing need to justify
cost and resources for software and system development and other IT services by
demonstrating their impact on an organisation's higher-level goals. For both,
linking business goals and software-related efforts in an organization is
necessary. However, this is a challenging task, and there is a lack of methods
addressing this gap. The GQM+Strategies approach effectively links goals and
strategies on all levels of an organization by means of goal-oriented
measurement. The approach is based on rationales for deciding about options
when operationalizing goals and for evaluating the success of strategies with
respect to goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6227</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6227</id><created>2013-11-25</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Sharma</keyname><forenames>Manoj</forenames></author><author><keyname>Joshi</keyname><forenames>Gajanan</forenames></author><author><keyname>Pagare</keyname><forenames>Trupti</forenames></author><author><keyname>Palwe</keyname><forenames>Adarsha</forenames></author></authors><title>Experience of Developing a Meta-Semantic Search Engine</title><categories>cs.IR</categories><comments>4 pages, 9 figures, 1 table. CUBE 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thinking of todays web search scenario which is mainly keyword based, leads
to the need of effective and meaningful search provided by Semantic Web.
Existing search engines are vulnerable to provide relevant answers to users
query due to their dependency on simple data available in web pages. On other
hand, semantic search engines provide efficient and relevant results as the
semantic web manages information with well defined meaning using ontology. A
Meta-Search engine is a search tool that forwards users query to several
existing search engines and provides combined results by using their own page
ranking algorithm. SemanTelli is a meta semantic search engine that fetches
results from different semantic search engines such as Hakia, DuckDuckGo,
SenseBot through intelligent agents. This paper proposes enhancement of
SemanTelli with improved snippet analysis based page ranking algorithm and
support for image and news search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6229</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6229</id><created>2013-11-25</created><authors><author><keyname>Bala</keyname><forenames>Mohammad Irfan</forenames></author><author><keyname>Vij</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Intelligent Agent for Prediction in E- Negotiation: An Approach</title><categories>cs.MA</categories><comments>5 pages, 4 figures, 1 table. CUBE 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of web technologies it becomes more and more important
to make the traditional negotiation pricing mechanism automated and
intelligent. The behaviour of software agents which negotiate on behalf of
humans is determined by their tactics in the form of decision functions.
Prediction of partners behaviour in negotiation has been an active research
direction in recent years as it will improve the utility gain for the adaptive
negotiation agent and also achieve the agreement much quicker or look after
much higher benefits. In this paper we review the various negotiation methods
and the existing architecture. Although negotiation is practically very complex
activity to automate without human intervention we have proposed architecture
for predicting the opponents behaviour which will take into consideration
various factors which affect the process of negotiation. The basic concept is
that the information about negotiators, their individual actions and dynamics
can be used by software agents equipped with adaptive capabilities to learn
from past negotiations and assist in selecting appropriate negotiation tactics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6230</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6230</id><created>2013-11-25</created><updated>2014-10-18</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>Privacy-Preserving Verifiable Incentive Mechanism for Crowdsourcing
  Market Applications</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a novel class of incentive mechanisms is proposed to attract
extensive users to truthfully participate in crowd sensing applications with a
given budget constraint. The class mechanisms also bring good service quality
for the requesters in crowd sensing applications. Although it is so important,
there still exists many verification and privacy challenges, including users'
bids and subtask information privacy and identification privacy, winners' set
privacy of the platform, and the security of the payment outcomes. In this
paper, we present a privacy-preserving verifiable incentive mechanism for crowd
sensing applications with the budget constraint, not only to explore how to
protect the privacies of users and the platform, but also to make the
verifiable payment correct between the platform and users for crowd sensing
applications. Results indicate that our privacy-preserving verifiable incentive
mechanism achieves the same results as the generic one without privacy
preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6233</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6233</id><created>2013-11-25</created><authors><author><keyname>More</keyname><forenames>Amruta</forenames></author><author><keyname>Vij</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Agent Based Negotiation using Cloud - an Approach in E-Commerce</title><categories>cs.MA</categories><comments>8 pages, 2 figures. CSI 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing allows subscription based access to computing. It also allows
storage services over Internet. Automated Negotiation is becoming an emerging,
and important area in the field of Multi Agent Systems in ECommerce. Multi
Agent based negotiation system is necessary to increase the efficiency of
E-negotiation process. Cloud computing provides security and privacy to the
user data and low maintenance costs. We propose a Negotiation system using
cloud. In this system, all product information and multiple agent details are
stored on cloud. Both parties select their agents through cloud for
negotiation. Agent acts as a negotiator. Agents have users details and their
requirements for a particular product. Using users requirement, agents
negotiate on some issues such as price, volume, duration, quality and so on.
After completing negotiation process, agents give feedback to the user about
whether negotiation is successful or not. This negotiation system is dynamic in
nature and increases the agents with the increase in participating user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6235</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6235</id><created>2013-11-25</created><updated>2014-10-13</updated><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>Internal Pattern Matching Queries in a Text and Applications</title><categories>cs.DS</categories><comments>31 pages, 9 figures; accepted to SODA 2015</comments><msc-class>68P05 (Primary) 68W05 (Secondary)</msc-class><acm-class>E.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider several types of internal queries: questions about subwords of a
text. As the main tool we develop an optimal data structure for the problem
called here internal pattern matching. This data structure provides
constant-time answers to queries about occurrences of one subword $x$ in
another subword $y$ of a given text, assuming that $|y|=\mathcal{O}(|x|)$,
which allows for a constant-space representation of all occurrences. This
problem can be viewed as a natural extension of the well-studied pattern
matching problem. The data structure has linear size and admits a linear-time
construction algorithm.
  Using the solution to the internal pattern matching problem, we obtain very
efficient data structures answering queries about: primitivity of subwords,
periods of subwords, general substring compression, and cyclic equivalence of
two subwords. All these results improve upon the best previously known
counterparts. The linear construction time of our data structure also allows to
improve the algorithm for finding $\delta$-subrepetitions in a text (a more
general version of maximal repetitions, also called runs). For any fixed
$\delta$ we obtain the first linear-time algorithm, which matches the linear
time complexity of the algorithm computing runs. Our data structure has already
been used as a part of the efficient solutions for subword suffix rank &amp;
selection, as well as substring compression using Burrows-Wheeler transform
composed with run-length encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6236</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6236</id><created>2013-11-25</created><authors><author><keyname>Soriente</keyname><forenames>Claudio</forenames></author><author><keyname>Karame</keyname><forenames>Ghassan</forenames></author><author><keyname>Ritzdorf</keyname><forenames>Hubert</forenames></author><author><keyname>Marinovic</keyname><forenames>Srdjan</forenames></author><author><keyname>Capkun</keyname><forenames>Srdjan</forenames></author></authors><title>Commune: Shared Ownership in an Agnostic Cloud</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although cloud storage platforms promise a convenient way for users to share
files and engage in collaborations, they require all files to have a single
owner who unilaterally makes access control decisions. Existing clouds are,
thus, agnostic to shared ownership. This can be a significant limitation in
many collaborations because one owner can, for example, delete files and revoke
access without consulting the other collaborators.
  In this paper, we first formally define a notion of shared ownership within a
file access control model. We then propose a solution, called Commune, to the
problem of distributively enforcing shared ownership in agnostic clouds, so
that access grants require the support of a pre-arranged threshold of owners.
Commune can be used in existing clouds without requiring any modifications to
the platforms. We analyze the security of our solution and evaluate its
scalability and performance by means of an implementation integrated with
Amazon S3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6239</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6239</id><created>2013-11-25</created><updated>2014-10-10</updated><authors><author><keyname>Bourrier</keyname><forenames>Anthony</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author><author><keyname>Peleg</keyname><forenames>Tomer</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Patrick</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Fundamental performance limits for ideal decoders in high-dimensional
  linear inverse problems</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, 60(12):7928-7946,
  December 2014</journal-ref><doi>10.1109/TIT.2014.2364403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on characterizing the fundamental performance limits that
can be expected from an ideal decoder given a general model, ie, a general
subset of &quot;simple&quot; vectors of interest. First, we extend the so-called notion
of instance optimality of a decoder to settings where one only wishes to
reconstruct some part of the original high dimensional vector from a
low-dimensional observation. This covers practical settings such as medical
imaging of a region of interest, or audio source separation when one is only
interested in estimating the contribution of a specific instrument to a musical
recording. We define instance optimality relatively to a model much beyond the
traditional framework of sparse recovery, and characterize the existence of an
instance optimal decoder in terms of joint properties of the model and the
considered linear operator. Noiseless and noise-robust settings are both
considered. We show somewhat surprisingly that the existence of noise-aware
instance optimal decoders for all noise levels implies the existence of a
noise-blind decoder. A consequence of our results is that for models that are
rich enough to contain an orthonormal basis, the existence of an L2/L2 instance
optimal decoder is only possible when the linear operator is not substantially
dimension-reducing. This covers well-known cases (sparse vectors, low-rank
matrices) as well as a number of seemingly new situations (structured sparsity
and sparse inverse covariance matrices for instance). We exhibit an
operator-dependent norm which, under a model-specific generalization of the
Restricted Isometry Property (RIP), always yields a feasible instance
optimality property. This norm can be upper bounded by an atomic norm relative
to the considered model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6240</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6240</id><created>2013-11-25</created><authors><author><keyname>Sonawani</keyname><forenames>Shilpa</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>A Decision Tree Approach to Classify Web Services using Quality
  Parameters</title><categories>cs.IR</categories><comments>9 pages, 3 tables; ICWA 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase in the number of web services, many web services are
available on internet providing the same functionality, making it difficult to
choose the best one, fulfilling users all requirements. This problem can be
solved by considering the quality of web services to distinguish functionally
similar web services. Nine different quality parameters are considered. Web
services can be classified and ranked using decision tree approach since they
do not require long training period and can be easily interpreted. Various
decision tree and rules approaches available are applied and tested to find the
optimal decision method to correctly classify functionally similar web services
considering their quality parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6243</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6243</id><created>2013-11-25</created><authors><author><keyname>Sinha</keyname><forenames>Sukanta</forenames></author><author><keyname>Dattagupta</keyname><forenames>Rana</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Web-page Indexing based on the Prioritize Ontology Terms</title><categories>cs.IR</categories><comments>9 pages, 3 figures, 2 tables. ICWA 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this world, globalization has become a basic and most popular human trend.
To globalize information, people are going to publish the documents in the
internet. As a result, information volume of internet has become huge. To
handle that huge volume of information, Web searcher uses search engines. The
Webpage indexing mechanism of a search engine plays a big role to retrieve Web
search results in a faster way from the huge volume of Web resources. Web
researchers have introduced various types of Web-page indexing mechanism to
retrieve Webpages from Webpage repository. In this paper, we have illustrated a
new approach of design and development of Webpage indexing. The proposed
Webpage indexing mechanism has applied on domain specific Webpages and we have
identified the Webpage domain based on an Ontology. In our approach, first we
prioritize the Ontology terms that exist in the Webpage content then apply our
own indexing mechanism to index that Webpage. The main advantage of storing an
index is to optimize the speed and performance while finding relevant documents
from the domain specific search engine storage area for a user given search
query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6245</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6245</id><created>2013-11-25</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Shikalgar</keyname><forenames>Sajeeda</forenames></author></authors><title>A Model Approach to Build Basic Ontology</title><categories>cs.IR</categories><comments>12 page, 3 fugures, 2 tables. ICWA 2013 International Conference.
  arXiv admin note: text overlap with arXiv:1207.2606 by other authors without
  attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As todays world grows with the technology on the other hand it seems to be
small with the World Wide Web. With the use of Internet more and more
information can be search from the web. When Users fires a query they want
relevancy in obtained results. In general, search engines perform the ranking
of web pages in an offline mode, which is after the web pages have been
retrieved and stored in the database. But most of the time this method does not
provide relevant results as most of the search engines were using some ranking
algorithms like page Rank, HITS, SALSA and Hilltop. Where these algorithms does
not always provides the results based on the semantic web. So a concept of
Ontology is been introduced in search engines to get more meaningful and
relevant results with respect to the users query.Ontologies are used to capture
knowledge about some domain of interest. Ontology describes the concepts in the
domain and also the relationships that hold between those concepts. Different
ontology languages provide different facilities. The most recent development in
standard ontology languages is OWL (Ontology Web Language) from the World Wide
Web Consortium. OWL makes it possible to describe concept to its full extent
and enables the search engines to provide accurate results to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6247</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6247</id><created>2013-11-25</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Full-Duplex Relaying with Half-Duplex Relays</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider &quot;virtual&quot; full-duplex relaying by means of half-duplex relays. In
this configuration, each relay stage in a multi-hop relaying network is formed
by at least two relays, used alternatively in transmit and receive modes, such
that while one relay transmits its signal to the next stage, the other relay
receives a signal from the previous stage. With such a pipelined scheme, the
source is active and sends a new information message in each time slot. We
consider the achievable rates for different coding schemes and compare them
with a cut-set upper bound, which is tight in certain conditions. In
particular, we show that both lattice-based Compute and Forward (CoF) and
Quantize reMap and Forward (QMF) yield attractive performance and can be easily
implemented. In particular, QMF in this context does not require &quot;long&quot;
messages and joint (non-unique) decoding, if the quantization mean-square
distortion at the relays is chosen appropriately. Also, in the multi-hop case
the gap of QMF from the cut-set upper bound grows logarithmically with the
number of stages, and not linearly as in the case of &quot;noise level&quot;
quantization. Furthermore, we show that CoF is particularly attractive in the
case of multi-hop relaying, when the channel gains have fluctuations not larger
than 3dB, yielding a rate that does not depend on the number of relaying
stages. In particular, we argue that such architecture may be useful for a
wireless backhaul with line-of-sight propagation between the relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6249</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6249</id><created>2013-11-25</created><authors><author><keyname>Schenk</keyname><forenames>Julia</forenames></author><author><keyname>Prechelt</keyname><forenames>Lutz</forenames></author><author><keyname>Salinger</keyname><forenames>Stephan</forenames></author></authors><title>Distributed-Pair Programming can work well and is not just Distributed
  Pair-Programming</title><categories>cs.SE</categories><acm-class>D.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Distributed Pair Programming can be performed via screensharing
or via a distributed IDE. The latter offers the freedom of concurrent editing
(which may be helpful or damaging) and has even more awareness deficits than
screen sharing. Objective: Characterize how competent distributed pair
programmers may handle this additional freedom and these additional awareness
deficits and characterize the impacts on the pair programming process. Method:
A revelatory case study, based on direct observation of a single, highly
competent distributed pair of industrial software developers during a 3-day
collaboration. We use recordings of these sessions and conceptualize the
phenomena seen. Results: 1. Skilled pairs may bridge the awareness deficits
without visible obstruction of the overall process. 2. Skilled pairs may use
the additional editing freedom in a useful limited fashion, resulting in
potentially better fluency of the process than local pair programming.
Conclusion: When applied skillfully in an appropriate context, distributed-pair
programming can (not will!) work at least as well as local pair programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6250</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6250</id><created>2013-11-25</created><updated>2014-05-21</updated><authors><author><keyname>Carapelle</keyname><forenames>Claudia</forenames><affiliation>Leipzig University</affiliation></author><author><keyname>Feng</keyname><forenames>Shiguang</forenames><affiliation>Leipzig University</affiliation></author><author><keyname>Gil</keyname><forenames>Oliver Fern&#xe1;ndez</forenames><affiliation>Leipzig University</affiliation></author><author><keyname>Quaas</keyname><forenames>Karin</forenames><affiliation>Leipzig University</affiliation></author></authors><title>On the Expressiveness of TPTL and MTL over \omega-Data Words</title><categories>cs.LO</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 151, 2014, pp. 174-187</journal-ref><doi>10.4204/EPTCS.151.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric Temporal Logic (MTL) and Timed Propositional Temporal Logic (TPTL) are
prominent extensions of Linear Temporal Logic to specify properties about data
languages. In this paper, we consider the class of data languages of
non-monotonic data words over the natural numbers. We prove that, in this
setting, TPTL is strictly more expressive than MTL. To this end, we introduce
Ehrenfeucht-Fraisse (EF) games for MTL. Using EF games for MTL, we also prove
that the MTL definability decision problem (&quot;Given a TPTL-formula, is the
language defined by this formula definable in MTL?&quot;) is undecidable. We also
define EF games for TPTL, and we show the effect of various syntactic
restrictions on the expressiveness of MTL and TPTL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6259</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6259</id><created>2013-11-25</created><authors><author><keyname>Konkoli</keyname><forenames>Zoran</forenames></author><author><keyname>Wendin</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Toward bio-inspired information processing with networks of nano-scale
  switching elements</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unconventional computing explores multi-scale platforms connecting
molecular-scale devices into networks for the development of scalable
neuromorphic architectures, often based on new materials and components with
new functionalities. We review some work investigating the functionalities of
locally connected networks of different types of switching elements as
computational substrates. In particular, we discuss reservoir computing with
networks of nonlinear nanoscale components. In usual neuromorphic paradigms,
the network synaptic weights are adjusted as a result of a training/learning
process. In reservoir computing, the non-linear network acts as a dynamical
system mixing and spreading the input signals over a large state space, and
only a readout layer is trained. We illustrate the most important concepts with
a few examples, featuring memristor networks with time-dependent and history
dependent resistances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6264</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6264</id><created>2013-11-25</created><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>Rosbach</keyname><forenames>Alla</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Eschbach</keyname><forenames>Robert</forenames></author></authors><title>Inspection and Test Process Integration Based on Explicit Test
  Prioritization Strategies</title><categories>cs.SE</categories><comments>12 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-27213-4_12</comments><journal-ref>Proceedings of the Software Quality Days (SWQD), pages 181-192,
  Vienna, Austria, January 17-19 2012</journal-ref><doi>10.1007/978-3-642-27213-4_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's software quality assurance techniques are often applied in isolation.
Consequently, synergies resulting from systematically integrating different
quality assurance activities are often not exploited. Such combinations promise
benefits, such as a reduction in quality assurance effort or higher defect
detection rates. The integration of inspection and testing, for instance, can
be used to guide testing activities. For example, testing activities can be
focused on defect-prone parts based upon inspection results. Existing
approaches for predicting defect-prone parts do not make systematic use of the
results from inspections. This article gives an overview of an integrated
inspection and testing approach, and presents a preliminary case study aiming
at verifying a study design for evaluating the approach. First results from
this preliminary case study indicate that synergies resulting from the
integration of inspection and testing might exist, and show a trend that
testing activities could be guided based on inspection results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6272</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6272</id><created>2013-11-25</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author></authors><title>Service based hight-speed railway base station arrangement</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by the Journal of Wireless
  Communications and Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To provide stable and high data rate wireless access for passengers in the
train, it is necessary to properly deploy base stations along the railway. We
consider this issue from the perspective of service, which is defined as the
integral of the time-varying instantaneous channel capacity. With large-scale
fading assumption, it will be shown that the total service of each base station
is inversely proportional to the velocity of the train. Besides, we find that
if the ratio of the service provided by a base station in its service region to
its total service is given, the base station interval (i.e. the distance
between two adjacent base stations) is a constant regardless of the velocity of
the train. On the other hand, if a certain amount of service is required, the
interval will increase with the velocity of the train. The above results apply
not only to simple curve rails, like line rail and arc rail, but also to any
irregular curve rail, provided that the train is travelling at a constant
velocity. Furthermore, the new developed results are applied to analyze the
on-off transmission strategy of base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6275</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6275</id><created>2013-11-25</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Dong</keyname><forenames>Yunquan</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author></authors><title>Channel Service Based High Speed Railway Base Station Arrangement</title><categories>cs.IT math.IT</categories><comments>Accepted by Int. Workshop on High Mobile Wireless Comms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of high-speed railways, demands on high mobility
wireless communication increase greatly. To provide stable and high data rate
wireless access for users in the train, it is necessary to properly deploy base
stations along the railway. In this paper, we consider this issue from the
perspective of channel service which is defined as the integral of the
time-varying instantaneous channel capacity. It will show that the total
service quantity of each base station is a constant. In order to keep high
service efficiency of the railway communication system with multiple base
stations along the railway, we need to use the time division to schedule the
multiple stations and allow one base station to work when the train is running
close to it. In this way, we find a fact that if the ratio of the service
quantity provided by each station to its total service quantity is given, the
base station interval(i.e. the distance between two adjacent base stations) is
a constant, regardless of the speed of the train. On the other hand, interval
between two neighboring base stations will increase with the speed of the
train. Furthermore, using the concept of channel service, we also analyze the
transmission strategy of base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6280</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6280</id><created>2013-11-25</created><authors><author><keyname>Banchs</keyname><forenames>Albert</forenames></author><author><keyname>Ortin</keyname><forenames>Jorge</forenames></author><author><keyname>Garcia-Saavedra</keyname><forenames>Andres</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author><author><keyname>Serrano</keyname><forenames>Pablo</forenames></author></authors><title>Thwarting Selfish Behavior in 802.11 WLANs</title><categories>cs.NI</categories><comments>14 pages, 7 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 802.11e standard enables user configuration of several MAC parameters,
making WLANs vulnerable to users that selfishly configure these parameters to
gain throughput. In this paper we propose a novel distributed algorithm to
thwart such selfish behavior. The key idea of the algorithm is for honest
stations to react, upon detecting a selfish station, by using a more aggressive
configuration that penalizes this station. We show that the proposed algorithm
guarantees global stability while providing good response times. By conducting
a game theoretic analysis of the algorithm based on repeated games, we also
show its effectiveness against selfish stations. Simulation results confirm
that the proposed algorithm optimizes throughput performance while discouraging
selfish behavior. We also present an experimental prototype of the proposed
algorithm demonstrating that it can be implemented on commodity hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6304</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6304</id><created>2013-11-25</created><authors><author><keyname>Liang</keyname><forenames>Min</forenames></author></authors><title>Tripartite Blind Quantum Computation</title><categories>quant-ph cs.CR</categories><comments>18 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a model of tripartite blind quantum computation (TBQC),
in which three independent participants hold different resources and accomplish
a computational task through cooperation. The three participants are called
C,S,T separately, where C needs to compute on his private data, and T has the
required quantum algorithm, and S provides sufficient quantum computational
resources. Then two concrete TBQC protocols are constructed. The first protocol
is designed based on Broadbent-Fitzsimons-Kashefi protocol, and it cannot
prevent from collusive attack of two participants. Then based on universal
quantum circuit, we present the second protocol which can prevent from
collusive attack. In the latter protocol, for each appearance of $R$-gate in
the circuit, one call to a classical AND-BOX is required for privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6309</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6309</id><created>2013-11-25</created><updated>2014-06-13</updated><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Pereszl&#xe9;nyi</keyname><forenames>Attila</forenames></author><author><keyname>Yao</keyname><forenames>Penghui</forenames></author></authors><title>A parallel repetition theorem for entangled two-player one-round games
  under product distributions</title><categories>quant-ph cs.CC</categories><comments>14 pages. Accepted by CCC 2014, camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a parallel repetition theorem for the entangled value $\omega^*(G)$
of any two-player one-round game $G$ where the questions $(x,y) \in
\mathcal{X}\times\mathcal{Y}$ to Alice and Bob are drawn from a product
distribution on $\mathcal{X}\times\mathcal{Y}$. We show that for the $k$-fold
product $G^k$ of the game $G$ (which represents the game $G$ played in parallel
$k$ times independently),
  $ \omega^*(G^k)
=\left(1-(1-\omega^*(G))^3\right)^{\Omega\left(\frac{k}{\log(|\mathcal{A}|
\cdot |\mathcal{B}|)}\right)} $, where $\mathcal{A}$ and $\mathcal{B}$
represent the sets from which the answers of Alice and Bob are drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6329</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6329</id><created>2013-11-25</created><updated>2014-02-25</updated><authors><author><keyname>Polikarpova</keyname><forenames>Nadia</forenames></author><author><keyname>Tschannen</keyname><forenames>Julian</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Flexible Invariants Through Semantic Collaboration</title><categories>cs.SE</categories><comments>22 pages</comments><journal-ref>Proceedings of the 19th International Symposium on Formal Methods
  (FM). Lecture Notes in Computer Science, 8442:514--530, Springer, May 2014</journal-ref><doi>10.1007/978-3-319-06410-9_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modular reasoning about class invariants is challenging in the presence of
dependencies among collaborating objects that need to maintain global
consistency. This paper presents semantic collaboration: a novel methodology to
specify and reason about class invariants of sequential object-oriented
programs, which models dependencies between collaborating objects by semantic
means. Combined with a simple ownership mechanism and useful default schemes,
semantic collaboration achieves the flexibility necessary to reason about
complicated inter-object dependencies but requires limited annotation burden
when applied to standard specification patterns. The methodology is implemented
in AutoProof, our program verifier for the Eiffel programming language (but it
is applicable to any language supporting some form of representation
invariants). An evaluation on several challenge problems proposed in the
literature demonstrates that it can handle a variety of idiomatic collaboration
patterns, and is more widely applicable than the existing invariant
methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6331</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6331</id><created>2013-11-25</created><updated>2015-09-18</updated><authors><author><keyname>D&#xfa;nlaing</keyname><forenames>Colm &#xd3;</forenames></author></authors><title>Compact families of Jordan curves and convex hulls in three dimensions</title><categories>cs.CG math.DS</categories><comments>49 pages, 20 figures</comments><report-no>TCDMATH 13-15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for certain families of semi-algebraic convex bodies in 3
dimensions, the convex hull of $n$ disjoint bodies has $O(n\lambda_s(n))$
features, where $s$ is a constant depending on the family: $\lambda_s(n)$ is
the maximum length of order-$s$ Davenport-Schinzel sequences with $n$ letters.
The argument is based on an apparently new idea of `compact families' of convex
bodies or discs, and of `crossing content' of disc intersections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6334</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6334</id><created>2013-11-25</created><authors><author><keyname>Dhanjal</keyname><forenames>Charanpal</forenames><affiliation>LTCI</affiliation></author><author><keyname>Cl&#xe9;men&#xe7;on</keyname><forenames>St&#xe9;phan</forenames><affiliation>LTCI</affiliation></author></authors><title>Learning Reputation in an Authorship Network</title><categories>cs.SI cs.IR cs.LG stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of searching for experts in a given academic field is hugely
important in both industry and academia. We study exactly this issue with
respect to a database of authors and their publications. The idea is to use
Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to perform
topic modelling in order to find authors who have worked in a query field. We
then construct a coauthorship graph and motivate the use of influence
maximisation and a variety of graph centrality measures to obtain a ranked list
of experts. The ranked lists are further improved using a Markov Chain-based
rank aggregation approach. The complete method is readily scalable to large
datasets. To demonstrate the efficacy of the approach we report on an extensive
set of computational simulations using the Arnetminer dataset. An improvement
in mean average precision is demonstrated over the baseline case of simply
using the order of authors found by the topic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6335</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6335</id><created>2013-11-25</created><authors><author><keyname>Rheinl&#xe4;nder</keyname><forenames>Astrid</forenames></author><author><keyname>Heise</keyname><forenames>Arvid</forenames></author><author><keyname>Hueske</keyname><forenames>Fabian</forenames></author><author><keyname>Leser</keyname><forenames>Ulf</forenames></author><author><keyname>Naumann</keyname><forenames>Felix</forenames></author></authors><title>SOFA: An Extensible Logical Optimizer for UDF-heavy Dataflows</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen an increased interest in large-scale analytical
dataflows on non-relational data. These dataflows are compiled into execution
graphs scheduled on large compute clusters. In many novel application areas the
predominant building blocks of such dataflows are user-defined predicates or
functions (UDFs). However, the heavy use of UDFs is not well taken into account
for dataflow optimization in current systems.
  SOFA is a novel and extensible optimizer for UDF-heavy dataflows. It builds
on a concise set of properties for describing the semantics of Map/Reduce-style
UDFs and a small set of rewrite rules, which use these properties to find a
much larger number of semantically equivalent plan rewrites than possible with
traditional techniques. A salient feature of our approach is extensibility: We
arrange user-defined operators and their properties into a subsumption
hierarchy, which considerably eases integration and optimization of new
operators. We evaluate SOFA on a selection of UDF-heavy dataflows from
different domains and compare its performance to three other algorithms for
dataflow optimization. Our experiments reveal that SOFA finds efficient plans,
outperforming the best plans found by its competitors by a factor of up to 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6355</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6355</id><created>2013-11-06</created><authors><author><keyname>Wang</keyname><forenames>Xinxi</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Hsu</keyname><forenames>David</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author></authors><title>Exploration in Interactive Personalized Music Recommendation: A
  Reinforcement Learning Approach</title><categories>cs.MM cs.IR cs.LG</categories><acm-class>H.3.3; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current music recommender systems typically act in a greedy fashion by
recommending songs with the highest user ratings. Greedy recommendation,
however, is suboptimal over the long term: it does not actively gather
information on user preferences and fails to recommend novel songs that are
potentially interesting. A successful recommender system must balance the needs
to explore user preferences and to exploit this information for recommendation.
This paper presents a new approach to music recommendation by formulating this
exploration-exploitation trade-off as a reinforcement learning task called the
multi-armed bandit. To learn user preferences, it uses a Bayesian model, which
accounts for both audio content and the novelty of recommendations. A
piecewise-linear approximation to the model and a variational inference
algorithm are employed to speed up Bayesian inference. One additional benefit
of our approach is a single unified model for both music recommendation and
playlist generation. Both simulation results and a user study indicate strong
potential for the new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6360</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6360</id><created>2013-11-25</created><updated>2014-08-04</updated><authors><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Performance Guarantees for Adaptive Estimation of Sparse Signals</title><categories>cs.IT math.IT stat.ME</categories><comments>34 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies adaptive sensing for estimating the nonzero amplitudes of
a sparse signal with the aim of providing analytical guarantees on the
performance gain due to adaptive resource allocation. We consider a previously
proposed optimal two-stage policy for allocating sensing resources. For
positive powers q, we derive tight upper bounds on the mean qth-power error
resulting from the optimal two-stage policy and corresponding lower bounds on
the improvement over non-adaptive uniform sensing. It is shown that the
adaptation gain is related to the detectability of nonzero signal components as
characterized by Chernoff coefficients, thus quantifying analytically the
dependence on the sparsity level of the signal, the signal-to-noise ratio, and
the sensing resource budget. For fixed sparsity levels and increasing
signal-to-noise ratio or sensing budget, we obtain the rate of convergence to
oracle performance and the rate at which the fraction of resources spent on the
first exploratory stage decreases to zero. For a vanishing fraction of nonzero
components, the gain increases without bound as a function of signal-to-noise
ratio and sensing budget. Numerical simulations demonstrate that the bounds on
adaptation gain are quite tight in non-asymptotic regimes as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6371</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6371</id><created>2013-11-25</created><updated>2013-11-27</updated><authors><author><keyname>Shang</keyname><forenames>Lifeng</forenames></author><author><keyname>Chan</keyname><forenames>Antoni B.</forenames></author></authors><title>On Approximate Inference for Generalized Gaussian Process Models</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized Gaussian process model (GGPM) is a unifying framework that
encompasses many existing Gaussian process (GP) models, such as GP regression,
classification, and counting. In the GGPM framework, the observation likelihood
of the GP model is itself parameterized using the exponential family
distribution (EFD). In this paper, we consider efficient algorithms for
approximate inference on GGPMs using the general form of the EFD. A particular
GP model and its associated inference algorithms can then be formed by changing
the parameters of the EFD, thus greatly simplifying its creation for
task-specific output domains. We demonstrate the efficacy of this framework by
creating several new GP models for regressing to non-negative reals and to real
intervals. We also consider a closed-form Taylor approximation for efficient
inference on GGPMs, and elaborate on its connections with other model-specific
heuristic closed-form approximations. Finally, we present a comprehensive set
of experiments to compare approximate inference algorithms on a wide variety of
GGPMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6372</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6372</id><created>2013-11-25</created><updated>2014-05-29</updated><authors><author><keyname>Rhebergen</keyname><forenames>Sander</forenames></author><author><keyname>Wells</keyname><forenames>Garth N.</forenames></author><author><keyname>Katz</keyname><forenames>Richard F.</forenames></author><author><keyname>Wathen</keyname><forenames>Andrew J.</forenames></author></authors><title>Analysis of block-preconditioners for models of coupled magma/mantle
  dynamics</title><categories>math.NA cs.CE physics.geo-ph</categories><msc-class>65F08, 76M10, 86A17, 86-08</msc-class><journal-ref>SIAM J. Sci. Comput., 36(4), A1960-A1977</journal-ref><doi>10.1137/130946678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the iterative solution of a finite element
discretisation of the magma dynamics equations. In simplified form, the magma
dynamics equations share some features of the Stokes equations. We therefore
formulate, analyse and numerically test a Elman, Silvester and Wathen-type
block preconditioner for magma dynamics. We prove analytically and demonstrate
numerically the optimality of the preconditioner. The presented analysis
highlights the dependence of the preconditioner on parameters in the magma
dynamics equations that can affect convergence of iterative linear solvers. The
analysis is verified through a range of two- and three-dimensional numerical
examples on unstructured grids, from simple illustrative problems through to
large problems on subduction zone-like geometries. The computer code to
reproduce all numerical examples is freely available as supporting material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6392</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6392</id><created>2013-11-25</created><updated>2013-12-27</updated><authors><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Comprehensive Approach to Universal Piecewise Nonlinear Regression
  Based on Trees</title><categories>cs.LG stat.ML</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate adaptive nonlinear regression and introduce
tree based piecewise linear regression algorithms that are highly efficient and
provide significantly improved performance with guaranteed upper bounds in an
individual sequence manner. We use a tree notion in order to partition the
space of regressors in a nested structure. The introduced algorithms adapt not
only their regression functions but also the complete tree structure while
achieving the performance of the &quot;best&quot; linear mixture of a doubly exponential
number of partitions, with a computational complexity only polynomial in the
number of nodes of the tree. While constructing these algorithms, we also avoid
using any artificial &quot;weighting&quot; of models (with highly data dependent
parameters) and, instead, directly minimize the final regression error, which
is the ultimate performance goal. The introduced methods are generic such that
they can readily incorporate different tree construction methods such as random
trees in their framework and can use different regressor or partitioning
functions as demonstrated in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6396</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6396</id><created>2013-11-25</created><updated>2014-01-22</updated><authors><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Unified Approach to Universal Prediction: Generalized Upper and Lower
  Bounds</title><categories>cs.LG</categories><comments>Submitted to IEEE Transactions on Neural Networks and Learning
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sequential prediction of real-valued, arbitrary and unknown
sequences under the squared error loss as well as the best parametric predictor
out of a large, continuous class of predictors. Inspired by recent results from
computational learning theory, we refrain from any statistical assumptions and
define the performance with respect to the class of general parametric
predictors. In particular, we present generic lower and upper bounds on this
relative performance by transforming the prediction task into a parameter
learning problem. We first introduce the lower bounds on this relative
performance in the mixture of experts framework, where we show that for any
sequential algorithm, there always exists a sequence for which the performance
of the sequential algorithm is lower bounded by zero. We then introduce a
sequential learning algorithm to predict such arbitrary and unknown sequences,
and calculate upper bounds on its total squared prediction error for every
bounded sequence. We further show that in some scenarios we achieve matching
lower and upper bounds demonstrating that our algorithms are optimal in a
strong minimax sense such that their performances cannot be improved further.
As an interesting result we also prove that for the worst case scenario, the
performance of randomized algorithms can be achieved by sequential algorithms
so that randomized algorithms does not improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6401</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6401</id><created>2013-11-25</created><authors><author><keyname>Samalam</keyname><forenames>Vijay K</forenames></author></authors><title>A model for generating tunable clustering coefficients independent of
  the number of nodes in scale free and random networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic networks display a wide range of high average clustering
coefficients independent of the number of nodes in the network. In particular,
the local clustering coefficient decreases with the degree of the subtending
node in a complicated manner not explained by any current models. While a
number of hypotheses have been proposed to explain some of these observed
properties, there are no solvable models that explain them all. We propose a
novel growth model for both random and scale free networks that is capable of
predicting both tunable clustering coefficients independent of the network
size, and the inverse relationship between the local clustering coefficient and
node degree observed in most networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6402</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6402</id><created>2013-11-25</created><updated>2014-04-24</updated><authors><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Robust Least Squares Methods Under Bounded Data Uncertainties</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1203.4160</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating an unknown deterministic signal that is
observed through an unknown deterministic data matrix under additive noise. In
particular, we present a minimax optimization framework to the least squares
problems, where the estimator has imperfect data matrix and output vector
information. We define the performance of an estimator relative to the
performance of the optimal least squares (LS) estimator tuned to the underlying
unknown data matrix and output vector, which is defined as the regret of the
estimator. We then introduce an efficient robust LS estimation approach that
minimizes this regret for the worst possible data matrix and output vector,
where we refrain from any structural assumptions on the data. We demonstrate
that minimizing this worst-case regret can be cast as a semi-definite
programming (SDP) problem. We then consider the regularized and structured LS
problems and present novel robust estimation methods by demonstrating that
these problems can also be cast as SDP problems. We illustrate the merits of
the proposed algorithms with respect to the well-known alternatives in the
literature through our simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6421</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6421</id><created>2013-11-25</created><authors><author><keyname>Crescenzi</keyname><forenames>Pierluigi</forenames></author><author><keyname>Gildea</keyname><forenames>Daniel</forenames></author><author><keyname>Marino</keyname><forenames>Andrea</forenames></author><author><keyname>Rossi</keyname><forenames>Gianluca</forenames></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames></author></authors><title>Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies</title><categories>cs.FL cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed
translation schemata, are unlike context-free grammars in that they do not have
a binary normal form. In general, parsing with SCFGs takes space and time
polynomial in the length of the input strings, but with the degree of the
polynomial depending on the permutations of the SCFG rules. We consider linear
parsing strategies, which add one nonterminal at a time. We show that for a
given input permutation, the problems of finding the linear parsing strategy
with the minimum space and time complexity are both NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6425</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6425</id><created>2013-11-25</created><authors><author><keyname>Fiori</keyname><forenames>Marcelo</forenames></author><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Mus&#xe9;</keyname><forenames>Pablo</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</title><categories>math.OC cs.LG stat.ML</categories><comments>NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph matching is a challenging problem with very important applications in a
wide range of fields, from image and video analysis to biological and
biomedical problems. We propose a robust graph matching algorithm inspired in
sparsity-related techniques. We cast the problem, resembling group or
collaborative sparsity formulations, as a non-smooth convex optimization
problem that can be efficiently solved using augmented Lagrangian techniques.
The method can deal with weighted or unweighted graphs, as well as multimodal
data, where different graphs represent different types of data. The proposed
approach is also naturally integrated with collaborative graph inference
techniques, solving general network inference problems where the observed
variables, possibly coming from different modalities, are not in
correspondence. The algorithm is tested and compared with state-of-the-art
graph matching techniques in both synthetic and real graphs. We also present
results on multimodal graphs and applications to collaborative inference of
brain connectivity from alignment-free functional magnetic resonance imaging
(fMRI) data. The code is publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6441</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6441</id><created>2013-11-25</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Choi</keyname><forenames>Lark Kwon</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Bovik</keyname><forenames>Alan C.</forenames></author></authors><title>Modeling the Time-varying Subjective Quality of HTTP Video Streams with
  Rate Adaptations</title><categories>cs.MM</categories><doi>10.1109/TIP.2014.2312613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Newly developed HTTP-based video streaming technologies enable flexible
rate-adaptation under varying channel conditions. Accurately predicting the
users' Quality of Experience (QoE) for rate-adaptive HTTP video streams is thus
critical to achieve efficiency. An important aspect of understanding and
modeling QoE is predicting the up-to-the-moment subjective quality of a video
as it is played, which is difficult due to hysteresis effects and
nonlinearities in human behavioral responses. This paper presents a
Hammerstein-Wiener model for predicting the time-varying subjective quality
(TVSQ) of rate-adaptive videos. To collect data for model parameterization and
validation, a database of longer-duration videos with time-varying distortions
was built and the TVSQs of the videos were measured in a large-scale subjective
study. The proposed method is able to reliably predict the TVSQ of rate
adaptive videos. Since the Hammerstein-Wiener model has a very simple
structure, the proposed method is suitable for on-line TVSQ prediction in HTTP
based streaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6453</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6453</id><created>2013-11-25</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoqing</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author><author><keyname>Bovik</keyname><forenames>Alan C.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Rate Adaptation and Admission Control for Video Transmission with
  Subjective Quality Constraints</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adapting video data rate during streaming can effectively reduce the risk of
playback interruptions caused by channel throughput fluctuations. The
variations in rate, however, also introduce video quality fluctuations and thus
potentially affects viewers' Quality of Experience (QoE). We show how the QoE
of video users can be improved by rate adaptation and admission control. We
conducted a subjective study wherein we found that viewers' QoE was strongly
correlated with the empirical cumulative distribution function (eCDF) of the
predicted video quality. Based on this observation, we propose a
rate-adaptation algorithm that can incorporate QoE constraints on the empirical
cumulative quality distribution per user. We then propose a threshold-based
admission control policy to block users whose empirical cumulative quality
distribution is not likely to satisfy their QoE constraint. We further devise
an online adaptation algorithm to automatically optimize the threshold.
Extensive simulation results show that the proposed scheme can reduce network
resource consumption by $40\%$ over conventional average-quality maximized
rate-adaptation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6460</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6460</id><created>2013-11-25</created><authors><author><keyname>Barmase</keyname><forenames>Swapnil</forenames></author><author><keyname>Das</keyname><forenames>Saurav</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author></authors><title>Wavelet Transform-Based Analysis of QRS complex in ECG Signals</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we have reported a wavelet based time-frequency
multiresolution analysis of an ECG signal. The ECG (electrocardiogram), which
records hearts electrical activity, is able to provide with useful information
about the type of Cardiac disorders suffered by the patient depending upon the
deviations from normal ECG signal pattern. We have plotted the coefficients of
continuous wavelet transform using Morlet wavelet. We used different ECG signal
available at MIT-BIH database and performed a comparative study. We
demonstrated that the coefficient at a particular scale represents the presence
of QRS signal very efficiently irrespective of the type or intensity of noise,
presence of unusually high amplitude of peaks other than QRS peaks and Base
line drift errors. We believe that the current studies can enlighten the path
towards development of very lucid and time efficient algorithms for identifying
and representing the QRS complexes that can be done with normal computers and
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6474</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6474</id><created>2013-11-25</created><authors><author><keyname>Schwarz</keyname><forenames>Martin</forenames></author><author><keyname>Cubitt</keyname><forenames>Toby S.</forenames></author><author><keyname>Verstraete</keyname><forenames>Frank</forenames></author></authors><title>An Information-Theoretic Proof of the Constructive Commutative Quantum
  Lov\'asz Local Lemma</title><categories>quant-ph cs.CC</categories><comments>7 pages, 9 pages appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quantum Lov\'asz Local Lemma (QLLL) [AKS12] establishes
non-constructively that any quantum system constrained by a local Hamiltonian
has a zero-energy ground state, if the local Hamiltonian terms overlap only in
a certain restricted way. In this paper, we present an efficient quantum
algorithm to prepare this ground state for the special case of commuting
projector terms. The related classical problem has been open for more than 34
years. Our algorithm follows the breakthrough ideas of Moser's [Moser09]
classical algorithm and lifts his information theoretic argument to the quantum
setting. A similar result has been independently published by Arad and Sattath
[AS13] recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6492</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6492</id><created>2013-11-25</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Performance Evaluation of Multiterminal Backhaul Compression for Cloud
  Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>A shorter version of the paper has been submitted to CISS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud radio access networks (C-RANs), the baseband processing of the
available macro- or pico/femto-base stations (BSs) is migrated to control
units, each of which manages a subset of BS antennas. The centralized
information processing at the control units enables effective interference
management. The main roadblock to the implementation of C-RANs hinges on the
effective integration of the radio units, i.e., the BSs, with the backhaul
network. This work first reviews in a unified way recent results on the
application of advanced multiterminal, as opposed to standard point-to-point,
backhaul compression techniques. The gains provided by multiterminal backhaul
compression are then confirmed via extensive simulations based on standard
cellular models. As an example, it is observed that multiterminal compression
strategies provide performance gains of more than 60% for both the uplink and
the downlink in terms of the cell-edge throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6500</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6500</id><created>2013-11-11</created><authors><author><keyname>Goudeseune</keyname><forenames>Camille</forenames></author></authors><title>Stitched Panoramas from Toy Airborne Video Cameras</title><categories>cs.CV</categories><comments>7 pages, 9 figures</comments><acm-class>I.3.3; I.4; J.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Effective panoramic photographs are taken from vantage points that are high.
High vantage points have recently become easier to reach as the cost of
quadrotor helicopters has dropped to nearly disposable levels. Although cameras
carried by such aircraft weigh only a few grams, their low-quality video can be
converted into panoramas of high quality and high resolution. Also, the small
size of these aircraft vastly reduces the risks inherent to flight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6505</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6505</id><created>2013-11-25</created><authors><author><keyname>Elliott</keyname><forenames>James</forenames></author><author><keyname>Hoemmen</keyname><forenames>Mark</forenames></author><author><keyname>Mueller</keyname><forenames>Frank</forenames></author></authors><title>Evaluating the Impact of SDC on the GMRES Iterative Solver</title><categories>cs.DC</categories><doi>10.1109/IPDPS.2014.123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing parallelism and transistor density, along with increasingly
tighter energy and peak power constraints, may force exposure of occasionally
incorrect computation or storage to application codes. Silent data corruption
(SDC) will likely be infrequent, yet one SDC suffices to make numerical
algorithms like iterative linear solvers cease progress towards the correct
answer. Thus, we focus on resilience of the iterative linear solver GMRES to a
single transient SDC. We derive inexpensive checks to detect the effects of an
SDC in GMRES that work for a more general SDC model than presuming a bit flip.
Our experiments show that when GMRES is used as the inner solver of an
inner-outer iteration, it can &quot;run through&quot; SDC of almost any magnitude in the
computationally intensive orthogonalization phase. That is, it gets the right
answer using faulty data without any required roll back. Those SDCs which it
cannot run through, get caught by our detection scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6510</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6510</id><created>2013-11-25</created><authors><author><keyname>Lapedriza</keyname><forenames>Agata</forenames></author><author><keyname>Pirsiavash</keyname><forenames>Hamed</forenames></author><author><keyname>Bylinskii</keyname><forenames>Zoya</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Are all training examples equally valuable?</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When learning a new concept, not all training examples may prove equally
useful for training: some may have higher or lower training value than others.
The goal of this paper is to bring to the attention of the vision community the
following considerations: (1) some examples are better than others for training
detectors or classifiers, and (2) in the presence of better examples, some
examples may negatively impact performance and removing them may be beneficial.
In this paper, we propose an approach for measuring the training value of an
example, and use it for ranking and greedily sorting examples. We test our
methods on different vision tasks, models, datasets and classifiers. Our
experiments show that the performance of current state-of-the-art detectors and
classifiers can be improved when training on a subset, rather than the whole
training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6526</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6526</id><created>2013-11-25</created><authors><author><keyname>Borghol</keyname><forenames>Youmna</forenames></author><author><keyname>Ardon</keyname><forenames>Sebastien</forenames></author><author><keyname>Carlsson</keyname><forenames>Niklas</forenames></author><author><keyname>Eager</keyname><forenames>Derek</forenames></author><author><keyname>Mahanti</keyname><forenames>Anirban</forenames></author></authors><title>The Untold Story of the Clones: Content-agnostic Factors that Impact
  YouTube Video Popularity</title><categories>cs.SI cs.CY</categories><comments>Dataset available at: http://www.ida.liu.se/~nikca/papers/kdd12.html</comments><acm-class>E.0; C.4; H.3</acm-class><journal-ref>Proceedings of the 18th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining (KDD 2012)</journal-ref><doi>10.1145/2339530.2339717</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video dissemination through sites such as YouTube can have widespread impacts
on opinions, thoughts, and cultures. Not all videos will reach the same
popularity and have the same impact. Popularity differences arise not only
because of differences in video content, but also because of other
&quot;content-agnostic&quot; factors. The latter factors are of considerable interest but
it has been difficult to accurately study them. For example, videos uploaded by
users with large social networks may tend to be more popular because they tend
to have more interesting content, not because social network size has a
substantial direct impact on popularity. In this paper, we develop and apply a
methodology that is able to accurately assess, both qualitatively and
quantitatively, the impacts of various content-agnostic factors on video
popularity. When controlling for video content, we observe a strong linear
&quot;rich-get-richer&quot; behavior, with the total number of previous views as the most
important factor except for very young videos. The second most important factor
is found to be video age. We analyze a number of phenomena that may contribute
to rich-get-richer, including the first-mover advantage, and search bias
towards popular videos. For young videos we find that factors other than the
total number of previous views, such as uploader characteristics and number of
keywords, become relatively more important. Our findings also confirm that
inaccurate conclusions can be reached when not controlling for content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6531</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6531</id><created>2013-11-25</created><updated>2016-03-07</updated><authors><author><keyname>Chv&#xe1;tal</keyname><forenames>Va&#x161;ek</forenames></author><author><keyname>Goldsmith</keyname><forenames>Mark</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author></authors><title>Brains and pseudorandom generators</title><categories>math.DS cs.CR cs.NE math.NA</categories><comments>This paper misinterprets the notion of a pseudorandom generator. For
  this reason, it has been withdrawn by the authors. Its main result,
  interpreted in terms of pseudorandom functions, reappears in arXiv:1603.01573
  [math.DS]</comments><msc-class>92B20, 65C10, 37B15, 62P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model
of the central nervous system; motivated by EEG recordings of normal brain
activity, Chv\' atal and Goldsmith asked whether or not this model can be
engineered to provide pseudorandom number generators. We supply evidence
suggesting that the answer is negative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6536</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6536</id><created>2013-11-25</created><authors><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author><author><keyname>de Rooij</keyname><forenames>Steven</forenames></author></authors><title>Universal Codes from Switching Strategies</title><categories>cs.IT cs.LG math.IT</categories><journal-ref>IEEE Transactions on Information Theory, 59(11):7168-7185,
  November 2013</journal-ref><doi>10.1109/TIT.2013.2273353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss algorithms for combining sequential prediction strategies, a task
which can be viewed as a natural generalisation of the concept of universal
coding. We describe a graphical language based on Hidden Markov Models for
defining prediction strategies, and we provide both existing and new models as
examples. The models include efficient, parameterless models for switching
between the input strategies over time, including a model for the case where
switches tend to occur in clusters, and finally a new model for the scenario
where the prediction strategies have a known relationship, and where jumps are
typically between strongly related ones. This last model is relevant for coding
time series data where parameter drift is expected. As theoretical ontributions
we introduce an interpolation construction that is useful in the development
and analysis of new algorithms, and we establish a new sophisticated lemma for
analysing the individual sequence regret of parameterised models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6542</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6542</id><created>2013-11-25</created><authors><author><keyname>Qu</keyname><forenames>Meixia</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author><author><keyname>Zhu</keyname><forenames>Daming</forenames></author><author><keyname>Luan</keyname><forenames>Junfeng</forenames></author></authors><title>Implementing program extraction from CL1-proofs</title><categories>cs.LO</categories><comments>1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computability logic (CoL) is a formal theory of interactive computation. It
understands computational problems as games played by two players: a machine
and its environment, uses logical formalism to describe valid principles of
computability and formulas to represent computational problems. Logic CL1 is a
deductive system for a fragment of CoL. The logical vocabulary contains all of
the operators of classical logic and choice operators, the atoms represent
elementary games i.e. predicates of classical logic. In this paper, we present
a program that takes a CL1-proof of an arbitrary formula $F$, and extract a
winning strategy for $F$ from that proof then play $F$ using that strategy. We
hope this paper would provide a starting point for further work in program
extraction of the CoL-based arithmetic and other CoL-based applied systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6543</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6543</id><created>2013-11-25</created><updated>2014-05-05</updated><authors><author><keyname>Zhang</keyname><forenames>Haibin</forenames><affiliation>Macquarie University, Sydney, Australia</affiliation></author><author><keyname>Wang</keyname><forenames>Yan</forenames><affiliation>Macquarie University, Sydney, Australia</affiliation></author><author><keyname>Zhang</keyname><forenames>Xiuzhen</forenames><affiliation>RMIT University, Melbourne, Australia</affiliation></author><author><keyname>Lim</keyname><forenames>Ee-Peng</forenames><affiliation>Singapore Management University, Singapore</affiliation></author></authors><title>ReputationPro: The Efficient Approaches to Contextual Transaction Trust
  Computation in E-Commerce Environments</title><categories>cs.DS cs.DB</categories><comments>This paper has been withdrawn by the author due to the loss of some
  figures</comments><journal-ref>ACM Trans. Web 9, 1, Article 2 (January 2015)</journal-ref><doi>10.1145/2697390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In e-commerce environments, the trustworthiness of a seller is utterly
important to potential buyers, especially when the seller is unknown to them.
Most existing trust evaluation models compute a single value to reflect the
general trust level of a seller without taking any transaction context
information into account. In this paper, we first present a trust vector
consisting of three values for Contextual Transaction Trust (CTT). In the
computation of three CTT values, the identified three important context
dimensions, including product category, transaction amount and transaction
time, are taken into account. In particular, with different parameters
regarding context dimensions that are specified by a buyer, different sets of
CTT values can be calculated. As a result, all these values can outline the
reputation profile of a seller that indicates the dynamic trust levels of a
seller in different product categories, price ranges, time periods, and any
necessary combination of them. We term this new model as ReputationPro.
However, in ReputationPro, the computation of reputation profile requires novel
algorithms for the precomputation of aggregates over large-scale ratings and
transaction data of three context dimensions as well as new data structures for
appropriately indexing aggregation results to promptly answer buyers' CTT
requests. To solve these challenging problems, we then propose a new index
scheme CMK-tree. After that, we further extend CMK-tree and propose a
CMK-treeRS approach to reducing the storage space allocated to each seller.
Finally, the experimental results illustrate that the CMK-tree is superior in
efficiency for computing CTT values to all three existing approaches in the
literature. In addition, though with reduced storage space, the CMK-treeRS
approach can further improve the performance in answering buyers' CTT queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6547</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6547</id><created>2013-11-25</created><updated>2015-07-14</updated><authors><author><keyname>Scheinberg</keyname><forenames>Katya</forenames></author><author><keyname>Tang</keyname><forenames>Xiaocheng</forenames></author></authors><title>Practical Inexact Proximal Quasi-Newton Method with Global Complexity
  Analysis</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently several methods were proposed for sparse optimization which make
careful use of second-order information [10, 28, 16, 3] to improve local
convergence rates. These methods construct a composite quadratic approximation
using Hessian information, optimize this approximation using a first-order
method, such as coordinate descent and employ a line search to ensure
sufficient descent. Here we propose a general framework, which includes
slightly modified versions of existing algorithms and also a new algorithm,
which uses limited memory BFGS Hessian approximations, and provide a novel
global convergence rate analysis, which covers methods that solve subproblems
via coordinate descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6550</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6550</id><created>2013-11-25</created><authors><author><keyname>Monakhov</keyname><forenames>Yuri</forenames></author><author><keyname>Fayman</keyname><forenames>Olga</forenames></author></authors><title>Simulation Model Of Functional Stability Of Business Processes</title><categories>cs.OH</categories><comments>10 pages, 12 figures</comments><journal-ref>Int. Journal of Engineering Research and Application ISSN :
  2248-9622, Vol. 3, Issue 6, Nov-Dec 2013, pp. 819-828</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functioning of business processes of high-tech enterprise is in a constant
interaction with the environment. Herewith a wide range of such interaction
represents a variety of conflicts affecting the achievement of the goals of
business processes. All these things lead to the disruption of functioning of
business processes. That's why modern enterprises should have mechanisms to
provide a new property of business processes - ability to maintain and/or
restore functions in various adverse effects. This property is called
functional stability of business processes (FSBP). In this article we offer,
showcase and test the new approach to assessing the results of business process
re-engineering by simulating their functional stability before and after
re-engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6556</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6556</id><created>2013-11-26</created><updated>2014-12-08</updated><authors><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author><author><keyname>Desai</keyname><forenames>Kalpit</forenames></author><author><keyname>Sasidharan</keyname><forenames>Sanand</forenames></author><author><keyname>Sundararajan</keyname><forenames>Ramasubramanian</forenames></author></authors><title>Double Ramp Loss Based Reject Option Classifier</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider the problem of learning reject option classifiers. The goodness
of a reject option classifier is quantified using $0-d-1$ loss function wherein
a loss $d \in (0,.5)$ is assigned for rejection. In this paper, we propose {\em
double ramp loss} function which gives a continuous upper bound for $(0-d-1)$
loss. Our approach is based on minimizing regularized risk under the double
ramp loss using {\em difference of convex (DC) programming}. We show the
effectiveness of our approach through experiments on synthetic and benchmark
datasets. Our approach performs better than the state of the art reject option
classification approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6563</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6563</id><created>2013-11-26</created><authors><author><keyname>Atzemoglou</keyname><forenames>Philip</forenames></author></authors><title>Higher-order semantics for quantum programming languages with classical
  control</title><categories>cs.LO cs.PL math.CT quant-ph</categories><comments>DPhil Thesis. See full text for unabridged abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis studies the categorical formalisation of quantum computing,
through the prism of type theory, in a three-tier process. The first stage of
our investigation involves the creation of the dagger lambda calculus, a lambda
calculus for dagger compact categories. Our second contribution lifts the
expressive power of the dagger lambda calculus, to that of a quantum
programming language, by adding classical control in the form of complementary
classical structures and dualisers. Finally, our third contribution
demonstrates how our lambda calculus can be applied to various well known
problems in quantum computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6566</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6566</id><created>2013-11-26</created><authors><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author><author><keyname>Goswami</keyname><forenames>Bidisha</forenames></author><author><keyname>Ngenzi</keyname><forenames>Alexander</forenames></author><author><keyname>Khanam</keyname><forenames>Aquila</forenames></author></authors><title>A Randomized Generic Lucas Seed Algorithm (RGLSA) with Tail Boosting for
  Threat Modeling in Virtual Machines</title><categories>cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is about a self-propagating and self-replicating model of malicious
seeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6570</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6570</id><created>2013-11-26</created><updated>2013-12-04</updated><authors><author><keyname>Hakuta</keyname><forenames>Shizuya</forenames></author><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Nakano</keyname><forenames>Keisuke</forenames></author><author><keyname>Iwasaki</keyname><forenames>Hideya</forenames></author></authors><title>XQuery Streaming by Forest Transducers</title><categories>cs.DB</categories><comments>Full version of the paper in the Proceedings of the 30th IEEE
  International Conference on Data Engineering (ICDE 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming of XML transformations is a challenging task and only very few
systems support streaming. Research approaches generally define custom
fragments of XQuery and XPath that are amenable to streaming, and then design
custom algorithms for each fragment. These languages have several shortcomings.
Here we take a more principles approach to the problem of streaming
XQuery-based transformations. We start with an elegant transducer model for
which many static analysis problems are well-understood: the Macro Forest
Transducer (MFT). We show that a large fragment of XQuery can be translated
into MFTs --- indeed, a fragment of XQuery, that can express important features
that are missing from other XQuery stream engines, such as GCX: our fragment of
XQuery supports XPath predicates and let-statements. We then rely on a
streaming execution engine for MFTs, one which uses a well-founded set of
optimizations from functional programming, such as strictness analysis and
deforestation. Our prototype achieves time and memory efficiency comparable to
the fastest known engine for XQuery streaming, GCX. This is surprising because
our engine relies on the OCaml built in garbage collector and does not use any
specialized buffer management, while GCX's efficiency is due to clever and
explicit buffer management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6578</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6578</id><created>2013-11-26</created><authors><author><keyname>Randhe</keyname><forenames>Vrushali</forenames></author><author><keyname>Chougule</keyname><forenames>Archana</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Reverse Proxy Framework using Sanitization Technique for Intrusion
  Prevention in Database</title><categories>cs.DB cs.CR</categories><comments>9 pages, 6 figures, 3 tables; CIIT 2013 International Conference,
  Mumbai</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing importance of the internet in our day to day life, data
security in web application has become very crucial. Ever increasing on line
and real time transaction services have led to manifold rise in the problems
associated with the database security. Attacker uses illegal and unauthorized
approaches to hijack the confidential information like username, password and
other vital details. Hence the real time transaction requires security against
web based attacks. SQL injection and cross site scripting attack are the most
common application layer attack. The SQL injection attacker pass SQL statement
through a web applications input fields, URL or hidden parameters and get
access to the database or update it. The attacker take a benefit from user
provided data in such a way that the users input is handled as a SQL code.
Using this vulnerability an attacker can execute SQL commands directly on the
database. SQL injection attacks are most serious threats which take users input
and integrate it into SQL query. Reverse Proxy is a technique which is used to
sanitize the users inputs that may transform into a database attack. In this
technique a data redirector program redirects the users input to the proxy
server before it is sent to the application server. At the proxy server, data
cleaning algorithm is triggered using a sanitizing application. In this
framework we include detection and sanitization of the tainted information
being sent to the database and innovate a new prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6591</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6591</id><created>2013-11-26</created><authors><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>On the Complexity and Approximation of Binary Evidence in Lifted
  Inference</title><categories>cs.AI</categories><comments>To appear in Advances in Neural Information Processing Systems 26
  (NIPS), Lake Tahoe, USA, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lifted inference algorithms exploit symmetries in probabilistic models to
speed up inference. They show impressive performance when calculating
unconditional probabilities in relational models, but often resort to
non-lifted inference when computing conditional probabilities. The reason is
that conditioning on evidence breaks many of the model's symmetries, which can
preempt standard lifting techniques. Recent theoretical results show, for
example, that conditioning on evidence which corresponds to binary relations is
#P-hard, suggesting that no lifting is to be expected in the worst case. In
this paper, we balance this negative result by identifying the Boolean rank of
the evidence as a key parameter for characterizing the complexity of
conditioning in lifted inference. In particular, we show that conditioning on
binary evidence with bounded Boolean rank is efficient. This opens up the
possibility of approximating evidence by a low-rank Boolean matrix
factorization, which we investigate both theoretically and empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6594</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6594</id><created>2013-11-26</created><updated>2014-05-20</updated><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>&#xc1;ngela</forenames></author><author><keyname>Rabin</keyname><forenames>Neta</forenames></author><author><keyname>Fishelov</keyname><forenames>Dalia</forenames></author><author><keyname>Dorronsoro</keyname><forenames>Jos&#xe9; R.</forenames></author></authors><title>Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis</title><categories>cs.AI cs.LG stat.ML</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-linear dimensionality reduction techniques such as manifold learning
algorithms have become a common way for processing and analyzing
high-dimensional patterns that often have attached a target that corresponds to
the value of an unknown function. Their application to new points consists in
two steps: first, embedding the new data point into the low dimensional space
and then, estimating the function value on the test point from its neighbors in
the embedded space.
  However, finding the low dimension representation of a test point, while easy
for simple but often not powerful enough procedures such as PCA, can be much
more complicated for methods that rely on some kind of eigenanalysis, such as
Spectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target
function is to be evaluated, averaging methods like nearest neighbors may give
unstable results if the function is noisy. Thus, the smoothing of the target
function with respect to the intrinsic, low-dimensional representation that
describes the geometric structure of the examined data is a challenging task.
  In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension
of the standard Laplacian Pyramids model that incorporates a modified LOOCV
procedure that avoids the large cost of the standard one and offers the
following advantages: (i) it selects automatically the optimal function
resolution (stopping time) adapted to the data and its noise, (ii) it is easy
to apply as it does not require parameterization, (iii) it does not overfit the
training set and (iv) it adds no extra cost compared to other classical
interpolation methods. We illustrate numerically ALP's behavior on a synthetic
problem and apply it to the computation of the DM projection of new patterns
and to the extension to them of target function values on a radiation
forecasting problem over very high dimensional patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6605</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6605</id><created>2013-11-26</created><authors><author><keyname>Dreyfus</keyname><forenames>Alois</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC, UMR 6174</affiliation></author><author><keyname>Heam</keyname><forenames>Pierre-Cyrille</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC, FEMTO-ST</affiliation></author><author><keyname>Kouchnarenko</keyname><forenames>Olga</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC, FEMTO-ST</affiliation></author></authors><title>Enhancing Approximations for Regular Reachability Analysis</title><categories>cs.FL cs.LO</categories><proxy>ccsd</proxy><journal-ref>Implementation and Application of Automata 7982 (2013) 331-339</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces two mechanisms for computing over-approximations of
sets of reachable states, with the aim of ensuring termination of state-space
exploration. The first mechanism consists in over-approximating the automata
representing reachable sets by merging some of their states with respect to
simple syntactic criteria, or a combination of such criteria. The second
approximation mechanism consists in manipulating an auxiliary automaton when
applying a transducer representing the transition relation to an automaton
encoding the initial states. In addition, for the second mechanism we propose a
new approach to refine the approximations depending on a property of interest.
The proposals are evaluated on examples of mutual exclusion protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6606</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6606</id><created>2013-11-26</created><authors><author><keyname>Dreyfus</keyname><forenames>Alois</forenames><affiliation>UMR 6174, INRIA Nancy - Grand Est / LORIA / LIFC</affiliation></author><author><keyname>Heam</keyname><forenames>Pierre-Cyrille</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC, FEMTO-ST</affiliation></author><author><keyname>Kouchnarenko</keyname><forenames>Olga</forenames><affiliation>UMR 6174, INRIA Nancy - Grand Est / LORIA / LIFC</affiliation></author></authors><title>Random Grammar-based Testing for Covering All Non-Terminals</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>2013 IEEE Sixth International Conference on Software Testing,
  Verification and Validation - CSTVA Workshop (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of software testing, generating complex data inputs is
frequently performed using a grammar-based specification. For combinatorial
reasons, an exhaustive generation of the data -- of a given size -- is
practically impossible, and most approaches are either based on random
techniques or on coverage criteria. In this paper, we show how to combine these
two techniques by biasing the random generation in order to optimise the
probability of satisfying a coverage criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6609</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6609</id><created>2013-11-26</created><updated>2014-06-20</updated><authors><author><keyname>Ferraro</keyname><forenames>Giovanna</forenames></author><author><keyname>Iovanella</keyname><forenames>Antonio</forenames></author></authors><title>Choreography In Inter-Organizational Innovation Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the concept of choreography with respect to
inter-organizational innovation networks, as they constitute an attractive
environment to create innovation in different sectors. We argue that
choreography governs behaviours by shaping the level of connectivity and
cohesion among network members. It represents a valid organizational system
able to sustain some activities and to reach effects generating innovation
outcomes. This issue is tackled introducing a new framework in which we propose
a network model as prerequisite for our hypothesis. The analysis is focused on
inter-organizational innovation networks characterized by the presence of hubs,
semi-peripheral and peripheral members lacking hierarchical authority. We
sustain that the features of a network, bringing to synchronization phenomena,
are extremely similar to those existing in innovation network characterized by
the emergence of choreography. The effectiveness of our model is verified by
providing a real case study that gives preliminary empirical hints on the
network aptitude to perform choreography. Indeed, the innovation network
analysed in the case study reveals characteristics causing synchronization and
consequently the establishment of choreography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6615</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6615</id><created>2013-11-26</created><updated>2014-02-11</updated><authors><author><keyname>Rybicki</keyname><forenames>Bartosz</forenames></author><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames></author></authors><title>Improved approximation algorithm for Fault-Tolerant Facility Placement</title><categories>cs.DS</categories><comments>We modify one figure; fix a small problem with Lemma 5(iv); add one
  refference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Fault-Tolerant Facility Placement problem ($FTFP$), which is
a generalization of the classical Uncapacitated Facility Location problem
($UFL$). In the $FTFP$ problem we have a set of clients $C$ and a set of
facilities $F$. Each facility $i \in F$ can be opened many times. For each
opening of facility $i$ we pay $f_i \geq 0$. Our goal is to connect each client
$j \in C$ with $r_j \geq 1$ open facilities in a way that minimizes the total
cost of open facilities and established connections.
  In a series of recent papers $FTFP$ was essentially reduced to $FTFL$ and
then to $UFL$ showing it could be approximated with ratio $1.575$. In this
paper we show that $FTFP$ can actually be approximated even better. We consider
approximation ratio as a function of $r = min_{j \in C} r_j$ (minimum
requirement of a client). With increasing $r$ the approximation ratio of our
algorithm $\lambda_r$ converges to one. Furthermore, for $r &gt; 1$ the value of
$\lambda_r$ is less than 1.463 (hardness of approximation of $UFL$). We also
show a lower bound of 1.278 for the approximability of the Fault-Tolerant
Facility Location problem ($FTFL$) for arbitrary $r$. Already for $r &gt; 3$ we
obtain that $FTFP$ can be approximated with ratio 1.275, showing that under
standard complexity theoretic assumptions $FTFP$ is strictly better
approximable than $FTFL$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6635</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6635</id><created>2013-11-26</created><updated>2016-01-14</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Multiuser Random Coding Techniques for Mismatched Decoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory (revised Oct.
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies multiuser random coding techniques for channel coding with
a given (possibly suboptimal) decoding rule. For the mismatched discrete
memoryless multiple-access channel, error exponents are obtained that are tight
with respect to the ensemble average, and positive within the interior of
Lapidoth's achievable rate region. In the case of maximum-likelihood decoding,
the ensemble tightness of the exponent of Liu and Hughes is proved. Equivalent
primal and dual forms of the rate regions are given, and the latter are shown
to extend immediately to channels with infinite and continuous alphabets. In
the setting of single-user mismatched decoding, similar analysis techniques are
applied to a refined version of superposition coding, which is shown to achieve
rates at least as high as standard superposition coding for any set of
random-coding parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6638</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6638</id><created>2013-11-26</created><updated>2013-11-27</updated><authors><author><keyname>Chen</keyname><forenames>Binyi</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>$K$-anonymous Signaling Scheme</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We incorporate signaling scheme into Ad Auction setting, to achieve better
welfare and revenue while protect users' privacy. We propose a new
\emph{$K$-anonymous signaling scheme setting}, prove the hardness of the
corresponding welfare/revenue maximization problem, and finally propose the
algorithms to approximate the optimal revenue or welfare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6647</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6647</id><created>2013-11-26</created><authors><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>DoF Analysis of the K-user MISO Broadcast Channel with Alternating CSIT</title><categories>cs.IT math.IT</categories><comments>24 pages, submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a $K$-user multiple-input single-output (MISO) broadcast channel
(BC) where the channel state information (CSI) of user $i(i=1,2,\ldots,K)$ may
be either perfect (P), delayed (D) or not known (N) at the transmitter with
probabilities $\lambda_P^i$, $\lambda_D^i$ and $\lambda_N^i$, respectively. In
this channel, according to the three possible CSIT for each user, joint CSIT of
the $K$ users could have at most $3^K$ realizations. Although the results by
Tandon et al. show that the Degrees of Freedom (DoF) region for the two user
MISO BC with symmetric marginal probabilities (i.e., $\lambda_Q^i=\lambda_Q
\forall i\in \{1,2,\ldots,K\}, Q\in \{P,D,N\}$) depends only on the marginal
probabilities, we show that this interesting result does not hold in general
when the number of users is more than two. In other words, the DoF region is a
function of the \textit{CSIT pattern}, or equivalently, all the joint
probabilities. In this paper, given the marginal probabilities of CSIT, we
derive an outer bound for the DoF region of the $K$-user MISO BC. Subsequently,
the achievability of these outer bounds are considered in certain scenarios.
Finally, we show the dependence of the DoF region on the joint probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6658</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6658</id><created>2013-11-26</created><authors><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Efficiency Improvement of Measurement Pose Selection Techniques in Robot
  Calibration</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>The IFAC Conference on Manufacturing Modeling, Management and
  Control (MIM 2013), Saint Petersburg : Russie, F\'ed\'eration De (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the design of experiments for manipulator geometric and
elastostatic calibration based on the test-pose approach. The main attention is
paid to the efficiency improvement of numerical techniques employed in the
selection of optimal measurement poses for calibration experiments. The
advantages of the developed technique are illustrated by simulation examples
that deal with the geometric calibration of the industrial robot of serial
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6659</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6659</id><created>2013-11-26</created><authors><author><keyname>Omrana</keyname><forenames>Hajar</forenames></author><author><keyname>Belouadha</keyname><forenames>Fatima-Zahra</forenames></author><author><keyname>Roudies</keyname><forenames>Ounsa</forenames></author></authors><title>MARTE Profile-based MDA approach for semantic NFP-aware Web services</title><categories>cs.SE</categories><comments>10 pages, 5 figures</comments><journal-ref>European Journal of Scientific Research,Volume 94, issue 4, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non Functional Properties (NFPs) such as security, quality of service and
business related properties enhance the service description and provide
necessary information about the fitness of its behaviour. These properties have
become crucial criteria for efficient selection and composition of Web
services. However, they belong to different domains, are complex, change
frequently and have to be semantically described. The W3C standard
WSPolicy,recommended to describe these properties doesn t define standardized
specifications that cover all NFPs domains. Moreover, it doesn t provide an
easy manner to express them independently of domains, and doesn t support their
semantic. This paper proposes a Model driven approach to describe and
automatically generate enriched Web services including semantic NFPs. It
explores both the use of the OMG Profile for Modelling and Analysis of
Real-Time Embedded Systems (MARTE) and the W3C standards. Mapping rules, from
NFPs profile to WS-Policy and SAWSDL files, transforms NFPs into policies
associated with WSDL elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6670</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6670</id><created>2013-11-26</created><authors><author><keyname>Ghouaiel</keyname><forenames>Nehla</forenames><affiliation>IRIT</affiliation></author><author><keyname>Cieutat</keyname><forenames>Jean-Marc</forenames><affiliation>IRIT</affiliation></author><author><keyname>Jessel</keyname><forenames>Jean-Pierre</forenames><affiliation>IRIT</affiliation></author></authors><title>Mobile Augmented Reality Applications to Discover New Environments</title><categories>cs.CY</categories><comments>Science and Information Conference 2013, France (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although man has become sedentary over time, his wish to travel the world
remains as strong as ever. The aim of this paper is to show how techniques
based on imagery and Augmented Reality (AR) can prove to be of great help when
discovering a new urban environment and observing the evolution of the natural
environment. The study's support is naturally the Smartphone which in just a
few years has become our most familiar device, which we take with us
practically everywhere we go in our daily lives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6671</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6671</id><created>2013-11-26</created><updated>2013-12-03</updated><authors><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author></authors><title>A Deterministic Polynomial Space Construction for eps-nets under any
  Norm</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic polynomial space construction for nearly optimal
eps-nets with respect to any input n-dimensional convex body K and norm |.|.
More precisely, our algorithm can build and iterate over an eps-net of K with
respect to |.| in time 2^O(n) x (size of the optimal net) using only
poly(n)-space. This improves on previous constructions of Alon et al [STOC
2013] which achieve either a 2^O(n) approximation or an n^O(n) approximation of
the optimal net size using 2^n space and poly(n)-space respectively. As in
their work, our algorithm relies on the mathematically classical approach of
building thin lattice coverings of space, which reduces the task of
constructing eps-nets to the problem of enumerating lattice points. Our main
technical contribution is a deterministic 2^O(n)-time and poly(n)-space
construction of thin lattice coverings of space with respect to any convex
body, where enumeration in these lattices can be efficiently performed using
poly(n)-space. This also yields the first existential construction of
poly(n)-space enumerable thin covering lattices for general convex bodies,
which we believe is of independent interest. Our construction combines the use
of the M-ellipsoid from convex geometry with lattice sparsification and
densification techniques.
  As an application, we give a 2^O(n)(1+1/eps)^n time and poly(n)-space
deterministic algorithm for computing a (1+eps)^n approximation to the volume
of a general convex body, which nearly matches the lower bounds for volume
estimation in the oracle model (the dependence on eps is larger by a factor 2
in the exponent). This improves on the previous results of Dadush and Vempala
[PNAS 2013], which gave the above result only for symmetric bodies and achieved
a dependence on eps of (1+log^{5/2}(1/eps)/eps^2)^n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6674</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6674</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Dumas</keyname><forenames>Claire</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Modelling of the gravity compensators in robotic manufacturing cells</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>The IFAC Conference on Manufacturing Modeling, Management and
  Control (MIM 2013), Saint Petersburg : Russian Federation (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the modeling and identification of the gravity
compensators used in heavy industrial robots. The main attention is paid to the
geometrical parameters identification and calibration accuracy. To reduce
impact of the measurement errors, the design of calibration experiments is
used. The advantages of the developed technique are illustrated by experimental
results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6676</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6676</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>ABBA</keyname><forenames>Gabriel</forenames><affiliation>LCFC</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Robust algorithm for calibration of robotic manipulator model</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>The IFAC Conference on Manufacturing Modeling, Management and
  Control (MIM 2013), Saint Petersburg : Russian Federation (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the robust identification of geometrical and
elastostatic parameters of robotic manipulator. The main attention is paid to
the efficiency improvement of the identification algorithm. To increase the
identification accuracy, it is proposed to apply the weighted least square
technique that employs a new algorithm for assigning of the weighting
coefficients. The latter allows taking into account variation of the
measurement system precision in different directions and throughout the robot
workspace. The advantages of the proposed approach are illustrated by an
application example that deals with the elasto-static calibration of industrial
robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6677</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6677</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Advanced robot calibration using partial pose measurements</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>18th International Conference on Methods and Models in Automation
  and Robotics (MMAR 2013), Mi{\ke}dzyzdroje : Poland (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the calibration of serial industrial robots using
partial pose measurements. In contrast to other works, the developed advanced
robot calibration technique is suitable for geometrical and elastostatic
calibration. The main attention is paid to the model parameters identification
accuracy. To reduce the impact of measurement errors, it is proposed to use
directly position measurements of several points instead of computing
orientation of the end-effector. The proposed approach allows us to avoid the
problem of non-homogeneity of the least-square objective, which arises in the
classical identification technique with the full-pose information. The
developed technique does not require any normalization and can be efficiently
applied both for geometric and elastostatic identification. The advantages of a
new approach are confirmed by comparison analysis that deals with the
efficiency evaluation of different identification strategies. The obtained
results have been successfully applied to the elastostatic parameters
identification of the industrial robot employed in a machining work-cell for
aerospace industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6685</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6685</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author></authors><title>CAD-based approach for identification of elasto-static parameters of
  robotic manipulators</title><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:0909.1460</comments><proxy>ccsd</proxy><journal-ref>Finite Elements in Analysis and Design 75 (2013) 19-30</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an approach for the identification of elasto-static
parameters of a robotic manipulator using the virtual experiments in a CAD
environment. It is based on the numerical processing of the data extracted from
the finite element analysis results, which are obtained for isolated
manipulator links. This approach allows to obtain the desired stiffness
matrices taking into account the complex shape of the links, couplings between
rotational/translational deflections and particularities of the joints
connecting adjacent links. These matrices are integral parts of the manipulator
lumped stiffness model that are widely used in robotics due to its high
computational efficiency. To improve the identification accuracy,
recommendations for optimal settings of the virtual experiments are given, as
well as relevant statistical processing techniques are proposed. Efficiency of
the developed approach is confirmed by a simulation study that shows that the
accuracy in evaluating the stiffness matrix elements is about 0.1%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6709</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6709</id><created>2013-11-26</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Chougule</keyname><forenames>Archana</forenames></author></authors><title>A Framework for Semi-automated Web Service Composition in Semantic Web</title><categories>cs.AI</categories><comments>6 pages, 9 figures; CUBE 2013 International Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Number of web services available on Internet and its usage are increasing
very fast. In many cases, one service is not enough to complete the business
requirement; composition of web services is carried out. Autonomous composition
of web services to achieve new functionality is generating considerable
attention in semantic web domain. Development time and effort for new
applications can be reduced with service composition. Various approaches to
carry out automated composition of web services are discussed in literature.
Web service composition using ontologies is one of the effective approaches. In
this paper we demonstrate how the ontology based composition can be made faster
for each customer. We propose a framework to provide precomposed web services
to fulfil user requirements. We detail how ontology merging can be used for
composition which expedites the whole process. We discuss how framework
provides customer specific ontology merging and repository. We also elaborate
on how merging of ontologies is carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6714</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6714</id><created>2013-11-26</created><authors><author><keyname>B&#xf6;ttcher</keyname><forenames>Stefan</forenames></author><author><keyname>Hartel</keyname><forenames>Rita</forenames></author><author><keyname>Rabe</keyname><forenames>Jonathan</forenames></author></authors><title>Efficient XML Keyword Search based on DAG-Compression</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to XML query languages as e.g. XPath which require knowledge on
the query language as well as on the document structure, keyword search is open
to anybody. As the size of XML sources grows rapidly, the need for efficient
search indices on XML data that support keyword search increases. In this
paper, we present an approach of XML keyword search which is based on the DAG
of the XML data, where repeated substructures are considered only once, and
therefore, have to be searched only once. As our performance evaluation shows,
this DAG-based extension of the set intersection search algorithm[1], [2], can
lead to search times that are on large documents more than twice as fast as the
search times of the XML-based approach. Additionally, we utilize a smaller
index, i.e., we consume less main memory to compute the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6716</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6716</id><created>2013-11-26</created><authors><author><keyname>Kumar</keyname><forenames>Mrinal</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>The Limits of Depth Reduction for Arithmetic Formulas: It's all about
  the top fan-in</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, a very exciting and promising method for proving lower
bounds for arithmetic circuits has been proposed. This method combines the
method of {\it depth reduction} developed in the works of Agrawal-Vinay [AV08],
Koiran [Koi12] and Tavenas [Tav13], and the use of the shifted partial
derivative complexity measure developed in the works of Kayal [Kay12] and Gupta
et al [GKKS13a]. These results inspired a flurry of other beautiful results and
strong lower bounds for various classes of arithmetic circuits, in particular a
recent work of Kayal et al [KSS13] showing superpolynomial lower bounds for
{\it regular} arithmetic formulas via an {\it improved depth reduction} for
these formulas. It was left as an intriguing question if these methods could
prove superpolynomial lower bounds for general (homogeneous) arithmetic
formulas, and if so this would indeed be a breakthrough in arithmetic circuit
complexity.
  In this paper we study the power and limitations of depth reduction and
shifted partial derivatives for arithmetic formulas. We do it via studying the
class of depth 4 homogeneous arithmetic circuits. We show: (1) the first {\it
superpolynomial lower bounds} for the class of homogeneous depth 4 circuits
with top fan-in $o(\log n)$. The core of our result is to show {\it improved
depth reduction} for these circuits. (2) We show that improved depth reduction
{\it is not possible} when the top fan-in is $\Omega(\log n)$. In particular
this shows that the depth reduction procedure of Koiran and Tavenas [Koi12,
Tav13] cannot be improved even for homogeneous formulas, thus strengthening the
results of Fournier et al [FLMS13] who showed that depth reduction is tight for
circuits, and answering some of the main open questions of [KSS13, FLMS13].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6718</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6718</id><created>2013-11-26</created><updated>2014-01-08</updated><authors><author><keyname>Perea-Vega</keyname><forenames>Diego</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Girard</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Efficient Heuristic for Resource Allocation in Zero-forcing OFDMA-SDMA
  Systems with Minimum Rate Constraints</title><categories>cs.NI</categories><comments>8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  4G wireless access systems require high spectral efficiency to support the
ever increasing number of users and data rates for real time applications.
Multi-antenna OFDM-SDMA systems can provide the required high spectral
efficiency and dynamic usage of the channel, but the resource allocation
process becomes extremely complex because of the augmented degrees of freedom.
In this paper, we propose two heuristics to solve the resource allocation
problem that have very low computational complexity and give performances not
far from the optimal. The proposed heuristics select a set of users for each
subchannel, but contrary to the reported methods that solve the throughput
maximization problem, our heuristics consider the set of real-time (RT) users
to ensure that their minimum rate requirements are met. We compare the
heuristics' performance against an upper bound and other methods proposed in
the literature and find that they give a somewhat lower performance, but
support a wider range of minimum rates while reducing the computational
complexity. The gap between the objective achieved by the heuristics and the
upper bound is not large. In our experiments this gap is 10.7% averaging over
all performed numerical evaluations for all system configurations. The increase
in the range of the supported minimum rates when compared with a method
reported in the literature is 14.6% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6728</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6728</id><created>2013-11-26</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author></authors><title>Numerical Investigations on Quasi Steady-State Model for Voltage
  Stability: Limitations and Nonlinear Analysis</title><categories>cs.SY</categories><comments>This paper has been accepted by International Transactions on
  Electrical Energy Systems. arXiv admin note: substantial text overlap with
  arXiv:1311.6163</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, several numerical examples to illustrate limitations of Quasi
Steady-State (QSS) model in long-term voltage stability analysis are presented.
In those cases, the QSS model provided incorrect stability assessment. Causes
of failure of the QSS model are explained and analyzed in nonlinear system
framework. Sufficient conditions of the QSS model for correct approximation are
suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6740</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6740</id><created>2013-11-19</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author></authors><title>Hilditchs Algorithm Based Tamil Character Recognition</title><categories>cs.CV</categories><comments>7 pages 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Character identification plays a vital role in the contemporary world of
Image processing. It can solve many composite problems and makes humans work
easier. An instance is Handwritten Character detection. Handwritten recognition
is not a novel expertise, but it has not gained community notice until Now. The
eventual aim of designing Handwritten Character recognition structure with an
accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character
recognition system uses the Neural Networks to distinguish them. Neural Network
and structural characteristics are used to instruct and recognize written
characters. After training and testing the exactness rate reached 99%. This
correctness rate is extremely high. In this paper we are exploring image
processing through the Hilditch algorithm foundation and structural
characteristics of a character in the image. And we recognized some character
of the Tamil language, and we are trying to identify all the character of Tamil
In our future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6751</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6751</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Stiffness modeling of robotic manipulator with gravity compensator</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>6th Int. Workshop on Computational Kinematics (CK2013), Barcelona
  : Spain (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the stiffness modeling of robotic manipulators with
gravity compensators. The main attention is paid to the development of the
stiffness model of a spring-based compensator located between sequential links
of a serial structure. The derived model allows us to describe the compensator
as an equivalent non-linear virtual spring integrated in the corresponding
actuated joint. The obtained results have been efficiently applied to the
stiffness modeling of a heavy industrial robot of the Kuka family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6754</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6754</id><created>2013-11-13</created><authors><author><keyname>Monteil</keyname><forenames>Thierry</forenames></author></authors><title>Spreading huge free software without internet connection, via
  self-replicating USB keys</title><categories>cs.OH</categories><comments>5 pages, accepted to Extremecom 2013</comments><acm-class>C.2.1; C.2.4; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and discuss an affordable way to spread huge software without
relying on internet connection, via the use of self-replicating live USB keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6758</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6758</id><created>2013-11-24</created><authors><author><keyname>Ott</keyname><forenames>Patrick</forenames></author><author><keyname>Everingham</keyname><forenames>Mark</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author></authors><title>Detection of Partially Visible Objects</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An &quot;elephant in the room&quot; for most current object detection and localization
methods is the lack of explicit modelling of partial visibility due to
occlusion by other objects or truncation by the image boundary. Based on a
sliding window approach, we propose a detection method which explicitly models
partial visibility by treating it as a latent variable. A novel non-maximum
suppression scheme is proposed which takes into account the inferred partial
visibility of objects while providing a globally optimal solution. The method
gives more detailed scene interpretations than conventional detectors in that
we are able to identify the visible parts of an object. We report improved
average precision on the PASCAL VOC 2010 dataset compared to a baseline
detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6785</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6785</id><created>2013-11-26</created><updated>2014-10-08</updated><authors><author><keyname>Beguerisse-D&#xed;az</keyname><forenames>Mariano</forenames></author><author><keyname>Gardu&#xf1;o-Hern&#xe1;ndez</keyname><forenames>Guillermo</forenames></author><author><keyname>Vangelov</keyname><forenames>Borislav</forenames></author><author><keyname>Yaliraki</keyname><forenames>Sophia N.</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Interest communities and flow roles in directed networks: the Twitter
  network of the UK riots</title><categories>physics.soc-ph cs.SI</categories><comments>32 pages, 14 figures. Supplementary Spreadsheet available from:
  http://www2.imperial.ac.uk/~mbegueri/Docs/riotsCommunities.zip or
  http://rsif.royalsocietypublishing.org/content/11/101/20140940/suppl/DC1</comments><journal-ref>J. R. Soc. Interface 6 December 2014 vol. 11 no. 101 20140940</journal-ref><doi>10.1098/rsif.2014.0940</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directionality is a crucial ingredient in many complex networks in which
information, energy or influence are transmitted. In such directed networks,
analysing flows (and not only the strength of connections) is crucial to reveal
important features of the network that might go undetected if the orientation
of connections is ignored. We showcase here a flow-based approach for community
detection in networks through the study of the network of the most influential
Twitter users during the 2011 riots in England. Firstly, we use directed Markov
Stability to extract descriptions of the network at different levels of
coarseness in terms of interest communities, i.e., groups of nodes within which
flows of information are contained and reinforced. Such interest communities
reveal user groupings according to location, profession, employer, and topic.
The study of flows also allows us to generate an interest distance, which
affords a personalised view of the attention in the network as viewed from the
vantage point of any given user. Secondly, we analyse the profiles of incoming
and outgoing long-range flows with a combined approach of role-based similarity
and the novel relaxed minimum spanning tree algorithm to reveal that the users
in the network can be classified into five roles. These flow roles go beyond
the standard leader/follower dichotomy and differ from classifications based on
regular/structural equivalence. We then show that the interest communities fall
into distinct informational organigrams characterised by a different mix of
user roles reflecting the quality of dialogue within them. Our generic
framework can be used to provide insight into how flows are generated,
distributed, preserved and consumed in directed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6799</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6799</id><created>2013-10-30</created><updated>2013-12-10</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Dash</keyname><forenames>Debadatta</forenames></author><author><keyname>Barmase</keyname><forenames>Swapnil</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K</forenames></author></authors><title>Wavelet and Fast Fourier Transform based analysis of Solar Image</title><categories>cs.CV cs.CE</categories><comments>This paper has been withdrawn by the author due to some modifications
  are required for this current paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both of Wavelet and Fast Fourier Transform are strong signal processing tools
in the field of Data Analysis. In this paper fast fourier transform (FFT) and
Wavelet Transform are employed to observe some important features of Solar
image (December, 2004). We have tried to find out the periodicity and coherence
of different sections of the solar image. We plotted the distribution of energy
in solar surface by analyzing the solar image with scalograms and
3D-coefficient plots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6802</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6802</id><created>2013-11-26</created><updated>2014-07-30</updated><authors><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author><author><keyname>Weinsberg</keyname><forenames>Udi</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Taft</keyname><forenames>Nina</forenames></author></authors><title>Recommending with an Agenda: Active Learning of Private Attributes using
  Matrix Factorization</title><categories>cs.LG cs.CY</categories><comments>This is the extended version of a paper that appeared in ACM RecSys
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems leverage user demographic information, such as age,
gender, etc., to personalize recommendations and better place their targeted
ads. Oftentimes, users do not volunteer this information due to privacy
concerns, or due to a lack of initiative in filling out their online profiles.
We illustrate a new threat in which a recommender learns private attributes of
users who do not voluntarily disclose them. We design both passive and active
attacks that solicit ratings for strategically selected items, and could thus
be used by a recommender system to pursue this hidden agenda. Our methods are
based on a novel usage of Bayesian matrix factorization in an active learning
setting. Evaluations on multiple datasets illustrate that such attacks are
indeed feasible and use significantly fewer rated items than static inference
methods. Importantly, they succeed without sacrificing the quality of
recommendations to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6809</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6809</id><created>2013-11-26</created><authors><author><keyname>Sayin</keyname><forenames>Muhammed O.</forenames></author><author><keyname>Vanli</keyname><forenames>N. Denizcan</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Novel Family of Adaptive Filtering Algorithms Based on The Logarithmic
  Cost</title><categories>cs.LG</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2333559</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel family of adaptive filtering algorithms based on a
relative logarithmic cost. The new family intrinsically combines the higher and
lower order measures of the error into a single continuous update based on the
error amount. We introduce important members of this family of algorithms such
as the least mean logarithmic square (LMLS) and least logarithmic absolute
difference (LLAD) algorithms that improve the convergence performance of the
conventional algorithms. However, our approach and analysis are generic such
that they cover other well-known cost functions as described in the paper. The
LMLS algorithm achieves comparable convergence performance with the least mean
fourth (LMF) algorithm and extends the stability bound on the step size. The
LLAD and least mean square (LMS) algorithms demonstrate similar convergence
performance in impulse-free noise environments while the LLAD algorithm is
robust against impulsive interferences and outperforms the sign algorithm (SA).
We analyze the transient, steady state and tracking performance of the
introduced algorithms and demonstrate the match of the theoretical analyzes and
simulation results. We show the extended stability bound of the LMLS algorithm
and analyze the robustness of the LLAD algorithm against impulsive
interferences. Finally, we demonstrate the performance of our algorithms in
different scenarios through numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6810</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6810</id><created>2013-11-26</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wu</keyname><forenames>Yier</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Dumas</keyname><forenames>Claire</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Caro</keyname><forenames>St&#xe9;phane</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Furet</keyname><forenames>Beno&#xee;t</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatol</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Identification of geometrical and elastostatic parameters of heavy
  industrial robots</title><categories>cs.RO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1311.6674</comments><proxy>ccsd</proxy><journal-ref>The 2013 IEEE International Conference on Robotics and Automation
  (ICRA 2013), Karlsruhe : Germany (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the stiffness modeling of heavy industrial robots with
gravity compensators. The main attention is paid to the identification of
geometrical and elastostatic parameters and calibration accuracy. To reduce
impact of the measurement errors, the set of manipulator configurations for
calibration experiments is optimized with respect to the proposed performance
measure related to the end-effector position accuracy. Experimental results are
presented that illustrate the advantages of the developed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6811</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6811</id><created>2013-11-26</created><authors><author><keyname>Song</keyname><forenames>Jian</forenames></author><author><keyname>Bian</keyname><forenames>Yatao</forenames></author><author><keyname>Yan</keyname><forenames>Junchi</forenames></author><author><keyname>Zhao</keyname><forenames>Xu</forenames></author><author><keyname>Liu</keyname><forenames>Yuncai</forenames></author></authors><title>Digitize Your Body and Action in 3-D at Over 10 FPS: Real Time Dense
  Voxel Reconstruction and Marker-less Motion Tracking via GPU Acceleration</title><categories>cs.GR</categories><comments>23 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach to reconstruct 3-D human motion from
multi-cameras and track human skeleton using the reconstructed human 3-D point
(voxel) cloud. We use an improved and more robust algorithm, probabilistic
shape from silhouette to reconstruct human voxel. In addition, the annealed
particle filter is applied for tracking, where the measurement is computed
using the reprojection of reconstructed voxel. We use two different ways to
accelerate the approach. For the CPU only acceleration, we leverage Intel TBB
to speed up the hot spot of the computational overhead and reached an
accelerating ratio of 3.5 on a 4-core CPU. Moreover, we implement an
intensively paralleled version via GPU acceleration without TBB. Taking account
all data transfer and computing time, the GPU version is about 400 times faster
than the original CPU implementation, leading the approach to run at a
real-time speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6821</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6821</id><created>2013-11-26</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>A Class of Orthogonal Sequences</title><categories>cs.CR</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a class of random orthogonal sequences associated with
the number theoretic Hilbert transform. We present a constructive procedure for
finding the random sequences for different modulus values. These random
sequences have autocorrelation function that is zero everywhere excepting at
the origin. These sequences may be used as keys and in other cryptography
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6830</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6830</id><created>2013-11-26</created><authors><author><keyname>Balle</keyname><forenames>Borja</forenames></author></authors><title>Ergodicity of Random Walks on Random DFA</title><categories>cs.FL cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a DFA we consider the random walk that starts at the initial state and
at each time step moves to a new state by taking a random transition from the
current state. This paper shows that for typical DFA this random walk induces
an ergodic Markov chain. The notion of typical DFA is formalized by showing
that ergodicity holds with high probability when a DFA is sampled uniformly at
random from the set of all automata with a fixed number of states. We also show
the same result applies to DFA obtained by minimizing typical DFA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6834</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6834</id><created>2013-11-26</created><updated>2015-01-16</updated><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>Semi-Supervised Sparse Coding</title><categories>stat.ML cs.LG</categories><doi>10.1109/IJCNN.2014.6889449</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding approximates the data sample as a sparse linear combination of
some basic codewords and uses the sparse codes as new presentations. In this
paper, we investigate learning discriminative sparse codes by sparse coding in
a semi-supervised manner, where only a few training samples are labeled. By
using the manifold structure spanned by the data set of both labeled and
unlabeled samples and the constraints provided by the labels of the labeled
samples, we learn the variable class labels for all the samples. Furthermore,
to improve the discriminative ability of the learned sparse codes, we assume
that the class labels could be predicted from the sparse codes directly using a
linear classifier. By solving the codebook, sparse codes, class labels and
classifier parameters simultaneously in a unified objective function, we
develop a semi-supervised sparse coding algorithm. Experiments on two
real-world pattern recognition problems demonstrate the advantage of the
proposed methods over supervised sparse coding methods on partially labeled
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6837</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6837</id><created>2013-11-26</created><authors><author><keyname>Malandrino</keyname><forenames>Francesco</forenames></author><author><keyname>Casetti</keyname><forenames>Claudio</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla-Fabiana</forenames></author><author><keyname>Limani</keyname><forenames>Zana</forenames></author></authors><title>Fast Resource Scheduling in HetNets with D2D Support</title><categories>cs.NI</categories><comments>9 pages, 5 figures, Accepted to the IEEE International Conference on
  Computer Communications (INFOCOM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource allocation in LTE networks is known to be an NP-hard problem. In
this paper, we address an even more complex scenario: an LTE-based, 2-tier
heterogeneous network where D2D mode is supported under the network control.
All communications (macrocell, microcell and D2D-based) share the same
frequency bands, hence they may interfere. We then determine (i) the network
node that should serve each user and (ii) the radio resources to be scheduled
for such communication. To this end, we develop an accurate model of the system
and apply approximate dynamic programming to solve it. Our algorithms allow us
to deal with realistic, large-scale scenarios. In such scenarios, we compare
our approach to today's networks where eICIC techniques and proportional
fairness scheduling are implemented. Results highlight that our solution
increases the system throughput while greatly reducing energy consumption. We
also show that D2D mode can effectively support content delivery without
significantly harming macrocells or microcells traffic, leading to an increased
system capacity. Interestingly, we find that D2D mode can be a low-cost
alternative to microcells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6838</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6838</id><created>2013-11-26</created><authors><author><keyname>Amin</keyname><forenames>Kareem</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author><author><keyname>Syed</keyname><forenames>Umar</forenames></author></authors><title>Learning Prices for Repeated Auctions with Strategic Buyers</title><categories>cs.LG cs.GT</categories><comments>Neural Information Processing Systems (NIPS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by real-time ad exchanges for online display advertising, we
consider the problem of inferring a buyer's value distribution for a good when
the buyer is repeatedly interacting with a seller through a posted-price
mechanism. We model the buyer as a strategic agent, whose goal is to maximize
her long-term surplus, and we are interested in mechanisms that maximize the
seller's long-term revenue. We define the natural notion of strategic regret
--- the lost revenue as measured against a truthful (non-strategic) buyer. We
present seller algorithms that are no-(strategic)-regret when the buyer
discounts her future surplus --- i.e. the buyer prefers showing advertisements
to users sooner rather than later. We also give a lower bound on strategic
regret that increases as the buyer's discounting weakens and shows, in
particular, that any seller algorithm will suffer linear strategic regret if
there is no discounting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6839</identifier>
 <datestamp>2013-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6839</id><created>2013-11-26</created><authors><author><keyname>Schaefer</keyname><forenames>Marcus</forenames></author></authors><title>Picking Planar Edges; or, Drawing a Graph with a Planar Subgraph</title><categories>cs.CG cs.DM math.CO</categories><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$ and a subset $F \subseteq E(G)$ of its edges, is there a
drawing of $G$ in which all edges of $F$ are free of crossings? We show that
this question can be solved in polynomial time using a Hanani-Tutte style
approach. If we require the drawing of $G$ to be straight-line, and allow at
most one crossing along each edge in $F$, the problem turns out to be as hard
as the existential theory of the real numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6848</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6848</id><created>2013-11-26</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>Random Residue Sequences and the Number Theoretic Hilbert Transform</title><categories>cs.CR</categories><comments>12 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents random residue sequences derived from the number
theoretic Hilbert (NHT) transform and their correlation properties. The
autocorrelation of a NHT derived sequence is zero for all non-zero shifts which
illustrates that these are self-orthogonal sequences. The cross correlation
function between two sequences may be computed with respect to the moduli of
the either sequence. There appears to be some kind of an inverse qualitative
relationship between these two different computations for many sets of residue
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6853</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6853</id><created>2013-11-26</created><updated>2014-05-30</updated><authors><author><keyname>Salim</keyname><forenames>Omar H.</forenames></author><author><keyname>Nasir</keyname><forenames>Ali A.</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author></authors><title>Channel, Phase Noise, and Frequency Offset in OFDM Systems: Joint
  Estimation, Data Detection, and Hybrid Cramer-Rao Lower Bound</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Communications, vol. 62, no.9, pp. 3311-3325,
  Sep. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oscillator phase noise (PHN) and carrier frequency offset (CFO) can adversely
impact the performance of orthogonal frequency division multiplexing (OFDM)
systems, since they can result in inter carrier interference and rotation of
the signal constellation. In this paper, we propose an expectation conditional
maximization (ECM) based algorithm for joint estimation of channel, PHN, and
CFO in OFDM systems. We present the signal model for the estimation problem and
derive the hybrid Cramer-Rao lower bound (HCRB) for the joint estimation
problem. Next, we propose an iterative receiver based on an extended Kalman
filter for joint data detection and PHN tracking. Numerical results show that,
compared to existing algorithms, the performance of the proposed ECM-based
estimator is closer to the derived HCRB and outperforms the existing estimation
algorithms at moderate-to-high signal-to-noise ratio (SNR). In addition, the
combined estimation algorithm and iterative receiver are more computationally
efficient than existing algorithms and result in improved average uncoded and
coded bit error rate (BER) performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6863</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6863</id><created>2013-11-26</created><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szybowski</keyname><forenames>J.</forenames></author></authors><title>Pairwise Comparisons Simplified</title><categories>cs.DM cs.GT</categories><comments>15 pages, two figures</comments><msc-class>65F30</msc-class><acm-class>C.4; D.2.8; G.1.2; G.1.6; H.1.1; H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines the notion of generators of a pairwise comparisons
matrix. Such approach decreases the number of pairwise comparisons from $n\cdot
(n-1)$ to $n-1$. An algorithm of reconstructing of the PC matrix from its set
of generators is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6868</identifier>
 <datestamp>2013-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6868</id><created>2013-11-27</created><authors><author><keyname>Veliz-Cuba</keyname><forenames>Alan</forenames></author><author><keyname>Laubenbacher</keyname><forenames>Reinhard</forenames></author><author><keyname>Aguilar</keyname><forenames>Boris</forenames></author></authors><title>Dimension Reduction of Large AND-NOT Network Models</title><categories>q-bio.MN cs.CE cs.SI q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean networks have been used successfully in modeling biological networks
and provide a good framework for theoretical analysis. However, the analysis of
large networks is not trivial. In order to simplify the analysis of such
networks, several model reduction algorithms have been proposed; however, it is
not clear if such algorithms scale well with respect to the number of nodes.
The goal of this paper is to propose and implement an algorithm for the
reduction of AND-NOT network models for the purpose of steady state
computation. Our method of network reduction is the use of &quot;steady state
approximations&quot; that do not change the number of steady states. Our algorithm
is designed to work at the wiring diagram level without the need to evaluate or
simplify Boolean functions. Also, our implementation of the algorithm takes
advantage of the sparsity typical of discrete models of biological systems. The
main features of our algorithm are that it works at the wiring diagram level,
it runs in polynomial time, and it preserves the number of steady states. We
used our results to study AND-NOT network models of gene networks and showed
that our algorithm greatly simplifies steady state analysis. Furthermore, our
algorithm can handle sparse AND-NOT networks with up to 1000000 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6870</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6870</id><created>2013-11-27</created><authors><author><keyname>Shang</keyname><forenames>Jin</forenames></author><author><keyname>Tai</keyname><forenames>Nengling</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author></authors><title>Multi-agent based protection system for distribution system with DG</title><categories>cs.MA</categories><comments>8 pages, 5 figures, 1 table, 21 conference</comments><msc-class>93B99</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the basic structure of multi-agent based protection
system for distribution system with DGs. The entire system consists of
intelligent agents and communication system. Intelligent agents can be divided
into three layers, the bottom layer, the middle layer and the upper layer. The
design of the agent in different layer is analyzed in detail. Communication
system is the bridge of multi-agent system (MAS). The transmission mode,
selective communication and other principles are discussed to improve the
transmission efficiency. Finally, some evaluations are proposed, which provides
the design of MAS with reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6876</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6876</id><created>2013-11-27</created><authors><author><keyname>Yao</keyname><forenames>Yuan</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Xie</keyname><forenames>Tao</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author><author><keyname>Xu</keyname><forenames>Feng</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Want a Good Answer? Ask a Good Question First!</title><categories>cs.DB cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community Question Answering (CQA) websites have become valuable repositories
which host a massive volume of human knowledge. To maximize the utility of such
knowledge, it is essential to evaluate the quality of an existing question or
answer, especially soon after it is posted on the CQA website.
  In this paper, we study the problem of inferring the quality of questions and
answers through a case study of a software CQA (Stack Overflow). Our key
finding is that the quality of an answer is strongly positively correlated with
that of its question. Armed with this observation, we propose a family of
algorithms to jointly predict the quality of questions and answers, for both
quantifying numerical quality scores and differentiating the high-quality
questions/answers from those of low quality. We conduct extensive experimental
evaluations to demonstrate the effectiveness and efficiency of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6877</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6877</id><created>2013-11-27</created><authors><author><keyname>Vijayvargiya</keyname><forenames>Gaurav</forenames></author><author><keyname>Silakari</keyname><forenames>Sanjay</forenames></author><author><keyname>Pandey</keyname><forenames>Rajeev</forenames></author></authors><title>A Survey: Various Techniques of Image Compression</title><categories>cs.IT cs.MM math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses about various image compression techniques. On the basis
of analyzing the various image compression techniques this paper presents a
survey of existing research papers. In this paper we analyze different types of
existing method of image compression. Compression of an image is significantly
different then compression of binary raw data. To solve these use different
types of techniques for image compression. Now there is question may be arise
that how to image compress and which types of technique is used. For this
purpose there are basically two types are method are introduced namely lossless
and lossy image compression techniques. In present time some other techniques
are added with basic method. In some area neural network genetic algorithms are
used for image compression.
  Keywords-Image Compression; Lossless; Lossy; Redundancy; Benefits of
Compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6879</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6879</id><created>2013-11-27</created><authors><author><keyname>Das</keyname><forenames>Sukanta</forenames></author><author><keyname>Sikdar</keyname><forenames>Biplab K</forenames></author></authors><title>Analysis and synthesis of nonlinear reversible cellular automata in
  linear time</title><categories>cs.FL nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular automata (CA) have been found as an attractive modeling tool for
various applications, such as, pattern recognition, image processing, data
compression, encryption, and specially for VLSI design &amp; test. For such
applications, mostly a special class of CA, called as linear/additive CA, have
been utilized. Since linear/additive CA refer a limited number of candidate CA,
while searching for solution to a problem, the best result may not be expected.
The nonlinear CA can be a better alternative to linear/additive CA for
achieving desired solutions in different applications. However, the nonlinear
CA are yet to be characterized to fit the design for modeling an application.
This work targets characterization of the nonlinear CA to utilize the huge
search space of nonlinear CA while developing applications in VLSI domain. An
analytical framework is developed to explore the properties of CA rules. The
characterization is directed to deal with the reversibility, as the reversible
CA are primarily targeted for VLSI applications. The reported characterization
enables us to design two algorithms of linear time complexities -- one for
identification and nother for synthesis of nonlinear reversible CA. Finally,
the CA rules are classified into 6 classes for developing further efficient
synthesis algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6880</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6880</id><created>2013-11-27</created><authors><author><keyname>Cheng</keyname><forenames>Zhiyu</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>The Degrees of Freedom of the $K$-pair-user Full-Duplex Two-way
  Interference Channel with and without a MIMO Relay</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Journal on Selected Areas in Communications
  (JSAC) in October 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a $K$-pair-user two-way interference channel (TWIC), $2K$ messages and
$2K$ transmitters/receivers form a $K$-user IC in the forward direction ($K$
messages) and another $K$-user IC in the backward direction which operate in
full-duplex mode. All nodes may interact, or adapt inputs to past received
signals. We derive a new outer bound to demonstrate that the optimal degrees of
freedom (DoF, also known as the multiplexing gain) is $K$: full-duplex
operation doubles the DoF, but interaction does not further increase the DoF.
We next characterize the DoF of the $K$-pair-user TWIC with a MIMO, full-duplex
relay. If the relay is non-causal/instantaneous (at time $k$ forwards a
function of its received signals up to time $k$) and has $2K$ antennas, we
demonstrate a one-shot scheme where the relay mitigates all interference to
achieve the interference-free $2K$ DoF. In contrast, if the relay is causal (at
time $k$ forwards a function of its received signals up to time $k-1$), we show
that a full-duplex MIMO relay cannot increase the DoF of the $K$-pair-user TWIC
beyond $K$, as if no relay or interaction is present. We comment on reducing
the number of antennas at the instantaneous relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6881</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6881</id><created>2013-11-27</created><authors><author><keyname>Pandey</keyname><forenames>Abhishek</forenames><affiliation>Dept. of CSE, UIT-RGPV</affiliation></author><author><keyname>Deen</keyname><forenames>Anjna Jayant</forenames><affiliation>Dept. of CSE, UIT-RGPV</affiliation></author><author><keyname>Pandey</keyname><forenames>Rajeev</forenames><affiliation>Dept. of CSE, UIT-RGPV</affiliation></author></authors><title>Color and Shape Content Based Image Classification using RBF Network and
  PSO Technique: A Survey</title><categories>cs.CV cs.LG cs.NE</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The improvement of the accuracy of image query retrieval used image
classification technique. Image classification is well known technique of
supervised learning. The improved method of image classification increases the
working efficiency of image query retrieval. For the improvements of
classification technique we used RBF neural network function for better
prediction of feature used in image retrieval.Colour content is represented by
pixel values in image classification using radial base function(RBF) technique.
This approach provides better result compare to SVM technique in image
representation.Image is represented by matrix though RBF using pixel values of
colour intensity of image. Firstly we using RGB colour model. In this colour
model we use red, green and blue colour intensity values in matrix.SVM with
partical swarm optimization for image classification is implemented in content
of images which provide better Results based on the proposed approach are found
encouraging in terms of color image classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6883</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6883</id><created>2013-11-27</created><authors><author><keyname>Minooei</keyname><forenames>Hadi</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Near-Optimal and Robust Mechanism Design for Covering Problems with
  Correlated Players</title><categories>cs.GT cs.DS</categories><acm-class>F.2.2; G.1.6; G.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing incentive-compatible, ex-post
individually rational (IR) mechanisms for covering problems in the Bayesian
setting, where players' types are drawn from an underlying distribution and may
be correlated, and the goal is to minimize the expected total payment made by
the mechanism. We formulate a notion of incentive compatibility (IC) that we
call {\em robust Bayesian IC} (robust BIC) that is substantially more robust
than BIC, and develop black-box reductions from robust-BIC mechanism design to
algorithm design. For single-dimensional settings, this black-box reduction
applies even when we only have an LP-relative {\em approximation algorithm} for
the algorithmic problem. Thus, we obtain near-optimal mechanisms for various
covering settings including single-dimensional covering problems, multi-item
procurement auctions, and multidimensional facility location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6887</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6887</id><created>2013-11-27</created><updated>2014-04-09</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Ayan</forenames></author><author><keyname>Xiong</keyname><forenames>Ying</forenames></author><author><keyname>Sun</keyname><forenames>Baochen</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Scharstein</keyname><forenames>Daniel</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Modeling Radiometric Uncertainty for Vision with Tone-mapped Color
  Images</title><categories>cs.CV</categories><journal-ref>IEEE Trans. PAMI 36 (2014) 2185-2198</journal-ref><doi>10.1109/TPAMI.2014.2318713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To produce images that are suitable for display, tone-mapping is widely used
in digital cameras to map linear color measurements into narrow gamuts with
limited dynamic range. This introduces non-linear distortion that must be
undone, through a radiometric calibration process, before computer vision
systems can analyze such photographs radiometrically. This paper considers the
inherent uncertainty of undoing the effects of tone-mapping. We observe that
this uncertainty varies substantially across color space, making some pixels
more reliable than others. We introduce a model for this uncertainty and a
method for fitting it to a given camera or imaging pipeline. Once fit, the
model provides for each pixel in a tone-mapped digital photograph a probability
distribution over linear scene colors that could have induced it. We
demonstrate how these distributions can be useful for visual inference by
incorporating them into estimation algorithms for a representative set of
vision tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6897</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6897</id><created>2013-11-27</created><authors><author><keyname>Li</keyname><forenames>Xiaoliang</forenames></author><author><keyname>Sun</keyname><forenames>Yao</forenames></author></authors><title>Analyzing Multiplicities of a Zero-dimensional Regular Set's Zeros Using
  Pseudo Squarefree Decomposition</title><categories>cs.SC</categories><comments>13 pages</comments><msc-class>68W30, 13P99</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we are concerned with the problem of counting the
multiplicities of a zero-dimensional regular set's zeros. We generalize the
squarefree decomposition of univariate polynomials to the so-called pseudo
squarefree decomposition of multivariate polynomials, and then propose an
algorithm for decomposing a regular set into a finite number of simple sets.
From the output of this algorithm, the multiplicities of zeros could be
directly read out, and the real solution isolation with multiplicity can also
be easily produced. Experiments with a preliminary implementation show the
efficiency of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6902</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6902</id><created>2013-11-27</created><authors><author><keyname>Casta&#xf1;eda</keyname><forenames>Armando</forenames></author><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Good, Better, Best! - Unbeatable Protocols for Consensus and Set
  Consensus</title><categories>cs.DC</categories><report-no>Hebrew University of Jerusalem Center for the Study of Rationality
  discussion paper 653</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the very first consensus protocols for the synchronous model were
designed to match the worst-case lower bound, deciding in exactly t+1 rounds in
all runs, it was soon realized that they could be strictly improved upon by
early stopping protocols. These dominate the first ones, by always deciding in
at most t+1 rounds, but often much faster. A protocol is unbeatable if it can't
be strictly dominated. Unbeatability is often a much more suitable notion of
optimality for distributed protocols than worst-case performance. Using a
knowledge-based analysis, this paper studies unbeatability for both consensus
and k-set consensus. We present unbeatable solutions to non-uniform consensus
and k-set consensus, and uniform consensus in synchronous message-passing
contexts with crash failures.
  The k-set consensus problem is much more technically challenging than
consensus, and its analysis has triggered the development of the topological
approach to distributed computing. Worst-case lower bounds for this problem
have required either techniques based on algebraic topology, or reduction-based
proofs. Our proof of unbeatability is purely combinatorial, and is a direct,
albeit nontrivial, generalization of the one for consensus. We also present an
alternative topological unbeatability proof that allows to understand the
connection between the connectivity of protocol complexes and the decision time
of processes.
  For the synchronous model, only solutions to the uniform variant of k-set
consensus have been offered. Based on our unbeatable protocols for uniform
consensus and for non-uniform k-set consensus, we present a uniform k-set
consensus protocol that strictly dominates all known solutions to this problem
in the synchronous model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6907</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6907</id><created>2013-11-27</created><authors><author><keyname>M&#xe9;tivier</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Loudni</keyname><forenames>Samir</forenames></author><author><keyname>Charnois</keyname><forenames>Thierry</forenames></author></authors><title>A Constraint Programming Approach for Mining Sequential Patterns in a
  Sequence Database</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint-based pattern discovery is at the core of numerous data mining
tasks. Patterns are extracted with respect to a given set of constraints
(frequency, closedness, size, etc). In the context of sequential pattern
mining, a large number of devoted techniques have been developed for solving
particular classes of constraints. The aim of this paper is to investigate the
use of Constraint Programming (CP) to model and mine sequential patterns in a
sequence database. Our CP approach offers a natural way to simultaneously
combine in a same framework a large set of constraints coming from various
origins. Experiments show the feasibility and the interest of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6914</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6914</id><created>2013-11-27</created><authors><author><keyname>Freris</keyname><forenames>Nikolaos</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Model-based clock synchronization protocol for wireless sensor networks</title><categories>cs.NI</categories><comments>Technical report - this work was submitted to IEEE/ACM Transactions
  on Networking, Nov. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless sensor network, nodes communicate time-stamped packets in order
to synchronize their clocks, i.e., estimate each other's time display. We
introduce and analyze a parametrized stochastic model for clocks and use it to
calculate, at any given time, the Maximum-Likelihood (ML) estimate of the
relative skew/offset between two communicating nodes. For network
synchronization, event-based Kalman-Bucy filtering gives rise to a centralized
scheme, and we propose an efficient distributed suboptimal algorithm. We study
the performance both analytically and experimentally and provide provable
guarantees. We summarize our findings into defining a new distributed
model-based clock synchronization protocol (MBCSP), and present a comparative
simulation study of its accuracy versus prior art to showcase improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6916</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6916</id><created>2013-11-27</created><updated>2014-07-11</updated><authors><author><keyname>Lu</keyname><forenames>Zhenqi</forenames></author><author><keyname>Ying</keyname><forenames>Rendong</forenames></author><author><keyname>Jiang</keyname><forenames>Sumxin</forenames></author><author><keyname>Zhang</keyname><forenames>Zenghui</forenames></author><author><keyname>Liu</keyname><forenames>Peilin</forenames></author><author><keyname>Yu</keyname><forenames>Wenxian</forenames></author></authors><title>Spectral Compressive Sensing with Model Selection</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, 1 table, published in ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of existing approaches to the recovery of frequency-sparse
signals from compressed measurements is limited by the coherence of required
sparsity dictionaries and the discretization of frequency parameter space. In
this paper, we adopt a parametric joint recovery-estimation method based on
model selection in spectral compressive sensing. Numerical experiments show
that our approach outperforms most state-of-the-art spectral CS recovery
approaches in fidelity, tolerance to noise and computation efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6929</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6929</id><created>2013-11-27</created><authors><author><keyname>Protzenko</keyname><forenames>Jonathan</forenames></author></authors><title>Illustrating the Mezzo programming language</title><categories>cs.PL</categories><acm-class>D.3.2</acm-class><journal-ref>1st French Singaporean Workshop on Formal Methods and Applications
  (FSFMA 2013)</journal-ref><doi>10.4230/OASIcs.FSFMA.2013.68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When programmers want to prove strong program invariants, they are usually
faced with a choice between using theorem provers and using traditional
programming languages. The former requires them to provide program proofs,
which, for many applications, is considered a heavy burden. The latter provides
less guarantees and the programmer usually has to write run-time assertions to
compensate for the lack of suitable invariants expressible in the type system.
  We introduce Mezzo, a programming language in the tradition of ML, in which
the usual concept of a type is replaced by a more precise notion of a
permission. Programs written in Mezzo usually enjoy stronger guarantees than
programs written in pure ML. However, because Mezzo is based on a type system,
the reasoning requires no user input. In this paper, we illustrate the key
concepts of Mezzo, highlighting the static guarantees our language provides.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6932</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6932</id><created>2013-11-27</created><authors><author><keyname>Cozzolino</keyname><forenames>Davide</forenames></author><author><keyname>Gragnaniello</keyname><forenames>Diego</forenames></author><author><keyname>Verdoliva</keyname><forenames>Luisa</forenames></author></authors><title>A novel framework for image forgery localization</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Image forgery localization is a very active and open research field for the
difficulty to handle the large variety of manipulations a malicious user can
perform by means of more and more sophisticated image editing tools. Here, we
propose a localization framework based on the fusion of three very different
tools, based, respectively, on sensor noise, patch-matching, and machine
learning. The binary masks provided by these tools are finally fused based on
some suitable reliability indexes. According to preliminary experiments on the
training set, the proposed framework provides often a very good localization
accuracy and sometimes valuable clues for visual scrutiny.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6933</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6933</id><created>2013-11-27</created><authors><author><keyname>Oza</keyname><forenames>Vaibhavi</forenames></author><author><keyname>Kettunen</keyname><forenames>Petri</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Attaining High-performing Software Teams with Agile and Lean Practices:
  An Empirical Case Study</title><categories>cs.SE</categories><comments>4 pages. Proceedings of the International Software Technology
  Exchange Workshop 2011 (STEW 2011), Stockholm, Sweden, November 23 2011. The
  final publication is available at
  http://www.swedsoft.se/wp-content/uploads/2011/09/stew2011_submission_17.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an empirical study on how self- organized software teams
could attain high performance using agile and lean practices. In particular,
the paper qualitatively examines characteristics of high performance and self-
organization in one project team. The case under study is a customer-driven
student project, carried out to develop an alpha-version prototype. The paper
also studies how certain agile software practices aid in initialising
self-organization in the team. The main results indicate that self-organization
as supported by certain Agile and Lean practices helps teams in achieving
higher performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6934</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6934</id><created>2013-11-27</created><authors><author><keyname>Cozzolino</keyname><forenames>Davide</forenames></author><author><keyname>Gragnaniello</keyname><forenames>Diego</forenames></author><author><keyname>Verdoliva</keyname><forenames>Luisa</forenames></author></authors><title>Image forgery detection based on the fusion of machine learning and
  block-matching methods</title><categories>cs.CV</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Dense local descriptors and machine learning have been used with success in
several applications, like classification of textures, steganalysis, and
forgery detection. We develop a new image forgery detector building upon some
descriptors recently proposed in the steganalysis field suitably merging some
of such descriptors, and optimizing a SVM classifier on the available training
set. Despite the very good performance, very small forgeries are hardly ever
detected because they contribute very little to the descriptors. Therefore we
also develop a simple, but extremely specific, copy-move detector based on
region matching and fuse decisions so as to reduce the missing detection rate.
Overall results appear to be extremely encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6948</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6948</id><created>2013-11-27</created><updated>2014-05-20</updated><authors><author><keyname>Iliev</keyname><forenames>Bozhidar Z.</forenames><affiliation>Institute for Nuclear Research and Nuclear Energy, Bulgarian Academy of Sciences, Sofia, Bulgaria</affiliation></author></authors><title>Measuring the evaluation and impact of scientific works and their
  authors</title><categories>physics.soc-ph cs.DL</categories><comments>36 LaTeX pages. The packages AMS-LaTeX and amsfonts are required.
  Changes in the second revised version: English grammar improvements, some
  Internet related resources are updated and new once are added, other minor
  changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems for evaluation and impact of published scientific works and their
authors are discussed. The role of citations in this process is pointed out.
Different bibliometric indicators are reviewed in this connection and ways for
generation of new bibliometric indices are given. The influence of different
circumstances, like self-citations, number of authors, time dependence and
publication types, on the evaluation and impact of scientific papers are
considered. The repercussion of works citations and their content is
investigated in this respect. Attention is paid also on implicit citations
which are not covered by the modern bibliometrics but often are reflected in
the peer reviews. Some aspects of the Web analogues of citations and new
possibilities of the Internet resources in evaluating authors achievements are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6949</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6949</id><created>2013-11-27</created><authors><author><keyname>Bonetto</keyname><forenames>Riccardo</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Rossi</keyname><forenames>Michele</forenames></author></authors><title>Distributed Power Loss Minimization in Residential Micro Grids: a
  Communications Perspective</title><categories>cs.OH</categories><comments>11 pages, 14 figures, smart grids optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constantly increasing number of power generation devices based on
renewables is calling for a transition from the centralized control of
electrical distribution grids to a distributed control scenario. In this
context, distributed generators (DGs) are exploited to achieve other objectives
beyond supporting loads, such as the minimization of the power losses along the
distribution lines. The aim of this work is that of designing a full-fledged
system that extends existing state of the art algorithms for the distributed
minimization of power losses. We take into account practical aspects such as
the design of a communication and coordination protocol that is resilient to
link failures and manages channel access, message delivery and DG coordination.
Thus, we analyze the performance of the resulting optimization and
communication scheme in terms of power loss reduction, reduction of aggregate
power demand, convergence rate and resilience to communication link failures.
After that, we discuss the results of a thorough simulation campaign, obtained
using topologies generated through a statistical approach that has been
validated in previous research, by also assessing the performance deviation
with respect to localized schemes, where the DGs are operated independently.
Our results reveal that the convergence and stability performance of the
selected algorithms vary greatly. However, configurations exist for which
convergence is possible within five to ten communication steps and, when just
30% of the nodes are DGs, the aggregate power demand is roughly halved. Also,
some of the considered approaches are quite robust against link failures as
they still provide gains with respect to the localized solutions for failure
rates as high as 50%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6976</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6976</id><created>2013-11-27</created><updated>2014-05-13</updated><authors><author><keyname>Fruergaard</keyname><forenames>Bjarne &#xd8;rum</forenames></author><author><keyname>Hansen</keyname><forenames>Toke Jansen</forenames></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author></authors><title>Dimensionality reduction for click-through rate prediction: Dense versus
  sparse representation</title><categories>stat.ML cs.LG stat.AP stat.ME</categories><comments>Presented at the Probabilistic Models for Big Data workshop at NIPS
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In online advertising, display ads are increasingly being placed based on
real-time auctions where the advertiser who wins gets to serve the ad. This is
called real-time bidding (RTB). In RTB, auctions have very tight time
constraints on the order of 100ms. Therefore mechanisms for bidding
intelligently such as clickthrough rate prediction need to be sufficiently
fast. In this work, we propose to use dimensionality reduction of the
user-website interaction graph in order to produce simplified features of users
and websites that can be used as predictors of clickthrough rate. We
demonstrate that the Infinite Relational Model (IRM) as a dimensionality
reduction offers comparable predictive performance to conventional
dimensionality reduction schemes, while achieving the most economical usage of
features and fastest computations at run-time. For applications such as
real-time bidding, where fast database I/O and few computations are key to
success, we thus recommend using IRM based features as predictors to exploit
the recommender effects from bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6981</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6981</id><created>2013-11-12</created><authors><author><keyname>Shukla</keyname><forenames>Anupam</forenames></author><author><keyname>Ojha</keyname><forenames>Gaurav</forenames></author><author><keyname>Acharya</keyname><forenames>Sachin</forenames></author><author><keyname>Jain</keyname><forenames>Shubham</forenames></author></authors><title>A customized flocking algorithm for swarms of sensors tracking a swarm
  of targets</title><categories>cs.OH cs.NI</categories><comments>13 Pages, 11 Figures, SAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless mobile sensor networks (WMSNs) are groups of mobile sensing agents
with multi-modal sensing capabilities that communicate over wireless networks.
WMSNs have more flexibility in terms of deployment and exploration abilities
over static sensor networks. Sensor networks have a wide range of applications
in security and surveillance systems, environmental monitoring, data gathering
for network-centric healthcare systems, monitoring seismic activities and
atmospheric events, tracking traffic congestion and air pollution levels,
localization of autonomous vehicles in intelligent transportation systems, and
detecting failures of sensing, storage, and switching components of smart
grids. The above applications require target tracking for processes and events
of interest occurring in an environment. Various methods and approaches have
been proposed in order to track one or more targets in a pre-defined area.
Usually, this turns out to be a complicated job involving higher order
mathematics coupled with artificial intelligence due to the dynamic nature of
the targets. To optimize the resources we need to have an approach that works
in a more straightforward manner while resulting in fairly satisfactory data.
In this paper we have discussed the various cases that might arise while
flocking a group of sensors to track targets in a given environment. The
approach has been developed from scratch although some basic assumptions have
been made keeping in mind some previous theories. This paper outlines a
customized approach for feasibly tracking swarms of targets in a specific area
so as to minimize the resources and optimize tracking efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.6996</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.6996</id><created>2013-11-13</created><authors><author><keyname>Dwyer</keyname><forenames>Tim</forenames></author><author><keyname>Mears</keyname><forenames>Christopher</forenames></author><author><keyname>Morgan</keyname><forenames>Kerri</forenames></author><author><keyname>Niven</keyname><forenames>Todd</forenames></author><author><keyname>Marriott</keyname><forenames>Kim</forenames></author><author><keyname>Wallace</keyname><forenames>Mark</forenames></author></authors><title>Improved Optimal and Approximate Power Graph Compression for Clearer
  Visualisation of Dense Graphs</title><categories>cs.CG cs.HC</categories><comments>Extended technical report accompanying the PacificVis 2013 paper of
  the same name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawings of highly connected (dense) graphs can be very difficult to read.
Power Graph Analysis offers an alternate way to draw a graph in which sets of
nodes with common neighbours are shown grouped into modules. An edge connected
to the module then implies a connection to each member of the module. Thus, the
entire graph may be represented with much less clutter and without loss of
detail. A recent experimental study has shown that such lossless compression of
dense graphs makes it easier to follow paths. However, computing optimal power
graphs is difficult. In this paper, we show that computing the optimal
power-graph with only one module is NP-hard and therefore likely NP-hard in the
general case. We give an ILP model for power graph computation and discuss why
ILP and CP techniques are poorly suited to the problem. Instead, we are able to
find optimal solutions much more quickly using a custom search method. We also
show how to restrict this type of search to allow only limited back-tracking to
provide a heuristic that has better speed and better results than previously
known heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7011</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7011</id><created>2013-11-27</created><authors><author><keyname>Perez-Riverol</keyname><forenames>Yasset</forenames></author><author><keyname>Alvarez</keyname><forenames>Roberto Vera</forenames></author></authors><title>A UML-based Approach to Design Parallel and Distributed Applications</title><categories>cs.SE cs.DC cs.DS</categories><comments>5 Figures, Work presented in two conferences and related with the
  design of a Parallel Program for Conformational Search in small molecules
  (PMID: 23030613)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Parallel and distributed application design is a major area of interest in
the domain of high performance scientific and industrial computing. Over the
years, various approaches have been proposed to aid parallel program developers
to modeling their applications. In this paper it will be used some concepts
from agile development methodologies and Unified Modeling Language (UML) to
modeling parallel and distributed applications. The UML-based approach of this
paper describes through different artifacts and graphs the main flows of events
in the development of parallel and high performance applications. Here, we
presented three work flows to describe and to model our parallel program,
Domain Model, Design and Modeling and Test. All these phases of the development
software allow to programmers convert the requirements of the problem in a good
and efficient solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7038</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7038</id><created>2013-11-27</created><authors><author><keyname>Kim</keyname><forenames>Hye Jung</forenames></author><author><keyname>Nation</keyname><forenames>J. B.</forenames></author><author><keyname>Shepler</keyname><forenames>Anne V.</forenames></author></authors><title>Group Coding with Complex Isometries</title><categories>math.CO cs.IT math.IT math.RT</categories><comments>34 pages include two appendices</comments><msc-class>94B60, 20G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate group coding for arbitrary finite groups acting linearly on a
vector space. These yield robust codes based on real or complex matrix groups.
We give necessary and sufficient conditions for correct subgroup decoding using
geometric notions of minimal length coset representatives. The infinite family
of complex reflection groups G(r,1,n) produces effective codes of arbitrarily
large size that can be decoded in relatively few steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7045</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7045</id><created>2013-11-27</created><updated>2014-07-18</updated><authors><author><keyname>Pohl</keyname><forenames>Volker</forenames></author><author><keyname>Yang</keyname><forenames>Fanny</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Phase retrieval from low-rate samples</title><categories>cs.IT math.IT</categories><comments>Preprint accepted for publication in Sampling Theory in Signal and
  Image Processing -- Special issue on SampTa 2013</comments><msc-class>49N45, 90C22, 94A12, 94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers the phase retrieval problem in N-dimensional complex
vector spaces. It provides two sets of deterministic measurement vectors which
guarantee signal recovery for all signals, excluding only a specific subspace
and a union of subspaces, respectively. A stable analytic reconstruction
procedure of low complexity is given. Additionally it is proven that signal
recovery from these measurements can be solved exactly via a semidefinite
program. A practical implementation with 4 deterministic diffraction patterns
is provided and some numerical experiments with noisy measurements complement
the analytic approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7071</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7071</id><created>2013-11-27</created><updated>2013-12-03</updated><authors><author><keyname>Liu</keyname><forenames>Zitao</forenames></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author></authors><title>Sparse Linear Dynamical System with Its Application in Multivariate
  Clinical Time Series</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appear in Neural Information Processing Systems(NIPS) Workshop on
  Machine Learning for Clinical Data Analysis and Healthcare 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Linear Dynamical System (LDS) is an elegant mathematical framework for
modeling and learning multivariate time series. However, in general, it is
difficult to set the dimension of its hidden state space. A small number of
hidden states may not be able to model the complexities of a time series, while
a large number of hidden states can lead to overfitting. In this paper, we
study methods that impose an $\ell_1$ regularization on the transition matrix
of an LDS model to alleviate the problem of choosing the optimal number of
hidden states. We incorporate a generalized gradient descent method into the
Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to
iteratively achieve sparsity on the transition matrix of an LDS model. We show
that our Sparse Linear Dynamical System (SLDS) improves the predictive
performance when compared to ordinary LDS on a multivariate clinical time
series dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7080</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7080</id><created>2013-11-27</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>Cross-Domain Sparse Coding</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding has shown its power as an effective data representation method.
However, up to now, all the sparse coding approaches are limited within the
single domain learning problem. In this paper, we extend the sparse coding to
cross domain learning problem, which tries to learn from a source domain to a
target domain with significant different distribution. We impose the Maximum
Mean Discrepancy (MMD) criterion to reduce the cross-domain distribution
difference of sparse codes, and also regularize the sparse codes by the class
labels of the samples from both domains to increase the discriminative ability.
The encouraging experiment results of the proposed cross-domain sparse coding
algorithm on two challenging tasks --- image classification of photograph and
oil painting domains, and multiple user spam detection --- show the advantage
of the proposed method over other cross-domain data representation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7084</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7084</id><created>2013-11-27</created><updated>2013-12-02</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Wang</keyname><forenames>Carol</forenames></author></authors><title>Explicit rank-metric codes list-decodable with optimal redundancy</title><categories>cs.IT cs.CC cs.DM math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct an explicit family of linear rank-metric codes over any field
${\mathbb F}_h$ that enables efficient list decoding up to a fraction $\rho$ of
errors in the rank metric with a rate of $1-\rho-\epsilon$, for any desired
$\rho \in (0,1)$ and $\epsilon &gt; 0$. Previously, a Monte Carlo construction of
such codes was known, but this is in fact the first explicit construction of
positive rate rank-metric codes for list decoding beyond the unique decoding
radius.
  Our codes are subcodes of the well-known Gabidulin codes, which encode
linearized polynomials of low degree via their values at a collection of
linearly independent points. The subcode is picked by restricting the message
polynomials to an ${\mathbb F}_h$-subspace that evades the structured subspaces
over an extension field ${\mathbb F}_{h^t}$ that arise in the linear-algebraic
list decoder for Gabidulin codes due to Guruswami and Xing (STOC'13). This
subspace is obtained by combining subspace designs contructed by Guruswami and
Kopparty (FOCS'13) with subspace evasive varieties due to Dvir and Lovett
(STOC'12).
  We establish a similar result for subspace codes, which are a collection of
subspaces, every pair of which have low-dimensional intersection, and which
have received much attention recently in the context of network coding. We also
give explicit subcodes of folded Reed-Solomon (RS) codes with small folding
order that are list-decodable (in the Hamming metric) with optimal redundancy,
motivated by the fact that list decoding RS codes reduces to list decoding such
folded RS codes. However, as we only list decode a subcode of these codes, the
Johnson radius continues to be the best known error fraction for list decoding
RS codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7090</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7090</id><created>2013-11-27</created><updated>2014-01-29</updated><authors><author><keyname>Martins</keyname><forenames>Manuel A.</forenames><affiliation>University of Aveiro</affiliation></author><author><keyname>Madeira</keyname><forenames>Alexandre</forenames><affiliation>CCTC, Minho University &amp; Dep. Mathematics, Aveiro University &amp; Critical Software</affiliation></author><author><keyname>Barbosa</keyname><forenames>Luis S.</forenames><affiliation>Dep. Informatics &amp; CCTC, Minho University, Braga, Portugal</affiliation></author></authors><title>The role of logical interpretations in program development</title><categories>cs.LO</categories><proxy>Logical Methods In Computer Science</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (January
  3, 2014) lmcs:706</journal-ref><doi>10.2168/LMCS-10(1:1)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stepwise refinement of algebraic specifications is a well known formal
methodology for program development. However, traditional notions of refinement
based on signature morphisms are often too rigid to capture a number of
relevant transformations in the context of software design, reuse, and
adaptation. This paper proposes a new approach to refinement in which signature
morphisms are replaced by logical interpretations as a means to witness
refinements. The approach is first presented in the context of equational
logic, and later generalised to deductive systems of arbitrary dimension. This
allows, for example, refining sentential into equational specifications and the
latter into modal ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7093</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7093</id><created>2013-11-27</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Habachi</keyname><forenames>Oussama</forenames><affiliation>LIA</affiliation></author><author><keyname>Piunovskiy</keyname><forenames>Alexei</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Infinite Horizon Optimal Impulsive Control Theory with Application to
  Internet Congestion Control</title><categories>cs.NI math.OC</categories><comments>No. RR-8403 (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop Bellman equation based approach for infinite time horizon optimal
impulsive control problems. Both discounted and time average criteria are
considered. We establish very general and at the same time natural conditions
under which a canonical control triplet produces an optimal feedback policy.
Then, we apply our general results for Internet congestion control providing a
convenient setting for the design of active queue management algorithms. In
particular, our general theoretical results suggest a simple threshold-based
active queue management scheme which takes into account the main parameters of
the transmission control protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7105</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7105</id><created>2013-11-27</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Deterministic Approximate Counting for Degree-$2$ Polynomial Threshold
  Functions</title><categories>cs.CC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a {\em deterministic} algorithm for approximately computing the
fraction of Boolean assignments that satisfy a degree-$2$ polynomial threshold
function. Given a degree-2 input polynomial $p(x_1,\dots,x_n)$ and a parameter
$\eps &gt; 0$, the algorithm approximates \[ \Pr_{x \sim \{-1,1\}^n}[p(x) \geq 0]
\] to within an additive $\pm \eps$ in time $\poly(n,2^{\poly(1/\eps)})$. Note
that it is NP-hard to determine whether the above probability is nonzero, so
any sort of multiplicative approximation is almost certainly impossible even
for efficient randomized algorithms. This is the first deterministic algorithm
for this counting problem in which the running time is polynomial in $n$ for
$\eps= o(1)$. For &quot;regular&quot; polynomials $p$ (those in which no individual
variable's influence is large compared to the sum of all $n$ variable
influences) our algorithm runs in $\poly(n,1/\eps)$ time. The algorithm also
runs in $\poly(n,1/\eps)$ time to approximate $\Pr_{x \sim N(0,1)^n}[p(x) \geq
0]$ to within an additive $\pm \eps$, for any degree-2 polynomial $p$.
  As an application of our counting result, we give a deterministic FPT
multiplicative $(1 \pm \eps)$-approximation algorithm to approximate the $k$-th
absolute moment $\E_{x \sim \{-1,1\}^n}[|p(x)^k|]$ of a degree-2 polynomial.
The algorithm's running time is of the form $\poly(n) \cdot f(k,1/\eps)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7113</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7113</id><created>2013-11-27</created><updated>2014-04-20</updated><authors><author><keyname>Buzaglo</keyname><forenames>Sarit</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Systematic Codes for Rank Modulation</title><categories>cs.IT math.IT</categories><comments>to be presented ISIT2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to construct systematic error-correcting codes for
permutations and multi-permutations in the Kendall's $\tau$-metric. These codes
are important in new applications such as rank modulation for flash memories.
The construction is based on error-correcting codes for multi-permutations and
a partition of the set of permutations into error-correcting codes. For a given
large enough number of information symbols $k$, and for any integer $t$, we
present a construction for ${(k+r,k)}$ systematic $t$-error-correcting codes,
for permutations from $S_{k+r}$, with less redundancy symbols than the number
of redundancy symbols in the codes of the known constructions. In particular,
for a given $t$ and for sufficiently large $k$ we can obtain $r=t+1$. The same
construction is also applied to obtain related systematic error-correcting
codes for multi-permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7115</identifier>
 <datestamp>2013-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7115</id><created>2013-11-27</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>Deterministic Approximate Counting for Juntas of Degree-$2$ Polynomial
  Threshold Functions</title><categories>cs.CC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $g: \{-1,1\}^k \to \{-1,1\}$ be any Boolean function and $q_1,\dots,q_k$
be any degree-2 polynomials over $\{-1,1\}^n.$ We give a \emph{deterministic}
algorithm which, given as input explicit descriptions of $g,q_1,\dots,q_k$ and
an accuracy parameter $\eps&gt;0$, approximates \[\Pr_{x \sim
\{-1,1\}^n}[g(\sign(q_1(x)),\dots,\sign(q_k(x)))=1]\] to within an additive
$\pm \eps$. For any constant $\eps &gt; 0$ and $k \geq 1$ the running time of our
algorithm is a fixed polynomial in $n$. This is the first fixed polynomial-time
algorithm that can deterministically approximately count satisfying assignments
of a natural class of depth-3 Boolean circuits.
  Our algorithm extends a recent result \cite{DDS13:deg2count} which gave a
deterministic approximate counting algorithm for a single degree-2 polynomial
threshold function $\sign(q(x)),$ corresponding to the $k=1$ case of our
result.
  Our algorithm and analysis requires several novel technical ingredients that
go significantly beyond the tools required to handle the $k=1$ case in
\cite{DDS13:deg2count}. One of these is a new multidimensional central limit
theorem for degree-2 polynomials in Gaussian random variables which builds on
recent Malliavin-calculus-based results from probability theory. We use this
CLT as the basis of a new decomposition technique for $k$-tuples of degree-2
Gaussian polynomials and thus obtain an efficient deterministic approximate
counting algorithm for the Gaussian distribution. Finally, a third new
ingredient is a &quot;regularity lemma&quot; for \emph{$k$-tuples} of degree-$d$
polynomial threshold functions. This generalizes both the regularity lemmas of
\cite{DSTW:10,HKM:09} and the regularity lemma of Gopalan et al \cite{GOWZ10}.
Our new regularity lemma lets us extend our deterministic approximate counting
results from the Gaussian to the Boolean domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7117</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7117</id><created>2013-11-27</created><updated>2014-12-15</updated><authors><author><keyname>Cavallo</keyname><forenames>Bren</forenames></author><author><keyname>Kahrobaei</keyname><forenames>Delaram</forenames></author></authors><title>Secret Sharing using Non-Commutative Groups and the Shortlex Order</title><categories>math.GR cs.CR</categories><comments>AMS CONM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review the Habeeb-Kahrobaei-Shpilrain secret sharing scheme
and introduce a variation based on the shortlex order on a free group. Drawing
inspiration from adjustments to classical schemes, we also present a method
that allows for the protocol to remain secure after multiple secrets are
shared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7139</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7139</id><created>2013-11-27</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Introduction to Neutrosophic Measure, Neutrosophic Integral, and
  Neutrosophic Probability</title><categories>cs.AI</categories><comments>140 pages. 10 figures</comments><msc-class>28E10</msc-class><journal-ref>Published as a book by Sitech in 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce for the first time the notions of neutrosophic
measure and neutrosophic integral, and we develop the 1995 notion of
neutrosophic probability. We present many practical examples. It is possible to
define the neutrosophic measure and consequently the neutrosophic integral and
neutrosophic probability in many ways, because there are various types of
indeterminacies, depending on the problem we need to solve. Neutrosophics study
the indeterminacy. Indeterminacy is different from randomness. It can be caused
by physical space materials and type of construction, by items involved in the
space, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7178</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7178</id><created>2013-11-27</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco</forenames></author></authors><title>Efficient deterministic approximate counting for low-degree polynomial
  threshold functions</title><categories>cs.CC cs.DS math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic algorithm for approximately counting satisfying
assignments of a degree-$d$ polynomial threshold function (PTF). Given a
degree-$d$ input polynomial $p(x_1,\dots,x_n)$ over $R^n$ and a parameter
$\epsilon&gt; 0$, our algorithm approximates $\Pr_{x \sim \{-1,1\}^n}[p(x) \geq
0]$ to within an additive $\pm \epsilon$ in time $O_{d,\epsilon}(1)\cdot
\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is
impossible even for randomized algorithms assuming $NP\not=RP$.) Note that the
running time of our algorithm (as a function of $n^d$, the number of
coefficients of a degree-$d$ PTF) is a \emph{fixed} polynomial. The fastest
previous algorithm for this problem (due to Kane), based on constructions of
unconditional pseudorandom generators for degree-$d$ PTFs, runs in time
$n^{O_{d,c}(1) \cdot \epsilon^{-c}}$ for all $c &gt; 0$.
  The key novel contributions of this work are: A new multivariate central
limit theorem, proved using tools from Malliavin calculus and Stein's Method.
This new CLT shows that any collection of Gaussian polynomials with small
eigenvalues must have a joint distribution which is very close to a
multidimensional Gaussian distribution. A new decomposition of low-degree
multilinear polynomials over Gaussian inputs. Roughly speaking we show that (up
to some small error) any such polynomial can be decomposed into a bounded
number of multilinear polynomials all of which have extremely small
eigenvalues. We use these new ingredients to give a deterministic algorithm for
a Gaussian-space version of the approximate counting problem, and then employ
standard techniques for working with low-degree PTFs (invariance principles and
regularity lemmas) to reduce the original approximate counting problem over the
Boolean hypercube to the Gaussian version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7181</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7181</id><created>2013-11-27</created><authors><author><keyname>Zhao</keyname><forenames>Jianhua</forenames></author><author><keyname>LI</keyname><forenames>Xuandong</forenames></author></authors><title>Formal Verification of `Programming to Interfaces' Programs</title><categories>cs.LO</categories><comments>40Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a formal approach to specify and verify object-oriented
programs written in the `programming to interfaces' paradigm. Besides the
methods to be invoked by its clients, an interface also declares a set of
abstract function/predicate symbols, together with a set of constraints on
these symbols. For each method declared in this interface, a specification
template is given using these abstract symbols. A class implementing this
interface can give its own definitions to the abstract symbols, as long as all
the constraints are satisfied. This class implements all the methods declared
in the interface such that the method specification templates declared in the
interface are satisfied w.r.t. the definitions of the abstract function symbols
in this class. Based on the constraints on the abstract symbols, the client
code using interfaces can be specified and verified precisely without knowing
what classes implement these interfaces. Given more information about the
implementing classes, the specifications of the client code can be specialized
into more precise ones without re-verifying the client code.
  Several commonly used interfaces and their implementations (including
Iterator, Observer, Comparable, and Comparator) are used to demonstrate that
the approach in this paper is both precise and flexible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7182</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7182</id><created>2013-11-27</created><authors><author><keyname>Heinrich</keyname><forenames>Stuart</forenames></author></authors><title>Public Key Infrastructure based on Authentication of Media Attestments</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many users would prefer the privacy of end-to-end encryption in their online
communications if it can be done without significant inconvenience. However,
because existing key distribution methods cannot be fully trusted enough for
automatic use, key management has remained a user problem. We propose a
fundamentally new approach to the key distribution problem by empowering
end-users with the capacity to independently verify the authenticity of public
keys using an additional media attestment. This permits client software to
automatically lookup public keys from a keyserver without trusting the
keyserver, because any attempted MITM attacks can be detected by end-users.
Thus, our protocol is designed to enable a new breed of messaging clients with
true end-to-end encryption built in, without the hassle of requiring users to
manually manage the public keys, that is verifiably secure against MITM
attacks, and does not require trusting any third parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7183</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7183</id><created>2013-11-27</created><authors><author><keyname>Yang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Wang</keyname><forenames>Hongqiang</forenames></author></authors><title>Knowledge-Aided STAP Using Low Rank and Geometry Properties</title><categories>cs.IT math.IT</categories><comments>16 figures, 12 pages. IEEE Transactions on Aerospace and Electronic
  Systems, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents knowledge-aided space-time adaptive processing (KA-STAP)
algorithms that exploit the low-rank dominant clutter and the array geometry
properties (LRGP) for airborne radar applications. The core idea is to exploit
the fact that the clutter subspace is only determined by the space-time
steering vectors,
  {red}{where the Gram-Schmidt orthogonalization approach is employed to
compute the clutter subspace. Specifically, for a side-looking uniformly spaced
linear array, the} algorithm firstly selects a group of linearly independent
space-time steering vectors using LRGP that can represent the clutter subspace.
By performing the Gram-Schmidt orthogonalization procedure, the orthogonal
bases of the clutter subspace are obtained, followed by two approaches to
compute the STAP filter weights. To overcome the performance degradation caused
by the non-ideal effects, a KA-STAP algorithm that combines the covariance
matrix taper (CMT) is proposed. For practical applications, a reduced-dimension
version of the proposed KA-STAP algorithm is also developed. The simulation
results illustrate the effectiveness of our proposed algorithms, and show that
the proposed algorithms converge rapidly and provide a SINR improvement over
existing methods when using a very small number of snapshots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7184</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7184</id><created>2013-11-27</created><authors><author><keyname>Lee</keyname><forenames>Jason D</forenames></author><author><keyname>Gilad-Bachrach</keyname><forenames>Ran</forenames></author><author><keyname>Caruana</keyname><forenames>Rich</forenames></author></authors><title>Using Multiple Samples to Learn Mixture Models</title><categories>stat.ML cs.LG</categories><comments>Published in Neural Information Processing Systems (NIPS) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the mixture models problem it is assumed that there are $K$ distributions
$\theta_{1},\ldots,\theta_{K}$ and one gets to observe a sample from a mixture
of these distributions with unknown coefficients. The goal is to associate
instances with their generating distributions, or to identify the parameters of
the hidden distributions. In this work we make the assumption that we have
access to several samples drawn from the same $K$ underlying distributions, but
with different mixing weights. As with topic modeling, having multiple samples
is often a reasonable assumption. Instead of pooling the data into one sample,
we prove that it is possible to use the differences between the samples to
better recover the underlying structure. We present algorithms that recover the
underlying structure under milder assumptions than the current state of art
when either the dimensionality or the separation is high. The methods, when
applied to topic modeling, allow generalization to words not present in the
training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7186</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7186</id><created>2013-11-27</created><authors><author><keyname>Jayawardena</keyname><forenames>Srimal</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Brewer</keyname><forenames>Nathan</forenames></author></authors><title>A Novel Illumination-Invariant Loss for Monocular 3D Pose Estimation</title><categories>cs.CV</categories><comments>Digital Image Computing Techniques and Applications (DICTA), 2011
  International Conference on</comments><doi>10.1109/DICTA.2011.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of identifying the 3D pose of a known object from a given 2D
image has important applications in Computer Vision. Our proposed method of
registering a 3D model of a known object on a given 2D photo of the object has
numerous advantages over existing methods. It does not require prior training,
knowledge of the camera parameters, explicit point correspondences or matching
features between the image and model. Unlike techniques that estimate a partial
3D pose (as in an overhead view of traffic or machine parts on a conveyor
belt), our method estimates the complete 3D pose of the object. It works on a
single static image from a given view under varying and unknown lighting
conditions. For this purpose we derive a novel illumination-invariant distance
measure between the 2D photo and projected 3D model, which is then minimised to
find the best pose parameters. Results for vehicle pose detection in real
photographs are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7194</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7194</id><created>2013-11-27</created><authors><author><keyname>Trifonov</keyname><forenames>Dmitry</forenames></author></authors><title>Real-time High Resolution Fusion of Depth Maps on GPU</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system for live high quality surface reconstruction using a single moving
depth camera on a commodity hardware is presented. High accuracy and real-time
frame rate is achieved by utilizing graphics hardware computing capabilities
via OpenCL and by using sparse data structure for volumetric surface
representation. Depth sensor pose is estimated by combining serial texture
registration algorithm with iterative closest points algorithm (ICP) aligning
obtained depth map to the estimated scene model. Aligned surface is then fused
into the scene. Kalman filter is used to improve fusion quality. Truncated
signed distance function (TSDF) stored as block-based sparse buffer is used to
represent surface. Use of sparse data structure greatly increases accuracy of
scanned surfaces and maximum scanning area. Traditional GPU implementation of
volumetric rendering and fusion algorithms were modified to exploit sparsity to
achieve desired performance. Incorporation of texture registration for sensor
pose estimation and Kalman filter for measurement integration improved accuracy
and robustness of scanning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7198</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7198</id><created>2013-11-27</created><authors><author><keyname>Mohan</keyname><forenames>Karthik</forenames></author></authors><title>ADMM Algorithm for Graphical Lasso with an $\ell_{\infty}$ Element-wise
  Norm Constraint</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Graphical lasso with an additional $\ell_{\infty}$
element-wise norm constraint on the precision matrix. This problem has
applications in high-dimensional covariance decomposition such as in
\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. We
also use a continuation strategy on the penalty parameter to have a fast
implemenation of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7200</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7200</id><created>2013-11-27</created><authors><author><keyname>Chakraborty</keyname><forenames>Ayan</forenames></author><author><keyname>Munshi</keyname><forenames>Shiladitya</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Searching and Establishment of S-P-O Relationships for Linked RDF Graphs
  : An Adaptive Approach</title><categories>cs.IR cs.DB</categories><comments>5 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1107.1104 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the coming era of semantic web linked data analysis is a very burning
issue for efficient searching and retrieval of information. One way of
establishing this link is to implement subject predicate object relationship
through Set Theory approach which is already done in our previous work. For
analyzing inter relationship between two RDF Graphs, RDF- Schema (RDFS) should
also be taken care of. In the present paper, an adaptive combination rule based
framework has been proposed for establishment of S P O relationship and RDF
Graph searching is reported. Hence the identification of criteria for
inter-relationship of RDF Graphs opens up new road in semantic search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7201</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7201</id><created>2013-11-27</created><authors><author><keyname>Munshi</keyname><forenames>Shiladitya</forenames></author><author><keyname>Chakraborty</keyname><forenames>Ayan</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Theories of Hypergraph-Graph (HG(2)) Data Structure</title><categories>cs.DS</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current paper introduces a Hypergraph Graph model of data storage which can
be represented as a hybrid data structure based on Hypergraph and Graph. The
pro-posed data structure is claimed to realize complex combinatorial
structures. The formal definition of the data structure is presented along with
the proper justification from real world scenarios. The paper reports some
elementary concepts of Hypergraph and presents theoretical aspects of the
proposed data structure including the concepts of Path, Cycle etc. The detailed
analysis of weighted HG 2 is presented along with discussions on Cost involved
with HG 2 paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7202</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7202</id><created>2013-11-27</created><authors><author><keyname>Munshi</keyname><forenames>Shiladitya</forenames></author><author><keyname>Chakraborty</keyname><forenames>Ayan</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Integrating RDF into Hypergraph-Graph (HG(2)) Data Structure</title><categories>cs.DS</categories><comments>5 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current paper discusses the methodologies involved in integrating Resource
Description Framework into a HyperGraph Graph HG 2 data structure in order to
preserve the semantics of the information contained in RDF document for dealing
future cross platform information portability issues. The entire semantic web
is mostly dominated by few information frameworks like RDF, Topic Map, OWL etc.
Hence semantic web currently faces the problem of non existence of common
information meta-model which can integrate them all for ex-panded semantic
search. On the background of development of Hyper Graph Graph HG 2 data
structure, an RDF document if integrated to it, maintains the original
semantics and exposes some critical semantic and object mapping lift as well
which could further be exploited for semantic search and information
transitional problems. The focus of the paper is to present the mapping
constructs between RDF elements and HyperGraph Graph HG 2 elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7203</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7203</id><created>2013-11-27</created><authors><author><keyname>Kahanwal</keyname><forenames>Dr. Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Dr. T. P.</forenames></author></authors><title>Function Overloading Implementation in C++</title><categories>cs.PL</categories><comments>7 pages, 9 Figures</comments><journal-ref>International Journal of Advances in Engineering Sciences (e-ISSN:
  2231-0347, ISSN: 2231-2013), Volume 3, Issue 4, pp. 12-18, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article the function overloading in object-oriented programming is
elaborated and how they are implemented in C++. The language supports a variety
of programming styles. Here we are describing the polymorphism and its types in
brief. The main stress is given on the function overloading implementation
styles in the language. The polymorphic nature of languages has advantages like
that we can add new code without requiring changes to the other classes and
interfaces (in Java language) are easily implemented. In this technique, the
run-time overhead is also introduced in dynamic binding which increases the
execution time of the software. But there are no such types of overheads in
this static type of polymorphism because everything is resolved at the time of
compile time. Polymorphism; Function Overloading; Static Polymorphism;
Overloading; Compile Time Polymorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7204</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7204</id><created>2013-11-27</created><authors><author><keyname>Wanaskar</keyname><forenames>Ujwala</forenames></author><author><keyname>Vij</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>A Hybrid Web Recommendation System based on the Improved Association
  Rule Mining Algorithm</title><categories>cs.IR</categories><comments>9 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the growing interest of web recommendation systems those are applied to
deliver customized data for their users, we started working on this system.
Generally the recommendation systems are divided into two major categories such
as collaborative recommendation system and content based recommendation system.
In case of collaborative recommen-dation systems, these try to seek out users
who share same tastes that of given user as well as recommends the websites
according to the liking given user. Whereas the content based recommendation
systems tries to recommend web sites similar to those web sites the user has
liked. In the recent research we found that the efficient technique based on
asso-ciation rule mining algorithm is proposed in order to solve the problem of
web page recommendation. Major problem of the same is that the web pages are
given equal importance. Here the importance of pages changes according to the
fre-quency of visiting the web page as well as amount of time user spends on
that page. Also recommendation of newly added web pages or the pages those are
not yet visited by users are not included in the recommendation set. To
over-come this problem, we have used the web usage log in the adaptive
association rule based web mining where the asso-ciation rules were applied to
personalization. This algorithm was purely based on the Apriori data mining
algorithm in order to generate the association rules. However this method also
suffers from some unavoidable drawbacks. In this paper we are presenting and
investigating the new approach based on weighted Association Rule Mining
Algorithm and text mining. This is improved algorithm which adds semantic
knowledge to the results, has more efficiency and hence gives better quality
and performances as compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7210</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7210</id><created>2013-11-28</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Chathly</keyname><forenames>Falguni J.</forenames></author><author><keyname>Jadhav</keyname><forenames>Nagesh N.</forenames></author></authors><title>QoS Based Framework for Effective Web Services in Cloud Computing</title><categories>cs.DC</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enhancements in technology always follow Consumer requirements. Consumer
requires best of service with least possible mismatch and on time. Numerous
applications available today are based on Web Services and Cloud Computing.
Recently, there exist many Web Services with similar functional
characteristics. Choosing a right Service from group of similar Web Service is
a complicated task for Service Consumer. In that case, Service Consumer can
discover the required Web Service using non functional attributes of the Web
Services such as QoS. Proposed layered architecture and Web Service Cloud
i.e.WS Cloud computing Framework synthesizes the Non functional attributes that
includes reliability, availability, response time, latency etc. The Service
Consumer is projected to provide the QoS requirements as part of Service
discovery query. This framework will discover and filter the Web Services form
the cloud and rank them according to Service Consumer preferences to facilitate
Service on time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7213</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7213</id><created>2013-11-28</created><authors><author><keyname>Soleimani-Pouri</keyname><forenames>Mohammad</forenames></author><author><keyname>Rezvanian</keyname><forenames>Alireza</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Finding a Maximum Clique using Ant Colony Optimization and Particle
  Swarm Optimization in Social Networks</title><categories>cs.SI cs.NE</categories><comments>4 pages, 3 figures, conference</comments><doi>10.1109/ASONAM.2012.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction between users in online social networks plays a key role in
social network analysis. One on important types of social group is full
connected relation between some users, which known as clique structure.
Therefore finding a maximum clique is essential for some analysis. In this
paper, we proposed a new method using ant colony optimization algorithm and
particle swarm optimization algorithm. In the proposed method, in order to
attain better results, it is improved process of pheromone update by particle
swarm optimization. Simulation results on popular standard social network
benchmarks in comparison standard ant colony optimization algorithm are shown a
relative enhancement of proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7215</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7215</id><created>2013-11-28</created><authors><author><keyname>Mousavian</keyname><forenames>Aylin</forenames></author><author><keyname>Rezvanian</keyname><forenames>Alireza</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Solving Minimum Vertex Cover Problem Using Learning Automata</title><categories>cs.AI cs.DM</categories><comments>5 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum vertex cover problem is an NP-Hard problem with the aim of finding
minimum number of vertices to cover graph. In this paper, a learning automaton
based algorithm is proposed to find minimum vertex cover in graph. In the
proposed algorithm, each vertex of graph is equipped with a learning automaton
that has two actions in the candidate or non-candidate of the corresponding
vertex cover set. Due to characteristics of learning automata, this algorithm
significantly reduces the number of covering vertices of graph. The proposed
algorithm based on learning automata iteratively minimize the candidate vertex
cover through the update its action probability. As the proposed algorithm
proceeds, a candidate solution nears to optimal solution of the minimum vertex
cover problem. In order to evaluate the proposed algorithm, several experiments
conducted on DIMACS dataset which compared to conventional methods.
Experimental results show the major superiority of the proposed algorithm over
the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7219</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7219</id><created>2013-11-28</created><updated>2014-06-12</updated><authors><author><keyname>Makwana</keyname><forenames>Trupti M. Kodinariya Dr. Prashant R.</forenames></author></authors><title>Partitioning Clustering algorithms for handling numerical and
  categorical data: a review</title><categories>cs.DB</categories><comments>paper has been withdrawn by the author due to a crucial error in
  equation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is widely used in different field such as biology, psychology, and
economics. Most traditional clustering algorithms are limited to handling
datasets that contain either numeric or categorical attributes. However,
datasets with mixed types of attributes are common in real life data mining
applications. In this paper, we review partitioning based algorithm such as
K-prototype, Extension of K-prototype, K-histogram, Fuzzy approaches, genetic
approaches, etc. These algorithm works on both numerical and categorical data.
The approaches has been proposed to handle mixed data are based on four
different perceptive: i) split data set into two part such that each part
contain either numerical or categorical data, then apply separate clustering
algorithm on each data set, finally combined the result of both clustering
algorithm, ii) converting categorical attribute into numerical attribute and
apply numerical attribute clustering algorithm; iii) discrimination of
numerical attribute and apply categorical based clustering algorithm; iv)
Conversion of the categorical attributes into binary ones and apply any
numerical based clustering algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7225</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7225</id><created>2013-11-28</created><updated>2016-02-01</updated><authors><author><keyname>Tseng</keyname><forenames>Chun-Kai</forenames></author><author><keyname>Wu</keyname><forenames>Sau-Hsuan</forenames></author></authors><title>Link Quality Control Mechanism for Selective and Opportunistic AF
  Relaying in Cooperative ARQs: A MLSD Perspective</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors due to an improper proof
  for Theorem 2. To avoid a misleading understanding, we thus decide to
  withdraw this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating relaying techniques into Automatic Repeat reQuest (ARQ)
mechanisms gives a general impression of diversity and throughput enhancements.
Allowing overhearing among multiple relays is also a known approach to increase
the number of participating relays in ARQs. However, when opportunistic
amplify-and-forward (AF) relaying is applied to cooperative ARQs, the system
design becomes nontrivial and even involved. Based on outage analysis, the
spatial and temporal diversities are first found sensitive to the received
signal qualities of relays, and a link quality control mechanism is then
developed to prescreen candidate relays in order to explore the diversity of
cooperative ARQs with a selective and opportunistic AF (SOAF) relaying method.
According to the analysis, the temporal and spatial diversities can be fully
exploited if proper thresholds are set for each hop along the relaying routes.
The SOAF relaying method is further examined from a packet delivery viewpoint.
By the principle of the maximum likelihood sequence detection (MLSD),
sufficient conditions on the link quality are established for the proposed
SOAF-relaying-based ARQ scheme to attain its potential diversity order in the
packet error rates (PERs) of MLSD. The conditions depend on the minimum
codeword distance and the average signal-to-noise ratio (SNR). Furthermore,
from a heuristic viewpoint, we also develop a threshold searching algorithm for
the proposed SOAF relaying and link quality method to exploit both the
diversity and the SNR gains in PER. The effectiveness of the proposed
thresholding mechanism is verified via simulations with trellis codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7229</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7229</id><created>2013-11-28</created><authors><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author><author><keyname>Swierkot</keyname><forenames>Kamil</forenames></author></authors><title>Hierarchies</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity theory for the local distributed setting introduced
by Korman, Peleg and Fraigniaud. They have defined three complexity classes LD
(Local Decision), NLD (Nondeterministic Local Decision) and NLD^#n. The class
LD consists of all languages which can be decided with a constant number of
communication rounds. The class NLD consists of all languages which can be
verified by a nondeterministic algorithm with a constant number of
communication rounds. In order to define the nondeterministic classes, they
have transferred the notation of nondeterminism into the distributed setting by
the use of certificates and verifiers. The class NLD^#n consists of all
languages which can be verified by a nondeterministic algorithm where each node
has access to an oracle for the number of nodes. They have shown the hierarchy
LD subset NLD subset NLD^#n.
  Our main contributions are strict hierarchies within the classes defined by
Korman, Peleg and Fraigniaud. We define additional complexity classes: the
class LD(t) consists of all languages which can be decided with at most t
communication rounds. The class NLD-O(f) consists of all languages which can be
verified by a local verifier such that the size of the certificates that are
needed to verify the language are bounded by a function from O(f). Our main
results are refined strict hierarchies within these nondeterministic classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7235</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7235</id><created>2013-11-28</created><authors><author><keyname>Antonanzas-Torres</keyname><forenames>F.</forenames></author><author><keyname>de Pis&#xf3;n</keyname><forenames>F. J. Mart&#xed;nez</forenames></author><author><keyname>Antonanzas</keyname><forenames>J.</forenames></author><author><keyname>Perpi&#xf1;&#xe1;n</keyname><forenames>O.</forenames></author></authors><title>Downscaling of global solar irradiation in R</title><categories>physics.ao-ph cs.CE</categories><comments>23 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A methodology for downscaling solar irradiation from satellite-derived
databases is described using R software. Different packages such as raster,
parallel, solaR, gstat, sp and rasterVis are considered in this study for
improving solar resource estimation in areas with complex topography, in which
downscaling is a very useful tool for reducing inherent deviations in
satellite-derived irradiation databases, which lack of high global spatial
resolution. A topographical analysis of horizon blocking and sky-view is
developed with a digital elevation model to determine what fraction of hourly
solar irradiation reaches the Earth's surface. Eventually, kriging with
external drift is applied for a better estimation of solar irradiation
throughout the region analyzed. This methodology has been implemented as an
example within the region of La Rioja in northern Spain, and the mean absolute
error found is a striking 25.5% lower than with the original database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7236</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7236</id><created>2013-11-28</created><updated>2014-04-09</updated><authors><author><keyname>Schnoor</keyname><forenames>Henning</forenames></author><author><keyname>Woizekowski</keyname><forenames>Oliver</forenames></author></authors><title>Active Linkability Attacks</title><categories>cs.CR</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study linking attacks on communication protocols. We show that an active
attacker is strictly more powerful in this setting than previously-considered
passive attackers. We introduce a formal model to reason about active
linkability attacks, formally define security against these attacks and give
very general conditions for both security and insecurity of protocols. In
addition, we introduce a composition-like technique that allows to obtain
security proofs by only studying small components of a protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7237</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7237</id><created>2013-11-28</created><authors><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Beamforming for MISO Interference Channels with QoS and RF Energy
  Transfer</title><categories>cs.IT math.IT</categories><comments>Under Review at IEEE Transactions on Wireless Communications.
  Original Submission Date: 04/07/2013. Revision 1 Submission Date: 09/10/2013</comments><journal-ref>S. Timotheou, I. Krikidis, G. Zheng, and B. Ottersten,
  &quot;Beamforming for MISO Interference Channels with QoS and RF Energy Transfer,&quot;
  IEEE Transactions on Wireless Communications, vol.13, no.5, pp.2646--2658,
  May 2014</journal-ref><doi>10.1109/TWC.2014.032514.131199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiuser multiple-input single-output interference channel
where the receivers are characterized by both quality-of-service (QoS) and
radio-frequency (RF) energy harvesting (EH) constraints. We consider the power
splitting RF-EH technique where each receiver divides the received signal into
two parts a) for information decoding and b) for battery charging. The minimum
required power that supports both the QoS and the RF-EH constraints is
formulated as an optimization problem that incorporates the transmitted power
and the beamforming design at each transmitter as well as the power splitting
ratio at each receiver. We consider both the cases of fixed beamforming and
when the beamforming design is incorporated into the optimization problem. For
fixed beamforming we study three standard beamforming schemes, the zero-forcing
(ZF), the regularized zero-forcing (RZF) and the maximum ratio transmission
(MRT); a hybrid scheme, MRT-ZF, comprised of a linear combination of MRT and ZF
beamforming is also examined. The optimal solution for ZF beamforming is
derived in closed-form, while optimization algorithms based on second-order
cone programming are developed for MRT, RZF and MRT-ZF beamforming to solve the
problem. In addition, the joint-optimization of beamforming and power
allocation is studied using semidefinite programming (SDP) with the aid of rank
relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7242</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7242</id><created>2013-11-28</created><authors><author><keyname>Protzenko</keyname><forenames>Jonathan</forenames></author><author><keyname>Pottier</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Programming with Permissions in Mezzo</title><categories>cs.PL</categories><acm-class>D.3.2</acm-class><journal-ref>ICFP 2013, Proceedings of the 18th ACM SIGPLAN international
  conference on Functional programming</journal-ref><doi>10.1145/2500365.2500598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Mezzo, a typed programming language of ML lineage. Mezzo is
equipped with a novel static discipline of duplicable and affine permissions,
which controls aliasing and ownership. This rules out certain mistakes,
including representation exposure and data races, and enables new idioms, such
as gradual initialization, memory re-use, and (type)state changes. Although the
core static discipline disallows sharing a mutable data structure, Mezzo offers
several ways of working around this restriction, including a novel dynamic
ownership control mechanism which we dub &quot;adoption and abandon&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7245</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7245</id><created>2013-11-28</created><updated>2014-06-13</updated><authors><author><keyname>Papadopoulos</keyname><forenames>A.</forenames></author><author><keyname>Georgiadis</keyname><forenames>L.</forenames></author></authors><title>Multiuser Broadcast Erasure Channel with Feedback and Side Information,
  and Related Index Coding Results</title><categories>cs.IT math.IT</categories><comments>In the replacement version we modified slightly the title and the
  abstract of the original submission, in order to reflect better the content
  of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the N-user broadcast erasure channel with public feedback and
side information. Before the beginning of transmission, each receiver knows a
function of the messages of some of the other receivers. This situation arises
naturally in wireless and in particular cognitive networks where a node may
overhear transmitted messages destined to other nodes before transmission over
a given broadcast channel begins. We provide an upper bound to the capacity
region of this system. Furthermore, when the side information is linear, we
show that the bound is tight for the case of two-user broadcast channels. The
special case where each user knows the whole or nothing of the message of each
other node, constitutes a generalization of the index coding problem. For this
instance, and when there are no channel errors, we show that the bound reduces
to the known Maximum Weighted Acyclic Induced Subgraph bound. We also show how
to convert the capacity upper bound to transmission completion rate (broadcast
rate) lower bound and provide examples of codes for certain information graphs
for which the bound is either achieved of closely approximated
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7251</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7251</id><created>2013-11-28</created><authors><author><keyname>Shtok</keyname><forenames>Joseph</forenames></author><author><keyname>Zibulevsky</keyname><forenames>Michael</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Spatially-Adaptive Reconstruction in Computed Tomography using Neural
  Networks</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a supervised machine learning approach for boosting existing
signal and image recovery methods and demonstrate its efficacy on example of
image reconstruction in computed tomography. Our technique is based on a local
nonlinear fusion of several image estimates, all obtained by applying a chosen
reconstruction algorithm with different values of its control parameters.
Usually such output images have different bias/variance trade-off. The fusion
of the images is performed by feed-forward neural network trained on a set of
known examples. Numerical experiments show an improvement in reconstruction
quality relatively to existing direct and iterative reconstruction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7256</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7256</id><created>2013-11-28</created><authors><author><keyname>Gu&#xe9;neau</keyname><forenames>Arma&#xeb;l</forenames></author><author><keyname>Pottier</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Protzenko</keyname><forenames>Jonathan</forenames></author></authors><title>The ins and outs of iteration in Mezzo</title><categories>cs.PL</categories><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a talk proposal for HOPE 2013. Using iteration over a collection as a
case study, we wish to illustrate the strengths and weaknesses of the prototype
programming language Mezzo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7259</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7259</id><created>2013-11-28</created><authors><author><keyname>Argyriou</keyname><forenames>Evmorfia N.</forenames></author><author><keyname>Symvonis</keyname><forenames>Antonios</forenames></author><author><keyname>Vassiliou</keyname><forenames>Vassilis</forenames></author></authors><title>A Fraud Detection Visualization System Utilizing Radial Drawings and
  Heat-maps</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a prototype system developed in cooperation with a business
organization that combines information visualization and pattern-matching
techniques to detect fraudulent activity by employees. The system is built upon
common fraud patterns searched while trying to detect occupational fraud
suggested by internal auditors of a business company. The main visualization of
the system consists of a multi-layer radial drawing that represents the
activity of the employees and clients. Each layer represents a different
examined pattern whereas heat-maps indicating suspicious activity are
incorporated in the visualization. The data are first preprocessed based on a
decision tree generated by the examined patterns and each employee is assigned
a value indicating whether or not there exist indications of fraud. The
visualization is presented as an animation and the employees are visualized one
by one according to their severity values together with their related clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7278</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7278</id><created>2013-11-28</created><updated>2015-01-20</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Linear list-approximation for short programs (or the power of a few
  random bits)</title><categories>cs.CC</categories><msc-class>68Q17, 68Q30</msc-class><acm-class>F.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $c$-short program for a string $x$ is a description of $x$ of length at
most $C(x) + c$, where $C(x)$ is the Kolmogorov complexity of $x$. We show that
there exists a randomized algorithm that constructs a list of $n$ elements that
contains a $O(\log n)$-short program for $x$. We also show a polynomial-time
randomized construction that achieves the same list size for $O(\log^2
n)$-short programs. These results beat the lower bounds shown by Bauwens et al.
\cite{bmvz:c:shortlist} for deterministic constructions of such lists. We also
prove tight lower bounds for the main parameters of our result. The
constructions use only $O(\log n)$ ($O(\log^2 n)$ for the polynomial-time
result) random bits . Thus using only few random bits it is possible to do
tasks that cannot be done by any deterministic algorithm regardless of its
running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7283</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7283</id><created>2013-11-28</created><updated>2014-12-05</updated><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Topology of the view complex</title><categories>cs.DC math.CO</categories><comments>accepted for publication in Homotopy, Homology Appl</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a family of simplicial complexes, which we call the
view complexes. Our choice of objects of study is motivated by theoretical
distributed computing, since the view complex is a key simplicial construction
used for protocol complexes in the snapshot computational model. We show that
the view complex $\view$ can be collapsed to the well-known complex
$\chi(\Delta^n)$, called standard chromatic subdivision of a simplex, and that
$\chi(\Delta^n)$ is itself collapsible. Furthermore, we show that the collapses
can be performed simultaneously in entire orbits of the natural symmetric group
action. Our results yield a purely combinatorial and constructive understanding
of the topology of view complexes, at the same time as they enhance our
knowledge about the standard chromatic subdivision of a simplex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7289</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7289</id><created>2013-11-28</created><updated>2014-03-05</updated><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Weak symmetry breaking and abstract simplex paths</title><categories>cs.DC math.CO</categories><comments>revised version, 30 pages To appear in Mathematical Structures in
  Computer Science</comments><journal-ref>Math. Struct. in Comp. Science 25 (2015) 1432-1462</journal-ref><doi>10.1017/S0960129514000085</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by questions in theoretical distributed computing, we develop the
combinatorial theory of abstract simplex path subdivisions. Our main
application is a short and structural proof of the theorem of Castaneda and
Rajsbaum. This theorem in turn implies the solvability of the weak symmetry
breaking task in the immediate snapshot wait-free model in the case when the
number of processes is not a power of a prime number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7295</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7295</id><created>2013-11-28</created><authors><author><keyname>Aragon-Camarasa</keyname><forenames>Gerardo</forenames></author><author><keyname>Oehler</keyname><forenames>Susanne B.</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Li</keyname><forenames>Sun</forenames></author><author><keyname>Cockshott</keyname><forenames>Paul</forenames></author><author><keyname>Siebert</keyname><forenames>J. Paul</forenames></author></authors><title>Glasgow's Stereo Image Database of Garments</title><categories>cs.RO cs.CV</categories><comments>7 pages, 6 figure, image database</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To provide insight into cloth perception and manipulation with an active
binocular robotic vision system, we compiled a database of 80 stereo-pair
colour images with corresponding horizontal and vertical disparity maps and
mask annotations, for 3D garment point cloud rendering has been created and
released. The stereo-image garment database is part of research conducted under
the EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs to
a wider database collection released through CloPeMa (www.clopema.eu). This
database is based on 16 different off-the-shelve garments. Each garment has
been imaged in five different pose configurations on the project's binocular
robot head. A full copy of the database is made available for scientific
research only at https://sites.google.com/site/ugstereodatabase/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7298</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7298</id><created>2013-11-28</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>List decoding - random coding exponents and expurgated exponents</title><categories>cs.IT math.IT</categories><comments>28 pages; submitted to the IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some new results are derived concerning random coding error exponents and
expurgated exponents for list decoding with a deterministic list size $L$. Two
asymptotic regimes are considered, the fixed list-size regime, where $L$ is
fixed independently of the block length $n$, and the exponential list-size,
where $L$ grows exponentially with $n$. We first derive a general upper bound
on the list-decoding average error probability, which is suitable for both
regimes. This bound leads to more specific bounds in the two regimes. In the
fixed list-size regime, the bound is related to known bounds and we establish
its exponential tightness. In the exponential list-size regime, we establish
the achievability of the well known sphere packing lower bound. Relations to
guessing exponents are also provided. An immediate byproduct of our analysis in
both regimes is the universality of the maximum mutual information (MMI) list
decoder in the error exponent sense. Finally, we consider expurgated bounds at
low rates, both using Gallager's approach and the Csisz\'ar-K\&quot;orner-Marton
approach, which is, in general better (at least for $L=1$). The latter
expurgated bound, which involves the notion of {\it multi-information}, is also
modified to apply to continuous alphabet channels, and in particular, to the
Gaussian memoryless channel, where the expression of the expurgated bound
becomes quite explicit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7302</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7302</id><created>2013-11-28</created><updated>2015-11-08</updated><authors><author><keyname>Tsilimantos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Jaffr&#xe8;s-Runser</keyname><forenames>Katia</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Spectral and Energy Efficiency Trade-Offs in Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple and effective method to study the spectral and
energy efficiency (SE-EE) trade-off in cellular networks, an issue that has
attracted significant recent interest in the wireless community. The proposed
theoretical framework is based on an optimal radio resource allocation of
transmit power and bandwidth for the downlink direction, applicable for an
orthogonal cellular network. The analysis is initially focused on a single cell
scenario, for which in addition to the solution of the main SE-EE optimization
problem, it is proved that a traffic repartition scheme can also be adopted as
a way to simplify this approach. By exploiting this interesting result along
with properties of stochastic geometry, this work is extended to a more
challenging multi-cell environment, where interference is shown to play an
essential role and for this reason several interference reduction techniques
are investigated. Special attention is also given to the case of low signal to
noise ratio (SNR) and a way to evaluate the upper bound on EE in this regime is
provided. This methodology leads to tractable analytical results under certain
common channel properties, and thus allows the study of various models without
the need for demanding system-level simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7307</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7307</id><created>2013-11-28</created><updated>2014-10-28</updated><authors><author><keyname>Boneva</keyname><forenames>Iovka</forenames></author><author><keyname>Ciucanu</keyname><forenames>Radu</forenames></author><author><keyname>Staworko</keyname><forenames>S&#x142;awek</forenames></author></authors><title>Schemas for Unordered XML on a DIME</title><categories>cs.DB</categories><comments>Theory of Computing Systems</comments><doi>10.1007/s00224-014-9593-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate schema languages for unordered XML having no relative order
among siblings. First, we propose unordered regular expressions (UREs),
essentially regular expressions with unordered concatenation instead of
standard concatenation, that define languages of unordered words to model the
allowed content of a node (i.e., collections of the labels of children).
However, unrestricted UREs are computationally too expensive as we show the
intractability of two fundamental decision problems for UREs: membership of an
unordered word to the language of a URE and containment of two UREs.
Consequently, we propose a practical and tractable restriction of UREs,
disjunctive interval multiplicity expressions (DIMEs).
  Next, we employ DIMEs to define languages of unordered trees and propose two
schema languages: disjunctive interval multiplicity schema (DIMS), and its
restriction, disjunction-free interval multiplicity schema (IMS). We study the
complexity of the following static analysis problems: schema satisfiability,
membership of a tree to the language of a schema, schema containment, as well
as twig query satisfiability, implication, and containment in the presence of
schema. Finally, we study the expressive power of the proposed schema languages
and compare them with yardstick languages of unordered trees (FO, MSO, and
Presburger constraints) and DTDs under commutative closure. Our results show
that the proposed schema languages are capable of expressing many practical
languages of unordered trees and enjoy desirable computational properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7313</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7313</id><created>2013-11-28</created><updated>2013-12-05</updated><authors><author><keyname>Haslinger</keyname><forenames>Evelyn Nicole</forenames></author><author><keyname>Lopez-Herrejon</keyname><forenames>Roberto E.</forenames></author><author><keyname>Egyed</keyname><forenames>Alexander</forenames></author></authors><title>Improving CASA Runtime Performance by Exploiting Basic Feature Model
  Analysis</title><categories>cs.SE</categories><comments>This new version corrects the acknowledgements removes some license
  information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Software Product Line Engineering (SPLE) families of systems are designed,
rather than developing the individual systems independently. Combinatorial
Interaction Testing has proven to be effective for testing in the context of
SPLE, where a representative subset of products is chosen for testing in place
of the complete family. Such a subset of products can be determined by
computing a so called t-wise Covering Array (tCA), whose computation is
NP-complete. Recently, reduction rules that exploit basic feature model
analysis have been proposed that reduce the number of elements that need to be
considered during the computation of tCAs for Software Product Lines (SPLs). We
applied these rules to CASA, a simulated annealing algorithm for tCA generation
for SPLs. We evaluated the adapted version of CASA using 133 publicly available
feature models and could record on average a speedup of $61.8\%$ of median
execution time, while at the same time preserving the coverage of the generated
array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7318</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7318</id><created>2013-11-28</created><updated>2015-06-30</updated><authors><author><keyname>Chithrabhanu</keyname><forenames>P.</forenames></author><author><keyname>Aadhi</keyname><forenames>A.</forenames></author><author><keyname>Reddy</keyname><forenames>Salla Gangi</forenames></author><author><keyname>Prabhakar</keyname><forenames>Shashi</forenames></author><author><keyname>Samanta</keyname><forenames>G. K.</forenames></author><author><keyname>Paul</keyname><forenames>Goutam</forenames></author><author><keyname>Singh</keyname><forenames>R. P.</forenames></author></authors><title>Three particle Hyper Entanglement: Teleportation and Quantum Key
  Distribution</title><categories>quant-ph cs.CR</categories><comments>8 pages, 8 figures, 2 tables, To appear in Quantum Information
  Processing Journal (accepted on June 22, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scheme to generate three particle hyper-entanglement utilizing
polarization and orbital angular momentum (OAM) of a photon. We show that the
generated state can be used to teleport a two-qubit state described by the
polarization and the OAM. The proposed quantum system has also been used to
describe a new efficient quantum key distribution (QKD) protocol. We give a
sketch of the experimental arrangement to realize the proposed teleportation
and the QKD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7327</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7327</id><created>2013-11-28</created><authors><author><keyname>Petridis</keyname><forenames>Sergios</forenames></author><author><keyname>Giannakopoulos</keyname><forenames>Theodoros</forenames></author><author><keyname>Spyropoulos</keyname><forenames>Costantine D.</forenames></author></authors><title>Unobtrusive Low Cost Pupil Size Measurements using Web cameras</title><categories>cs.CV</categories><comments>2nd International Workshop on Artificial Intelligence and Netmedicine
  (NetMed'13), pages 9-20</comments><acm-class>J.3; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unobtrusive every day health monitoring can be of important use for the
elderly population. In particular, pupil size may be a valuable source of
information, since, apart from pathological cases, it can reveal the emotional
state, the fatigue and the ageing. To allow for unobtrusive monitoring to gain
acceptance, one should seek for efficient methods of monitoring using com- mon
low-cost hardware. This paper describes a method for monitoring pupil sizes
using a common web camera in real time. Our method works by first detecting the
face and the eyes area. Subsequently, optimal iris and sclera location and
radius, modelled as ellipses, are found using efficient filtering. Finally, the
pupil center and radius is estimated by optimal filtering within the area of
the iris. Experimental result show both the efficiency and the effectiveness of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7357</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7357</id><created>2013-11-28</created><updated>2014-12-13</updated><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Kamali</keyname><forenames>Shahin</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author></authors><title>On the List Update Problem with Advice</title><categories>cs.DS</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the online list update problem under the advice model of
computation. Under this model, an online algorithm receives partial information
about the unknown parts of the input in the form of some bits of advice
generated by a benevolent offline oracle. We show that advice of linear size is
required and sufficient for a deterministic algorithm to achieve an optimal
solution or even a competitive ratio better than 15/14. On the other hand, we
show that surprisingly two bits of advice is sufficient to break the lower
bound of 2 on the competitive ratio of deterministic online algorithms and
achieve a deterministic algorithm with a competitive ratio of 5/3. In this
upper-bound argument, the bits of advice determine the algorithm with smaller
cost among three classical online algorithms, TIMESTAMP and two members of the
MTF2 family of algorithms. We also show that MTF2 algorithms are
2.5-competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7359</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7359</id><created>2013-11-28</created><authors><author><keyname>Kloos</keyname><forenames>Tobias</forenames></author><author><keyname>St&#xf6;ckler</keyname><forenames>Joachim</forenames></author></authors><title>Zak transforms and Gabor frames of totally positive functions and
  exponential B-splines</title><categories>cs.IT math.IT math.NA</categories><msc-class>42C15, 41A15, 42C40</msc-class><doi>10.1016/j.jat.2014.05.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study totally positive (TP) functions of finite type and exponential
B-splines as window functions for Gabor frames. We establish the connection of
the Zak transform of these two classes of functions and prove that the Zak
transforms have only one zero in their fundamental domain of quasi-periodicity.
Our proof is based on the variation-diminishing property of shifts of
exponential B-splines. For the exponential B-spline B_m of order m, we
determine a large set of lattice parameters a,b&gt;0 such that the Gabor family of
time-frequency shifts is a frame for L^2(R). By the connection of its Zak
transform to the Zak transform of TP functions of finite type, our result
provides an alternative proof that TP functions of finite type provide Gabor
frames for all lattice parameters with ab&lt;1. For even two-sided exponentials
and the related exponential B-spline of order 2, we find lower frame-bounds A,
which show the asymptotically linear decay A (1-ab) as the density ab of the
time-frequency lattice tends to the critical density ab=1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7369</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7369</id><created>2013-11-28</created><authors><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author><author><keyname>Sedjelmaci</keyname><forenames>Sidi Mohamed</forenames><affiliation>LIPN</affiliation></author></authors><title>Worst-Case Analysis of Weber's Algorithm</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>11 pages</comments><proxy>ccsd</proxy><journal-ref>Information Processing Letters 72, 3-4 (1999) 125-130</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Ken Weber introduced an algorithm for finding the $(a,b)$-pairs
satisfying $au+bv\equiv 0\pmod{k}$, with $0&lt;|a|,|b|&lt;\sqrt{k}$, where $(u,k)$
and $(v,k)$ are coprime. It is based on Sorenson's and Jebelean's &quot;$k$-ary
reduction&quot; algorithms. We provide a formula for $N(k)$, the maximal number of
iterations in the loop of Weber's GCD algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7373</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7373</id><created>2013-11-28</created><authors><author><keyname>Fanaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Schmid</keyname><forenames>Natalia A.</forenames></author></authors><title>Limited-Feedback-Based Channel-Aware Power Allocation for Linear
  Distributed Estimation</title><categories>cs.IT math.IT</categories><comments>5 Pages, 3 Figures, 1 Algorithm, Forty Seventh Annual Asilomar
  Conference on Signals, Systems, and Computers (ASILOMAR 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of distributed best linear unbiased
estimation (BLUE) of a random parameter at the fusion center (FC) of a wireless
sensor network (WSN). In particular, the application of limited-feedback
strategies for the optimal power allocation in distributed estimation is
studied. In order to find the BLUE estimator of the unknown parameter, the FC
combines spatially distributed, linearly processed, noisy observations of local
sensors received through orthogonal channels corrupted by fading and additive
Gaussian noise. Most optimal power-allocation schemes proposed in the
literature require the feedback of the exact instantaneous channel state
information from the FC to local sensors. This paper proposes a
limited-feedback strategy in which the FC designs an optimal codebook
containing the optimal power-allocation vectors, in an iterative offline
process, based on the generalized Lloyd algorithm with modified distortion
functions. Upon observing a realization of the channel vector, the FC finds the
closest codeword to its corresponding optimal power-allocation vector and
broadcasts the index of the codeword. Each sensor will then transmit its analog
observations using its optimal quantized amplification gain. This approach
eliminates the requirement for infinite-rate digital feedback links and is
scalable, especially in large WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7385</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7385</id><created>2013-11-28</created><updated>2014-07-11</updated><authors><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam, NL</affiliation></author><author><keyname>Chater</keyname><forenames>Nick</forenames><affiliation>University of Warwick, UK</affiliation></author></authors><title>Algorithmic Identification of Probabilities</title><categories>cs.LG</categories><comments>19 pages LaTeX.Corrected errors and rewrote the entire paper. arXiv
  admin note: text overlap with arXiv:1208.5003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TThe problem is to identify a probability associated with a set of natural
numbers, given an infinite data sequence of elements from the set. If the given
sequence is drawn i.i.d. and the probability mass function involved (the
target) belongs to a computably enumerable (c.e.) or co-computably enumerable
(co-c.e.) set of computable probability mass functions, then there is an
algorithm to almost surely identify the target in the limit. The technical tool
is the strong law of large numbers. If the set is finite and the elements of
the sequence are dependent while the sequence is typical in the sense of
Martin-L\&quot;of for at least one measure belonging to a c.e. or co-c.e. set of
computable measures, then there is an algorithm to identify in the limit a
computable measure for which the sequence is typical (there may be more than
one such measure). The technical tool is the theory of Kolmogorov complexity.
We give the algorithms and consider the associated predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7388</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7388</id><created>2013-11-28</created><authors><author><keyname>Siddiqui</keyname><forenames>Ahmad Tasnim</forenames></author><author><keyname>Aljahdali</keyname><forenames>Sultan</forenames></author></authors><title>Web Mining Techniques in E-Commerce Applications</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1208.1926 by other authors</comments><journal-ref>International Journal of Computer Applications, Volume 69 No.8,
  May 2013</journal-ref><doi>10.5120/11864-7648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today web is the best medium of communication in modern business. Many
companies are redefining their business strategies to improve the business
output. Business over internet provides the opportunity to customers and
partners where their products and specific business can be found. Nowadays
online business breaks the barrier of time and space as compared to the
physical office. Big companies around the world are realizing that e-commerce
is not just buying and selling over Internet, rather it improves the efficiency
to compete with other giants in the market. For this purpose data mining
sometimes called as knowledge discovery is used. Web mining is data mining
technique that is applied to the WWW. There are vast quantities of information
available over the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7400</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7400</id><created>2013-11-28</created><authors><author><keyname>Moawad</keyname><forenames>Hafez</forenames></author><author><keyname>Shaaban</keyname><forenames>Eman</forenames></author><author><keyname>Fayed</keyname><forenames>Zaki Taha</forenames></author></authors><title>Stop_times based Routing Protocol for VANET</title><categories>cs.NI</categories><comments>6 pages, 4 figure, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 81(18):4-9,
  November 2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/14221-2029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Network (VANET) is a special class of Mobile Ad hoc Network
(MANET) where vehicles are considered as MANET nodes with wireless links. The
key difference of VANET and MANET is the special mobility pattern and rapidly
changeable topology. There has been significant interest in improving safety
and traffic efficiency using VANET. The design of routing protocols in VANET is
important and necessary issue for support the smart ITS. Existing routing
protocols of MANET are not suitable for VANET. AOMDV is the most important on
demand multipath routing protocol. This paper proposes SSD-AOMDV as VANET
routing protocol. SSD-AOMDV improves AOMDV to suit VANET characteristics.
SSD-AOMDV adds the mobility parameters: Stop_times, Speed and Direction to hop
count as new AOMDV routing metric to select next hop during the route discovery
phase. Stop_times metric is added to simulate buses mobility pattern and
traffic lights at intersections. Simulation results show that SSD-AOMDV
achieves better performance compared to AOMDV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7401</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7401</id><created>2013-11-28</created><authors><author><keyname>Didden</keyname><forenames>Eva-Maria</forenames></author><author><keyname>Thorarinsdottir</keyname><forenames>Thordis Linda</forenames></author><author><keyname>Lenkoski</keyname><forenames>Alex</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Shape from Texture using Locally Scaled Point Processes</title><categories>stat.AP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape from texture refers to the extraction of 3D information from 2D images
with irregular texture. This paper introduces a statistical framework to learn
shape from texture where convex texture elements in a 2D image are represented
through a point process. In a first step, the 2D image is preprocessed to
generate a probability map corresponding to an estimate of the unnormalized
intensity of the latent point process underlying the texture elements. The
latent point process is subsequently inferred from the probability map in a
non-parametric, model free manner. Finally, the 3D information is extracted
from the point pattern by applying a locally scaled point process model where
the local scaling function represents the deformation caused by the projection
of a 3D surface onto a 2D image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7403</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7403</id><created>2013-11-28</created><authors><author><keyname>Forsyth</keyname><forenames>Michael</forenames></author><author><keyname>Jayakumar</keyname><forenames>Amlesh</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Remarks on Privileged Words</title><categories>cs.FL cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the notion of privileged word, recently introduced by Peltomaki. A
word w is privileged if it is of length &lt;=1, or has a privileged border that
occurs exactly twice in w. We prove the following results: (1) if w^k is
privileged for some k &gt;=1, then w^j is privileged for all j &gt;= 0; (2) the
language of privileged words is neither regular nor context-free; (3) there is
a linear-time algorithm to check if a given word is privileged; and (4) there
are at least 2^{n-5}/n^2 privileged binary words of length n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7407</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7407</id><created>2013-11-28</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Hastad</keyname><forenames>Johan</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Srinivasan</keyname><forenames>Srikanth</forenames></author><author><keyname>Varma</keyname><forenames>Girish</forenames></author></authors><title>Super-polylogarithmic hypergraph coloring hardness via low-degree long
  codes</title><categories>cs.CC</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove improved inapproximability results for hypergraph coloring using the
low-degree polynomial code (aka, the 'short code' of Barak et. al. [FOCS 2012])
and the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate
this code for inapproximability results. In particular, we prove
quasi-NP-hardness of the following problems on $n$-vertex hyper-graphs:
  * Coloring a 2-colorable 8-uniform hypergraph with
$2^{2^{\Omega(\sqrt{\log\log n})}}$ colors.
  * Coloring a 4-colorable 4-uniform hypergraph with
$2^{2^{\Omega(\sqrt{\log\log n})}}$ colors.
  * Coloring a 3-colorable 3-uniform hypergraph with $(\log
n)^{\Omega(1/\log\log\log n)}$ colors.
  In each of these cases, the hardness results obtained are (at least)
exponentially stronger than what was previously known for the respective cases.
In fact, prior to this result, polylog n colors was the strongest quantitative
bound on the number of colors ruled out by inapproximability results for
O(1)-colorable hypergraphs.
  The fundamental bottleneck in obtaining coloring inapproximability results
using the low- degree long code was a multipartite structural restriction in
the PCP construction of Dinur-Guruswami. We are able to get around this
restriction by simulating the multipartite structure implicitly by querying
just one partition (albeit requiring 8 queries), which yields our result for
2-colorable 8-uniform hypergraphs. The result for 4-colorable 4-uniform
hypergraphs is obtained via a 'query doubling' method. For 3-colorable
3-uniform hypergraphs, we exploit the ternary domain to design a test with an
additive (as opposed to multiplicative) noise function, and analyze its
efficacy in killing high weight Fourier coefficients via the pseudorandom
properties of an associated quadratic form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7421</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7421</id><created>2013-11-28</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Wong</keyname><forenames>Walter</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>In-Network Caching vs. Redundancy Elimination</title><categories>cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-level Redundancy Elimination (RE) techniques have been proposed to
reduce the amount of traffic in the Internet. and the costs of the WAN access
in the Internet. RE middleboxes are usually placed in the network access
gateways and strip off the repeated data from the packets. More recently,
generic network-level caching architectures have been proposed as alternative
to reduce the redundant data traffic in the network, presenting benefits and
drawbacks compared to RE. In this paper, we compare a generic in-network
caching architecture against state-of-the-art redundancy elimination (RE)
solutions on real network topologies, presenting the advantages of each
technique. Our results show that in-network caching architectures outperform
state-of-the-art RE solutions across a wide range of traffic characteristics
and parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7422</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7422</id><created>2013-11-28</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>LiteLab: Efficient Large-scale Network Experiments</title><categories>cs.DC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale network experiments is a challenging problem. Simulations,
emulations, and real-world testbeds all have their advantages and
disadvantages. In this paper we present LiteLab, a light-weight platform
specialized for large-scale networking experiments. We cover in detail its
design, key features, and architecture. We also perform an extensive evaluation
of LiteLab's performance and accuracy and show that it is able to both simulate
network parameters with high accuracy, and also able to scale up to very large
networks. LiteLab is flexible, easy to deploy, and allows researchers to
perform large-scale network experiments with a short development cycle. We have
used LiteLab for many different kinds of network experiments and are planning
to make it available for others to use as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7430</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7430</id><created>2013-11-28</created><authors><author><keyname>Spurek</keyname><forenames>P.</forenames></author><author><keyname>Chaikouskaya</keyname><forenames>A.</forenames></author><author><keyname>Tabor</keyname><forenames>J.</forenames></author><author><keyname>Zaj&#x105;c</keyname><forenames>E.</forenames></author></authors><title>A local Gaussian filter and adaptive morphology as tools for completing
  partially discontinuous curves</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for extraction and analysis of curve--type
structures which consist of disconnected components. Such structures are found
in electron--microscopy (EM) images of metal nanograins, which are widely used
in the field of nanosensor technology.
  The topography of metal nanograins in compound nanomaterials is crucial to
nanosensor characteristics. The method of completing such templates consists of
three steps. In the first step, a local Gaussian filter is used with different
weights for each neighborhood. In the second step, an adaptive morphology
operation is applied to detect the endpoints of curve segments and connect
them. In the last step, pruning is employed to extract a curve which optimally
fits the template.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7434</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7434</id><created>2013-11-28</created><updated>2015-04-26</updated><authors><author><keyname>Hernandez</keyname><forenames>Joshua</forenames></author><author><keyname>Tsotsos</keyname><forenames>Konstantine</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Observability, Identifiability and Sensitivity of Vision-Aided
  Navigation</title><categories>cs.RO</categories><report-no>Technical Report UCLA CSD TR130022, Aug. 20, 2013 (Revised Nov. 12,
  2013 and May 10, 2014)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the observability of motion estimates from the fusion of visual
and inertial sensors. Because the model contains unknown parameters, such as
sensor biases, the problem is usually cast as a mixed identification/filtering,
and the resulting observability analysis provides a necessary condition for any
algorithm to converge to a unique point estimate. Unfortunately, most models
treat sensor bias rates as noise, independent of other states including biases
themselves, an assumption that is patently violated in practice. When this
assumption is lifted, the resulting model is not observable, and therefore past
analyses cannot be used to conclude that the set of states that are
indistinguishable from the measurements is a singleton. In other words, the
resulting model is not observable. We therefore re-cast the analysis as one of
sensitivity: Rather than attempting to prove that the indistinguishable set is
a singleton, which is not the case, we derive bounds on its volume, as a
function of characteristics of the input and its sufficient excitation. This
provides an explicit characterization of the indistinguishable set that can be
used for analysis and validation purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7435</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7435</id><created>2013-11-28</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>Experimenting with BitTorrent on a Cluster: A Good or a Bad Idea?</title><categories>cs.DC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of large-scale network systems and applications is usually done in
one of three ways: simulations, real deployment on Internet, or on an emulated
network testbed such as a cluster. Simulations can study very large systems but
often abstract out many practical details, whereas real world tests are often
quite small, on the order of a few hundred nodes at most, but have very
realistic conditions. Clusters and other dedicated testbeds offer a middle
ground between the two: large systems with real application code. They also
typically allow configuring the testbed to enable repeatable experiments. In
this paper we explore how to run large BitTorrent experiments in a cluster
setup. We have chosen BitTorrent because the source code is available and it
has been a popular target for research. Our contribution is twofold. First, we
show how to tweak and configure the BitTorrent client to allow for a maximum
number of clients to be run on a single machine, without running into any
physical limits of the machine. Second, our results show that the behavior of
BitTorrent can be very sensitive to the configuration and we revisit some
existing BitTorrent research and consider the implications of our findings on
previously published results. As we show in this paper, BitTorrent can change
its behavior in subtle ways which are sometimes ignored in published works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7442</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7442</id><created>2013-11-28</created><updated>2013-12-12</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author><author><keyname>Harel</keyname><forenames>Jonathan</forenames></author></authors><title>Irreducibility is Minimum Synergy Among Parts</title><categories>cs.IT math.IT</categories><comments>15 pages, 6 figures. Only minor changes from v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For readers already familiar with Partial Information Decomposition (PID), we
show that PID's definition of synergy enables quantifying at least four
different notions of irreducibility. First, we show four common notions of
&quot;parts&quot; give rise to a spectrum of four distinct measures of irreducibility.
Second, we introduce a nonnegative expression based on PID for each notion of
irreducibility. Third, we delineate these four notions of irreducibility with
exemplary binary circuits. This work will become more useful once the
complexity community has converged on a palatable $\operatorname{I}_{\cap}$ or
$\operatorname{I}_{\cup}$ measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7449</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7449</id><created>2013-11-28</created><authors><author><keyname>Bradonji&#x107;</keyname><forenames>Milan</forenames></author><author><keyname>Saniee</keyname><forenames>Iraj</forenames></author></authors><title>Bootstrap Percolation on Periodic Trees</title><categories>math.PR cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study bootstrap percolation with the threshold parameter $\theta \geq 2$
and the initial probability $p$ on infinite periodic trees that are defined as
follows. Each node of a tree has degree selected from a finite predefined set
of non-negative integers and starting from any node, all nodes at the same
graph distance from it have the same degree. We show the existence of the
critical threshold $p_f(\theta) \in (0,1)$ such that with high probability, (i)
if $p &gt; p_f(\theta)$ then the periodic tree becomes fully active, while (ii) if
$p &lt; p_f(\theta)$ then a periodic tree does not become fully active. We also
derive a system of recurrence equations for the critical threshold
$p_f(\theta)$ and compute these numerically for a collection of periodic trees
and various values of $\theta$, thus extending previous results for regular
(homogeneous) trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7458</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7458</id><created>2013-11-28</created><authors><author><keyname>Pudasaini</keyname><forenames>Subodh</forenames></author><author><keyname>Shin</keyname><forenames>Seokjoo</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Optimum Tag Reading Efficiency of Multi-Packet Reception Capable RFID
  Readers</title><categories>cs.NI</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximizing the tag reading rate of a reader is one of the most important
design objectives in RFID systems as the tag reading rate is inversely
proportional to the time required to completely read all the tags within the
reader's radio field. To this end, numerous techniques have been independently
suggested so far and they can be broadly categorized into pure advancements in
the link-layer tag anti-collision protocols and pure advancements in the
physical-layer RF transmission/reception model. In this paper, we show by
rigorous mathematical analysis and Monte Carlo simulations that how such two
independent approaches can be coupled to attain the optimum tag reading
efficiency in a RFID system considering a Dynamic Frame Slotted Aloha based
link layer anti-collision protocol at tags and a Multi-Packet Reception capable
RF reception model at the reader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7462</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7462</id><created>2013-11-28</created><authors><author><keyname>Choi</keyname><forenames>Yi-King</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author><author><keyname>Mourrain</keyname><forenames>Bernard</forenames></author><author><keyname>Tu</keyname><forenames>Changhe</forenames></author><author><keyname>Jia</keyname><forenames>Xiaohong</forenames></author><author><keyname>Sun</keyname><forenames>Feng</forenames></author></authors><title>Continuous Collision Detection for Composite Quadric Models</title><categories>cs.GR</categories><comments>23 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A composite quadric model (CQM) is an object modeled by piecewise linear or
quadric patches. We study the continuous detection problem of a special type of
CQM objects which are commonly used in CAD/CAM, that is, the boundary surfaces
of such a CQM intersect only in straight line segments or conic curve segments.
We present a framework for continuous collision detection (CCD) of this special
type of CQM (which we also call CQM for brevity) in motion. We derive algebraic
formulations and compute numerically the first contact time instants and the
contact points of two moving CQMs in $\mathbb R^3$. Since it is difficult to
process CCD of two CQMs in a direct manner because they are composed of
semi-algebraic varieties, we break down the problem into subproblems of solving
CCD of pairs of boundary elements of the CQMs. We present procedures to solve
CCD of different types of boundary element pairs in different dimensions. Some
CCD problems are reduced to their equivalents in a lower dimensional setting,
where they can be solved more efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7466</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7466</id><created>2013-11-28</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Linear Network Error Correction Multicast/Broadcast/Dispersion/Generic
  Codes</title><categories>cs.IT math.IT</categories><comments>Single column, 38 pages. Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the practical network communications, many internal nodes in the network
are required to not only transmit messages but decode source messages. For
different applications, four important classes of linear network codes in
network coding theory, i.e., linear multicast, linear broadcast, linear
dispersion, and generic network codes, have been studied extensively. More
generally, when channels of communication networks are noisy, information
transmission and error correction have to be under consideration
simultaneously, and thus these four classes of linear network codes are
generalized to linear network error correction (LNEC) coding, and we say them
LNEC multicast, broadcast, dispersion, and generic codes, respectively.
Furthermore, in order to characterize their efficiency of information
transmission and error correction, we propose the (weakly, strongly) extended
Singleton bounds for them, and define the corresponding optimal codes, i.e.,
LNEC multicast/broadcast/dispersion/generic MDS codes, which satisfy the
corresponding Singleton bounds with equality. The existences of such MDS codes
are discussed in detail by algebraic methods and the constructive algorithms
are also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7477</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7477</id><created>2013-11-29</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author><author><keyname>Wiesenberger</keyname><forenames>Matthias</forenames></author></authors><title>A conservative discontinuous Galerkin scheme for the 2D incompressible
  Navier--Stokes equations</title><categories>physics.comp-ph cs.NA math.NA physics.flu-dyn physics.plasm-ph</categories><doi>10.1016/j.cpc.2014.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a conservative discretization of the
two-dimensional incompressible Navier--Stokes equations. We propose an
extension of Arakawa's classical finite difference scheme for fluid flow in the
vorticity-stream function formulation to a high order discontinuous Galerkin
approximation. In addition, we show numerical simulations that demonstrate the
accuracy of the scheme and verify the conservation properties, which are
essential for long time integration. Furthermore, we discuss the massively
parallel implementation on graphic processing units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7518</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7518</id><created>2013-11-29</created><authors><author><keyname>Wang</keyname><forenames>Jianping</forenames></author><author><keyname>Zhang</keyname><forenames>Ke</forenames></author><author><keyname>Du</keyname><forenames>Xianyu</forenames></author><author><keyname>Zhen</keyname><forenames>He</forenames></author><author><keyname>Yan</keyname><forenames>Jing</forenames></author></authors><title>Power Penalty Due to First-order PMD in Optical OFDM/QAM and FBMC/OQAM
  Transmission System</title><categories>cs.IT math.IT</categories><comments>10 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Polarization mode dispersion (PMD) is a challenge for high-data-rate
optical-communication systems. More researches are desirable for impairments
that is induced by PMD in high-speed optical orthogonal frequency division
multiplexing (OFDM) transmission system. In this paper, an approximately
analytical method for evaluating the power penalty due to first-order PMD in
optical OFDM with quadrature amplitude modulation (OFDM/QAM) and filter bank
based multi-carrier with offset quadrature amplitude modulation (FBMC/OQAM)
transmission system is presented. The simulation results show that, compared
with the single carrier with quadrature phase shift keying(SC-QPSK), both the
OFDM/QAM and the FBMC/OQAM can decrease the power penalty caused by PMD by
half. Furthermore, the FBMC/OQAM shows better power penalty immunity than the
OFDM/QAM under the influence of first order PMD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7523</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7523</id><created>2013-11-29</created><authors><author><keyname>Kambites</keyname><forenames>Mark</forenames></author><author><keyname>Kazda</keyname><forenames>Alexandr</forenames></author></authors><title>The word problem for free adequate semigroups</title><categories>math.RA cs.CC math.GR</categories><comments>12 pages</comments><msc-class>20M05 (primary), 08A50 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of computation in finitely generated free left, right
and two-sided adequate semigroups and monoids. We present polynomial time
(quadratic in the RAM model of computation) algorithms to solve the word
problem and compute normal forms in each of these, and hence also to test
whether any given identity holds in the classes of left, right and/or two-sided
adequate semigroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7535</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7535</id><created>2013-11-29</created><authors><author><keyname>Burghard</keyname><forenames>Oliver</forenames></author><author><keyname>Berner</keyname><forenames>Alexander</forenames></author><author><keyname>Wand</keyname><forenames>Michael</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy</forenames></author><author><keyname>Seidel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Klein</keyname><forenames>Reinhard</forenames></author></authors><title>Compact Part-Based Shape Spaces for Dense Correspondences</title><categories>cs.GR</categories><acm-class>I.3.5; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of establishing dense correspondences within a set of
related shapes of strongly varying geometry. For such input, traditional shape
matching approaches often produce unsatisfactory results. We propose an
ensemble optimization method that improves given coarse correspondences to
obtain dense correspondences. Following ideas from minimum description length
approaches, it maximizes the compactness of the induced shape space to obtain
high-quality correspondences. We make a number of improvements that are
important for computer graphics applications: Our approach handles meshes of
general topology and handles partial matching between input of varying
topology. To this end we introduce a novel part-based generative statistical
shape model. We develop a novel analysis algorithm that learns such models from
training shapes of varying topology. We also provide a novel synthesis method
that can generate new instances with varying part layouts and subject to
generic variational constraints. In practical experiments, we obtain a
substantial improvement in correspondence quality over state-of-the-art
methods. As example application, we demonstrate a system that learns shape
families as assemblies of deformable parts and permits real-time editing with
continuous and discrete variability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7536</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7536</id><created>2013-11-29</created><authors><author><keyname>Van Gorp</keyname><forenames>Pieter</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Rose</keyname><forenames>Louis M.</forenames><affiliation>University of York</affiliation></author><author><keyname>Krause</keyname><forenames>Christian</forenames><affiliation>SAP</affiliation></author></authors><title>Proceedings Sixth Transformation Tool Contest</title><categories>cs.SE cs.PL</categories><proxy>EPTCS</proxy><acm-class>F.4.3;I.2.8;D.3.3</acm-class><journal-ref>EPTCS 135, 2013</journal-ref><doi>10.4204/EPTCS.135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the Transformation Tool Contest (TTC) series is to compare the
expressiveness, the usability and the performance of graph and model
transformation tools along a number of selected case studies. Participants want
to learn about the pros and cons of each tool considering different
applications. A deeper understanding of the relative merits of different tool
features will help to further improve graph and model transformation tools and
to indicate open problems.
  TTC 2013 involved 18 offline case study solutions: 6 solutions to the
FlowGraphs case, 9 solutions to the Petri Nets to Statecharts case and 3
solutions to the Restructuring case. 13 of the 18 solutions have undergone a
non-blind peer review before the workshop and were presented and evaluated
during the workshop in Budapest. This volume contains the submissions that have
passed an additional (post-workshop, blind) reviewing round.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7562</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7562</id><created>2013-11-29</created><authors><author><keyname>B&#xfc;rger</keyname><forenames>Mathias</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Dynamic coupling design for nonlinear output agreement and time-varying
  flow control</title><categories>cs.SY</categories><comments>submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of output agreement in networks of nonlinear
dynamical systems under time-varying disturbances, using dynamic diffusive
couplings. Necessary conditions are derived for general networks of nonlinear
systems, and these conditions are explicitly interpreted as conditions relating
the node dynamics and the network topology. For the class of incrementally
passive systems, necessary and sufficient conditions for output agreement are
derived. The approach proposed in the paper lends itself to solve flow control
problems in distribution networks. As a first case study, the internal model
approach is used for designing a controller that achieves an optimal routing
and inventory balancing in a dynamic transportation network with storage and
time-varying supply and demand. It is in particular shown that the time-varying
optimal routing problem can be solved by applying an internal model controller
to the dual variables of a certain convex network optimization problem. As a
second case study, we show that droop-controllers in microgrids have also an
interpretation as internal model controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7584</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7584</id><created>2013-11-29</created><updated>2014-04-13</updated><authors><author><keyname>Data</keyname><forenames>Deepesh</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Manoj M.</forenames></author></authors><title>On the Communication Complexity of Secure Computation</title><categories>cs.CR cs.IT math.IT</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theoretically secure multi-party computation (MPC) is a central
primitive of modern cryptography. However, relatively little is known about the
communication complexity of this primitive.
  In this work, we develop powerful information theoretic tools to prove lower
bounds on the communication complexity of MPC. We restrict ourselves to a
3-party setting in order to bring out the power of these tools without
introducing too many complications. Our techniques include the use of a data
processing inequality for residual information - i.e., the gap between mutual
information and G\'acs-K\&quot;orner common information, a new information
inequality for 3-party protocols, and the idea of distribution switching by
which lower bounds computed under certain worst-case scenarios can be shown to
apply for the general case.
  Using these techniques we obtain tight bounds on communication complexity by
MPC protocols for various interesting functions. In particular, we show
concrete functions that have &quot;communication-ideal&quot; protocols, which achieve the
minimum communication simultaneously on all links in the network. Also, we
obtain the first explicit example of a function that incurs a higher
communication cost than the input length in the secure computation model of
Feige, Kilian and Naor (1994), who had shown that such functions exist. We also
show that our communication bounds imply tight lower bounds on the amount of
randomness required by MPC protocols for many interesting functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7589</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7589</id><created>2013-11-29</created><updated>2015-08-05</updated><authors><author><keyname>Renault</keyname><forenames>Marc P.</forenames></author><author><keyname>Ros&#xe9;n</keyname><forenames>Adi</forenames></author><author><keyname>van Stee</keyname><forenames>Rob</forenames></author></authors><title>Online Algorithms with Advice for Bin Packing and Scheduling Problems</title><categories>cs.DS</categories><comments>20 pages</comments><doi>10.1016/j.tcs.2015.07.050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting of online computation with advice, and study the bin
packing problem and a number of scheduling problems. We show that it is
possible, for any of these problems, to arbitrarily approach a competitive
ratio of $1$ with only a constant number of bits of advice per request. For the
bin packing problem, we give an online algorithm with advice that is
$(1+\varepsilon)$-competitive and uses $O\left(\frac{1}{\varepsilon}\log
\frac{1}{\varepsilon} \right)$ bits of advice per request. For scheduling on
$m$ identical machines, with the objective function of any of makespan, machine
covering and the minimization of the $\ell_p$ norm, $p &gt;1$, we give similar
results. We give online algorithms with advice which are
$(1+\varepsilon)$-competitive ($(1/(1-\varepsilon))$-competitive for machine
covering) and also use $O\left(\frac{1}{\varepsilon}\log \frac{1}{\varepsilon}
\right)$ bits of advice per request. We complement our results by giving a
lower bound showing that for any online algorithm with advice to be optimal,
for any of the above scheduling problems, a non-constant number (namely, at
least $\left(1 - \frac{2m}{n}\right)\log m$, where $n$ is the number of jobs
and $m$ is the number of machines) of bits of advice per request is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7590</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7590</id><created>2013-11-29</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Universal Polar Decoding with Channel Knowledge at the Encoder</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar coding over a class of binary discrete memoryless channels with channel
knowledge at the encoder is studied. It is shown that polar codes achieve the
capacity of convex and one-sided classes of symmetric channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7631</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7631</id><created>2013-11-29</created><updated>2014-09-12</updated><authors><author><keyname>Diaz</keyname><forenames>Josep</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author></authors><title>Absorption Time of the Moran Process</title><categories>cs.DM cs.DS math.PR</categories><comments>minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Moran process models the spread of mutations in populations on graphs. We
investigate the absorption time of the process, which is the time taken for a
mutation introduced at a randomly chosen vertex to either spread to the whole
population, or to become extinct. It is known that the expected absorption time
for an advantageous mutation is O(n^4) on an n-vertex undirected graph, which
allows the behaviour of the process on undirected graphs to be analysed using
the Markov chain Monte Carlo method. We show that this does not extend to
directed graphs by exhibiting an infinite family of directed graphs for which
the expected absorption time is exponential in the number of vertices. However,
for regular directed graphs, we show that the expected absorption time is
Omega(n log n) and O(n^2). We exhibit families of graphs matching these bounds
and give improved bounds for other families of graphs, based on isoperimetric
number. Our results are obtained via stochastic dominations which we
demonstrate by establishing a coupling in a related continuous-time model. The
coupling also implies several natural domination results regarding the fixation
probability of the original (discrete-time) process, resolving a conjecture of
Shakarian, Roos and Johnson.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7635</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7635</id><created>2013-11-25</created><updated>2014-01-10</updated><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author></authors><title>Concurrent bisimulation algorithm</title><categories>cs.LO cs.DC</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coarsest bisimulation-finding problem plays an important role in the
formal analysis of concurrent systems. For example, solving this problem allows
the behavior of different processes to be compared or specifications to be
verified. Hence, in this paper an efficient concurrent bisimulation algorithm
is presented. It is based on the sequential Paige and Tarjan algorithm and the
concept of the state signatures. The original solution follows Hopcroft's
principle &quot;process the smaller half&quot;. The presented algorithm uses its
generalized version &quot;process all but the largest one&quot; better suited for
concurrent and parallel applications. The running time achieved is comparable
with the best known sequential and concurrent solutions. At the end of the
work, the results of tests carried out are presented. The question of the lower
bound for the running time of the optimal algorithm is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7656</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7656</id><created>2013-11-29</created><authors><author><keyname>Langovoy</keyname><forenames>Mikhail</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Statistical estimation for optimization problems on graphs</title><categories>stat.ML cs.DM math.OC stat.CO stat.ME</categories><comments>Paper for the NIPS Workshop on Discrete Optimization for Machine
  Learning (DISCML) (2011): Uncertainty, Generalization and Feedback</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large graphs abound in machine learning, data mining, and several related
areas. A useful step towards analyzing such graphs is that of obtaining certain
summary statistics - e.g., or the expected length of a shortest path between
two nodes, or the expected weight of a minimum spanning tree of the graph, etc.
These statistics provide insight into the structure of a graph, and they can
help predict global properties of a graph. Motivated thus, we propose to study
statistical properties of structured subgraphs (of a given graph), in
particular, to estimate the expected objective function value of a
combinatorial optimization problem over these subgraphs. The general task is
very difficult, if not unsolvable; so for concreteness we describe a more
specific statistical estimation problem based on spanning trees. We hope that
our position paper encourages others to also study other types of graphical
structures for which one can prove nontrivial statistical estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7662</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7662</id><created>2013-11-29</created><authors><author><keyname>Neyshabur</keyname><forenames>Behnam</forenames></author><author><keyname>Yadollahpour</keyname><forenames>Payman</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>The Power of Asymmetry in Binary Hashing</title><categories>cs.LG cs.CV cs.IR</categories><comments>Accepted to NIPS 2013, 9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When approximating binary similarity using the hamming distance between short
binary hashes, we show that even if the similarity is symmetric, we can have
shorter and more accurate hashes by using two distinct code maps. I.e. by
approximating the similarity between $x$ and $x'$ as the hamming distance
between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as
the hamming distance between $f(x)$ and $f(x')$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7676</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7676</id><created>2013-11-29</created><updated>2014-02-07</updated><authors><author><keyname>Gao</keyname><forenames>Song</forenames></author><author><keyname>Li</keyname><forenames>Linna</forenames></author><author><keyname>Li</keyname><forenames>Wenwen</forenames></author><author><keyname>Janowicz</keyname><forenames>Krzysztof</forenames></author><author><keyname>Zhang</keyname><forenames>Yue</forenames></author></authors><title>Constructing Gazetteers from Volunteered Big Geo-Data Based on Hadoop</title><categories>cs.DC</categories><comments>45 pages, 10 figures</comments><acm-class>H.2.4; H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional gazetteers are built and maintained by authoritative mapping
agencies. In the age of Big Data, it is possible to construct gazetteers in a
data-driven approach by mining rich volunteered geographic information (VGI)
from the Web. In this research, we build a scalable distributed platform and a
high-performance geoprocessing workflow based on the Hadoop ecosystem to
harvest crowd-sourced gazetteer entries. Using experiments based on geotagged
datasets in Flickr, we find that the MapReduce-based workflow running on the
spatially enabled Hadoop cluster can reduce the processing time compared with
traditional desktop-based operations by an order of magnitude. We demonstrate
how to use such a novel spatial-computing infrastructure to facilitate
gazetteer research. In addition, we introduce a provenance-based trust model
for quality assurance. This work offers new insights on enriching future
gazetteers with the use of Hadoop clusters, and makes contributions in
connecting GIS to the cloud computing environment for the next frontier of Big
Geo-Data analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7679</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7679</id><created>2013-11-29</created><authors><author><keyname>Liu</keyname><forenames>Xudong</forenames></author><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Zhang</keyname><forenames>Yuyu</forenames></author><author><keyname>Yan</keyname><forenames>Qiang</forenames></author><author><keyname>Pang</keyname><forenames>Liang</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Sun</keyname><forenames>Hanxiao</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author></authors><title>Combination of Diverse Ranking Models for Personalized Expedia Hotel
  Searches</title><categories>cs.LG</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ICDM Challenge 2013 is to apply machine learning to the problem of hotel
ranking, aiming to maximize purchases according to given hotel characteristics,
location attractiveness of hotels, user's aggregated purchase history and
competitive online travel agency information for each potential hotel choice.
This paper describes the solution of team &quot;binghsu &amp; MLRush &amp; BrickMover&quot;. We
conduct simple feature engineering work and train different models by each
individual team member. Afterwards, we use listwise ensemble method to combine
each model's output. Besides describing effective model and features, we will
discuss about the lessons we learned while using deep learning in this
competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7683</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7683</id><created>2013-11-29</created><updated>2016-02-01</updated><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames></author></authors><title>Robust Equilibria in Concurrent Games</title><categories>cs.GT</categories><comments>32 pages To appear in FoSSaCS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding robust equilibria in multiplayer concurrent
games with mean payoff objectives. A $(k,t)$-robust equilibrium is a strategy
profile such that no coalition of size $k$ can improve the payoff of one its
member by deviating, and no coalition of size $t$ can decrease the payoff of
other players. We are interested in pure equilibria, that is, solutions that
can be implemented using non-randomized strategies. We suggest a general
transformation from multiplayer games to two-player games such that pure
equilibria in the first game correspond to winning strategies in the second
one. We then devise from this transformation, an algorithm which computes
equilibria in mean-payoff games. Robust equilibria in mean-payoff games reduce
to winning strategies in multidimensional mean-payoff games for some threshold
satisfying some constraints. We then show that the existence of such equilibria
can be decided in polynomial space, and that the decision problem is
PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1311.7685</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1311.7685</id><created>2013-11-29</created><updated>2014-01-06</updated><authors><author><keyname>Kothari</keyname><forenames>Robin</forenames></author></authors><title>An optimal quantum algorithm for the oracle identification problem</title><categories>quant-ph cs.CC</categories><comments>16 pages; v2: minor changes</comments><journal-ref>Proceedings of the 31st International Symposium on Theoretical
  Aspects of Computer Science (STACS 2014), Leibniz International Proceedings
  in Informatics 25, pp. 482-493 (2014)</journal-ref><doi>10.4230/LIPIcs.STACS.2014.482</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the oracle identification problem, we are given oracle access to an
unknown N-bit string x promised to belong to a known set C of size M and our
task is to identify x. We present a quantum algorithm for the problem that is
optimal in its dependence on N and M. Our algorithm considerably simplifies and
improves the previous best algorithm due to Ambainis et al. Our algorithm also
has applications in quantum learning theory, where it improves the complexity
of exact learning with membership queries, resolving a conjecture of Hunziker
et al.
  The algorithm is based on ideas from classical learning theory and a new
composition theorem for solutions of the filtered $\gamma_2$-norm semidefinite
program, which characterizes quantum query complexity. Our composition theorem
is quite general and allows us to compose quantum algorithms with
input-dependent query complexities without incurring a logarithmic overhead for
error reduction. As an application of the composition theorem, we remove all
log factors from the best known quantum algorithm for Boolean matrix
multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0001</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0001</id><created>2013-11-27</created><authors><author><keyname>Chakraborty</keyname><forenames>Ayan</forenames></author><author><keyname>Munshi</keyname><forenames>Shiladitya</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>A Proposal for the Characterization of Multi-Dimensional
  Inter-relationships of RDF Graphs Based on Set Theoretic Approach</title><categories>cs.DB</categories><comments>8 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1311.7200</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a Set Theoretic approach has been reported for analyzing
inter-relationship between any numbers of RDF Graphs. An RDF Graph represents
triples in Resource Description Format of semantic web. So the identification
and characterization of criteria for inter-relationship of RDF Graphs shows a
new road in semantic search. Using set theoretic approach, a sound framing
criteria can be designed that examine whether two RDF Graphs are related and if
yes, how these relationships could be described with formal set theory. Along
with this, by introducing RDF Schema, the inter-relationship status is refined
into n-dimensional induced relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0018</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0018</id><created>2013-11-29</created><authors><author><keyname>Scherer</keyname><forenames>Gabriel</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Hoffmann</keyname><forenames>Jan</forenames></author></authors><title>Tracking Data-Flow with Open Closure Types</title><categories>cs.PL</categories><comments>Logic for Programming Artificial Intelligence and Reasoning (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Type systems hide data that is captured by function closures in function
types. In most cases this is a beneficial design that favors simplicity and
compositionality. However, some applications require explicit information about
the data that is captured in closures. This paper introduces open closure
types, that is, function types that are decorated with type contexts. They are
used to track data-flow from the environment into the function closure. A
simply-typed lambda calculus is used to study the properties of the type theory
of open closure types. A distinctive feature of this type theory is that an
open closure type of a function can vary in different type contexts. To present
an application of the type theory, it is shown that a type derivation
establishes a simple non-interference property in the sense of information-flow
theory. A publicly available prototype implementation of the system can be used
to experiment with type derivations for example programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0022</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0022</id><created>2013-11-29</created><updated>2014-10-14</updated><authors><author><keyname>Randriambololona</keyname><forenames>Hugues</forenames></author></authors><title>On products and powers of linear codes under componentwise
  multiplication</title><categories>cs.IT math.AG math.IT</categories><comments>75 pages; expanded version of a talk at AGCT-14 (Luminy), to appear
  in vol. 637 of Contemporary Math., AMS, Apr. 2015; v3: minor typos corrected
  in the final &quot;open questions&quot; section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this text we develop the formalism of products and powers of linear codes
under componentwise multiplication. As an expanded version of the author's talk
at AGCT-14, focus is put mostly on basic properties and descriptive statements
that could otherwise probably not fit in a regular research paper. On the other
hand, more advanced results and applications are only quickly mentioned with
references to the literature. We also point out a few open problems.
  Our presentation alternates between two points of view, which the theory
intertwines in an essential way: that of combinatorial coding, and that of
algebraic geometry.
  In appendices that can be read independently, we investigate topics in
multilinear algebra over finite fields, notably we establish a criterion for a
symmetric multilinear map to admit a symmetric algorithm, or equivalently, for
a symmetric tensor to decompose as a sum of elementary symmetric tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0030</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0030</id><created>2013-11-29</created><updated>2014-03-10</updated><authors><author><keyname>Lyche</keyname><forenames>Tom</forenames></author><author><keyname>Muntingh</keyname><forenames>Georg</forenames></author></authors><title>A Hermite interpolatory subdivision scheme for $C^2$-quintics on the
  Powell-Sabin 12-split</title><categories>math.NA cs.CG</categories><comments>17 pages, 7 figures</comments><msc-class>41A15, 65D07, 65D17, 65M60</msc-class><journal-ref>Computer Aided Geometric Design. Volume 31, Issues 7 - 8, October
  2014, Pages 464 - 474</journal-ref><doi>10.1016/j.cagd.2014.03.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to construct a $C^1$-quadratic spline over an arbitrary
triangulation, one can split each triangle into 12 subtriangles, resulting in a
finer triangulation known as the Powell-Sabin 12-split. It has been shown
previously that the corresponding spline surface can be plotted quickly by
means of a Hermite subdivision scheme. In this paper we introduce a nodal
macro-element on the 12-split for the space of quintic splines that are locally
$C^3$ and globally $C^2$. For quickly evaluating any such spline, a Hermite
subdivision scheme is derived, implemented, and tested in the computer algebra
system Sage. Using the available first derivatives for Phong shading, visually
appealing plots can be generated after just a couple of refinements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0032</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0032</id><created>2013-11-29</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author><author><keyname>Martinez</keyname><forenames>Maria Vanina</forenames></author><author><keyname>Molinaro</keyname><forenames>Cristian</forenames></author><author><keyname>Predoiu</keyname><forenames>Livia</forenames></author><author><keyname>Simari</keyname><forenames>Gerardo I.</forenames></author></authors><title>Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports
  (Technical Report)</title><categories>cs.AI cs.DB</categories><comments>arXiv admin note: text overlap with arXiv:1106.3767 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of preferences in query answering, both in traditional databases and
in ontology-based data access, has recently received much attention, due to its
many real-world applications. In this paper, we tackle the problem of top-k
query answering in Datalog+/- ontologies subject to the querying user's
preferences and a collection of (subjective) reports of other users. Here, each
report consists of scores for a list of features, its author's preferences
among the features, as well as other information. Theses pieces of information
of every report are then combined, along with the querying user's preferences
and his/her trust into each report, to rank the query results. We present two
alternative such rankings, along with algorithms for top-k (atomic) query
answering under these rankings. We also show that, under suitable assumptions,
these algorithms run in polynomial time in the data complexity. We finally
present more general reports, which are associated with sets of atoms rather
than single atoms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0036</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0036</id><created>2013-11-29</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Balodis</keyname><forenames>Kaspars</forenames></author><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author></authors><title>Weak Parity</title><categories>cs.CC quant-ph</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the query complexity of Weak Parity: the problem of computing the
parity of an n-bit input string, where one only has to succeed on a 1/2+eps
fraction of input strings, but must do so with high probability on those inputs
where one does succeed. It is well-known that n randomized queries and n/2
quantum queries are needed to compute parity on all inputs. But surprisingly,
we give a randomized algorithm for Weak Parity that makes only
O(n/log^0.246(1/eps)) queries, as well as a quantum algorithm that makes only
O(n/sqrt(log(1/eps))) queries. We also prove a lower bound of
Omega(n/log(1/eps)) in both cases; and using extremal combinatorics, prove
lower bounds of Omega(log n) in the randomized case and Omega(sqrt(log n)) in
the quantum case for any eps&gt;0. We show that improving our lower bounds is
intimately related to two longstanding open problems about Boolean functions:
the Sensitivity Conjecture, and the relationships between query complexity and
polynomial degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0040</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0040</id><created>2013-11-29</created><updated>2013-12-17</updated><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Dynamic Interference Management</title><categories>cs.IT math.IT</categories><comments>Shorter version is in proceedings of the Asilomar Conference on
  Signals, Systems, and Computers, Nov. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A linear interference network is considered. Long-term fluctuations (shadow
fading) in the wireless channel can lead to any link being erased with
probability p. Each receiver is interested in one unique message that can be
available at M transmitters. In a cellular downlink scenario, the case where
M=1 reflects the cell association problem, and the case where M&gt;1 reflects the
problem of setting up the backhaul links for Coordinated Multi-Point (CoMP)
transmission. In both cases, we analyze Degrees of Freedom (DoF) optimal
schemes for the case of no erasures, and propose new schemes with better
average DoF performance at high probabilities of erasure. For M=1, we
characterize the average per user DoF, and identify the optimal assignment of
messages to transmitters at each value of p. For general values of M, we show
that there is no strategy for assigning messages to transmitters in large
networks that is optimal for all values of p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0042</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0042</id><created>2013-11-29</created><authors><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>Boosting the Basic Counting on Distributed Streams</title><categories>cs.DS cs.DB cs.DC</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classic basic counting problem in the distributed streaming
model that was studied by Gibbons and Tirthapura (GT). In the solution for
maintaining an $(\epsilon,\delta)$-estimate, as what GT's method does, we make
the following new contributions: (1) For a bit stream of size $n$, where each
bit has a probability at least $\gamma$ to be 1, we exponentially reduced the
average total processing time from GT's $\Theta(n \log(1/\delta))$ to
$O((1/(\gamma\epsilon^2))(\log^2 n) \log(1/\delta))$, thus providing the first
sublinear-time streaming algorithm for this problem. (2) In addition to an
overall much faster processing speed, our method provides a new tradeoff that a
lower accuracy demand (a larger value for $\epsilon$) promises a faster
processing speed, whereas GT's processing speed is $\Theta(n \log(1/\delta))$
in any case and for any $\epsilon$. (3) The worst-case total time cost of our
method matches GT's $\Theta(n\log(1/\delta))$, which is necessary but rarely
occurs in our method. (4) The space usage overhead in our method is a lower
order term compared with GT's space usage and occurs only $O(\log n)$ times
during the stream processing and is too negligible to be detected by the
operating system in practice. We further validate these solid theoretical
results with experiments on both real-world and synthetic data, showing that
our method is faster than GT's by a factor of several to several thousands
depending on the stream size and accuracy demands, without any detectable space
usage overhead. Our method is based on a faster sampling technique that we
design for boosting GT's method and we believe this technique can be of other
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0045</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0045</id><created>2013-11-29</created><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Glaropoulos</keyname><forenames>Ioannis</forenames></author><author><keyname>Fodor</keyname><forenames>Viktoria</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Dimou</keyname><forenames>Konstantinos</forenames></author></authors><title>Energy Efficient Spectrum Sensing and Handoff Strategies in Cognitive
  Radio Networks</title><categories>cs.NI cs.ET cs.PF</categories><comments>7 pages, 5 figures, submitted to IEEE Commun. Mag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The limited spectrum resources and dramatic growth of high data rate
communications have motivated opportunistic spectrum access using the promising
concept of cognitive radio networks. Although this concept has emerged
primarily to enhance spectrum utilization, the importance of energy consumption
poses new challenges, because energy efficiency and communication performance
can be at odds. In this paper, the existing approaches to energy efficiency
spectrum sensing and handoff are classified. The tradeoff between energy
consumption and throughput is established as function of the numerous design
parameters of cognitive radio networks, both in the case of local and of
cooperative spectrum sensing. It is argued that a number of important aspects
still needs to be researched, such as fairness, dynamic behavior, reactive and
proactive schemes for energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0048</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0048</id><created>2013-11-29</created><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author></authors><title>Stochastic Optimization of Smooth Loss</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first prove a high probability bound rather than an
expectation bound for stochastic optimization with smooth loss. Furthermore,
the existing analysis requires the knowledge of optimal classifier for tuning
the step size in order to achieve the desired bound. However, this information
is usually not accessible in advanced. We also propose a strategy to address
the limitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0049</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0049</id><created>2013-11-29</created><authors><author><keyname>Khan</keyname><forenames>Shehroz S.</forenames></author><author><keyname>Madden</keyname><forenames>Michael G.</forenames></author></authors><title>One-Class Classification: Taxonomy of Study and Review of Techniques</title><categories>cs.LG cs.AI</categories><comments>24 pages + 11 pages of references, 8 figures</comments><journal-ref>The Knowledge Engineering Review, pp 1-30, 2014</journal-ref><doi>10.1017/S026988891300043X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-class classification (OCC) algorithms aim to build classification models
when the negative class is either absent, poorly sampled or not well defined.
This unique situation constrains the learning of efficient classifiers by
defining class boundary just with the knowledge of positive class. The OCC
problem has been considered and applied under many research themes, such as
outlier/novelty detection and concept learning. In this paper we present a
unified view of the general problem of OCC by presenting a taxonomy of study
for OCC problems, which is based on the availability of training data,
algorithms used and the application domains applied. We further delve into each
of the categories of the proposed taxonomy and present a comprehensive
literature review of the OCC algorithms, techniques and methodologies with a
focus on their significance, limitations and applications. We conclude our
paper by discussing some open research problems in the field of OCC and present
our vision for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0054</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0054</id><created>2013-11-29</created><updated>2014-07-06</updated><authors><author><keyname>Orhan</keyname><forenames>Oner</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Energy Harvesting Broadband Communication Systems with Processing Energy
  Cost</title><categories>cs.IT math.IT</categories><comments>published in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication over a broadband fading channel powered by an energy harvesting
transmitter is studied. Assuming non-causal knowledge of energy/data arrivals
and channel gains, optimal transmission schemes are identified by taking into
account the energy cost of the processing circuitry as well as the transmission
energy. A constant processing cost for each active sub-channel is assumed.
Three different system objectives are considered: i) throughput maximization,
in which the total amount of transmitted data by a deadline is maximized for a
backlogged transmitter with a finite capacity battery; ii) energy maximization,
in which the remaining energy in an infinite capacity battery by a deadline is
maximized such that all the arriving data packets are delivered; iii)
transmission completion time minimization, in which the delivery time of all
the arriving data packets is minimized assuming infinite size battery. For each
objective, a convex optimization problem is formulated, the properties of the
optimal transmission policies are identified, and an algorithm which computes
an optimal transmission policy is proposed. Finally, based on the insights
gained from the offline optimizations, low-complexity online algorithms
performing close to the optimal dynamic programming solution for the throughput
and energy maximization problems are developed under the assumption that the
energy/data arrivals and channel states are known causally at the transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0060</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0060</id><created>2013-11-29</created><authors><author><keyname>Basciftci</keyname><forenames>Y. Ozan</forenames></author><author><keyname>Gungor</keyname><forenames>Onur</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Ozguner</keyname><forenames>Fusun</forenames></author></authors><title>On the Secrecy Capacity of Block Fading Channels with a Hybrid Adversary</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a block fading wiretap channel, where a transmitter attempts to
send messages securely to a receiver in the presence of a hybrid half-duplex
adversary, which arbitrarily decides to either jam or eavesdrop the
transmitter-to- receiver channel. We provide bounds to the secrecy capacity for
various possibilities on receiver feedback and show special cases where the
bounds are tight. We show that, without any feedback from the receiver, the
secrecy capacity is zero if the transmitter-to-adversary channel stochastically
dominates the effective transmitter-to-receiver channel. However, the secrecy
capacity is non-zero even when the receiver is allowed to feed back only one
bit at the end of each block. Our novel achievable strategy improves the rates
proposed in the literature for the non-hybrid adversarial model. We also
analyze the effect of multiple adversaries and delay constraints on the secrecy
capacity. We show that our novel time sharing approach leads to positive
secrecy rates even under strict delay constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0072</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0072</id><created>2013-11-30</created><authors><author><keyname>Vu</keyname><forenames>Ngoc-Son</forenames></author><author><keyname>Nguyen</keyname><forenames>Thanh Phuong</forenames></author><author><keyname>Garcia</keyname><forenames>Christophe</forenames></author></authors><title>Improving Texture Categorization with Biologically Inspired Filtering</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the domain of texture classification, a lot of effort has been spent
on local descriptors, leading to many powerful algorithms. However,
preprocessing techniques have received much less attention despite their
important potential for improving the overall classification performance. We
address this question by proposing a novel, simple, yet very powerful
biologically-inspired filtering (BF) which simulates the performance of human
retina. In the proposed approach, given a texture image, after applying a DoG
filter to detect the &quot;edges&quot;, we first split the filtered image into two &quot;maps&quot;
alongside the sides of its edges. The feature extraction step is then carried
out on the two &quot;maps&quot; instead of the input image. Our algorithm has several
advantages such as simplicity, robustness to illumination and noise, and
discriminative power. Experimental results on three large texture databases
show that with an extremely low computational cost, the proposed method
improves significantly the performance of many texture classification systems,
notably in noisy environments. The source codes of the proposed algorithm can
be downloaded from https://sites.google.com/site/nsonvu/code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0078</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0078</id><created>2013-11-30</created><authors><author><keyname>Berry</keyname><forenames>G&#xe9;rard</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Serrano</keyname><forenames>Manuel</forenames></author></authors><title>Hop and HipHop : Multitier Web Orchestration</title><categories>cs.PL</categories><comments>International Conference on Distributed Computing and Internet
  Technology (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rich applications merge classical computing, client-server concurrency,
web-based interfaces, and the complex time- and event-based reactive
programming found in embedded systems. To handle them, we extend the Hop web
programming platform by HipHop, a domain-specific language dedicated to
event-based process orchestration. Borrowing the synchronous reactive model of
Esterel, HipHop is based on synchronous concurrency and preemption primitives
that are known to be key components for the modular design of complex reactive
behaviors. HipHop departs from Esterel by its ability to handle the dynamicity
of Web applications, thanks to the reflexivity of Hop. Using a music player
example, we show how to modularly build a non-trivial Hop application using
HipHop orchestration code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0084</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0084</id><created>2013-11-30</created><updated>2014-07-27</updated><authors><author><keyname>Baccini</keyname><forenames>Alberto</forenames></author><author><keyname>Barabesi</keyname><forenames>Lucio</forenames></author><author><keyname>Cioni</keyname><forenames>Martina</forenames></author><author><keyname>Pisani</keyname><forenames>Caterina</forenames></author></authors><title>Crossing the hurdle: the determinants of individual scientific
  performance</title><categories>physics.soc-ph cs.DL stat.AP</categories><comments>Revised version accepted for publication by Scientometrics</comments><msc-class>62J12</msc-class><doi>10.1007/s11192-014-1395-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An original cross sectional dataset referring to a medium sized Italian
university is implemented in order to analyze the determinants of scientific
research production at individual level. The dataset includes 942 permanent
researchers of various scientific sectors for a three year time span (2008 -
2010). Three different indicators - based on the number of publications or
citations - are considered as response variables. The corresponding
distributions are highly skewed and display an excess of zero - valued
observations. In this setting, the goodness of fit of several Poisson mixture
regression models are explored by assuming an extensive set of explanatory
variables. As to the personal observable characteristics of the researchers,
the results emphasize the age effect and the gender productivity gap, as
previously documented by existing studies. Analogously, the analysis confirm
that productivity is strongly affected by the publication and citation
practices adopted in different scientific disciplines. The empirical evidence
on the connection between teaching and research activities suggests that no
univocal substitution or complementarity thesis can be claimed: a major
teaching load does not affect the odds to be a non-active researcher and does
not significantly reduce the number of publications for active researchers. In
addition, new evidence emerges on the effect of researchers administrative
tasks, which seem to be negatively related with researcher's productivity, and
on the composition of departments. Researchers' productivity is apparently
enhanced by operating in department filled with more administrative and
technical staff, and it is not significantly affected by the composition of the
department in terms of senior or junior researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0086</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0086</id><created>2013-11-30</created><updated>2013-12-15</updated><authors><author><keyname>Ferrucci</keyname><forenames>Filomena</forenames></author><author><keyname>Kechadi</keyname><forenames>M-Tahar</forenames></author><author><keyname>Salza</keyname><forenames>Pasquale</forenames></author><author><keyname>Sarro</keyname><forenames>Federica</forenames></author></authors><title>A Framework for Genetic Algorithms Based on Hadoop</title><categories>cs.NE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Algorithms (GAs) are powerful metaheuristic techniques mostly used in
many real-world applications. The sequential execution of GAs requires
considerable computational power both in time and resources. Nevertheless, GAs
are naturally parallel and accessing a parallel platform such as Cloud is easy
and cheap. Apache Hadoop is one of the common services that can be used for
parallel applications. However, using Hadoop to develop a parallel version of
GAs is not simple without facing its inner workings. Even though some
sequential frameworks for GAs already exist, there is no framework supporting
the development of GA applications that can be executed in parallel. In this
paper is described a framework for parallel GAs on the Hadoop platform,
following the paradigm of MapReduce. The main purpose of this framework is to
allow the user to focus on the aspects of GA that are specific to the problem
to be addressed, being sure that this task is going to be correctly executed on
the Cloud with a good performance. The framework has been also exploited to
develop an application for Feature Subset Selection problem. A preliminary
analysis of the performance of the developed GA application has been performed
using three datasets and shown very promising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0114</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0114</id><created>2013-11-30</created><authors><author><keyname>Rashid</keyname><forenames>Mamoon</forenames></author><author><keyname>Chawla</keyname><forenames>Er. Rishma</forenames></author></authors><title>Extended Role Based Access Control with Blob Service on Cloud</title><categories>cs.DC cs.CR</categories><comments>6 page and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Role-based access control (RBAC) models have generated a great interest in
the security community as a powerful and generalized approach to security
management and ability to model organizational structure and their capability
to reduce administrative expenses. In this paper, we highlight the drawbacks of
latest developed RBAC models in terms of access control and authorization and
later provide a more viable extended-RBAC model, which enhances and extends its
powers to make any system more secure by adding valuable constraints. Later the
Blobs are stored on cloud server which is then accessed by the end users via
this Extended RBAC model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0116</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0116</id><created>2013-11-30</created><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Communication Through Collisions: Opportunistic Utilization of Past
  Receptions</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE INFOCOM 2014. arXiv admin note: text overlap with
  arXiv:1301.5309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When several wireless users are sharing the spectrum, packet collision is a
simple, yet widely used model for interference. Under this model, when
transmitters cause interference at any of the receivers, their collided packets
are discarded and need to be retransmitted. However, in reality, that receiver
can still store its analog received signal and utilize it for decoding the
packets in the future (for example, by successive interference cancellation
techniques). In this work, we propose a physical layer model for wireless
packet networks that allows for such flexibility at the receivers. We assume
that the transmitters will be aware of the state of the channel (i.e. when and
where collisions occur, or an unintended receiver overhears the signal) with
some delay, and propose several coding opportunities that can be utilized by
the transmitters to exploit the available signal at the receivers for
interference management (as opposed to discarding them). We analyze the
achievable throughput of our strategy in a canonical interference channel with
two transmitter-receiver pairs, and demonstrate the gain over conventional
schemes. By deriving an outer-bound, we also prove the optimality of our scheme
for the corresponding model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0121</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0121</id><created>2013-11-30</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author></authors><title>Semantics of Interaction</title><categories>cs.LO</categories><comments>34 pages. Appeared in in Proceedings of the 1996 CLiCS Summer School,
  Isaac Newton Institute, P. Dybjer and A. Pitts, eds. (Cambridge University
  Press) 1997, 1--31</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an introduction to Game Semantics based on some lecture notes given
at the CLiCS II summer school in Cambridge in 1995. We will focus on the recent
(1994) work on Game semantics, which has led to some striking advances in the
Full Abstraction problem for PCF and other programming languages. Our aim is to
give a genuinely elementary first introduction; we therefore present a
simplified version of game semantics, which nonetheless contains most of the
essential concepts. The more complex game semantics used by Abramsky,
Jagadeesan and Malacaria and by Hyland and Ong to construct fully abstract
models for PCF can be seen as refinements of what we present. Some background
in category theory, type theory and linear logic would be helpful in reading
these notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0127</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0127</id><created>2013-11-30</created><authors><author><keyname>Bauters</keyname><forenames>Kim</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>De Cock</keyname><forenames>Martine</forenames></author><author><keyname>Vermeir</keyname><forenames>Dirk</forenames></author></authors><title>Characterizing and Extending Answer Set Semantics using Possibility
  Theory</title><categories>cs.AI cs.LO</categories><comments>39 pages and 16 pages appendix with proofs. This article has been
  accepted for publication in Theory and Practice of Logic Programming,
  Copyright Cambridge University Press</comments><acm-class>D.1.6; F.1.3</acm-class><doi>10.1017/S147106841300063X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answer Set Programming (ASP) is a popular framework for modeling
combinatorial problems. However, ASP cannot easily be used for reasoning about
uncertain information. Possibilistic ASP (PASP) is an extension of ASP that
combines possibilistic logic and ASP. In PASP a weight is associated with each
rule, where this weight is interpreted as the certainty with which the
conclusion can be established when the body is known to hold. As such, it
allows us to model and reason about uncertain information in an intuitive way.
In this paper we present new semantics for PASP, in which rules are interpreted
as constraints on possibility distributions. Special models of these
constraints are then identified as possibilistic answer sets. In addition,
since ASP is a special case of PASP in which all the rules are entirely
certain, we obtain a new characterization of ASP in terms of constraints on
possibility distributions. This allows us to uncover a new form of disjunction,
called weak disjunction, that has not been previously considered in the
literature. In addition to introducing and motivating the semantics of weak
disjunction, we also pinpoint its computational complexity. In particular,
while the complexity of most reasoning tasks coincides with standard
disjunctive ASP, we find that brave reasoning for programs with weak
disjunctions is easier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0130</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0130</id><created>2013-11-30</created><authors><author><keyname>Akanbi</keyname><forenames>Adeyinka K.</forenames></author><author><keyname>Agunbiade</keyname><forenames>O. Y</forenames></author></authors><title>Integration of a city GIS data with Google Map API and Google Earth API
  for a web based 3D Geospatial Application</title><categories>cs.CY</categories><comments>4 pages, 7 figures</comments><journal-ref>International Journal of Science and Research, 2(11), pp. 200-203</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Geospatial applications are becoming indispensible part of information
systems, they provides detailed informations regarding the attribute data of
spatial objects in real world. Due to the rapid technological developments in
web based geographical information systems, the uses of web based geospatial
application varies from Geotagging to Geolocation capabilities. Therefore,
effective utilization of web based information system can only be realized by
representing the world in its original view, where attributes data of spatial
objects are integrated with spatial object and available for the user on the
web, using integrated Google API and Google Earth API. In this study a city in
the south-western part of Nigeria called EDE is examined and used as a case
study. Using Google Map API and Google Earth API, the attribute data of the
study area stored in XML databases will be integrated with the corresponding
existing spatial data of the study area; to create a web based 3D geospatial
application. We envisage that this system will enhance the effectiveness of
web-based Geographical Information System (GIS) and the overall user
experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0132</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0132</id><created>2013-11-30</created><updated>2014-04-12</updated><authors><author><keyname>Tahmasbi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Shahrasbi</keyname><forenames>Amirbehshad</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Critical Graphs in Index Coding</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define critical graphs as minimal graphs that support a
given set of rates for the index coding problem, and study them for both the
one-shot and asymptotic setups. For the case of equal rates, we find the
critical graph with minimum number of edges for both one-shot and asymptotic
cases. For the general case of possibly distinct rates, we show that for
one-shot and asymptotic linear index coding, as well as asymptotic non-linear
index coding, each critical graph is a union of disjoint strongly connected
subgraphs (USCS). On the other hand, we identify a non-USCS critical graph for
a one-shot non-linear index coding problem. Next, we identify a few graph
structures that are critical. We also generalize some of our results to the
groupcast problem. In addition, we show that the capacity region of the index
coding is additive for union of disjoint graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0133</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0133</id><created>2013-11-30</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Bayhan</keyname><forenames>Suzan</forenames></author><author><keyname>Kangasharju</keyname><forenames>Jussi</forenames></author></authors><title>Effects of Cooperation Policy and Network Topology on Performance of
  In-Network Caching</title><categories>cs.NI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model the cooperation policy with only two parameters -- search radius $r$
and number of copies in the network $N_{copy}$. These two parameters represent
the range of cooperation and tolerance of duplicates. We show how cooperation
policy impacts content distribution, and further illustrate the relation
between content popularity and topological properties. Our work leads many
implications on how to take advantage of topological properties in in-network
caching strategy design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0137</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0137</id><created>2013-11-30</created><authors><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Fouz</keyname><forenames>Mahmoud</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Approximation Algorithms for Non-Single-minded Profit-Maximization
  Problems with Limited Supply</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider {\em profit-maximization} problems for {\em combinatorial
auctions} with {\em non-single minded valuation functions} and {\em limited
supply}.
  We obtain fairly general results that relate the approximability of the
profit-maximization problem to that of the corresponding {\em
social-welfare-maximization} (SWM) problem, which is the problem of finding an
allocation $(S_1,\ldots,S_n)$ satisfying the capacity constraints that has
maximum total value $\sum_j v_j(S_j)$. For {\em subadditive valuations} (and
hence {\em submodular, XOS valuations}), we obtain a solution with profit
$\OPT_\swm/O(\log c_{\max})$, where $\OPT_\swm$ is the optimum social welfare
and $c_{\max}$ is the maximum item-supply; thus, this yields an $O(\log
c_{\max})$-approximation for the profit-maximization problem. Furthermore,
given {\em any} class of valuation functions, if the SWM problem for this
valuation class has an LP-relaxation (of a certain form) and an algorithm
&quot;verifying&quot; an {\em integrality gap} of $\al$ for this LP, then we obtain a
solution with profit $\OPT_\swm/O(\al\log c_{\max})$, thus obtaining an
$O(\al\log c_{\max})$-approximation.
  For the special case, when the tree is a path, we also obtain an incomparable
$O(\log m)$-approximation (via a different approach) for subadditive
valuations, and arbitrary valuations with unlimited supply. Our approach for
the latter problem also gives an $\frac{e}{e-1}$-approximation algorithm for
the multi-product pricing problem in the Max-Buy model, with limited supply,
improving on the previously known approximation factor of 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0138</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0138</id><created>2013-11-30</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>The bitwise operations related to a fast sorting algorithm</title><categories>cs.PL</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 9, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the work we discuss the benefit of using bitwise operations in
programming. Some interesting examples in this respect have been shown. What is
described in detail is an algorithm for sorting an integer array with the
substantial use of the bitwise operations. Besides its correctness we strictly
prove that the described algorithm works in time O(n). In the work during the
realisation of each of the examined algorithms we use the apparatus of the
object-oriented programming with the syntax and the semantics of the
programming language C++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0144</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0144</id><created>2013-11-30</created><updated>2013-12-12</updated><authors><author><keyname>Fan</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Yanjing</forenames></author><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author></authors><title>Knowing Whether</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing whether a proposition is true means knowing that it is true or
knowing that it is false. In this paper, we study logics with a modal operator
Kw for knowing whether but without a modal operator K for knowing that. This
logic is not a normal modal logic, because we do not have Kw (phi -&gt; psi) -&gt;
(Kw phi -&gt; Kw psi). Knowing whether logic cannot define many common frame
properties, and its expressive power less than that of basic modal logic over
classes of models without reflexivity. These features make axiomatizing knowing
whether logics non-trivial. We axiomatize knowing whether logic over various
frame classes. We also present an extension of knowing whether logic with
public announcement operators and we give corresponding reduction axioms for
that. We compare our work in detail to two recent similar proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0146</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0146</id><created>2013-11-30</created><authors><author><keyname>Xia</keyname><forenames>Minghua</forenames></author><author><keyname>A&#xef;ssa</keyname><forenames>Sonia</forenames></author></authors><title>Impact of Co-Channel Interference on Performance of Multi-Hop Relaying
  over Nakagami-$m$ Fading Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, accepted by IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the impact of co-channel interferences (CCIs) on the
system performance of multi-hop amplify-and-forward (AF) relaying, in a simple
and explicit way. For generality, the desired channels along consecutive
relaying hops and the CCIs at all nodes are subject to Nakagami-$m$ fading with
different shape factors. This study reveals that the diversity gain is
determined only by the fading shape factor of the desired channels, regardless
of the interference and the number of relaying hops. On the other hand,
although the coding gain is in general a complex function of various system
parameters, if the desired channels are subject to Rayleigh fading, the coding
gain is inversely proportional to the accumulated interference at the
destination, i.e. the product of the number of relaying hops and the average
interference-to-noise ratio, irrespective of the fading distribution of the
CCIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0148</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0148</id><created>2013-11-30</created><authors><author><keyname>Aamir</keyname><forenames>Muhammad</forenames></author></authors><title>On Replacing PID Controller with ANN Controller for DC Motor Position
  Control</title><categories>cs.SY</categories><comments>9 pages, 9 figures, 2 tables</comments><journal-ref>International Journal of Research Studies in Computing, Consortia
  Academia Publishing, vol. 2 no. 1, pp. 21-29, April 2013</journal-ref><doi>10.5861/ijrsc.2013.236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process industry implements many techniques with certain parameters in
its operations to control the working of several actuators on field. Amongst
these actuators, DC motor is a very common machine. The angular position of DC
motor can be controlled to drive many processes such as the arm of a robot. The
most famous and well known controller for such applications is PID controller.
It uses proportional, integral and derivative functions to control the input
signal before sending it to the plant unit. In this paper, another controller
based on Artificial Neural Network (ANN) control is examined to replace the PID
controller for controlling the angular position of a DC motor to drive a robot
arm. Simulation is performed in MATLAB after training the neural network
(supervised learning) and it is shown that results are acceptable and
applicable in process industry for reference control applications. The paper
also indicates that the ANN controller can be less complicated and less costly
to implement in industrial control applications as compared to some other
proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0156</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0156</id><created>2013-11-30</created><authors><author><keyname>Kantere</keyname><forenames>Verena</forenames></author></authors><title>Datom: Towards modular data management</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent technology breakthroughs have enabled data collection of unprecedented
scale, rate, variety and complexity that has led to an explosion in data
management requirements. Existing theories and techniques are not adequate to
fulfil these requirements. We endeavour to rethink the way data management
research is being conducted and we propose to work towards modular data
management that will allow for unification of the expression of data management
problems and systematization of their solution. The core of such an approach is
the novel notion of a datom, i.e. a data management atom, which encapsulates
generic data management provision. The datom is the foundation for comparison,
customization and re-usage of data management problems and solutions. The
proposed approach can signal a revolution in data management research and a
long anticipated evolution in data management engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0158</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0158</id><created>2013-11-30</created><authors><author><keyname>Conca</keyname><forenames>Aldo</forenames></author><author><keyname>Edidin</keyname><forenames>Dan</forenames></author><author><keyname>Hering</keyname><forenames>Milena</forenames></author><author><keyname>Vinzant</keyname><forenames>Cynthia</forenames></author></authors><title>An algebraic characterization of injectivity in phase retrieval</title><categories>math.FA cs.IT math.AG math.IT</categories><comments>11 pages</comments><journal-ref>Applied and Computational Harmonic Analysis 38:2 (2015) pp.
  346-356</journal-ref><doi>10.1016/j.acha.2014.06.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complex frame is a collection of vectors that span $\mathbb{C}^M$ and
define measurements, called intensity measurements, on vectors in
$\mathbb{C}^M$. In purely mathematical terms, the problem of phase retrieval is
to recover a complex vector from its intensity measurements, namely the modulus
of its inner product with these frame vectors. We show that any vector is
uniquely determined (up to a global phase factor) from $4M-4$ generic
measurements. To prove this, we identify the set of frames defining
non-injective measurements with the projection of a real variety and bound its
dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0162</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0162</id><created>2013-11-30</created><authors><author><keyname>Bezzubtseva</keyname><forenames>Anastasia</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>A Typology of Collaboration Platform Users</title><categories>cs.CY cs.HC cs.SI stat.ML</categories><msc-class>68U35, 91D30</msc-class><acm-class>K.4.3</acm-class><journal-ref>R. Tagiew et al. (Eds.) Proc. of Int. Workshop on Experimental
  Economics in Machine Learning 2012. Published by KU-Leuven, ISBN
  978-9-08-140992-6, pp. 9-19</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a review of the existing typologies of Internet
service users. We zoom in on social networking services including blogs and
crowdsourcing websites. Based on the results of the analysis of the considered
typologies obtained by means of FCA we developed a new user typology of a
certain class of Internet services, namely a collaboration innovation platform.
Cluster analysis of data extracted from the collaboration platform Witology was
used to divide more than 500 participants into six groups based on three
activity indicators: idea generation, commenting, and evaluation (assigning
marks) The obtained groups and their percentages appear to follow the &quot;90 - 9 -
1&quot; rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0169</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0169</id><created>2013-11-30</created><updated>2014-01-16</updated><authors><author><keyname>Sinatra</keyname><forenames>Roberta</forenames></author><author><keyname>Szell</keyname><forenames>Michael</forenames></author></authors><title>Entropy and the Predictability of Online Life</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 5 figures</comments><journal-ref>Entropy 2014, 16, 543-556</journal-ref><doi>10.3390/e16010543</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Using mobile phone records and information theory measures, our daily lives
have been recently shown to follow strict statistical regularities, and our
movement patterns are to a large extent predictable. Here, we apply entropy and
predictability measures to two data sets of the behavioral actions and the
mobility of a large number of players in the virtual universe of a massive
multiplayer online game. We find that movements in virtual human lives follow
the same high levels of predictability as offline mobility, where future
movements can to some extent be predicted well if the temporal correlations of
visited places are accounted for. Time series of behavioral actions show
similar high levels of predictability, even when temporal correlations are
neglected. Entropy conditional on specific behavioral actions reveals that in
terms of predictability negative behavior has a wider variety than positive
actions. The actions which contain information to best predict an individual's
subsequent action are negative, such as attacks or enemy markings, while
positive actions of friendship marking, trade and communication contain the
least amount of predictive information. These observations show that predicting
behavioral actions requires less information than predicting the mobility
patterns of humans for which the additional knowledge of past visited locations
is crucial, and that the type and sign of a social relation has an essential
impact on the ability to determine future behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0171</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0171</id><created>2013-11-30</created><updated>2015-12-10</updated><authors><author><keyname>H&#xe9;bert-Dufresne</keyname><forenames>Laurent</forenames></author><author><keyname>Laurence</keyname><forenames>Edward</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author><author><keyname>Young</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Dub&#xe9;</keyname><forenames>Louis J.</forenames></author></authors><title>Complex networks as an emerging property of hierarchical preferential
  attachment</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><journal-ref>Phys. Rev. E 92, 062809 (2015)</journal-ref><doi>10.1103/PhysRevE.92.062809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real complex systems are not rigidly structured; no clear rules or blueprints
exist for their construction. Yet, amidst their apparent randomness, complex
structural properties universally emerge. We propose that an important class of
complex systems can be modeled as an organization of many embedded levels
(potentially infinite in number), all of them following the same universal
growth principle known as preferential attachment. We give examples of such
hierarchy in real systems, for instance in the pyramid of production entities
of the film industry. More importantly, we show how real complex networks can
be interpreted as a projection of our model, from which their scale
independence, their clustering, their hierarchy, their fractality and their
navigability naturally emerge. Our results suggest that complex networks,
viewed as growing systems, can be quite simple, and that the apparent
complexity of their structure is largely a reflection of their unobserved
hierarchical nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0175</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0175</id><created>2013-11-30</created><updated>2014-08-23</updated><authors><author><keyname>Caulfield</keyname><forenames>Benjamin</forenames></author></authors><title>On Even Linear Indexed Languages with a Reduction to the Learning of
  Context-Free Languages</title><categories>cs.FL</categories><comments>Submitted to Information Processing Letters Certain proofs were not
  considered rigorous enough for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a restricted form of linear indexed grammars, called even
linear indexed grammars, which yield the even linear indexed languages. These
languages properly contain the context-free languages and are contained in the
set of linear indexed languages. We show that several patterns found in natural
languages are also generated by these grammars, including crossing
dependencies, copying, and multiple agreements. We discuss the learning problem
for even linear indexed languages and show that it is reducible to that of the
context-free languages. The closure properties for this class of languages are
also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0182</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0182</id><created>2013-12-01</created><authors><author><keyname>Wu</keyname><forenames>Haocheng</forenames></author><author><keyname>Hu</keyname><forenames>Yunhua</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author></authors><title>Query Segmentation for Relevance Ranking in Web Search</title><categories>cs.IR</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we try to answer the question of how to improve the
state-of-the-art methods for relevance ranking in web search by query
segmentation. Here, by query segmentation it is meant to segment the input
query into segments, typically natural language phrases, so that the
performance of relevance ranking in search is increased. We propose employing
the re-ranking approach in query segmentation, which first employs a generative
model to create top $k$ candidates and then employs a discriminative model to
re-rank the candidates to obtain the final segmentation result. The method has
been widely utilized for structure prediction in natural language processing,
but has not been applied to query segmentation, as far as we know. Furthermore,
we propose a new method for using the result of query segmentation in relevance
ranking, which takes both the original query words and the segmented query
phrases as units of query representation. We investigate whether our method can
improve three relevance models, namely BM25, key n-gram model, and dependency
model. Our experimental results on three large scale web search datasets show
that our method can indeed significantly improve relevance ranking in all the
three cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0186</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0186</id><created>2013-12-01</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>On an Algorithm for Obtaining All Binary Matrices of Special Class
  Related to V. E. Tarakanov's Formula</title><categories>cs.DS math.CO</categories><msc-class>05B20</msc-class><journal-ref>Journal of Mathematical Sciences and Applications, 2013, Vol. 1,
  No. 2, 36-38</journal-ref><doi>10.12691/jmsa-1-2-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for obtaining all n\times n binary matrices having exactly 2
units in every row and every column is described in the paper. After analysing
the work of the algorithm a formula for calculating the number of these
matrices has been obtained. This formula is known and has been obtained using
other methods, which by their nature are purely analytical and not
constructive. Thus a new, constructive proof of this known formula has been
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0189</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0189</id><created>2013-12-01</created><authors><author><keyname>Jamil</keyname><forenames>Hasan M.</forenames></author></authors><title>Empowering Evolving Social Network Users with Privacy Rights</title><categories>cs.DB cs.CR cs.SI</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable concerns exist over privacy on social networks, and huge debates
persist about how to extend the artifacts users need to effectively protect
their rights to privacy. While many interesting ideas have been proposed, no
single approach appears to be comprehensive enough to be the front runner. In
this paper, we propose a comprehensive and novel reference conceptual model for
privacy in constantly evolving social networks and establish its novelty by
briefly contrasting it with contemporary research. We also present the contours
of a possible query language that we can develop with desirable features in
light of the reference model, and refer to a new query language, {\em PiQL},
developed on the basis of this model that aims to support user driven privacy
policy authoring and enforcement. The strength of our model is that such
extensions are now possible by developing appropriate linguistic constructs as
part of query languages such as SQL, as demonstrated in PiQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0190</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0190</id><created>2013-12-01</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>Inclusion of regular and linear languages in group languages</title><categories>cs.FL</categories><comments>14 pages</comments><msc-class>68Q45, 68Q70</msc-class><journal-ref>International J. of Math. Sci. &amp; Engg. Appls. (IJMSEA), ISSN
  0973-9424, Vol. 7 No. I (January, 2013), pp. 323-336</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Let $\Sigma = X\cup X^{-1} = \{ x_1 ,x_2 ,..., x_m ,x_1^{-1} ,x_2^{-1} ,...,
x_m^{-1} \}$ and let $G$ be a group with set of generators $\Sigma$. Let
$\mathfrak{L} (G) =\left\{ \left. \omega \in \Sigma^* \; \right\vert \;\omega
\equiv e \; (\textrm{mod} \; G) \right\} \subseteq \Sigma^*$ be the group
language representing $G$, where $\Sigma^*$ is a free monoid over $\Sigma$ and
$e$ is the identity in $G$. The problem of determining whether a context-free
language is subset of a group language is discussed. Polynomial algorithms are
presented for testing whether a regular language, or a linear language is
included in a group language. A few finite sets are built, such that each of
them is included in the group language $\mathfrak{L} (G)$ if and only if the
respective context-free language is included in $\mathfrak{L} (G)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0192</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0192</id><created>2013-12-01</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>Random Permutations, Random Sudoku Matrices and Randomized Algorithms</title><categories>math.CO cs.DS math.PR</categories><msc-class>05B20, 65C05, 68W40</msc-class><journal-ref>International J. of Math. Sci. &amp; Engg. Appls. (IJMSEA), ISSN
  0973-9424, Vol. 6 No. VI (November, 2012), pp. 291-302</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some randomized algorithms, used to obtain a random $n^2 \times n^2$ Sudoku
matrix, where $n$ is a natural number, is reviewed in this study. Below is
described the set $\Pi_n$ of all $(2n) \times n$ matrices, consisting of
elements of the set $\mathbb{Z}_n =\{ 1,2,\ldots ,n\}$, such that every row is
a permutation. It is proved that such matrices would be particularly useful in
developing efficient algorithms in generating Sudoku matrices. An algorithm to
obtain random $\Pi_n$ matrices is presented in this paper. The algorithms are
evaluated according to two criteria - probability evaluation, and time
evaluation. This type of criteria is interesting from both theoretical and
practical point of view because they are particularly useful in the analysis of
computer programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0193</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0193</id><created>2013-12-01</created><updated>2014-04-24</updated><authors><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Yu</keyname><forenames>Hsiang-Fu</forenames></author><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit</forenames></author></authors><title>NOMAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous
  and Decentralized matrix completion</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an efficient parallel distributed algorithm for matrix completion,
named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous
and Decentralized matrix completion). NOMAD is a decentralized algorithm with
non-blocking communication between processors. One of the key features of NOMAD
is that the ownership of a variable is asynchronously transferred between
processors in a decentralized fashion. As a consequence it is a lock-free
parallel algorithm. In spite of being an asynchronous algorithm, the variable
updates of NOMAD are serializable, that is, there is an equivalent update
ordering in a serial implementation. NOMAD outperforms synchronous algorithms
which require explicit bulk synchronization after every iteration: our
extensive empirical evaluation shows that not only does our algorithm perform
well in distributed setting on commodity hardware, but also outperforms
state-of-the-art algorithms on a HPC cluster both in multi-core and distributed
memory settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0194</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0194</id><created>2013-12-01</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>Some Combinatorial Problems on Binary Matrices in Programming Courses</title><categories>cs.DS math.CO</categories><msc-class>97P50, 68R05, 05B20</msc-class><journal-ref>Informational Technologies in Education, 2012, 12, 39-43</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study proves the existence of an algorithm to receive all elements of a
class of binary matrices without obtaining redundant elements, e. g. without
obtaining binary matrices that do not belong to the class. This makes it
possible to avoid checking whether each of the objects received possesses the
necessary properties. This significantly improves the efficiency of the
algorithm in terms of the criterion of time. Certain useful educational effects
related to the analysis of such problems in programming classes are also
pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0200</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0200</id><created>2013-12-01</created><authors><author><keyname>Bardin</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Gotlieb</keyname><forenames>Arnaud</forenames></author></authors><title>A Combined Approach for Constraints over Finite Domains and Arrays</title><categories>cs.LO cs.AI cs.SE</categories><acm-class>I.2.3; F.3.1; F.4.1; D.2.4; D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arrays are ubiquitous in the context of software verification. However,
effective reasoning over arrays is still rare in CP, as local reasoning is
dramatically ill-conditioned for constraints over arrays. In this paper, we
propose an approach combining both global symbolic reasoning and local
consistency filtering in order to solve constraint systems involving arrays
(with accesses, updates and size constraints) and finite-domain constraints
over their elements and indexes. Our approach, named FDCC, is based on a
combination of a congruence closure algorithm for the standard theory of arrays
and a CP solver over finite domains. The tricky part of the work lies in the
bi-directional communication mechanism between both solvers. We identify the
significant information to share, and design ways to master the communication
overhead. Experiments on random instances show that FDCC solves more formulas
than any portfolio combination of the two solvers taken in isolation, while
overhead is kept reasonable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0202</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0202</id><created>2013-12-01</created><authors><author><keyname>Hou</keyname><forenames>T. Y.</forenames></author><author><keyname>Shi</keyname><forenames>Z.</forenames></author><author><keyname>Tavallali</keyname><forenames>P.</forenames></author></authors><title>Sparse Time Frequency Representations and Dynamical Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish a connection between the recently developed
data-driven time-frequency analysis \cite{HS11,HS13-1} and the classical second
order differential equations. The main idea of the data-driven time-frequency
analysis is to decompose a multiscale signal into a sparsest collection of
Intrinsic Mode Functions (IMFs) over the largest possible dictionary via
nonlinear optimization. These IMFs are of the form $a(t) \cos(\theta(t))$ where
the amplitude $a(t)$ is positive and slowly varying. The non-decreasing phase
function $\theta(t)$ is determined by the data and in general depends on the
signal in a nonlinear fashion. One of the main results of this paper is that we
show that each IMF can be associated with a solution of a second order ordinary
differential equation of the form $\ddot{x}+p(x,t)\dot{x}+q(x,t)=0$. Further,
we propose a localized variational formulation for this problem and develop an
effective $l^1$-based optimization method to recover $p(x,t)$ and $q(x,t)$ by
looking for a sparse representation of $p$ and $q$ in terms of the polynomial
basis. Depending on the form of nonlinearity in $p(x,t)$ and $q(x,t)$, we can
define the degree of nonlinearity for the associated IMF. %and the
corresponding coefficients for the associated highest order nonlinear terms.
This generalizes a concept recently introduced by Prof. N. E. Huang et al.
\cite{Huang11}. Numerical examples will be provided to illustrate the
robustness and stability of the proposed method for data with or without noise.
This manuscript should be considered as a proof of concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0229</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0229</id><created>2013-12-01</created><authors><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Five Disruptive Technology Directions for 5G</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New research directions will lead to fundamental changes in the design of
future 5th generation (5G) cellular networks. This paper describes five
technologies that could lead to both architectural and component disruptive
design changes: device-centric architectures, millimeter Wave, Massive-MIMO,
smarter devices, and native support to machine-2-machine. The key ideas for
each technology are described, along with their potential impact on 5G and the
research challenges that remain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0232</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0232</id><created>2013-12-01</created><updated>2015-03-23</updated><authors><author><keyname>Tyagi</keyname><forenames>Hemant</forenames></author><author><keyname>Stich</keyname><forenames>Sebastian</forenames></author><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author></authors><title>Stochastic continuum armed bandit problem of few linear parameters in
  high dimensions</title><categories>stat.ML cs.LG math.OC</categories><comments>(i) 17 pages. (ii) Corrected statements of Theorems 1,2 along with
  minor corrections at other places in the draft. (iii) Part of a journal draft
  along with http://arxiv.org/abs/1304.5793, which has been accepted in :
  Theory of Computing Systems (TOCS) - Special issue on WAOA 2013. This is
  available online at
  http://link.springer.com/article/10.1007%2Fs00224-014-9570-8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stochastic continuum armed bandit problem where the arms are
indexed by the $\ell_2$ ball $B_{d}(1+\nu)$ of radius $1+\nu$ in
$\mathbb{R}^d$. The reward functions $r :B_{d}(1+\nu) \rightarrow \mathbb{R}$
are considered to intrinsically depend on $k \ll d$ unknown linear parameters
so that $r(\mathbf{x}) = g(\mathbf{A} \mathbf{x})$ where $\mathbf{A}$ is a full
rank $k \times d$ matrix. Assuming the mean reward function to be smooth we
make use of results from low-rank matrix recovery literature and derive an
efficient randomized algorithm which achieves a regret bound of $O(C(k,d)
n^{\frac{1+k}{2+k}} (\log n)^{\frac{1}{2+k}})$ with high probability. Here
$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds
or the sampling budget which is assumed to be known beforehand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0233</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0233</id><created>2013-12-01</created><updated>2015-09-09</updated><authors><author><keyname>Yankelevsky</keyname><forenames>Yael</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author></authors><title>On Optimal Disc Covers and a New Characterization of the Steiner Center</title><categories>cs.CG</categories><comments>14 pages, 11 figures; minor corrections, revised proof of theorem 2</comments><report-no>CIS-2013-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given N points in the plane $P_1 P_2...P_N$ and a location $\Omega$, the
union of discs with diameters $[\Omega P_i], i = 1, 2,...N$ covers the convex
hull of the points. The location $\Omega_s$ minimizing the area covered by the
union of discs, is shown to be the Steiner center of the convex hull of the
points. Similar results for $d$-dimensional Euclidean space are conjectured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0249</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0249</id><created>2013-12-01</created><authors><author><keyname>Gharzouli</keyname><forenames>Mohamed</forenames></author></authors><title>Reuse of existing applications during the development of Enterprise
  Portals integrating Web Services</title><categories>cs.DC cs.SE</categories><comments>06 pages, 4 figures, 10th Maghrebian Conference on Information
  Technologies MCSEAI08, April 28-30 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During these last years, the use of the web technologies in the enterprises
becomes an essential factor to define a new business model. Among these
technologies, web services and enterprise portals have gathered to integrate
existing heterogeneous systems such as e-commerce, eservices hub and
e-learning. However, the design and the modeling of the portals, on the one
hand, and their integration with the existing applications, on the other hand,
are still two points open for discussion. The first problem is related to the
development of the lifecycle that can be used for designing and modeling the
enterprise portals. For the second problem, which is the integration of the
existing applications, the discussion is intended towards the use of
technologies based on web services. In This paper, we present a software
engineering solution for the development of Web services-based enterprise
portals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0256</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0256</id><created>2013-12-01</created><updated>2016-02-02</updated><authors><author><keyname>Vehkapera</keyname><forenames>Mikko</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author></authors><title>Analysis of Regularized LS Reconstruction and Random Matrix Ensembles in
  Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>revised version accepted for publication in IEEE Trans. Inform.
  Theory; 25 pages, 5 figures</comments><doi>10.1109/TIT.2016.2525824</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance of regularized least-squares estimation in noisy compressed
sensing is analyzed in the limit when the dimensions of the measurement matrix
grow large. The sensing matrix is considered to be from a class of random
ensembles that encloses as special cases standard Gaussian, row-orthogonal,
geometric and so-called T-orthogonal constructions. Source vectors that have
non-uniform sparsity are included in the system model. Regularization based on
l1-norm and leading to LASSO estimation, or basis pursuit denoising, is given
the main emphasis in the analysis. Extensions to l2-norm and &quot;zero-norm&quot;
regularization are also briefly discussed. The analysis is carried out using
the replica method in conjunction with some novel matrix integration results.
Numerical experiments for LASSO are provided to verify the accuracy of the
analytical results. The numerical experiments show that for noisy compressed
sensing, the standard Gaussian ensemble is a suboptimal choice for the
measurement matrix. Orthogonal constructions provide a superior performance in
all considered scenarios and are easier to implement in practical applications.
It is also discovered that for non-uniform sparsity patterns the T-orthogonal
matrices can further improve the mean square error behavior of the
reconstruction when the noise level is not too high. However, as the additive
noise becomes more prominent in the system, the simple row-orthogonal
measurement matrix appears to be the best choice out of the considered
ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0264</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0264</id><created>2013-12-01</created><updated>2014-06-09</updated><authors><author><keyname>Allen</keyname><forenames>F.</forenames></author><author><keyname>Greiner</keyname><forenames>R.</forenames></author><author><keyname>Wishart</keyname><forenames>D.</forenames></author></authors><title>Competitive Fragmentation Modeling of ESI-MS/MS spectra for putative
  metabolite identification</title><categories>cs.CE</categories><comments>Preprint. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s11306-014-0676-4. Metabolomics 2014</comments><doi>10.1007/s11306-014-0676-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrospray tandem mass spectrometry (ESI-MS/MS) is commonly used in high
throughput metabolomics. One of the key obstacles to the effective use of this
technology is the difficulty in interpreting measured spectra to accurately and
efficiently identify metabolites. Traditional methods for automated metabolite
identification compare the target MS or MS/MS spectrum to the spectra in a
reference database, ranking candidates based on the closeness of the match.
However the limited coverage of available databases has led to an interest in
computational methods for predicting reference MS/MS spectra from chemical
structures.
  This work proposes a probabilistic generative model for the MS/MS
fragmentation process, which we call Competitive Fragmentation Modeling (CFM),
and a machine learning approach for learning parameters for this model from
MS/MS data. We show that CFM can be used in both a MS/MS spectrum prediction
task (ie, predicting the mass spectrum from a chemical structure), and in a
putative metabolite identification task (ranking possible structures for a
target MS/MS spectrum).
  In the MS/MS spectrum prediction task, CFM shows significantly improved
performance when compared to a full enumeration of all peaks corresponding to
substructures of the molecule. In the metabolite identification task, CFM
obtains substantially better rankings for the correct candidate than existing
methods (MetFrag and FingerID) on tripeptide and metabolite data, when querying
PubChem or KEGG for candidate structures of similar mass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0285</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0285</id><created>2013-12-01</created><authors><author><keyname>Golab</keyname><forenames>Lukasz</forenames></author><author><keyname>Hadjieleftheriou</keyname><forenames>Marios</forenames></author><author><keyname>Karloff</keyname><forenames>Howard</forenames></author><author><keyname>Saha</keyname><forenames>Barna</forenames></author></authors><title>Distributed Data Placement via Graph Partitioning</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the widespread use of shared-nothing clusters of servers, there has been
a proliferation of distributed object stores that offer high availability,
reliability and enhanced performance for MapReduce-style workloads. However,
relational workloads cannot always be evaluated efficiently using MapReduce
without extensive data migrations, which cause network congestion and reduced
query throughput. We study the problem of computing data placement strategies
that minimize the data communication costs incurred by typical relational query
workloads in a distributed setting.
  Our main contribution is a reduction of the data placement problem to the
well-studied problem of {\sc Graph Partitioning}, which is NP-Hard but for
which efficient approximation algorithms exist. The novelty and significance of
this result lie in representing the communication cost exactly and using
standard graphs instead of hypergraphs, which were used in prior work on data
placement that optimized for different objectives (not communication cost).
  We study several practical extensions of the problem: with load balancing,
with replication, with materialized views, and with complex query plans
consisting of sequences of intermediate operations that may be computed on
different servers. We provide integer linear programs (IPs) that may be used
with any IP solver to find an optimal data placement. For the no-replication
case, we use publicly available graph partitioning libraries (e.g., METIS) to
efficiently compute nearly-optimal solutions. For the versions with
replication, we introduce two heuristics that utilize the {\sc Graph
Partitioning} solution of the no-replication case. Using the TPC-DS workload,
it may take an IP solver weeks to compute an optimal data placement, whereas
our reduction produces nearly-optimal solutions in seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0286</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0286</id><created>2013-12-01</created><updated>2014-07-20</updated><authors><author><keyname>Hamilton</keyname><forenames>William L.</forenames></author><author><keyname>Fard</keyname><forenames>Mahdi Milani</forenames></author><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author></authors><title>Efficient Learning and Planning with Compressed Predictive States</title><categories>cs.LG stat.ML</categories><comments>45 pages, 10 figures, submitted to the Journal of Machine Learning
  Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictive state representations (PSRs) offer an expressive framework for
modelling partially observable systems. By compactly representing systems as
functions of observable quantities, the PSR learning approach avoids using
local-minima prone expectation-maximization and instead employs a globally
optimal moment-based algorithm. Moreover, since PSRs do not require a
predetermined latent state structure as an input, they offer an attractive
framework for model-based reinforcement learning when agents must plan without
a priori access to a system model. Unfortunately, the expressiveness of PSRs
comes with significant computational cost, and this cost is a major factor
inhibiting the use of PSRs in applications. In order to alleviate this
shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR
learning approach combines recent advancements in dimensionality reduction,
incremental matrix decomposition, and compressed sensing. We show how this
approach provides a principled avenue for learning accurate approximations of
PSRs, drastically reducing the computational costs associated with learning
while also providing effective regularization. Going further, we propose a
planning framework which exploits these learned models. And we show that this
approach facilitates model-learning and planning in large complex partially
observable domains, a task that is infeasible without the principled use of
compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0288</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0288</id><created>2013-12-01</created><updated>2014-10-13</updated><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Khanfir</keyname><forenames>Hajer</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Kamoun</keyname><forenames>Mohamed</forenames></author></authors><title>Preliminary Results on 3D Channel Modeling: From Theory to
  Standardization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three dimensional beamforming (3D) (also elevation beamforming) is now
gaining a growing interest among researchers in wireless communication. The
reason can be attributed to its potential to enable a variety of strategies
like sector or user specific elevation beamforming and cell-splitting. Since
these techniques cannot be directly supported by current LTE releases, the 3GPP
is now working on defining the required technical specifications. In
particular, a large effort is currently made to get accurate 3D channel models
that support the elevation dimension. This step is necessary as it will
evaluate the potential of 3D and FD(Full Dimensional) beamforming techniques to
benefit from the richness of real channels. This work aims at presenting the
on-going 3GPP study item &quot;Study on 3D-channel model for Elevation Beamforming
and FD-MIMO studies for LTE&quot;, and positioning it with respect to previous
standardization works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0298</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0298</id><created>2013-12-01</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>On quadratic orbital networks</title><categories>math.DS cs.DM</categories><comments>13 figures 15 pages</comments><msc-class>05C82, 90B10, 91D30, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These are some informal remarks on quadratic orbital networks over finite
fields. We discuss connectivity, Euler characteristic, number of cliques,
planarity, diameter and inductive dimension. We find a non-trivial disconnected
graph for d=3. We prove that for d=1 generators, the Euler characteristic is
always non-negative and for d=2 and large enough p the Euler characteristic is
negative. While for d=1, all networks are planar, we suspect that for d larger
or equal to 2 and large enough prime p, all networks are non-planar. As a
consequence on bounds for the number of complete sub graphs of a fixed
dimension, the inductive dimension of all these networks goes 1 as p goes to
infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0308</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0308</id><created>2013-12-01</created><authors><author><keyname>Chazal</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Stochastic Convergence of Persistence Landscapes and Silhouettes</title><categories>math.ST cs.CG math.AT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is a widely used tool in Topological Data Analysis that
encodes multiscale topological information as a multi-set of points in the
plane called a persistence diagram. It is difficult to apply statistical theory
directly to a random sample of diagrams. Instead, we can summarize the
persistent homology with the persistence landscape, introduced by Bubenik,
which converts a diagram into a well-behaved real-valued function. We
investigate the statistical properties of landscapes, such as weak convergence
of the average landscapes and convergence of the bootstrap. In addition, we
introduce an alternate functional summary of persistent homology, which we call
the silhouette, and derive an analogous statistical theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0309</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0309</id><created>2013-12-01</created><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Times in noise-based logic: increased dimensions of logic hyperspace</title><categories>cs.ET</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Time shifts beyond the correlation time of the logic and reference signals
create new elements that are orthogonal to the original components. This fact
can be utilized to increase the number of dimensions of the logic space while
keeping the number of reference noises fixed. Using just a single noise and
time shifts can realize exponentially large hyperspaces with large numbers of
dimensions. Other, independent applications of time shifts include holographic
noise-based logic systems and changing commutative operations into
non-commuting ones. For the sake of simplicity, these ideas are illustrated by
deterministic time shifts, even though random timing and random time shifts
would yield the most robust systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0317</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0317</id><created>2013-12-01</created><authors><author><keyname>Jiang</keyname><forenames>Chunxiao</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>K. J. Ray</forenames></author></authors><title>Evolutionary Dynamics of Information Diffusion over Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.2920</comments><doi>10.1109/JSTSP.2014.2313024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current social networks are of extremely large-scale generating tremendous
information flows at every moment. How information diffuse over social networks
has attracted much attention from both industry and academics. Most of the
existing works on information diffusion analysis are based on machine learning
methods focusing on social network structure analysis and empirical data
mining. However, the dynamics of information diffusion, which are heavily
influenced by network users' decisions, actions and their socio-economic
interactions, is generally ignored by most of existing works. In this paper, we
propose an evolutionary game theoretic framework to model the dynamic
information diffusion process in social networks. Specifically, we derive the
information diffusion dynamics in complete networks, uniform degree and
non-uniform degree networks, with the highlight of two special networks,
Erd\H{o}s-R\'enyi random network and the Barab\'asi-Albert scale-free network.
We find that the dynamics of information diffusion over these three kinds of
networks are scale-free and the same with each other when the network scale is
sufficiently large. To verify our theoretical analysis, we perform simulations
for the information diffusion over synthetic networks and real-world Facebook
networks. Moreover, we also conduct experiment on Twitter hashtags dataset,
which shows that the proposed game theoretic model can well fit and predict the
information diffusion over real social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0333</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0333</id><created>2013-12-02</created><authors><author><keyname>Shan</keyname><forenames>Hangguan</forenames></author><author><keyname>Ni</keyname><forenames>Zhifeng</forenames></author><author><keyname>Zhuang</keyname><forenames>Weihua</forenames></author><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>State Transition Analysis of Time-Frequency Resource Conversion-based
  Call Admission Control for LTE-Type Cellular Network</title><categories>cs.NI</categories><comments>10 pages</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address network congestion stemmed from traffic generated by advanced user
equipments, in [1] we propose a novel network resource allocation strategy,
time-frequency resource conversion (TFRC), via exploiting user behavior, a
specific kind of context information. Considering an LTE-type cellular network,
a call admission control policy called double-threshold guard channel policy is
proposed there to facilitate the implementation of TFRC. In this report, we
present state transition analysis of this TFRC-based call admission control
policy for an LTE-type cellular network. Overall, there are five categories of
events that can trigger a transition of the system state: 1) a new call
arrival; 2) a handoff user arrival; 3) a handoff user departure; 4) a call
termination; and 5) a periodic time-frequency resource conversion. We analyze
them case by case in this report and the validation of the analysis has been
provided in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0336</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0336</id><created>2013-12-02</created><authors><author><keyname>Nagananda</keyname><forenames>K. G.</forenames></author><author><keyname>Kishore</keyname><forenames>Shalinee</forenames></author></authors><title>A Unifying Framework for the Electrical Structure-Based Approach to PMU
  Placement in Electric Power Systems</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Smart Grid. arXiv admin note: text
  overlap with arXiv:1309.1300</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The electrical structure of the power grid is utilized to address the phasor
measurement unit (PMU) placement problem. First, we derive the connectivity
matrix of the network using the resistance distance metric and employ it in the
linear program formulation to obtain the optimal number of PMUs, for complete
network observability without zero injection measurements. This approach was
developed by the author in an earlier work, but the solution methodology to
address the location problem did not fully utilize the electrical properties of
the network, resulting in an ambiguity. In this paper, we settle this issue by
exploiting the coupling structure of the grid derived using the singular value
decomposition (SVD)-based analysis of the resistance distance matrix to solve
the location problem. Our study, which is based on recent advances in complex
networks that promote the electrical structure of the grid over its topological
structure and the SVD analysis which throws light on the electrical coupling of
the network, results in a unified framework for the electrical structure-based
PMU placement. The proposed method is tested on IEEE bus systems, and the
results uncover intriguing connections between the singular vectors and average
resistance distance between buses in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0341</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0341</id><created>2013-12-02</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>The TTC 2013 Flowgraphs Case</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 3-7</journal-ref><doi>10.4204/EPTCS.135.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This case for the Transformation Tool Contest 2013 is about evaluating the
scope and usability of transformation languages and tools for a set of four
tasks requiring very different capabilities. One task deals with typical
model-to-model transformation problem, there's a model-to-text problem, there
are two in-place transformation problems, and finally there's a task dealing
with validation of models resulting from the transformations.
  The tasks build upon each other, but the transformation case project also
provides all intermediate models, thus making it possible to skip tasks that
are not suited for a particular tool, or for parallelizing the work among
members of participating teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0342</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0342</id><created>2013-12-02</created><authors><author><keyname>Van Gorp</keyname><forenames>Pieter</forenames><affiliation>Eindhoven University of Technology</affiliation></author><author><keyname>Rose</keyname><forenames>Louis M.</forenames><affiliation>University of York</affiliation></author></authors><title>The Petri-Nets to Statecharts Transformation Case</title><categories>cs.PL</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><acm-class>I.2.2;F.4.3</acm-class><journal-ref>EPTCS 135, 2013, pp. 16-31</journal-ref><doi>10.4204/EPTCS.135.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a case study for the sixth Transformation Tool Contest.
The case is based on a mapping from Petri-Nets to statecharts (i.e., from flat
process models to hierarchical ones). The case description separates a simple
mapping phase from a phase that involves the step by step destruction Petri-Net
elements and the corresponding construction of a hierarchy of statechart
elements. Although the focus of this case study is on the comparison of the
runtime performance of solutions, we also include correctness tests as well as
bonus criteria for evaluating transformation language and tool features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0343</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0343</id><created>2013-12-02</created><authors><author><keyname>Cosentino</keyname><forenames>Valerio</forenames><affiliation>AtlanMod, INRIA &amp; Ecole des Mines de Nantes, France</affiliation></author><author><keyname>Tisi</keyname><forenames>Massimo</forenames><affiliation>AtlanMod, INRIA &amp; Ecole des Mines de Nantes, France</affiliation></author><author><keyname>B&#xfc;ttner</keyname><forenames>Fabian</forenames><affiliation>AtlanMod, INRIA &amp; Ecole des Mines de Nantes, France</affiliation></author></authors><title>Analyzing Flowgraphs with ATL</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 32-36</journal-ref><doi>10.4204/EPTCS.135.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution to the Flowgraphs case study for the
Transformation Tool Contest 2013 (TTC 2013). Starting from Java source code, we
execute a chain of model transformations to derive a simplified model of the
program, its control flow graph and its data flow graph. Finally we develop a
model transformation that validates the program flow by comparing it with a set
of flow specifications written in a domain specific language. The proposed
solution has been implemented using ATL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0344</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0344</id><created>2013-12-02</created><authors><author><keyname>Hinkel</keyname><forenames>Georg</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Goldschmidt</keyname><forenames>Thomas</forenames><affiliation>ABB Corporate Research</affiliation></author><author><keyname>Happe</keyname><forenames>Lucia</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>An NMF solution for the Flowgraphs case at the TTC 2013</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 37-42</journal-ref><doi>10.4204/EPTCS.135.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems are getting more and more complex. Model-driven engineering
(MDE) offers ways to handle such increased complexity by lifting development to
a higher level of abstraction. A key part in MDE are transformations that
transform any given model into another. These transformations are used to
generate all kinds of software artifacts from models. However, there is little
consensus about the transformation tools. Thus, the Transformation Tool Contest
(TTC) 2013 aims to compare different transformation engines. This is achieved
through three different cases that have to be tackled. One of these cases is
the Flowgraphs case. A solution has to transform a Java code model into a
simplified version and has to derive control and data flow. This paper presents
the solution for this case using NMF Transformations as transformation engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0346</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0346</id><created>2013-12-02</created><authors><author><keyname>Cuadrado</keyname><forenames>Jes&#xfa;s S&#xe1;nchez</forenames><affiliation>Universidad Aut&#xf3;noma de Madrid</affiliation></author></authors><title>Solving the Flowgraphs Case with Eclectic</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 43-56</journal-ref><doi>10.4204/EPTCS.135.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution for the Flow Graphs case of the Transformation
Tool Contest 2013, using the Eclectic model transformation tool. The solution
makes use of several languages of Eclectic, showing how it is possible to
combine them to address a non-trivial transformation problem in a concise and
modulary way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0347</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0347</id><created>2013-12-02</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Solving the TTC 2013 Flowgraphs Case with FunnyQT</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 57-68</journal-ref><doi>10.4204/EPTCS.135.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FunnyQT is a model querying and model transformation library for the
functional Lisp-dialect Clojure providing a rich and efficient querying and
transformation API.
  This paper describes the FunnyQT solution to the TTC 2013 Flowgraphs
Transformation Case. It solves all four tasks, and it has won the best
efficiency award for this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0348</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0348</id><created>2013-12-02</created><authors><author><keyname>Anjorin</keyname><forenames>Anthony</forenames><affiliation>Technische Universit&#xe4;t Darmstadt</affiliation></author><author><keyname>Lauder</keyname><forenames>Marius</forenames><affiliation>Technische Universit&#xe4;t Darmstadt</affiliation></author></authors><title>A Solution to the Flowgraphs Case Study using Triple Graph Grammars and
  eMoflon</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 69-74</journal-ref><doi>10.4204/EPTCS.135.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After 20 years of Triple Graph Grammars (TGGs) and numerous actively
maintained implementations, there is now a need for challenging examples and
success stories to show that TGGs can be used for real-world bidirectional
model transformations. Our primary goal in recent years has been to increase
the expressiveness of TGGs by providing a set of pragmatic features that allow
a controlled fallback to programmed graph transformations and Java.
  Based on the Flowgraphs case study of the Transformation Tool Contest (TTC
2013), we present (i) attribute constraints used to express complex
bidirectional attribute manipulation, (ii) binding expressions for specifying
arbitrary context relationships, and (iii) post-processing methods as a black
box extension for TGG rules. In each case, we discuss the enabled trade-off
between guaranteed formal properties and expressiveness. Our solution,
implemented with our metamodelling and model transformation tool eMoflon
(www.emoflon.org), is available as a virtual machine hosted on Share.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0349</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0349</id><created>2013-12-02</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Solving the Class Diagram Restructuring Transformation Case with FunnyQT</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 75-82</journal-ref><doi>10.4204/EPTCS.135.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FunnyQT is a model querying and model transformation library for the
functional Lisp-dialect Clojure providing a rich and efficient querying and
transformation API.
  This paper describes the FunnyQT solution to the TTC 2013 Class Diagram
Restructuring Transformation Case. This solution and the GROOVE solution share
the best overall solution award for this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0350</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0350</id><created>2013-12-02</created><authors><author><keyname>Smid</keyname><forenames>Wietse</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Rensink</keyname><forenames>Arend</forenames><affiliation>University of Twente</affiliation></author></authors><title>Class Diagram Restructuring with GROOVE</title><categories>cs.LO cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><acm-class>D.2.2; D.2.4; E.1; F.3.1</acm-class><journal-ref>EPTCS 135, 2013, pp. 83-87</journal-ref><doi>10.4204/EPTCS.135.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the GROOVE solution to the &quot;Class Diagram Restructuring&quot;
case study of the Tool Transformation Contest 2013. We show that the visual
rule formalism enables the required restructuring to be formulated in a very
concise manner. Moreover, the GROOVE functionality for state space exploration
allows checking confluence. Performance-wise, however, the solution does not
scale well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0351</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0351</id><created>2013-12-02</created><authors><author><keyname>Horn</keyname><forenames>Tassilo</forenames><affiliation>University Koblenz-Landau</affiliation></author></authors><title>Solving the Petri-Nets to Statecharts Transformation Case with FunnyQT</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 88-94</journal-ref><doi>10.4204/EPTCS.135.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FunnyQT is a model querying and model transformation library for the
functional Lisp-dialect Clojure providing a rich and efficient querying and
transformation API.
  This paper describes the FunnyQT solution to the TTC 2013 Petri-Nets to
Statcharts Transformation Case. This solution has won the best overall solution
award and the best efficiency award for this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0352</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0352</id><created>2013-12-02</created><authors><author><keyname>Lano</keyname><forenames>K.</forenames><affiliation>King's College London</affiliation></author><author><keyname>Kolahdouz-Rahimi</keyname><forenames>S.</forenames><affiliation>King's College London</affiliation></author><author><keyname>Maroukian</keyname><forenames>K.</forenames><affiliation>King's College London</affiliation></author></authors><title>Solving the Petri-Nets to Statecharts Transformation Case with UML-RSDS</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 101-105</journal-ref><doi>10.4204/EPTCS.135.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a solution to the Petri-Nets to statecharts case using
UML-RSDS. We show how a highly declarative solution which is confluent and
invertible can be given using this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0354</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0354</id><created>2013-12-02</created><authors><author><keyname>Izs&#xf3;</keyname><forenames>Benedek</forenames><affiliation>BME</affiliation></author><author><keyname>Heged&#xfc;s</keyname><forenames>&#xc1;bel</forenames><affiliation>BME</affiliation></author><author><keyname>Bergmann</keyname><forenames>G&#xe1;bor</forenames><affiliation>BME</affiliation></author><author><keyname>Horv&#xe1;th</keyname><forenames>&#xc1;kos</forenames><affiliation>BME</affiliation></author><author><keyname>R&#xe1;th</keyname><forenames>Istv&#xe1;n</forenames><affiliation>BME</affiliation></author></authors><title>PN2SC Case Study: An EMF-IncQuery solution</title><categories>cs.SE</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 106-114</journal-ref><doi>10.4204/EPTCS.135.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a solution for the Petri-Net to Statecharts case study of
the Transformation Tool Contest 2013, using EMF-IncQuery and Xtend for
implementing the model transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0355</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0355</id><created>2013-12-02</created><authors><author><keyname>Rossman</keyname><forenames>Benjamin</forenames></author></authors><title>Formulas vs. Circuits for Small Distance Connectivity</title><categories>cs.CC</categories><msc-class>68Q15</msc-class><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first super-polynomial separation in the power of bounded-depth
boolean formulas vs. circuits. Specifically, we consider the problem Distance
$k(n)$ Connectivity, which asks whether two specified nodes in a graph of size
$n$ are connected by a path of length at most $k(n)$. This problem is solvable
(by the recursive doubling technique) on {\bf circuits} of depth $O(\log k)$
and size $O(kn^3)$. In contrast, we show that solving this problem on {\bf
formulas} of depth $\log n/(\log\log n)^{O(1)}$ requires size $n^{\Omega(\log
k)}$ for all $k(n) \leq \log\log n$. As corollaries:
  (i) It follows that polynomial-size circuits for Distance $k(n)$ Connectivity
require depth $\Omega(\log k)$ for all $k(n) \leq \log\log n$. This matches the
upper bound from recursive doubling and improves a previous $\Omega(\log\log
k)$ lower bound of Beame, Pitassi and Impagliazzo [BIP98].
  (ii) We get a tight lower bound of $s^{\Omega(d)}$ on the size required to
simulate size-$s$ depth-$d$ circuits by depth-$d$ formulas for all $s(n) =
n^{O(1)}$ and $d(n) \leq \log\log\log n$. No lower bound better than
$s^{\Omega(1)}$ was previously known for any $d(n) \nleq O(1)$.
  Our proof technique is centered on a new notion of pathset complexity, which
roughly speaking measures the minimum cost of constructing a set of (partial)
paths in a universe of size $n$ via the operations of union and relational
join, subject to certain density constraints. Half of our proof shows that
bounded-depth formulas solving Distance $k(n)$ Connectivity imply upper bounds
on pathset complexity. The other half is a combinatorial lower bound on pathset
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0356</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0356</id><created>2013-12-02</created><authors><author><keyname>Mart&#xed;nez-Ruiz</keyname><forenames>Tom&#xe1;s</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>F&#xe9;lix</forenames></author><author><keyname>Piattini</keyname><forenames>Mario</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Applying AOSE Concepts to Model Crosscutting Variability in Variant-Rich
  Processes</title><categories>cs.SE</categories><comments>5 pages. The final version of this paper is available at
  http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6068365</comments><journal-ref>Proceedings of the 37th EUROMICRO Conference on Software
  Engineering and Advanced Applications (SEAA 2011)</journal-ref><doi>10.1109/SEAA.2011.58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software process models need to be variant-rich, in the sense that they
should be systematically customizable to specific project goals and project
environments. It is currently very difficult to model Variant-Rich Process
(VRP) because variability mechanisms are largely missing in modern process
modeling languages. Variability mechanisms from other domains, such as
programming languages, might be suitable for the representation of variability
and could be adapted to the modeling of software processes. Mechanisms from
Software Product Line Engineering (SPLE) and concepts from Aspect- Oriented
Software Engineering (AOSE) show particular promise when modeling variability.
This paper presents an approach that integrates variability concepts from SPLE
and AOSE in the design of a VRP approach for the systematic support of
tailoring in software processes. This approach has also been implemented in
SPEM, resulting in the vSPEM notation. It has been used in a pilot application,
which indicates that our approach based on AOSE can make process tailoring
easier and more productive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0363</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0363</id><created>2013-12-02</created><updated>2014-06-26</updated><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Optimal Stochastic Coordinated Beamforming for Wireless Cooperative
  Networks with CSI Uncertainty</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmit optimization and resource allocation for wireless cooperative
networks with channel state information (CSI) uncertainty are important but
challenging problems in terms of both the uncertainty modeling and performance
op- timization. In this paper, we establish a generic stochastic coordinated
beamforming (SCB) framework that provides flex- ibility in the channel
uncertainty modeling, while guaranteeing optimality in the transmission
strategies. We adopt a general stochastic model for the CSI uncertainty, which
is applicable for various practical scenarios. The SCB problem turns out to be
a joint chance constrained program (JCCP) and is known to be highly
intractable. In contrast to all the previous algo- rithms for JCCP that can
only find feasible but sub-optimal solutions, we propose a novel stochastic DC
(difference-of-convex) programming algorithm with optimality guarantee, which
can serve as the benchmark for evaluating heuristic and sub-optimal algorithms.
The key observation is that the highly intractable probability constraint can
be equivalently reformulated as a DC constraint. This further enables efficient
algorithms to achieve optimality. Simulation results will illustrate the
convergence, conservativeness, stability and performance gains of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0372</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0372</id><created>2013-12-02</created><authors><author><keyname>Fossorier</keyname><forenames>Marc</forenames></author></authors><title>Polar Codes: Graph Representation and Duality</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an iterative construction of a polar code and
develop properties of the dual of a polar code. Based on this approach, belief
propagation of a polar code can be presented in the context of low-density
parity check codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0378</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0378</id><created>2013-12-02</created><updated>2014-04-07</updated><authors><author><keyname>Spirkl</keyname><forenames>Sophie</forenames></author></authors><title>The guillotine approach for TSP with neighborhoods revisited</title><categories>cs.CG</categories><msc-class>90C27 68W25 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Euclidean TSP with neighborhoods (TSPN) is the following problem: Given a
set R of k regions, find a shortest tour that visits at least one point from
each region. We study the special cases of disjoint, connected, alpha-fat
regions (i.e., every region P contains a disk of diameter diam(P)/alpha) and
disjoint unit disks.
  For the latter, Dumitrescu and Mitchell proposed an algorithm based on
Mitchell's guillotine subdivision approach for the Euclidean TSP and claimed it
to be a PTAS. However, their proof contains a severe gap, which we will close
in the following. Bodlaender et al. remark that their techniques for the
minimum corridor connection problem carry over to the TSPN and yield an
alternative PTAS for this problem.
  For disjoint connected alpha-fat regions of varying size, Mitchell proposed a
slightly different PTAS candidate. We will expose several further problems and
gaps in this approach. Some of them we can close, but overall, for alpha-fat
regions, the existence of a PTAS for the TSPN remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0387</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0387</id><created>2013-12-02</created><updated>2015-02-26</updated><authors><author><keyname>Ashok</keyname><forenames>Pradeesha</forenames></author><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author></authors><title>On Strong Centerpoints</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in $\mathbb{R}^d$ and $\mathcal{F}$ be a
family of geometric objects. We call a point $x \in P$ a strong centerpoint of
$P$ w.r.t $\mathcal{F}$ if $x$ is contained in all $F \in \mathcal{F}$ that
contains more than $cn$ points from $P$, where $c$ is a fixed constant. A
strong centerpoint does not exist even when $\mathcal{F}$ is the family of
halfspaces in the plane. We prove the existence of strong centerpoints with
exact constants for convex polytopes defined by a fixed set of orientations. We
also prove the existence of strong centerpoints for abstract set systems with
bounded intersection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0389</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0389</id><created>2013-12-02</created><authors><author><keyname>Ghasemalizadeh</keyname><forenames>Hossein</forenames></author><author><keyname>Razzazi</keyname><forenames>Mohammadreza</forenames></author></authors><title>Output sensitive algorithm for covering many points</title><categories>cs.CG</categories><comments>9 Pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of points and a positive integer $m$ are given and our goal is to cover
the maximum number of these point with $m$ disks. We devise the first output
sensitive algorithm for this problem. We introduce a parameter $\rho$ as the
maximum number of points that one disk can cover. In this paper first we solve
the problem for $m=2$ in $O({n\rho} + {\rho ^3}\log \rho ))$ time. The previous
algorithm for this problem runs in $O({n^3}\log n)$ time. Our algorithm
outperforms the previous algorithm because $\rho$ is much smaller than $n$ in
many cases. Then we extend the algorithm for any value of $m$ and we solve the
problem in $O(m{n\rho} + {(m\rho )^{2m - 1}}\log m\rho )$ time. The previous
algorithm for this problem runs in $O({n^{2m - 1}}\log n)$ time. Our algorithm
runs faster than the previous algorithm because $m\rho$ is smaller than $n$ in
many cases. Our technique to obtain an output sensitive algorithm is to use a
greedy algorithm to confine the areas that we should search to obtain the
result. Our technique in this paper may be applicable in other set covering
problems that deploy a greedy algorithm, to obtain faster solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0403</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0403</id><created>2013-12-02</created><authors><author><keyname>Wang</keyname><forenames>Junyuan</forenames></author><author><keyname>Dai</keyname><forenames>Lin</forenames></author></authors><title>Asymptotic Rate Analysis of Downlink Multi-user Systems with Co-located
  and Distributed Antennas</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A great deal of efforts have been made on the performance evaluation of
distributed antenna systems (DASs). Most of them assume a regular base-station
(BS) antenna layout where the number of BS antennas is usually small. With the
growing interest in cellular systems with large antenna arrays at BSs, it
becomes increasingly important for us to study how the BS antenna layout
affects the rate performance when a massive number of BS antennas are employed.
  This paper presents a comparative study of the asymptotic rate performance of
downlink multi-user systems with multiple BS antennas either co-located or
uniformly distributed within a circular cell. Two representative linear
precoding schemes, maximum ratio transmission (MRT) and zero-forcing
beamforming (ZFBF), are considered, with which the effect of BS antenna layout
on the rate performance is characterized. The analysis shows that as the number
of BS antennas $L$ and the number of users $K$ grow infinitely while
$L/K{\rightarrow}\upsilon$, the asymptotic average user rates with the
co-located antenna (CA) layout for both MRT and ZFBF are logarithmic functions
of the ratio $\upsilon$. With the distributed antenna (DA) layout, in contrast,
the scaling behavior of the average user rate closely depends on the precoding
schemes. With ZFBF, for instance, the average user rate grows unboundedly as
$L, K{\rightarrow} \infty$ and $L/K{\rightarrow}\upsilon{&gt;}1$, which indicates
that substantial rate gains over the CA layout can be achieved when the number
of BS antennas $L$ is large. The gain, nevertheless, becomes marginal when MRT
is adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0412</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0412</id><created>2013-12-02</created><authors><author><keyname>Bleier</keyname><forenames>Arnim</forenames></author></authors><title>Practical Collapsed Stochastic Variational Inference for the HDP</title><categories>cs.LG</categories><comments>NIPS Workshop; Topic Models: Computation, Application, and Evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances have made it feasible to apply the stochastic variational
paradigm to a collapsed representation of latent Dirichlet allocation (LDA).
While the stochastic variational paradigm has successfully been applied to an
uncollapsed representation of the hierarchical Dirichlet process (HDP), no
attempts to apply this type of inference in a collapsed setting of
non-parametric topic modeling have been put forward so far. In this paper we
explore such a collapsed stochastic variational Bayes inference for the HDP.
The proposed online algorithm is easy to implement and accounts for the
inference of hyper-parameters. First experiments show a promising improvement
in predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0451</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0451</id><created>2013-12-02</created><updated>2014-01-21</updated><authors><author><keyname>Berend</keyname><forenames>Daniel</forenames></author><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author></authors><title>Consistency of weighted majority votes</title><categories>math.PR cs.LG stat.ML</categories><msc-class>60C05, 60F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classical decision-theoretic problem of weighted expert voting
from a statistical learning perspective. In particular, we examine the
consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush
weighted majority and related rules. In the case of known expert competence
levels, we give sharp error estimates for the optimal rule. When the competence
levels are unknown, they must be empirically estimated. We provide frequentist
and Bayesian analyses for this situation. Some of our proof techniques are
non-standard and may be of independent interest. The bounds we derive are
nearly optimal, and several challenging open problems are posed. Experimental
results are provided to illustrate the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0453</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0453</id><created>2013-12-02</created><updated>2013-12-28</updated><authors><author><keyname>Dubey</keyname><forenames>Abhishek</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>Comprehensive Border Bases for Zero Dimensional Parametric Polynomial
  Ideals</title><categories>cs.SC</categories><comments>15 pages, 8 sections and 3 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the idea of comprehensive Gr\&quot;{o}bner bases given by
Weispfenning (1992) to border bases for zero dimensional parametric polynomial
ideals. For this, we introduce a notion of comprehensive border bases and
border system, and prove their existence even in the cases where they do not
correspond to any term order. We further present algorithms to compute
comprehensive border bases and border system. Finally, we study the relation
between comprehensive Gr\&quot;{o}bner bases and comprehensive border bases w.r.t. a
term order and give an algorithm to compute such comprehensive border bases
from comprehensive Gr\&quot;{o}bner bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0455</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0455</id><created>2013-12-02</created><authors><author><keyname>Kupriianova</keyname><forenames>O.</forenames></author><author><keyname>Lauter</keyname><forenames>Ch.</forenames></author><author><keyname>Muller</keyname><forenames>J. -M.</forenames></author></authors><title>Radix Conversion for IEEE754-2008 Mixed Radix Floating-Point Arithmetic</title><categories>cs.MS</categories><doi>10.1109/ACSSC.2013.6810471</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversion between binary and decimal floating-point representations is
ubiquitous. Floating-point radix conversion means converting both the exponent
and the mantissa. We develop an atomic operation for FP radix conversion with
simple straight-line algorithm, suitable for hardware design. Exponent
conversion is performed with a small multiplication and a lookup table. It
yields the correct result without error. Mantissa conversion uses a few
multiplications and a small lookup table that is shared amongst all types of
conversions. The accuracy changes by adjusting the computing precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0461</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0461</id><created>2013-12-02</created><authors><author><keyname>Ortac</keyname><forenames>Alper</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Mezini</keyname><forenames>Mira</forenames></author></authors><title>Abmash: Mashing Up Legacy Web Applications by Automated Imitation of
  Human Actions</title><categories>cs.SE</categories><comments>Software: Practice and Experience (2013) -</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many business web-based applications do not offer applications programming
interfaces (APIs) to enable other applications to access their data and
functions in a programmatic manner. This makes their composition difficult (for
instance to synchronize data between two applications). To address this
challenge, this paper presents Abmash, an approach to facilitate the
integration of such legacy web applications by automatically imitating human
interactions with them. By automatically interacting with the graphical user
interface (GUI) of web applications, the system supports all forms of
integrations including bi-directional interactions and is able to interact with
AJAX-based applications. Furthermore, the integration programs are easy to
write since they deal with end-user, visual user-interface elements. The
integration code is simple enough to be called a &quot;mashup&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0462</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0462</id><created>2013-12-02</created><authors><author><keyname>Cheng</keyname><forenames>Jin-San</forenames></author><author><keyname>Jin</keyname><forenames>Kai</forenames></author></authors><title>A Generic Position Based Method for Real Root Isolation of
  Zero-Dimensional Polynomial Systems</title><categories>cs.SC</categories><comments>24 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve the local generic position method for isolating the real roots of
a zero-dimensional bivariate polynomial system with two polynomials and extend
the method to general zero-dimensional polynomial systems. The method mainly
involves resultant computation and real root isolation of univariate polynomial
equations. The roots of the system have a linear univariate representation. The
complexity of the method is $\tilde{O}_B(N^{10})$ for the bivariate case, where
$N=\max(d,\tau)$, $d$ resp., $\tau$ is an upper bound on the degree, resp., the
maximal coefficient bitsize of the input polynomials. The algorithm is
certified with probability 1 in the multivariate case. The implementation shows
that the method is efficient, especially for bivariate polynomial systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0465</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0465</id><created>2013-12-02</created><authors><author><keyname>Warrender</keyname><forenames>Jennifer D.</forenames></author><author><keyname>Lord</keyname><forenames>Phillip</forenames></author></authors><title>A pattern-driven approach to biomedical ontology engineering</title><categories>cs.CE cs.DL</categories><comments>13 pages, submitted to SWAT4LS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing ontologies can be expensive, time-consuming, as well as difficult
to develop and maintain. This is especially true for more expressive and/or
larger ontologies. Some ontologies are, however, relatively repetitive, reusing
design patterns; building these with both generic and bespoke patterns should
reduce duplication and increase regularity which in turn should impact on the
cost of development.
  Here we report on the usage of patterns applied to two biomedical ontologies:
firstly a novel ontology for karyotypes which has been built ground-up using a
pattern based approach; and, secondly, our initial refactoring of the SIO
ontology to make explicit use of patterns at development time. To enable this,
we use the Tawny-OWL library which enables full-programmatic development of
ontologies. We show how this approach can generate large numbers of classes
from much simpler data structures which is highly beneficial within biomedical
ontology engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0466</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0466</id><created>2013-12-02</created><updated>2014-03-07</updated><authors><author><keyname>Mokhov</keyname><forenames>Serguei A.</forenames></author></authors><title>Intensional Cyberforensics</title><categories>cs.CR cs.DC cs.LO cs.NI cs.PL</categories><comments>412 pages, 94 figures, 18 tables, 19 algorithms and listings; PhD
  thesis; v2 corrects some typos and refs; also available on Spectrum at
  http://spectrum.library.concordia.ca/977460/</comments><msc-class>03B15, 03D65, 68Q15, 68Q55, 68Q60</msc-class><acm-class>D.3; K.6; D.2; C.2; H.3; I.5; I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on the application of intensional logic to cyberforensic
analysis and its benefits and difficulties are compared with the
finite-state-automata approach. This work extends the use of the intensional
programming paradigm to the modeling and implementation of a cyberforensics
investigation process with backtracing of event reconstruction, in which
evidence is modeled by multidimensional hierarchical contexts, and proofs or
disproofs of claims are undertaken in an eductive manner of evaluation. This
approach is a practical, context-aware improvement over the finite state
automata (FSA) approach we have seen in previous work. As a base implementation
language model, we use in this approach a new dialect of the Lucid programming
language, called Forensic Lucid, and we focus on defining hierarchical contexts
based on intensional logic for the distributed evaluation of cyberforensic
expressions. We also augment the work with credibility factors surrounding
digital evidence and witness accounts, which have not been previously modeled.
The Forensic Lucid programming language, used for this intensional
cyberforensic analysis, formally presented through its syntax and operational
semantics. In large part, the language is based on its predecessor and
codecessor Lucid dialects, such as GIPL, Indexical Lucid, Lucx, Objective
Lucid, and JOOIP bound by the underlying intensional programming paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0482</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0482</id><created>2013-11-27</created><authors><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Yih</keyname><forenames>Wen-tau</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Learning Semantic Representations for the Phrase Translation Model</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel semantic-based phrase translation model. A pair
of source and target phrases are projected into continuous-valued vector
representations in a low-dimensional latent semantic space, where their
translation score is computed by the distance between the pair in this new
space. The projection is performed by a multi-layer neural network whose
weights are learned on parallel training data. The learning is aimed to
directly optimize the quality of end-to-end machine translation results.
Experimental evaluation has been performed on two Europarl translation tasks,
English-French and German-English. The results show that the new semantic-based
phrase translation model significantly improves the performance of a
state-of-the-art phrase-based statistical machine translation sys-tem, leading
to a gain of 0.7-1.0 BLEU points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0485</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0485</id><created>2013-12-02</created><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author><author><keyname>Kruger</keyname><forenames>Anton</forenames></author></authors><title>Precise Semidefinite Programming Formulation of Atomic Norm Minimization
  for Recovering d-Dimensional ($d\geq 2$) Off-the-Grid Frequencies</title><categories>cs.IT math.IT math.OC stat.ML</categories><comments>4 pages, double-column,1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research in off-the-grid compressed sensing (CS) has demonstrated
that, under certain conditions, one can successfully recover a spectrally
sparse signal from a few time-domain samples even though the dictionary is
continuous. In particular, atomic norm minimization was proposed in
\cite{tang2012csotg} to recover $1$-dimensional spectrally sparse signal.
However, in spite of existing research efforts \cite{chi2013compressive}, it
was still an open problem how to formulate an equivalent positive semidefinite
program for atomic norm minimization in recovering signals with $d$-dimensional
($d\geq 2$) off-the-grid frequencies. In this paper, we settle this problem by
proposing equivalent semidefinite programming formulations of atomic norm
minimization to recover signals with $d$-dimensional ($d\geq 2$) off-the-grid
frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0489</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0489</id><created>2013-12-02</created><authors><author><keyname>Desmet</keyname><forenames>Antoine</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Capacity Based Evacuation with Dynamic Exit Signs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exit paths in buildings are designed to minimise evacuation time when the
building is at full capacity. We present an evacuation support system which
does this regardless of the number of evacuees. The core concept is to even-out
congestion in the building by diverting evacuees to less-congested paths in
order to make maximal usage of all accessible routes throughout the entire
evacuation process. The system issues a set of flow-optimal routes using a
capacity-constrained routing algorithm which anticipates evolutions in path
metrics using the concept of &quot;future capacity reservation&quot;. In order to direct
evacuees in an intuitive manner whilst implementing the routing algorithm's
scheme, we use dynamic exit signs, i.e. whose pointing direction can be
controlled. To make this system practical and minimise reliance on sensors
during the evacuation, we use an evacuee mobility model and make several
assumptions on the characteristics of the evacuee flow. We validate this
concept using simulations, and show how the underpinning assumptions may limit
the system's performance, especially in low-headcount evacuations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0493</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0493</id><created>2013-12-02</created><authors><author><keyname>&#x130;rsoy</keyname><forenames>Ozan</forenames></author><author><keyname>Cardie</keyname><forenames>Claire</forenames></author></authors><title>Bidirectional Recursive Neural Networks for Token-Level Labeling with
  Structure</title><categories>cs.LG cs.CL stat.ML</categories><comments>9 pages, 5 figures, NIPS Deep Learning Workshop 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep architectures, such as recurrent and recursive neural networks
have been successfully applied to various natural language processing tasks.
Inspired by bidirectional recurrent neural networks which use representations
that summarize the past and future around an instance, we propose a novel
architecture that aims to capture the structural information around an input,
and use it to label instances. We apply our method to the task of opinion
expression extraction, where we employ the binary parse tree of a sentence as
the structure, and word vector representations as the initial representation of
a single token. We conduct preliminary experiments to investigate its
performance and compare it to the sequential approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0496</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0496</id><created>2013-12-02</created><updated>2014-05-19</updated><authors><author><keyname>Gajser</keyname><forenames>David</forenames></author></authors><title>Verifying whether One-Tape Non-Deterministic Turing Machines Run in Time
  $Cn+D$</title><categories>cs.CC cs.FL</categories><comments>12 pages + 5 pages appendix</comments><journal-ref>Theoretical Computer Science, Volume 600, p. 86-97, 2015</journal-ref><doi>10.1016/j.tcs.2015.07.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the following family of problems, parameterized by integers $C\geq
2$ and $D\geq 1$: Does a given one-tape non-deterministic $q$-state Turing
machine make at most $Cn+D$ steps on all computations on all inputs of length
$n$, for all $n$?
  Assuming a fixed tape and input alphabet, we show that these problems are
co-NP-complete and we provide good non-deterministic and co-non-deterministic
lower bounds. Specifically, these problems can not be solved in
$o(q^{(C-1)/4})$ non-deterministic time by multi-tape Turing machines. We also
show that the complements of these problems can be solved in $O(q^{C+2})$
non-deterministic time and not in $o(q^{(C-1)/2})$ non-deterministic time by
multi-tape Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0497</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0497</id><created>2013-12-02</created><authors><author><keyname>Hellweg</keyname><forenames>Frank</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Property-Testing in Sparse Directed Graphs: 3-Star-Freeness and
  Connectivity</title><categories>cs.DS</categories><comments>Results partly published at ESA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study property testing in directed graphs in the bounded degree model,
where we assume that an algorithm may only query the outgoing edges of a
vertex, a model proposed by Bender and Ron in 2002. As our first main result,
we we present a property testing algorithm for strong connectivity in this
model, having a query complexity of $\mathcal{O}(n^{1-\epsilon/(3+\alpha)})$
for arbitrary $\alpha&gt;0$; it is based on a reduction to estimating the vertex
indegree distribution. For subgraph-freeness we give a property testing
algorithm with a query complexity of $\mathcal{O}(n^{1-1/k})$, where $k$ is the
number of connected componentes in the queried subgraph which have no incoming
edge. We furthermore take a look at the problem of testing whether a weakly
connected graph contains vertices with a degree of least $3$, which can be
viewed as testing for freeness of all orientations of $3$-stars; as our second
main result, we show that this property can be tested with a query complexity
of $\mathcal{O}(\sqrt{n})$ instead of, what would be expected,
$\Omega(n^{2/3})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0499</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0499</id><created>2013-12-02</created><updated>2014-04-24</updated><authors><author><keyname>Scouarnec</keyname><forenames>Nicolas Le</forenames></author><author><keyname>Neumann</keyname><forenames>Christoph</forenames></author><author><keyname>Straub</keyname><forenames>Gilles</forenames></author></authors><title>Cache policies for cloud-based systems: To keep or not to keep</title><categories>cs.NI cs.DC</categories><comments>Proceedings of IEEE International Conference on Cloud Computing 2014
  (CLOUD 14)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study cache policies for cloud-based caching. Cloud-based
caching uses cloud storage services such as Amazon S3 as a cache for data items
that would have been recomputed otherwise. Cloud-based caching departs from
classical caching: cloud resources are potentially infinite and only paid when
used, while classical caching relies on a fixed storage capacity and its main
monetary cost comes from the initial investment. To deal with this new context,
we design and evaluate a new caching policy that minimizes the overall cost of
a cloud-based system. The policy takes into account the frequency of
consumption of an item and the cloud cost model. We show that this policy is
easier to operate, that it scales with the demand and that it outperforms
classical policies managing a fixed capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0510</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0510</id><created>2013-11-21</created><updated>2014-01-13</updated><authors><author><keyname>Demichev</keyname><forenames>A.</forenames></author><author><keyname>Ilyin</keyname><forenames>V.</forenames></author><author><keyname>Kryukov</keyname><forenames>A.</forenames></author><author><keyname>Polyakov</keyname><forenames>S.</forenames></author></authors><title>Fault Tolerance of Small-World Regular and Stochastic Interconnection
  Networks</title><categories>cs.SI cs.DC physics.soc-ph</categories><comments>9 pages, 5 figures; LaTeX; typos corrected, figures improved,
  acknowledgements added</comments><journal-ref>Vychisl. Metody Programm (Numerical Methods and Programming)
  vol.15 (2014) 36 - 48 (in Russian)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resilience of the most important properties of stochastic and regular
(deterministic) small-world interconnection networks is studied. It is shown
that in the broad range of values of the fraction of faulty nodes the networks
under consideration possess high fault tolerance, the deterministic networks
being slightly better than the stochastic ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0512</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0512</id><created>2013-12-02</created><updated>2014-03-13</updated><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Karl</keyname><forenames>W. Clem</forenames></author></authors><title>Sensing-Aware Kernel SVM</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for designing kernels for support vector machines
(SVMs) when the class label is linked to the observation through a latent state
and the likelihood function of the observation given the state (the sensing
model) is available. We show that the Bayes-optimum decision boundary is a
hyperplane under a mapping defined by the likelihood function. Combining this
with the maximum margin principle yields kernels for SVMs that leverage
knowledge of the sensing model in an optimal way. We derive the optimum kernel
for the bag-of-words (BoWs) sensing model and demonstrate its superior
performance over other kernels in document and image classification tasks.
These results indicate that such optimum sensing-aware kernel SVMs can match
the performance of rather sophisticated state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0516</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0516</id><created>2013-12-02</created><updated>2014-02-13</updated><authors><author><keyname>Kekatos</keyname><forenames>Vassilis</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author></authors><title>Grid Topology Identification using Electricity Prices</title><categories>cs.LG cs.SY stat.AP stat.ML</categories><comments>PES General Meeting 2014 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of recovering the topology of a grid using solely publicly
available market data is explored here. In contemporary whole-sale electricity
markets, real-time prices are typically determined by solving the
network-constrained economic dispatch problem. Under a linear DC model,
locational marginal prices (LMPs) correspond to the Lagrange multipliers of the
linear program involved. The interesting observation here is that the matrix of
spatiotemporally varying LMPs exhibits the following property: Once
premultiplied by the weighted grid Laplacian, it yields a low-rank and sparse
matrix. Leveraging this rich structure, a regularized maximum likelihood
estimator (MLE) is developed to recover the grid Laplacian from the LMPs. The
convex optimization problem formulated includes low rank- and
sparsity-promoting regularizers, and it is solved using a scalable algorithm.
Numerical tests on prices generated for the IEEE 14-bus benchmark provide
encouraging topology recovery results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0520</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0520</id><created>2013-12-02</created><authors><author><keyname>Keepanasseril</keyname><forenames>Arun</forenames></author><author><keyname>McKibbon</keyname><forenames>Kathleen Ann</forenames></author><author><keyname>Iorio</keyname><forenames>Alfonso</forenames></author></authors><title>Theoretical Foundation for Research in Communication using Information
  and Communication Technology Devices in Healthcare: An Interdisciplinary
  Scoping Review</title><categories>cs.OH</categories><comments>Pages: 35</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faulty communication between team members is one of the most important
factors preventing substantial improvement in patient safety. Aviation, nuclear
power and defense have been able to improve their safety record by adopting
theory and model based solutions. In contrast, healthcare's thrust towards
modern communication devices is largely devoid of theoretical foundation. The
objective of this scoping review is to compile communication theories,
frameworks, and models used by high risk organizations outside healthcare to
study and resolve workplace communication issues. The healthcare databases
searched included Medline, CINAHL, EMBASE, and PsycInfo. In addition, we
searched engineering and science literature to include articles in the fields
of information sciences, computer sciences, nuclear power generation, aviation,
the military and other domains such as sociology that address the science and
theory of communication. Comprehensive searching was also done in the
communication studies literature. We also reviewed conference proceedings and
grey literature and conducted citation tracking. Our initial systematic search
yielded 15,365 articles. Hand searching and reviewing references resulted in a
set of 181 articles. 144 full text articles were read and 40 of them were
selected to be included in the review. We were able to identify 14 theories and
12 models which could be applied in hospital communication research. However,
it must be noted that most of them have not yet been applied in biomedical
research in hospital communication and as such their applicability can only be
suggested-a gap which future research may be able to address. Formulation of a
custom model representing the unique features and complexities of communication
within hospitals is recommended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0522</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0522</id><created>2013-12-02</created><authors><author><keyname>Kaufman</keyname><forenames>Brett</forenames></author><author><keyname>Lilleberg</keyname><forenames>Jorma</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Analog Baseband Cancellation for Full-Duplex: An Experiment Driven
  Analysis</title><categories>cs.NI</categories><comments>22 pages, 12 figures, submitted to IEEE JSAC special issue on
  Full-Duplex</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent wireless testbed implementations have proven that full-duplex
communication is in fact possible and can outperform half-duplex systems. Many
of these implementations modify existing half-duplex systems to operate in
full-duplex. To realize the full potential of full-duplex, radios need to be
designed with self-interference in mind. In our work, we use a novel patch
antenna prototype in an experimental setup to characterize the
self-interference channel between transmit and receive radios. We derive an
equivalent analytical baseband model and propose analog baseband cancellation
techniques to complement the RF cancellation provided by the patch antenna
prototype. Our results show that a wide bandwidth, moderate isolation scheme
achieves up to 2.4 bps/Hz higher achievable rate than a narrow bandwidth, high
isolation scheme. Furthermore, the analog baseband cancellation yields a
10-10,000 improvement in BER over RF only cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0525</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0525</id><created>2013-12-02</created><authors><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Near Optimal Compressed Sensing of Sparse Rank-One Matrices via Sparse
  Power Factorization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing of simultaneously sparse and rank-one matrices enables
recovery of sparse signals from a few linear measurements of their bilinear
form. One important question is how many measurements are needed for a stable
reconstruction in the presence of measurement noise. Unlike the conventional
compressed sensing for sparse vectors, where convex relaxation via the
$\ell_1$-norm achieves near optimal performance, for compressed sensing of
sparse and rank-one matrices, recently it has been shown by Oymak et al. that
convex programmings using the nuclear norm and the mixed norm are highly
suboptimal even in the noise-free scenario.
  We propose an alternating minimization algorithm called sparse power
factorization (SPF) for compressed sensing of sparse rank-one matrices.
Starting from a particular initialization, SPF achieves stable recovery and
requires number of measurements within a logarithmic factor of the
information-theoretic fundamental limit. For fast-decaying sparse signals, SPF
starting from an initialization with low computational cost also achieves
stable reconstruction with the same number of measurements. Numerical results
show that SPF empirically outperforms the best known combinations of mixed norm
and nuclear norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0526</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0526</id><created>2013-12-02</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Boldi</keyname><forenames>Paolo</forenames></author><author><keyname>Ottaviano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Venturini</keyname><forenames>Rossano</forenames></author><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Cache-Oblivious Peeling of Random Hypergraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of a peeling order in a randomly generated hypergraph is the
most time-consuming step in a number of constructions, such as perfect hashing
schemes, random $r$-SAT solvers, error-correcting codes, and approximate set
encodings. While there exists a straightforward linear time algorithm, its poor
I/O performance makes it impractical for hypergraphs whose size exceeds the
available internal memory.
  We show how to reduce the computation of a peeling order to a small number of
sequential scans and sorts, and analyze its I/O complexity in the
cache-oblivious model. The resulting algorithm requires $O(\mathrm{sort}(n))$
I/Os and $O(n \log n)$ time to peel a random hypergraph with $n$ edges.
  We experimentally evaluate the performance of our implementation of this
algorithm in a real-world scenario by using the construction of minimal perfect
hash functions (MPHF) as our test case: our algorithm builds a MPHF of $7.6$
billion keys in less than $21$ hours on a single machine. The resulting data
structure is both more space-efficient and faster than that obtained with the
current state-of-the-art MPHF construction for large-scale key sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0579</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0579</id><created>2013-12-02</created><authors><author><keyname>Grubb</keyname><forenames>Alexander</forenames></author><author><keyname>Munoz</keyname><forenames>Daniel</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author></authors><title>SpeedMachines: Anytime Structured Prediction</title><categories>cs.LG</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured prediction plays a central role in machine learning applications
from computational biology to computer vision. These models require
significantly more computation than unstructured models, and, in many
applications, algorithms may need to make predictions within a computational
budget or in an anytime fashion. In this work we propose an anytime technique
for learning structured prediction that, at training time, incorporates both
structural elements and feature computation trade-offs that affect test-time
inference. We apply our technique to the challenging problem of scene
understanding in computer vision and demonstrate efficient and anytime
predictions that gradually improve towards state-of-the-art classification
performance as the allotted time increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0596</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0596</id><created>2013-12-02</created><authors><author><keyname>Hinkel</keyname><forenames>Georg</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Goldschmidt</keyname><forenames>Thomas</forenames><affiliation>ABB Corporate Research</affiliation></author><author><keyname>Happe</keyname><forenames>Lucia</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>An NMF solution for the Petri Nets to State Charts case study at the TTC
  2013</title><categories>cs.SE cs.PL</categories><comments>In Proceedings TTC 2013, arXiv:1311.7536. arXiv admin note:
  substantial text overlap with arXiv:1312.0344</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 135, 2013, pp. 95-100</journal-ref><doi>10.4204/EPTCS.135.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems are getting more and more complex. Model-driven engineering
(MDE) offers ways to handle such increased complexity by lifting development to
a higher level of abstraction. A key part in MDE are transformations that
transform any given model into another. These transformations are used to
generate all kinds of software artifacts from models. However, there is little
consensus about the transformation tools. Thus, the Transformation Tool Contest
(TTC) 2013 aims to compare different transformation engines. This is achieved
through three different cases that have to be tackled. One of these cases is
the Petri Net to State Chart case. A solution has to transform a Petri Net to a
State Chart and has to derive a hierarchical structure within the State Chart.
This paper presents the solution for this case using NMF Transformations as
transformation engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0617</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0617</id><created>2013-11-22</created><authors><author><keyname>Zheng</keyname><forenames>Yi</forenames></author><author><keyname>He</keyname><forenames>Zhi-Zhu</forenames></author><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Jing</forenames></author></authors><title>Fully Automatic Liquid Metal Printer towards Personal Electronics
  Manufacture</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Printed electronics is quickly emerging with tremendous value in a wide
variety of latest electrical engineering areas. However, restricted to the
rather limited conductive inks and printing strategies, the currently existing
electronics manufacturing tools are generally complicated, expensive, time,
material, water and energy consuming, and thus mainly restricted to the
industrial use. Here from an alternative way, the present article demonstrated
for the first time an entirely automatic printing system for personal
electronics manufacturing through introducing a composite liquid metal ink
delivery and printing mechanism to overcome the large surface tension facing
the solution, and integrating it with the notebook computer controlling
algorithm. With the developed printer, any desired electronically conductive
patterns spanning from single wires to various complex structures like
integrated circuit (IC), antenna, PCB, RFID, electronic greeting cards,
decoration arts, classical buildings (White House, Great Wall etc.) or more
do-it-yourself (DIY) circuits were demonstrated to be printed out in a moment
with high precision. And the total cost for the whole system has reached
personal affordability, which is hard to offer by so far the state of the art
technologies. Some fundamental fluid dynamics mechanisms related to the
proposed tapping mode enabled reliable printing and adhesion of the liquid
metal electronics on the flexible substrate was systematically disclosed
through theoretical interpretation and experimental measurements. This clearly
beyond-the-lab technology and pervasively available liquid metal printer opens
the way for large scale home level electronics making in the coming time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0624</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0624</id><created>2013-12-02</created><updated>2013-12-13</updated><authors><author><keyname>Shalit</keyname><forenames>Uri</forenames></author><author><keyname>Chechik</keyname><forenames>Gal</forenames></author></authors><title>Efficient coordinate-descent for orthogonal matrices through Givens
  rotations</title><categories>cs.LG stat.ML</categories><comments>A shorter version of this paper will appear in the proceedings of the
  31st International Conference for Machine Learning (ICML 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing over the set of orthogonal matrices is a central component in
problems like sparse-PCA or tensor decomposition. Unfortunately, such
optimization is hard since simple operations on orthogonal matrices easily
break orthogonality, and correcting orthogonality usually costs a large amount
of computation. Here we propose a framework for optimizing orthogonal matrices,
that is the parallel of coordinate-descent in Euclidean spaces. It is based on
{\em Givens-rotations}, a fast-to-compute operation that affects a small number
of entries in the learned matrix, and preserves orthogonality. We show two
applications of this approach: an algorithm for tensor decomposition that is
used in learning mixture models, and an algorithm for sparse-PCA. We study the
parameter regime where a Givens rotation approach converges faster and achieves
a superior model on a genome-wide brain-wide mRNA expression dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0629</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0629</id><created>2013-12-02</created><authors><author><keyname>Almajadub</keyname><forenames>Fatma</forenames></author><author><keyname>Razaque</keyname><forenames>Abdul</forenames></author><author><keyname>Fattah</keyname><forenames>Eman Abdel</forenames></author></authors><title>Stream Control Transmission Protocol (SCTP): Robust and Efficient for
  Data Centre Applications</title><categories>cs.NI</categories><comments>05 PAGES AND 08 FIGURES</comments><report-no>A156</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to rapid advancement in modern technology, as one of the major concerns
is the stability of business. The organizations depend on their systems to
provide robust and faster processing of information for their operations.
Efficient data centers are key sources to handle these operations. If the
organizational system is not fully functional, the performance of organization
may be impaired or clogged completely. With the developments of real-time
applications into data centers for data communications, there is a need to use
an alternative of the standard TCP protocol to provide reliable data transfer.
Stream Control Transmission Protocol (SCTP) consists of several well built-in
characteristics that make it capable to work efficiently with real-time
applications. In this paper, we evaluate an optimized version of STCP. The
optimized version of SCTP is tested against a non optimized version of STCP and
TCP in a data center environment. Simulations of the protocols are carried
using NS2 simulator
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0631</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0631</id><created>2013-12-02</created><authors><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Allahverdyan</keyname><forenames>Armen E.</forenames></author></authors><title>Phase Transitions in Community Detection: A Solvable Toy Model</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph stat.ML</categories><comments>6 pages, 6 figures</comments><doi>10.1209/0295-5075/106/48004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was shown that there is a phase transition in the community
detection problem. This transition was first computed using the cavity method,
and has been proved rigorously in the case of $q=2$ groups. However, analytic
calculations using the cavity method are challenging since they require us to
understand probability distributions of messages. We study analogous
transitions in so-called &quot;zero-temperature inference&quot; model, where this
distribution is supported only on the most-likely messages. Furthermore,
whenever several messages are equally likely, we break the tie by choosing
among them with equal probability. While the resulting analysis does not give
the correct values of the thresholds, it does reproduce some of the qualitative
features of the system. It predicts a first-order detectability transition
whenever $q &gt; 2$, while the finite-temperature cavity method shows that this is
the case only when $q &gt; 4$. It also has a regime analogous to the &quot;hard but
detectable&quot; phase, where the community structure can be partially recovered,
but only when the initial messages are sufficiently accurate. Finally, we study
a semisupervised setting where we are given the correct labels for a fraction
$\rho$ of the nodes. For $q &gt; 2$, we find a regime where the accuracy jumps
discontinuously at a critical value of $\rho$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0638</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0638</id><created>2013-12-02</created><authors><author><keyname>Hu</keyname><forenames>Yingjie</forenames></author><author><keyname>Lv</keyname><forenames>Zhenhua</forenames></author><author><keyname>Wu</keyname><forenames>Jianping</forenames></author><author><keyname>Janowicz</keyname><forenames>Krzysztof</forenames></author><author><keyname>Zhao</keyname><forenames>Xizhi</forenames></author><author><keyname>Yu</keyname><forenames>Bailang</forenames></author></authors><title>A Multi-stage Collaborative 3D GIS to Support Public Participation</title><categories>cs.CY</categories><comments>36 pages, 10 figures, accepted by the International Journal of
  Digital Earth</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a collaborative 3D GIS to support public participation.
Realizing that public-involved decision making is often a multi-stage process,
the proposed system is designed to provide coherent support for collaborations
in the different stages. We differentiate ubiquitous participation and
intensive participation, and identify their suitable application stages. The
proposed system, then, supports both of the two types of participation by
providing synchronous and asynchronous collaboration functionalities. Applying
the concept of Digital Earth, the proposed system also features a virtual
globe-based user interface. Such an interface integrates a variety of data,
functions and services into a unified virtual environment which is delivered to
both experts and public participants through the Internet. The system has been
designed as a general software framework, and can be tailored for specific
projects. In this study, we demonstrate it using a scene modeling case and
provide a preliminary evaluation towards its usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0641</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0641</id><created>2013-12-02</created><updated>2013-12-05</updated><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Simple Bounds for Noisy Linear Inverse Problems with Exact Side
  Information</title><categories>cs.IT math.IT math.OC math.ST stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the linear inverse problem where we wish to estimate a
structured signal $x$ from its corrupted observations. When the problem is
ill-posed, it is natural to make use of a convex function $f(\cdot)$ that
exploits the structure of the signal. For example, $\ell_1$ norm can be used
for sparse signals. To carry out the estimation, we consider two well-known
convex programs: 1) Second order cone program (SOCP), and, 2) Lasso. Assuming
Gaussian measurements, we show that, if precise information about the value
$f(x)$ or the $\ell_2$-norm of the noise is available, one can do a
particularly good job at estimation. In particular, the reconstruction error
becomes proportional to the &quot;sparsity&quot; of the signal rather than the ambient
dimension of the noise vector. We connect our results to existing works and
provide a discussion on the relation of our results to the standard
least-squares problem. Our error bounds are non-asymptotic and sharp, they
apply to arbitrary convex functions and do not assume any distribution on the
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0649</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0649</id><created>2013-12-02</created><authors><author><keyname>Yu</keyname><forenames>Louis Lei</forenames></author><author><keyname>Asur</keyname><forenames>Sitaram</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Dynamics of Trends and Attention in Chinese Social Media</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.0327</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a tremendous rise in the growth of online social networks all
over the world in recent years. It has facilitated users to generate a large
amount of real-time content at an incessant rate, all competing with each other
to attract enough attention and become popular trends. While Western online
social networks such as Twitter have been well studied, the popular Chinese
microblogging network Sina Weibo has had relatively lower exposure. In this
paper, we analyze in detail the temporal aspect of trends and trend-setters in
Sina Weibo, contrasting it with earlier observations in Twitter. We find that
there is a vast difference in the content shared in China when compared to a
global social network such as Twitter. In China, the trends are created almost
entirely due to the retweets of media content such as jokes, images and videos,
unlike Twitter where it has been shown that the trends tend to have more to do
with current global events and news stories. We take a detailed look at the
formation, persistence and decay of trends and examine the key topics that
trend in Sina Weibo. One of our key findings is that retweets are much more
common in Sina Weibo and contribute a lot to creating trends. When we look
closer, we observe that most trends in Sina Weibo are due to the continuous
retweets of a small percentage of fraudulent accounts. These fake accounts are
set up to artificially inflate certain posts, causing them to shoot up into
Sina Weibo's trending list, which are in turn displayed as the most popular
topics to users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0650</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0650</id><created>2013-12-02</created><authors><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author><author><keyname>Reiffers</keyname><forenames>Alexandre</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Differential Games of Competition in Online Content Diffusion</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to online contents represents a large share of the Internet traffic.
Most such contents are multimedia items which are user-generated, i.e., posted
online by the contents' owners. In this paper we focus on how those who provide
contents can leverage online platforms in order to profit from their large base
of potential viewers.
  Actually, platforms like Vimeo or YouTube provide tools to accelerate the
dissemination of contents, i.e., recommendation lists and other re-ranking
mechanisms. Hence, the popularity of a content can be increased by paying a
cost for advertisement: doing so, it will appear with some priority in the
recommendation lists and will be accessed more frequently by the platform
users.
  Ultimately, such acceleration mechanism engenders a competition among online
contents to gain popularity. In this context, our focus is on the structure of
the acceleration strategies which a content provider should use in order to
optimally promote a content given a certain daily budget. Such a best response
indeed depends on the strategies adopted by competing content providers. Also,
it is a function of the potential popularity of a content and the fee paid for
the platform advertisement service.
  We formulate the problem as a differential game and we solve it for the
infinite horizon case by deriving the structure of certain Nash equilibria of
the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0653</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0653</id><created>2013-12-02</created><updated>2014-08-25</updated><authors><author><keyname>Hejda</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Pelantov&#xe1;</keyname><forenames>Edita</forenames></author></authors><title>Spectral properties of cubic complex Pisot units</title><categories>math.MG cs.DM math.CO math.NT</categories><comments>accepted to Math. Comp., 21 pages, 7 figures, 2 tables, 23 references</comments><msc-class>11A63, 52C23, 52C10 (Primary), 11H99, 11-04 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a real number $\beta&gt;1$, Erd\H{o}s, Jo\'o and Komornik study distances
between consecutive points in the set $X^m(\beta)=\bigl\{\sum_{j=0}^n a_j
\beta^j : n\in\mathbb N,\,a_j\in\{0,1,\dots,m\}\bigr\}$. Pisot numbers play a
crucial role for the properties of $X^m(\beta)$. Following the work of Za\&quot;imi,
who considered $X^m(\gamma)$ with $\gamma\in\mathbb{C}\setminus\mathbb{R}$ and
$|\gamma|&gt;1$, we show that for any non-real $\gamma$ and $m &lt; |\gamma|^2-1$,
the set $X^m(\gamma)$ is not relatively dense in the complex plane.
  Then we focus on complex Pisot units with a positive real conjugate $\gamma'$
and $m &gt; |\gamma|^2-1$. If the number $1/\gamma'$ satisfies Property (F), we
deduce that $X^m(\gamma)$ is uniformly discrete and relatively dense, i.e.,
$X^m(\gamma)$ is a Delone set. Moreover, we present an algorithm for
determining two parameters of the Delone set $X^m(\gamma)$ which are analogous
to minimal and maximal distances in the real case $X^m(\beta)$. For $\gamma$
satisfying $\gamma^3 + \gamma^2 + \gamma - 1 = 0$, explicit formulas for the
two parameters are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0655</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0655</id><created>2013-12-02</created><authors><author><keyname>Geng</keyname><forenames>Quan</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>The Optimal Mechanism in Differential Privacy: Multidimensional Setting</title><categories>cs.CR</categories><comments>18 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1212.1186</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the optimal $\epsilon$-differentially private mechanism for a
general two-dimensional real-valued (histogram-like) query function under a
utility-maximization (or cost-minimization) framework for the $\ell^1$ cost
function. We show that the optimal noise probability distribution has a
correlated multidimensional staircase-shaped probability density function.
Compared with the Laplacian mechanism, we show that in the high privacy regime
(as $\epsilon \to 0$), the Laplacian mechanism is approximately optimal; and in
the low privacy regime (as $\epsilon \to +\infty$), the optimal cost is
$\Theta(e^{-\frac{\epsilon}{3}})$, while the cost of the Laplacian mechanism is
$\frac{2\Delta}{\epsilon}$, where $\Delta$ is the sensitivity of the query
function. We conclude that the gain is more pronounced in the low privacy
regime. We conjecture that the optimality of the staircase mechanism holds for
vector-valued (histogram-like) query functions with arbitrary dimension, and
holds for many other classes of cost functions as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0658</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0658</id><created>2013-12-02</created><authors><author><keyname>Cai</keyname><forenames>Yufei</forenames></author><author><keyname>Giarrusso</keyname><forenames>Paolo G.</forenames></author><author><keyname>Rendel</keyname><forenames>Tillmann</forenames></author><author><keyname>Ostermann</keyname><forenames>Klaus</forenames></author></authors><title>A Theory of Changes for Higher-Order Languages - Incrementalizing
  {\lambda}-Calculi by Static Differentiation</title><categories>cs.PL</categories><comments>11 pages; unpublished preprint, under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If the result of an expensive computation is invalidated by a small change to
the input, the old result should be updated incrementally instead of
reexecuting the whole computation. We incrementalize programs through their
derivative. A derivative maps changes in the program's input directly to
changes in the program's output, without reexecuting the original program. We
present a program transformation taking programs to their derivatives, which is
fully static and automatic, supports first-class functions, and produces
derivatives amenable to standard optimization.
  We prove the program transformation correct in Agda for a family of
simply-typed {\lambda}-calculi, parameterized by base types and primitives. A
precise interface specifies what is required to incrementalize the chosen
primitives.
  We investigate performance by a case study: We implement in Scala the program
transformation, a plugin and improve performance of a nontrivial program by
orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0659</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0659</id><created>2013-12-02</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Zhang</keyname><forenames>Jian A.</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Thiebaux</keyname><forenames>Sylvie</forenames></author></authors><title>Prioritizing Consumers in Smart Grid: A Game Theoretic Approach</title><categories>cs.SY cs.GT</categories><comments>10 pages, 8 figures. IEEE Transactions on Smart Grid, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an energy management technique for a consumer-to-grid
system in smart grid. The benefit to consumers is made the primary concern to
encourage consumers to participate voluntarily in energy trading with the
central power station (CPS) in situations of energy deficiency. A novel system
model motivating energy trading under the goal of social optimality is
proposed. A single-leader multiple-follower Stackelberg game is then studied to
model the interactions between the CPS and a number of energy consumers (ECs),
and to find optimal distributed solutions for the optimization problem based on
the system model. The CPS is considered as a leader seeking to minimize its
total cost of buying energy from the ECs, and the ECs are the followers who
decide on how much energy they will sell to the CPS for maximizing their
utilities. It is shown that the game, which can be implemented distributedly,
possesses a socially optimal solution, in which the benefits-sum to all
consumers is maximized, as the total cost to the CPS is minimized. Numerical
analysis confirms the effectiveness of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0677</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0677</id><created>2013-12-02</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>Formal Model of Web Service Composition: An Actor-Based Approach to
  Unifying Orchestration and Choreography</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1306.5535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Service Composition creates new composite Web Services from the
collection of existing ones to be composed further and embodies the added
values and potential usages of Web Services. Web Service Composition includes
two aspects: Web Service orchestration denoting a workflow-like composition
pattern and Web Service choreography which represents an aggregate composition
pattern. There were only a few works which give orchestration and choreography
a relationship. In this paper, we introduce an architecture of Web Service
Composition runtime which establishes a natural relationship between
orchestration and choreography through a deep analysis of the two ones. Then we
use an actor-based approach to design a language called AB-WSCL to support such
an architecture. To give AB-WSCL a firmly theoretic foundation, we establish
the formal semantics of AB-WSCL based on concurrent rewriting theory for
actors. Conclusions that well defined relationships exist among the components
of AB-WSCL using a notation of Compositionality is drawn based on semantics
analysis. Our works can be bases of a modeling language, simulation tools,
verification tools of Web Service Composition at design time, and also a Web
Service Composition runtime with correctness analysis support itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0685</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0685</id><created>2013-12-02</created><authors><author><keyname>Mehmetoglu</keyname><forenames>Mustafa S.</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Optimization of zero-delay mappings for distributed coding by
  deterministic annealing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the optimization of zero-delay analog mappings in a
network setting that involves distributed coding. The cost surface is known to
be non-convex, and known greedy methods tend to get trapped in poor locally
optimal solutions that depend heavily on initialization. We derive an
optimization algorithm based on the principles of &quot;deterministic annealing&quot;, a
powerful global optimization framework that has been successfully employed in
several disciplines, including, in our recent work, to a simple zero-delay
analog communications problem. We demonstrate strict superiority over the
descent based methods, as well as present example mappings whose properties
lend insights on the workings of the solution and relations with digital
distributed coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0686</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0686</id><created>2013-12-02</created><updated>2015-07-13</updated><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>A Process Algebra for Games</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1311.2960</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using formal tools in computer science to describe games is an interesting
problem. We give games, exactly two person games, an axiomatical foundation
based on the process algebra ACP (Algebra of Communicating Process). A fresh
operator called opponent's alternative composition operator (OA) is introduced
into ACP to describe game trees and game strategies, called GameACP. And its
sound and complete axiomatical system is naturally established. To model the
outcomes of games (the co-action of the player and the opponent),
correspondingly in GameACP, the execution of GameACP processes, another
operator called playing operator (PO) is extended into GameACP. We also
establish a sound and complete axiomatical system for PO. Finally, we give the
correctness theorem between the outcomes of games and the deductions of GameACP
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0694</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0694</id><created>2013-12-02</created><updated>2014-07-12</updated><authors><author><keyname>Siek</keyname><forenames>Jeremy G.</forenames></author><author><keyname>Vitousek</keyname><forenames>Michael M.</forenames></author></authors><title>Monotonic References for Gradual Typing</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an alternative approach to handling mutable references (aka.
pointers) within a gradually typed language that has different efficiency
characteristics than the prior approach of Herman et al. [2010]. In particular,
we reduce the costs of reading and writing through references in statically
typed regions of code. We reduce the costs to be the same as they would in a
statically typed language, that is, simply the cost of a load or store
instruction (for primitive data types). This reduction in cost is especially
important for programmers who would like to use gradual typing to facilitate
transitioning from a dynamically-typed prototype of an algorithm to a
statically-typed, high-performance implementation. The programmers we have in
mind are scientists and engineers who currently prototype in Matlab and then
manually translate their algorithms into Fortran. We present the static and
dynamic semantics for mutable references and a mechanized proof of type safety
using the Isabelle proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0697</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0697</id><created>2013-12-02</created><authors><author><keyname>de Brecht</keyname><forenames>Matthew</forenames></author></authors><title>Levels of discontinuity, limit-computability, and jump operators</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general theory of jump operators, which is intended to provide
an abstraction of the notion of &quot;limit-computability&quot; on represented spaces.
Jump operators also provide a framework with a strong categorical flavor for
investigating degrees of discontinuity of functions and hierarchies of sets on
represented spaces. We will provide a thorough investigation within this
framework of a hierarchy of $\Delta^0_2$-measurable functions between arbitrary
countably based $T_0$-spaces, which captures the notion of computing with
ordinal mind-change bounds. Our abstract approach not only raises new questions
but also sheds new light on previous results. For example, we introduce a
notion of &quot;higher order&quot; descriptive set theoretical objects, we generalize a
recent characterization of the computability theoretic notion of &quot;lowness&quot; in
terms of adjoint functors, and we show that our framework encompasses ordinal
quantifications of the non-constructiveness of Hilbert's finite basis theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0700</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0700</id><created>2013-12-03</created><authors><author><keyname>Arslan</keyname><forenames>Suayb S.</forenames></author></authors><title>Redundancy and Aging of Efficient Multidimensional MDS-Parity Protected
  Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, Accepted for publication in IEEE Transactions on
  Device and Materials Reliability (TDMR), Nov. 2013</comments><doi>10.1109/TDMR.2013.2293491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effect of redundancy on the aging of an efficient Maximum Distance
Separable (MDS) parity--protected distributed storage system that consists of
multidimensional arrays of storage units is explored. In light of the
experimental evidences and survey data, this paper develops generalized
expressions for the reliability of array storage systems based on more
realistic time to failure distributions such as Weibull. For instance, a
distributed disk array system is considered in which the array components are
disseminated across the network and are subject to independent failure rates.
Based on such, generalized closed form hazard rate expressions are derived.
These expressions are extended to estimate the asymptotical reliability
behavior of large scale storage networks equipped with MDS parity-based
protection. Unlike previous studies, a generic hazard rate function is assumed,
a generic MDS code for parity generation is used, and an evaluation of the
implications of adjustable redundancy level for an efficient distributed
storage system is presented. Results of this study are applicable to any
erasure correction code as long as it is accompanied with a suitable structure
and an appropriate encoding/decoding algorithm such that the MDS property is
maintained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0707</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0707</id><created>2013-12-03</created><updated>2014-12-01</updated><authors><author><keyname>Roosta-Khorasani</keyname><forenames>Farbod</forenames></author><author><keyname>Doel</keyname><forenames>Kees van den</forenames></author><author><keyname>Ascher</keyname><forenames>Uri</forenames></author></authors><title>Data completion and stochastic algorithms for PDE inversion problems
  with many measurements</title><categories>cs.NA math.NA</categories><journal-ref>Electronic Transactions on Numerical Analysis. 42 (2014) 177-196</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse problems involving systems of partial differential equations (PDEs)
with many measurements or experiments can be very expensive to solve
numerically. In a recent paper we examined dimensionality reduction methods,
both stochastic and deterministic, to reduce this computational burden,
assuming that all experiments share the same set of receivers. In the present
article we consider the more general and practically important case where
receivers are not shared across experiments. We propose a data completion
approach to alleviate this problem. This is done by means of an approximation
using an appropriately restricted gradient or Laplacian regularization,
extending existing data for each experiment to the union of all receiver
locations. Results using the method of simultaneous sources (SS) with the
completed data are then compared to those obtained by a more general but slower
random subset (RS) method which requires no modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0713</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0713</id><created>2013-12-03</created><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>Kremer</keyname><forenames>Stephan</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Assmann</keyname><forenames>Danilo</forenames></author></authors><title>Guiding Testing Activities by Predicting Defect-prone Parts Using
  Product and Inspection Metrics</title><categories>cs.SE</categories><comments>8 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6328182</comments><doi>10.1109/SEAA.2012.30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product metrics, such as size or complexity, are often used to identify
defect-prone parts or to focus quality assurance activities. In contrast,
quality information that is available early, such as information provided by
inspections, is usually not used. Currently, only little experience is
documented in the literature on whether data from early defect detection
activities can support the identification of defect-prone parts later in the
development process. This article compares selected product and inspection
metrics commonly used to predict defect-prone parts. Based on initial
experience from two case studies performed in different environments, the
suitability of different metrics for predicting defect-prone parts is
illustrated. These studies revealed that inspection defect data seems to be a
suitable predictor, and a combination of certain inspection and product metrics
led to the best prioritizations in our contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0714</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0714</id><created>2013-12-03</created><authors><author><keyname>Rusu</keyname><forenames>Andrei</forenames></author></authors><title>On sufficient conditions for expressibility of constants in the 4-valued
  extension of the propositional provability logic $GL$</title><categories>cs.LO</categories><comments>17 pages, 2 tables</comments><msc-class>03F45,</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In the present paper we consider the simplest non-classical extension $GL4$
of the well-known propositional provability logic $GL$ together with the notion
of expressibility of formulas in a logic proposed by A. V. Kuznetsov.
Conditions for expressibility of constants in the 4-valued extension
$L\mathfrak{B}_2$ of $GL$ are found out, which were first announced in a
author's paper in 1996.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0718</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0718</id><created>2013-12-03</created><updated>2014-03-26</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Zhi Ning</forenames></author></authors><title>Electromagnetic Lens-focusing Antenna Enabled Massive MIMO: Performance
  Improvement and Cost Reduction</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) techniques have been recently
advanced to tremendously improve the performance of wireless communication
networks. However, the use of very large antenna arrays at the base stations
(BSs) brings new issues, such as the significantly increased hardware and
signal processing costs. In order to reap the enormous gain of massive MIMO and
yet reduce its cost to an affordable level, this paper proposes a novel system
design by integrating an electromagnetic (EM) lens with the large antenna
array, termed the EM-lens enabled MIMO. The EM lens has the capability of
focusing the power of an incident wave to a small area of the antenna array,
while the location of the focal area varies with the angle of arrival (AoA) of
the wave. Therefore, in practical scenarios where the arriving signals from
geographically separated users have different AoAs, the EM-lens enabled system
provides two new benefits, namely energy focusing and spatial interference
rejection. By taking into account the effects of imperfect channel estimation
via pilot-assisted training, in this paper we analytically show that the
average received signal-to-noise ratio (SNR) in both the single-user and
multiuser uplink transmissions can be strictly improved by the EM-lens enabled
system. Furthermore, we demonstrate that the proposed design makes it possible
to considerably reduce the hardware and signal processing costs with only
slight degradations in performance. To this end, two complexity/cost reduction
schemes are proposed, which are small-MIMO processing with parallel receiver
filtering applied over subgroups of antennas to reduce the computational
complexity, and channel covariance based antenna selection to reduce the
required number of radio frequency (RF) chains. Numerical results are provided
to corroborate our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0720</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0720</id><created>2013-12-03</created><authors><author><keyname>Zhao</keyname><forenames>Tao</forenames></author><author><keyname>Yang</keyname><forenames>Pengkun</forenames></author><author><keyname>Pan</keyname><forenames>Huimin</forenames></author><author><keyname>Deng</keyname><forenames>Ruichen</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Software Defined Radio Implementation of Signaling Splitting in
  Hyper-Cellular Network</title><categories>cs.NI</categories><comments>4 pages, 4 figures, accepted by the Second Workshop of Software Radio
  Implementation Forum (SRIF 2013)</comments><acm-class>C.2.1</acm-class><doi>10.1145/2491246.2491258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and implementation of signaling splitting
scheme in hyper-cellular network on a software defined radio platform.
Hyper-cellular network is a novel architecture of future mobile communication
systems in which signaling and data are decoupled at the air interface to
mitigate the signaling overhead and allow energy efficient operation of base
stations. On an open source software defined radio platform, OpenBTS, we
investigate the feasibility of signaling splitting for GSM protocol and
implement a novel system which can prove the proposed concept. Standard GSM
handsets can camp on the network with the help of signaling base station, and
data base station will be appointed to handle phone calls on demand. Our work
initiates the systematic approach to study hyper-cellular concept in real
wireless environment with both software and hardware implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0722</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0722</id><created>2013-12-03</created><updated>2014-06-14</updated><authors><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Moysoglou</keyname><forenames>Yannis</forenames></author></authors><title>Sherali-Adams gaps, flow-cover inequalities and generalized
  configurations for capacity-constrained Facility Location</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1305.5998</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric facility location is a well-studied problem for which linear
programming methods have been used with great success in deriving approximation
algorithms. The capacity-constrained generalizations, such as capacitated
facility location (CFL) and lower-bounded facility location (LBFL), have proved
notorious as far as LP-based approximation is concerned: while there are
local-search-based constant-factor approximations, there is no known linear
relaxation with constant integrality gap. According to Williamson and Shmoys
devising a relaxation-based approximation for \cfl\ is among the top 10 open
problems in approximation algorithms.
  This paper advances significantly the state-of-the-art on the effectiveness
of linear programming for capacity-constrained facility location through a host
of impossibility results for both CFL and LBFL. We show that the relaxations
obtained from the natural LP at $\Omega(n)$ levels of the Sherali-Adams
hierarchy have an unbounded gap, partially answering an open question of
\cite{LiS13, AnBS13}. Here, $n$ denotes the number of facilities in the
instance. Building on the ideas for this result, we prove that the standard CFL
relaxation enriched with the generalized flow-cover valid inequalities
\cite{AardalPW95} has also an unbounded gap. This disproves a long-standing
conjecture of \cite{LeviSS12}. We finally introduce the family of proper
relaxations which generalizes to its logical extreme the classic star
relaxation and captures general configuration-style LPs. We characterize the
behavior of proper relaxations for CFL and LBFL through a sharp threshold
phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0723</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0723</id><created>2013-12-03</created><updated>2014-03-22</updated><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author></authors><title>The Input/Output Complexity of Triangle Enumeration</title><categories>cs.DS</categories><comments>Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART Symposium on
  Principles of Database Systems, PODS 2014</comments><acm-class>F.2.2; H.2.8</acm-class><doi>10.1145/2594538.2594552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the well-known problem of enumerating all triangles of an
undirected graph. Our focus is on determining the input/output (I/O) complexity
of this problem. Let $E$ be the number of edges, $M&lt;E$ the size of internal
memory, and $B$ the block size. The best results obtained previously are
sort$(E^{3/2})$ I/Os (Dementiev, PhD thesis 2006) and $O(E^2/(MB))$ I/Os (Hu et
al., SIGMOD 2013), where sort$(n)$ denotes the number of I/Os for sorting $n$
items. We improve the I/O complexity to $O(E^{3/2}/(\sqrt{M} B))$ expected
I/Os, which improves the previous bounds by a factor
$\min(\sqrt{E/M},\sqrt{M})$. Our algorithm is cache-oblivious and also I/O
optimal: We show that any algorithm enumerating $t$ distinct triangles must
always use $\Omega(t/(\sqrt{M} B))$ I/Os, and there are graphs for which
$t=\Omega(E^{3/2})$. Finally, we give a deterministic cache-aware algorithm
using $O(E^{3/2}/(\sqrt{M} B))$ I/Os assuming $M\geq E^\varepsilon$ for a
constant $\varepsilon &gt; 0$. Our results are based on a new color coding
technique, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0728</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0728</id><created>2013-12-03</created><authors><author><keyname>Calofir</keyname><forenames>Vasile</forenames></author><author><keyname>Tanasa</keyname><forenames>Valentin</forenames></author><author><keyname>Fagarasan</keyname><forenames>Ioana</forenames></author><author><keyname>Stamatescu</keyname><forenames>Iulia</forenames></author><author><keyname>Arghira</keyname><forenames>Nicoleta</forenames></author><author><keyname>Stamatescu</keyname><forenames>Grigore</forenames></author></authors><title>A Backstepping Control Method for a Nonlinear Process - Two
  Coupled-Tanks</title><categories>cs.SY</categories><comments>8 pages, 3 figures. CIEM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to compute a level backstepping control strategy for
a coupled tanks system. The coupled tanks plant is a component included in the
water treatment system of power plants. The nonlinear-model of the process was
designed and implemented in Matlab- Simulink. The advantages of the control
method proposed is that it takes into consideration the nonlinearity which can
be useful for stabilization and a larger operating point with specified
performances. The backstepping control method is computed using the nonlinear
model of the system and the performance was validated on the physical plant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0735</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0735</id><created>2013-12-03</created><authors><author><keyname>Lamy</keyname><forenames>Jean-Baptiste</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Ellini</keyname><forenames>Anis</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Ebrahiminia</keyname><forenames>Vahid</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Zucker</keyname><forenames>Jean-Daniel</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Falcoff</keyname><forenames>Hector</forenames><affiliation>SFTG</affiliation></author><author><keyname>Venot</keyname><forenames>Alain</forenames><affiliation>LIM\&amp;BIO</affiliation></author></authors><title>Use of the C4.5 machine learning algorithm to test a clinical
  guideline-based decision support system</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Studies in Health Technology and Informatics 136 (2008) 223-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Well-designed medical decision support system (DSS) have been shown to
improve health care quality. However, before they can be used in real clinical
situations, these systems must be extensively tested, to ensure that they
conform to the clinical guidelines (CG) on which they are based. Existing
methods cannot be used for the systematic testing of all possible test cases.
We describe here a new exhaustive dynamic verification method. In this method,
the DSS is considered to be a black box, and the Quinlan C4.5 algorithm is used
to build a decision tree from an exhaustive set of DSS input vectors and
outputs. This method was successfully used for the testing of a medical DSS
relating to chronic diseases: the ASTI critiquing module for type 2 diabetes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0736</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0736</id><created>2013-12-03</created><authors><author><keyname>Lamy</keyname><forenames>Jean-Baptiste</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Ebrahiminia</keyname><forenames>Vahid</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Seroussi</keyname><forenames>Brigitte</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Bouaud</keyname><forenames>Jacques</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Simon</keyname><forenames>Christian</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Favre</keyname><forenames>Madeleine</forenames><affiliation>SFTG</affiliation></author><author><keyname>Falcoff</keyname><forenames>Hector</forenames><affiliation>SFTG</affiliation></author><author><keyname>Venot</keyname><forenames>Alain</forenames><affiliation>LIM\&amp;BIO</affiliation></author></authors><title>A generic system for critiquing physicians' prescriptions: usability,
  satisfaction and lessons learnt</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Studies in Health Technology and Informatics 169 (2011) 125-9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clinical decision support systems have been developed to help physicians to
take clinical guidelines into account during consultations. The ASTI critiquing
module is one such systems; it provides the physician with automatic criticisms
when a drug prescription does not follow the guidelines. It was initially
developed for hypertension and type 2 diabetes, but is designed to be generic
enough for application to all chronic diseases. We present here the results of
usability and satisfaction evaluations for the ASTI critiquing module, obtained
with GPs for a newly implemented guideline concerning dyslipaemia, and we
discuss the lessons learnt and the difficulties encountered when building a
generic DSS for critiquing physicians' prescriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0742</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0742</id><created>2013-12-03</created><authors><author><keyname>Pacheco</keyname><forenames>Leandro</forenames></author><author><keyname>Sciascia</keyname><forenames>Daniele</forenames></author><author><keyname>Pedone</keyname><forenames>Fernando</forenames></author></authors><title>Parallel Deferred Update Replication</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deferred update replication (DUR) is an established approach to implementing
highly efficient and available storage. While the throughput of read-only
transactions scales linearly with the number of deployed replicas in DUR, the
throughput of update transactions experiences limited improvements as replicas
are added. This paper presents Parallel Deferred Update Replication (P-DUR), a
variation of classical DUR that scales both read-only and update transactions
with the number of cores available in a replica. In addition to introducing the
new approach, we describe its full implementation and compare its performance
to classical DUR and to Berkeley DB, a well-known standalone database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0750</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0750</id><created>2013-12-03</created><authors><author><keyname>Lamy</keyname><forenames>Jean-Baptiste</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Tsopra</keyname><forenames>Rosy</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Venot</keyname><forenames>Alain</forenames><affiliation>LIM\&amp;BIO</affiliation></author><author><keyname>Duclos</keyname><forenames>Catherine</forenames><affiliation>LIM\&amp;BIO</affiliation></author></authors><title>A semi-automatic semantic method for mapping SNOMED CT concepts to VCM
  Icons</title><categories>cs.AI cs.HC</categories><proxy>ccsd</proxy><journal-ref>Studies in Health Technology and Informatics 192 (2013) 42-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VCM (Visualization of Concept in Medicine) is an iconic language for
representing key medical concepts by icons. However, the use of this language
with reference terminologies, such as SNOMED CT, will require the mapping of
its icons to the terms of these terminologies. Here, we present and evaluate a
semi-automatic semantic method for the mapping of SNOMED CT concepts to VCM
icons. Both SNOMED CT and VCM are compositional in nature; SNOMED CT is
expressed in description logic and VCM semantics are formalized in an OWL
ontology. The proposed method involves the manual mapping of a limited number
of underlying concepts from the VCM ontology, followed by automatic generation
of the rest of the mapping. We applied this method to the clinical findings of
the SNOMED CT CORE subset, and 100 randomly-selected mappings were evaluated by
three experts. The results obtained were promising, with 82 of the SNOMED CT
concepts correctly linked to VCM icons according to the experts. Most of the
errors were easy to fix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0760</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0760</id><created>2013-12-03</created><authors><author><keyname>Mogali</keyname><forenames>Jayanth Krishna</forenames></author><author><keyname>Pediredla</keyname><forenames>Adithya Kumar</forenames></author><author><keyname>Seelamantula</keyname><forenames>Chandra Sekhar</forenames></author></authors><title>Template-Based Active Contours</title><categories>cs.CV</categories><comments>Active Contours, Snakes, Affine matching, Contrast function, Shape
  constraint, Image segmentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a generalized active contour formalism for image segmentation
based on shape templates. The shape template is subjected to a restricted
affine transformation (RAT) in order to segment the object of interest. RAT
allows for translation, rotation, and scaling, which give a total of five
degrees of freedom. The proposed active contour comprises an inner and outer
contour pair, which are closed and concentric. The active contour energy is a
contrast function defined based on the intensities of pixels that lie inside
the inner contour and those that lie in the annulus between the inner and outer
contours. We show that the contrast energy functional is optimal under certain
conditions. The optimal RAT parameters are computed by maximizing the contrast
function using a gradient descent optimizer. We show that the calculations are
made efficient through use of Green's theorem. The proposed formalism is
capable of handling a variety of shapes because for a chosen template,
optimization is carried with respect to the RAT parameters only. The proposed
formalism is validated on multiple images to show robustness to Gaussian and
Poisson noise, to initialization, and to partial loss of structure in the
object to be segmented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0768</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0768</id><created>2013-12-03</created><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>Eschbach</keyname><forenames>Robert</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>The Relevance of Assumptions and Context Factors for the Integration of
  Inspections and Testing</title><categories>cs.SE</categories><comments>4 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6068373</comments><doi>10.1109/SEAA.2011.77</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating inspection processes with testing processes promises to deliver
several benefits, including reduced effort for quality assurance or higher
defect detection rates. Systematic integration of these processes requires
knowledge regarding the relationships between these processes, especially
regarding the relationship between inspection defects and test defects. Such
knowledge is typically context-dependent and needs to be gained analytically or
empirically. If such kind of knowledge is not available, assumptions need to be
made for a specific context. This article describes the relevance of
assumptions and context factors for integrating inspection and testing
processes and provides mechanisms for deriving assumptions in a systematic
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0786</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0786</id><created>2013-12-03</created><updated>2014-02-19</updated><authors><author><keyname>Liao</keyname><forenames>Yiyi</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Image Representation Learning Using Graph Regularized Auto-Encoders</title><categories>cs.LG</categories><comments>9pages</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of image representation for the tasks of unsupervised
learning and semi-supervised learning. In those learning tasks, the raw image
vectors may not provide enough representation for their intrinsic structures
due to their highly dense feature space. To overcome this problem, the raw
image vectors should be mapped to a proper representation space which can
capture the latent structure of the original data and represent the data
explicitly for further learning tasks such as clustering.
  Inspired by the recent research works on deep neural network and
representation learning, in this paper, we introduce the multiple-layer
auto-encoder into image representation, we also apply the locally invariant
ideal to our image representation with auto-encoders and propose a novel
method, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact
representation which uncovers the hidden semantics and simultaneously respects
the intrinsic geometric structure.
  Extensive experiments on image clustering show encouraging results of the
proposed algorithm in comparison to the state-of-the-art algorithms on
real-word cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0788</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0788</id><created>2013-12-03</created><updated>2014-08-08</updated><authors><author><keyname>Gallego</keyname><forenames>Guillermo</forenames></author><author><keyname>Yezzi</keyname><forenames>Anthony</forenames></author></authors><title>A compact formula for the derivative of a 3-D rotation in exponential
  coordinates</title><categories>cs.CV math.OC</categories><comments>6 pages</comments><doi>10.1007/s10851-014-0528-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a compact formula for the derivative of a 3-D rotation matrix with
respect to its exponential coordinates. A geometric interpretation of the
resulting expression is provided, as well as its agreement with other
less-compact but better-known formulas. To the best of our knowledge, this
simpler formula does not appear anywhere in the literature. We hope by
providing this more compact expression to alleviate the common pressure to
reluctantly resort to alternative representations in various computational
applications simply as a means to avoid the complexity of differential analysis
in exponential coordinates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0790</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0790</id><created>2013-12-03</created><updated>2014-03-14</updated><authors><author><keyname>Chaudhari</keyname><forenames>Sneha</forenames></author><author><keyname>Dayama</keyname><forenames>Pankaj</forenames></author><author><keyname>Pandit</keyname><forenames>Vinayaka</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Indrajit</forenames></author></authors><title>Test Set Selection using Active Information Acquisition for Predictive
  Models</title><categories>cs.AI cs.LG stat.ML</categories><comments>The paper has been withdrawn by the authors. The current version is
  incomplete and the work is still on going. The algorithm gives poor results
  for a particular setting and we are working on it. However, we are not
  planning to submit a revision of the paper. This work is going to take some
  time and we want to withdraw the current version since it is not in a good
  shape and needs a lot more work to be in publishable condition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider active information acquisition when the prediction
model is meant to be applied on a targeted subset of the population. The goal
is to label a pre-specified fraction of customers in the target or test set by
iteratively querying for information from the non-target or training set. The
number of queries is limited by an overall budget. Arising in the context of
two rather disparate applications- banking and medical diagnosis, we pose the
active information acquisition problem as a constrained optimization problem.
We propose two greedy iterative algorithms for solving the above problem. We
conduct experiments with synthetic data and compare results of our proposed
algorithms with few other baseline approaches. The experimental results show
that our proposed approaches perform better than the baseline schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0801</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0801</id><created>2013-12-03</created><authors><author><keyname>Ghosh</keyname><forenames>Pramit</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Datta</keyname><forenames>Soma</forenames></author></authors><title>Adaptive Intelligent Controller for Household Cooling Systems</title><categories>cs.CY</categories><comments>6 pages, Conference</comments><journal-ref>International Conference on Integrated Intelligent Computing-
  ICIIC 2010 ISBN:978-0-7695-4152-5/10</journal-ref><doi>10.1109/ICIIC.2010.24</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper presents a household cooling system controller which is adaptive
and intelligent in nature. It is able to control the speed of a household
cooling fan or an air conditioner based on the real time data namely room
temperature, humidity and time i.e. duration, which are collected from
environment. To control the speed in an adaptive and intelligent manner an
associative memory neural network has been used. This embedded system is able
to learn from training set i.e. the user can teach the system about his/her
feelings through training data sets. When the system starts up it allows the
fan to run freely i.e. at full speed and after certain interval it takes the
environmental parameters like room temperature, humidity and time i.e.
duration, as an input and after that system takes the decision and controls the
speed of the fan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0803</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0803</id><created>2013-12-03</created><updated>2014-04-06</updated><authors><author><keyname>Najafi</keyname><forenames>Amir</forenames></author><author><keyname>Joudaki</keyname><forenames>Amir</forenames></author><author><keyname>Fatemizadeh</keyname><forenames>Emad</forenames></author></authors><title>Nonlinear Dimensionality Reduction via Path-Based Isometric Mapping</title><categories>cs.CG</categories><comments>(29) pages, (12) figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear dimensionality reduction methods have demonstrated top-notch
performance in many pattern recognition and image classification tasks. Despite
their popularity, they suffer from highly expensive time and memory
requirements, which render them inapplicable to large-scale datasets. To
leverage such cases we propose a new method called &quot;Path-Based Isomap&quot;. Similar
to Isomap, we exploit geodesic paths to find the low-dimensional embedding.
However, instead of preserving pairwise geodesic distances, the low-dimensional
embedding is computed via a path-mapping algorithm. Due to the much fewer
number of paths compared to number of data points, a significant improvement in
time and memory complexity without any decline in performance is achieved. The
method demonstrates state-of-the-art performance on well-known synthetic and
real-world datasets, as well as in the presence of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0804</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0804</id><created>2013-12-03</created><authors><author><keyname>Ghosh</keyname><forenames>Pramit</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>Round-The-Clock Urine Sugar Monitoring System for Diabetic Patients</title><categories>cs.CY</categories><comments>5 pages, International Conference</comments><journal-ref>International Conference on Systems in Medicine and Biology-ICSMB
  2010 ISBN: 978-1-61284-039-0</journal-ref><doi>10.1109/ICSMB.2010.5735397</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  It is known that diabetes can not be cured completely, but it can be
controlled. The objective of this work is to provide an automatic system that
will be able to help the diabetic patient to control the blood sugar. This
system measures the blood sugar level of the people from their urine
round-the-clock. A recorded message based on this input may be displayed so
that apart from patient himself others can be informed about his/her present
sugar level. That should help him/her in taking medicine; controlling diet etc.
This work is an application of image processing and fuzzy logic. It is known
that Benedict's reagent changes its colour based on the sugar level. This
colour change information is sensed by the transducer and fed to the fuzzy
logic unit for decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0809</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0809</id><created>2013-12-03</created><authors><author><keyname>Ghosh</keyname><forenames>Pramit</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>Automatic White Blood Cell Measuring Aid for Medical Diagnosis</title><categories>cs.CY cs.CV</categories><comments>6 pages, International Conference</comments><journal-ref>International Conference on Process Automation, Control and
  Computing- PACC 2011 ISBN: 978-1-61284-762-7</journal-ref><doi>10.1109/PACC.2011.5978895</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Blood related invasive pathological investigations play a major role in
diagnosis of diseases. But in India and other third world countries there are
no enough pathological infrastructures for medical diagnosis. Moreover, most of
the remote places of those countries have neither pathologists nor physicians.
Telemedicine partially solves the lack of physicians. But the pathological
investigation infrastructure can not be integrated with the telemedicine
technology. The objective of this work is to automate the blood related
pathological investigation process. Detection of different white blood cells
has been automated in this work. This system can be deployed in the remote area
as a supporting aid for telemedicine technology and only high school education
is sufficient to operate it. The proposed system achieved 97.33 percent
accuracy for the samples collected to test this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0821</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0821</id><created>2013-12-03</created><updated>2014-09-05</updated><authors><author><keyname>Zhang</keyname><forenames>Renyuan</forenames></author><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Gan</keyname><forenames>Yongmei</forenames></author><author><keyname>Wonham</keyname><forenames>W. M.</forenames></author></authors><title>Delay-Robustness in Distributed Control of Timed Discrete-Event Systems
  Based on Supervisor Localization</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently we studied communication delay in distributed control of untimed
discrete-event systems based on supervisor localization. We proposed a property
called delay-robustness: the overall system behavior controlled by distributed
controllers with communication delay is logically equivalent to its delay-free
counterpart. In this paper we extend our previous work to timed discrete-event
systems, in which communication delays are counted by a special clock event
{\it tick}. First, we propose a timed channel model and define timed
delay-robustness; for the latter, a polynomial verification procedure is
presented. Next, if the delay-robust property does not hold, we introduce
bounded delay-robustness, and present an algorithm to compute the maximal delay
bound (measured by number of ticks) for transmitting a channeled event.
Finally, we demonstrate delay-robustness on the example of an under-load
tap-changing transformer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0825</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0825</id><created>2013-12-03</created><authors><author><keyname>Cai</keyname><forenames>Sheng</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author></authors><title>FRANTIC: A Fast Reference-based Algorithm for Network Tomography via
  Compressive Sensing</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications. A preliminary
  version of this paper will be presented at the 6th International Conference
  on Communication System &amp; Networks (COMSNETS) - 2014. A poster based on this
  work was also presented at International Symposium on Information Theory 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of link and node delay estimation in undirected networks
when at most k out of n links or nodes in the network are congested. Our
approach relies on end-to-end measurements of path delays across pre-specified
paths in the network. We present a class of algorithms that we call FRANTIC.
The FRANTIC algorithms are motivated by compressive sensing; however, unlike
traditional compressive sensing, the measurement design here is constrained by
the network topology and the matrix entries are constrained to be positive
integers. A key component of our design is a new compressive sensing algorithm
SHO-FA-INT that is related to the prior SHO-FA algorithm for compressive
sensing, but unlike SHO-FA, the matrix entries here are drawn from the set of
integers {0, 1, ..., M}. We show that O(k log n /log M) measurements suffice
both for SHO-FA-INT and FRANTIC. Further, we show that the computational
complexity of decoding is also O(k log n/log M) for each of these algorithms.
Finally, we look at efficient constructions of the measurement operations
through Steiner Trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0841</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0841</id><created>2013-12-03</created><authors><author><keyname>Ruijl</keyname><forenames>Ben</forenames></author><author><keyname>Vermaseren</keyname><forenames>Jos</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author></authors><title>Combining Simulated Annealing and Monte Carlo Tree Search for Expression
  Simplification</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications of computer algebra large expressions must be simplified
to make repeated numerical evaluations tractable. Previous works presented
heuristically guided improvements, e.g., for Horner schemes. The remaining
expression is then further reduced by common subexpression elimination. A
recent approach successfully applied a relatively new algorithm, Monte Carlo
Tree Search (MCTS) with UCT as the selection criterion, to find better variable
orderings. Yet, this approach is fit for further improvements since it is
sensitive to the so-called exploration-exploitation constant $C_p$ and the
number of tree updates $N$. In this paper we propose a new selection criterion
called Simulated Annealing UCT (SA-UCT) that has a dynamic
exploration-exploitation parameter, which decreases with the iteration number
$i$ and thus reduces the importance of exploration over time. First, we provide
an intuitive explanation in terms of the exploration-exploitation behavior of
the algorithm. Then, we test our algorithm on three large expressions of
different origins. We observe that SA-UCT widens the interval of good initial
values $C_p$ where best results are achieved. The improvement is large (more
than a tenfold) and facilitates the selection of an appropriate $C_p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0852</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0852</id><created>2013-12-03</created><authors><author><keyname>Bandyopadhyay</keyname><forenames>Samir Kumar</forenames></author><author><keyname>Arunkumar</keyname><forenames>S</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Saptarshi</forenames></author></authors><title>Feature Extraction of Human Lip Prints</title><categories>cs.CV</categories><comments>8 pages, 18 figures</comments><journal-ref>Journal of Current Computer Science and Technology Vol. 2 Issue 1
  [2012] 01-08</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods have been used for identification of human by recognizing lip prints.
Human lips have a number of elevation and depressions features called lip
prints and examination of lip prints is referred to as cheiloscopy. Lip prints
of each human being are unique in nature like many others features of human. In
this paper lip print is first smoothened using a Gaussian Filter. Next Sobel
Edge Detector and Canny Edge Detector are used to detect the vertical and
horizontal groove pattern in the lip. This method of identification will be
useful both in criminal forensics and personal identification. It is our
assumption that study of lip prints and their types are well connected to play
a song in a better way that are well accepted to people who loves to hear
songs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0860</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0860</id><created>2013-12-03</created><authors><author><keyname>Hu</keyname><forenames>Zhiting</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Yao</keyname><forenames>Junjie</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author><author><keyname>Yin</keyname><forenames>Hongzhi</forenames></author><author><keyname>Cui</keyname><forenames>Bin</forenames></author></authors><title>Community Specific Temporal Topic Discovery from Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 16 figures, submitted to VLDB 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying temporal dynamics of topics in social media is very useful to
understand online user behaviors. Most of the existing work on this subject
usually monitors the global trends, ignoring variation among communities. Since
users from different communities tend to have varying tastes and interests,
capturing community-level temporal change can improve the understanding and
management of social content. Additionally, it can further facilitate the
applications such as community discovery, temporal prediction and online
marketing. However, this kind of extraction becomes challenging due to the
intricate interactions between community and topic, and intractable
computational complexity.
  In this paper, we take a unified solution towards the community-level topic
dynamic extraction. A probabilistic model, CosTot (Community Specific
Topics-over-Time) is proposed to uncover the hidden topics and communities, as
well as capture community-specific temporal dynamics. Specifically, CosTot
considers text, time, and network information simultaneously, and well
discovers the interactions between community and topic over time. We then
discuss the approximate inference implementation to enable scalable computation
of model parameters, especially for large social data. Based on this, the
application layer support for multi-scale temporal analysis and community
exploration is also investigated.
  We conduct extensive experimental studies on a large real microblog dataset,
and demonstrate the superiority of proposed model on tasks of time stamp
prediction, link prediction and topic perplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0882</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0882</id><created>2013-12-03</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>On the Throughput of Hybrid-ARQ under QoS Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid Automatic Repeat Request (HARQ) is a high performance communication
protocol, leading to effective use of the wireless channel and the resources
with only limited feedback about the channel state information (CSI) to the
transmitter. In this paper, the throughput of HARQ with incremental redundancy
(IR) and fixed transmission rate is studied in the presence of quality of
service (QoS) constraints imposed as limitations on buffer overflow
probabilities. In particular, tools from the theory of renewal processes and
stochastic network calculus are employed to characterize the maximum arrival
rates that can be supported by the wireless channel when HARQ-IR is adopted.
Effective capacity is employed as the throughput metric and a closed-form
expression for the effective capacity of HARQ-IR is determined for small values
of the QoS exponent. The impact of the fixed transmission rate, QoS
constraints, and hard deadline limitations on the throughput is investigated
and comparisons with regular ARQ operation are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0883</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0883</id><created>2013-12-03</created><authors><author><keyname>Coarasa</keyname><forenames>Antonio Hernandez</forenames></author><author><keyname>Nintanavongsa</keyname><forenames>Prusayon</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Chowdhury</keyname><forenames>Kaushik R.</forenames></author></authors><title>Impact of Mobile Transmitter Sources on Radio Frequency Wireless Energy
  Harvesting</title><categories>cs.NI</categories><comments>5 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless energy harvesting sensor networks constitute a new paradigm, where
the motes deployed in the field are no longer constrained by the limited
battery resource, but are able to re-charge themselves through directed
electromagnetic energy transfer. The energy sources, which we call actors, are
mobile and move along pre-decided patterns while radiating an appropriate level
of energy, sufficient enough to charge the sensors at an acceptable rate. This
is the first work that investigates the impact of energy transfer, especially
concerning the energy gain in the sensors, the energy spent by the actors, and
the overall lifetime in the resulting mobile sensor-actor networks. We propose
two event-specific mobility models, where the events occur at the centers of a
Voronoi tessellation, and the actors move along either (i)the edges of the
Voronoi cells, or (ii) directly from one event center to another. We undertake
a comprehensive simulation based study using traces obtained from our
experimental energy harvesting circuits powering Mica2 motes. Our results
reveal several non-intuitive outcomes, and provide guidelines on which mobility
model may be adopted based on the distribution of the events and actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0884</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0884</id><created>2013-12-03</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Linear transformation distance for bichromatic matchings</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P=B\cup R$ be a set of $2n$ points in general position, where $B$ is a
set of $n$ blue points and $R$ a set of $n$ red points. A \emph{$BR$-matching}
is a plane geometric perfect matching on $P$ such that each edge has one red
endpoint and one blue endpoint. Two $BR$-matchings are compatible if their
union is also plane.
  The \emph{transformation graph of $BR$-matchings} contains one node for each
$BR$-matching and an edge joining two such nodes if and only if the
corresponding two $BR$-matchings are compatible. In SoCG 2013 it has been shown
by Aloupis, Barba, Langerman, and Souvaine that this transformation graph is
always connected, but its diameter remained an open question. In this paper we
provide an alternative proof for the connectivity of the transformation graph
and prove an upper bound of $2n$ for its diameter, which is asymptotically
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0885</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0885</id><created>2013-11-29</created><updated>2014-10-05</updated><authors><author><keyname>Saha</keyname><forenames>Dipankar</forenames></author><author><keyname>Chatterjee</keyname><forenames>Aanan</forenames></author><author><keyname>Chatterjee</keyname><forenames>Sayan</forenames></author><author><keyname>Sarkar</keyname><forenames>C. K.</forenames></author></authors><title>Row-Based Dual Vdd Assignment, for a Level Converter Free CSA Design and
  Its Near-Threshold Operation</title><categories>cs.AR</categories><comments>Final Version of this work is available @ Advances in Electrical
  Engineering, Hindawi Publishing Corporation (Volume 2014 (2014), Article ID
  814975, 6 pages)</comments><doi>10.1155/2014/814975</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subthreshold circuit designs are very much popular for some of the ultra low
power applications, where the minimum energy consumption is the primary
concern. But, due to the weak driving current, these circuits generally suffer
from huge performance degradation. Therefore, in this paper, we primarily
targeted to analyze the performance of a Near-Threshold Circuit (NTC), which
retains the excellent energy efficiency of the subthreshold design, while
improving the performance to a certain extent. A modified row-based dual Vdd
4-operand CSA (Carry Save Adder) design has been reported in the present work
using 45 nm technology. Moreover, to find out the effectiveness of the
near-threshold operation of the 4-operand CSA design; it has been compared with
the other design styles. From the simulation results, obtained for the
frequency of 20 MHz, we found that the proposed scheme of CSA design consumes
3.009*10-7 Watt of Average Power (Pavg), which is almost 90.9 % lesser than
that of the conventional CSA design. Whereas, looking at the perspective of
maximum delay at output, the proposed scheme of CSA design provides a fair
44.37 % improvement, compared to that of the subthreshold CSA design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0903</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0903</id><created>2013-12-03</created><authors><author><keyname>Deineko</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Klinz</keyname><forenames>Bettina</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Uniqueness in quadratic and hyperbolic 0-1 programming problems</title><categories>math.CO cs.CC</categories><comments>6 pages</comments><msc-class>90C20, 90C32, 68Q17</msc-class><journal-ref>Operations research letters 41, 2013, 633-635</journal-ref><doi>10.1016/j.orl.2013.08.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the question of deciding whether a quadratic or a hyperbolic 0-1
programming instance has a unique optimal solution. Both uniqueness questions
are known to be NP-hard, but are unlikely to be contained in the class NP. We
precisely pinpoint their computational complexity by showing that they both are
complete for the complexity class {\mbox{$\Delta_2$P}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0910</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0910</id><created>2013-12-03</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Rieder</keyname><forenames>Steven</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>MPWide: a light-weight library for efficient message passing over wide
  area networks</title><categories>cs.DC cs.NI</categories><comments>accepted by the Journal Of Open Research Software, 13 pages, 4
  figures, 1 table</comments><journal-ref>Journal of Open Research Software 1(1):e9, 2013</journal-ref><doi>10.5334/jors.ah</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present MPWide, a light weight communication library which allows
efficient message passing over a distributed network. MPWide has been designed
to connect application running on distributed (super)computing resources, and
to maximize the communication performance on wide area networks for those
without administrative privileges. It can be used to provide message-passing
between application, move files, and make very fast connections in
client-server environments. MPWide has already been applied to enable
distributed cosmological simulations across up to four supercomputers on two
continents, and to couple two different bloodflow simulations to form a
multiscale simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0912</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0912</id><created>2013-12-03</created><authors><author><keyname>Sarraute</keyname><forenames>Carlos</forenames></author><author><keyname>Calderon</keyname><forenames>Gervasio</forenames></author></authors><title>Evolution of Communities with Focus on Stability</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>AST at 42nd JAIIO, September 16-20, 2013, Cordoba, Argentina. arXiv
  admin note: substantial text overlap with arXiv:1311.5502</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Community detection is an important tool for analyzing the social graph of
mobile phone users. The problem of finding communities in static graphs has
been widely studied. However, since mobile social networks evolve over time,
static graph algorithms are not sufficient. To be useful in practice (e.g. when
used by a telecom analyst), the stability of the partitions becomes critical.
We tackle this particular use case in this paper: tracking evolution of
communities in dynamic scenarios with focus on stability. We propose two
modifications to a widely used static community detection algorithm: we
introduce fixed nodes and preferential attachment to pre-existing communities.
We then describe experiments to study the stability and quality of the
resulting partitions on real-world social networks, represented by monthly call
graphs for millions of subscribers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0914</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0914</id><created>2013-12-03</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>Characterizing the Rate Region of the (4,3,3) Exact-Repair Regenerating
  Codes</title><categories>cs.IT math.IT</categories><comments>This is the extended version of the conference paper arXiv:1305.2440,
  with more details on the computed aided proof approach, as well a further
  simplified outer bound proof. Accepted for publication in IEEE JSAC on
  Communication Methodologies for the Next-Generation Storage Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact-repair regenerating codes are considered for the case (n,k,d)=(4,3,3),
for which a complete characterization of the rate region is provided. This
characterization answers in the affirmative the open question whether there
exists a non-vanishing gap between the optimal bandwidth-storage tradeoff of
the functional-repair regenerating codes (i.e., the cut-set bound) and that of
the exact-repair regenerating codes. To obtain an explicit information
theoretic converse, a computer-aided proof (CAP) approach based on primal and
dual relation is developed. This CAP approach extends Yeung's linear
programming (LP) method, which was previously only used on information
theoretic problems with a few random variables due to the exponential growth of
the number of variables in the corresponding LP problem. The symmetry in the
exact-repair regenerating code problem allows an effective reduction of the
number of variables, and together with several other problem-specific
reductions, the LP problem is reduced to a manageable scale. For the
achievability, only one non-trivial corner point of the rate region needs to be
addressed in this case, for which an explicit binary code construction is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0917</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0917</id><created>2013-12-03</created><updated>2013-12-04</updated><authors><author><keyname>Gianelle</keyname><forenames>A.</forenames></author><author><keyname>Amerio</keyname><forenames>S.</forenames></author><author><keyname>Bastieri</keyname><forenames>D.</forenames></author><author><keyname>Corvo</keyname><forenames>M.</forenames></author><author><keyname>Ketchum</keyname><forenames>W.</forenames></author><author><keyname>Liu</keyname><forenames>T.</forenames></author><author><keyname>Lonardo</keyname><forenames>A.</forenames></author><author><keyname>Lucchesi</keyname><forenames>D.</forenames></author><author><keyname>Poprocki</keyname><forenames>S.</forenames></author><author><keyname>Rivera</keyname><forenames>R.</forenames></author><author><keyname>Tosoratto</keyname><forenames>L.</forenames></author><author><keyname>Vicini</keyname><forenames>P.</forenames></author><author><keyname>Wittich</keyname><forenames>P.</forenames></author></authors><title>Applications of Many-Core Technologies to On-line Event Reconstruction
  in High Energy Physics Experiments</title><categories>physics.ins-det cs.DC hep-ex</categories><comments>Proceedings for 2013 IEEE NSS/MIC conference; fixed author list
  omission</comments><doi>10.1109/NSSMIC.2013.6829552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in many-core architectures applied to real time selections is
growing in High Energy Physics (HEP) experiments. In this paper we describe
performance measurements of many-core devices when applied to a typical HEP
online task: the selection of events based on the trajectories of charged
particles. We use as benchmark a scaled-up version of the algorithm used at CDF
experiment at Tevatron for online track reconstruction - the SVT algorithm - as
a realistic test-case for low-latency trigger systems using new computing
architectures for LHC experiment. We examine the complexity/performance
trade-off in porting existing serial algorithms to many-core devices. We
measure performance of different architectures (Intel Xeon Phi and AMD GPUs, in
addition to NVidia GPUs) and different software environments (OpenCL, in
addition to NVidia CUDA). Measurements of both data processing and data
transfer latency are shown, considering different I/O strategies to/from the
many-core devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0925</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0925</id><created>2013-12-03</created><updated>2014-05-14</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author></authors><title>Understanding Alternating Minimization for Matrix Completion</title><categories>cs.LG cs.DS stat.ML</categories><comments>Slightly improved main theorem and a correction: The tail bound
  stated in Lemma A.5 of the previous version is incorrect. See manuscript for
  fix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating Minimization is a widely used and empirically successful
heuristic for matrix completion and related low-rank optimization problems.
Theoretical guarantees for Alternating Minimization have been hard to come by
and are still poorly understood. This is in part because the heuristic is
iterative and non-convex in nature. We give a new algorithm based on
Alternating Minimization that provably recovers an unknown low-rank matrix from
a random subsample of its entries under a standard incoherence assumption. Our
results reduce the sample size requirements of the Alternating Minimization
approach by at least a quartic factor in the rank and the condition number of
the unknown matrix. These improvements apply even if the matrix is only close
to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in
the dimension of the matrix and, in a broad range of parameters, gives the
strongest sample bounds among all subquadratic time algorithms that we are
aware of.
  Underlying our work is a new robust convergence analysis of the well-known
Power Method for computing the dominant singular vectors of a matrix. This
viewpoint leads to a conceptually simple understanding of Alternating
Minimization. In addition, we contribute a new technique for controlling the
coherence of intermediate solutions arising in iterative algorithms based on a
smoothed analysis of the QR factorization. These techniques may be of interest
beyond their application here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0932</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0932</id><created>2013-12-03</created><updated>2015-05-26</updated><authors><author><keyname>Aguerri</keyname><forenames>I&#xf1;aki Estella</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Joint Source-Channel Coding with Time-Varying Channel and
  Side-Information</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of a Gaussian source over a time-varying Gaussian channel is
studied in the presence of time-varying correlated side information at the
receiver. A block fading model is considered for both the channel and the side
information, whose states are assumed to be known only at the receiver. The
optimality of separate source and channel coding in terms of average end-to-end
distortion is shown when the channel is static while the side information state
follows a discrete or a continuous and quasiconcave distribution. When both the
channel and side information states are time-varying, separate source and
channel coding is suboptimal in general. A partially informed encoder lower
bound is studied by providing the channel state information to the encoder.
Several achievable transmission schemes are proposed based on uncoded
transmission, separate source and channel coding, joint decoding as well as
hybrid digital-analog transmission. Uncoded transmission is shown to be optimal
for a class of continuous and quasiconcave side information state
distributions, while the channel gain may have an arbitrary distribution. To
the best of our knowledge, this is the first example in which the uncoded
transmission achieves the optimal performance thanks to the time-varying nature
of the states, while it is suboptimal in the static version of the same
problem. Then, the optimal \emph{distortion exponent}, that quantifies the
exponential decay rate of the expected distortion in the high SNR regime, is
characterized for Nakagami distributed channel and side information states, and
it is shown to be achieved by hybrid digital-analog and joint decoding schemes
in certain cases, illustrating the suboptimality of pure digital or analog
transmission in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0938</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0938</id><created>2013-12-02</created><authors><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Epidemic Thresholds with External Agents</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 2 figures (to appear in INFOCOM 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of external infection sources on phase transitions in
epidemic processes. In particular, we consider an epidemic spreading on a
network via the SIS/SIR dynamics, which in addition is aided by external agents
- sources unconstrained by the graph, but possessing a limited infection rate
or virulence. Such a model captures many existing models of externally aided
epidemics, and finds use in many settings - epidemiology, marketing and
advertising, network robustness, etc. We provide a detailed characterization of
the impact of external agents on epidemic thresholds. In particular, for the
SIS model, we show that any external infection strategy with constant virulence
either fails to significantly affect the lifetime of an epidemic, or at best,
sustains the epidemic for a lifetime which is polynomial in the number of
nodes. On the other hand, a random external-infection strategy, with rate
increasing linearly in the number of infected nodes, succeeds under some
conditions to sustain an exponential epidemic lifetime. We obtain similar sharp
thresholds for the SIR model, and discuss the relevance of our results in a
variety of settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0940</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0940</id><created>2013-12-03</created><authors><author><keyname>Ghosh</keyname><forenames>Pramit</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>Medical Aid for Automatic Detection of Malaria</title><categories>cs.CY cs.CV</categories><comments>8 pages, International Conference on Computer Information Systems and
  Industrial Management Applications 2011. arXiv admin note: text overlap with
  arXiv:1312.0809</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The analysis and counting of blood cells in a microscope image can provide
useful information concerning to the health of a person. In particular,
morphological analysis of red blood cells deformations can effectively detect
important disease like malaria. Blood images, obtained by the microscope, which
is coupled with a digital camera, are analyzed by the computer for diagnosis or
can be transmitted easily to clinical centers than liquid blood samples.
Automatic analysis system for the presence of Plasmodium in microscopic image
of blood can greatly help pathologists and doctors that typically inspect blood
films manually. Unfortunately, the analysis made by human experts is not rapid
and not yet standardized due to the operators capabilities and tiredness. The
paper shows how effectively and accurately it is possible to identify the
Plasmodium in the blood film. In particular, the paper presents how to enhance
the microscopic image and filter out the unnecessary segments followed by the
threshold based segmentation and recognize the presence of Plasmodium. The
proposed system can be deployed in the remote area as a supporting aid for
telemedicine technology and only basic training is sufficient to operate it.
This system achieved more than 98 percentage accuracy for the samples collected
to test this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0972</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0972</id><created>2013-12-03</created><updated>2014-12-30</updated><authors><author><keyname>Gad</keyname><forenames>Eyal En</forenames><affiliation>Andrew</affiliation></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames><affiliation>Andrew</affiliation></author><author><keyname>Anxiao</keyname><affiliation>Andrew</affiliation></author><author><keyname>Jiang</keyname></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Rank-Modulation Rewrite Coding for Flash Memories</title><categories>cs.IT math.IT</categories><comments>Revised version for IEEE transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current flash memory technology focuses on the cost minimization of its
static storage capacity. However, the resulting approach supports a relatively
small number of program-erase cycles. This technology is effective for consumer
devices (e.g., smartphones and cameras) where the number of program-erase
cycles is small. However, it is not economical for enterprise storage systems
that require a large number of lifetime writes. The proposed approach in this
paper for alleviating this problem consists of the efficient integration of two
key ideas: (i) improving reliability and endurance by representing the
information using relative values via the rank modulation scheme and (ii)
increasing the overall (lifetime) capacity of the flash device via rewriting
codes, namely, performing multiple writes per cell before erasure. This paper
presents a new coding scheme that combines rank modulation with rewriting. The
key benefits of the new scheme include: (i) the ability to store close to 2
bits per cell on each write with minimal impact on the lifetime of the memory,
and (ii) efficient encoding and decoding algorithms that make use of
capacity-achieving write-once-memory (WOM) codes that were proposed recently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0976</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0976</id><created>2013-12-03</created><updated>2014-05-12</updated><authors><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author></authors><title>Multilinguals and Wikipedia Editing</title><categories>cs.CY cs.CL cs.DL cs.SI physics.soc-ph</categories><acm-class>H.5.4; H.5.3</acm-class><journal-ref>Proceedings of the 6th Annual ACM Web Science Conference, WebSci
  2014, ACM</journal-ref><doi>10.1145/2615569.2615684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article analyzes one month of edits to Wikipedia in order to examine the
role of users editing multiple language editions (referred to as multilingual
users). Such multilingual users may serve an important function in diffusing
information across different language editions of the encyclopedia, and prior
work has suggested this could reduce the level of self-focus bias in each
edition. This study finds multilingual users are much more active than their
single-edition (monolingual) counterparts. They are found in all language
editions, but smaller-sized editions with fewer users have a higher percentage
of multilingual users than larger-sized editions. About a quarter of
multilingual users always edit the same articles in multiple languages, while
just over 40% of multilingual users edit different articles in different
languages. When non-English users do edit a second language edition, that
edition is most frequently English. Nonetheless, several regional and
linguistic cross-editing patterns are also present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0979</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0979</id><created>2013-12-03</created><authors><author><keyname>Boyer</keyname><forenames>Michel</forenames></author><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Mor</keyname><forenames>Tal</forenames></author></authors><title>Attacks on Fixed Apparatus Quantum Key Distribution Schemes</title><categories>quant-ph cs.CR</categories><comments>10 pages, 3 figure, 8 tables. A preliminary version of this
  manuscript appeared in TPNC '12: Proceedings of the 1st International
  Conference on Theory and Practice of Natural Computing, LNCS, Vol. 7505
  (2012)</comments><journal-ref>Physical Review A 90, 012329 (2014)</journal-ref><doi>10.1103/PhysRevA.90.012329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider quantum key distribution implementations in which the receiver's
apparatus is fixed and does not depend on his choice of basis at each qubit
transmission. We show that, although theoretical quantum key distribution is
proven secure, such implementations are totally insecure against a strong
eavesdropper that has one-time (single) access to the receiver's equipment. The
attack we present here, the &quot;fixed-apparatus attack&quot; causes a potential risk to
the usefulness of several recent implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0984</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0984</id><created>2013-12-03</created><updated>2015-12-15</updated><authors><author><keyname>Perrey</keyname><forenames>Heiner</forenames></author><author><keyname>Landsmann</keyname><forenames>Martin</forenames></author><author><keyname>Ugus</keyname><forenames>Osman</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>TRAIL: Topology Authentication in RPL</title><categories>cs.NI cs.CR</categories><proxy>Matthias W&#xc3;&#x83;&#xc2;&#xa4;hlisch</proxy><acm-class>C.2.2; C.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IPv6 Routing Protocol for Low-Power and Lossy Networks (RPL) was recently
introduced as the new routing standard for the Internet of Things. Although RPL
defines basic security modes, it remains vulnerable to topological attacks
which facilitate blackholing, interception, and resource exhaustion. We are
concerned with analyzing the corresponding threats and protecting future RPL
deployments from such attacks.
  Our contributions are twofold. First, we analyze the state of the art, in
particular the protective scheme VeRA and present two new rank order attacks as
well as extensions to mitigate them. Second, we derive and evaluate TRAIL, a
generic scheme for topology authentication in RPL. TRAIL solely relies on the
basic assumptions of RPL that (1) the root node serves as a trust anchor and
(2) each node interconnects to the root as part of a hierarchy. Using proper
reachability tests, TRAIL scalably and reliably identifies any topological
attacker without strong cryptographic efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.0994</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.0994</id><created>2013-12-03</created><authors><author><keyname>Mercian</keyname><forenames>Anu</forenames></author><author><keyname>McGarry</keyname><forenames>Michael P.</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author></authors><title>Impact of Report Message Scheduling (RMS) in 1G/10G EPON and GPON
  (Extended Version)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide array of dynamic bandwidth allocation (DBA) mechanisms have recently
been proposed for improving bandwidth utilization and reducing idle times and
packets delays in passive optical networks (PONs). The DBA evaluation studies
commonly assumed that the report message for communicating the bandwidth
demands of the distributed optical network units (ONUs) to the central optical
line terminal (OLT) is scheduled for the end of an ONU's upstream transmission,
after the ONU's payload data transmissions. In this article, we conduct a
detailed investigation of the impact of the report message scheduling (RMS),
either at the beginning (i.e., before the pay load data) or the end of an ONU
upstream transmission on PON performance. We analytically characterize the
reduction in channel idle time with reporting at the beginning of an upstream
transmission compared to reporting at the end. Our extensive simulation
experiments consider both the Ethernet Passive Optical Networking (EPON)
standard and the Gigabit PON (GPON) standard. We find that for DBAs with
offline sizing and scheduling of ONU upstream transmission grants at the end of
a polling cycle, which processes requests from all ONUs, reporting at the
beginning gives substantial reductions of mean packet delay at high loads. For
high-performing DBAs with online grant sizing and scheduling, which immediately
processes individual ONU requests, or interleaving of ONUs groups, both
reporting at the beginning or end give essentially the same average packet
delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1001</identifier>
 <datestamp>2014-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1001</id><created>2013-12-03</created><updated>2014-04-04</updated><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>Optimal detection of intersections between convex polyhedra</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a polyhedron $P$ in $\mathbb{R}^d$, denote by $|P|$ its combinatorial
complexity, i.e., the number of faces of all dimensions of the polyhedra. In
this paper, we revisit the classic problem of preprocessing polyhedra
independently so that given two preprocessed polyhedra $P$ and $Q$ in
$\mathbb{R}^d$, each translated and rotated, their intersection can be tested
rapidly.
  For $d=3$ we show how to perform such a test in $O(\log |P| + \log |Q|)$ time
after linear preprocessing time and space. This running time is the best
possible and improves upon the last best known query time of $O(\log|P|
\log|Q|)$ by Dobkin and Kirkpatrick (1990).
  We then generalize our method to any constant dimension $d$, achieving the
same optimal $O(\log |P| + \log |Q|)$ query time using a representation of size
$O(|P|^{\lfloor d/2\rfloor + \varepsilon})$ for any $\varepsilon&gt;0$ arbitrarily
small. This answers an even older question posed by Dobkin and Kirkpatrick 30
years ago.
  In addition, we provide an alternative $O(\log |P| + \log |Q|)$ algorithm to
test the intersection of two convex polygons $P$ and $Q$ in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1003</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1003</id><created>2013-12-03</created><authors><author><keyname>Senanayake</keyname><forenames>Upul</forenames></author><author><keyname>Prabuddha</keyname><forenames>Rahal</forenames></author><author><keyname>Ragel</keyname><forenames>Roshan</forenames></author></authors><title>High Throughput Virtual Screening with Data Level Parallelism in
  Multi-core Processors</title><categories>cs.AI cs.PF</categories><comments>Information and Automation for Sustainability (ICIAfS), 2012 IEEE 6th
  International Conference on</comments><doi>10.1109/ICIAFS.2012.6419885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the throughput of molecular docking, a computationally intensive
phase of the virtual screening process, is a highly sought area of research
since it has a significant weight in the drug designing process. With such
improvements, the world might find cures for incurable diseases like HIV
disease and Cancer sooner. Our approach presented in this paper is to utilize a
multi-core environment to introduce Data Level Parallelism (DLP) to the
Autodock Vina software, which is a widely used for molecular docking software.
Autodock Vina already exploits Instruction Level Parallelism (ILP) in
multi-core environments and therefore optimized for such environments. However,
with the results we have obtained, it can be clearly seen that our approach has
enhanced the throughput of the already optimized software by more than six
times. This will dramatically reduce the time consumed for the lead
identification phase in drug designing along with the shift in the processor
technology from multi-core to many-core of the current era. Therefore, we
believe that the contribution of this project will effectively make it possible
to expand the number of small molecules docked against a drug target and
improving the chances to design drugs for incurable diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1004</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1004</id><created>2013-12-03</created><authors><author><keyname>Chen</keyname><forenames>Ko-Feng</forenames></author><author><keyname>Liu</keyname><forenames>Yen-Cheng</forenames></author><author><keyname>Su</keyname><forenames>Yu T.</forenames></author></authors><title>Composite Channel Estimation in Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures, submitted to IEEE Journal of Selected Topics in
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiuser (MU) multiple-input multiple-output (MIMO)
time-division duplexing (TDD) system in which the base station (BS) is equipped
with a large number of antennas for communicating with single-antenna mobile
users. In such a system the BS has to estimate the channel state information
(CSI) that includes large-scale fading coefficients (LSFCs) and small-scale
fading coefficients (SSFCs) by uplink pilots. Although information about the
former FCs are indispensable in a MU-MIMO or distributed MIMO system, they are
usually ignored or assumed perfectly known when treating the MIMO CSI
estimation problem. We take advantage of the large spatial samples of a massive
MIMO BS to derive accurate LSFC estimates in the absence of SSFC information.
With estimated LSFCs, SSFCs are then obtained using a rank-reduced (RR) channel
model which in essence transforms the channel vector into a lower dimension
representation.
  We analyze the mean squared error (MSE) performance of the proposed composite
channel estimator and prove that the separable angle of arrival (AoA)
information provided by the RR model is beneficial for enhancing the
estimator's performance, especially when the angle spread of the uplink signal
is not too large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1017</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1017</id><created>2013-12-03</created><updated>2015-03-20</updated><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author><author><keyname>Seeman</keyname><forenames>Lior</forenames></author></authors><title>The Truth Behind the Myth of the Folk Theorem</title><categories>cs.GT</categories><comments>Based on work presented in ITCS 2014 and WINE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing an $\epsilon$-Nash equilibrium in repeated
games. Earlier work by Borgs et al. [2010] suggests that this problem is
intractable. We show that if we make a slight change to their model---modeling
the players as polynomial-time Turing machines that maintain state ---and make
some standard cryptographic hardness assumptions (the existence of public-key
encryption), the problem can actually be solved in polynomial time. Our
algorithm works not only for games with a finite number of players, but also
for constant-degree graphical games.
  As Nash equilibrium is a weak solution concept for extensive form games, we
additionally define and study an appropriate notion of a subgame-perfect
equilibrium for computationally bounded players, and show how to efficiently
find such an equilibrium in repeated games (again, making standard
cryptographic hardness assumptions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1020</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1020</id><created>2013-12-03</created><updated>2015-04-04</updated><authors><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Sha</keyname><forenames>Wei E. I.</forenames></author><author><keyname>Chao</keyname><forenames>Hongyang</forenames></author><author><keyname>Jin</keyname><forenames>Zhu</forenames></author></authors><title>High-quality Image Restoration from Partial Mixed Adaptive-Random
  Measurements</title><categories>cs.IT math.IT</categories><comments>16 pages, 8 figures</comments><doi>10.1007/s11042-015-2566-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel framework to construct an efficient sensing (measurement) matrix,
called mixed adaptive-random (MAR) matrix, is introduced for directly acquiring
a compressed image representation. The mixed sampling (sensing) procedure
hybridizes adaptive edge measurements extracted from a low-resolution image
with uniform random measurements predefined for the high-resolution image to be
recovered. The mixed sensing matrix seamlessly captures important information
of an image, and meanwhile approximately satisfies the restricted isometry
property. To recover the high-resolution image from MAR measurements, the total
variation algorithm based on the compressive sensing theory is employed for
solving the Lagrangian regularization problem. Both peak signal-to-noise ratio
and structural similarity results demonstrate the MAR sensing framework shows
much better recovery performance than the completely random sensing one. The
work is particularly helpful for high-performance and lost-cost data
acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1024</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1024</id><created>2013-12-04</created><updated>2014-02-26</updated><authors><author><keyname>Williamson</keyname><forenames>Adam R.</forenames></author><author><keyname>Marshall</keyname><forenames>Matthew J.</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Reliability-output Decoding of Tail-biting Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transaction on Communications. 10 pages, 8 figures,
  2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present extensions to Raghavan and Baum's reliability-output Viterbi
algorithm (ROVA) to accommodate tail-biting convolutional codes. These
tail-biting reliability-output algorithms compute the exact word-error
probability of the decoded codeword after first calculating the posterior
probability of the decoded tail-biting codeword's starting state. One approach
employs a state-estimation algorithm that selects the maximum a posteriori
state based on the posterior distribution of the starting states. Another
approach is an approximation to the exact tail-biting ROVA that estimates the
word-error probability. A comparison of the computational complexity of each
approach is discussed in detail. The presented reliability-output algorithms
apply to both feedforward and feedback tail-biting convolutional encoders.
These tail-biting reliability-output algorithms are suitable for use in
reliability-based retransmission schemes with short blocklengths, in which
terminated convolutional codes would introduce rate loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1027</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1027</id><created>2013-12-04</created><updated>2013-12-10</updated><authors><author><keyname>Zhandry</keyname><forenames>Mark</forenames></author></authors><title>A Note on the Quantum Collision and Set Equality Problems</title><categories>cs.CC quant-ph</categories><comments>10 pages. v2: fixed typos. v3: added set equality result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results showing a quantum query complexity of $\Theta(N^{1/3})$ for the
collision problem do not apply to random functions. The issues are two-fold.
First, the $\Omega(N^{1/3})$ lower bound only applies when the range is no
larger than the domain, which precludes many of the cryptographically
interesting applications. Second, most of the results in the literature only
apply to $r$-to-1 functions, which are quite different from random functions.
Understanding the collision problem for random functions is of great importance
to cryptography, and we seek to fill the gaps of knowledge for this problem. To
that end, we prove that, as expected, a quantum query complexity of
$\Theta(N^{1/3})$ holds for all interesting domain and range sizes. Our proofs
are simple, and combine existing techniques with several novel tricks to obtain
the desired results. Using our techniques, we also give an optimal
$\Omega(N^{1/3})$ lower bound for the set equality problem. This new lower
bound can be used to improve the relationship between classical randomized
query complexity and quantum query complexity for so-called
permutation-symmetric functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1031</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1031</id><created>2013-12-04</created><updated>2014-03-23</updated><authors><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author></authors><title>Analysis of Distributed Stochastic Dual Coordinate Ascent</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In \citep{Yangnips13}, the author presented distributed stochastic dual
coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss
minimization. Extraordinary performances have been observed and reported for
the well-motivated updates, as referred to the practical updates, compared to
the naive updates. However, no serious analysis has been provided to understand
the updates and therefore the convergence rates. In the paper, we bridge the
gap by providing a theoretical analysis of the convergence rates of the
practical DisDCA algorithm. Our analysis helped by empirical studies has shown
that it could yield an exponential speed-up in the convergence by increasing
the number of dual updates at each iteration. This result justifies the
superior performances of the practical DisDCA as compared to the naive variant.
As a byproduct, our analysis also reveals the convergence behavior of the
one-communication DisDCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1037</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1037</id><created>2013-12-04</created><updated>2013-12-05</updated><authors><author><keyname>Ram</keyname><forenames>B Hari</forenames></author><author><keyname>Vaishnavi</keyname><forenames>G Kanchana</forenames></author><author><keyname>Giridhar</keyname><forenames>K</forenames></author></authors><title>Blind Fractional Interference Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional Interference Alignment (FIA) is a transmission scheme which
achieves any value between [0,1] for the Symbols transmitted per Antenna per
Channel use (SpAC). FIA was designed in [1] specifically for Finite Alphabet
(FA) signals, under the constraint that the Minimum Distance (MD) detector is
used at all the receivers. Similar to classical interference alignment, the FIA
precoder also needs perfect channel state information at all the transmitters
(CSIT). In this work, a novel Blind Fractional Interference Alignment (B-FIA)
scheme is introduced, where the basic assumption is that CSIT is not available.
We consider two popular channel models, namely: Broadcast channel, and
Interference channel. For these two channel models, the maximum achievable
value of SpAC satisfying the constraints of the MD detector is obtained, but
with no CSIT, and also a precoder design is provided to obtain any value of
SpAC in the achievable range.
  Further, the precoder structure provided has one distinct advantage:
interference channel state information at the receiver (I-CSIR) is not needed,
when all the transmitters and receivers are equipped with one antenna each.
When two or more antennas are used at both ends, I-CSIR must be available to
obtain the maximum achievable value of SpAC. The receiver designs for both the
Minimum Distance and the Maximum Likelihood (ML) decoders are discussed, where
the interference statistics is estimated from the received signal samples.
Simulation results of the B-FIA show that the ML decoder with estimated
statistics achieves a significantly better error rate performance when compared
to the MD decoder with known statistics, since the MD decoder assumes the
interference plus noise term as colored Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1038</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1038</id><created>2013-12-04</created><updated>2015-01-26</updated><authors><author><keyname>Adler</keyname><forenames>Aviv</forenames></author><author><keyname>de Berg</keyname><forenames>Mark</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author><author><keyname>Solovey</keyname><forenames>Kiril</forenames></author></authors><title>Efficient Multi-Robot Motion Planning for Unlabeled Discs in Simple
  Polygons</title><categories>cs.CG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following motion-planning problem: we are given $m$ unit
discs in a simple polygon with $n$ vertices, each at their own start position,
and we want to move the discs to a given set of $m$ target positions. Contrary
to the standard (labeled) version of the problem, each disc is allowed to be
moved to any target position, as long as in the end every target position is
occupied. We show that this unlabeled version of the problem can be solved in
$O(n\log n+mn+m^2)$ time, assuming that the start and target positions are at
least some minimal distance from each other. This is in sharp contrast to the
standard (labeled) and more general multi-robot motion-planning problem for
discs moving in a simple polygon, which is known to be strongly NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1040</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1040</id><created>2013-12-04</created><authors><author><keyname>Kowalczyk</keyname><forenames>Martin</forenames></author><author><keyname>Barthel</keyname><forenames>Henning</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Heidrich</keyname><forenames>Jens</forenames></author><author><keyname>Trendowicz</keyname><forenames>Adam</forenames></author></authors><title>A Deployment Process for Strategic Measurement Systems</title><categories>cs.SE</categories><comments>12 pages. Proceedings of the 8th Software Measurement European Forum
  (SMEF 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explicitly linking software-related activities to an organisation's
higher-level goals has been shown to be critical for organizational success.
GQM+Strategies provides mechanisms for explicitly linking goals and strategies,
based on goal-oriented strategic measurement systems. Deploying such strategic
measurement systems in an organization is highly challenging. Experience has
shown that a clear deployment strategy is needed for achieving sustainable
success. In particular, an adequate deployment process as well as corresponding
tool support can facilitate the deployment. This paper introduces the
systematical GQM+Strategies deployment process and gives an overview of
GQM+Strategies modelling and associated tool support. Additionally, it provides
an overview of industrial applications and describes success factors and
benefits for the usage of GQM+Strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1042</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1042</id><created>2013-12-04</created><authors><author><keyname>Kl&#xe4;s</keyname><forenames>Michael</forenames></author><author><keyname>Lampasona</keyname><forenames>Constanza</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Adapting Software Quality Models: Practical Challenges, Approach, and
  First Empirical Results</title><categories>cs.SE</categories><comments>8 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6068366</comments><journal-ref>Proceedings of the 37th EUROMICRO Conference on Software
  Engineering and Advanced Applications (SEAA 2011), pages 341-348, Oulu,
  Finland, August 30 - September 2 2011</journal-ref><doi>10.1109/SEAA.2011.62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring and evaluating software quality has become a fundamental task. Many
models have been proposed to support stakeholders in dealing with software
quality. However, in most cases, quality models do not fit perfectly for the
target application context. Since approaches for efficiently adapting quality
models are largely missing, many quality models in practice are built from
scratch or reuse only high-level concepts of existing models. We present a
tool-supported approach for the efficient adaptation of quality models. An
initial empirical investigation indicates that the quality models obtained
applying the proposed approach are considerably more consistently and
appropriately adapted than those obtained following an ad-hoc approach.
Further, we could observe that model adaptation is significantly more efficient
(~factor 8) when using this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1043</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1043</id><created>2013-12-04</created><authors><author><keyname>Elberzhager</keyname><forenames>Frank</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Using Early Quality Assurance Metrics to Focus Testing Activities</title><categories>cs.SE</categories><comments>8 pages. The final publication is available at
  http://www.shaker.de/de/content/catalogue/index.asp?
  lang=de&amp;ID=8&amp;ISBN=978-3-8440-0557-8&amp;search=yes</comments><journal-ref>Proceedings of the International Conference on Software Process
  and Product Measurement (MetriKon 2011), pages 29-36, Kaiserslautern,
  Germany, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing of software or software-based systems and services is considered as
one of the most effort-consuming activities in the lifecycle. This applies
especially to those domains where highly iterative development and continuous
integration cannot be applied. Several approaches have been proposed to use
measurement as a means to improve test effectiveness and efficiency. Most of
them rely on using product data, historical data, or in-process data that is
not related to quality assurance ac- tivities. Very few approaches use data
from early quality assurance activities such as inspection data in order to
focus testing activities and thereby reduce test effort. This article gives an
overview of potential benefits of using data from early defect detection
activities, potentially in addition to other data, in order to focus testing
activities. In addition, the article sketches an integrated inspection and
testing process and its evaluation in the context of two case studies. Taking
the study limitations into account, the results show an overall reduction of
testing effort by up to 34%, which mirrors an efficiency improvement of up to
about 50% for testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1053</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1053</id><created>2013-12-04</created><updated>2014-03-30</updated><authors><author><keyname>Doku-Amponsah</keyname><forenames>K.</forenames></author><author><keyname>Mettle</keyname><forenames>F. O.</forenames></author><author><keyname>Narh-Ansah</keyname><forenames>T.</forenames></author></authors><title>Large deviations, Basic information theorem for fitness preferential
  attachment random networks</title><categories>cs.IT cs.SI math.IT math.PR</categories><comments>11 pages</comments><msc-class>60F10, 05C80, 4A15, 94A24</msc-class><journal-ref>International Journal of Statistics and Probability,Vol 3, No.2,
  101-109(2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fitness preferential attachment random networks, we define the empirical
degree and pair measure, which counts the number of vertices of a given degree
and the number of edges with given fits, and the sample path empirical degree
distribution. For the empirical degree and pair distribution for the fitness
preferential attachment random networks, we find a large deviation upper bound.
From this result we obtain a weak law of large numbers for the empirical degree
and pair distribution, and the basic information theorem or an asymptotic
equipartition property for fitness preferential attachment random networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1054</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1054</id><created>2013-12-04</created><updated>2014-05-19</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author></authors><title>Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures
  of Gaussians</title><categories>cs.DS cs.LG math.PR math.ST stat.TH</categories><comments>31 pages, to appear in COLT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an algorithm for properly learning mixtures of two
single-dimensional Gaussians without any separability assumptions. Given
$\tilde{O}(1/\varepsilon^2)$ samples from an unknown mixture, our algorithm
outputs a mixture that is $\varepsilon$-close in total variation distance, in
time $\tilde{O}(1/\varepsilon^5)$. Our sample complexity is optimal up to
logarithmic factors, and significantly improves upon both Kalai et al., whose
algorithm has a prohibitive dependence on $1/\varepsilon$, and Feldman et al.,
whose algorithm requires bounds on the mixture parameters and depends
pseudo-polynomially in these parameters.
  One of our main contributions is an improved and generalized algorithm for
selecting a good candidate distribution from among competing hypotheses.
Namely, given a collection of $N$ hypotheses containing at least one candidate
that is $\varepsilon$-close to an unknown distribution, our algorithm outputs a
candidate which is $O(\varepsilon)$-close to the distribution. The algorithm
requires ${O}(\log{N}/\varepsilon^2)$ samples from the unknown distribution and
${O}(N \log N/\varepsilon^2)$ time, which improves previous such results (such
as the Scheff\'e estimator) from a quadratic dependence of the running time on
$N$ to quasilinear. Given the wide use of such results for the purpose of
hypothesis selection, our improved algorithm implies immediate improvements to
any such use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1060</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1060</id><created>2013-12-04</created><updated>2014-06-26</updated><authors><author><keyname>Ahmadinezhad</keyname><forenames>Hamid</forenames></author><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>An algebraic study of linkages with helical joints</title><categories>cs.RO math.AG</categories><journal-ref>J. Pure Appl. Algebra 219 (2015), 2245-2259</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods from algebra and algebraic geometry have been used in various ways to
study linkages in kinematics. These methods have failed so far for the study of
linkages with helical joints (joints with screw motion), because of the
presence of some non-algebraic relations. In this article, we explore a
delicate reduction of some analytic equations in kinematics to algebraic
questions via a theorem of Ax. As an application, we give a classification of
mobile closed 5-linkages with revolute, prismatic, and helical joints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1070</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1070</id><created>2013-12-04</created><authors><author><keyname>Lakshmi</keyname><forenames>K Vasanta</forenames></author><author><keyname>Acharya</keyname><forenames>Aravind</forenames></author><author><keyname>Komondoor</keyname><forenames>Raghavan</forenames></author></authors><title>Checking Temporal Properties of Presburger Counter Systems using
  Reachability Analysis</title><categories>cs.LO</categories><comments>34 pages, 7 figures, 3 algorithms, 10 theorems, Appendix with proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counter systems are a well-known and powerful modeling notation for
specifying infinite-state systems. In this paper we target the problem of
checking temporal properties of counter systems. We first focus on checking
liveness properties only, and propose two semi decision techniques for these
properties. Both these techniques return a formula that encodes the set of
reachable states of a given system that satisfy a given liveness property. A
novel aspect of our techniques is that they use reachability analysis
techniques, which are well studied in the literature, as black boxes, and are
hence able to compute precise answers on a much wider class of systems than
previous approaches for the same problem. Secondly, they compute their results
by iterative expansion or contraction, and hence permit an approximate solution
to be obtained at any point. We state the formal properties of our techniques,
and also provide experimental results using standard benchmarks to show the
usefulness of our approaches. Finally, we provide a technique for checking
arbitrary CTL temporal properties, which use the liveness property checking
techniques mentioned above as black boxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1075</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1075</id><created>2013-12-04</created><updated>2014-02-03</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Krichene</keyname><forenames>Walid</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre M.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>A Necessary and Sufficient Condition for the Existence of Potential
  Functions for Heterogeneous Routing Games</title><categories>cs.GT cs.SY math.OC</categories><comments>Improved Literature Review; Updated Introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a heterogeneous routing game in which vehicles might belong to more
than one type. The type determines the cost of traveling along an edge as a
function of the flow of various types of vehicles over that edge. We relax the
assumptions needed for the existence of a Nash equilibrium in this
heterogeneous routing game. We extend the available results to present
necessary and sufficient conditions for the existence of a potential function.
We characterize a set of tolls that guarantee the existence of a potential
function when only two types of users are participating in the game. We present
an upper bound for the price of anarchy (i.e., the worst-case ratio of the
social cost calculated for a Nash equilibrium over the social cost for a
socially optimal flow) for the case in which only two types of players are
participating in a game with affine edge cost functions. A heterogeneous
routing game with vehicle platooning incentives is used as an example
throughout the article to clarify the concepts and to validate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1085</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1085</id><created>2013-12-04</created><updated>2014-12-28</updated><authors><author><keyname>Iutzeler</keyname><forenames>Franck</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Ciblat</keyname><forenames>Philippe</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Explicit Convergence Rate of a Distributed Alternating Direction Method
  of Multipliers</title><categories>cs.DC math.OC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of N agents seeking to solve distributively the minimization
problem $\inf_{x} \sum_{n = 1}^N f_n(x)$ where the convex functions $f_n$ are
local to the agents. The popular Alternating Direction Method of Multipliers
has the potential to handle distributed optimization problems of this kind. We
provide a general reformulation of the problem and obtain a class of
distributed algorithms which encompass various network architectures. The rate
of convergence of our method is considered. It is assumed that the infimum of
the problem is reached at a point $x_\star$, the functions $f_n$ are twice
differentiable at this point and $\sum \nabla^2 f_n(x_\star) &gt; 0$ in the
positive definite ordering of symmetric matrices. With these assumptions, it is
shown that the convergence to the consensus $x_\star$ is linear and the exact
rate is provided. Application examples where this rate can be optimized with
respect to the ADMM free parameter $\rho$ are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1094</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1094</id><created>2013-12-04</created><updated>2014-07-08</updated><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author></authors><title>Interaction Graphs: Exponentials</title><categories>cs.LO math.LO</categories><msc-class>03B70, 03F52, 28E15</msc-class><acm-class>F.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is the fourth of a series exposing a systematic combinatorial
approach to Girard's Geometry of Interaction program. This program aims at
obtaining particular realizability models for linear logic that accounts for
the dynamics of cut-elimination. This fourth paper tackles the complex issue of
defining exponential connectives in this framework. In order to succeed in
this, we use the notion of graphings, a generalization of graphs which was
defined in earlier work. We explain how we can use this framework to define a
GoI for Elementary Linear Logic (ELL) with second-order quantification, a
sub-system of linear logic that captures the class of elementary time
computable functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1099</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1099</id><created>2013-12-04</created><authors><author><keyname>Petralia</keyname><forenames>Francesca</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Multiscale Dictionary Learning for Estimating Conditional Distributions</title><categories>stat.ML cs.LG</categories><journal-ref>Proceeding of Neural Information Processing Systems, Lake Tahoe,
  Nevada December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonparametric estimation of the conditional distribution of a response given
high-dimensional features is a challenging problem. It is important to allow
not only the mean but also the variance and shape of the response density to
change flexibly with features, which are massive-dimensional. We propose a
multiscale dictionary learning model, which expresses the conditional response
density as a convex combination of dictionary densities, with the densities
used and their weights dependent on the path through a tree decomposition of
the feature space. A fast graph partitioning algorithm is applied to obtain the
tree decomposition, with Bayesian methods then used to adaptively prune and
average over different sub-trees in a soft probabilistic manner. The algorithm
scales efficiently to approximately one million features. State of the art
predictive performance is demonstrated for toy examples and two neuroscience
applications including up to a million features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1120</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1120</id><created>2013-12-04</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Jagadeesan</keyname><forenames>Radha</forenames></author></authors><title>A Game Semantics for Generic Polymorphism</title><categories>cs.LO</categories><comments>41 pages</comments><journal-ref>Annals of Pure and Applied Logic, vol 133, 3-37, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genericity is the idea that the same program can work at many different data
types. Longo, Milstead and Soloviev proposed to capture the inability of
generic programs to probe the structure of their instances by the following
equational principle: if two generic programs, viewed as terms of type $\forall
X. \, A[X]$, are equal at any given instance $A[T]$, then they are equal at all
instances. They proved that this rule is admissible in a certain extension of
System F, but finding a semantically motivated model satisfying this principle
remained an open problem.
  In the present paper, we construct a categorical model of polymorphism, based
on game semantics, which contains a large collection of generic types. This
model builds on two novel constructions:
  -- A direct interpretation of variable types as games, with a natural notion
of substitution of games. This allows moves in games A[T] to be decomposed into
the generic part from A, and the part pertaining to the instance T. This leads
to a simple and natural notion of generic strategy.
  -- A &quot;relative polymorphic product&quot; which expresses quantification over the
type variable X in the variable type A with respect to a &quot;universe'&quot; which is
explicitly given as an additional parameter B. We then solve a recursive
equation involving this relative product to obtain a universe in a suitably
&quot;absolute&quot; sense.
  Full Completeness for ML types (universal closures of quantifier-free types)
is proved for this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1121</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1121</id><created>2013-12-04</created><authors><author><keyname>Palczewska</keyname><forenames>Anna</forenames></author><author><keyname>Palczewski</keyname><forenames>Jan</forenames></author><author><keyname>Robinson</keyname><forenames>Richard Marchese</forenames></author><author><keyname>Neagu</keyname><forenames>Daniel</forenames></author></authors><title>Interpreting random forest classification models using a feature
  contribution method</title><categories>cs.LG</categories><acm-class>I.5.2; I.2.1; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model interpretation is one of the key aspects of the model evaluation
process. The explanation of the relationship between model variables and
outputs is relatively easy for statistical models, such as linear regressions,
thanks to the availability of model parameters and their statistical
significance. For &quot;black box&quot; models, such as random forest, this information
is hidden inside the model structure. This work presents an approach for
computing feature contributions for random forest classification models. It
allows for the determination of the influence of each variable on the model
prediction for an individual instance. By analysing feature contributions for a
training dataset, the most significant variables can be determined and their
typical contribution towards predictions made for individual classes, i.e.,
class-specific feature contribution &quot;patterns&quot;, are discovered. These patterns
represent a standard behaviour of the model and allow for an additional
assessment of the model reliability for a new data. Interpretation of feature
contributions for two UCI benchmark datasets shows the potential of the
proposed methodology. The robustness of results is demonstrated through an
extensive analysis of feature contributions calculated for a large number of
generated random forest models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1134</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1134</id><created>2013-12-04</created><updated>2014-05-12</updated><authors><author><keyname>Xiang</keyname><forenames>Zhengzheng</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Massive MIMO Multicasting in Noncooperative Cellular Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE JSAC Special Issue on 5G Wireless Communication
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the massive multiple-input multiple-output (MIMO) multicast
transmission in cellular networks where each base station (BS) is equipped with
a large-scale antenna array and transmits a common message using a single
beamformer to multiple mobile users. We first show that when each BS knows the
perfect channel state information (CSI) of its own served users, the
asymptotically optimal beamformer at each BS is a linear combination of the
channel vectors of its multicast users. Moreover, the optimal combining
coefficients are obtained in closed form. Then we consider the imperfect CSI
scenario where the CSI is obtained through uplink channel estimation in
timedivision duplex systems. We propose a new pilot scheme that estimates the
composite channel which is a linear combination of the individual channels of
multicast users in each cell. This scheme is able to completely eliminate pilot
contamination. The pilot power control for optimizing the multicast beamformer
at each BS is also derived. Numerical results show that the asymptotic
performance of the proposed scheme is close to the ideal case with perfect CSI.
Simulation also verifies the effectiveness of the proposed scheme with finite
number of antennas at each BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1142</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1142</id><created>2013-12-04</created><authors><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author><author><keyname>Panzer</keyname><forenames>Heiko K. F.</forenames></author><author><keyname>Lohmann</keyname><forenames>Boris</forenames></author></authors><title>ADI iteration for Lyapunov equations: a tangential approach and adaptive
  shift selection</title><categories>math.NA cs.SY math.DS</categories><comments>15 pages, 2 figures</comments><msc-class>65F10, 93A15, 93C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new version of the alternating directions implicit (ADI) iteration for the
solution of large-scale Lyapunov equations is introduced. It generalizes the
hitherto existing iteration, by incorporating tangential directions in the way
they are already available for rational Krylov subspaces. Additionally, first
strategies to adaptively select shifts and tangential directions in each
iteration are presented. Numerical examples emphasize the potential of the new
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1146</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1146</id><created>2013-12-04</created><authors><author><keyname>Roub&#xed;&#x10d;kov&#xe1;</keyname><forenames>Anna</forenames></author><author><keyname>Serina</keyname><forenames>Ivan</forenames></author></authors><title>Case-Based Merging Techniques in OAKPLAN</title><categories>cs.AI</categories><comments>preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Case-based planning can take advantage of former problem-solving experiences
by storing in a plan library previously generated plans that can be reused to
solve similar planning problems in the future. Although comparative worst-case
complexity analyses of plan generation and reuse techniques reveal that it is
not possible to achieve provable efficiency gain of reuse over generation, we
show that the case-based planning approach can be an effective alternative to
plan generation when similar reuse candidates can be chosen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1147</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1147</id><created>2013-12-04</created><authors><author><keyname>Pad</keyname><forenames>Pedram</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Optimality of Operator-Like Wavelets for Representing Sparse AR(1)
  Processes</title><categories>cs.IT math.IT</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the Karhunen-Lo\`{e}ve transform (KLT) of Gaussian
first-order auto-regressive (AR(1)) processes results in sinusoidal basis
functions. The same sinusoidal bases come out of the independent-component
analysis (ICA) and actually correspond to processes with completely independent
samples. In this paper, we relax the Gaussian hypothesis and study how
orthogonal transforms decouple symmetric-alpha-stable (S$\alpha$S) AR(1)
processes. The Gaussian case is not sparse and corresponds to $\alpha=2$, while
$0&lt;\alpha&lt;2$ yields processes with sparse linear-prediction error. In the
presence of sparsity, we show that operator-like wavelet bases do outperform
the sinusoidal ones. Also, we observe that, for processes with very sparse
increments ($0&lt;\alpha\leq 1$), the operator-like wavelet basis is
indistinguishable from the ICA solution obtained through numerical
optimization. We consider two criteria for independence. The first is the
Kullback-Leibler divergence between the joint probability density function
(pdf) of the original signal and the product of the marginals in the
transformed domain. The second is a divergence between the joint pdf of the
original signal and the product of the marginals in the transformed domain,
which is based on Stein's formula for the mean-square estimation error in
additive Gaussian noise. Our framework then offers a unified view that
encompasses the discrete cosine transform (known to be asymptotically optimal
for $\alpha=2$) and Haar-like wavelets (for which we achieve optimality for
$0&lt;\alpha\leq1$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1148</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1148</id><created>2013-12-04</created><updated>2013-12-05</updated><authors><author><keyname>Kernbach</keyname><forenames>Serge</forenames></author></authors><title>Unconventional research in USSR and Russia: short overview</title><categories>cs.OH physics.hist-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work briefly surveys unconventional research in Russia from the end of
the 19th until the beginning of the 21th centuries in areas related to
generation and detection of a 'high-penetrating' emission of non-biological
origin. The overview is based on open scientific and journalistic materials.
The unique character of this research and its history, originating from
governmental programs of the USSR, is shown. Relations to modern studies on
biological effects of weak electromagnetic emission, several areas of
bioinformatics and theories of physical vacuum are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1160</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1160</id><created>2013-12-04</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>Yao's millionaires' problem and decoy-based public key encryption by
  classical physics</title><categories>cs.CR physics.class-ph physics.comp-ph physics.pop-ph</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use various laws of classical physics to offer several solutions of Yao's
millionaires' problem without using any one-way functions. We also describe
several informationally secure public key encryption protocols, i.e., protocols
secure against passive computationally unbounded adversary. This introduces a
new paradigm of decoy-based cryptography, as opposed to &quot;traditional&quot;
complexity-based cryptography. In particular, our protocols do not employ any
one-way functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1172</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1172</id><created>2013-12-04</created><authors><author><keyname>K&#xf6;bler</keyname><forenames>Johannes</forenames></author><author><keyname>Kuhnert</keyname><forenames>Sebastian</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>Circular-arc hypergraphs: Rigidity via Connectedness</title><categories>cs.DM</categories><comments>21 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A circular-arc hypergraph $H$ is a hypergraph admitting an arc ordering, that
is, a circular ordering of the vertex set $V(H)$ such that every hyperedge is
an arc of consecutive vertices. An arc ordering is tight if, for any two
hyperedges $A$ and $B$ such that $A$ is a nonempty subset of $B$ and $B$ is not
equal to $V(H)$, the corresponding arcs share a common endpoint. We give
sufficient conditions for $H$ to have, up to reversing, a unique arc ordering
and a unique tight arc ordering. These conditions are stated in terms of
connectedness properties of $H$.
  It is known that $G$ is a proper circular-arc graph exactly when its closed
neighborhood hypergraph $N[G]$ admits a tight arc ordering. We explore
connectedness properties of $N[G]$ and prove that, if $G$ is a connected,
twin-free, proper circular-arc graph with non-bipartite complement, then $N[G]$
has, up to reversing, a unique arc ordering. If the complement of $G$ is
bipartite and connected, then $N[G]$ has, up to reversing, two tight arc
orderings. As a corollary, we notice that in both of the two cases $G$ has an
essentially unique intersection representation. The last result also follows
from the work by Deng, Hell, and Huang based on a theory of local tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1178</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1178</id><created>2013-12-04</created><authors><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Routing of Physarum polycephalum signals using simple chemicals</title><categories>cs.ET</categories><comments>19 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work the chemotaxis towards simple organic chemicals was
assessed. We utilise the knowledge gained from these chemotactic assays to
route Physarum polycephalum at a series of junctions. By applying chemical
inputs at a simple T-junction we were able to reproducibly control the path
taken by the plasmodium of P. Polycephalum. Where the chemoattractant farnesene
was used at one input a routed signal could be reproducibly generated i.e. P.
Polycephalum moves towards the source of chemoattractant. Where the
chemoattractant was applied at both inputs the signal was reproducibly split.
If a chemorepellent was used then the signal was reproducibly suppressed. If no
chemical input was used in the simple circuit then a random signal was
generated, whereby P. Polycephalum would move towards one output at the
junction, but the direction was randomly selected. We extended this study to a
more complex series of T-junctions to explore further the potential of routing
P. Polycephalum. Although many of the circuits were completed effectively, any
errors from the implementation of the simple T-junction were magnified. There
were also issues with cascading effects through multiple junctions. For example
signal splitting could be reproducibly initiated at the first junction but not
at subsequent junctions. This work highlights the potential for exploiting
chemotaxis to achieve complex and reliable routing of P. Polycephalum signals.
This may be useful in implementing computing algorithms, design of autonomous
robots and directed material synthesis. In additional experiments we showed
that the application of chemoattractant compounds at specific locations on a
homogeneous substrate could be used to reliably control the spatial
configuration of P. Polycephalum. This may have applications in implementing
geometric calculations and in robot navigation tasks such as mapping chemical
plumes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1181</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1181</id><created>2013-12-04</created><authors><author><keyname>Contreras</keyname><forenames>David</forenames></author><author><keyname>Hitschfeld-Kahler</keyname><forenames>Nancy</forenames></author></authors><title>Study on Delaunay tessellations of 1-irregular cuboids for 3D mixed
  element meshes</title><categories>cs.CG</categories><report-no>TR/DCC-2013-6</report-no><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed elements meshes based on the modified octree approach contain several
co-spherical point configurations. While generating Delaunay tessellations to
be used together with the finite volume method, it is not necessary to
partition them into tetrahedra; co-spherical elements can be used as final
elements. This paper presents a study of all co-spherical elements that appear
while tessellating a 1-irregular cuboid (cuboid with at most one Steiner point
on its edges) with different aspect ratio. Steiner points can be located at any
position between the edge endpoints. When Steiner points are located at edge
midpoints, 24 co-spherical elements appear while tessellating 1-irregular
cubes. By inserting internal faces and edges to these new elements, this number
is reduced to 13. When 1-irregular cuboids with aspect ratio equal to
$\sqrt{2}$ are tessellated, 10 co-spherical elements are required. If
1-irregular cuboids have aspect ratio between 1 and $\sqrt{2}$, all the
tessellations are adequate for the finite volume method. When Steiner points
are located at any position, the study was done for a specific Steiner point
distribution on a cube. 38 co-spherical elements were required to tessellate
all the generated 1-irregular cubes. Statistics about the impact of each new
element in the tessellations of 1-irregular cuboids are also included. This
study was done by developing an algorithm that construct Delaunay tessellations
by starting from a Delaunay tetrahedral mesh built by Qhull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1187</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1187</id><created>2013-12-04</created><updated>2014-02-12</updated><authors><author><keyname>Scott</keyname><forenames>Bruce D.</forenames></author><author><keyname>Weinberg</keyname><forenames>Volker</forenames></author><author><keyname>Hoenen</keyname><forenames>Olivier</forenames></author><author><keyname>Karmakar</keyname><forenames>Anupam</forenames></author><author><keyname>Fazendeiro</keyname><forenames>Luis</forenames></author></authors><title>Scalability of the plasma physics code GEM</title><categories>cs.DC cs.PF physics.comp-ph physics.plasm-ph</categories><comments>9 pages, 6 figures, PRACE Whitepaper</comments><report-no>WP125</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a detailed weak scaling analysis of GEM, a 3D MPI-parallelised
gyrofluid code used in theoretical plasma physics at the Max Planck Institute
of Plasma Physics, IPP at Garching b. M\&quot;unchen, Germany. Within a PRACE
Preparatory Access Project various versions of the code have been analysed on
the HPC systems SuperMUC at LRZ and JUQUEEN at J\&quot;ulich Supercomputing Centre
(JSC) to improve the parallel scalability of the application. The diagnostic
tool Scalasca has been used to filter out suboptimal routines. The code uses
the electromagnetic gyrofluid model which is a superset of magnetohydrodynamic
and drift-Alfv\'en microturbulance and also includes several relevant kinetic
processes. GEM can be used with different geometries depending on the targeted
use case, and has been proven to show good scalability when the computational
domain is distributed amongst two dimensions. Such a distribution allows grids
with sufficient size to describe small scale tokamak devices. In order to
enable simulation of very large tokamaks (such as the next generation nuclear
fusion device ITER in Cadarache, France) the third dimension has been
parallelised and weak scaling has been achieved for significantly larger grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1188</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1188</id><created>2013-12-04</created><authors><author><keyname>Izmaylova</keyname><forenames>Anastasia</forenames></author><author><keyname>Klint</keyname><forenames>Paul</forenames></author><author><keyname>Shahi</keyname><forenames>Ashim</forenames></author><author><keyname>Vinju</keyname><forenames>Jurgen</forenames></author></authors><title>M3: An Open Model for Measuring Code Artifacts</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document details design considerations of M3: a meta model for source
code artifacts
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1225</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1225</id><created>2013-12-04</created><authors><author><keyname>Armstrong</keyname><forenames>Alasdair</forenames></author><author><keyname>Gomes</keyname><forenames>Victor B. F.</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Algebraic Principles for Rely-Guarantee Style Concurrency Verification
  Tools</title><categories>cs.LO cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide simple equational principles for deriving rely-guarantee-style
inference rules and refinement laws based on idempotent semirings. We link the
algebraic layer with concrete models of programs based on languages and
execution traces. We have implemented the approach in Isabelle/HOL as a
lightweight concurrency verification tool that supports reasoning about the
control and data flow of concurrent programs with shared variables at different
levels of abstraction. This is illustrated on two simple verification examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1231</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1231</id><created>2013-12-04</created><updated>2015-08-10</updated><authors><author><keyname>Bauer</keyname><forenames>Ulrich</forenames></author><author><keyname>Edelsbrunner</keyname><forenames>Herbert</forenames></author></authors><title>The Morse theory of \v{C}ech and Delaunay complexes</title><categories>cs.CG math.AT math.GT math.MG</categories><comments>17 pages. Full version, completely rewritten and significantly
  extended</comments><msc-class>52C99, 51F99, 55U10, 57Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite set of points in $\mathbb R^n$ and a radius parameter, we
study the \v{C}ech, Delaunay-\v{C}ech, Delaunay (or alpha), and Wrap complexes
in the light of generalized discrete Morse theory. Establishing the \v{C}ech
and Delaunay complexes as sublevel sets of generalized discrete Morse
functions, we prove that the four complexes are simple-homotopy equivalent by a
sequence of simplicial collapses, which are explicitly described by a single
discrete gradient field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1243</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1243</id><created>2013-12-04</created><updated>2016-02-28</updated><authors><author><keyname>Patel</keyname><forenames>Siddharth</forenames></author><author><keyname>Sevlian</keyname><forenames>Raffi</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Pricing Residential Electricity Based on Individual Consumption
  Behaviors</title><categories>math.OC cs.SI cs.SY</categories><comments>Previous, shorter version published as part of Power and Energy
  General Meeting 2014. Current version prepared for submission to IEEE
  Transactions on Power Systems</comments><doi>10.1109/PESGM.2014.6939793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional practice of retail electric utilities is to aggregate
customers geographically. The utility purchases electricity for its customers
via bulk transactions on the wholesale market, and it passes these costs along
to its customers, the end consumers, through their rate plan. Typically, all
residential consumers are offered the same per unit rate plan, which leads to
cost sharing. Some consumers use their electricity at peak hours, when it is
more expensive on the wholesale market, and others consume mostly at off peak
hours, when it is cheaper, but they all enjoy the same per unit rate through
their utility. This paper proposed a method for the utility to segment a
population of consumers on the basis of their individual consumption patterns.
An optimal recruitment algorithm was developed to aggregate consumers into
groups with a relatively low per unit cost of electricity on the wholesale
market. It was then proposed that the utility should group together enough
consumers to ensure an adequately low forecast error, which is related to risks
it faces in wholesale market transactions. Finally, it was shown that by
repeated application of this process, the utility could segment the entire
population into groups and offer them differentiated rate plans based on their
actual consumption behavior. These groupings are stable in the sense that no
one consumer can unilaterally improve her outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1254</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1254</id><created>2013-12-04</created><updated>2015-03-24</updated><authors><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Hao</keyname><forenames>Ruru</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author><author><keyname>Su</keyname><forenames>Zhixun</forenames></author></authors><title>Parallel matrix factorization for low-rank tensor completion</title><categories>cs.NA math.NA stat.CO</categories><comments>25 pages, 12 figures</comments><journal-ref>Inverse Problems and Imaging. Volume 9, No.2, 601-624, 2015</journal-ref><doi>10.3934/ipi.2015.9.601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order low-rank tensors naturally arise in many applications including
hyperspectral data recovery, video inpainting, seismic data recon- struction,
and so on. We propose a new model to recover a low-rank tensor by
simultaneously performing low-rank matrix factorizations to the all-mode ma-
tricizations of the underlying tensor. An alternating minimization algorithm is
applied to solve the model, along with two adaptive rank-adjusting strategies
when the exact rank is not known.
  Phase transition plots reveal that our algorithm can recover a variety of
synthetic low-rank tensors from significantly fewer samples than the compared
methods, which include a matrix completion method applied to tensor recovery
and two state-of-the-art tensor completion methods. Further tests on real-
world data show similar advantages. Although our model is non-convex, our
algorithm performs consistently throughout the tests and give better results
than the compared methods, some of which are based on convex models. In
addition, the global convergence of our algorithm can be established in the
sense that the gradient of Lagrangian function converges to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1258</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1258</id><created>2013-12-04</created><authors><author><keyname>Payette</keyname><forenames>Sandra</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author></authors><title>Flexible and Extensible Digital Object and Repository Architecture
  (FEDORA)</title><categories>cs.DL</categories><comments>European Conference on Research and Advanced Technology for Digital
  Libraries, Heraklion, Crete, published in Lecture Notes in Computer Science,
  Springer, 1998, http://www.springerlink.com/content/lcj85y9pg6ng32k6/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a digital object and respository architecture for storing and
disseminating digital library content. The key features of the architecture
are: (1) support for heterogeneous data types; (2) accommodation of new types
as they emerge; (3) aggregation of mixed, possibly distributed, data into
complex objects; (4) the ability to specify multiple content disseminations of
these objects; and (5) the ability to associate rights management schemes with
these disseminations. This architecture is being implemented in the context of
a broader research project to develop next-generation service modules for a
layered digital library architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1260</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1260</id><created>2013-12-04</created><authors><author><keyname>Payette</keyname><forenames>Sandra</forenames></author><author><keyname>Lagoze</keyname><forenames>Carl</forenames></author></authors><title>Policy-Carrying, Policy-Enforcing Digital Objects</title><categories>cs.DL</categories><comments>European Conference on Research and Advanced Technology for Digital
  Libraries, Lisbon, Portugal, published in Lecture Notes in Computer Science,
  Springer, 2000, http://www.springerlink.com/content/lw0bjhnyvluj0433/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the motivation for moving policy enforcement for access control
down to the digital object level. The reasons for this include handling of
item-specific behaviors, adapting to evolution of digital objects, and
permitting objects to move among repositories and portable devices. We then
describe our experiments that integrate the Fedora architecture for digital
objects and repositories and the PoET implementation of security automata to
effect such objectcentric policy enforcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1273</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1273</id><created>2013-12-03</created><authors><author><keyname>Mitavskiy</keyname><forenames>Boris</forenames></author><author><keyname>He</keyname><forenames>Jun</forenames></author></authors><title>Design and Analysis of an Estimation of Distribution Approximation
  Algorithm for Single Machine Scheduling in Uncertain Environments</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current work we introduce a novel estimation of distribution algorithm
to tackle a hard combinatorial optimization problem, namely the single-machine
scheduling problem, with uncertain delivery times. The majority of the existing
research coping with optimization problems in uncertain environment aims at
finding a single sufficiently robust solution so that random noise and
unpredictable circumstances would have the least possible detrimental effect on
the quality of the solution. The measures of robustness are usually based on
various kinds of empirically designed averaging techniques. In contrast to the
previous work, our algorithm aims at finding a collection of robust schedules
that allow for a more informative decision making. The notion of robustness is
measured quantitatively in terms of the classical mathematical notion of a norm
on a vector space. We provide a theoretical insight into the relationship
between the properties of the probability distribution over the uncertain
delivery times and the robustness quality of the schedules produced by the
algorithm after a polynomial runtime in terms of approximation ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1277</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1277</id><created>2013-12-04</created><updated>2015-11-19</updated><authors><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Bandits and Experts in Metric Spaces</title><categories>cs.DS cs.LG</categories><comments>This manuscript is a merged and definitive version of (R. Kleinberg,
  Slivkins, Upfal: STOC 2008) and (R. Kleinberg, Slivkins: SODA 2010), with a
  significantly revised presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-armed bandit problem, an online algorithm chooses from a set of
strategies in a sequence of trials so as to maximize the total payoff of the
chosen strategies. While the performance of bandit algorithms with a small
finite strategy set is quite well understood, bandit problems with large
strategy sets are still a topic of very active investigation, motivated by
practical applications such as online auctions and web advertisement. The goal
of such research is to identify broad and natural classes of strategy sets and
payoff functions which enable the design of efficient solutions.
  In this work we study a very general setting for the multi-armed bandit
problem in which the strategies form a metric space, and the payoff function
satisfies a Lipschitz condition with respect to the metric. We refer to this
problem as the &quot;Lipschitz MAB problem&quot;. We present a solution for the
multi-armed bandit problem in this setting. That is, for every metric space we
define an isometry invariant which bounds from below the performance of
Lipschitz MAB algorithms for this metric space, and we present an algorithm
which comes arbitrarily close to meeting this bound. Furthermore, our technique
gives even better results for benign payoff functions. We also address the
full-feedback (&quot;best expert&quot;) version of the problem, where after every round
the payoffs from all arms are revealed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1286</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1286</id><created>2013-11-04</created><authors><author><keyname>Istiadi</keyname></author><author><keyname>Azhari</keyname></author></authors><title>An Ontology Model for Organizing Information Resources Sharing on
  Personal Web</title><categories>cs.DL cs.IR</categories><journal-ref>The Fifth International Symposium on Computational Science (ISCS
  2012) Yogyakarta, Indonesia, May 15-16, 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Retrieve information resources made by the machine processing may refer to
multiple sources. A personal web as part of information resources in the
Internet requires a feature that can be understood by computer machines.
Therefore, in this paper an ontology semantic web approach is used to map the
resources in a meaningful scheme. In the design of concept, resources on the
web are viewed as documents that have some property and ownership. Domain
interest or web scope is used to describe a classification of resources that
navigate into relevant documents. If instances are completed to the concept,
then the ontology file can be loaded and shared as annotation on personal web.
This allows computer machine to query multiple ontology from different personal
webs that use it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1299</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1299</id><created>2013-12-02</created><updated>2013-12-09</updated><authors><author><keyname>Meunier</keyname><forenames>Pierre-&#xc9;tienne</forenames></author></authors><title>The self-assembly of paths and squares at temperature 1</title><categories>cs.CC cs.CG cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1306.6710 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the number of tile types required to build squares of size n x
n, in Winfree's abstract Tile Assembly Model, when restricted to using only
non-cooperative tile bindings, is at least 2n-1, which is also the best known
upper bound. Non-cooperative self-assembly, also known as temperature 1, is
where tiles bind to each other if they match on one or more sides, whereas in
cooperative binding, some tiles can bind only if they match on multiple sides.
  Our proof introduces a new programming technique for temperature 1, that
disproves the very intuitive and commonly held belief that, in the same model,
assembling paths between two points A and B cannot be done with less tile types
than the Manhattan distance between them. Then, we prove a necessary condition
for these &quot;efficient paths&quot; to be assembled, and show that this necessary
condition cannot hold in completely filled squares.
  This result proves the oldest conjecture in algorithmic self-assembly,
published by Rothemund and Winfree in STOC 2000, in the case where growth
starts from a corner of the square. As a corollary, we establish n as a lower
bound on the tile complexity of the general case. The problem of determining
the minimal number of tile types to self-assemble a shape is known to be
Sigma^p_2-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1309</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1309</id><created>2013-12-04</created><authors><author><keyname>Mohanty</keyname><forenames>Kaniska</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>On the DoF Region of the K-user MISO Broadcast Channel with Hybrid CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An outer bound for the degrees of freedom (DoF) region of the K-user
multiple-input single-output (MISO) broadcast channel (BC) is developed under
the hybrid channel state information at transmitter (CSIT) model, in which the
transmitter has instantaneous CSIT of channels to a subset of the receivers and
delayed CSIT of channels to the rest of the receivers. For the 3-user MISO BC,
when the transmitter has instantaneous CSIT of the channel to one receiver and
delayed CSIT of channels to the other two, two new communication schemes are
designed, which are able to achieve the DoF tuple of
$\left(1,\frac{1}{3},\frac{1}{3}\right)$, with a sum DoF of $\frac{5}{3}$, that
is greater than the sum DoF achievable only with delayed CSIT. Another
communication scheme showing the benefit of the alternating CSIT model is also
developed, to obtain the DoF tuple of $\left(1,\frac{4}{9},\frac{4}{9}\right)$
for the 3-user MISO BC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1311</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1311</id><created>2013-12-04</created><authors><author><keyname>Kaszian</keyname><forenames>Jonas</forenames></author><author><keyname>Moree</keyname><forenames>Pieter</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Periodic Structure of the Exponential Pseudorandom Number Generator</title><categories>math.NT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the periodic structure of the exponential pseudorandom number
generator obtained from the map $x\mapsto g^x\pmod p$ that acts on the set
$\{1, \ldots, p-1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1325</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1325</id><created>2013-12-04</created><updated>2013-12-09</updated><authors><author><keyname>Zieve</keyname><forenames>Michael E.</forenames></author></authors><title>Permutation polynomials induced from permutations of subfields, and some
  complete sets of mutually orthogonal latin squares</title><categories>math.NT cs.IT math.CO math.IT</categories><comments>13 pages; many new results</comments><msc-class>11T06 (Primary) 05A05, 11T55 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general technique for obtaining permutation polynomials over a
finite field from permutations of a subfield. By applying this technique to the
simplest classes of permutation polynomials on the subfield, we obtain several
new families of permutation polynomials. Some of these have the additional
property that both f(x) and f(x)+x induce permutations of the field, which has
combinatorial consequences. We use some of our permutation polynomials to
exhibit complete sets of mutually orthogonal latin squares. In addition, we
solve the open problem from a recent paper by Wu and Lin, and we give simpler
proofs of much more general versions of the results in two other recent papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1349</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1349</id><created>2013-12-04</created><updated>2014-09-06</updated><authors><author><keyname>En&#xdf;lin</keyname><forenames>Torsten A.</forenames></author><author><keyname>Junklewitz</keyname><forenames>Henrik</forenames></author><author><keyname>Winderling</keyname><forenames>Lars</forenames></author><author><keyname>Greiner</keyname><forenames>Maksim</forenames></author><author><keyname>Selig</keyname><forenames>Marco</forenames></author></authors><title>Improving self-calibration</title><categories>astro-ph.IM cs.IT math.IT physics.data-an stat.ML</categories><comments>17 pages, 3 figures, revised version, title changed</comments><doi>10.1103/PhysRevE.90.043301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Response calibration is the process of inferring how much the measured data
depend on the signal one is interested in. It is essential for any quantitative
signal estimation on the basis of the data. Here, we investigate
self-calibration methods for linear signal measurements and linear dependence
of the response on the calibration parameters. The common practice is to
augment an external calibration solution using a known reference signal with an
internal calibration on the unknown measurement signal itself. Contemporary
self-calibration schemes try to find a self-consistent solution for signal and
calibration by exploiting redundancies in the measurements. This can be
understood in terms of maximizing the joint probability of signal and
calibration. However, the full uncertainty structure of this joint probability
around its maximum is thereby not taken into account by these schemes.
Therefore better schemes -- in sense of minimal square error -- can be designed
by accounting for asymmetries in the uncertainty of signal and calibration. We
argue that at least a systematic correction of the common self-calibration
scheme should be applied in many measurement situations in order to properly
treat uncertainties of the signal on which one calibrates. Otherwise the
calibration solutions suffer from a systematic bias, which consequently
distorts the signal reconstruction. Furthermore, we argue that non-parametric,
signal-to-noise filtered calibration should provide more accurate
reconstructions than the common bin averages and provide a new, improved
self-calibration scheme. We illustrate our findings with a simplistic numerical
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1369</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1369</id><created>2013-12-04</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Quasi-Polynomial Time Approximation Scheme for Sparse Subsets of
  Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how to approximate, in quasi-polynomial time, the largest
independent set of polygons, in a given set of polygons. Our algorithm works by
extending the result of Adamaszek and Wiese \cite{aw-asmwi-13, aw-qmwis-14} to
polygons of arbitrary complexity. Surprisingly, the algorithm also works or
computing the largest subset of the given set of polygons that has some
sparsity condition. For example, we show that one can approximate the largest
subset of polygons, such that the intersection graph of the subset does not
contain a cycle of length $4$ (i.e., $K_{2,2}$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1375</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1375</id><created>2013-12-04</created><authors><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>Impact of receiver reaction mechanisms on the performance of molecular
  communication networks</title><categories>q-bio.MN cs.IT math.IT</categories><doi>10.1109/TNANO.2015.2393866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a molecular communication network, transmitters and receivers communicate
by using signalling molecules. At the receivers, the signalling molecules
react, via a chain of chemical reactions, to produce output molecules. The
counts of output molecules over time is considered to be the output signal of
the receiver. This output signal is used to detect the presence of signalling
molecules at the receiver. The output signal is noisy due to the stochastic
nature of diffusion and chemical reactions. The aim of this paper is to
characterise the properties of the output signals for two types of receivers,
which are based on two different types of reaction mechanisms. We derive
analytical expressions for the mean, variance and frequency properties of these
two types of receivers. These expressions allow us to study the properties of
these two types of receivers. In addition, our model allows us to study the
effect of the diffusibility of the receiver membrane on the performance of the
receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1378</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1378</id><created>2013-12-04</created><updated>2013-12-06</updated><authors><author><keyname>Coras</keyname><forenames>Florin</forenames></author><author><keyname>Domingo-Pascual</keyname><forenames>Jordi</forenames></author><author><keyname>Lewis</keyname><forenames>Darrel</forenames></author><author><keyname>Cabellos-Aparicio</keyname><forenames>Albert</forenames></author></authors><title>An Analytical Model for Loc/ID Mappings Caches</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concerns regarding the scalability of the inter-domain routing have
encouraged researchers to start elaborating a more robust Internet
architecture. While consensus on the exact form of the solution is yet to be
found, the need for a semantic decoupling of a node's location and identity is
generally accepted as a promising way forward. However, this typically requires
the use of caches that store temporal bindings between the two namespaces, to
avoid hampering router packet forwarding speeds. In this article, we propose a
methodology for an analytical analysis of cache performance that relies on the
working-set theory. We first identify the conditions that network traffic must
comply with for the theory to be applicable and then develop a model that
predicts average cache miss rates relying on easily measurable traffic
parameters. We validate the result by emulation, using real packet traces
collected at the egress points of a campus and an academic network. To prove
its versatility, we extend the model to consider cache polluting user traffic
and observe that simple, low intensity attacks drastically reduce performance,
whereby manufacturers should either overprovision router memory or implement
more complex cache eviction policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1382</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1382</id><created>2013-12-04</created><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>Solomon</keyname><forenames>Shay</forenames></author></authors><title>Orienting Fully Dynamic Graphs with Worst-Case Time Bounds</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In edge orientations, the goal is usually to orient (direct) the edges of an
undirected $n$-vertex graph $G$ such that all out-degrees are bounded. When the
graph $G$ is fully dynamic, i.e., admits edge insertions and deletions, we wish
to maintain such an orientation while keeping a tab on the update time. Low
out-degree orientations turned out to be a surprisingly useful tool, with
several algorithmic applications involving static or dynamic graphs.
  Brodal and Fagerberg (1999) initiated the study of the edge orientation
problem in terms of the graph's arboricity, which is very natural in this
context. They provided a solution with constant out-degree and \emph{amortized}
logarithmic update time for all graphs with constant arboricity, which include
all planar and excluded-minor graphs. However, it remained an open question
(first proposed by Brodal and Fagerberg, later by others) to obtain similar
bounds with worst-case update time.
  We resolve this 15 year old question in the affirmative, by providing a
simple algorithm with worst-case bounds that nearly match the previous
amortized bounds. Our algorithm is based on a new approach of a combinatorial
invariant, and achieves a logarithmic out-degree with logarithmic worst-case
update times. This result has applications in various dynamic graph problems
such as maintaining a maximal matching, where we obtain $O(\log n)$ worst-case
update time compared to the $O(\frac{\log n}{\log\log n})$ amortized update
time of Neiman and Solomon (2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1385</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1385</id><created>2013-12-04</created><authors><author><keyname>Payette</keyname><forenames>Sandra</forenames></author><author><keyname>Staples</keyname><forenames>Thornton</forenames></author></authors><title>The Mellon Fedora Project: Digital Library Architecture Meets XML and
  Web Services</title><categories>cs.DL</categories><journal-ref>Sixth European Conference on Research and Advanced Technology for
  Digital Libraries, Lecture Notes in Computer Science, Springer, September
  2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The University of Virginia received a grant of $1,000,000 from the Andrew W.
Mellon Foundation to enable the Library, in collaboration with Cornell
University, to build a digital object repository system based on the Flexible
Extensible Digital Object and Repository Architecture (Fedora). The new system
demonstrates how distributed digital library architecture can be deployed using
web-based technologies, including XML and Web services. The new system is
designed to be a foundation upon which interoperable web-based digital
libraries can be built. Virginia and collaborating partners in the US and UK
will evaluate the system using a diverse set of digital collections. The
software will be made available to the public as an open-source release.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1394</identifier>
 <datestamp>2014-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1394</id><created>2013-12-04</created><updated>2014-03-31</updated><authors><author><keyname>Ratliff</keyname><forenames>Lillian J.</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Incentive Design and Utility Learning via Energy Disaggregation</title><categories>math.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The utility company has many motivations for modifying energy consumption
patterns of consumers such as revenue decoupling and demand response programs.
We model the utility company--consumer interaction as a principal--agent
problem. We present an iterative algorithm for designing incentives while
estimating the consumer's utility function. Incentives are designed using the
aggregated as well as the disaggregated (device level) consumption data. We
simulate the iterative control (incentive design) and estimation (utility
learning and disaggregation) process for examples including the design of
incentives based on the aggregate consumption data as well as the disaggregated
consumption data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1397</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1397</id><created>2013-12-04</created><authors><author><keyname>Lee</keyname><forenames>Phillip</forenames></author><author><keyname>Clark</keyname><forenames>Andrew</forenames></author><author><keyname>Bushnell</keyname><forenames>Linda</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author></authors><title>A Passivity Framework for Modeling and Mitigating Wormhole Attacks on
  Networked Control Systems</title><categories>cs.SY cs.CR cs.NI</categories><comments>35 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked control systems consist of distributed sensors and actuators that
communicate via a wireless network. The use of an open wireless medium and
unattended deployment leaves these systems vulnerable to intelligent
adversaries whose goal is to disrupt the system performance. In this paper, we
study the wormhole attack on a networked control system, in which an adversary
establishes a link between two distant regions of the network by using either
high-gain antennas, as in the out-of-band wormhole, or colluding network nodes
as in the in-band wormhole. Wormholes allow the adversary to violate the timing
constraints of real-time control systems by delaying or dropping packets, and
cannot be detected using cryptographic mechanisms alone. We study the impact of
the wormhole attack on the network flows and delays and introduce a
passivity-based control-theoretic framework for modeling the wormhole attack.
We develop this framework for both the in-band and out-of-band wormhole attacks
as well as complex, hereto-unreported wormhole attacks consisting of arbitrary
combinations of in-and out-of band wormholes. We integrate existing mitigation
strategies into our framework, and analyze the throughput, delay, and stability
properties of the overall system. Through simulation study, we show that, by
selectively dropping control packets, the wormhole attack can cause
disturbances in the physical plant of a networked control system, and
demonstrate that appropriate selection of detection parameters mitigates the
disturbances due to the wormhole while satisfying the delay constraints of the
physical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1399</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1399</id><created>2013-12-04</created><updated>2013-12-16</updated><authors><author><keyname>Plotkin</keyname><forenames>Gordon D</forenames><affiliation>Laboratory for Foundations of Computer Science, School of Informatics, Universit</affiliation></author><author><keyname>Pretnar</keyname><forenames>Matija</forenames><affiliation>Faculty for mathematics and physics, University of Ljubljana</affiliation></author></authors><title>Handling Algebraic Effects</title><categories>cs.LO cs.PL</categories><comments>36 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  17, 2013) lmcs:705</journal-ref><doi>10.2168/LMCS-9(4:23)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic effects are computational effects that can be represented by an
equational theory whose operations produce the effects at hand. The free model
of this theory induces the expected computational monad for the corresponding
effect. Algebraic effects include exceptions, state, nondeterminism,
interactive input/output, and time, and their combinations. Exception handling,
however, has so far received no algebraic treatment. We present such a
treatment, in which each handler yields a model of the theory for exceptions,
and each handling construct yields the homomorphism induced by the universal
property of the free model. We further generalise exception handlers to
arbitrary algebraic effects. The resulting programming construct includes many
previously unrelated examples from both theory and practice, including
relabelling and restriction in Milner's CCS, timeout, rollback, and stream
redirection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1411</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1411</id><created>2013-12-04</created><updated>2014-06-09</updated><authors><author><keyname>Alglave</keyname><forenames>Jade</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Nimal</keyname><forenames>Vincent</forenames></author><author><keyname>Poetzl</keyname><forenames>Daniel</forenames></author></authors><title>Don't sit on the fence: A static analysis approach to automatic fence
  insertion</title><categories>cs.LO cs.SE</categories><comments>19 pages, 19 figures</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern architectures rely on memory fences to prevent undesired weakenings of
memory consistency. As the fences' semantics may be subtle, the automation of
their placement is highly desirable. But precise methods for restoring
consistency do not scale to deployed systems code. We choose to trade some
precision for genuine scalability: our technique is suitable for large code
bases. We implement it in our new musketeer tool, and detail experiments on
more than 350 executables of packages found in Debian Linux 7.1, e.g. memcached
(about 10000 LoC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1412</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1412</id><created>2013-12-04</created><updated>2014-03-25</updated><authors><author><keyname>d'Eon</keyname><forenames>Eugene</forenames></author></authors><title>Rigorous asymptotic and moment-preserving diffusion approximations for
  generalized linear Boltzmann transport in arbitrary dimension</title><categories>cs.GR math-ph math.MP nucl-th</categories><comments>Accepted to JCTT on May 26, 2014. Final revision. Title change and
  some minor additional explanation in places</comments><doi>10.1080/00411450.2014.910231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive new diffusion solutions to the monoenergetic generalized linear
Boltzmann transport equation (GLBE) for the stationary collision density and
scalar flux about an isotropic point source in an infinite $d$-dimensional
absorbing medium with isotropic scattering. We consider both classical
transport theory with exponentially-distributed free paths in arbitrary
dimensions as well as a number of non-classical transport theories
(non-exponential random flights) that describe a broader class of transport
processes within partially-correlated random media. New rigorous asymptotic
diffusion approximations are derived where possible. We also generalize
Grosjean's moment-preserving approach of separating the first (or uncollided)
distribution from the collided portion and approximating only the latter using
diffusion. We find that for any spatial dimension and for many free-path
distributions Grosjean's approach produces compact, analytic approximations
that are, overall, more accurate for high absorption and for small
source-detector separations than either $P_1$ diffusion or rigorous asymptotic
diffusion. These diffusion-based approximations are exact in the first two even
spatial moments, which we derive explicitly for various non-classical transport
types. We also discuss connections between the random-flight-theory derivation
of the Green's function and the discrete spectrum of the transport operator and
report some new observations regarding the discrete eigenvalues of the
transport operator for general dimensions and free-path distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1413</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1413</id><created>2013-12-04</created><authors><author><keyname>Iwen</keyname><forenames>Mark</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author></authors><title>Fast Subspace Approximation via Greedy Least-Squares</title><categories>cs.CG math.NA</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this note, we develop fast and deterministic dimensionality reduction
techniques for a family of subspace approximation problems. Let $P\subset
\mathbbm{R}^N$ be a given set of $M$ points. The techniques developed herein
find an $O(n \log M)$-dimensional subspace that is guaranteed to always contain
a near-best fit $n$-dimensional hyperplane $\mathcal{H}$ for $P$ with respect
to the cumulative projection error $(\sum_{{\bf x} \in P} \| {\bf x} -
\Pi_\mathcal{H} {\bf x} \|^p_2)^{1/p}$, for any chosen $p &gt; 2$. The
deterministic algorithm runs in $\tilde{O} (MN^2)$-time, and can be randomized
to run in only $\tilde{O} (MNn)$-time while maintaining its error guarantees
with high probability. In the case $p = \infty$ the dimensionality reduction
techniques can be combined with efficient algorithms for computing the John
ellipsoid of a data set in order to produce an $n$-dimensional subspace whose
maximum $\ell_2$-distance to any point in the convex hull of $P$ is minimized.
The resulting algorithm remains $\tilde{O} (MNn)$-time. In addition, the
dimensionality reduction techniques developed herein can also be combined with
other existing subspace approximation algorithms for $2 &lt; p \leq \infty$ -
including more accurate algorithms based on convex programming relaxations - in
order to reduce their runtimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1421</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1421</id><created>2013-12-04</created><authors><author><keyname>Khoshnevisan</keyname><forenames>Mostafa</forenames></author><author><keyname>Laneman</keyname><forenames>J Nicholas</forenames></author></authors><title>Intermittent Communication</title><categories>cs.IT math.IT</categories><comments>To be submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a model for intermittent communications that can capture bursty
transmissions or a sporadically available channel, where in either case the
receiver does not know a priori when the transmissions will occur. Focusing on
the point-to point case, we develop two decoding structures and their
achievable rates for such communication scenarios. One structure determines the
transmitted codeword, and another scheme first detects the locations of
codeword symbols and then uses them to decode. We introduce the concept of
partial divergence and study some of its properties in order to obtain stronger
achievability results. As the system becomes more intermittent, the achievable
rates decrease due to the additional uncertainty about the positions of the
codeword symbols at the decoder. Additionally, we provide upper bounds on the
capacity of binary noiseless intermittent communication with the help of a
genie-aided encoder and decoder. The upper bounds imply a tradeoff between the
capacity and the intermittency rate of the communication system. Finally, we
obtain lower and upper bounds on the capacity per unit cost of intermittent
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1423</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1423</id><created>2013-12-04</created><authors><author><keyname>Fuad</keyname><forenames>Muhammad Marwan Muhammad</forenames></author></authors><title>ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of
  Sequential Data Using Sigma Grams</title><categories>cs.NE cs.AI</categories><comments>The Tenth Australasian Data Mining Conference - AusDM 2012, Sydney,
  Australia, 5-7 December, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of similarity search is one of the main problems in computer
science. This problem has many applications in text-retrieval, web search,
computational biology, bioinformatics and others. Similarity between two data
objects can be depicted using a similarity measure or a distance metric. There
are numerous distance metrics in the literature, some are used for a particular
data type, and others are more general. In this paper we present a new distance
metric for sequential data which is based on the sum of n-grams. The novelty of
our distance is that these n-grams are weighted using artificial bee colony; a
recent optimization algorithm based on the collective intelligence of a swarm
of bees on their search for nectar. This algorithm has been used in optimizing
a large number of numerical problems. We validate the new distance
experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1431</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1431</id><created>2013-12-04</created><authors><author><keyname>Lubin</keyname><forenames>Miles</forenames></author><author><keyname>Dunning</keyname><forenames>Iain</forenames></author></authors><title>Computing in Operations Research using Julia</title><categories>math.OC cs.NA cs.PL</categories><comments>Source code included in supplement</comments><doi>10.1287/ijoc.2014.0623</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state of numerical computing is currently characterized by a divide
between highly efficient yet typically cumbersome low-level languages such as
C, C++, and Fortran and highly expressive yet typically slow high-level
languages such as Python and MATLAB. This paper explores how Julia, a modern
programming language for numerical computing which claims to bridge this divide
by incorporating recent advances in language and compiler design (such as
just-in-time compilation), can be used for implementing software and algorithms
fundamental to the field of operations research, with a focus on mathematical
optimization. In particular, we demonstrate algebraic modeling for linear and
nonlinear optimization and a partial implementation of a practical simplex
code. Extensive cross-language benchmarks suggest that Julia is capable of
obtaining state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1437</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1437</id><created>2013-12-05</created><authors><author><keyname>Pudasaini</keyname><forenames>Subodh</forenames></author><author><keyname>Shin</keyname><forenames>Seokjoo</forenames></author></authors><title>Initial Ranging for Prioritized Network Entry in IEEE 802.16 Network</title><categories>cs.NI</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prioritized network entry is desirable for establishing preferential network
connectivity for the higher priority users when different priority users exist
over a given network. In line with such desirability, we propose a simple but
efficient priority differentiated initial ranging mechanism considering an
Orthogonal Frequency Division Multiple Access (OFDMA) based IEEE 802.16
network. In the proposed mechanism, we introduce an approach that integrates an
explicit CDMA-ranging code reservation scheme with a Ranging Slot Selection
Window (RSSW) differentiation scheme. Simulation results are provided to
characterize the performance of the proposed mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1444</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1444</id><created>2013-12-05</created><updated>2014-08-22</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Energy Beamforming with One-Bit Feedback</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Signal Processing</comments><doi>10.1109/TSP.2014.2352604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless energy transfer (WET) has attracted significant attention recently
for providing energy supplies wirelessly to electrical devices without the need
of wires or cables. Among different types of WET techniques, the radio
frequency (RF) signal enabled far-field WET is most practically appealing to
power energy constrained wireless networks in a broadcast manner. To overcome
the significant path loss over wireless channels, multi-antenna or
multiple-input multiple-output (MIMO) techniques have been proposed to enhance
the transmission efficiency and distance for RF-based WET. However, in order to
reap the large energy beamforming gain in MIMO WET, acquiring the channel state
information (CSI) at the energy transmitter (ET) is an essential task. This
task is particularly challenging for WET systems, since existing channel
training and feedback methods used for communication receivers may not be
implementable at the energy receiver (ER) due to its hardware limitation. To
tackle this problem, in this paper we consider a multiuser MIMO system for WET,
where a multiple-antenna ET broadcasts wireless energy to a group of
multiple-antenna ERs concurrently via transmit energy beamforming. By taking
into account the practical energy harvesting circuits at the ER, we propose a
new channel learning method that requires only one feedback bit from each ER to
the ET per feedback interval. The feedback bit indicates the increase or
decrease of the harvested energy by each ER between the present and previous
intervals, which can be measured without changing the existing hardware at the
ER. Based on such feedback information, the ET adjusts transmit beamforming in
different training intervals and at the same time obtains improved estimates of
the MIMO channels to ERs by applying a new approach termed analytic center
cutting plane method (ACCPM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1447</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1447</id><created>2013-12-05</created><authors><author><keyname>Yang</keyname><forenames>Qing</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Asynchronous Convolutional-Coded Physical-Layer Network Coding</title><categories>cs.IT math.IT</categories><comments>28 pages, journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the decoding process of asynchronous
convolutional-coded physical-layer network coding (PNC) systems. Specifically,
we put forth a layered decoding framework for convolutional-coded PNC
consisting of three layers: symbol realignment layer, codeword realignment
layer, and joint channel-decoding network coding (Jt-CNC) decoding layer. Our
framework can deal with phase asynchrony and symbol arrival-time asynchrony
between the signals simultaneously transmitted by multiple sources. A salient
feature of this framework is that it can handle both fractional and integral
symbol offsets; previously proposed PNC decoding algorithms (e.g., XOR-CD and
reduced-state Viterbi algorithms) can only deal with fractional symbol offset.
Moreover, the Jt-CNC algorithm, based on belief propagation (BP), is
BER-optimal for synchronous PNC and near optimal for asynchronous PNC.
Extending beyond convolutional codes, we further generalize the Jt-CNC decoding
algorithm for all cyclic codes. Our simulation shows that Jt-CNC outperforms
the previously proposed XOR-CD algorithm and reduced-state Viterbi algorithm by
2dB for synchronous PNC. For phase-asynchronous PNC, Jt-CNC is 4dB better than
the other two algorithms. Importantly, for real wireless environment testing,
we have also implemented our decoding algorithm in a PNC system built on the
USRP software radio platform. Our experiment shows that the proposed Jt-CNC
decoder works well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1448</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1448</id><created>2013-12-05</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Food Recommendation using Ontology and Heuristics</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are needed to find food items of ones interest. We review
recommender systems and recommendation methods. We propose a food
personalization framework based on adaptive hypermedia. We extend Hermes
framework with food recommendation functionality. We combine TF-IDF term
extraction method with cosine similarity measure. Healthy heuristics and
standard food database are incorporated into the knowledgebase. Based on the
performed evaluation, we conclude that semantic recommender systems in general
outperform traditional recommenders systems with respect to accuracy,
precision, and recall, and that the proposed recommender has a better F-measure
than existing semantic recommenders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1450</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1450</id><created>2013-12-05</created><updated>2014-04-15</updated><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chua</keyname><forenames>Kee-Chaing</forenames></author></authors><title>Multi-Antenna Wireless Powered Communication with Energy Beamforming</title><categories>cs.IT math.IT</categories><comments>submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The newly emerging wireless powered communication networks (WPCNs) have
recently drawn significant attention, where radio signals are used to power
wireless terminals for information transmission. In this paper, we study a WPCN
where one multi-antenna access point (AP) coordinates energy transfer and
information transfer to/from a set of single-antenna users. A
harvest-then-transmit protocol is assumed where the AP first broadcasts
wireless power to all users via energy beamforming in the downlink (DL), and
then the users send their independent information to the AP simultaneously in
the uplink (UL) using their harvested energy. To optimize the users' throughput
and yet guarantee their rate fairness, we maximize the minimum throughput among
all users by a joint design of the DL-UL time allocation, the DL energy
beamforming, and the UL transmit power allocation plus receive beamforming. We
solve this non-convex problem optimally by two steps. First, we fix the DL-UL
time allocation and obtain the optimal DL energy beamforming, UL power
allocation and receive beamforming to maximize the minimum
signal-to-interference-plus-noise ratio (SINR) of all users. This problem is
shown to be in general non-convex; however, we convert it equivalently to a
spectral radius minimization problem, which can be solved efficiently by
applying the alternating optimization based on the non-negative matrix theory.
Then, the optimal time allocation is found by a one-dimension search to
maximize the minimum rate of all users. Furthermore, two suboptimal designs of
lower complexity are proposed, and their throughput performance is compared
against that of the optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1452</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1452</id><created>2013-12-05</created><authors><author><keyname>Fagerholm</keyname><forenames>Fabian</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Developer Experience: Concept and Definition</title><categories>cs.SE</categories><comments>5 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6225984</comments><journal-ref>Proceedings of the International Conference on Software and System
  Process (ICSSP 2012), pages 73-77, Zurich, Switzerland, June 2-3 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New ways of working such as globally distributed development or the
integration of self-motivated external developers into software ecosystems will
require a better and more comprehensive understanding of developers' feelings,
perceptions, motivations and identification with their tasks in their
respective project environments. User experience is a concept that captures how
persons feel about products, systems and services. It evolved from disciplines
such as interaction design and usability to a much richer scope that includes
feelings, motivations, and satisfaction. Similarly, developer experience could
be defined as a means for capturing how developers think and feel about their
activities within their working environments, with the assumption that an
improvement of the developer experience has positive impacts on characteristics
such as sustained team and project performance. This article motivates the
importance of developer experience, sketches related approaches from other
domains, proposes a definition of developer experience that is derived from
similar concepts in other domains, describes an ongoing empirical study to
better understand developer experience, and finally gives an outlook on planned
future research activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1460</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1460</id><created>2013-12-05</created><authors><author><keyname>Kahanwal</keyname><forenames>Dr. Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Dr. Tejinder Pal</forenames></author></authors><title>Towards the Framework of Information Security</title><categories>cs.CR</categories><comments>4 pages, 4 figures, 1 table</comments><journal-ref>Journal of Current Engineering Research, 2(2), pp. 31-34, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays modern society is extremely dependent on computer based information
systems. Many of the organizations would simply not be able to function
properly without services provided by these systems, just like financing
organizations. Although interruption might decrease the efficiency of an
organization, theft or unintentional disclosure of entrusted private data could
have more serious consequences, such as legal actions as well as loss of
business due to lack of trust from potential users. This dependence on
information systems has lead to a need for securing these systems and this in
turn has created a need for knowing how secure they are. The introduction of
the information society has changed how people interact with government
agencies. Government agencies are now encouraged to uphold a 24-hour electronic
service to the citizens. The introduction of government services on the
Internet is meant to facilitate communication with agencies, decrease service
times and to lessen the amount of papers that needs to be processed. The
increased connectivity to the Internet results in a rising demand for
information security in these systems. In this article, we have discussed about
many file data breaches in the past and current history and they are going to
increase day by day as the reports by DataLossDB (Open Security Foundation)
organization, a non-profit organization in US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1461</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1461</id><created>2013-12-05</created><authors><author><keyname>Pramanik</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author></authors><title>Multi-Sensor Image Fusion Based on Moment Calculation</title><categories>cs.CV</categories><comments>5 pages, International Conference</comments><journal-ref>IEEE International Conference on Parallel, Distributed and Grid
  Computing-PDGC, 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  An image fusion method based on salient features is proposed in this paper.
In this work, we have concentrated on salient features of the image for fusion
in order to preserve all relevant information contained in the input images and
tried to enhance the contrast in fused image and also suppressed noise to a
maximum extent. In our system, first we have applied a mask on two input images
in order to conserve the high frequency information along with some low
frequency information and stifle noise to a maximum extent. Thereafter, for
identification of salience features from sources images, a local moment is
computed in the neighborhood of a coefficient. Finally, a decision map is
generated based on local moment in order to get the fused image. To verify our
proposed algorithm, we have tested it on 120 sensor image pairs collected from
Manchester University UK database. The experimental results show that the
proposed method can provide superior fused image in terms of several
quantitative fusion evaluation index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1462</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1462</id><created>2013-12-05</created><authors><author><keyname>Pramanik</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author></authors><title>Geometric Feature Based Face-Sketch Recognition</title><categories>cs.CV</categories><comments>7 pages, International Conference</comments><journal-ref>IEEE International Conf on Pattern Recognition, Informatics and
  Medical Engineering,PRIME-2012,March 21-23</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper presents a novel facial sketch image or face-sketch recognition
approach based on facial feature extraction. To recognize a face-sketch, we
have concentrated on a set of geometric face features like eyes, nose,
eyebrows, lips, etc and their length and width ratio because it is difficult to
match photos and sketches because they belong to two different modalities. In
this system, first the facial features/components from training images are
extracted, then ratios of length, width, and area etc. are calculated and those
are stored as feature vectors for individual images. After that the mean
feature vectors are computed and subtracted from each feature vector for
centering of the feature vectors. In the next phase, feature vector for the
incoming probe face-sketch is also computed in similar fashion. Here, K-NN
classifier is used to recognize probe face-sketch. It is experimentally
verified that the proposed method is robust against faces are in a frontal
pose, with normal lighting and neutral expression and have no occlusions. The
experiment has been conducted with 80 male and female face images from
different face databases. It has useful applications for both law enforcement
and digital entertainment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1474</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1474</id><created>2013-12-05</created><authors><author><keyname>Chakrabarti</keyname><forenames>Anindya S.</forenames></author><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author></authors><title>Self-organized coordination in collective response of non-interacting
  agents: Emergence of bimodality in box-office success</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures + 2 pages supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many socio-economic phenomena are characterized by the appearance of a few
&quot;hit&quot; products having a substantially higher popularity compared to their often
equivalent competitors, reflected in a bimodal distribution of response
(success). Using the example of box-office performance of movies, we show that
the empirically observed bimodality can emerge via self-organization in a model
where agents (theatres) independently decide whether to adapt a new movie. The
response exhibits extreme variability even in the absence of learning or
communication between agents and suggests that properly timing the release is a
key determinant of box-office success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1482</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1482</id><created>2013-12-05</created><authors><author><keyname>Schuh</keyname><forenames>Fabian</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Low Complexity Decoding for Punctured Trellis-Coded Modulation Over
  Intersymbol Interference Channels</title><categories>cs.IT math.IT</categories><comments>4 pages, 7 pictured, accepted for 2014 International Zurich Seminar
  on Communications</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Classical trellis-coded modulation (TCM) as introduced by Ungerboeck in
1976/1983 uses a signal constellation of twice the cardinality compared to an
uncoded transmission with one bit of redundancy per PAM symbol, i.e.,
application of codes with rates $\frac{n-1}{n}$ when $2^{n}$ denotes the
cardinality of the signal constellation. The original approach therefore only
comprises integer transmission rates, i.e., $R=\left\{ 2,\,3,\,4\,\ldots
\right\}$, additionally, when transmitting over an intersymbol interference
(ISI) channel an optimum decoding scheme would perform equalization and
decoding of the channel code jointly. In this paper, we allow rate adjustment
for TCM by means of puncturing the convolutional code (CC) on which a TCM
scheme is based on. In this case a nontrivial mapping of the output symbols of
the CC to signal points results in a time-variant trellis. We propose an
efficient technique to integrate an ISI-channel into this trellis and show that
the computational complexity can be significantly reduced by means of a reduced
state sequence estimation (RSSE) algorithm for time-variant trellises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1492</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1492</id><created>2013-12-05</created><updated>2014-07-19</updated><authors><author><keyname>Kurlin</keyname><forenames>Vitaliy</forenames></author></authors><title>A fast and robust algorithm to count topologically persistent holes in
  noisy clouds</title><categories>cs.CG cs.CV math.AT</categories><comments>Full version of the paper that has appeared in Proceedings of IEEE
  conference CVPR 2014: Computer Vision and Pattern Recognition, Columbus,
  Ohio, USA (10 pages, 20 figures, 3 appendices, more examples will be at
  http://kurlin.org)</comments><msc-class>68U05 (Primary) 65D18, 68U10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preprocessing a 2D image often produces a noisy cloud of interest points. We
study the problem of counting holes in unorganized clouds in the plane. The
holes in a given cloud are quantified by the topological persistence of their
boundary contours when the cloud is analyzed at all possible scales. We design
the algorithm to count holes that are most persistent in the filtration of
offsets (neighborhoods) around given points. The input is a cloud of $n$ points
in the plane without any user-defined parameters. The algorithm has $O(n\log
n)$ time and $O(n)$ space. The output is the array (number of holes, relative
persistence in the filtration). We prove theoretical guarantees when the
algorithm finds the correct number of holes (components in the complement) of
an unknown shape approximated by a cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1494</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1494</id><created>2013-12-05</created><authors><author><keyname>Kurlin</keyname><forenames>Vitaliy</forenames></author></authors><title>Approximating persistent homology for a cloud of $n$ points in a
  subquadratic time</title><categories>cs.CG cs.CV math.AT</categories><comments>12 pages, 2 figures, more examples will be at http://kurlin.org</comments><msc-class>68U05 (Primary) 65D18, 68U10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vietoris-Rips filtration for an $n$-point metric space is a sequence of
large simplicial complexes adding a topological structure to the otherwise
disconnected space. The persistent homology is a key tool in topological data
analysis and studies topological features of data that persist over many
scales. The fastest algorithm for computing persistent homology of a filtration
has time $O(M(u)+u^2\log^2 u)$, where $u$ is the number of updates (additions
or deletions of simplices), $M(u)=O(u^{2.376})$ is the time for multiplication
of $u\times u$ matrices. For a space of $n$ points given by their pairwise
distances, we approximate the Vietoris-Rips filtration by a zigzag filtration
consisting of $u=o(n)$ updates, which is sublinear in $n$. The constant depends
on a given error of approximation and on the doubling dimension of the metric
space. Then the persistent homology of this sublinear-size filtration can be
computed in time $o(n^2)$, which is subquadratic in $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1512</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1512</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>An adaptive block based integrated LDP,GLCM,and Morphological features
  for Face Recognition</title><categories>cs.CV</categories><comments>7 pages, Science Academy Publisher, United Kingdom</comments><journal-ref>International Journal of Research and Reviews in Computer
  Science-IJRRCS,2011 ISSN: 2079-2557, Vol. 2, No. 5, October 2011</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper proposes a technique for automatic face recognition using
integrated multiple feature sets extracted from the significant blocks of a
gradient image. We discuss about the use of novel morphological, local
directional pattern (LDP) and gray-level co-occurrence matrix GLCM based
feature extraction technique to recognize human faces. Firstly, the new
morphological features i.e., features based on number of runs of pixels in four
directions (N,NE,E,NW) are extracted, together with the GLCM based statistical
features and LDP features that are less sensitive to the noise and
non-monotonic illumination changes, are extracted from the significant blocks
of the gradient image. Then these features are concatenated together. We
integrate the above mentioned methods to take full advantage of the three
approaches. Extraction of the significant blocks from the absolute gradient
image and hence from the original image to extract pertinent information with
the idea of dimension reduction forms the basis of the work. The efficiency of
our method is demonstrated by the experiment on 1100 images from the FRAV2D
face database, 2200 images from the FERET database, where the images vary in
pose, expression, illumination and scale and 400 images from the ORL face
database, where the images slightly vary in pose. Our method has shown 90.3%,
93% and 98.75% recognition accuracy for the FRAV2D, FERET and the ORL database
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1517</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1517</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>A Gabor block based Kernel Discriminative Common Vector (KDCV) approach
  using cosine kernels for Human Face Recognition</title><categories>cs.CV</categories><comments>9 pages,Hindawi Publishing Corporation, Received 14 March 2012;
  Revised 16 July 2012; Accepted 13 August 2012. International Journal of
  Computational Intelligence and Neuroscience,2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper a nonlinear Gabor Wavelet Transform (GWT) discriminant feature
extraction approach for enhanced face recognition is proposed. Firstly, the
low-energized blocks from Gabor wavelet transformed images are extracted.
Secondly, the nonlinear discriminating features are analyzed and extracted from
the selected low-energized blocks by the generalized Kernel Discriminative
Common Vector (KDCV) method. The KDCV method is extended to include cosine
kernel function in the discriminating method. The KDCV with the cosine kernels
is then applied on the extracted low energized discriminating feature vectors
to obtain the real component of a complex quantity for face recognition. In
order to derive positive kernel discriminative vectors; we apply only those
kernel discriminative eigenvectors that are associated with non-zero
eigenvalues. The feasibility of the low energized Gabor block based generalized
KDCV method with cosine kernel function models has been successfully tested for
image classification using the L1, L2 distance measures; and the cosine
similarity measure on both frontal and pose-angled face recognition.
Experimental results on the FRAV2D and the FERET database demonstrate the
effectiveness of this new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1520</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1520</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>A Face Recognition approach based on entropy estimate of the nonlinear
  DCT features in the Logarithm Domain together with Kernel Entropy Component
  Analysis</title><categories>cs.CV</categories><comments>9 pages,Published Online August 2013 in MECS. International Journal
  of Information Technology and Computer Science, 2013. arXiv admin note: text
  overlap with arXiv:1112.3712 by other authors</comments><doi>10.5815/ijitcs.2013.09.03</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exploits the feature extraction capabilities of the discrete
cosine transform (DCT) together with an illumination normalization approach in
the logarithm domain that increase its robustness to variations in facial
geometry and illumination. Secondly in the same domain the entropy measures are
applied on the DCT coefficients so that maximum entropy preserving pixels can
be extracted as the feature vector. Thus the informative features of a face can
be extracted in a low dimensional space. Finally, the kernel entropy component
analysis (KECA) with an extension of arc cosine kernels is applied on the
extracted DCT coefficients that contribute most to the entropy estimate to
obtain only those real kernel ECA eigenvectors that are associated with
eigenvalues having high positive entropy contribution. The resulting system was
successfully tested on real image sequences and is robust to significant
partial occlusion and illumination changes, validated with the experiments on
the FERET, AR, FRAV2D and ORL face databases. Experimental comparison is
demonstrated to prove the superiority of the proposed approach in respect to
recognition accuracy. Using specificity and sensitivity we find that the best
is achieved when Renyi entropy is applied on the DCT coefficients. Extensive
experimental comparison is demonstrated to prove the superiority of the
proposed approach in respect to recognition accuracy. Moreover, the proposed
approach is very simple, computationally fast and can be implemented in any
real-time face recognition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1523</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1523</id><created>2013-12-05</created><authors><author><keyname>Averbuch</keyname><forenames>A.</forenames></author><author><keyname>Shabtai</keyname><forenames>R. Hollander</forenames></author><author><keyname>Roditty</keyname><forenames>Y.</forenames></author></authors><title>Efficient construction of broadcast graphs</title><categories>cs.DM</categories><comments>19 pages, 3 figures. Submitted on January 10th 2012 to Applied
  Descrete Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A broadcast graph is a connected graph, $G=(V,E)$, $ |V |=n$, in which each
vertex can complete broadcasting of one message within at most $t=\lceil \log
n\rceil$ time units. A minimum broadcast graph on $n$ vertices is a broadcast
graph with the minimum number of edges over all broadcast graphs on $n$
vertices. The cardinality of the edge set of such a graph is denoted by $B(n)$.
In this paper we construct a new broadcast graph with
  $B(n) \le (k+1)N -(t-\frac{k}{2}+2)2^{k}+t-k+2$, for $n=N=(2^{k}-1)2^{t+1-k}$
and
  $B(n) \le (k+1-p)n -(t-\frac{k}{2}+p+2)2^{k}+t-k -(p-2)2^{p}$, for $2^{t} &lt;
n&lt;(2^{k}-1)2^{t+1-k}$, where $t \geq 7$, $2 \le k \le \lfloor t/2 \rfloor -1$
for even $n$ and $2 \le k \le \lceil t/2 \rceil -1$ for odd $n$, $d=N-n$, $x=
\lfloor \frac{d}{2^{t+1-k}} \rfloor$ and $ p = \lfloor \log_{2}{(x+1)} \rfloor$
if $x&gt;0$ and $p=0$ if $x=0$.
  The new bound is an improvement upon the bound presented by Harutyunyan and
Liestman (2012) for odd values of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1526</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1526</id><created>2013-12-05</created><authors><author><keyname>Amiri</keyname><forenames>Saeed</forenames></author><author><keyname>Golshani</keyname><forenames>Ali</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author><author><keyname>Siebertz</keyname><forenames>Sebastian</forenames></author></authors><title>Vertex Disjoint Path in Upward Planar Graphs</title><categories>cs.CC cs.DS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-vertex disjoint paths problem is one of the most studied problems in
algorithmic graph theory. In 1994, Schrijver proved that the problem can be
solved in polynomial time for every fixed $k$ when restricted to the class of
planar digraphs and it was a long standing open question whether it is
fixed-parameter tractable (with respect to parameter $k$) on this restricted
class. Only recently, \cite{CMPP}.\ achieved a major breakthrough and answered
the question positively. Despite the importance of this result (and the
brilliance of their proof), it is of rather theoretical importance. Their proof
technique is both technically extremely involved and also has at least double
exponential parameter dependence. Thus, it seems unrealistic that the algorithm
could actually be implemented. In this paper, therefore, we study a smaller
class of planar digraphs, the class of upward planar digraphs, a well studied
class of planar graphs which can be drawn in a plane such that all edges are
drawn upwards. We show that on the class of upward planar digraphs the problem
(i) remains NP-complete and (ii) the problem is fixed-parameter tractable.
While membership in FPT follows immediately from \cite{CMPP}'s general result,
our algorithm has only single exponential parameter dependency compared to the
double exponential parameter dependence for general planar digraphs.
Furthermore, our algorithm can easily be implemented, in contrast to the
algorithm in \cite{CMPP}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1528</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1528</id><created>2013-12-05</created><authors><author><keyname>Dokuchaev</keyname><forenames>M.</forenames></author><author><keyname>Novikov</keyname><forenames>B.</forenames></author><author><keyname>Zholtkevych</keyname><forenames>G.</forenames></author></authors><title>Partial actions and automata</title><categories>cs.FL math.GR</categories><comments>13 pages</comments><msc-class>20M35(Primary), 20M30(Secondary)</msc-class><journal-ref>Algebra and Discrete Mathematics, 11(2011), N 2, 51-63</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the notion of a partial action of a monoid to introduce a
generalization of automata, which we call &quot;a preautomaton&quot;. We study properties
of preautomata and of languages recognized by preautomata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1529</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1529</id><created>2013-12-05</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Instruction sequence expressions for the Karatsuba multiplication
  algorithm</title><categories>cs.PL</categories><comments>14 pages. arXiv admin note: substantial text overlap with
  arXiv:1308.0219</comments><acm-class>F.1.1; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Karatsuba multiplication algorithm is an algorithm for computing the
product of two natural numbers represented in the binary number system. This
means that the algorithm actually computes a function on bit strings. The
restriction of this function to bit strings of any given length can be computed
according to the Karatsuba multiplication algorithm by a finite instruction
sequence that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. We
describe the instruction sequences concerned for the restrictions to bit
strings of the different lengths by uniform terms from an algebraic theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1530</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1530</id><created>2013-12-05</created><updated>2014-07-06</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Hatano</keyname><forenames>Kohei</forenames></author><author><keyname>Takimoto</keyname><forenames>Eiji</forenames></author></authors><title>Bandit Online Optimization Over the Permutahedron</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The permutahedron is the convex polytope with vertex set consisting of the
vectors $(\pi(1),\dots, \pi(n))$ for all permutations (bijections) $\pi$ over
$\{1,\dots, n\}$. We study a bandit game in which, at each step $t$, an
adversary chooses a hidden weight weight vector $s_t$, a player chooses a
vertex $\pi_t$ of the permutahedron and suffers an observed loss of
$\sum_{i=1}^n \pi(i) s_t(i)$.
  A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a
regret of $O(n\sqrt{T \log n})$ for a time horizon of $T$. Unfortunately,
CombBand requires at each step an $n$-by-$n$ matrix permanent approximation to
within improved accuracy as $T$ grows, resulting in a total running time that
is super linear in $T$, making it impractical for large time horizons.
  We provide an algorithm of regret $O(n^{3/2}\sqrt{T})$ with total time
complexity $O(n^3T)$. The ideas are a combination of CombBand and a recent
algorithm by Ailon (2013) for online optimization over the permutahedron in the
full information setting. The technical core is a bound on the variance of the
Plackett-Luce noisy sorting process's &quot;pseudo loss&quot;. The bound is obtained by
establishing positive semi-definiteness of a family of 3-by-3 matrices
generated from rational functions of exponentials of 3 parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1558</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1558</id><created>2013-12-05</created><updated>2013-12-11</updated><authors><author><keyname>Hamrouni</keyname><forenames>Tarek</forenames></author><author><keyname>Yahia</keyname><forenames>Sadok Ben</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>Efficient construction of the lattice of frequent closed patterns and
  simultaneous extraction of generic bases of rules</title><categories>cs.DS</categories><comments>50 pages, in French</comments><journal-ref>Mathematics and Social Sciences, Volume 49, Number 195, 2011(3),
  pages 5-54</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years, the amount of collected data, in various computer
science applications, has grown considerably. These large volumes of data need
to be analyzed in order to extract useful hidden knowledge. This work focuses
on association rule extraction. This technique is one of the most popular in
data mining. Nevertheless, the number of extracted association rules is often
very high, and many of them are redundant. In this paper, we propose a new
algorithm, called PRINCE. Its main feature is the construction of a partially
ordered structure for extracting subsets of association rules, called generic
bases. Without loss of information these subsets form representation of the
whole association rule set. To reduce the cost of such a construction, the
partially ordered structure is built thanks to the minimal generators
associated to frequent closed patterns. The closed ones are simultaneously
derived with generic bases thanks to a simple bottom-up traversal of the
obtained structure. The experimentations we carried out in benchmark and &quot;worst
case&quot; contexts showed the efficiency of the proposed algorithm, compared to
algorithms like CLOSE, A-CLOSE and TITANIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1559</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1559</id><created>2013-12-05</created><updated>2016-02-03</updated><authors><author><keyname>Rok</keyname><forenames>Alexandre</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Outerstring graphs are $\chi$-bounded</title><categories>math.CO cs.CG cs.DM</categories><comments>Minor corrections and updates</comments><msc-class>05C62, 05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An outerstring graph is an intersection graph of curves lying in a halfplane
with one endpoint on the boundary of the halfplane. It is proved that the
outerstring graphs are $\chi$-bounded, that is, their chromatic number is
bounded by a function of their clique number. This generalizes a series of
previous results on $\chi$-boundedness of outerstring graphs with various
restrictions of the shape of the curves or the number of times the pairs of
curves can intersect. This also implies that the intersection graphs of
$x$-monotone curves with bounded clique number have chromatic number $O(\log
n)$, improving the previous polylogarithmic upper bound. The assumption that
each curve has an endpoint on the boundary of the halfplane is justified by the
known fact that triangle-free intersection graphs of straight-line segments can
have arbitrarily large chromatic number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1577</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1577</id><created>2013-12-05</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>On Coordinating Ultra-Dense Wireless Access Networks: Optimization
  Modeling, Algorithms and Insights</title><categories>cs.IT math.IT</categories><comments>ART-COMP PE7/396 Research Project Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network densification along with universal resources reuse is expected to
play a key role in the realization of 5G radio access as an enabler for
delivering most of the anticipated network capacity improvements. On the one
hand, neither the expected additional spectrum allocation nor the forthcoming
novel air-interface processing techniques will be sufficient for sustaining the
anticipated exponentially-increasing mobile data traffic. On the other hand,
enhanced ultra-dense infrastructure deployments are expected to provide
remarkable capacity gains, regardless of the evolutionary or revolutionary
approach followed towards 5G development. In this work, we thoroughly examine
global network coordination as the main enabler for future 5G large dense
small-cell deployments. We propose a powerful radio resources coordination
framework through which interference management is handled network-wise and
jointly over multiple dimensions. In particular, we explore strategies for
pairing serving and served access nodes, partitioning the available network
resources, as well as dynamically allocating power per pair, towards optimizing
system performance and guaranteeing individual minimum performance levels. We
develop new optimization formulations, providing network scaling performance
upper bounds, along with lower complexity algorithmic solutions tailored to
large networks. We apply the proposed solutions to dense network deployments,
in order to obtain useful insights on network performance and optimization,
such as rate scaling, infrastructure density, optimal bandwidth partitioning
and spatial reuse factor optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1583</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1583</id><created>2013-12-05</created><authors><author><keyname>Niederreiter</keyname><forenames>Harald</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Sequences with high nonlinear complexity</title><categories>cs.IT math.IT math.NT</categories><msc-class>11K45, 68Q30, 94A55, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve lower bounds on the $k$th-order nonlinear complexity of
pseudorandom sequences over finite fields and we establish a probabilistic
result on the behavior of the $k$th-order nonlinear complexity of random
sequences over finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1593</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1593</id><created>2013-12-05</created><authors><author><keyname>Aktas</keyname><forenames>Tugcan</forenames></author><author><keyname>Yilmaz</keyname><forenames>A. Ozgur</forenames></author><author><keyname>Aktas</keyname><forenames>Emre</forenames></author></authors><title>Performance Analysis of Network Coded Systems Under Quasi-static
  Rayleigh Fading Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 7 figures, Submitted to IEEE Transactions on
  Communications. arXiv admin note: text overlap with arXiv:1301.6471</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the area of basic and network coded cooperative communication, the
expected end-to-end bit error rate (BER) values are frequently required to
compare the proposed coding, relaying, and decoding techniques. Instead of
obtaining these values via time consuming Monte Carlo simulations, deriving
closed form expressions using approximations is crucial. In this work, the
ultimate goal is to derive an approximate average BER expression for a network
coded system. While reaching this goal, we firstly consider the cooperative
systems' instantaneous BER values that are commonly composed of Q-functions of
more than one variables. For these Q-functions, we investigate the convergence
characteristics of the sampling property and generalize this property to
arbitrary functions of multiple variables. Second, we adapt the equivalent
channel approach to the network coded scenario for the ease of analysis and
propose a network decoder with reduced complexity. Finally, by combining these
techniques, we show that the obtained closed form expressions well agree with
simulation results in a wide SNR range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1611</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1611</id><created>2013-12-05</created><authors><author><keyname>Kharitonov</keyname><forenames>Eugene</forenames></author><author><keyname>Macdonald</keyname><forenames>Craig</forenames></author><author><keyname>Serdyukov</keyname><forenames>Pavel</forenames></author><author><keyname>Ounis</keyname><forenames>Iadh</forenames></author></authors><title>Intent Models for Contextualising and Diversifying Query Suggestions</title><categories>cs.IR</categories><comments>A short version of this paper was presented at CIKM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The query suggestion or auto-completion mechanisms help users to type less
while interacting with a search engine. A basic approach that ranks suggestions
according to their frequency in the query logs is suboptimal. Firstly, many
candidate queries with the same prefix can be removed as redundant. Secondly,
the suggestions can also be personalised based on the user's context. These two
directions to improve the aforementioned mechanisms' quality can be in
opposition: while the latter aims to promote suggestions that address search
intents that a user is likely to have, the former aims to diversify the
suggestions to cover as many intents as possible. We introduce a
contextualisation framework that utilises a short-term context using the user's
behaviour within the current search session, such as the previous query, the
documents examined, and the candidate query suggestions that the user has
discarded. This short-term context is used to contextualise and diversify the
ranking of query suggestions, by modelling the user's information need as a
mixture of intent-specific user models. The evaluation is performed offline on
a set of approximately 1.0M test user sessions. Our results suggest that the
proposed approach significantly improves query suggestions compared to the
baseline approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1613</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1613</id><created>2013-12-05</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>Max-Min Distance Nonnegative Matrix Factorization</title><categories>stat.ML cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Matrix Factorization (NMF) has been a popular representation
method for pattern classification problem. It tries to decompose a nonnegative
matrix of data samples as the product of a nonnegative basic matrix and a
nonnegative coefficient matrix, and the coefficient matrix is used as the new
representation. However, traditional NMF methods ignore the class labels of the
data samples. In this paper, we proposed a supervised novel NMF algorithm to
improve the discriminative ability of the new representation. Using the class
labels, we separate all the data sample pairs into within-class pairs and
between-class pairs. To improve the discriminate ability of the new NMF
representations, we hope that the maximum distance of the within-class pairs in
the new NMF space could be minimized, while the minimum distance of the
between-class pairs pairs could be maximized. With this criterion, we construct
an objective function and optimize it with regard to basic and coefficient
matrices and slack variables alternatively, resulting in a iterative algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1629</identifier>
 <datestamp>2013-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1629</id><created>2013-12-05</created><authors><author><keyname>Thakur</keyname><forenames>Manoj Rameshchandra</forenames></author><author><keyname>Khilnani</keyname><forenames>Divye Raj</forenames></author><author><keyname>Gupta</keyname><forenames>Kushagra</forenames></author><author><keyname>Jain</keyname><forenames>Sandeep</forenames></author><author><keyname>Agarwal</keyname><forenames>Vineet</forenames></author><author><keyname>Sane</keyname><forenames>Suneeta</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Dhekne</keyname><forenames>Prabhakar S</forenames></author></authors><title>Detection and prevention of botnets and malware in an enterprise network</title><categories>cs.CR</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant threats faced by enterprise networks today is
from Bots. A Bot is a program that operates as an agent for a user and runs
automated tasks over the internet, at a much higher rate than would be possible
for a human alone. A collection of Bots in a network, used for malicious
purposes is referred to as a Botnet. Bot attacks can range from localized
attacks like key-logging to network intensive attacks like Distributed Denial
of Service (DDoS). In this paper, we suggest a novel approach that can detect
and combat Bots. The proposed solution adopts a two pronged strategy which we
have classified into the standalone algorithm and the network algorithm. The
standalone algorithm runs independently on each node of the network. It
monitors the active processes on the node and tries to identify Bot processes
using parameters such as response time and output to input traffic ratio. If a
suspicious process has been identified the network algorithm is triggered. The
network algorithm will then analyze conversations to and from the hosts of the
network using the transport layer flow records. It then tries to deduce the Bot
pattern as well as Bot signatures which can subsequently be used by the
standalone algorithm to thwart Bot processes at their very onset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1638</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1638</id><created>2013-12-05</created><updated>2014-05-07</updated><authors><author><keyname>Witherden</keyname><forenames>Freddie D</forenames></author><author><keyname>Farrington</keyname><forenames>Antony M</forenames></author><author><keyname>Vincent</keyname><forenames>Peter E</forenames></author></authors><title>PyFR: An Open Source Framework for Solving Advection-Diffusion Type
  Problems on Streaming Architectures using the Flux Reconstruction Approach</title><categories>physics.comp-ph cs.NA math.NA</categories><msc-class>65M60 65M70</msc-class><doi>10.1016/j.cpc.2014.07.011</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  High-order numerical methods for unstructured grids combine the superior
accuracy of high-order spectral or finite difference methods with the geometric
flexibility of low-order finite volume or finite element schemes. The Flux
Reconstruction (FR) approach unifies various high-order schemes for
unstructured grids within a single framework. Additionally, the FR approach
exhibits a significant degree of element locality, and is thus able to run
efficiently on modern streaming architectures, such as Graphical Processing
Units (GPUs). The aforementioned properties of FR mean it offers a promising
route to performing affordable, and hence industrially relevant,
scale-resolving simulations of hitherto intractable unsteady flows within the
vicinity of real-world engineering geometries. In this paper we present PyFR,
an open-source Python based framework for solving advection-diffusion type
problems on streaming architectures using the FR approach. The framework is
designed to solve a range of governing systems on mixed unstructured grids
containing various element types. It is also designed to target a range of
hardware platforms via use of an in-built domain specific language based on the
Mako templating engine. The current release of PyFR is able to solve the
compressible Euler and Navier-Stokes equations on grids of quadrilateral and
triangular elements in two dimensions, and hexahedral elements in three
dimensions, targeting clusters of CPUs, and NVIDIA GPUs. Results are presented
for various benchmark flow problems, single-node performance is discussed, and
scalability of the code is demonstrated on up to 104 NVIDIA M2090 GPUs. The
software is freely available under a 3-Clause New Style BSD license (see
www.pyfr.org).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1655</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1655</id><created>2013-12-05</created><updated>2014-07-17</updated><authors><author><keyname>Bardet</keyname><forenames>Magali</forenames></author><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>On the Complexity of the F5 Gr\&quot;obner basis Algorithm</title><categories>cs.SC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of Gr\&quot;obner bases computation, in particular in the
generic situation where the variables are in simultaneous Noether position with
respect to the system.
  We give a bound on the number of polynomials of degree $d$ in a Gr\&quot;obner
basis computed by Faug\`ere's $F_5$ algorithm~(Fau02) in this generic case for
the grevlex ordering (which is also a bound on the number of polynomials for a
reduced Gr\&quot;obner basis, independently of the algorithm used). Next, we analyse
more precisely the structure of the polynomials in the Gr\&quot;obner bases with
signatures that $F_5$ computes and use it to bound the complexity of the
algorithm.
  Our estimates show that the version of~$F_5$ we analyse, which uses only
standard Gaussian elimination techniques, outperforms row reduction of the
Macaulay matrix with the best known algorithms for moderate degrees, and even
for degrees up to the thousands if Strassen's multiplication is used. The
degree being fixed, the factor of improvement grows exponentially with the
number of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1661</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1661</id><created>2013-12-05</created><updated>2016-03-07</updated><authors><author><keyname>Vasiliev</keyname><forenames>Alexander</forenames></author></authors><title>Quantum Communications Based on Quantum Hashing</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider an application of the recently proposed quantum
hashing technique for computing Boolean functions in the quantum communication
model. The combination of binary functions on non-binary quantum hash function
is done via polynomial presentation, which we have called a characteristic of a
Boolean function. Based on the characteristic polynomial presentation of
Boolean functions and quantum hashing technique we present a method for
computing Boolean functions in the quantum one-way communication model, where
one of the parties performs his computations and sends a message to the other
party, who must output the result after his part of computations. Some of the
results are also true in a more restricted Simultaneous Message Passing model
with no shared resources, in which communicating parties can interact only via
the referee. We give several examples of Boolean functions whose polynomial
presentations have specific properties allowing for construction of quantum
communication protocols that are provably exponentially better than classical
ones in the simultaneous message passing setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1664</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1664</id><created>2013-12-05</created><updated>2014-09-11</updated><authors><author><keyname>Vergne</keyname><forenames>Ana&#xef;s</forenames><affiliation>INRIA Sophia Antipolis / INRIA Saclay - Ile de France</affiliation></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author></authors><title>Simplicial Homology for Future Cellular Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simplicial homology is a tool that provides a mathematical way to compute the
connectivity and the coverage of a cellular network without any node location
information. In this article, we use simplicial homology in order to not only
compute the topology of a cellular network, but also to discover the clusters
of nodes still with no location information. We propose three algorithms for
the management of future cellular networks. The first one is a frequency
auto-planning algorithm for the self-configuration of future cellular networks.
It aims at minimizing the number of planned frequencies while maximizing the
usage of each one. Then, our energy conservation algorithm falls into the
self-optimization feature of future cellular networks. It optimizes the energy
consumption of the cellular network during off-peak hours while taking into
account both coverage and user traffic. Finally, we present and discuss the
performance of a disaster recovery algorithm using determinantal point
processes to patch coverage holes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1666</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1666</id><created>2013-12-05</created><updated>2015-06-16</updated><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Semi-Stochastic Gradient Descent Methods</title><categories>stat.ML cs.LG cs.NA math.NA math.OC</categories><comments>19 pages, 3 figures, 2 algorithms, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of minimizing the average of a large
number ($n$) of smooth convex loss functions. We propose a new method, S2GD
(Semi-Stochastic Gradient Descent), which runs for one or several epochs in
each of which a single full gradient and a random number of stochastic
gradients is computed, following a geometric law. The total work needed for the
method to output an $\varepsilon$-accurate solution in expectation, measured in
the number of passes over data, or equivalently, in units equivalent to the
computation of a single gradient of the loss, is
$O((\kappa/n)\log(1/\varepsilon))$, where $\kappa$ is the condition number.
This is achieved by running the method for $O(\log(1/\varepsilon))$ epochs,
with a single gradient evaluation and $O(\kappa)$ stochastic gradient
evaluations in each. The SVRG method of Johnson and Zhang arises as a special
case. If our method is limited to a single epoch only, it needs to evaluate at
most $O((\kappa/\varepsilon)\log(1/\varepsilon))$ stochastic gradients. In
contrast, SVRG requires $O(\kappa/\varepsilon^2)$ stochastic gradients. To
illustrate our theoretical results, S2GD only needs the workload equivalent to
about 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for
a problem with $n=10^9$ and $\kappa=10^3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1672</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1672</id><created>2013-12-05</created><updated>2014-10-31</updated><authors><author><keyname>de Haan</keyname><forenames>Ronald</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>The Parameterized Complexity of Reasoning Problems Beyond NP</title><categories>cs.CC cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's propositional satisfiability (SAT) solvers are extremely powerful and
can be used as an efficient back-end for solving NP-complete problems. However,
many fundamental problems in knowledge representation and reasoning are located
at the second level of the Polynomial Hierarchy or even higher, and hence
polynomial-time transformations to SAT are not possible, unless the hierarchy
collapses. Recent research shows that in certain cases one can break through
these complexity barriers by fixed-parameter tractable (fpt) reductions which
exploit structural aspects of problem instances in terms of problem parameters.
  In this paper we develop a general theoretical framework that supports the
classification of parameterized problems on whether they admit such an
fpt-reduction to SAT or not. This framework is based on several new
parameterized complexity classes. As a running example, we use the framework to
classify the complexity of the consistency problem for disjunctive answer set
programming, with respect to various natural parameters. We underpin the
robustness of our theory by providing a characterization of the new complexity
classes in terms of weighted QBF satisfiability, alternating Turing machines,
and first-order model checking. In addition, we provide a compendium of
parameterized problems that are complete for the new complexity classes,
including problems related to Knowledge Representation and Reasoning, Logic,
and Combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1674</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1674</id><created>2013-12-05</created><updated>2013-12-23</updated><authors><author><keyname>Huang</keyname><forenames>Ming-Deh</forenames></author><author><keyname>Narayanan</keyname><forenames>Anand Kumar</forenames></author></authors><title>On the relation generation method of Joux for computing discrete
  logarithms</title><categories>cs.CC cs.CR math.NT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.1206</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In \cite{joux}, Joux devised an algorithm to compute discrete logarithms
between elements in a certain subset of the multiplicative group of an
extension of the finite field $\mathbb{F}_{p^n}$ in time polynomial in $p$ and
$n$. Shortly after, Barbulescu, Gaudry, Joux and Thome \cite{bgjt} proposed a
descent algorithm that in $(p n)^{\mathcal{O}(\log n)}$ time projects an
arbitrary element in $\mathbb{F}_{p^n}^\times$ as a product of powers of
elements in the aforementioned subset. Together, these two algorithms yield a
quasi-polynomial time algorithm for computing discrete logarithms in finite
fields of small characteristic. The success of both the algorithms are reliant
on heuristic assumptions. We identify obstructions that prevent certain
heuristic assumptions they make from being true in general. Further, we
describe methods to overcome these obstructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1678</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1678</id><created>2013-12-05</created><updated>2014-10-20</updated><authors><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Pinchasi</keyname><forenames>Rom</forenames></author></authors><title>Note on the number of edges in families with linear union-complexity</title><categories>math.CO cs.CG</categories><comments>background and related work is now more complete; presentation
  improved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple argument showing that the number of edges in the
intersection graph $G$ of a family of $n$ sets in the plane with a linear
union-complexity is $O(\omega(G)n)$. In particular, we prove $\chi(G)\leq
\text{col}(G)&lt; 19\omega(G)$ for intersection graph $G$ of a family of
pseudo-discs, which improves a previous bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1681</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1681</id><created>2013-12-05</created><authors><author><keyname>Pramanik</keyname><forenames>Sourav</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Dr. Debotosh</forenames></author></authors><title>An Approach: Modality Reduction and Face-Sketch Recognition</title><categories>cs.CV</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1312.1462</comments><journal-ref>International Journal of Computational Intelligence and
  Informatics, Vol.1: No. 2, July-September 2011</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  To recognize face sketch through face photo database is a challenging task
for todays researchers. Because face photo images in training set and face
sketch images in testing set have different modality. Difference between two
face photos of difference person is smaller than the difference between same
person in a face photo and face sketched. In this paper, for reduction of the
modality between face photo and face sketch we first bring face photo and face
sketch images in a new dimension using 2D Discrete Haar wavelet transform with
scale 3 followed by a negative approach. After that, extract features from
transformed images using Principal Component Analysis (PCA). Thereafter, we use
SVM classifier and K-NN classifier for better classification. Our proposed
method is experimentally verified by its robustness against faces that are
captured in a good lighting condition and in a frontal pose. The experiment has
been conducted with 100 male and female face images as training set and 100
male and female face sketch images as testing set collected from CUHK training
and testing cropped photos and CUHK training and testing cropped sketches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1683</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1683</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Face Recognition using Hough Peaks extracted from the significant blocks
  of the Gradient Image</title><categories>cs.CV</categories><comments>6 pages. arXiv admin note: substantial text overlap with
  arXiv:1312.1512</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering, ISSN: 2277 128X, Volume 2, Issue 1, January 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper proposes a new technique for automatic face recognition using
integrated peaks of the Hough transformed significant blocks of the binary
gradient image. In this approach firstly the gradient of an image is calculated
and a threshold is set to obtain a binary gradient image, which is less
sensitive to noise and illumination changes. Secondly, significant blocks are
extracted from the absolute gradient image, to extract pertinent information
with the idea of dimension reduction. Finally the best fitted Hough peaks are
extracted from the Hough transformed significant blocks for efficient face
recognition. Then these Hough peaks are concatenated together, which are used
as feature in classification process. The efficiency of the proposed method is
demonstrated by the experiment on 1100 images from the FRAV2D face database,
2200 images from the FERET database, where the images vary in pose, expression,
illumination and scale and 400 images from the ORL face database, where the
images slightly vary in pose. Our method has shown 93.3%, 88.5% and 99%
recognition accuracy for the FRAV2D, FERET and the ORL database respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1684</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1684</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>High Performance Human Face Recognition using Gabor based Pseudo Hidden
  Markov Model</title><categories>cs.CV</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1312.1517</comments><journal-ref>International Journal of Applied Evolutionary Computation, 2013
  4(1), 81-102, January-March 2013</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper introduces a novel methodology that combines the multi-resolution
feature of the Gabor wavelet transformation (GWT) with the local interactions
of the facial structures expressed through the Pseudo Hidden Markov model
(PHMM). Unlike the traditional zigzag scanning method for feature extraction a
continuous scanning method from top-left corner to right then top-down and
right to left and so on until right-bottom of the image i.e. a spiral scanning
technique has been proposed for better feature selection. Unlike traditional
HMMs, the proposed PHMM does not perform the state conditional independence of
the visible observation sequence assumption. This is achieved via the concept
of local structures introduced by the PHMM used to extract facial bands and
automatically select the most informative features of a face image. Thus, the
long-range dependency problem inherent to traditional HMMs has been drastically
reduced. Again with the use of most informative pixels rather than the whole
image makes the proposed method reasonably faster for face recognition. This
method has been successfully tested on frontal face images from the ORL, FRAV2D
and FERET face databases where the images vary in pose, illumination,
expression, and scale. The FERET data set contains 2200 frontal face images of
200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects
and the full ORL database is considered. The results reported in this
application are far better than the recent and most referred systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1685</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1685</id><created>2013-12-05</created><authors><author><keyname>Kar</keyname><forenames>Arindam</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Human Face Recognition using Gabor based Kernel Entropy Component
  Analysis</title><categories>cs.CV</categories><comments>October, 2012. International Journal of Computer Vision and Image
  Processing : IGI Global(USA), 2012. arXiv admin note: substantial text
  overlap with arXiv:1312.1517, arXiv:1312.1520</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, we present a novel Gabor wavelet based Kernel Entropy
Component Analysis (KECA) method by integrating the Gabor wavelet
transformation (GWT) of facial images with the KECA method for enhanced face
recognition performance. Firstly, from the Gabor wavelet transformed images the
most important discriminative desirable facial features characterized by
spatial frequency, spatial locality and orientation selectivity to cope with
the variations due to illumination and facial expression changes were derived.
After that KECA, relating to the Renyi entropy is extended to include cosine
kernel function. The KECA with the cosine kernels is then applied on the
extracted most important discriminating feature vectors of facial images to
obtain only those real kernel ECA eigenvectors that are associated with
eigenvalues having positive entropy contribution. Finally, these real KECA
features are used for image classification using the L1, L2 distance measures;
the Mahalanobis distance measure and the cosine similarity measure. The
feasibility of the Gabor based KECA method with the cosine kernel has been
successfully tested on both frontal and pose-angled face recognition, using
datasets from the ORL, FRAV2D and the FERET database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1706</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1706</id><created>2013-12-05</created><updated>2014-02-22</updated><authors><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Swapping Variables for High-Dimensional Sparse Regression with
  Correlated Measurements</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Parts of this paper have appeared in NIPS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the high-dimensional sparse linear regression problem of
accurately estimating a sparse vector using a small number of linear
measurements that are contaminated by noise. It is well known that the standard
cadre of computationally tractable sparse regression algorithms---such as the
Lasso, Orthogonal Matching Pursuit (OMP), and their extensions---perform poorly
when the measurement matrix contains highly correlated columns. To address this
shortcoming, we develop a simple greedy algorithm, called SWAP, that
iteratively swaps variables until convergence. SWAP is surprisingly effective
in handling measurement matrices with high correlations. In fact, we prove that
SWAP outputs the true support, the locations of the non-zero entries in the
sparse vector, under a relatively mild condition on the measurement matrix.
Furthermore, we show that SWAP can be used to boost the performance of any
sparse regression algorithm. We empirically demonstrate the advantages of SWAP
by comparing it with several state-of-the-art sparse regression algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1718</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1718</id><created>2013-12-05</created><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author></authors><title>Upper semicomputable sumtests for lower semicomputable semimeasures</title><categories>cs.CC</categories><comments>10 pages</comments><msc-class>03D32</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sumtest for a discrete semimeasure $P$ is a function $f$ mapping bitstrings
to non-negative rational numbers such that \[
  \sum P(x)f(x) \le 1 \,.
  \] Sumtests are the discrete analogue of Martin-L\&quot;of tests. The behavior of
sumtests for computable $P$ seems well understood, but for some applications
lower semicomputable $P$ seem more appropriate. In the case of tests for
independence, it is natural to consider upper semicomputable tests (see
[B.Bauwens and S.Terwijn, Theory of Computing Systems 48.2 (2011): 247-268]).
  In this paper, we characterize upper semicomputable sumtests relative to any
lower semicomputable semimeasures using Kolmogorov complexity. It is studied to
what extend such tests are pathological: can upper semicomputable sumtests for
$m(x)$ be large? It is shown that the logarithm of such tests does not exceed
$\log |x| + O(\log^{(2)} |x|)$ (where $|x|$ denotes the length of $x$ and
$\log^{(2)} = \log\log$) and that this bound is tight, i.e. there is a test
whose logarithm exceeds $\log |x| - O(\log^{(2)} |x|$) infinitely often.
Finally, it is shown that for each such test $e$ the mutual information of a
string with the Halting problem is at least $\log e(x)-O(1)$; thus $e$ can only
be large for ``exotic'' strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1719</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1719</id><created>2013-12-05</created><updated>2014-05-15</updated><authors><author><keyname>Bosshart</keyname><forenames>Pat</forenames></author><author><keyname>Daly</keyname><forenames>Dan</forenames></author><author><keyname>Izzard</keyname><forenames>Martin</forenames></author><author><keyname>McKeown</keyname><forenames>Nick</forenames></author><author><keyname>Rexford</keyname><forenames>Jennifer</forenames></author><author><keyname>Schlesinger</keyname><forenames>Cole</forenames></author><author><keyname>Talayco</keyname><forenames>Dan</forenames></author><author><keyname>Vahdat</keyname><forenames>Amin</forenames></author><author><keyname>Varghese</keyname><forenames>George</forenames></author><author><keyname>Walker</keyname><forenames>David</forenames></author></authors><title>Programming Protocol-Independent Packet Processors</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P4 is a high-level language for programming protocol-independent packet
processors. P4 works in conjunction with SDN control protocols like OpenFlow.
In its current form, OpenFlow explicitly specifies protocol headers on which it
operates. This set has grown from 12 to 41 fields in a few years, increasing
the complexity of the specification while still not providing the flexibility
to add new headers. In this paper we propose P4 as a strawman proposal for how
OpenFlow should evolve in the future. We have three goals: (1)
Reconfigurability in the field: Programmers should be able to change the way
switches process packets once they are deployed. (2) Protocol independence:
Switches should not be tied to any specific network protocols. (3) Target
independence: Programmers should be able to describe packet-processing
functionality independently of the specifics of the underlying hardware. As an
example, we describe how to use P4 to configure a switch to add a new
hierarchical label.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1725</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1725</id><created>2013-12-05</created><authors><author><keyname>Kurlin</keyname><forenames>Vitaliy</forenames></author></authors><title>Book embeddings of Reeb graphs</title><categories>cs.CG cs.CV math.GT</categories><comments>12 pages, 5 figures, more examples will be at http://kurlin.org</comments><msc-class>68R10 (Primary) 68U05, 57Q35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be a simplicial complex with a piecewise linear function
$f:X\to\mathbb{R}$. The Reeb graph $Reeb(f,X)$ is the quotient of $X$, where we
collapse each connected component of $f^{-1}(t)$ to a single point. Let the
nodes of $Reeb(f,X)$ be all homologically critical points where any homology of
the corresponding component of the level set $f^{-1}(t)$ changes. Then we can
label every arc of $Reeb(f,X)$ with the Betti numbers
$(\beta_1,\beta_2,\dots,\beta_d)$ of the corresponding $d$-dimensional
component of a level set. The homology labels give more information about the
original complex $X$ than the classical Reeb graph. We describe a canonical
embedding of a Reeb graph into a multi-page book (a star cross a line) and give
a unique linear code of this book embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1727</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1727</id><created>2013-12-05</created><authors><author><keyname>Geng</keyname><forenames>Quan</forenames></author><author><keyname>Do</keyname><forenames>Hieu T.</forenames></author></authors><title>On the Capacity Region of Broadcast Packet Erasure Relay Networks With
  Feedback</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a new outer bound on the capacity region of broadcast traffic in
multiple input broadcast packet erasure channels with feedback, and extend this
outer bound to packet erasure relay networks with feedback. We show the
tightness of the outer bound for various classes of networks. An important
engineering implication of this work is that for network coding schemes for
parallel broadcast channels, the ``xor'' packets should be sent over correlated
broadcast subchannels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1732</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1732</id><created>2013-12-05</created><authors><author><keyname>Vidal</keyname><forenames>Gerard</forenames></author><author><keyname>Baptista</keyname><forenames>Murilo</forenames></author><author><keyname>Mancini</keyname><forenames>Hector</forenames></author></authors><title>A fast and light stream cipher for smartphones</title><categories>cs.CR nlin.CD</categories><doi>10.1140/epjst/e2014-02185-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a stream cipher based on a chaotic dynamical system. Using a
chaotic trajectory sampled under certain rules in order to avoid any attempt to
reconstruct the original one, we create a binary pseudo-random keystream that
can only be exactly reproduced by someone that has fully knowledge of the
communication system parameters formed by a transmitter and a receiver and
sharing the same initial conditions. The plaintext is XORed with the keystream
creating the ciphertext, the encrypted message. This keystream passes the NISTs
randomness test and has been implemented in a videoconference App for
smartphones, in order to show the fast and light nature of the proposed
encryption system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1737</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1737</id><created>2013-12-05</created><authors><author><keyname>Louradour</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Kermorvant</keyname><forenames>Christopher</forenames></author></authors><title>Curriculum Learning for Handwritten Text Line Recognition</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNN) have recently achieved the best performance
in off-line Handwriting Text Recognition. At the same time, learning RNN by
gradient descent leads to slow convergence, and training times are particularly
long when the training database consists of full lines of text. In this paper,
we propose an easy way to accelerate stochastic gradient descent in this
set-up, and in the general context of learning to recognize sequences. The
principle is called Curriculum Learning, or shaping. The idea is to first learn
to recognize short sequences before training on all available training
sequences. Experiments on three different handwritten text databases (Rimes,
IAM, OpenHaRT) show that a simple implementation of this strategy can
significantly speed up the training of RNN for Text Recognition, and even
significantly improve performance in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1740</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1740</id><created>2013-12-05</created><updated>2015-03-28</updated><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Sch&#xfc;lke</keyname><forenames>Christophe</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Approximate message-passing with spatially coupled structured operators,
  with applications to compressed sensing and sparse superposition codes</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>20 pages, 10 figures</comments><journal-ref>J. Stat. Mech. (2015) P05013</journal-ref><doi>10.1088/1742-5468/2015/05/P05013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of Approximate Message-Passing, a solver for linear
sparse estimation problems such as compressed sensing, when the i.i.d matrices
-for which it has been specifically designed- are replaced by structured
operators, such as Fourier and Hadamard ones. We show empirically that after
proper randomization, the structure of the operators does not significantly
affect the performances of the solver. Furthermore, for some specially designed
spatially coupled operators, this allows a computationally fast and memory
efficient reconstruction in compressed sensing up to the
information-theoretical limit. We also show how this approach can be applied to
sparse superposition codes, allowing the Approximate Message-Passing decoder to
perform at large rates for moderate block length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1743</identifier>
 <datestamp>2014-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1743</id><created>2013-12-05</created><updated>2014-06-13</updated><authors><author><keyname>Ramanan</keyname><forenames>Deva</forenames></author></authors><title>Dual coordinate solvers for large-scale structural SVMs</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript describes a method for training linear SVMs (including binary
SVMs, SVM regression, and structural SVMs) from large, out-of-core training
datasets. Current strategies for large-scale learning fall into one of two
camps; batch algorithms which solve the learning problem given a finite
datasets, and online algorithms which can process out-of-core datasets. The
former typically requires datasets small enough to fit in memory. The latter is
often phrased as a stochastic optimization problem; such algorithms enjoy
strong theoretical properties but often require manual tuned annealing
schedules, and may converge slowly for problems with large output spaces (e.g.,
structural SVMs). We discuss an algorithm for an &quot;intermediate&quot; regime in which
the data is too large to fit in memory, but the active constraints (support
vectors) are small enough to remain in memory. In this case, one can design
rather efficient learning algorithms that are as stable as batch algorithms,
but capable of processing out-of-core datasets. We have developed such a
MATLAB-based solver and used it to train a collection of recognition systems
for articulated pose estimation, facial analysis, 3D object recognition, and
action classification, all with publicly-available code. This writeup describes
the solver in detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1752</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1752</id><created>2013-12-05</created><authors><author><keyname>Fuad</keyname><forenames>Muhammad Marwan Muhammad</forenames></author></authors><title>Particle Swarm Optimization of Information-Content Weighting of Symbolic
  Aggregate Approximation</title><categories>cs.NE cs.AI</categories><comments>The 8th International Conference on Advanced Data Mining and
  Applications (ADMA 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-inspired optimization algorithms have been gaining more popularity
recently. One of the most important of these algorithms is particle swarm
optimization (PSO). PSO is based on the collective intelligence of a swam of
particles. Each particle explores a part of the search space looking for the
optimal position and adjusts its position according to two factors; the first
is its own experience and the second is the collective experience of the whole
swarm. PSO has been successfully used to solve many optimization problems. In
this work we use PSO to improve the performance of a well-known representation
method of time series data which is the symbolic aggregate approximation (SAX).
As with other time series representation methods, SAX results in loss of
information when applied to represent time series. In this paper we use PSO to
propose a new minimum distance WMD for SAX to remedy this problem. Unlike the
original minimum distance, the new distance sets different weights to different
segments of the time series according to their information content. This
weighted minimum distance enhances the performance of SAX as we show through
experiments using different time series datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1755</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1755</id><created>2013-12-05</created><authors><author><keyname>Rosenbaum</keyname><forenames>David J.</forenames></author><author><keyname>Wagner</keyname><forenames>Fabian</forenames></author></authors><title>Beating the Generator-Enumeration Bound for $p$-Group Isomorphism</title><categories>cs.DS cs.DM</categories><comments>15 pages. This is an updated and improved version of the results for
  p-groups in arXiv:1205.0642 and TR11-052 in ECCC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the group isomorphism problem: given two finite groups G and H
specified by their multiplication tables, decide if G cong H. For several
decades, the n^(log_p n + O(1)) generator-enumeration bound (where p is the
smallest prime dividing the order of the group) has been the best worst-case
result for general groups. In this work, we show the first improvement over the
generator-enumeration bound for p-groups, which are believed to be the hard
case of the group isomorphism problem. We start by giving a Turing reduction
from group isomorphism to n^((1 / 2) log_p n + O(1)) instances of p-group
composition-series isomorphism. By showing a Karp reduction from p-group
composition-series isomorphism to testing isomorphism of graphs of degree at
most p + O(1) and applying algorithms for testing isomorphism of graphs of
bounded degree, we obtain an n^(O(p)) time algorithm for p-group
composition-series isomorphism. Combining these two results yields an algorithm
for p-group isomorphism that takes at most n^((1 / 2) log_p n + O(p)) time.
This algorithm is faster than generator-enumeration when p is small and slower
when p is large. Choosing the faster algorithm based on p and n yields an upper
bound of n^((1 / 2 + o(1)) log n) for p-group isomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1756</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1756</id><created>2013-12-05</created><updated>2014-08-25</updated><authors><author><keyname>Guo</keyname><forenames>Yinghao</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Joint Energy and Spectrum Cooperation for Cellular Communication Systems</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powered by renewable energy sources, cellular communication systems usually
have different wireless traffic loads and available resources over time. To
match their traffics, it is beneficial for two neighboring systems to cooperate
in resource sharing when one is excessive in one resource (e.g., spectrum),
while the other is sufficient in another (e.g., energy). In this paper, we
propose a joint energy and spectrum cooperation scheme between different
cellular systems to reduce their operational costs. When the two systems are
fully cooperative in nature (e.g., belonging to the same entity), we formulate
the cooperation problem as a convex optimization problem to minimize their
weighted sum cost and obtain the optimal solution in closed form. We also study
another partially cooperative scenario where the two systems have their own
interests. We show that the two systems seek for partial cooperation as long as
they find inter-system complementarity between the energy and spectrum
resources. Under the partial cooperation conditions, we propose a distributed
algorithm for the two systems to gradually and simultaneously reduce their
costs from the non-cooperative benchmark to the Pareto optimum. This
distributed algorithm also has proportional fair cost reduction by reducing
each system's cost proportionally over iterations. Finally, we provide
numerical results to validate the convergence of the distributed algorithm to
the Pareto optimality and compare the centralized and distributed cost
reduction approaches for fully and partially cooperative scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1760</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1760</id><created>2013-12-05</created><authors><author><keyname>Fuad</keyname><forenames>Muhammad Marwan Muhammad</forenames></author></authors><title>Towards Normalizing the Edit Distance Using a Genetic Algorithms Based
  Scheme</title><categories>cs.NE cs.AI</categories><comments>The 8th International Conference on Advanced Data Mining and
  Applications (ADMA 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The normalized edit distance is one of the distances derived from the edit
distance. It is useful in some applications because it takes into account the
lengths of the two strings compared. The normalized edit distance is not
defined in terms of edit operations but rather in terms of the edit path. In
this paper we propose a new derivative of the edit distance that also takes
into consideration the lengths of the two strings, but the new distance is
related directly to the edit distance. The particularity of the new distance is
that it uses the genetic algorithms to set the values of the parameters it
uses. We conduct experiments to test the new distance and we obtain promising
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1763</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1763</id><created>2013-12-05</created><updated>2014-04-15</updated><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author></authors><title>Optimal Error Rates for Interactive Coding II: Efficiency and List
  Decoding</title><categories>cs.DS cs.IT math.IT</categories><comments>preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study coding schemes for error correction in interactive communications.
Such interactive coding schemes simulate any $n$-round interactive protocol
using $N$ rounds over an adversarial channel that corrupts up to $\rho N$
transmissions. Important performance measures for a coding scheme are its
maximum tolerable error rate $\rho$, communication complexity $N$, and
computational complexity.
  We give the first coding scheme for the standard setting which performs
optimally in all three measures: Our randomized non-adaptive coding scheme has
a near-linear computational complexity and tolerates any error rate $\delta &lt;
1/4$ with a linear $N = \Theta(n)$ communication complexity. This improves over
prior results which each performed well in two of these measures.
  We also give results for other settings of interest, namely, the first
computationally and communication efficient schemes that tolerate $\rho &lt;
\frac{2}{7}$ adaptively, $\rho &lt; \frac{1}{3}$ if only one party is required to
decode, and $\rho &lt; \frac{1}{2}$ if list decoding is allowed. These are the
optimal tolerable error rates for the respective settings. These coding schemes
also have near linear computational and communication complexity.
  These results are obtained via two techniques: We give a general black-box
reduction which reduces unique decoding, in various settings, to list decoding.
We also show how to boost the computational and communication efficiency of any
list decoder to become near linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1764</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1764</id><created>2013-12-05</created><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Haeupler</keyname><forenames>Bernhard</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Optimal Error Rates for Interactive Coding I: Adaptivity and Other
  Settings</title><categories>cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of interactive communication in the presence of
adversarial errors and present tight bounds on the tolerable error-rates in a
number of different settings.
  Most significantly, we explore adaptive interactive communication where the
communicating parties decide who should speak next based on the history of the
interaction. Braverman and Rao [STOC'11] show that non-adaptively one can code
for any constant error rate below 1/4 but not more. They asked whether this
bound could be improved using adaptivity. We answer this open question in the
affirmative (with a slightly different collection of resources): Our adaptive
coding scheme tolerates any error rate below 2/7 and we show that tolerating a
higher error rate is impossible. We also show that in the setting of Franklin
et al. [CRYPTO'13], where parties share randomness not known to the adversary,
adaptivity increases the tolerable error rate from 1/2 to 2/3. For
list-decodable interactive communications, where each party outputs a constant
size list of possible outcomes, the tight tolerable error rate is 1/2.
  Our negative results hold even if the communication and computation are
unbounded, whereas for our positive results communication and computation are
polynomially bounded. Most prior work considered coding schemes with linear
amount of communication, while allowing unbounded computations. We argue that
studying tolerable error rates in this relaxed context helps to identify a
setting's intrinsic optimal error rate. We set forward a strong working
hypothesis which stipulates that for any setting the maximum tolerable error
rate is independent of many computational and communication complexity
measures. We believe this hypothesis to be a powerful guideline for the design
of simple, natural, and efficient coding schemes and for understanding the
(im)possibilities of coding for interactive communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1766</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1766</id><created>2013-12-06</created><updated>2014-12-02</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Zhou</keyname><forenames>Yiqing</forenames></author></authors><title>Matrix-Monotonic Optimization for MIMO Systems</title><categories>cs.IT math.IT</categories><comments>37 Pages, 5 figures, IEEE Transactions on Signal Processing, Final
  Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For MIMO systems, due to the deployment of multiple antennas at both the
transmitter and the receiver, the design variables e.g., precoders, equalizers,
training sequences, etc. are usually matrices. It is well known that matrix
operations are usually more complicated compared to their vector counterparts.
In order to overcome the high complexity resulting from matrix variables, in
this paper we investigate a class of elegant multi-objective optimization
problems, namely matrix-monotonic optimization problems (MMOPs). In our work,
various representative MIMO optimization problems are unified into a framework
of matrix-monotonic optimization, which includes linear transceiver design,
nonlinear transceiver design, training sequence design, radar waveform
optimization, the corresponding robust design and so on as its special cases.
Then exploiting the framework of matrix-monotonic optimization the optimal
structures of the considered matrix variables can be derived first. Based on
the optimal structure, the matrix-variate optimization problems can be greatly
simplified into the ones with only vector variables. In particular, the
dimension of the new vector variable is equal to the minimum number of columns
and rows of the original matrix variable. Finally, we also extend our work to
some more general cases with multiple matrix variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1780</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1780</id><created>2013-12-06</created><updated>2013-12-26</updated><authors><author><keyname>Hong</keyname><forenames>Hoon</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Special Algorithm for Stability Analysis of Multistable Biological
  Regulatory Systems</title><categories>cs.SC</categories><comments>24 pages, 5 algorithms, 10 figures</comments><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of counting (stable) equilibriums of an important
family of algebraic differential equations modeling multistable biological
regulatory systems. The problem can be solved, in principle, using real
quantifier elimination algorithms, in particular real root classification
algorithms. However, it is well known that they can handle only very small
cases due to the enormous computing time requirements. In this paper, we
present a special algorithm which is much more efficient than the general
methods. Its efficiency comes from the exploitation of certain interesting
structures of the family of differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1794</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1794</id><created>2013-12-06</created><updated>2015-04-03</updated><authors><author><keyname>Varin</keyname><forenames>Cristiano</forenames></author><author><keyname>Cattelan</keyname><forenames>Manuela</forenames></author><author><keyname>Firth</keyname><forenames>David</forenames></author></authors><title>Statistical Modelling of Citation Exchange Between Statistics Journals</title><categories>stat.AP cs.DL</categories><comments>To be published with discussion on Journal of the Royal Statistical
  Society Series A</comments><journal-ref>Journal of the Royal Statistical Society Series A Volume 179,
  Issue 1 Pages 1 - 318, January 2016</journal-ref><doi>10.1111/rssa.12124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rankings of scholarly journals based on citation data are often met with
skepticism by the scientific community. Part of the skepticism is due to
disparity between the common perception of journals' prestige and their ranking
based on citation counts. A more serious concern is the inappropriate use of
journal rankings to evaluate the scientific influence of authors. This paper
focuses on analysis of the table of cross-citations among a selection of
Statistics journals. Data are collected from the Web of Science database
published by Thomson Reuters. Our results suggest that modelling the exchange
of citations between journals is useful to highlight the most prestigious
journals, but also that journal citation data are characterized by considerable
heterogeneity, which needs to be properly summarized. Inferential conclusions
require care in order to avoid potential over-interpretation of insignificant
differences between journal ratings. Comparison with published ratings of
institutions from the UK's Research Assessment Exercise shows strong
correlation at aggregate level between assessed research quality and journal
citation `export scores' within the discipline of Statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1799</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1799</id><created>2013-12-06</created><updated>2014-05-18</updated><authors><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Niu</keyname><forenames>Kai</forenames></author><author><keyname>Lin</keyname><forenames>Jiaru</forenames></author></authors><title>Space-Time Polar Coded Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polar codes are proven to be capacity-achieving and are shown to have
equivalent or even better finite-length performance than the turbo/LDPC codes
under some improved decoding algorithms over the additive white Gaussian noise
(AWGN) channels. Polar coding is based on the so-called channel polarization
phenomenon induced by a transform over the underlying binary-input channel. The
channel polarization is found to be universal in many signal processing
problems and has been applied to the coded modulation schemes. In this paper,
the channel polarization is further extended to the multiple antenna
transmission following a multilevel coding principle. The multiple-input
multile-output (MIMO) channel under quadrature amplitude modulation (QAM) are
transformed into a series of synthesized binary-input channels under a
three-stage channel transform. Based on this generalized channel polarization,
the proposed space-time polar coded modulation (STPCM) scheme allows a joint
optimization of the binary polar coding, modulation and MIMO transmission. In
addition, a practical solution of polar code construction over the fading
channels is also provided, where the fading channels are approximated by an
AWGN channel which shares the same capacity with the original. The simulations
over the MIMO channel with uncorrelated Rayleigh fast fading show that the
proposed STPCM scheme can outperform the bit-interleaved turbo coded scheme in
all the simulated cases, where the latter is adopted in many existing
communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1810</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1810</id><created>2013-12-06</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Tejinder Pal</forenames></author><author><keyname>Bhargava</keyname><forenames>Ruchira</forenames></author><author><keyname>Singh</keyname><forenames>Girish Pal</forenames></author></authors><title>File System - A Component of Operating System</title><categories>cs.OS</categories><comments>5 pages, 3 figures, 1 table</comments><journal-ref>Asian Journal of Computer Science and Information Technology 2(5),
  pp. 124-128, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The file system provides the mechanism for online storage and access to file
contents, including data and programs. This paper covers the high-level details
of file systems, as well as related topics such as the disk cache, the file
system interface to the kernel, and the user-level APIs that use the features
of the file system. It will give you a thorough understanding of how a file
system works in general. The main component of the operating system is the file
system. It is used to create, manipulate, store, and retrieve data. At the
highest level, a file system is a way to manage information on a secondary
storage medium. There are so many layers under and above the file system. All
the layers are to be fully described here. This paper will give the explanatory
knowledge of the file system designers and the researchers in the area. The
complete path from the user process to secondary storage device is to be
mentioned. File system is the area where the researchers are doing lot of job
and there is always a need to do more work. The work is going on for the
efficient, secure, energy saving techniques for the file systems. As we know
that the hardware is going to be fast in performance and low-priced day by day.
The software is not built to comeback with the hardware technology. So there is
a need to do research in this area to bridge the technology gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1812</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1812</id><created>2013-12-06</created><updated>2014-04-06</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Long multiplication by instruction sequences with backward jump
  instructions</title><categories>cs.PL</categories><comments>17 pages, the preliminaries are about the same as the preliminaries
  in arXiv:1308.0219 [cs.PL] and arXiv:1312.1529 [cs.PL]; minor errors
  corrected, references added; section on indirect addressing added</comments><acm-class>F.1.1; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For each function on bit strings, its restriction to bit strings of any given
length can be computed by a finite instruction sequence that contains only
instructions to set and get the content of Boolean registers, forward jump
instructions, and a termination instruction. Backward jump instructions are not
necessary for this, but instruction sequences can be significantly shorter with
them. We take the function on bit strings that models the multiplication of
natural numbers on their representation in the binary number system to
demonstrate this by means of a concrete example. The example is reason to
discuss points concerning the halting problem and the concept of an algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1817</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1817</id><created>2013-12-06</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Tejinder Pal</forenames></author></authors><title>Java File Security System (JFSS) Evaluation Using Software Engineering
  Approaches</title><categories>cs.SE</categories><journal-ref>International Journal of Advanced Research in Computer Science &amp;
  Software Engineering, 2(1), pp. 132-137, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Java File Security System (JFSS) [1] has been developed by us. That is an
ecrypted file system. It is developed by us because there are so many file data
breaches in the past and current history and they are going to increase day by
day as the reports by DataLossDB (Open Security Foundation) organization, a
non-profit organization in US so it is. The JFSS is evaluated regarding the two
software engineering approaches. One of them is size metric that is Lines of
Code (LOC) in the software product development. Another approach is the
customer oriented namely User Satisfaction Testing methodology. Satisfying our
customers is an essential element to stay in business in modern world of global
competition. We must satisfy and even delight our customers with the value of
our software products and services to gain their loyalty and repeat business.
Customer satisfaction is therefore a primary goal of process improvement
programs as well as quality predictions of our software. With the help of User
Satisfaction Index that is calculated for many parameters regarding the
customer satisfaction. Customer Satisfaction Surveys are the best way to find
the satisfaction level of our product quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1819</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1819</id><created>2013-12-06</created><authors><author><keyname>Kolliopoulos</keyname><forenames>Stavros G.</forenames></author><author><keyname>Moysoglou</keyname><forenames>Yannis</forenames></author></authors><title>Exponential lower bounds on the size of approximate formulations in the
  natural encoding for Capacitated Facility Location</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metric capacitated facility location is a well-studied problem for which,
while constant factor approximations are known, no efficient relaxation with
constant integrality gap is known. The question whether there is such a
relaxation is among the most important open problems of approximation
algorithms \cite{ShmoysWbook}.
  In this paper we show that, if one is restricted to linear programs that use
the natural encoding for facility location, at least an exponential number of
constraints is needed to achieve a constant gap. Our proof does not assume any
special property of the relaxation such as locality or symmetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1822</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1822</id><created>2013-12-06</created><authors><author><keyname>Kahanwal</keyname><forenames>Brijender</forenames></author><author><keyname>Singh</keyname><forenames>Tejinder Pal</forenames></author></authors><title>Towards the Framework of the File Systems Performance Evaluation
  Techniques and the Taxonomy of Replay Traces</title><categories>cs.OS</categories><comments>7 pages</comments><journal-ref>International Journal of Advanced Research in Computer Science,
  2(6) pp. 224-229, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the era of High Performance Computing (HPC). There is a great demand
of the best performance evaluation techniques for the file and storage systems.
The task of evaluation is both necessary and hard. It gives in depth analysis
of the target system and that becomes the decision points for the users. That
is also helpful for the inventors or developers to find out the bottleneck in
their systems. In this paper many performance evaluation techniques are
described for file and storage system evaluation and the main stress is given
on the important one that is replay traces. A survey has been done for the
performance evaluation techniques used by the researchers and on the replay
traces. And the taxonomy of the replay traces is described. The some of the
popular replay traces are just like, Tracefs [1], //Trace [2], Replayfs [3] and
VFS Interceptor [12]. At last we have concluded all the features that must be
considered when we are going to develop the new tool for the replay traces. The
complete work of this paper shows that the storage system developers must care
about all the techniques which can be used for the performance evaluation of
the file systems. So they can develop highly efficient future file and storage
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1824</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1824</id><created>2013-12-06</created><authors><author><keyname>Musa</keyname><forenames>Sajid</forenames></author><author><keyname>Ziatdinov</keyname><forenames>Rushan</forenames></author><author><keyname>Griffiths</keyname><forenames>Carol</forenames></author></authors><title>Introduction to computer animation and its possible educational
  applications</title><categories>cs.GR cs.CY</categories><comments>25 pages, 23 figures</comments><journal-ref>New Challenges in Education. Retrospection of history of education
  to the future in the interdisciplinary dialogue among didactics of various
  school subjects. Catholic University in Ruzomberok, Slovakia: VERBUM,
  pp.177-205, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animation, which is basically a form of pictorial presentation, has become
the most prominent feature of technology-based learning environments. It refers
to simulated motion pictures showing movement of drawn objects. Recently,
educational computer animation has turned out to be one of the most elegant
tools for presenting multimedia materials for learners, and its significance in
helping to understand and remember information has greatly increased since the
advent of powerful graphics-oriented computers. In this book chapter we
introduce and discuss the history of computer animation, its well-known
fundamental principles and some educational applications. It is however still
debatable if truly educational computer animations help in learning, as the
research on whether animation aids learners' understanding of dynamic phenomena
has come up with positive, negative and neutral results. We have tried to
provide as much detailed information on computer animation as we could, and we
hope that this book chapter will be useful for students who study computer
science, computer-assisted education or some other courses connected with
contemporary education, as well as researchers who conduct their research in
the field of computer animation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1826</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1826</id><created>2013-12-06</created><authors><author><keyname>Agrawal</keyname><forenames>Manindra</forenames></author><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Korwar</keyname><forenames>Arpita</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Hitting-sets for low-distance multilinear depth-3</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The depth-$3$ model has recently gained much importance, as it has become a
stepping-stone to understanding general arithmetic circuits. Its restriction to
multilinearity has known exponential lower bounds but no nontrivial blackbox
identity tests. In this paper we take a step towards designing such
hitting-sets. We define a notion of distance for multilinear depth-$3$ circuits
(say, in $n$ variables and $k$ product gates) that measures how far are the
partitions from a mere refinement. The $1$-distance strictly subsumes the
set-multilinear model, while $n$-distance captures general multilinear
depth-$3$. We design a hitting-set in time poly($n^{\delta\log k}$) for
$\delta$-distance. Further, we give an extension of our result to models where
the distance is large (close to $n$) but it is small when restricted to certain
variables. This implies the first subexponential whitebox PIT for the sum of
constantly many set-multilinear depth-$3$ circuits.
  We also explore a new model of read-once algebraic branching programs (ROABP)
where the factor-matrices are invertible (called invertible-factor ROABP). We
design a hitting-set in time poly($\text{size}^{w^2}$) for width-$w$
invertible-factor ROABP. Further, we could do without the invertibility
restriction when $w=2$. Previously, the best result for width-$2$ ROABP was
quasi-polynomial time (Forbes-Saptharishi-Shpilka, arXiv 2013).
  The common thread in all these results is the phenomenon of low-support `rank
concentration'. We exploit the structure of these models to prove
rank-concentration after a `small shift' in the variables. Our proof techniques
are stronger than the results of Agrawal-Saha-Saxena (STOC 2013) and
Forbes-Saptharishi-Shpilka (arXiv 2013); giving us quasi-polynomial-time
hitting-sets for models where no subexponential whitebox algorithms were known
before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1830</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1830</id><created>2013-12-06</created><updated>2013-12-09</updated><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>Quantization and Greed are Good: One bit Phase Retrieval, Robustness and
  Greedy Refinements</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>fixed typos; added a plot</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of robust phase recovery. We investigate
a novel approach based on extremely quantized (one-bit) phase-less measurements
and a corresponding recovery scheme. The proposed approach has surprising
robustness and stability properties and, unlike currently available methods,
allows to efficiently perform phase recovery from measurements affected by
severe (possibly unknown) non-linear perturbations, such as distortions (e.g.
clipping). Beyond robustness, we show how our approach can be used within
greedy approaches based on alternating minimization. In particular, we propose
novel initialization schemes for the alternating minimization achieving
favorable convergence properties with improved sample complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1831</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1831</id><created>2013-12-06</created><updated>2014-03-07</updated><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Welfare Maximization and Truthfulness in Mechanism Design with Ordinal
  Preferences</title><categories>cs.GT cs.DS</categories><comments>Some typos corrected</comments><acm-class>F.2.2; G.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study mechanism design problems in the {\em ordinal setting} wherein the
preferences of agents are described by orderings over outcomes, as opposed to
specific numerical values associated with them. This setting is relevant when
agents can compare outcomes, but aren't able to evaluate precise utilities for
them. Such a situation arises in diverse contexts including voting and matching
markets.
  Our paper addresses two issues that arise in ordinal mechanism design. To
design social welfare maximizing mechanisms, one needs to be able to
quantitatively measure the welfare of an outcome which is not clear in the
ordinal setting. Second, since the impossibility results of Gibbard and
Satterthwaite~\cite{Gibbard73,Satterthwaite75} force one to move to randomized
mechanisms, one needs a more nuanced notion of truthfulness.
  We propose {\em rank approximation} as a metric for measuring the quality of
an outcome, which allows us to evaluate mechanisms based on worst-case
performance, and {\em lex-truthfulness} as a notion of truthfulness for
randomized ordinal mechanisms. Lex-truthfulness is stronger than notions
studied in the literature, and yet flexible enough to admit a rich class of
mechanisms {\em circumventing classical impossibility results}. We demonstrate
the usefulness of the above notions by devising lex-truthful mechanisms
achieving good rank-approximation factors, both in the general ordinal setting,
as well as structured settings such as {\em (one-sided) matching markets}, and
its generalizations, {\em matroid} and {\em scheduling} markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1847</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1847</id><created>2013-12-06</created><updated>2014-02-19</updated><authors><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Rolfe</keyname><forenames>Jason</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Understanding Deep Architectures using a Recursive Convolutional Network</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge in designing convolutional network models is sizing them
appropriately. Many factors are involved in these decisions, including number
of layers, feature maps, kernel sizes, etc. Complicating this further is the
fact that each of these influence not only the numbers and dimensions of the
activation units, but also the total number of parameters. In this paper we
focus on assessing the independent contributions of three of these linked
variables: The numbers of layers, feature maps, and parameters. To accomplish
this, we employ a recursive convolutional network whose weights are tied
between layers; this allows us to vary each of the three factors in a
controlled setting. We find that while increasing the numbers of layers and
parameters each have clear benefit, the number of feature maps (and hence
dimensionality of the representation) appears ancillary, and finds most of its
benefit through the introduction of more weights. Our results (i) empirically
confirm the notion that adding layers alone increases computational power,
within the context of convolutional layers, and (ii) suggest that precise
sizing of convolutional feature map dimensions is itself of little concern;
more attention should be paid to the number of parameters in these layers
instead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1858</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1858</id><created>2013-12-06</created><updated>2014-02-07</updated><authors><author><keyname>Wilson</keyname><forenames>Dominic</forenames></author><author><keyname>Kaur</keyname><forenames>Devinder</forenames></author></authors><title>How Santa Fe Ants Evolve</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Santa Fe Ant model problem has been extensively used to investigate, test
and evaluate Evolutionary Computing systems and methods over the past two
decades. There is however no literature on its program structures that are
systematically used for fitness improvement, the geometries of those structures
and their dynamics during optimization. This paper analyzes the Santa Fe Ant
Problem using a new phenotypic schema and landscape analysis based on executed
instruction sequences. For the first time we detail systematic structural
features that give high fitness and the evolutionary dynamics of such
structures. The new schema avoids variances due to introns. We develop a
phenotypic variation method that tests the new understanding of the landscape.
We also develop a modified function set that tests newly identified
synchronization constraints. We obtain favorable computational efforts compared
to those in the literature, on testing the new variation and function set on
both the Santa Fe Trail, and the more computationally demanding Los Altos
Trail. Our findings suggest that for the Santa Fe Ant problem, a perspective of
program assembly from repetition of highly fit responses to trail conditions
leads to better analysis and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1860</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1860</id><created>2013-12-06</created><authors><author><keyname>Arfaoui</keyname><forenames>Olfa</forenames></author><author><keyname>Sassi-Hidri</keyname><forenames>Minyar</forenames></author></authors><title>Flexible queries in XML native databases</title><categories>cs.IR cs.DB</categories><comments>5 Pages, 1 Figure</comments><journal-ref>International Conference on Control, Engineering &amp; Information
  Technology (CEIT), Proceedings Engineering &amp; Technology, Vol. 4, pp. 100-104,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, most of the XML native databases (DB) flexible querying systems are
based on exploiting the tree structure of their semi structured data (SSD).
However, it becomes important to test the efficiency of Formal Concept Analysis
(FCA) formalism for this type of data since it has been proved a great
performance in the field of information retrieval (IR). So, the IR in XML
databases based on FCA is mainly based on the use of the lattice structure.
Each concept of this lattice can be interpreted as a pair (response, query). In
this work, we provide a new flexible modeling of XML DB based on fuzzy FCA as a
first step towards flexible querying of SSD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1870</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1870</id><created>2013-12-06</created><updated>2014-01-20</updated><authors><author><keyname>Joung</keyname><forenames>Jingon</forenames></author><author><keyname>Chia</keyname><forenames>Yeow Khiang</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Energy-Efficient, Large-scale Distributed-Antenna System (L-DAS) for
  Multiple Users</title><categories>cs.IT math.IT</categories><comments>29 pages, 7 figures, submitted to JSTSP</comments><doi>10.1109/JSTSP.2014.2309942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale distributed-antenna system (L-DAS) with very large number of
distributed antennas, possibly up to a few hundred antennas, is considered. A
few major issues of the L-DAS, such as high latency, energy consumption,
computational complexity, and large feedback (signaling) overhead, are
identified. The potential capability of the L-DAS is illuminated in terms of an
energy efficiency (EE) throughout the paper. We firstly and generally model the
power consumption of an L-DAS, and formulate an EE maximization problem. To
tackle two crucial issues, namely the huge computational complexity and large
amount of feedback (signaling) information, we propose a channel-gain-based
antenna selection (AS) method and an interference-based user clustering (UC)
method. The original problem is then split into multiple subproblems by a
cluster, and each cluster's precoding and power control are managed in parallel
for high EE. Simulation results reveal that i) using all antennas for
zero-forcing multiuser multiple-input multiple-output (MU-MIMO) is energy
inefficient if there is nonnegligible overhead power consumption on MU-MIMO
processing, and ii) increasing the number of antennas does not necessarily
result in a high EE. Furthermore, the results validate and underpin the EE
merit of the proposed L-DAS complied with the AS, UC, precoding, and power
control by comparing with non-clustering L-DAS and colocated antenna systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1882</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1882</id><created>2013-12-06</created><authors><author><keyname>Pesenson</keyname><forenames>Isaac Z.</forenames></author></authors><title>Shannon Sampling and Parseval Frames on Compact Manifolds</title><categories>cs.IT math.FA math.IT</categories><comments>Delivered during Sampta 2013 in Jacobs University in Bremen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our article is a summary of some results for Riemannian manifolds that were
obtained in \cite{gpes}-\cite{Pesssubm}. To the best of our knowledge these are
the pioneering papers which contain the most general results about frames,
Shannon sampling, and cubature formulas on compact and non-compact Riemannian
manifolds. In particular, the paper \cite{gpes} gives an &quot;end point&quot;
construction of tight localized frames on homogeneous compact manifolds. The
paper \cite{Pessubm} is the first systematic development of localized frames on
compact domains in Euclidean spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1887</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1887</id><created>2013-12-06</created><authors><author><keyname>Lemos</keyname><forenames>Julio</forenames></author></authors><title>Constraints on the search space of argumentation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing from research on computational models of argumentation (particularly
the Carneades Argumentation System), we explore the graphical representation of
arguments in a dispute; then, comparing two different traditions on the limits
of the justification of decisions, and devising an intermediate, semi-formal,
model, we also show that it can shed light on the theory of dispute resolution.
  We conclude our paper with an observation on the usefulness of highly
constrained reasoning for Online Dispute Resolution systems. Restricting the
search space of arguments exclusively to reasons proposed by the parties
(vetoing the introduction of new arguments by the human or artificial
arbitrator) is the only way to introduce some kind of decidability -- together
with foreseeability -- in the argumentation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1889</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1889</id><created>2013-11-18</created><authors><author><keyname>Sree</keyname><forenames>P. Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author><author><keyname>N</keyname><forenames>SSSN Usha Devi</forenames></author></authors><title>FELFCNCA: Fast &amp; Efficient Log File Compression Using Non Linear
  Cellular Automata Classifier</title><categories>cs.OH</categories><comments>International Journal on Communications (IJC) Volume 1 Issue 1,
  December 2012 http://www.seipub.org/ijc</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Log Files are created for Traffic Analysis, Maintenance, Software debugging,
customer management at multiple places like System Services, User Monitoring
Applications, Network servers, database management systems which must be kept
for long periods of time. These Log files may grow to huge sizes in this
complex systems and environments. For storage and convenience log files must be
compressed. Most of the existing algorithms do not take temporal redundancy
specific Log Files into consideration. We propose a Non Linear based Classifier
which introduces a multidimensional log file compression scheme described in
eight variants, differing in complexity and attained compression ratios. The
FELFCNCA scheme introduces a transformation for log file whose compressible
output is far better than general purpose algorithms. This proposed method was
found lossless and fully automatic. It does not impose any constraint on the
size of log file
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1897</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1897</id><created>2013-12-06</created><authors><author><keyname>Gruetze</keyname><forenames>Toni</forenames></author><author><keyname>Kasneci</keyname><forenames>Gjergji</forenames></author><author><keyname>Zuo</keyname><forenames>Zhe</forenames></author><author><keyname>Naumann</keyname><forenames>Felix</forenames></author></authors><title>Bootstrapped Grouping of Results to Ambiguous Person Name Queries</title><categories>cs.IR</categories><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some of the main ranking features of today's search engines reflect result
popularity and are based on ranking models, such as PageRank, implicit feedback
aggregation, and more. While such features yield satisfactory results for a
wide range of queries, they aggravate the problem of search for ambiguous
entities: Searching for a person yields satisfactory results only if the person
we are looking for is represented by a high-ranked Web page and all required
information are contained in this page. Otherwise, the user has to either
reformulate/refine the query or manually inspect low-ranked results to find the
person in question. A possible approach to solve this problem is to cluster the
results, so that each cluster represents one of the persons occurring in the
answer set. However clustering search results has proven to be a difficult
endeavor by itself, where the clusters are typically of moderate quality.
  A wealth of useful information about persons occurs in Web 2.0 platforms,
such as LinkedIn, Wikipedia, Facebook, etc. Being human-generated, the
information on these platforms is clean, focused, and already disambiguated. We
show that when searching for ambiguous person names the information from such
platforms can be bootstrapped to group the results according to the individuals
occurring in them. We have evaluated our methods on a hand-labeled dataset of
around 5,000 Web pages retrieved from Google queries on 50 ambiguous person
names.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1904</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1904</id><created>2013-12-06</created><authors><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author></authors><title>The PageRank Problem, Multi-Agent Consensus and Web Aggregation -- A
  Systems and Control Viewpoint</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PageRank is an algorithm introduced in 1998 and used by the Google Internet
search engine. It assigns a numerical value to each element of a set of
hyperlinked documents (that is, web pages) within the World Wide Web with the
purpose of measuring the relative importance of the page. The key idea in the
algorithm is to give a higher PageRank value to web pages which are visited
often by web surfers. On its website, Google describes PageRank as follows:
``PageRank reflects our view of the importance of web pages by considering more
than 500 million variables and 2 billion terms. Pages that are considered
important receive a higher PageRank and are more likely to appear at the top of
the search results.&quot; Today PageRank is a paradigmatic problem of great interest
in various areas, such as information technology, bibliometrics, biology, and
e-commerce, where objects are often ranked in order of importance. This article
considers a distributed randomized approach based on techniques from the area
of Markov chains using a graph representation consisting of nodes and links. We
also outline connections with other problems of current interest to the systems
and control community, which include ranking of control journals, consensus of
multi-agent systems, and aggregation-based techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1909</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1909</id><created>2013-11-18</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>JaJa</keyname><forenames>Joseph</forenames></author></authors><title>From Maxout to Channel-Out: Encoding Information on Sparse Pathways</title><categories>cs.NE cs.CV cs.LG stat.ML</categories><comments>10 pages including the appendix, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by an important insight from neural science, we propose a new
framework for understanding the success of the recently proposed &quot;maxout&quot;
networks. The framework is based on encoding information on sparse pathways and
recognizing the correct pathway at inference time. Elaborating further on this
insight, we propose a novel deep network architecture, called &quot;channel-out&quot;
network, which takes a much better advantage of sparse pathway encoding. In
channel-out networks, pathways are not only formed a posteriori, but they are
also actively selected according to the inference outputs from the lower
layers. From a mathematical perspective, channel-out networks can represent a
wider class of piece-wise continuous functions, thereby endowing the network
with more expressive power than that of maxout networks. We test our
channel-out networks on several well-known image classification benchmarks,
setting new state-of-the-art performance on CIFAR-100 and STL-10, which
represent some of the &quot;harder&quot; image classification benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1913</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1913</id><created>2013-12-06</created><authors><author><keyname>Aly</keyname><forenames>Robin</forenames></author><author><keyname>Eskevich</keyname><forenames>Maria</forenames></author><author><keyname>Ordelman</keyname><forenames>Roeland</forenames></author><author><keyname>Jones</keyname><forenames>Gareth J. F.</forenames></author></authors><title>Adapting Binary Information Retrieval Evaluation Metrics for
  Segment-based Retrieval Tasks</title><categories>cs.IR</categories><comments>Explanation of evaluation measures for the linking task of the
  MediaEval Workshop 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes metrics for the evaluation of the effectiveness of
segment-based retrieval based on existing binary information retrieval metrics.
This metrics are described in the context of a task for the hyperlinking of
video segments. This evaluation approach re-uses existing evaluation measures
from the standard Cranfield evaluation paradigm. Our adaptation approach can in
principle be used with any kind of effectiveness measure that uses binary
relevance, and for other segment-baed retrieval tasks. In our video
hyperlinking setting, we use precision at a cut-off rank n and mean average
precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1915</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1915</id><created>2013-12-06</created><authors><author><keyname>Cornejo</keyname><forenames>Alejandro</forenames></author><author><keyname>Nagpal</keyname><forenames>Radhika</forenames></author></authors><title>Long-Lived Distributed Relative Localization of Robot Swarms</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of having mobile robots in a multi-robot
system maintain an estimate of the relative position and relative orientation
of near-by robots in the environment. This problem is studied in the context of
large swarms of simple robots which are capable of measuring only the distance
to near-by robots.
  We present two distributed localization algorithms with different trade-offs
between their computational complexity and their coordination requirements. The
first algorithm does not require the robots to coordinate their motion. It
relies on a non-linear least squares based strategy to allow robots to compute
the relative pose of near-by robots. The second algorithm borrows tools from
distributed computing theory to coordinate which robots must remain stationary
and which robots are allowed to move. This coordination allows the robots to
use standard trilateration techniques to compute the relative pose of near-by
robots. Both algorithms are analyzed theoretically and validated through
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1918</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1918</id><created>2013-12-06</created><updated>2015-02-11</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Yeung</keyname><forenames>Raymond W.</forenames></author></authors><title>Cut-Set Bounds for Networks with Zero-Delay Nodes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory in Aug, 2012</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, pp. 3837-3850,
  Jul, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a network, a node is said to incur a delay if its encoding of each
transmitted symbol involves only its received symbols obtained before the time
slot in which the transmitted symbol is sent (hence the transmitted symbol sent
in a time slot cannot depend on the received symbol obtained in the same time
slot). A node is said to incur no delay if its received symbol obtained in a
time slot is available for encoding its transmitted symbol sent in the same
time slot. Under the classical model, every node in a discrete memoryless
network (DMN) incurs a unit delay, and the capacity region of the DMN satisfies
the well-known cut-set outer bound. In this paper, we propose a generalized
model for the DMN where some nodes may incur no delay. Under our generalized
model, we obtain a new cut-set outer bound, which is proved to be tight for
some two-node DMN and is shown to subsume an existing cut-set bound for the
causal relay network. In addition, we establish under the generalized model
another cut-set outer bound on the positive-delay region -- the set of
achievable rate tuples under the constraint that every node incurs a delay. We
use the cut-set bound on the positive-delay region to show that for some
two-node DMN under the generalized model, the positive-delay region is strictly
smaller than the capacity region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1920</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1920</id><created>2013-12-06</created><authors><author><keyname>Ameixieira</keyname><forenames>Carlos</forenames></author><author><keyname>Cardote</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Neves</keyname><forenames>Filipe</forenames></author><author><keyname>Meireles</keyname><forenames>Rui</forenames></author><author><keyname>Sargento</keyname><forenames>Susana</forenames></author><author><keyname>Coelho</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Afonso</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Areias</keyname><forenames>Bruno</forenames></author><author><keyname>Mota</keyname><forenames>Eduardo</forenames></author><author><keyname>Costa</keyname><forenames>Rui</forenames></author><author><keyname>Matos</keyname><forenames>Ricardo</forenames></author><author><keyname>Barros</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>HarborNet: A Real-World Testbed for Vehicular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a real-world testbed for research and development in vehicular
networking that has been deployed successfully in the sea port of Leix\~oes in
Portugal. The testbed allows for cloud-based code deployment, remote network
control and distributed data collection from moving container trucks, cranes,
tow boats, patrol vessels and roadside units, thereby enabling a wide range of
experiments and performance analyses. After describing the testbed architecture
and its various modes of operation, we give concrete examples of its use and
offer insights on how to build effective testbeds for wireless networking with
moving vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1931</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1931</id><created>2013-12-06</created><updated>2014-11-29</updated><authors><author><keyname>Bian</keyname><forenames>Liheng</forenames></author><author><keyname>Suo</keyname><forenames>Jinli</forenames></author><author><keyname>Chen</keyname><forenames>Feng</forenames></author><author><keyname>Dai</keyname><forenames>Qionghai</forenames></author></authors><title>Multi-frame denoising of high speed optical coherence tomography data
  using inter-frame and intra-frame priors</title><categories>cs.CV</categories><doi>10.1117/1.JBO.20.3.036006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical coherence tomography (OCT) is an important interferometric diagnostic
technique which provides cross-sectional views of the subsurface microstructure
of biological tissues. However, the imaging quality of high-speed OCT is
limited due to the large speckle noise. To address this problem, this paper
proposes a multi-frame algorithmic method to denoise OCT volume.
Mathematically, we build an optimization model which forces the temporally
registered frames to be low rank, and the gradient in each frame to be sparse,
under logarithmic image formation and noise variance constraints. Besides, a
convex optimization algorithm based on the augmented Lagrangian method is
derived to solve the above model. The results reveal that our approach
outperforms the other methods in terms of both speckle noise suppression and
crucial detail preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1955</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1955</id><created>2013-12-06</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Chen</keyname><forenames>Jing</forenames></author><author><keyname>Kannan</keyname><forenames>Sampath</forenames></author></authors><title>Optimal Provision-After-Wait in Healthcare</title><categories>cs.GT</categories><comments>One-page abstract appears at Innovations in Theoretical Computer
  Science (ITCS), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate computational and mechanism design aspects of scarce resource
allocation, where the primary rationing mechanism is through waiting times.
Specifically we consider allocating medical treatments to a population of
patients. Each patient needs exactly one treatment, and can choose from $k$
hospitals. Hospitals have different costs, which are fully paid by a third
party ---the &quot;payer&quot;. The payer has a fixed budget $B$, and each hospital will
have its own waiting time. At equilibrium, each patient will choose his most
preferred hospital given his intrinsic preferences and the waiting times. The
payer thus computes the waiting times so that at equilibrium the budget
constraint is satisfied and the social welfare is maximized.
  We first show that the optimization problem is NP-hard, yet if the budget can
be relaxed to $(1+\epsilon)B$ for an arbitrarily small $\epsilon$, then the
optimum under budget $B$ can be approximated efficiently. Next, we study the
endogenous emergence of waiting time from the dynamics between hospitals and
patients, and show that there is no need for the payer to explicitly enforce
the optimal waiting times. Under certain conditions, all he need is to enforce
the amount of money he wants to pay to each hospital. The dynamics will always
converge to the desired waiting times in finite time.
  We then go beyond equilibrium solutions and investigate the optimization
problem over a much larger class of mechanisms containing the equilibrium ones
as special cases. With two hospitals, we show that under a natural assumption
on the patients' preference profiles, optimal welfare is in fact attained by
the randomized assignment mechanism, which allocates patients to hospitals at
random subject to the budget constraint, but avoids waiting times.
  Finally, we discuss potential policy implications of our results, as well as
follow-up directions and open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1957</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1957</id><created>2013-12-06</created><updated>2014-10-26</updated><authors><author><keyname>Bao</keyname><forenames>Wei</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Uplink Interference Analysis for Two-tier Cellular Networks with Diverse
  Users under Random Spatial Patterns</title><categories>cs.NI cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tier architecture improves the spatial reuse of radio spectrum in
cellular networks, but it introduces complicated heterogeneity in the spatial
distribution of transmitters, which brings new challenges in interference
analysis. In this work, we present a stochastic geometric model to evaluate the
uplink interference in a two-tier network considering multi-type users and base
stations. Each type of tier-1 users and tier-2 base stations are modeled as
independent homogeneous Poisson point processes, and tier-2 users are modeled
as locally non-homogeneous clustered Poisson point processes centered at tier-2
base stations. By applying a superposition-aggregation-superposition approach,
we quantify the interference at both tiers. Our model is also able to capture
the impact of two types of exclusion regions, where either tier-2 base stations
or tier-2 users are restricted in order to avoid cross-tier interference. As an
important application of this analytical model, an intensity planning scenario
is investigated, in which we aim to maximize the total income of the network
operator with respect to the intensities of tier-2 cells, under constraints on
the outage probabilities of tier-1 and tier-2 users. The result of our
interference analysis suggests that this maximization can be converted to a
standard convex optimization problem. Finally, numerical studies further
demonstrate the correctness of our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1961</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1961</id><created>2013-12-06</created><updated>2013-12-11</updated><authors><author><keyname>Bui</keyname><forenames>Marc</forenames><affiliation>CHART</affiliation></author><author><keyname>Butelle</keyname><forenames>Franck</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>A Distributed Algorithm for Constructing a Minimum Diameter Spanning
  Tree</title><categories>cs.DC cs.DS cs.NI</categories><comments>Comments: 11 pages LaTeX, 2 figures; International Journal with
  referees article; New version (full paper design): results added in Section
  2.2 and 2.2; typos removed</comments><proxy>ccsd</proxy><msc-class>68R10, 68W15, 05C85,</msc-class><acm-class>F.2.2; G.2.2; E.1; C.2.4</acm-class><journal-ref>Journal of Parallel and Distributed Computing 64, 5 (2004) 571-577</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm, which solves the problem of distributively
finding a minimum diameter spanning tree of any (non-negatively) real-weighted
graph $G = (V,E,\omega)$. As an intermediate step, we use a new, fast,
linear-time all-pairs shortest paths distributed algorithm to find an absolute
center of $G$. The resulting distributed algorithm is asynchronous, it works
for named asynchronous arbitrary networks and achieves $\mathcal{O}(|V|)$ time
complexity and $\mathcal{O}\left(|V|\,|E|\right)$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1969</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1969</id><created>2013-12-06</created><authors><author><keyname>Cortes</keyname><forenames>Jordi M.</forenames></author><author><keyname>Nizamani</keyname><forenames>Sarwat</forenames></author><author><keyname>Memon</keyname><forenames>Nasrullah</forenames></author></authors><title>PSN: Portfolio Social Network</title><categories>cs.SI</categories><journal-ref>IJCEE 2014 Vol.6 (1): 12-15 ISSN: 1793-8163</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present a web-based information system which is a portfolio
social network (PSN) that provides solutions to recruiters and job seekers. The
proposed system enables users to create portfolios so that he/she can add his
specializations with piece of code, if any, specifically for software
engineers, which is accessible online. The unique feature of the system is to
enable the recruiters to quickly view the prominent skills of the users. A
comparative analysis of the proposed system with the state of the art systems
is presented. The comparative study reveals that the proposed system has
advanced functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1971</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1971</id><created>2013-12-06</created><authors><author><keyname>Nizamani</keyname><forenames>Sarwat</forenames></author><author><keyname>Memon</keyname><forenames>Nasrullah</forenames></author><author><keyname>Wiil</keyname><forenames>Uffe Kock</forenames></author><author><keyname>Karampelas</keyname><forenames>Panagiotis</forenames></author></authors><title>Modeling Suspicious Email Detection using Enhanced Feature Selection</title><categories>cs.AI</categories><journal-ref>IJMO 2012 Vol.2(4): 371-377 ISSN: 2010-3697</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper presents a suspicious email detection model which incorporates
enhanced feature selection. In the paper we proposed the use of feature
selection strategies along with classification technique for terrorists email
detection. The presented model focuses on the evaluation of machine learning
algorithms such as decision tree (ID3), logistic regression, Na\&quot;ive Bayes
(NB), and Support Vector Machine (SVM) for detecting emails containing
suspicious content. In the literature, various algorithms achieved good
accuracy for the desired task. However, the results achieved by those
algorithms can be further improved by using appropriate feature selection
mechanisms. We have identified the use of a specific feature selection scheme
that improves the performance of the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1973</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1973</id><created>2013-11-25</created><authors><author><keyname>Maggi</keyname><forenames>Lorenzo</forenames></author><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author></authors><title>Not Always Sparse: Flooding Time in Partially Connected Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study mobile ad hoc wireless networks using the notion of
evolving connectivity graphs. In such systems, the connectivity changes over
time due to the intermittent contacts of mobile terminals. In particular, we
are interested in studying the expected flooding time when full connectivity
cannot be ensured at each point in time. Even in this case, due to finite
contact times durations, connected components may appear in the connectivity
graph. Hence, this represents the intermediate case between extreme cases of
fully mobile ad hoc networks and fully static ad hoc networks. By using a
generalization of edge-Markovian graphs, we extend the existing models based on
sparse scenarios to this intermediate case and calculate the expected flooding
time. We also propose bounds that have reduced computational complexity.
Finally, numerical results validate our models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1983</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1983</id><created>2013-12-06</created><updated>2014-08-11</updated><authors><author><keyname>Livnat</keyname><forenames>Adi</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>Satisfiability and Evolution</title><categories>cs.CC q-bio.PE</categories><msc-class>92D15</msc-class><acm-class>F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, if truth assignments on $n$ variables reproduce through
recombination so that satisfaction of a particular Boolean function confers a
small evolutionary advantage, then a polynomially large population over
polynomially many generations (polynomial in $n$ and the inverse of the initial
satisfaction probability) will end up almost certainly consisting exclusively
of satisfying truth assignments. We argue that this theorem sheds light on the
problem of novelty in Evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1986</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1986</id><created>2013-12-06</created><updated>2015-12-10</updated><authors><author><keyname>Lee</keyname><forenames>Christina E.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Approximating the Stationary Probability of a Single State in a Markov
  chain</title><categories>cs.DS cs.SI</categories><comments>A short version appeared in NIPS Conference Dec 2013</comments><report-no>MIT LIDS Report 2914</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel iterative Monte Carlo method for
approximating the stationary probability of a single state of a positive
recurrent Markov chain. We utilize the characterization that the stationary
probability of a state $i$ is inversely proportional to the expected return
time of a random walk beginning at $i$. Our method obtains an
$\epsilon$-multiplicative close estimate with probability greater than $1 -
\alpha$ using at most $\tilde{O}\left(t_{\text{mix}} \ln(1/\alpha) / \pi_i
\epsilon^2 \right)$ simulated random walk steps on the Markov chain across all
iterations, where $t_{\text{mix}}$ is the standard mixing time and $\pi_i$ is
the stationary probability. In addition, the estimate at each iteration is
guaranteed to be an upper bound with high probability, and is decreasing in
expectation with the iteration count, allowing us to monitor the progress of
the algorithm and design effective termination criteria. We propose a
termination criteria which guarantees a $\epsilon (1 + 4 \ln(2)
t_{\text{mix}})$ multiplicative error performance for states with stationary
probability larger than $\Delta$, while providing an additive error for states
with stationary probability less than $\Delta \in (0,1)$. The algorithm along
with this termination criteria uses at most
$\tilde{O}\left(\frac{\ln(1/\alpha)}{\epsilon^2}
\min\left(\frac{t_{\text{mix}}}{\pi_i}, \frac{1}{\epsilon
\Delta}\right)\right)$ simulated random walk steps, which is bounded by a
constant with respect to the Markov Chain. We provide a tight analysis of our
algorithm based on a locally weighted variant of the mixing time. Our results
naturally extend for countably infinite state space Markov chains via Lyapunov
function analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.1993</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.1993</id><created>2013-12-06</created><updated>2014-09-04</updated><authors><author><keyname>Stippinger</keyname><forenames>Marcell</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Enhancing resilience of interdependent networks by healing</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>6 pages, 6 figures. Acknowledgements: This work was partially
  supported by the European Union and the European Social Fund through project
  FuturICT.hu (Grant No.: TAMOP-4.2.2.C-11/1/KONV-2012-0013). JK thanks
  MULTIPLEX, Grant No. 317532. Thanks are due to \'Eva R\'acz for her help at
  the early stage of this work and to Michael Danziger for a critical reading
  of the manuscript</comments><doi>10.1016/j.physa.2014.08.069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interdependent networks are characterized by two kinds of interactions: The
usual connectivity links within each network and the dependency links coupling
nodes of different networks. Due to the latter links such networks are known to
suffer from cascading failures and catastrophic breakdowns. When modeling these
phenomena, usually one assumes that a fraction of nodes gets damaged in one of
the networks, which is followed possibly by a cascade of failures. In real life
the initiating failures do not occur at once and effort is made replace the
ties eliminated due to the failing nodes. Here we study a dynamic extension of
the model of interdependent networks and introduce the possibility of link
formation with a probability w, called healing, to bridge non-functioning nodes
and enhance network resilience. A single random node is removed, which may
initiate an avalanche. After each removal step healing sets in resulting in a
new topology. Then a new node fails and the process continues until the giant
component disappears either in a catastrophic breakdown or in a smooth
transition. Simulation results are presented for square lattices as starting
networks under random attacks of constant intensity. We find that the shift in
the position of the breakdown has a power-law scaling as a function of the
healing probability with an exponent close to 1. Below a critical healing
probability, catastrophic cascades form and the average degree of surviving
nodes decreases monotonically, while above this value there are no macroscopic
cascades and the average degree has first an increasing character and decreases
only at the very late stage of the process. These findings facilitate to plan
intervention in case of crisis situation by describing the efficiency of
healing efforts needed to suppress cascading failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2018</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2018</id><created>2013-12-06</created><updated>2013-12-10</updated><authors><author><keyname>Arge</keyname><forenames>Lars</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>RAM-Efficient External Memory Sorting</title><categories>cs.DS</categories><comments>To appear in Proceedings of ISAAC 2013, getting the Best Paper Award</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years a large number of problems have been considered in external
memory models of computation, where the complexity measure is the number of
blocks of data that are moved between slow external memory and fast internal
memory (also called I/Os). In practice, however, internal memory time often
dominates the total running time once I/O-efficiency has been obtained. In this
paper we study algorithms for fundamental problems that are simultaneously
I/O-efficient and internal memory efficient in the RAM model of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2030</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2030</id><created>2013-12-06</created><authors><author><keyname>Zhou</keyname><forenames>Hui</forenames></author><author><keyname>Hu</keyname><forenames>Donglin</forenames></author><author><keyname>Reddy</keyname><forenames>Saketh Anuma</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author><author><keyname>Agrawal</keyname><forenames>Prathima</forenames></author></authors><title>On Cell Association and Scheduling Policies in Femtocell Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocells are recognized effective for improving network coverage and
capacity, and reducing power consumption due to the reduced range of wireless
transmissions. Although highly appealing, a plethora of challenging problems
need to be addressed for fully harvesting its potential. In this paper, we
investigate the problem of cell association and service scheduling in femtocell
networks. In addition to the general goal of offloading macro base station
(MBS) traffic, we also aim to minimize the latency of service requested by
users, while considering both open and closed access strategies. We show the
cell association problem is NP-hard, and propose several near-optimal solution
algorithms for assigning users to base stations (BS), including a sequential
fixing algorithm, a rounding approximation algorithm, a greedy approximation
algorithm, and a randomized algorithm. For the service scheduling problem, we
develop an optimal algorithm to minimize the average waiting time for the users
associated with the same BS. The proposed algorithms are analyzed with respect
to performance bounds, approximation ratios, and optimality, and are evaluated
with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2039</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2039</id><created>2013-12-06</created><authors><author><keyname>Zois</keyname><forenames>Daphney-Stavroula</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Active Classification for POMDPs: a Kalman-like State Estimator</title><categories>cs.SY math.OC</categories><comments>38 pages, 6 figures</comments><doi>10.1109/TSP.2014.2362098</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of state tracking with active observation control is considered
for a system modeled by a discrete-time, finite-state Markov chain observed
through conditionally Gaussian measurement vectors. The measurement model
statistics are shaped by the underlying state and an exogenous control input,
which influence the observations' quality. Exploiting an innovations approach,
an approximate minimum mean-squared error (MMSE) filter is derived to estimate
the Markov chain system state. To optimize the control strategy, the associated
mean-squared error is used as an optimization criterion in a partially
observable Markov decision process formulation. A stochastic dynamic
programming algorithm is proposed to solve for the optimal solution. To enhance
the quality of system state estimates, approximate MMSE smoothing estimators
are also derived. Finally, the performance of the proposed framework is
illustrated on the problem of physical activity detection in wireless body
sensing networks. The power of the proposed framework lies within its ability
to accommodate a broad spectrum of active classification applications including
sensor management for object classification and tracking, estimation of sparse
signals and radar scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2045</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2045</id><created>2013-12-06</created><updated>2014-05-24</updated><authors><author><keyname>Adhikary</keyname><forenames>Ansuman</forenames></author><author><keyname>Safadi</keyname><forenames>Ebrahim Al</forenames></author><author><keyname>Samimi</keyname><forenames>Mathew</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Joint Spatial Division and Multiplexing for mm-Wave Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in &quot;JSAC Special Issue in 5G Communication
  Systems&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO systems are well-suited for mm-Wave communications, as large
arrays can be built with reasonable form factors, and the high array gains
enable reasonable coverage even for outdoor communications. One of the main
obstacles for using such systems in frequency-division duplex mode, namely the
high overhead for the feedback of channel state information (CSI) to the
transmitter, can be mitigated by the recently proposed JSDM (Joint Spatial
Division and Multiplexing) algorithm. In this paper we analyze the performance
of this algorithm in some realistic propagation channels that take into account
the partial overlap of the angular spectra from different users, as well as the
sparsity of mm-Wave channels. We formulate the problem of user grouping for two
different objectives, namely maximizing spatial multiplexing, and maximizing
total received power, in a graph-theoretic framework. As the resulting problems
are numerically difficult, we proposed (sub optimum) greedy algorithms as
efficient solution methods. Numerical examples show that the different
algorithms may be superior in different settings.We furthermore develop a new,
&quot;degenerate&quot; version of JSDM that only requires average CSI at the transmitter,
and thus greatly reduces the computational burden. Evaluations in propagation
channels obtained from ray tracing results, as well as in measured outdoor
channels show that this low-complexity version performs surprisingly well in
mm-Wave channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2047</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2047</id><created>2013-12-06</created><authors><author><keyname>Mekki</keyname><forenames>Taher</forenames></author><author><keyname>Triki</keyname><forenames>Slim</forenames></author><author><keyname>Kamoun</keyname><forenames>Anas</forenames></author></authors><title>Diagnosis of Switching Systems using Hybrid Bond Graph</title><categories>cs.SY</categories><comments>8 pages</comments><journal-ref>International Journal of Computer Science Issues - IJCSI Volume
  10, Issue 1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid Bond Graph (HBG) is a Bond Graph-based modelling approach which
provides an effective tool not only for dynamic modeling but also for fault
detection and isolation (FDI) of switching systems. Bond graph (BG) has been
proven useful for FDI for continuous systems. In addition, BG provides the
causal relations between systems variables which allow FDI algorithms to be
developed systematically from the graph. There are many methods that exploit
structural relations and functional redundancy in the system model to find
efficient solutions for the residual generation and residual evaluation steps
in FDI of switching systems. This paper describes two different techniques,
quantitative and qualitative, based on common modelling approach that employs
HBG. In quantitative approach, global analytical redundancy relationships
(GARRs) are derived from the HBG model with a specified causality assignment
procedure. GARRs describe the system behaviour at all of its operating modes.
In qualitative approach, functional redundancy can be captured by a Temporal
Causal Graph (TCG), a directed graph that may include temporal information
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2048</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2048</id><created>2013-12-06</created><updated>2015-06-26</updated><authors><author><keyname>Hanley</keyname><forenames>Brian P.</forenames></author></authors><title>The False Premises and Promises of Bitcoin</title><categories>cs.CE q-fin.GN</categories><comments>28 pages, 6 figures. JEL: E21, E22, E42, E51, G21, G29, G28 Section
  2.6 has been broken out into a separate paper, and that unwieldy section is
  replaced by a short bit referencing that new paper titled, &quot;A zero-sum
  monetary system, interest rates, and implications.&quot;</comments><acm-class>J.4.1</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Designed to compete with fiat currencies, bitcoin proposes it is a
crypto-currency alternative. Bitcoin makes a number of false claims, including:
bitcoin can be a reserve currency for banking; hoarding equals saving, and that
we should believe bitcoin can expand by deflation to become a global
transactional currency supply. Bitcoin's developers combine technical
implementation proficiency with ignorance of currency and banking fundamentals.
This has resulted in a failed attempt to change finance. A set of
recommendations to change finance are provided in the Afterword:
Investment/venture banking for the masses; Venture banking to bring back what
investment banks once were; Open-outcry exchange for all CDS contracts;
Attempting to develop CDS type contracts on investments in startup and existing
enterprises; and Improving the connection between startup tech/ideas, business
organization and investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2052</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2052</id><created>2013-12-06</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>Power-Aware Hybrid Intrusion Detection System (PHIDS) using Cellular
  Automata in Wireless AdHoc Networks</title><categories>cs.NI</categories><journal-ref>WSEAS TRANSACTIONS on COMPUTERS,Issue 11, Volume 7, November
  2008,ISSN: 1109-2750</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adhoc wireless network with their changing topology and distributed nature
are more prone to intruders. The network monitoring functionality should be in
operation as long as the network exists with nil constraints. The efficiency of
an Intrusion detection system in the case of an adhoc network is not only
determined by its dynamicity in monitoring but also in its flexibility in
utilizing the available power in each of its nodes. In this paper we propose a
hybrid intrusion detection system, based on a power level metric for potential
adhoc hosts, which is used to determine the duration for which a particular
node can support a network monitoring node. Power aware hybrid intrusion
detection system focuses on the available power level in each of the nodes and
determines the network monitors. Power awareness in the network results in
maintaining power for network monitoring, with monitors changing often, since
it is an iterative power optimal solution to identify nodes for distributed
agent based intrusion detection. The advantage that this approach entails is
the inherent flexibility it provides, by means of considering only fewer nodes
for reestablishing network monitors. The detection of intrusions in the network
is done with the help of Cellular Automat CA. The CAs classify a packet routed
through the network either as normal or an intrusion. The use of CAs enable in
the identification of already occurred intrusions as well as new intrusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2060</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2060</id><created>2013-12-07</created><authors><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Ratliff</keyname><forenames>Lillian J.</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Blind Identification via Lifting</title><categories>cs.SY</categories><comments>Submitted to the IFAC World Congress 2014. arXiv admin note: text
  overlap with arXiv:1303.6719</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind system identification is known to be an ill-posed problem and without
further assumptions, no unique solution is at hand. In this contribution, we
are concerned with the task of identifying an ARX model from only output
measurements. We phrase this as a constrained rank minimization problem and
present a relaxed convex formulation to approximate its solution. To make the
problem well posed we assume that the sought input lies in some known linear
subspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2061</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2061</id><created>2013-12-07</created><authors><author><keyname>N</keyname><forenames>Krishna A</forenames></author><author><keyname>Prasad</keyname><forenames>B G</forenames></author></authors><title>Region and Location Based Indexing and Retrieval of MR-T2 Brain Tumor
  Images</title><categories>cs.CV cs.IR</categories><comments>10 pages</comments><journal-ref>International Journal of Information Processing, 7(3):16-25, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, region based and location based retrieval systems have been
implemented for retrieval of MR-T2 axial 2-D brain images. This is done by
extracting and characterizing the tumor portion of 2-D brain slices by use of a
suitable threshold computed over the entire image. Indexing and retrieval is
then performed by computing texture features based on gray-tone
spatial-dependence matrix of segmented regions. A Hash structure is used to
index all images. A combined index is adopted to point to all similar images in
terms of the texture features. At query time, only those images that are in the
same hash bucket as those of the queried image are compared for similarity,
thus reducing the search space and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2062</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2062</id><created>2013-12-07</created><authors><author><keyname>Sensarma</keyname><forenames>Debajit</forenames></author><author><keyname>Majumder</keyname><forenames>Koushik</forenames></author></authors><title>A Novel Hierarchical Ant based QoS aware Intelligent Routing Scheme for
  MANETS</title><categories>cs.NI cs.AI</categories><comments>15 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1308.2762</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.6, November 2013</journal-ref><doi>10.5121/ijcnc.2013.5614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MANET is a collection of mobile devices with no centralized control and no
pre-existing infrastructures. Due to the nodal mobility, supporting QoS during
routing in this type of networks is a very challenging task. To tackle this
type of overhead many routing algorithms with clustering approach have been
proposed. Clustering is an effective method for resource management regarding
network performance, routing protocol design, QoS etc. Most of the flat network
architecture contains homogeneous capacity of nodes but in real time nodes are
with heterogeneous capacity and transmission power. Hierarchical routing
provides routing through this kind of heterogeneous nodes. Here, routes can be
recorded hierarchically, across clusters to increase routing flexibility.
Besides this, it increases scalability and robustness of routes. In this paper,
a novel ant based QoS aware routing is proposed on a three level hierarchical
cluster based topology in MANET which will be more scalable and efficient
compared to flat architecture and will give better throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2063</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2063</id><created>2013-12-07</created><authors><author><keyname>Ingber</keyname><forenames>Amir</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>The Minimal Compression Rate for Similarity Identification</title><categories>cs.IT cs.DB cs.IR math.IT</categories><comments>45 pages, 6 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, data compression deals with the problem of concisely
representing a data source, e.g. a sequence of letters, for the purpose of
eventual reproduction (either exact or approximate). In this work we are
interested in the case where the goal is to answer similarity queries about the
compressed sequence, i.e. to identify whether or not the original sequence is
similar to a given query sequence. We study the fundamental tradeoff between
the compression rate and the reliability of the queries performed on compressed
data. For i.i.d. sequences, we characterize the minimal compression rate that
allows query answers, that are reliable in the sense of having a vanishing
false-positive probability, when false negatives are not allowed. The result is
partially based on a previous work by Ahlswede et al., and the inherently
typical subset lemma plays a key role in the converse proof. We then
characterize the compression rate achievable by schemes that use lossy source
codes as a building block, and show that such schemes are, in general,
suboptimal. Finally, we tackle the problem of evaluating the minimal
compression rate, by converting the problem to a sequence of convex programs
that can be solved efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2065</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2065</id><created>2013-12-07</created><authors><author><keyname>Sastry</keyname><forenames>S. Hanumanth</forenames></author><author><keyname>Babu</keyname><forenames>Prof. M. S. Prasada</forenames></author></authors><title>Implementation of CRISP Methodology for ERP Systems</title><categories>cs.DB</categories><comments>International Journal of Computer Science Engineering (IJCSE).
  http://www.ijcse.net/issue.php?file=vol02issue5 Volume 2 Issue 4 September
  2013. arXiv admin note: text overlap with arXiv:1211.5723 by other authors
  without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ERP systems contain huge amounts of data related to the actual execution of
business processes. These systems have a particular way of recording activities
which results in an unclear display of business processes in event logs.
Several works have been conducted on ERP systems, most of them focusing on the
development of new algorithms for the automatic discovery of business
processes. We focused on addressing issues like, how can organizations with ERP
systems apply process mining for analyzing their business processes in order to
improve them. The data handling aspect of ERP systems contrasts with those of
BPMS or workflow based systems, whose systematical storage of events
facilitates the application of process mining techniques. CRISP-DM has emerged
as the de facto standard for developing data mining and knowledge discovery
projects. Successful data mining requires three families of analytical
capabilities namely reporting, classification and forecasting. A data miner
uses more than one analytical method to get the best results. The objective of
this paper is to improve the usability and understandability of process mining
techniques, by implementing CRISP-DM methodology for their application in ERP
contexts, detailed in terms of specific implementation tools and step by step
coordination. Our study confirms that data discovery from ERP system improves
strategic and operational decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2069</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2069</id><created>2013-12-07</created><authors><author><keyname>Azimi</keyname><forenames>Ali</forenames></author><author><keyname>Kaffashpour</keyname><forenames>Azar</forenames></author></authors><title>Applying the Apriori algorithm for investigating the relationships
  between demographic characteristics of Iranian top 100 enterprises and the
  strcture of their commercial website</title><categories>cs.DB cs.CY</categories><comments>19 pages, 3 figures, 2 tables, 21 references, 2 appendix</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.6, November 2013</journal-ref><doi>10.5121/ijdkp.2013.3602</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This study was conducted with the main aim to investigate the relationships
between demographic characteristics of companies and the facilities required
for their commercial websites. The research samples are the top 100 Iranian
companies as ranked by the Iranian Industrial Management Institute; the method
applied is datamining, using Association Rules throught the Apriori algorithms.
To collect the data, an aithor-modified check list has been utilized, coverig
the three areas of faclities within commercial websites, i.e. fundamental,
information-providing, and service-delivering facilities. having extracted the
association rules between the mentioned two sets of variables, 68 rules with a
confidence rate of 90% and above were obtained, and based on their significance
were classified into two groups of must-have and should-have requirements; a
recommended package of facilities is hitherto offered to other companies which
intend to enter e-commerce through their commerical websites with regards to
each company's unique demographic characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2070</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2070</id><created>2013-12-07</created><updated>2013-12-11</updated><authors><author><keyname>Shrestha</keyname><forenames>Munik</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>A message-passing approach for threshold models of behavior in networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 6 figures</comments><doi>10.1103/PhysRevE.89.022805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple model of how social behaviors, like trends and opinions,
propagate in networks where individuals adopt the trend when they are informed
by threshold $T$ neighbors who are adopters. Using a dynamic message-passing
algorithm, we develop a tractable and computationally efficient method that
provides complete time evolution of each individual's probability of adopting
the trend or of the frequency of adopters and non-adopters in any arbitrary
networks. We validate the method by comparing it with Monte Carlo based agent
simulation in real and synthetic networks and provide an exact analytic scheme
for large random networks, where simulation results match well. Our approach is
general enough to incorporate non-Markovian processes and to include
heterogeneous thresholds and thus can be applied to explore rich sets of
complex heterogeneous agent-based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2074</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2074</id><created>2013-12-07</created><authors><author><keyname>Kumar</keyname><forenames>Ranjan</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Load Balancing using Ant Colony in Cloud Computing</title><categories>cs.DC cs.CY cs.SY</categories><comments>5 pages, 1 figure, 1 table</comments><journal-ref>International Journal of Information Technology Convergence and
  Services (IJITCS) Vol.3, No.5, Pp- 01-05, October 2013</journal-ref><doi>10.2013/VOL3/NO.5</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Ants are very small insects.They are capable to find food even they are
complete blind. The ants lives in their nest and their job is to search food
while they get hungry. We are not interested in their living style, such as how
they live, how they sleep. But we are interested in how they search for food,
and how they find the shortest path. The technique for finding the shortest
path are now applying in cloud computing. The Ant Colony approach towards Cloud
Computing gives better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2083</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2083</id><created>2013-12-07</created><authors><author><keyname>Beena</keyname><forenames>R.</forenames></author><author><keyname>Sarala</keyname><forenames>S.</forenames></author></authors><title>Code Coverage Based Test Case Selection and Prioritization</title><categories>cs.SE</categories><comments>11 pages,4 figures,10 tables</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.4, No.6, November 2013</journal-ref><doi>10.5121/ijsea.2013.4604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression Testing is exclusively executed to guarantee the desirable
functionality of existing software after pursuing quite a few amendments or
variations in it. Perhaps, it testifies the quality of the modified software by
concealing the regressions or software bugs in both functional and
non-functional applications of the system. In fact, the maintenance of test
suite is enormous as it necessitates a big investment of time and money on test
cases on a large scale. So, minimizing the test suite becomes the indispensable
requisite to lessen the budget on regression testing. Precisely, this research
paper aspires to present an innovative approach for the effective selection and
prioritization of test cases which in return may procure a maximum code
average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2086</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2086</id><created>2013-12-07</created><updated>2013-12-11</updated><authors><author><keyname>Filho</keyname><forenames>H&#xe9;lio B. Mac&#xea;do</forenames></author><author><keyname>Machado</keyname><forenames>Raphael C. S.</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina M. H.</forenames></author></authors><title>Hierarchical complexity of 2-clique-colouring weakly chordal graphs and
  perfect graphs having cliques of size at least 3</title><categories>cs.CC cs.DM math.CO</categories><comments>An extended abstract of this work was accepted for presentation at
  Latin 2014, the 11th Latin American Symposium on Theoretical Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A clique of a graph is a maximal set of vertices of size at least 2 that
induces a complete graph. A $k$-clique-colouring of a graph is a colouring of
the vertices with at most $k$ colours such that no clique is monochromatic.
D\'efossez proved that the 2-clique-colouring of perfect graphs is a
$\Sigma_2^P$-complete problem [J. Graph Theory 62 (2009) 139--156]. We
strengthen this result by showing that it is still $\Sigma_2^P$-complete for
weakly chordal graphs. We then determine a hierarchy of nested subclasses of
weakly chordal graphs whereby each graph class is in a distinct complexity
class, namely $\Sigma_2^P$-complete, $\mathcal{NP}$-complete, and
$\mathcal{P}$. We solve an open problem posed by Kratochv\'il and Tuza to
determine the complexity of 2-clique-colouring of perfect graphs with all
cliques having size at least 3 [J. Algorithms 45 (2002), 40--54], proving that
it is a $\Sigma_2^P$-complete problem. We then determine a hierarchy of nested
subclasses of perfect graphs with all cliques having size at least 3 whereby
each graph class is in a distinct complexity class, namely
$\Sigma_2^P$-complete, $\mathcal{NP}$-complete, and $\mathcal{P}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2087</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2087</id><created>2013-12-07</created><authors><author><keyname>Kirk</keyname><forenames>Nicholas H.</forenames></author></authors><title>Towards Structural Natural Language Formalization: Mapping Discourse to
  Controlled Natural Language</title><categories>cs.CL</categories><comments>The 17th Workshop on the Semantics and Pragmatics of Dialogue,
  Amsterdam, 16-18 December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author describes a conceptual study towards mapping grounded natural
language discourse representation structures to instances of controlled
language statements. This can be achieved via a pipeline of preexisting state
of the art technologies, namely natural language syntax to semantic discourse
mapping, and a reduction of the latter to controlled language discourse, given
a set of previously learnt reduction rules. Concludingly a description on
evaluation, potential and limitations for ontology-based reasoning is
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2091</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2091</id><created>2013-12-07</created><authors><author><keyname>Wang</keyname><forenames>Songyang</forenames></author><author><keyname>Ma</keyname><forenames>Ruyi</forenames></author></authors><title>Name: A Naming Mechanism for Delay/Disruption-Tolerant Network</title><categories>cs.NI</categories><comments>11 pages, 9 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.6, November 2013</journal-ref><doi>10.5121/ijcnc.2013.5615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and implementation of the naming mechanism
(NAME), a resource discovery and service location approach for
Delay/Disruption-Tolerant Network (DTN). First discuss the architecture of NAME
mainly including Name Knowledge Base, Name Dissemination, Name Resolution and
Name-based Routing. In the design and implementation of NAME, we introduce the
simple namespecifiers to describe name, the name-tree for name storage and the
efficient predicate-based routing algorithm. Future work is finally discussed
for completing NAME and providing APIs for abundant applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2094</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2094</id><created>2013-12-07</created><authors><author><keyname>Guo</keyname><forenames>Rui</forenames></author><author><keyname>Wang</keyname><forenames>Hongzhi</forenames></author><author><keyname>Chen</keyname><forenames>Mengwen</forenames></author><author><keyname>Li</keyname><forenames>Jianzhong</forenames></author><author><keyname>Gao</keyname><forenames>Hong</forenames></author></authors><title>Parallelization in Extracting Fresh Information from Online Social
  Network</title><categories>cs.SI</categories><comments>submitted to Knowledge-Based System</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Online Social Network (OSN) is one of the most hottest services in the past
years. It preserves the life of users and provides great potential for
journalists, sociologists and business analysts. Crawling data from social
network is a basic step for social network information analysis and processing.
As the network becomes huge and information on the network updates faster than
web pages, crawling is more difficult because of the limitations of band-width,
politeness etiquette and computation power. To extract fresh information from
social network efficiently and effectively, this paper presents a novel
crawling method and discusses parallelization architecture of social network.
To discover the feature of social network, we gather data from real social
network, analyze them and build a model to describe the discipline of users'
behavior. With the modeled behavior, we propose methods to predict users'
behavior. According to the prediction, we schedule our crawler more reasonably
and extract more fresh information with parallelization technologies.
Experimental results demonstrate that our strategies could obtain information
from OSN efficiently and effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2096</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2096</id><created>2013-12-07</created><authors><author><keyname>Guo</keyname><forenames>Rui</forenames></author><author><keyname>Wang</keyname><forenames>Hongzhi</forenames></author><author><keyname>Zhong</keyname><forenames>Lucheng</forenames></author><author><keyname>Li</keyname><forenames>Jianzhong</forenames></author><author><keyname>Gao</keyname><forenames>Hong</forenames></author></authors><title>Harbinger: An Analyzing and Predicting System for Online Social Network
  Users' Behavior</title><categories>cs.SI physics.soc-ph</categories><comments>submitted to DASFAA demo</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Online Social Network (OSN) is one of the hottest innovations in the past
years, and the active users are more than a billion. For OSN, users' behavior
is one of the important factors to study. This demonstration proposal presents
Harbinger, an analyzing and predicting system for OSN users' behavior. In
Harbinger, we focus on tweets' timestamps (when users post or share messages),
visualize users' post behavior as well as message retweet number and build
adjustable models to predict users' behavior. Predictions of users' behavior
can be performed with the discovered behavior models and the results can be
applied to many applications such as tweet crawler and advertisement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2098</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2098</id><created>2013-12-07</created><authors><author><keyname>Chen</keyname><forenames>Yen-Chi</forenames></author><author><keyname>Genovese</keyname><forenames>Christopher R.</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Uncertainty Measures and Limiting Distributions for Filament Estimation</title><categories>stat.ME cs.CG</categories><comments>Submitted to 30th Annual Symposium on Computational Geometry
  (SoCG2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A filament is a high density, connected region in a point cloud. There are
several methods for estimating filaments but these methods do not provide any
measure of uncertainty. We give a definition for the uncertainty of estimated
filaments and we study statistical properties of the estimated filaments. We
show how to estimate the uncertainty measures and we construct confidence sets
based on a bootstrapping technique. We apply our methods to astronomy data and
earthquake data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2108</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2108</id><created>2013-12-07</created><authors><author><keyname>Chellani</keyname><forenames>Geetanjali</forenames></author><author><keyname>Kalla</keyname><forenames>Anshuman</forenames></author></authors><title>A Review: Study of Handover Performance in Mobile IP</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2013.5608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Mobile Internet Protocol (Mobile IP) is an extension to the Internet
Protocol proposed by the Internet Engineering Task Force (IETF) that addresses
the mobility issues. In order to support un-interrupted services and seamless
mobility of nodes across the networks (and/or sub-networks) with permanent IP
addresses, handover is performed in mobile IP enabled networks. Handover in
mobile IP is source cause of performance degradation as it results in increased
latency and packet loss during handover. Other issues like scalability issues,
ordered packet delivery issues, control plane management issues etc are also
adversely affected by it. The paper provides a constructive survey by
classifying, discussing and comparing different handover techniques that have
been proposed so far, for enhancing the performance during handovers. Finally
some general solutions that have been used to solve handover related problems
are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2121</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2121</id><created>2013-12-07</created><authors><author><keyname>Benmerzoug</keyname><forenames>Djamel</forenames></author><author><keyname>Djaaboub</keyname><forenames>Salim</forenames></author><author><keyname>Mahmoudi</keyname><forenames>Hani</forenames></author></authors><title>Engineering Cooperative JADE Agents with the AMCIS Methodology: The
  Transportation Management Case Study</title><categories>cs.SE cs.MA</categories><comments>12 pages, 8 figures, CIIA'06 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses in detail important analysis and design issues emerged
during the development of an agent-based transportation e-market. This
discussion is based on concepts coming from the AMCIS methodology and the JADE
framework. The AMCIS methodology is specifically tailored to the analysis and
design of cooperative information agent-based systems, while it supports both
the levels of the individual agent structure and the agent society in the
Multi-Agents Systems (MAS) development process. According to AMCIS, MAS are
viewed as being composed of a number of autonomous cooperative agents that live
in an organized society, in which each agent plays one or more specific roles,
while their plans and interaction protocols are well defined. On the other hand
JADE is a FIPA specifications compliant agent development environment that
gives several facilities for an easy and fast implementation. Our aim is to
reveal the mapping that may exists between the basic concepts proposed by AMCIS
for agents specification and agents interactions and those provided by JADE for
agents implementation, and therefore to propose a kind of roadmap for agents
developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2132</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2132</id><created>2013-12-07</created><authors><author><keyname>Sadigh</keyname><forenames>Dorsa</forenames></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>Robust Subspace System Identification via Weighted Nuclear Norm
  Optimization</title><categories>cs.SY cs.LG stat.ML</categories><comments>Submitted to the IFAC World Congress 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace identification is a classical and very well studied problem in
system identification. The problem was recently posed as a convex optimization
problem via the nuclear norm relaxation. Inspired by robust PCA, we extend this
framework to handle outliers. The proposed framework takes the form of a convex
optimization problem with an objective that trades off fit, rank and sparsity.
As in robust PCA, it can be problematic to find a suitable regularization
parameter. We show how the space in which a suitable parameter should be sought
can be limited to a bounded open set of the two dimensional parameter space. In
practice, this is very useful since it restricts the parameter space that is
needed to be surveyed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2135</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2135</id><created>2013-12-07</created><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>A Repair Framework for Scalar MDS Codes</title><categories>cs.IT math.IT</categories><comments>10 Pages; accepted to IEEE JSAC -Distributed Storage 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several works have developed vector-linear maximum-distance separable (MDS)
storage codes that min- imize the total communication cost required to repair a
single coded symbol after an erasure, referred to as repair bandwidth (BW).
Vector codes allow communicating fewer sub-symbols per node, instead of the
entire content. This allows non trivial savings in repair BW. In sharp
contrast, classic codes, like Reed- Solomon (RS), used in current storage
systems, are deemed to suffer from naive repair, i.e. downloading the entire
stored message to repair one failed node. This mainly happens because they are
scalar-linear. In this work, we present a simple framework that treats scalar
codes as vector-linear. In some cases, this allows significant savings in
repair BW. We show that vectorized scalar codes exhibit properties that
simplify the design of repair schemes. Our framework can be seen as a finite
field analogue of real interference alignment. Using our simplified framework,
we design a scheme that we call clique-repair which provably identifies the
best linear repair strategy for any scalar 2-parity MDS code, under some
conditions on the sub-field chosen for vectorization. We specify optimal repair
schemes for specific (5,3)- and (6,4)-Reed- Solomon (RS) codes. Further, we
present a repair strategy for the RS code currently deployed in the Facebook
Analytics Hadoop cluster that leads to 20% of repair BW savings over naive
repair which is the repair scheme currently used for this code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2137</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2137</id><created>2013-12-07</created><authors><author><keyname>Palaz</keyname><forenames>Dimitri</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author><author><keyname>-Doss</keyname><forenames>Mathew Magimai.</forenames></author></authors><title>End-to-end Phoneme Sequence Recognition using Convolutional Neural
  Networks</title><categories>cs.LG cs.CL cs.NE</categories><comments>NIPS Deep Learning Workshop, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most phoneme recognition state-of-the-art systems rely on a classical neural
network classifiers, fed with highly tuned features, such as MFCC or PLP
features. Recent advances in ``deep learning'' approaches questioned such
systems, but while some attempts were made with simpler features such as
spectrograms, state-of-the-art systems still rely on MFCCs. This might be
viewed as a kind of failure from deep learning approaches, which are often
claimed to have the ability to train with raw signals, alleviating the need of
hand-crafted features. In this paper, we investigate a convolutional neural
network approach for raw speech signals. While convolutional architectures got
tremendous success in computer vision or text processing, they seem to have
been let down in the past recent years in the speech processing field. We show
that it is possible to learn an end-to-end phoneme sequence classifier system
directly from raw signal, with similar performance on the TIMIT and WSJ
datasets than existing systems based on MFCC, questioning the need of complex
hand-crafted features on large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2139</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2139</id><created>2013-12-07</created><updated>2014-08-20</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>Optimal rates for zero-order convex optimization: the power of two
  function evaluations</title><categories>math.OC cs.IT math.IT stat.ML</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider derivative-free algorithms for stochastic and non-stochastic
convex optimization problems that use only function values rather than
gradients. Focusing on non-asymptotic bounds on convergence rates, we show that
if pairs of function values are available, algorithms for $d$-dimensional
optimization that use gradient estimates based on random perturbations suffer a
factor of at most $\sqrt{d}$ in convergence rate over traditional stochastic
gradient methods. We establish such results for both smooth and non-smooth
cases, sharpening previous analyses that suggested a worse dimension
dependence, and extend our results to the case of multiple ($m \ge 2$)
evaluations. We complement our algorithmic development with
information-theoretic lower bounds on the minimax convergence rate of such
problems, establishing the sharpness of our achievable results up to constant
(sometimes logarithmic) factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2140</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2140</id><created>2013-12-07</created><authors><author><keyname>Mohammadi</keyname><forenames>Peyman</forenames></author><author><keyname>Hatamlou</keyname><forenames>Abdolreza</forenames></author><author><keyname>Masdari</keyname><forenames>Mohammad</forenames></author></authors><title>A Comparative Study on Remote Tracking of Parkinsons Disease Progression
  Using Data Mining Methods</title><categories>cs.CE cs.DB</categories><comments>13 Pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, applications of data mining methods are become more popular
in many fields of medical diagnosis and evaluations. The data mining methods
are appropriate tools for discovering and extracting of available knowledge in
medical databases. In this study, we divided 11 data mining algorithms into
five groups which are applied to a data set of patients clinical variables data
with Parkinsons Disease (PD) to study the disease progression. The data set
includes 22 properties of 42 people that all of our algorithms are applied to
this data set. The Decision Table with 0.9985 correlation coefficients has the
best accuracy and Decision Stump with 0.7919 correlation coefficients has the
lowest accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2141</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2141</id><created>2013-12-07</created><authors><author><keyname>Mehta</keyname><forenames>Jenish C.</forenames></author></authors><title>Dynamic Complexity of Planar 3-connected Graph Isomorphism</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Complexity (as introduced by Patnaik and Immerman) tries to express
how hard it is to update the solution to a problem when the input is changed
slightly. It considers the changes required to some stored data structure
(possibly a massive database) as small quantities of data (or a tuple) are
inserted or deleted from the database (or a structure over some vocabulary).
The main difference from previous notions of dynamic complexity is that instead
of treating the update quantitatively by finding the the time/space trade-offs,
it tries to consider the update qualitatively, by finding the complexity class
in which the update can be expressed (or made). In this setting, DynFO, or
Dynamic First-Order, is one of the smallest and the most natural complexity
class (since SQL queries can be expressed in First-Order Logic), and contains
those problems whose solutions (or the stored data structure from which the
solution can be found) can be updated in First-Order Logic when the data
structure undergoes small changes.
  Etessami considered the problem of isomorphism in the dynamic setting, and
showed that Tree Isomorphism can be decided in DynFO. In this work, we show
that isomorphism of Planar 3-connected graphs can be decided in DynFO+ (which
is DynFO with some polynomial precomputation). We maintain a canonical
description of 3-connected Planar graphs by maintaining a database which is
accessed and modified by First-Order queries when edges are added to or deleted
from the graph. We specifically exploit the ideas of Breadth-First Search and
Canonical Breadth-First Search to prove the results. We also introduce a novel
method for canonizing a 3-connected planar graph in First-Order Logic from
Canonical Breadth-First Search Trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2143</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2143</id><created>2013-12-07</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Sun</keyname><forenames>Xiaorui</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Zhao</keyname><forenames>Yu</forenames></author></authors><title>A composition theorem for parity kill number</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the parity complexity measures
${\mathsf{C}^{\oplus}_{\min}}[f]$ and ${\mathsf{DT^{\oplus}}}[f]$.
${\mathsf{C}^{\oplus}_{\min}}[f]$ is the \emph{parity kill number} of $f$, the
fewest number of parities on the input variables one has to fix in order to
&quot;kill&quot; $f$, i.e. to make it constant. ${\mathsf{DT^{\oplus}}}[f]$ is the depth
of the shortest \emph{parity decision tree} which computes $f$. These
complexity measures have in recent years become increasingly important in the
fields of communication complexity \cite{ZS09, MO09, ZS10, TWXZ13} and
pseudorandomness \cite{BK12, Sha11, CT13}.
  Our main result is a composition theorem for ${\mathsf{C}^{\oplus}_{\min}}$.
The $k$-th power of $f$, denoted $f^{\circ k}$, is the function which results
from composing $f$ with itself $k$ times. We prove that if $f$ is not a parity
function, then ${\mathsf{C}^{\oplus}_{\min}}[f^{\circ k}] \geq
\Omega({\mathsf{C}_{\min}}[f]^{k}).$ In other words, the parity kill number of
$f$ is essentially supermultiplicative in the \emph{normal} kill number of $f$
(also known as the minimum certificate complexity).
  As an application of our composition theorem, we show lower bounds on the
parity complexity measures of $\mathsf{Sort}^{\circ k}$ and $\mathsf{HI}^{\circ
k}$. Here $\mathsf{Sort}$ is the sort function due to Ambainis \cite{Amb06},
and $\mathsf{HI}$ is Kushilevitz's hemi-icosahedron function \cite{NW95}. In
doing so, we disprove a conjecture of Montanaro and Osborne \cite{MO09} which
had applications to communication complexity and computational learning theory.
In addition, we give new lower bounds for conjectures of \cite{MO09,ZS10} and
\cite{TWXZ13}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2154</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2154</id><created>2013-12-07</created><authors><author><keyname>Kobayashi</keyname><forenames>Tomoki</forenames></author><author><keyname>Eguchi</keyname><forenames>Koji</forenames></author></authors><title>Sequential Monte Carlo Inference of Mixed Membership Stochastic
  Blockmodels for Dynamic Social Networks</title><categories>cs.SI cs.LG stat.ML</categories><comments>NIPS 2013 Workshop on Frontiers of Network Analysis: Methods, Models,
  and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many kinds of data can be represented as a network or graph. It is crucial to
infer the latent structure underlying such a network and to predict unobserved
links in the network. Mixed Membership Stochastic Blockmodel (MMSB) is a
promising model for network data. Latent variables and unknown parameters in
MMSB have been estimated through Bayesian inference with the entire network;
however, it is important to estimate them online for evolving networks. In this
paper, we first develop online inference methods for MMSB through sequential
Monte Carlo methods, also known as particle filters. We then extend them for
time-evolving networks, taking into account the temporal dependency of the
network structure. We demonstrate through experiments that the time-dependent
particle filter outperformed several baselines in terms of prediction
performance in an online condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2159</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2159</id><created>2013-12-07</created><updated>2013-12-19</updated><authors><author><keyname>Brinton</keyname><forenames>Christopher G.</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author><author><keyname>Jain</keyname><forenames>Shaili</forenames></author><author><keyname>Lam</keyname><forenames>Henry</forenames></author><author><keyname>Liu</keyname><forenames>Zhenming</forenames></author><author><keyname>Wong</keyname><forenames>Felix Ming Fai</forenames></author></authors><title>Learning about social learning in MOOCs: From statistical analysis to
  generative model</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study user behavior in the courses offered by a major Massive Online Open
Course (MOOC) provider during the summer of 2013. Since social learning is a
key element of scalable education in MOOCs and is done via online discussion
forums, our main focus is in understanding forum activities. Two salient
features of MOOC forum activities drive our research: 1. High decline rate: for
all courses studied, the volume of discussions in the forum declines
continuously throughout the duration of the course. 2. High-volume, noisy
discussions: at least 30% of the courses produce new discussion threads at
rates that are infeasible for students or teaching staff to read through.
Furthermore, a substantial portion of the discussions are not directly
course-related.
  We investigate factors that correlate with the decline of activity in the
online discussion forums and find effective strategies to classify threads and
rank their relevance. Specifically, we use linear regression models to analyze
the time series of the count data for the forum activities and make a number of
observations, e.g., the teaching staff's active participation in the discussion
increases the discussion volume but does not slow down the decline rate. We
then propose a unified generative model for the discussion threads, which
allows us both to choose efficient thread classifiers and design an effective
algorithm for ranking thread relevance. Our ranking algorithm is further
compared against two baseline algorithms, using human evaluation from Amazon
Mechanical Turk.
  The authors on this paper are listed in alphabetical order. For media and
press coverage, please refer to us collectively, as &quot;researchers from the EDGE
Lab at Princeton University, together with collaborators at Boston University
and Microsoft Corporation.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2163</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2163</id><created>2013-12-07</created><authors><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Multipermutation Codes in the Ulam Metric for Nonvolatile Memories</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of multipermutation code design in the Ulam metric for
novel storage applications. Multipermutation codes are suitable for flash
memory where cell charges may share the same rank. Changes in the charges of
cells manifest themselves as errors whose effects on the retrieved signal may
be measured via the Ulam distance. As part of our analysis, we study
multipermutation codes in the Hamming metric, known as constant composition
codes. We then present bounds on the size of multipermutation codes and their
capacity, for both the Ulam and the Hamming metrics. Finally, we present
constructions and accompanying decoders for multipermutation codes in the Ulam
metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2164</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2164</id><created>2013-12-07</created><updated>2014-04-15</updated><authors><author><keyname>Du</keyname><forenames>Nan</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Budgeted Influence Maximization for Multiple Products</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The typical algorithmic problem in viral marketing aims to identify a set of
influential users in a social network, who, when convinced to adopt a product,
shall influence other users in the network and trigger a large cascade of
adoptions. However, the host (the owner of an online social platform) often
faces more constraints than a single product, endless user attentions,
unlimited budget and unbounded time; in reality, multiple products need to be
advertised, each user can tolerate only a small number of recommendations,
influencing user has a cost and advertisers have only limited budgets, and the
adoptions need to be maximized within a short time window.
  Given theses myriads of user, monetary, and timing constraints, it is
extremely challenging for the host to design principled and efficient viral
market algorithms with provable guarantees. In this paper, we provide a novel
solution by formulating the problem as a submodular maximization in a
continuous-time diffusion model under an intersection of a matroid and multiple
knapsack constraints. We also propose an adaptive threshold greedy algorithm
which can be faster than the traditional greedy algorithm with lazy evaluation,
and scalable to networks with million of nodes. Furthermore, our mathematical
formulation allows us to prove that the algorithm can achieve an approximation
factor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are
active, which also improves over previous guarantees from combinatorial
optimization literature. In the case when influencing each user has uniform
cost, the approximation becomes even better to a factor of $1/3$. Extensive
synthetic and real world experiments demonstrate that our budgeted influence
maximization algorithm achieves the-state-of-the-art in terms of both
effectiveness and scalability, often beating the next best by significant
margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2169</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2169</id><created>2013-12-07</created><updated>2014-03-29</updated><authors><author><keyname>Haija</keyname><forenames>Ahmad Abu Al</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Spectral Efficiency and Outage Performance for Hybrid D2D-Infrastructure
  Uplink Cooperation</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE Trans. Wireless Commun. for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a time-division uplink transmission scheme that is applicable to
future cellular systems by introducing hybrid device-to-device (D2D) and
infrastructure cooperation. We analyze its spectral efficiency and outage
performance and show that compared to existing frequency-division schemes, the
proposed scheme achieves the same or better spectral efficiency and outage
performance while having simpler signaling and shorter decoding delay. Using
time-division, the proposed scheme divides each transmission frame into three
phases with variable durations. The two user equipments (UEs) partially
exchange their information in the first two phases, then cooperatively transmit
to the base station (BS) in the third phase. We further formulate its common
and individual outage probabilities, taking into account outages at both UEs
and the BS. We analyze this outage performance in Rayleigh fading environment
assuming full channel state information (CSI) at the receivers and limited CSI
at the transmitters. Results show that comparing to non-cooperative
transmission, the proposed cooperation always improves the instantaneous
achievable rate region even under half-duplex transmission. Moreover, as the
received signal-to-noise ratio increases, this uplink cooperation significantly
reduces overall outage probabilities and achieves the full diversity order in
spite of additional outages at the UEs. These characteristics of the proposed
uplink cooperation make it appealing for deployment in future cellular
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2171</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2171</id><created>2013-12-07</created><updated>2014-11-24</updated><authors><author><keyname>Kapelner</keyname><forenames>Adam</forenames></author><author><keyname>Bleich</keyname><forenames>Justin</forenames></author></authors><title>bartMachine: Machine Learning with Bayesian Additive Regression Trees</title><categories>stat.ML cs.LG</categories><comments>39 pages, 13 figures, 4 tables, 2 appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new package in R implementing Bayesian additive regression trees
(BART). The package introduces many new features for data analysis using BART
such as variable selection, interaction detection, model diagnostic plots,
incorporation of missing data and the ability to save trees for future
prediction. It is significantly faster than the current R implementation,
parallelized, and capable of handling both large sample sizes and
high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2173</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2173</id><created>2013-12-07</created><updated>2014-04-24</updated><authors><author><keyname>Huang</keyname><forenames>Norman</forenames></author><author><keyname>Borodin</keyname><forenames>Allan</forenames></author></authors><title>Bounds on Double-Sided Myopic Algorithms for Unconstrained Non-monotone
  Submodular Maximization</title><categories>cs.DS</categories><comments>29 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unconstrained submodular maximization captures many NP-hard combinatorial
optimization problems, including Max-Cut, Max-Di-Cut, and variants of facility
location problems. Recently, Buchbinder et al. presented a surprisingly simple
linear time randomized greedy-like online algorithm that achieves a constant
approximation ratio of 1/2, matching optimally the hardness result of Feige et
al.. Motivated by the algorithm of Buchbinder et al., we introduce a precise
algorithmic model called double-sided myopic algorithms. We show that while the
algorithm of Buchbinder et al. can be realized as a randomized online
double-sided myopic algorithm, no such deterministic algorithm, even with
adaptive ordering, can achieve the same approximation ratio. With respect to
the Max-Di-Cut problem, we relate the Buchbinder et al. algorithm and our
myopic framework to the online algorithm and inapproximation of Bar-Noy and
Lampis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2175</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2175</id><created>2013-12-07</created><updated>2013-12-10</updated><authors><author><keyname>Roy</keyname><forenames>Bibhas</forenames></author><author><keyname>Banik</keyname><forenames>Suman</forenames></author><author><keyname>Dey</keyname><forenames>Parthi</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Chaki</keyname><forenames>Nabendu</forenames></author></authors><title>Ant Colony based Routing for Mobile Ad-Hoc Networks towards Improved
  Quality of Services</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad Hoc Network (MANET) is a dynamic multihop wireless network which is
established by a set of mobile nodes on a shared wireless channel. One of the
major issues in MANET is routing due to the mobility of the nodes. Routing
means the act of moving information across an internet work from a source to a
destination. When it comes to MANET, the complexity increases due to various
characteristics like dynamic topology, time varying QoS requirements, limited
resources and energy etc. QoS routing plays an important role for providing QoS
in wireless ad hoc networks. The most complex issue in this kind of networks is
to find a path between the communication end points satisfying QoS requirement
for the user. Nature-inspired algorithms (swarm intelligence) such as ant
colony optimization ACO)algorithms have shown to be a good technique for
developing routing algorithms for MANETs.
  In this paper, a new QoS algorithm for mobile ad hoc network has been
proposed. The proposed algorithm combines the idea of Ant Colony Optimization
(ACO) with Optimized Link State Routing (OLSR) protocol to identify multiple
stable paths between source and destination nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2177</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2177</id><created>2013-12-08</created><updated>2015-05-09</updated><authors><author><keyname>Zamani</keyname><forenames>Mahdi</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author></authors><title>Machine Learning Techniques for Intrusion Detection</title><categories>cs.CR cs.LG cs.NI</categories><comments>11 pages</comments><report-no>UNM-48066-F13</report-no><acm-class>C.2.0; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Intrusion Detection System (IDS) is a software that monitors a single or a
network of computers for malicious activities (attacks) that are aimed at
stealing or censoring information or corrupting network protocols. Most
techniques used in today's IDS are not able to deal with the dynamic and
complex nature of cyber attacks on computer networks. Hence, efficient adaptive
methods like various techniques of machine learning can result in higher
detection rates, lower false alarm rates and reasonable computation and
communication costs. In this paper, we study several such schemes and compare
their performance. We divide the schemes into methods based on classical
artificial intelligence (AI) and methods based on computational intelligence
(CI). We explain how various characteristics of CI techniques can be used to
build efficient IDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2182</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2182</id><created>2013-12-08</created><authors><author><keyname>Mayer</keyname><forenames>R. V.</forenames></author></authors><title>Solve of problems of mathematical theory of learning with using computer
  modeling methods</title><categories>cs.OH cs.CY</categories><comments>6 pages, 3 figures, Perspestive of Science and Education, 2013, N 5,
  pp. 95-100 (In Russian)</comments><msc-class>68U20</msc-class><acm-class>I.6.0</acm-class><journal-ref>Perspestive of Science and Education, 2013, N 5, pp. 95-100</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzed models of learning, which take into account that: 1) the rate of
increase of student's knowledge is proportional to the difference between
levels of teacher's requirements and prior knowledge; 2) if the requirements
are too high, then student motivation decreases and he stops learning. Was
proposed: 1) a one component model, coming from the fact that the training
information consists of equal elements; 2) a two component model that takes
into account that knowledge is assimilated with varying strength, 'trustworthy'
knowledge forgotten much slower then 'weak'; 3) two component model, which
takes into account the transition of 'weak' knowledge in 'trustworthy'
knowledge. The solution of the five predictors and optimization problems of
learning theory are represented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2183</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2183</id><created>2013-12-08</created><authors><author><keyname>Zhu</keyname><forenames>Jiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Maximum Likelihood Estimation from Sign Measurements with Sensing Matrix
  Perturbation</title><categories>cs.IT math.IT</categories><comments>29 pages, 9 figures, journal paper</comments><doi>10.1109/TSP.2014.2330350</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating an unknown deterministic parameter vector from sign
measurements with a perturbed sensing matrix is studied in this paper. We
analyze the best achievable mean square error (MSE) performance by exploring
the corresponding Cram\'{e}r-Rao Lower Bound (CRLB). To estimate the parameter,
the maximum likelihood (ML) estimator is utilized and its consistency is
proved. We show that the perturbation on the sensing matrix exacerbates the
performance of ML estimator in most cases. However, suitable perturbation may
improve the performance in some special cases. Then we reformulate the original
ML estimation problem as a convex optimization problem, which can be solved
efficiently. Furthermore, theoretical analysis implies that the
perturbation-ignored estimation is a scaled version with the same direction of
the ML estimation. Finally, numerical simulations are performed to validate our
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2185</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2185</id><created>2013-12-08</created><authors><author><keyname>Khopkar</keyname><forenames>Abhijeet</forenames></author><author><keyname>Govindrajan</keyname><forenames>Sathish</forenames></author></authors><title>Geometric graphs on convex point sets</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we introduce a family of bipartite graphs called path
restricted ordered bipartite graphs and present it as an abstract
generalization of some well known geometric graphs like unit distance graphs on
convex point sets. In the framework of convex point sets, we also focus on a
generalized version of Gabriel graphs known as locally Gabriel graphs or
$LGGs$. $LGGs$ can also be seen as the generalization of unit distance graphs.
The path restricted ordered bipartite graph is also a generalization of $LGGs$.
We study some structural properties of the path restricted ordered bipartite
graphs and also show that such graphs have the maximum edge complexity of
$\theta(n \log n)$. It gives an alternate proof to the well known result that
$UDGs$ and $LGGs$ on convex points have $O(n \log n)$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2188</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2188</id><created>2013-12-08</created><authors><author><keyname>Foukalas</keyname><forenames>F.</forenames></author><author><keyname>Karetsos</keyname><forenames>G. T.</forenames></author><author><keyname>Chatzimisios</keyname><forenames>P.</forenames></author></authors><title>Cross-layer Design of CSMA/CA with Spectrum Sensing for Cognitive Radio
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a cross-layer design (CLD) of carrier sensing multiple access with
collision avoidance (CSMA/CA) at the medium access control (MAC) layer with
spectrum sensing (SpSe) at the physical layer for cognitive radio networks
(CRNs). The proposed CLD relies on a Markov chain model with a state pair
containing both the SpSe and the CSMA/CA with exponential backoff from which we
derive the transmission and collision probabilities. Due to the 2-dimensions of
CSMA/CA model with exponential backoff, the resulted Markov chain is obtained
with 3-dimensions. Simulation and numerical results are derived and illustrated
highlighting the impact of SpSe in CSMA/CA with exponential backoff. The
obtained results could be used as performance criteria to evaluate the
performance of specific CRNs when they are deployed in a distributed
coordination fashion that is prone to collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2194</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2194</id><created>2013-12-08</created><authors><author><keyname>Rubin</keyname><forenames>Natan</forenames></author></authors><title>On Kinetic Delaunay Triangulations: A Near Quadratic Bound for Unit
  Speed Motions</title><categories>cs.CG cs.DS math.CO</categories><comments>138 pages+ Appendix of 7 pages. A preliminary version has appeared in
  Proceedings of the 54th Annual Symposium on Foundations of Computer Science
  (FOCS 2013). The paper extends the result of http://arxiv.org/abs/1304.3671
  to more general motions. The presentation is self-contained with main ideas
  delivered in Sections 1--4</comments><msc-class>52C45</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a collection of $n$ points in the plane, each moving along some
straight line at unit speed. We obtain an almost tight upper bound of
$O(n^{2+\epsilon})$, for any $\epsilon&gt;0$, on the maximum number of discrete
changes that the Delaunay triangulation $\mathbb{DT}(P)$ of $P$ experiences
during this motion. Our analysis is cast in a purely topological setting, where
we only assume that (i) any four points can be co-circular at most three times,
and (ii) no triple of points can be collinear more than twice; these
assumptions hold for unit speed motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2203</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2203</id><created>2013-12-08</created><authors><author><keyname>Nie</keyname><forenames>Kai</forenames></author><author><keyname>Yu</keyname><forenames>Man</forenames></author></authors><title>Research on fresh agriculture product based on overconfidence of the
  retailer under options and spot markets dominated</title><categories>cs.CE q-fin.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we analyze the application of options contract in special
commodity supply chain such as fresh agricultural products. This problem is
discussed in the point of the retailer. When spot market and future market are
both available, we discuss how the retailer chooses the optimal production.
Furthermore, overconfidence is introduced to the supply chain of the fresh
agricultural products, which has not happened before. Then,based on the
overconfidence of the retailer, we explore how overconfidence affects the
supply chain system under different circumstances. At last, we get the
conclusion that different overconfidence level has different affection on
retailer's optimal ordering quantity and profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2207</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2207</id><created>2013-12-08</created><updated>2014-08-08</updated><authors><author><keyname>Mittal</keyname><forenames>Sparsh</forenames></author></authors><title>A Cache Energy Optimization Technique for STT-RAM Last Level Cache</title><categories>cs.AR</categories><comments>This paper has been withdrawn by the author for revising experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Last level caches (LLCs) occupy a large chip-area and there size is expected
to grow further to offset the limitations of memory bandwidth and speed. Due to
high leakage consumption of SRAM device, caches designed with SRAM consume
large amount of energy. To address this, use of emerging technologies such as
spin torque transfer RAM (STT-RAM) has been investigated which have lower
leakage power dissipation. However, the high write latency and power of it may
lead to large energy consumption which present challenges in its use. In this
report, we propose a cache reconfiguration based technique for improving the
energy efficiency of STT-RAM based LLCs. Our technique dynamically adjusts the
active cache size to reduce the cache leakage energy consumption with minimum
performance loss. We choose a suitable value of STT-RAM retention time for
avoiding refresh overhead and gaining performance. Single-core simulations have
been performed using SPEC2006 benchmarks and Sniper x86-64 simulator. The
results show that while, compared to an STT-RAM LLC of similar area, an SRAM
LLC incurs nearly 100% loss in energy and 7.3% loss in performance; our
technique using STT-RAM cache saves 21.8% energy and incurs only 1.7% loss in
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2209</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2209</id><created>2013-12-08</created><authors><author><keyname>Tan</keyname><forenames>Yong</forenames></author></authors><title>Construct Graph Logic</title><categories>cs.DM math.CO</categories><comments>54 pages, 3 figures</comments><msc-class>68R10</msc-class><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, author uses set theory to construct a logic model of abstract
figure from binary relation. Based on the uniform quantified structure, author
gives two logic system for graph traversal and graph coloring respectively,
moreover shows a new method of cutting graph. Around this model, there are six
algorithms in this paper including exact graph traversal, Algebra calculation
of natural number, graph partition and graph coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2216</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2216</id><created>2013-12-08</created><authors><author><keyname>H&#xf6;lzenspies</keyname><forenames>Philip K. F.</forenames></author></authors><title>Proceedings Second Workshop on Trends in Functional Programming In
  Education</title><categories>cs.CY cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 136, 2013</journal-ref><doi>10.4204/EPTCS.136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Second International Workshop on Trends in Functional Programming in
Education, TFPIE 2013, was held on May 13, 2013 at Brigham Young University in
Provo, Utah, USA. The goal of TFPIE is to gather researchers, professors,
teachers, and all professionals interested in functional programming in
education. Submissions were vetted by the TFPIE 2013 program committee using
prevailing academic standards. The 2 articles in this volume were selected for
publication as the result of this process. Tobin-Hochstadt and Van Horn report
on their solution to the difficult transition between the first semester course
in functional programming (using languages, programming environment, etc.
intended for teaching) to the second semester course in object-oriented
programming (with a production-oriented language, environment, etc.). Finding
that this confusing circumstance made the key concepts hard to grasp for
students, the authors present and evaluate a new introduction to the second
semester course, based on the environment and languages the students used
before, that focusses on key object-oriented concepts. Caldwell lays out an
education narrative that focusses on reasoning about programs, using structural
induction principles. He argues that more such formal reasoning should get more
emphasis in programming education and demonstrates the feasibility thereof by
reporting on his experiences using this narrative in the functional programming
course at the University of Wyoming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2217</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2217</id><created>2013-12-08</created><updated>2014-05-21</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author></authors><title>New tabulation and sparse dynamic programming based techniques for
  sequence similarity problems</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating the length of a longest common subsequence (LCS) of two strings
$A$ and $B$ of length $n$ and $m$ is a classic research topic, with many
worst-case oriented results known. We present two algorithms for LCS length
calculation with respectively $O(mn \log\log n / \log^2 n)$ and $O(mn / \log^2
n + r)$ time complexity, the latter working for $r = o(mn / (\log n \log\log
n))$, where $r$ is the number of matches in the dynamic programming matrix. We
also describe conditions for a given problem sufficient to apply our
techniques, with several concrete examples presented, namely the edit distance,
LCTS and MerLCS problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2218</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2218</id><created>2013-12-08</created><authors><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London, UK</affiliation></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames><affiliation>University of Glasgow, UK</affiliation></author></authors><title>Proceedings 5th Workshop on Programming Language Approaches to
  Concurrency and Communication-cEntric Software</title><categories>cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013</journal-ref><doi>10.4204/EPTCS.137</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PLACES 2013 (full title: Programming Language Approaches to Concurrency- and
Communication-cEntric Software) was the sixth edition of the PLACES workshop
series. After the first PLACES, which was affiliated to DisCoTec in 2008, the
workshop has been part of ETAPS every year since 2009 and is now an established
part of the ETAPS satellite events. This year, PLACES was the best attended
workshop at ETAPS 2013.
  The workshop series was started in order to promote the application of novel
programming language ideas to the increasingly important problem of developing
software for systems in which concurrency and communication are intrinsic
aspects. This includes software for multi- and many-core systems, accelerators
and large-scale distributed and/or service-oriented systems. The scope of
PLACES includes new programming language features, whole new programming
language designs, new type systems, new semantic approaches, new program
analysis techniques, and new implementation mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2222</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2222</id><created>2013-12-08</created><updated>2014-04-08</updated><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>A Stability Result for Sparse Convolutions</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>(i) minor revision of the text (ii) use definition of Freiman
  dimension as in [3] (iii) updated references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will establish in this note a stability result for sparse convolutions on
torsion-free additive (discrete) abelian groups. Sparse convolutions on
torsion-free groups are free of cancellations and hence admit stability, i.e.
injectivity with a universal lower bound $\alpha=\alpha(s,f)$, only depending
on the cardinality $s$ and $f$ of the supports of both input sequences. More
precisely, we show that $\alpha$ depends only on $s$ and $f$ and not on the
ambient dimension. This statement follows from a reduction argument which
involves a compression into a small set preserving the additive structure of
the supports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2225</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2225</id><created>2013-12-08</created><authors><author><keyname>Lazzez</keyname><forenames>Amor</forenames></author></authors><title>VoIP Technology: Security Issues Analysis</title><categories>cs.NI cs.CR</categories><comments>9 pages</comments><journal-ref>International Journal of Emerging Trends &amp; Technology in Computer
  Science (IJETTCS), Volume 2, Issue 4, July-August 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over IP (VoIP) is the technology allowing voice and multimedia
transmissions as data packets over a private or a public IP network. Thanks to
the benefits that it may provide, the VoIP technology is increasingly
attracting attention and interest in the industry. Actually, VoIP allows
significant benefits for customers and communication services providers such as
cost savings, rich media service, phone and service portability, mobility, and
the integration with other applications. Nevertheless, the deployment of the
VoIP technology encounters many challenges such as architecture complexity,
interoperability issues, QoS issues, and security concerns. Among these
disadvantages, VoIP security issues are becoming more serious because
traditional security devices, protocols, and architectures cannot adequately
protect VoIP systems from recent intelligent attacks. The aim of this paper is
carry out a deep analysis of the security concerns of the VoIP technology.
Firstly, we present a brief overview about the VoIP technology. Then, we
discuss security attacks and vulnerabilities related to VoIP protocols and
devices. After that, we talk about the security profiles of the VoIP protocols,
and we present the main security components designed to help the deployment of
a reliable and secured VoIP systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2226</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2226</id><created>2013-12-08</created><updated>2014-03-24</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>On two Algorithmic Problems about Synchronizing Automata</title><categories>cs.FL cs.CC</categories><comments>Minor corrections</comments><acm-class>F.2.0; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two basic problems arising in the theory of synchronizing
automata: deciding, whether or not a given $n$-state automaton is synchronizing
and the problem of approximating the reset threshold for a given synchronizing
automaton.
  For the first problem of deciding whether or not a given $n$-state automaton
is synchronizing we present an algorithm based on~\cite{RandSynch} with linear
in $n$ expected time, while the best known algorithm is quadratic on each
instance.
  For the second problem, we prove that unless \textsf{P} = \textsf{NP}, no
polynomial time algorithm approximates the reset threshold for a given
$n$-state $2$-letter automata within performance ratio less than $0.5 c
\ln{(n)}$ where $c$ is a specific constant from~\cite{AMS6}. This improves the
previous result of the author~\cite{MyTOCS2013} about non-approximability
within any constant factor and also gives the positive answer to the
corresponding conjecture from~\cite{Gerb1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2227</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2227</id><created>2013-12-08</created><authors><author><keyname>Ciuonzo</keyname><forenames>Domenico</forenames></author><author><keyname>Rossi</keyname><forenames>Pierluigi Salvo</forenames></author></authors><title>Decision Fusion with Unknown Sensor Detection Probability</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence we study the problem of channel-aware decision fusion
when the sensor detection probability is not known at the decision fusion
center. Several alternatives proposed in the literature are compared and new
fusion rules (namely 'ideal sensors' and 'locally-optimum detection') are
proposed, showing attractive performance and linear complexity. Simulations are
provided to compare the performance of the aforementioned rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2232</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2232</id><created>2013-12-08</created><authors><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Algorithms for Joint Phase Estimation and Decoding for MIMO Systems in
  the Presence of Phase Noise</title><categories>cs.IT math.IT</categories><comments>13 pages, 6 figures, Submitted to IEEE Transactions on Signal
  Processing for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we derive the maximum a posteriori (MAP) symbol detector for a
multiple-input multiple-output system in the presence of Wiener phase noise due
to noisy local oscillators. As in single-antenna systems, the computation of
the optimal receiver is an infinite dimensional problem and is thus
unimplementable in practice. In this purview, we propose three suboptimal,
low-complexity algorithms for approximately implementing the MAP symbol
detector, which involve joint phase noise estimation and data detection. Our
first algorithm is obtained by means of the sum-product algorithm, where we use
the multivariate Tikhonov canonical distribution approach. In our next
algorithm, we derive an approximate MAP symbol detector based on the
smoother-detector framework, wherein the detector is properly designed by
incorporating the phase noise statistics from the smoother. The third algorithm
is derived based on the variational Bayesian framework. By simulations, we
evaluate the performance of the proposed algorithms for both uncoded and coded
data transmissions, and we observe that the proposed techniques significantly
outperform the other algorithms proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2233</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2233</id><created>2013-12-08</created><authors><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Duch&#xea;ne</keyname><forenames>Eric</forenames></author><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Invariant games and non-homogeneous Beatty sequences</title><categories>math.CO cs.DM</categories><comments>22 pages, 2 figures</comments><msc-class>91A46, 91A05, 68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize all the pairs of complementary non-homogenous Beatty
sequences $(A_n)_{n\ge 0}$ and $(B_n)_{n\ge 0}$ for which there exists an
invariant game having exactly $\{(A_n,B_n)\mid n\ge 0\}\cup \{(B_n,A_n)\mid
n\ge 0\}$ as set of $\mathcal{P}$-positions. Using the notion of Sturmian word
and tools arising in symbolic dynamics and combinatorics on words, this
characterization can be translated to a decision procedure relying only on a
few algebraic tests about algebraicity or rational independence. Given any four
real numbers defining the two sequences, up to these tests, we can therefore
decide whether or not such an invariant game exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2237</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2237</id><created>2013-12-08</created><authors><author><keyname>Hajeer</keyname><forenames>Mustafa H.</forenames></author><author><keyname>Singh</keyname><forenames>Alka</forenames></author><author><keyname>Dasgupta</keyname><forenames>Dipankar</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Clustering online social network communities using genetic algorithms</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To analyze the activities in an Online Social network (OSN), we introduce the
concept of &quot;Node of Attraction&quot; (NoA) which represents the most active node in
a network community. This NoA is identified as the origin/initiator of a
post/communication which attracted other nodes and formed a cluster at any
point in time. In this research, a genetic algorithm (GA) is used as a data
mining method where the main objective is to determine clusters of network
communities in a given OSN dataset. This approach is efficient in handling
different type of discussion topics in our studied OSN - comments, emails, chat
expressions, etc. and can form clusters according to one or more topics. We
believe that this work can be useful in finding the source for spread of this
GA-based clustering of online interactions and reports some results of
experiments with real-world data and demonstrates the performance of proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2241</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2241</id><created>2013-12-08</created><authors><author><keyname>Noormohammadpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Salehi</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Pari</keyname><forenames>Seyed Mohammad Asghari</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak Hossein</forenames></author><author><keyname>Bagheri</keyname><forenames>Hamidreza</forenames></author><author><keyname>Katz</keyname><forenames>Marcos</forenames></author></authors><title>ABMQ: An Agent-Based Modeler and Simulator for Self-Organization in
  MANETs using Qt</title><categories>cs.NI</categories><comments>6 pages, 8 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent-Based Modeling and Simulation (ABMS) is a simple and yet powerful
method for simulation of interactions among individual agents. Using ABMS,
different phenomena can be modeled and simulated without spending additional
time on unnecessary complexities. Although ABMS is well-matured in many
different fields such as economic, social, and natural phenomena, it has not
received much attention in the context of mobile ad-hoc networks (MANETs). In
this paper, we present ABMQ, a powerful Agent-Based platform suitable for
modeling and simulation of self-organization in wireless networks, and
particularly MANETs. By utilizing the unique potentials of Qt Application
Framework, ABMQ provides the ability to easily model and simulate
self-organizing algorithms, and then reuse the codes and models developed
during simulation process for building real third-party applications for
several desktop and mobile platforms, which substantially decreases the
development time and cost, and prevents probable bugs that can happen as a
result of rewriting codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2242</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2242</id><created>2013-12-08</created><authors><author><keyname>Mavridis</keyname><forenames>N.</forenames></author><author><keyname>Konstantopoulos</keyname><forenames>S.</forenames></author><author><keyname>Vetsikas</keyname><forenames>I.</forenames></author><author><keyname>Heldal</keyname><forenames>I.</forenames></author><author><keyname>Karampiperis</keyname><forenames>P.</forenames></author><author><keyname>Mathiason</keyname><forenames>G.</forenames></author><author><keyname>Thill</keyname><forenames>S.</forenames></author><author><keyname>Stathis</keyname><forenames>K.</forenames></author><author><keyname>Karkaletsis</keyname><forenames>V.</forenames></author></authors><title>CLIC: A Framework for Distributed, On-Demand, Human-Machine Cognitive
  Systems</title><categories>cs.AI</categories><acm-class>I.2; I.2.9; I.2.11; C.1.4; C.2.1; H.1.2; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Artificial Cognitive Systems (for example, intelligent robots)
share a number of limitations. First, they are usually made up only of machine
components; humans are only playing the role of user or supervisor. And yet,
there are tasks in which the current state of the art of AI has much worse
performance or is more expensive than humans: thus, it would be highly
beneficial to have a systematic way of creating systems with both human and
machine components, possibly with remote non-expert humans providing
short-duration real-time services. Second, their components are often dedicated
to only one system, and underutilized for a big part of their lifetime. Third,
there is no inherent support for robust operation, and if a new better
component becomes available, one cannot easily replace the old component.
Fourth, they are viewed as a resource to be developed and owned, not as a
utility. Thus, we are presenting CLIC: a framework for constructing cognitive
systems that overcome the above limitations. The architecture of CLIC provides
specific mechanisms for creating and operating cognitive systems that fulfill a
set of desiderata: First, that are distributed yet situated, interacting with
the physical world though sensing and actuation services, and that are also
combining human as well as machine services. Second, that are made up of
components that are time-shared and re-usable. Third, that provide increased
robustness through self-repair. Fourth, that are constructed and reconstructed
on the fly, with components that dynamically enter and exit the system during
operation, on the basis of availability, pricing, and need. Importantly, fifth,
the cognitive systems created and operated by CLIC do not need to be owned and
can be provided on demand, as a utility; thus transforming human-machine
situated intelligence to a service, and opening up many interesting
opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2243</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2243</id><created>2013-12-08</created><authors><author><keyname>Araujo</keyname><forenames>Virginia Maria</forenames></author><author><keyname>Vazquez</keyname><forenames>Jose Ayude</forenames></author></authors><title>Business and technical requirements of Software-as-a-Service:
  Implications in portuguese enterprise business context</title><categories>cs.SE</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol. 3, No.6, November 2013</journal-ref><doi>10.5121/ijfcst.2013.3601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-as-a-Service (SaaS) is a viable option for some companies bearing
their business processes. There is a considerable adoption rate, with companies
already using more than two services for over two years. However, while some
companies have plans to put more business processes supported by these services
in the near future, others do not know if they will. They have several concerns
regarding the software providers service level. These concerns are mainly
technical and functional issues, service availability and payment models. There
are major changes compared to the traditional software that have implications
on how the software is developed and made available to the users. The existing
research addresses specific aspects and few studies give a broader view of the
implications of SaaS for anyone who develops and provides software, and also
for those who consumes it as an end user. What are the real needs of the
Portuguese market? What fears and what is being done to mitigate them? Where
should we focus our attention related to the SaaS offering in order to create
more value? Thus, to analyze these questions four exploratory case studiesare
used to assess the possible implications of SaaS on software developers or
software providers based in Portugal and also on end-users. This article
appears in the context of a realistic and deep research that includes the
involvement of managers, leaders and decision makers of Portuguese companies,
to realize what actually constitutes a problem in SaaS and what effectively
companies would like to have available in this offer. The results of this study
reveal that SaaS effectively constitutes a very interesting and solid solution
for the development of Portuguese companies, however there is a lack for
greater efforts particularly in terms of customization for each customer
(tenant) and integration with the back-end on-premise applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2244</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2244</id><created>2013-12-08</created><updated>2014-04-21</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Time-dependent Hierarchical Dirichlet Model for Timeline Generation</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timeline Generation aims at summarizing news from different epochs and
telling readers how an event evolves. It is a new challenge that combines
salience ranking with novelty detection. For long-term public events, the main
topic usually includes various aspects across different epochs and each aspect
has its own evolving pattern. Existing approaches neglect such hierarchical
topic structure involved in the news corpus in timeline generation. In this
paper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for
timeline generation. Our model can aptly detect different levels of topic
information across corpus and such structure is further used for sentence
selection. Based on the topic mined fro HDM, sentences are selected by
considering different aspects such as relevance, coherence and coverage. We
develop experimental systems to evaluate 8 long-term events that public
concern. Performance comparison between different systems demonstrates the
effectiveness of our model in terms of ROUGE metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2245</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2245</id><created>2013-12-08</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Wong</keyname><forenames>Wiseley</forenames></author></authors><title>Edge-disjoint spanning trees and eigenvalues of regular graphs</title><categories>math.CO cs.DM</categories><comments>4 figures</comments><msc-class>05C50, 15A18, 05C42, 15A42</msc-class><journal-ref>Linear Algebra Appl. 437 (2012), no. 2, 630--647</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially answering a question of Paul Seymour, we obtain a sufficient
eigenvalue condition for the existence of $k$ edge-disjoint spanning trees in a
regular graph, when $k\in \{2,3\}$. More precisely, we show that if the second
largest eigenvalue of a $d$-regular graph $G$ is less than
$d-\frac{2k-1}{d+1}$, then $G$ contains at least $k$ edge-disjoint spanning
trees, when $k\in \{2,3\}$. We construct examples of graphs that show our
bounds are essentially best possible. We conjecture that the above statement is
true for any $k&lt;d/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2246</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2246</id><created>2013-12-08</created><authors><author><keyname>Frasheri</keyname><forenames>Neki</forenames></author></authors><title>E-Governance, International Cooperation and Security - New Millennium
  Challenges for a Small Country</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we analyze the Information and Communication Technologies (ICT)
and their impact on developing countries (DCs), making a criticism of different
views on the supposed role of ICT for the future of human society. This
criticism is seen from the point of view of a small developing post-communist
country as Albania, hoping that the conclusions would throw some light for
developing countries in general, especially those in a transition stage. We
examine some aspects of international collaboration and security, where the ICT
implemented in the public administration may have an important impact. The
development policies and practices are examined, including relations between
public and private sectors. Following arguments of many authors, we identify or
redefine some crucial factors that negatively impact the role of ICT in the
development of the country, its relations with the international community, and
ways to push forward its development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2247</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2247</id><created>2013-12-08</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Wong</keyname><forenames>Wiseley</forenames></author></authors><title>The spectrum and toughness of regular graphs</title><categories>math.CO cs.DM</categories><comments>15 pages, 1 figure, accepted to Discrete Applied Mathematics, special
  issue dedicated to the &quot;Applications of Graph Spectra in Computer Science&quot;
  Conference, Centre de Recerca Matematica (CRM), Bellaterra, Barcelona, June
  16-20, 2012</comments><msc-class>05C42, 05C50, 05E30, 15A18, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1995, Brouwer proved that the toughness of a connected $k$-regular graph
$G$ is at least $k/\lambda-2$, where $\lambda$ is the maximum absolute value of
the non-trivial eigenvalues of $G$. Brouwer conjectured that one can improve
this lower bound to $k/\lambda-1$ and that many graphs (especially graphs
attaining equality in the Hoffman ratio bound for the independence number) have
toughness equal to $k/\lambda$. In this paper, we improve Brouwer's spectral
bound when the toughness is small and we determine the exact value of the
toughness for many strongly regular graphs attaining equality in the Hoffman
ratio bound such as Lattice graphs, Triangular graphs, complements of
Triangular graphs and complements of point-graphs of generalized quadrangles.
For all these graphs with the exception of the Petersen graph, we confirm
Brouwer's intuition by showing that the toughness equals $k/(-\lambda_{min})$,
where $\lambda_{min}$ is the smallest eigenvalue of the adjacency matrix of the
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2249</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2249</id><created>2013-12-08</created><authors><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author></authors><title>Scalable Object Detection using Deep Neural Networks</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have recently achieved state-of-the-art
performance on a number of image recognition benchmarks, including the ImageNet
Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on
the localization sub-task was a network that predicts a single bounding box and
a confidence score for each object category in the image. Such a model captures
the whole-image context around the objects but cannot handle multiple instances
of the same object in the image without naively replicating the number of
outputs for each instance. In this work, we propose a saliency-inspired neural
network model for detection, which predicts a set of class-agnostic bounding
boxes along with a single score for each box, corresponding to its likelihood
of containing any object of interest. The model naturally handles a variable
number of instances for each class and allows for cross-class generalization at
the highest levels of the network. We are able to obtain competitive
recognition performance on VOC2007 and ILSVRC2012, while using only the top few
predicted locations in each image and a small number of neural network
evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2266</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2266</id><created>2013-12-08</created><updated>2013-12-31</updated><authors><author><keyname>Bangerth</keyname><forenames>Wolfgang</forenames></author><author><keyname>Heister</keyname><forenames>Timo</forenames></author><author><keyname>Heltai</keyname><forenames>Luca</forenames></author><author><keyname>Kanschat</keyname><forenames>Guido</forenames></author><author><keyname>Kronbichler</keyname><forenames>Martin</forenames></author><author><keyname>Maier</keyname><forenames>Matthias</forenames></author><author><keyname>Turcksin</keyname><forenames>Bruno</forenames></author><author><keyname>Young</keyname><forenames>Toby D.</forenames></author></authors><title>The deal.II Library, Version 8.1</title><categories>math.NA cs.MS cs.NA</categories><comments>v4: for deal.II version 8.1 v3: minor fixes. v2: correct the citation
  inside the article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an overview of the new features of the finite element
library deal.II version 8.1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2267</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2267</id><created>2013-12-08</created><authors><author><keyname>Zhang</keyname><forenames>Tian-Xian</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Kong</keyname><forenames>Lingjiang</forenames></author></authors><title>IRCI Free Range Reconstruction for SAR Imaging with Arbitrary Length
  OFDM Pulse</title><categories>cs.IT math.IT</categories><comments>29 pages, 10 figures, regular paper</comments><doi>10.1109/TSP.2014.2339796</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Our previously proposed OFDM with sufficient cyclic prefix (CP) synthetic
aperture radar (SAR) imaging algorithm is inter-range-cell interference (IRCI)
free and achieves ideally zero range sidelobes for range reconstruction. In
this OFDM SAR imaging algorithm, the minimum required CP length is almost equal
to the number of range cells in a swath, while the number of subcarriers of an
OFDM signal needs to be more than the CP length. This makes the length of a
transmitted OFDM sequence at least almost twice of the number of range cells in
a swath and for a wide swath imaging, the transmitted OFDM pulse length becomes
long, which may cause problems in some radar applications. In this paper, we
propose a CP based OFDM SAR imaging with arbitrary pulse length, which has IRCI
free range reconstruction and its pulse length is independent of a swath width.
We then present a novel design method for our proposed arbitrary length OFDM
pulses. Simulation results are presented to illustrate the performances of the
OFDM pulse design and the arbitrary pulse length CP based OFDM SAR imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2268</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2268</id><created>2013-12-08</created><updated>2013-12-13</updated><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>U&#x11f;ur</forenames><affiliation>Bo&#x11f;azi&#xe7;i University</affiliation></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames><affiliation>Bo&#x11f;azi&#xe7;i University</affiliation></author><author><keyname>Yakary\ilmaz</keyname><forenames>Abuzer</forenames><affiliation>University of Latvia</affiliation></author></authors><title>Finite automata with advice tapes</title><categories>cs.FL</categories><comments>Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a model of advised computation by finite automata where the advice
is provided on a separate tape. We consider several variants of the model where
the advice is deterministic or randomized, the input tape head is allowed
real-time, one-way, or two-way access, and the automaton is classical or
quantum. We prove several separation results among these variants, demonstrate
an infinite hierarchy of language classes recognized by automata with
increasing advice lengths, and establish the relationships between this and the
previously studied ways of providing advice to finite automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2287</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2287</id><created>2013-12-08</created><authors><author><keyname>Geng</keyname><forenames>Jun</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Quickest Search over Multiple Sequences with Mixed Observation</title><categories>cs.IT math.IT</categories><comments>53 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of sequentially finding an independent and identically
distributed (i.i.d.) sequence that is drawn from a probability distribution
$f_1$ by searching over multiple sequences, some of which are drawn from $f_1$
and the others of which are drawn from a different distribution $f_0$, is
considered. The observer is allowed to take one observation at a time. It has
been shown in a recent work that if each observation comes from one sequence,
the cumulative sum test is optimal. In this paper, we propose a new approach in
which each observation can be a linear combination of samples from multiple
sequences. The test has two stages. In the first stage, namely scanning stage,
one takes a linear combination of a pair of sequences with the hope of scanning
through sequences that are unlikely to be generated from $f_1$ and quickly
identifying a pair of sequences such that at least one of them is highly likely
to be generated by $f_1$. In the second stage, namely refinement stage, one
examines the pair identified from the first stage more closely and picks one
sequence to be the final sequence. The problem under this setup belongs to a
class of multiple stopping time problems. In particular, it is an ordered two
concatenated Markov stopping time problem. We obtain the optimal solution using
the tools from the multiple stopping time theory. The optimal solution has a
rather complex structure. For implementation purpose, a low complexity
algorithm is proposed, in which the observer adopts the cumulative sum test in
the scanning stage and adopts the sequential probability ratio test in the
refinement stage. The performance of this low complexity algorithm is analyzed
when the prior probability of $f_{1}$ occurring is small. Both analytical and
numerical simulation results show that this search strategy can significantly
reduce the searching time when $f_{1}$ is rare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2299</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2299</id><created>2013-12-08</created><updated>2014-02-20</updated><authors><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Stoddard</keyname><forenames>Greg</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Social Status and Badge Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many websites rely on user-generated content to provide value to consumers.
These websites typically incentivize participation by awarding users badges
based on their contributions. While these badges typically have no explicit
value, they act as symbols of social status within a community. In this paper,
we consider the design of badge mechanisms for the objective of maximizing the
total contributions made to a website. Users exert costly effort to make
contributions and, in return, are awarded with badges. A badge is only valued
to the extent that it signals social status and thus badge valuations are
determined endogenously by the number of users who earn each badge. The goal of
this paper is to study the design of optimal and approximately badge mechanisms
under these status valuations. We characterize badge mechanisms by whether they
use a coarse partitioning scheme, i.e. awarding the same badge to many users,
or use a fine partitioning scheme, i.e. awarding a unique badge to most users.
We find that the optimal mechanism uses both fine partitioning and coarse
partitioning. When status valuations exhibit a decreasing marginal value
property, we prove that coarse partitioning is a necessary feature of any
approximately optimal mechanism. Conversely, when status valuations exhibit an
increasing marginal value property, we prove that fine partitioning is
necessary for approximate optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2306</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2306</id><created>2013-12-09</created><authors><author><keyname>Patel</keyname><forenames>Rajendra</forenames></author><author><keyname>Rajawat</keyname><forenames>Arvind</forenames></author></authors><title>Dominant block guided optimal cache size estimation to maximize IPC of
  embedded software</title><categories>cs.PF cs.AR</categories><comments>10 Pages, 4 Figures, 5 Tables, International Journal of Embedded
  Systems and Applications (IJESA).
  http://airccse.org/journal/ijesa/current2013.html</comments><doi>10.5121/ijesa.2013.3303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded system software is highly constrained from performance, memory
footprint, energy consumption and implementing cost view point. It is always
desirable to obtain better Instructions per Cycle. Instruction cache has major
contribution in improving IPC. Cache memories are realized on the same chip
where the processor is running. This considerably increases the system cost as
well. Hence, it is required to maintain a trade off between cache sizes and
performance improvement offered. Determining the number of cache lines and size
of cache line are important parameters for cache designing. The design space
for cache is quite large. It is time taking to execute the given application
with different cache sizes on an instruction set simulator to figure out the
optimal cache size. In this paper, a technique is proposed to identify a number
of cache lines and cache line size for the L1 instruction cache that will offer
best or nearly best IPC. Cache size is derived, at a higher abstraction level,
from basic block analysis in the Low Level Virtual Machine environment. The
cache size estimated is cross validated by simulating the set of benchmark
applications with different cache sizes in simple scalar simulator. The
proposed method seems to be superior in terms of estimation accuracy and
estimation time as compared to the existing methods for estimation of optimal
cache size parameters like cache line size, number of cache lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2315</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2315</id><created>2013-12-09</created><authors><author><keyname>Naghshvar</keyname><forenames>Mohammad</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author></authors><title>Noisy Bayesian Active Learning</title><categories>cs.IT math.IT math.OC math.ST stat.TH</categories><comments>39 pages (one-column), 5 figures, submitted to IEEE Transactions on
  Information Theory</comments><msc-class>62L05, 62L10, 62B10, 62F03, 62F05, 93E35, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of noisy Bayesian active learning, where we are given
a finite set of functions $\mathcal{H}$, a sample space $\mathcal{X}$, and a
label set $\mathcal{L}$. One of the functions in $\mathcal{H}$ assigns labels
to samples in $\mathcal{X}$. The goal is to identify the function that
generates the labels even though the result of a label query on a sample is
corrupted by independent noise. More precisely, the objective is to declare one
of the functions in $\mathcal{H}$ as the true label generating function with
high confidence using as few label queries as possible, by selecting the
queries adaptively and in a strategic manner.
  Previous work in Bayesian active learning considers Generalized Binary
Search, and its variants for the noisy case, and analyzes the number of queries
required by these sampling strategies. In this paper, we show that these
schemes are, in general, suboptimal. Instead we propose and analyze an
alternative strategy for sample collection. Our sampling strategy is motivated
by a connection between Bayesian active learning and active hypothesis testing,
and is based on querying the label of a sample which maximizes the Extrinsic
Jensen-Shannon divergence at each step. We provide upper and lower bounds on
the performance of this sampling strategy, and show that these bounds are
better than previous bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2319</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2319</id><created>2013-12-09</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Torre</keyname><forenames>Alicia Fern&#xe1;ndez-del Viso</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Carlos Rebate</forenames></author></authors><title>A Risk-driven Model for Work Allocation in Global Software Development
  Projects</title><categories>cs.SE</categories><comments>10 pages. The final publication is available at
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6063144</comments><journal-ref>Proceedings of the IEEE International Conference on Global
  Software Engineering (ICGSE 2011), pages 15-24, Helsinki, Finland, August
  15-18 2011</journal-ref><doi>10.1109/ICGSE.2011.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risks and potential benefits of distributing software development projects
globally depend to a large extent on how to allocate work to different
development sites and regions. Existing methods in task allocation are likely
to omit the relevance of considering a multitude of criteria and the impact of
task distribution on risks and potential benefits. To assess risks stemming
from specific work distributions and to exploit organization-specific
experience, we have developed a customizable risk-driven model. It consists of
two main steps: Suggesting a set of task allocation alternatives based on
project- and site-specific characteristics and analyzing it with respect to
possible project risks stemming from the work distribution. To evaluate the
model, we conducted a series of semi-structured interviews in a multinational
IT company. The results of the evaluation show that the suggestions of the
model mostly comply with the retrospective views voiced by the involved
experienced managers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2323</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2323</id><created>2013-12-09</created><authors><author><keyname>A</keyname><forenames>Meiappane.</forenames></author><author><keyname>Venkatesan</keyname><forenames>Dr. V. Prasanna</forenames></author><author><keyname>S</keyname><forenames>Selva Murugan.</forenames></author><author><keyname>A</keyname><forenames>Arun.</forenames></author><author><keyname>A</keyname><forenames>Ramachandran.</forenames></author></authors><title>Architectural Pattern of Health Care System Using GSM Networks</title><categories>cs.SE cs.CY</categories><comments>7 pages</comments><journal-ref>(IJCTE), ISSN: 1793-8201. vol. 3, no. 1, pp. 64-70, February 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale networked environments, such as the Internet, possess the
characteristics of centralised data, centralised access and centralised
control; this gives the user a powerful mechanism for building and integrating
large repositories of centralised information from diverse resources set.
However, a centralised network system with GSM Networks development for a
hospital information systems or a health care information portal is still in
its infancy. The shortcomings of the currently available tools have made the
use of mobile devices more appealing. In mobile computing, the issues such as
low bandwidth, high latency wireless Networks, loss or degradation of wireless
connections, and network errors or failures need to be dealt with. Other issues
to be addressed include system adaptability, reliability, robustness,
extensibility, flexibility, and maintainability. GSM approach has emerged as
the most viable approach for development of intelligent software applications
for wireless mobile devices in a centralized environment, which gives higher
band width of 900 MHz for transmission. The e-healthcare system that we have
developed provides support for physicians, nurses, pharmacists and other
healthcare professionals, as well as for patients and medical devices used to
monitor patients. In this paper, we present the architecture and the
demonstration prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2325</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2325</id><created>2013-12-09</created><authors><author><keyname>Meiappane</keyname><forenames>A.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author><author><keyname>Jegatheeswari</keyname><forenames>V.</forenames></author><author><keyname>Kalpana</keyname><forenames>B.</forenames></author><author><keyname>Sarumathy</keyname><forenames>U.</forenames></author></authors><title>Pattern Based Adaptive Architecture for Internet Banking</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern plays a vital role in software architecture and it is a general
reusable solution to commonly occurring problem. Software architecture of a
system is the set of structures needed to reason about the system, which
comprise software elements, relations among them, and properties of both.
Patterns can be implemented at run-time; they identify key resource constraints
and best practices. Architecture Pattern expresses a fundamental structural
organization or schema for software systems. Patterns in software architecture,
offer the promise of helping the architect to identify combinations of
Architecture or Solution Building Blocks that have been proven to deliver
effective solutions. In Internet banking, we analyzed some attributes such as
reliability, security, availability, load balancing and so on. The use of
patterns, which is of a reusable component, is a good tool to help designers
build load balancing systems. In this paper we are going to propose pattern
based adaptive architecture for internet banking system and so the above
attributes can be improved by the usage of patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2327</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2327</id><created>2013-12-09</created><authors><author><keyname>Meiappane</keyname><forenames>A.</forenames></author><author><keyname>Prabavadhi</keyname><forenames>J.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author></authors><title>Strategy pattern: payment pattern for Internet banking</title><categories>cs.SE</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper brings out the design patterns according to the various services
involved in internet banking. The Pattern oriented Software Architecture uses
the pattern which eliminates the difficulty of reusability in a particular
context. The patterns are to be designed using BPM (Business Process Model) for
effective cross cutting on process level. For implementing the above said BPM,
the Internet banking has been taken to implement the pattern into it. The
Analysis and identification of various processes in Internet Banking have been
done, to identify the effective cross cutting features. With this process the
pattern has been designed, as a reusability component to be used by the
Software Architect. The pattern help us to resolve recurring problems
constructively and based on proven solutions and also support us in
understanding the architecture of a given software system. Once the model is
finalized by analyzing, we found payment in the process of internet banking has
a strategy pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2333</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2333</id><created>2013-12-09</created><authors><author><keyname>Elliott</keyname><forenames>James</forenames></author><author><keyname>Hoemmen</keyname><forenames>Mark</forenames></author><author><keyname>Mueller</keyname><forenames>Frank</forenames></author></authors><title>Exploiting Data Representation for Fault Tolerance</title><categories>cs.NA math.NA</categories><doi>10.1016/j.jocs.2015.12.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the link between data representation and soft errors in dot
products. We present an analytic model for the absolute error introduced should
a soft error corrupt a bit in an IEEE-754 floating-point number. We show how
this finding relates to the fundamental linear algebra concepts of
normalization and matrix equilibration. We present a case study illustrating
that the probability of experiencing a large error in a dot product is
minimized when both vectors are normalized. Furthermore, when data is
normalized we show that the absolute error is less than one or very large,
which allows us to detect large errors. We demonstrate how this finding can be
used by instrumenting the GMRES iterative solver. We count all possible errors
that can be introduced through faults in arithmetic in the computationally
intensive orthogonalization phase, and show that when scaling is used the
absolute error can be bounded above by one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2334</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2334</id><created>2013-12-09</created><updated>2014-09-11</updated><authors><author><keyname>Pretnar</keyname><forenames>Matija</forenames><affiliation>University of Ljubljana</affiliation></author></authors><title>Inferring Algebraic Effects</title><categories>cs.PL cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  12, 2014) lmcs:1004</journal-ref><doi>10.2168/LMCS-10(3:21)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complete polymorphic effect inference algorithm for an ML-style
language with handlers of not only exceptions, but of any other algebraic
effect such as input &amp; output, mutable references and many others. Our main aim
is to offer the programmer a useful insight into the effectful behaviour of
programs. Handlers help here by cutting down possible effects and the resulting
lengthy output that often plagues precise effect systems. Additionally, we
present a set of methods that further simplify the displayed types, some even
by deliberately hiding inferred information from the programmer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2337</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2337</id><created>2013-12-09</created><authors><author><keyname>Filakovsk&#xfd;</keyname><forenames>Marek</forenames></author><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Luk&#xe1;&#x161;</forenames></author></authors><title>Are two given maps homotopic? An algorithmic viewpoint</title><categories>math.AT cs.CG</categories><msc-class>Primary 55Q05, Secondary 55P40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two algorithms. In their simplest form, the first
algorithm decides the existence of a pointed homotopy between given simplicial
maps f, g from X to Y and the second computes the group $[\Sigma X,Y]^*$ of
pointed homotopy classes of maps from a suspension; in both cases, the target Y
is assumed simply connected and the algorithms run in polynomial time when the
dimension of X is fixed. More generally, these algorithms work relative to a
subspace A of X, fibrewise over a simply connected B and also equivariantly
when all spaces are equipped with a free action of a fixed finite group G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2338</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2338</id><created>2013-12-09</created><authors><author><keyname>Lin</keyname><forenames>Pin-Hsun</forenames></author><author><keyname>Villardi</keyname><forenames>Gabriel P.</forenames></author><author><keyname>Lan</keyname><forenames>Zhou</forenames></author><author><keyname>Harada</keyname><forenames>Hiroshi</forenames></author></authors><title>Practical Design for Multiple-Antenna Cognitive Radio Networks with
  Coexistence Constraint</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the practical design for the multiple-antenna
cognitive radio (CR) networks sharing the geographically used or unused
spectrum. We consider a single cell network formed by the primary users (PU),
which are half-duplex two-hop relay channels and the secondary users (SU) are
single user additive white Gaussian noise channels. In addition, the
coexistence constraint which requires PUs' coding schemes and rates unchanged
with the emergence of SU, should be satisfied. The contribution of this paper
are twofold. First, we explicitly design the scheme to pair the SUs to the
existing PUs in a single cell network. Second, we jointly design the nonlinear
precoder, relay beamformer, and the transmitter and receiver beamformers to
minimize the sum mean square error of the SU system. In the first part, we
derive an approximate relation between the relay ratio, chordal distance and
strengths of the vector channels, and the transmit powers. Based on this
relation, we are able to solve the optimal pairing between SUs and PUs
efficiently. In the second part, considering the feasibility of implementation,
we exploit the Tomlinson-Harashima precoding instead of the dirty paper coding
to mitigate the interference at the SU receiver, which is known side
information at the SU transmitter. To complete the design, we first approximate
the optimization problem as a convex one. Then we propose an iterative
algorithm to solve it with CVX. This joint design exploits all the degrees of
design. To the best of our knowledge, both the two parts have never been
considered in the literature. Numerical results show that the proposed pairing
scheme outperforms the greedy and random pairing with low complexity. Numerical
results also show that even if all the channel matrices are full rank, under
which the simple zero forcing scheme is infeasible, the proposed scheme can
still work well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2341</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2341</id><created>2013-12-09</created><authors><author><keyname>Meiappane</keyname><forenames>A.</forenames></author><author><keyname>Venkatesan</keyname><forenames>V. Prasanna</forenames></author><author><keyname>Prabavadhi</keyname><forenames>J.</forenames></author></authors><title>Visitor Pattern: Implementation of Enquiry Pattern for Internet Banking</title><categories>cs.SE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper brings out the design patterns according to the various services
involved in internet banking. The Pattern oriented Software Architecture uses
the pattern which eliminates the difficulty of reusability in a particular
context. The patterns are to be designed using BPM (Business Process Model) for
effective cross cutting on process level. For implementing the above said BPM,
the Internet banking has been taken to implement the pattern into it. The
Analysis and identification of various processes in Internet Banking have been
done, to identify the effective cross cutting features. With this process the
pattern has been designed, as a reusability component to be used by the
Software Architect. The pattern help us to resolve recurring problems
constructively and based on proven solutions and also support us in
understanding the architecture of a given software system. Once the model is
finalized by analyzing we found enquiry pattern as the visitor pattern and
implement the pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2342</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2342</id><created>2013-12-09</created><authors><author><keyname>Meiappane</keyname><forenames>A.</forenames></author><author><keyname>Chithra</keyname><forenames>B.</forenames></author><author><keyname>Venkataesan</keyname><forenames>Prasanna</forenames></author></authors><title>Evaluation of Software Architecture Quality Attribute for an Internet
  Banking System</title><categories>cs.SE</categories><comments>4 pages</comments><doi>10.5120/10189-5062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design phase plays a vital role than all other phases in the software
development. Software Architecture has to meet both the functional and
non-functional quality requirements. The Evaluation of Architecture has to be
performed, so that the developers are assured that their selected Architecture
will reduce the cost and effort and also enhances the various quality
attributes like Availability, Reusability, Performance, Modifiability and
Extendibility. The success of the system depends upon the Architecture
Evaluation by the essential method to the system. The overall ranking of the
candidate architecture is ascertained by assigning weight to the scenario and
scenario interaction. In this paper, SAAM method is taken to evaluate the two
architectures from the various available method and techniques to achieve the
various quality attributes by weight metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2344</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2344</id><created>2013-12-09</created><authors><author><keyname>Meiappane</keyname><forenames>A.</forenames></author><author><keyname>Venkataesan</keyname><forenames>Dr. V. Prasanna</forenames></author></authors><title>Request and notification Pattern for an internet banking System</title><categories>cs.SE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of software is enhanced by using the design patterns. The design
patterns are the reusable component used in the development of the software,
which delivers improved quality software to the end users. The researchers have
developed design patterns for user interface, e-commerce applications, mobile
applications, text classification and so on. There are no design patterns for
internet banking applications, but there is analysis pattern for banking. This
motivated to mine the design patterns for internet banking application. It can
be mined from the document of Business Process Management (BPM). In this paper
the request and notification are two patterns, that have been presented, which
have been mined from internet banking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2353</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2353</id><created>2013-12-09</created><authors><author><keyname>Martinenghi</keyname><forenames>Davide</forenames></author></authors><title>On the difference between checking integrity constraints before or after
  updates</title><categories>cs.DB</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrity checking is a crucial issue, as databases change their instance all
the time and therefore need to be checked continuously and rapidly. Decades of
research have produced a plethora of methods for checking integrity constraints
of a database in an incremental manner. However, not much has been said about
when to check integrity. In this paper, we study the differences and
similarities between checking integrity before an update (a.k.a. pre-test) or
after (a.k.a. post-test) in order to assess the respective convenience and
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2355</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2355</id><created>2013-12-09</created><authors><author><keyname>Martinenghi</keyname><forenames>Davide</forenames></author></authors><title>On the dependency on the size of the data when chasing under conceptual
  dependencies</title><categories>cs.DB</categories><comments>22 pages, 2 figures (one of which with 5 subfigures). arXiv admin
  note: substantial text overlap with arXiv:1003.3139</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conceptual dependencies (CDs) are particular kinds of key dependencies (KDs)
and inclusion dependencies (IDs) that precisely characterize relational
schemata modeled according to the main features of the Entity-Relationship (ER)
model. An instance for such a schema may be inconsistent (data violate the
dependencies) and incomplete (data constitute a piece of correct information,
but not necessarily all the relevant information). While undecidable under
general KDs and IDs, query answering under incomplete data is known to be
decidable for CDs. The known techniques are based on the chase -- a special
instance, organized in levels of depth, that is a representative of all the
instances that satisfy the dependencies and that include the initial instance.
Although the chase generally has infinite size, query answering can be
addressed by posing the query (or a rewriting thereof) on a finite, initial
part of the chase. Contrary to previous claims, we show that the maximum level
of such an initial part cannot be bounded by a constant that does not depend on
the size of the initial instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2358</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2358</id><created>2013-12-09</created><updated>2014-06-15</updated><authors><author><keyname>Zhou</keyname><forenames>Shenglong</forenames></author><author><keyname>Xiu</keyname><forenames>Naihua</forenames></author><author><keyname>Wang</keyname><forenames>Yingnan</forenames></author><author><keyname>Kong</keyname><forenames>Lingchen</forenames></author></authors><title>Exact Recovery for Sparse Signal via Weighted $l_1$ Minimization</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Numerical experiments in literature on compressed sensing have indicated that
the reweighted $l_1$ minimization performs exceptionally well in recovering
sparse signal. In this paper, we develop exact recovery conditions and
algorithm for sparse signal via weighted $l_1$ minimization from the insight of
the classical NSP (null space property) and RIC (restricted isometry constant)
bound. We first introduce the concept of WNSP (weighted null space property)
and reveal that it is a necessary and sufficient condition for exact recovery.
We then prove that the RIC bound by weighted $l_1$ minimization is
$\delta_{ak}&lt;\sqrt{\frac{a-1}{a-1+\gamma^2}}$, where $a&gt;1$, $0&lt;\gamma\leq1$ is
determined by an optimization problem over the null space. When $\gamma&lt; 1$
this bound is greater than $\sqrt{\frac{a-1}{a}}$ from $l_1$ minimization. In
addition, we also establish the bound on $\delta_k$ and show that it can be
larger than the sharp one 1/3 via $l_1$ minimization and also greater than
0.4343 via weighted $l_1$ minimization under some mild cases. Finally, we
achieve a modified iterative reweighted $l_1$ minimization (MIRL1) algorithm
based on our selection principle of weight, and the numerical experiments
demonstrate that our algorithm behaves much better than $l_1$ minimization and
iterative reweighted $l_1$ minimization (IRL1) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2359</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2359</id><created>2013-12-09</created><updated>2013-12-12</updated><authors><author><keyname>Breitsprecher</keyname><forenames>Thilo</forenames></author><author><keyname>Codescu</keyname><forenames>Mihai</forenames></author><author><keyname>Jucovschi</keyname><forenames>Constantin</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author><author><keyname>Wartzack</keyname><forenames>Sandro</forenames></author></authors><title>Towards Ontological Support for Principle Solutions in Mechanical
  Engineering</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The engineering design process follows a series of standardized stages of
development, which have many aspects in common with software engineering. Among
these stages, the principle solution can be regarded as an analogue of the
design specification, fixing as it does the way the final product works. It is
usually constructed as an abstract sketch (hand-drawn or constructed with a CAD
system) where the functional parts of the product are identified, and geometric
and topological constraints are formulated. Here, we outline a semantic
approach where the principle solution is annotated with ontological assertions,
thus making the intended requirements explicit and available for further
machine processing; this includes the automated detection of design errors in
the final CAD model, making additional use of a background ontology of
engineering knowledge. We embed this approach into a document-oriented design
workflow, in which the background ontology and semantic annotations in the
documents are exploited to trace parts and requirements through the design
process and across different applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2366</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2366</id><created>2013-12-09</created><authors><author><keyname>Kurada</keyname><forenames>Ramachandra Rao</forenames></author><author><keyname>Pavan</keyname><forenames>Dr. K Karteeka</forenames></author><author><keyname>Rao</keyname><forenames>Dr. AV Dattareya</forenames></author></authors><title>A preliminary survey on optimized multiobjective metaheuristic methods
  for data clustering using evolutionary approaches</title><categories>cs.NE</categories><comments>21 Pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5,No 5, Oct 2013, ISSN:0975-3826</journal-ref><doi>10.5121/ijcsit.2013.5504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present survey provides the state-of-the-art of research, copiously
devoted to Evolutionary Approach (EAs) for clustering exemplified with a
diversity of evolutionary computations. The Survey provides a nomenclature that
highlights some aspects that are very important in the context of evolutionary
data clustering. The paper missions the clustering trade-offs branched out with
wide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally,
this study addresses the potential challenges of MOEA design and data
clustering, along with conclusions and recommendations for novice and
researchers by positioning most promising paths of future research. MOEAs have
substantial success across a variety of MOP applications, from pedagogical
multifunction optimization to real-world engineering design. The survey paper
noticeably organizes the developments witnessed in the past three decades for
EAs based metaheuristics to solve multiobjective optimization problems (MOP)
and to derive significant progression in ruling high quality elucidations in a
single run. Data clustering is an exigent task, whose intricacy is caused by a
lack of unique and precise definition of a cluster. The discrete optimization
problem uses the cluster space to derive a solution for Multiobjective data
clustering. Discovery of a majority or all of the clusters (of illogical
shapes) present in the data is a long-standing goal of unsupervised predictive
learning problems or exploratory pattern analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2367</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2367</id><created>2013-12-09</created><authors><author><keyname>Kaufman</keyname><forenames>Tali</forenames></author><author><keyname>Lubotzky</keyname><forenames>Alexander</forenames></author></authors><title>High Dimensional Expanders and Property Testing</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the high dimensional expansion property as defined by Gromov,
Linial and Meshulam, for simplicial complexes is a form of testability. Namely,
a simplicial complex is a high dimensional expander iff a suitable property is
testable. Using this connection, we derive several testability results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2368</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2368</id><created>2013-12-09</created><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>He</keyname><forenames>Feidun</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>A Unified Markov Chain Approach to Analysing Randomised Search
  Heuristics</title><categories>math.OC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence, convergence rate and expected hitting time play fundamental
roles in the analysis of randomised search heuristics. This paper presents a
unified Markov chain approach to studying them. Using the approach, the
sufficient and necessary conditions of convergence in distribution are
established. Then the average convergence rate is introduced to randomised
search heuristics and its lower and upper bounds are derived. Finally, novel
average drift analysis and backward drift analysis are proposed for bounding
the expected hitting time. A computational study is also conducted to
investigate the convergence, convergence rate and expected hitting time. The
theoretical study belongs to a prior and general study while the computational
study belongs to a posterior and case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2371</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2371</id><created>2013-12-09</created><updated>2015-08-05</updated><authors><author><keyname>Christodoulou</keyname><forenames>George</forenames></author><author><keyname>Kov&#xe1;cs</keyname><forenames>Annam&#xe1;ria</forenames></author><author><keyname>Sgouritsa</keyname><forenames>Alkmini</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author></authors><title>Tight Bounds for the Price of Anarchy of Simultaneous First Price
  Auctions</title><categories>cs.GT</categories><comments>37 pages, 5 figures, ACM Transactions on Economics and Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Price of Anarchy of simultaneous first-price auctions for buyers
with submodular and subadditive valuations. The current best upper bounds for
the Bayesian Price of Anarchy of these auctions are e/(e-1) [Syrgkanis and
Tardos 2013] and 2 [Feldman et al. 2013], respectively. We provide matching
lower bounds for both cases even for the case of full information and for mixed
Nash equilibria via an explicit construction.
  We present an alternative proof of the upper bound of e/(e-1) for first-price
auctions with fractionally subadditive valuations which reveals the worst-case
price distribution, that is used as a building block for the matching lower
bound construction.
  We generalize our results to a general class of item bidding auctions that we
call bid-dependent auctions (including first-price auctions and all-pay
auctions) where the winner is always the highest bidder and each bidder's
payment depends only on his own bid.
  Finally, we apply our techniques to discriminatory price multi-unit auctions.
We complement the results of [de Keijzer et al. 2013] for the case of
subadditive valuations, by providing a matching lower bound of 2. For the case
of submodular valuations, we provide a lower bound of 1.109. For the same class
of valuations, we were able to reproduce the upper bound of e/(e-1) using our
non-smooth approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2375</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2375</id><created>2013-12-09</created><authors><author><keyname>Kurada</keyname><forenames>RamachandraRao</forenames></author><author><keyname>Pavan</keyname><forenames>Dr. K Karteeka</forenames></author></authors><title>Novel text categorization by amalgamation of augmented k-nearest
  neighborhood classification and k-medoids clustering</title><categories>cs.IR</categories><comments>13 Pages</comments><journal-ref>International Journal of Computational Science &amp; Information
  Technology(IJCSITY)Vol.1,No.4,Nov2013,ISSN: 2320-7442</journal-ref><doi>10.5121/ijcsity.2013.1406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning for text classification is the underpinning of document
cataloging, news filtering, document steering and exemplification. In text
mining realm, effective feature selection is significant to make the learning
task more accurate and competent. One of the traditional lazy text classifier
k-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity
between all the objects in training and testing sets, there by leads to
exaggeration of both computational complexity of the algorithm and massive
consumption of main memory. To diminish these shortcomings in viewpoint of a
data-mining practitioner an amalgamative technique is proposed in this paper
using a novel restructured version of kNN called AugmentedkNN(AkNN) and
k-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the
initial training set by imposing attribute feature selection for reduction of
high dimensionality, also it detects and excludes the high-fliers samples in
the initial training set and restructures a constrictedtraining set. The kMdd
clustering algorithm generates the cluster centers (as interior objects) for
each category and restructures the constricted training set with centroids.
This technique is amalgamated with AkNNclassifier that was prearranged with
text mining similarity measures. Eventually, significantweights and ranks were
assigned to each object in the new training set based upon their accessory
towards the object in testing set. Experiments conducted on Reuters-21578 a UCI
benchmark text mining data set, and comparisons with traditional kNNclassifier
designates the referredmethod yieldspreeminentrecitalin both clustering and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2378</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2378</id><created>2013-12-09</created><authors><author><keyname>Kurada</keyname><forenames>Ramachandra Rao</forenames></author></authors><title>Unsupervised classification of uncertain data objects in spatial
  databases using computational geometry and indexing techniques</title><categories>cs.DB</categories><comments>9 pages</comments><journal-ref>International Journal of Engineering Research and Applications
  (IJERA), Vol. 2, Issue 2, Mar-Apr 2012, pp.806-814, ISSN: 2248-9622</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised classification called clustering is a process of organizing
objects into groups whose members are similar in some way. Clustering of
uncertain data objects is a challenge in spatial data bases. In this paper we
use Probability Density Functions (PDF) to represent these uncertain data
objects, and apply Uncertain K-Means algorithm to generate the clusters. This
clustering algorithm uses the Expected Distance (ED) to compute the distance
between objects and cluster representatives. To further improve the performance
of UK-Means we propose a novel technique called Voronoi Diagrams from
Computational Geometry to prune the number of computations of ED. This
technique works efficiently but results pruning overheads. In order to reduce
these in pruning overhead we introduce R*-tree indexing over these uncertain
data objects, so that it reduces the computational cost and pruning overheads.
Our novel approach of integrating UK-Means with voronoi diagrams and R* Tree
applied over uncertain data objects generates imposing outcome when compared
with the accessible methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2381</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2381</id><created>2013-12-09</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Kubica</keyname><forenames>Marcin</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Szreder</keyname><forenames>Bartosz</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>A Note on the Longest Common Compatible Prefix Problem for Partial Words</title><categories>cs.DS cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a partial word $w$ the longest common compatible prefix of two positions
$i,j$, denoted $lccp(i,j)$, is the largest $k$ such that $w[i,i+k-1]\uparrow
w[j,j+k-1]$, where $\uparrow$ is the compatibility relation of partial words
(it is not an equivalence relation). The LCCP problem is to preprocess a
partial word in such a way that any query $lccp(i,j)$ about this word can be
answered in $O(1)$ time. It is a natural generalization of the longest common
prefix (LCP) problem for regular words, for which an $O(n)$ preprocessing time
and $O(1)$ query time solution exists.
  Recently an efficient algorithm for this problem has been given by F.
Blanchet-Sadri and J. Lazarow (LATA 2013). The preprocessing time was
$O(nh+n)$, where $h$ is the number of &quot;holes&quot; in $w$. The algorithm was
designed for partial words over a constant alphabet and was quite involved.
  We present a simple solution to this problem with slightly better runtime
that works for any linearly-sortable alphabet. Our preprocessing is in time
$O(n\mu+n)$, where $\mu$ is the number of blocks of holes in $w$. Our algorithm
uses ideas from alignment algorithms and dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2383</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2383</id><created>2013-12-09</created><authors><author><keyname>Klogo</keyname><forenames>Griffith S.</forenames></author><author><keyname>Gasonoo</keyname><forenames>Akpeko</forenames></author><author><keyname>Ampomah</keyname><forenames>Isaac K. E.</forenames></author></authors><title>On the Performance of Filters for Reduction of Speckle Noise in SAR
  Images off the Coast of the Gulf of Guinea</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methods
that have been proposed for the West African sub-region. With the increase in
the number of oil exploration companies in Ghana (and her neighbors) and the
rise in the coastal activities in the sub-region, there is the need for proper
monitoring of the environmental impact of these socio-economic activities on
the environment. Detection and near real-time information about oil spills are
fundamental in reducing oil spill environmental impact. SAR images are prone to
some noise, which is predominantly speckle noise around the coastal areas. This
paper evaluates the performance of the mean and median filters used in the
preprocessing filtering to reduce speckle noise in SAR images for most image
processing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2390</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2390</id><created>2013-12-09</created><authors><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author><author><keyname>Ma</keyname><forenames>Wann-Jiun</forenames></author><author><keyname>Yuksel</keyname><forenames>Serdar</forenames></author></authors><title>Stochastic Stability of Event-triggered Anytime Control</title><categories>math.OC cs.SY</categories><comments>IEEE Transactions on Automatic Control, under review</comments><msc-class>93C10, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate control of a non-linear process when communication and
processing capabilities are limited. The sensor communicates with a controller
node through an erasure channel which introduces i.i.d. packet dropouts.
Processor availability for control is random and, at times, insufficient to
calculate plant inputs. To make efficient use of communication and processing
resources, the sensor only transmits when the plant state lies outside a
bounded target set. Control calculations are triggered by the received data. If
a plant state measurement is successfully received and while the processor is
available for control, the algorithm recursively calculates a sequence of
tentative plant inputs, which are stored in a buffer for potential future use.
This safeguards for time-steps when the processor is unavailable for control.
We derive sufficient conditions on system parameters for stochastic stability
of the closed loop and illustrate performance gains through numerical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2406</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2406</id><created>2013-12-09</created><authors><author><keyname>Achieng</keyname><forenames>Mourine</forenames></author><author><keyname>Ruhode</keyname><forenames>Ephias</forenames></author></authors><title>The adoption and challenges of electronic voting technologies within the
  South African context</title><categories>cs.CY</categories><comments>12 pages, 1 figure, International Journal of Managing Information
  Technology (IJMIT) Vol.5, No.4, November 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Literature has shown that countries such as Brazil and India have
successfully implemented electronic voting systems and other countries are at
various piloting stages to address many challenges associated with manual paper
based system such ascosts of physical ballot paper and other overheads,
electoral delays, distribution of electoral materials, and general lack of
confidence in the electoral process. It is in this context that this study
explores how South African can leverage the opportunities that e-voting
presents. Manual voting is often tedious, non-secure, and time-consuming, which
leads us to think about using electronic facilities to make the process more
efficient. This study proposes that the adoption of electronic voting
technologies could perhaps mitigate some of these issues and challengesin the
process improving the electoral process. The study used an on-line
questionnaire which was administered to a broader group of voters and an
in-depth semi-structured interview with the Independent Electoral Commission
officials. The analysis is based on thematic analysis and diffusion of
innovations theory is adopted as a theoretical lens of analysis. The findings
reveal that relative advantage, compatibility and complexity would determine
the intentions of South African voters and the Electoral Management Bodies
(IEC) to adopt e-voting technologies. Moreover, the findings also reveal
several other factorsthat could influence the adoption process. The study is
limited to only voters in Cape Town and these voters were expected to have some
access to the internet. The sample size limits the generalizability of the
findings of this study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2447</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2447</id><created>2013-12-09</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author><author><keyname>Dodig-Crnkovic</keyname><forenames>Gordana</forenames></author></authors><title>Typologies of Computation and Computational Models</title><categories>cs.GL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We need much better understanding of information processing and computation
as its primary form. Future progress of new computational devices capable of
dealing with problems of big data, internet of things, semantic web, cognitive
robotics and neuroinformatics depends on the adequate models of computation. In
this article we first present the current state of the art through
systematization of existing models and mechanisms, and outline basic structural
framework of computation. We argue that defining computation as information
processing, and given that there is no information without (physical)
representation, the dynamics of information on the fundamental level is
physical/ intrinsic/ natural computation. As a special case, intrinsic
computation is used for designed computation in computing machinery. Intrinsic
natural computation occurs on variety of levels of physical processes,
containing the levels of computation of living organisms (including highly
intelligent animals) as well as designed computational devices. The present
article offers a typology of current models of computation and indicates future
paths for the advancement of the field; both by the development of new
computational models and by learning from nature how to better compute using
different mechanisms of intrinsic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2451</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2451</id><created>2013-12-06</created><authors><author><keyname>Nizamani</keyname><forenames>Sarwat</forenames></author><author><keyname>Memon</keyname><forenames>Nasrullah</forenames></author></authors><title>CEAI: CCM based Email Authorship Identification Model</title><categories>cs.LG</categories><journal-ref>Egyptian Informatics Journal,Volume 14, Issue 3, November 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present a model for email authorship identification (EAI) by
employing a Cluster-based Classification (CCM) technique. Traditionally,
stylometric features have been successfully employed in various authorship
analysis tasks; we extend the traditional feature-set to include some more
interesting and effective features for email authorship identification (e.g.
the last punctuation mark used in an email, the tendency of an author to use
capitalization at the start of an email, or the punctuation after a greeting or
farewell). We also included Info Gain feature selection based content features.
It is observed that the use of such features in the authorship identification
process has a positive impact on the accuracy of the authorship identification
task. We performed experiments to justify our arguments and compared the
results with other base line models. Experimental results reveal that the
proposed CCM-based email authorship identification model, along with the
proposed feature set, outperforms the state-of-the-art support vector machine
(SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. The
proposed model attains an accuracy rate of 94% for 10 authors, 89% for 25
authors, and 81% for 50 authors, respectively on Enron dataset, while 89.5%
accuracy has been achieved on authors' constructed real email dataset. The
results on Enron dataset have been achieved on quite a large number of authors
as compared to the models proposed by Iqbal et al. [1, 2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2457</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2457</id><created>2013-12-09</created><authors><author><keyname>Davies</keyname><forenames>Mike</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Compressed Quantitative MRI: Bloch Response Recovery through Iterated
  Projection</title><categories>cs.IT math.IT</categories><comments>5 pages 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recently proposed Magnetic Resonance Fingerprinting
technique, we develop a principled compressed sensing framework for
quantitative MRI. The three key components are: a random pulse excitation
sequence following the MRF technique; a random EPI subsampling strategy and an
iterative projection algorithm that imposes consistency with the Bloch
equations. We show that, as long as the excitation sequence possesses an
appropriate form of persistent excitation, we are able to achieve accurate
recovery of the proton density, $T_1$, $T_2$ and off-resonance maps
simultaneously from a limited number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2459</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2459</id><created>2013-12-09</created><updated>2014-10-16</updated><authors><author><keyname>Simas</keyname><forenames>Tiago</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M</forenames></author></authors><title>Distance Closures on Complex Networks</title><categories>cs.SI cond-mat.dis-nn cs.IR nlin.CG physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To expand the toolbox available to network science, we study the isomorphism
between distance and Fuzzy (proximity or strength) graphs. Distinct transitive
closures in Fuzzy graphs lead to closures of their isomorphic distance graphs
with widely different structural properties. For instance, the All Pairs
Shortest Paths (APSP) problem, based on the Dijkstra algorithm, is equivalent
to a metric closure, which is only one of the possible ways to calculate
shortest paths. Understanding and mapping this isomorphism is necessary to
analyse models of complex networks based on weighted graphs. Any conclusions
derived from such models should take into account the distortions imposed on
graph topology when converting proximity/strength into distance graphs, to
subsequently compute path length and shortest path measures. We characterise
the isomorphism using the max-min and Dombi disjunction/conjunction pairs. This
allows us to: (1) study alternative distance closures, such as those based on
diffusion, metric, and ultra-metric distances; (2) identify the operators
closest to the metric closure of distance graphs (the APSP), but which are
logically consistent; and (3) propose a simple method to compute alternative
distance closures using existing algorithms for the APSP. In particular, we
show that a specific diffusion distance is promising for community detection in
complex networks, and is based on desirable axioms for logical inference or
approximate reasoning on networks; it also provides a simple algebraic means to
compute diffusion processes on networks. Based on these results, we argue that
choosing different distance closures can lead to different conclusions about
indirect associations on network data, as well as the structure of complex
networks, and are thus important to consider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2465</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2465</id><created>2013-12-09</created><updated>2014-06-06</updated><authors><author><keyname>Davies</keyname><forenames>Mike</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>A Compressed Sensing Framework for Magnetic Resonance Fingerprinting</title><categories>cs.IT math.IT</categories><comments>32 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recently proposed Magnetic Resonance Fingerprinting (MRF)
technique, we develop a principled compressed sensing framework for
quantitative MRI. The three key components are: a random pulse excitation
sequence following the MRF technique; a random EPI subsampling strategy and an
iterative projection algorithm that imposes consistency with the Bloch
equations. We show that theoretically, as long as the excitation sequence
possesses an appropriate form of persistent excitation, we are able to
accurately recover the proton density, T1, T2 and off-resonance maps
simultaneously from a limited number of samples. These results are further
supported through extensive simulations using a brain phantom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2466</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2466</id><created>2013-12-09</created><authors><author><keyname>Kaufman</keyname><forenames>Brett</forenames></author><author><keyname>Lilleberg</keyname><forenames>Jorma</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>An Analog Baseband Approach for Designing Full-Duplex Radios</title><categories>cs.NI</categories><comments>5 pages, 8 figures, to appear in 2013 Asilomar Conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent wireless testbed implementations have proven that full-duplex
communication is in fact possible and can outperform half-duplex systems. Many
of these implementations modify existing half-duplex systems to operate in
full-duplex. To realize the full potential of full-duplex, radios need to be
designed with self-interference in mind. In our work, we use an experimental
setup with a patch antenna prototype to characterize the self-interference
channel between two radios. In doing so, we form an analytical model to design
analog baseband cancellation techniques. We show that our cancellation scheme
can provide up to 10 dB improved signal strength, 2.5 bps/Hz increase in rate,
and a 10,000 improvement in BER as compared to the RF only cancellation
provided by the patch antenna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2474</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2474</id><created>2013-12-09</created><authors><author><keyname>Vok&#x159;&#xed;nek</keyname><forenames>Luk&#xe1;&#x161;</forenames></author></authors><title>Computing the abelian heap of unpointed stable homotopy classes of maps</title><categories>math.AT cs.CG</categories><msc-class>Primary 55Q05, Secondary 55S91</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithmic computation of the set of unpointed stable homotopy classes of
equivariant fibrewise maps was described in a recent paper of the author and
his collaborators. In the present paper, we describe a simplification of this
computation that uses an abelian heap structure on this set that was observed
in another paper of the author. A heap is essentially a group without a choice
of its neutral element; in addition, we allow it to be empty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2482</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2482</id><created>2013-12-09</created><updated>2014-03-24</updated><authors><author><keyname>Berwald</keyname><forenames>Jesse</forenames></author><author><keyname>Gidea</keyname><forenames>Marian</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Automatic recognition and tagging of topologically different regimes in
  dynamical systems</title><categories>cs.CG cs.LG math.DS nlin.CD physics.data-an</categories><msc-class>37M10, 55U99, 37M20, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems are commonly modeled using nonlinear dynamical systems. These
models are often high-dimensional and chaotic. An important goal in studying
physical systems through the lens of mathematical models is to determine when
the system undergoes changes in qualitative behavior. A detailed description of
the dynamics can be difficult or impossible to obtain for high-dimensional and
chaotic systems. Therefore, a more sensible goal is to recognize and mark
transitions of a system between qualitatively different regimes of behavior. In
practice, one is interested in developing techniques for detection of such
transitions from sparse observations, possibly contaminated by noise. In this
paper we develop a framework to accurately tag different regimes of complex
systems based on topological features. In particular, our framework works with
a high degree of success in picking out a cyclically orbiting regime from a
stationary equilibrium regime in high-dimensional stochastic dynamical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2483</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2483</id><created>2013-12-09</created><updated>2014-03-24</updated><authors><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author><author><keyname>K&#xfc;nzler</keyname><forenames>Robin</forenames></author></authors><title>A Protocol for Generating Random Elements with their Probabilities</title><categories>cs.CC cs.CR</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an AM protocol that allows the verifier to sample elements x from a
probability distribution P, which is held by the prover. If the prover is
honest, the verifier outputs (x, P(x)) with probability close to P(x). In case
the prover is dishonest, one may hope for the following guarantee: if the
verifier outputs (x, p), then the probability that the verifier outputs x is
close to p. Simple examples show that this cannot be achieved. Instead, we show
that the following weaker condition holds (in a well defined sense) on average:
If (x, p) is output, then p is an upper bound on the probability that x is
output. Our protocol yields a new transformation to turn interactive proofs
where the verifier uses private random coins into proofs with public coins. The
verifier has better running time compared to the well-known Goldwasser-Sipser
transformation (STOC, 1986). For constant-round protocols, we only lose an
arbitrarily small constant in soundness and completeness, while our public-coin
verifier calls the private-coin verifier only once.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2490</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2490</id><created>2013-12-09</created><updated>2014-03-24</updated><authors><author><keyname>Holenstein</keyname><forenames>Thomas</forenames></author><author><keyname>K&#xfc;nzler</keyname><forenames>Robin</forenames></author></authors><title>A New View on Worst-Case to Average-Case Reductions for NP Problems</title><categories>cs.CC cs.CR</categories><acm-class>F.0; E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the result by Bogdanov and Trevisan (FOCS, 2003), who show that
under reasonable assumptions, there is no non-adaptive worst-case to
average-case reduction that bases the average-case hardness of an NP-problem on
the worst-case complexity of an NP-complete problem. We replace the hiding and
the heavy samples protocol in [BT03] by employing the histogram verification
protocol of Haitner, Mahmoody and Xiao (CCC, 2010), which proves to be very
useful in this context. Once the histogram is verified, our hiding protocol is
directly public-coin, whereas the intuition behind the original protocol
inherently relies on private coins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2496</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2496</id><created>2013-12-09</created><updated>2014-04-02</updated><authors><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>On the hardness of classically simulating the one clean qubit model</title><categories>quant-ph cond-mat.stat-mech cond-mat.str-el cs.CC</categories><comments>5 pages, 4 figures</comments><journal-ref>Phys. Rev. Lett. 112, 130502 (2014)</journal-ref><doi>10.1103/PhysRevLett.112.130502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic quantum computation with one quantum bit (DQC1) is a model of
quantum computing where the input restricted to containing a single qubit in a
pure state and with all other qubits in a completely-mixed state, with only a
single qubit measurement at the end of the computation [E. Knill and R.
Laflamme, Phys. Rev. Lett. {\bf81}, 5672 (1998)]. While it is known that DQC1
can efficiently solve several problems for which no known classical efficient
algorithms exist, the question of whether DQC1 is really more powerful than
classical computation remains open. In this paper, we introduce a slightly
modified version of DQC1, which we call DQC1$_k$, where $k$ output qubits are
measured, and show that DQC1$_k$ cannot be classically efficiently simulated
for any $k\geq3$ unless the polynomial hierarchy collapses at the third level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2501</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2501</id><created>2013-12-09</created><authors><author><keyname>Wimmer</keyname><forenames>Martin</forenames></author><author><keyname>Cederman</keyname><forenames>Daniel</forenames></author><author><keyname>Versaci</keyname><forenames>Francesco</forenames></author><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>Data Structures for Task-based Priority Scheduling</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many task-parallel applications can benefit from attempting to execute tasks
in a specific order, as for instance indicated by priorities associated with
the tasks. We present three lock-free data structures for priority scheduling
with different trade-offs on scalability and ordering guarantees. First we
propose a basic extension to work-stealing that provides good scalability, but
cannot provide any guarantees for task-ordering in-between threads. Next, we
present a centralized priority data structure based on $k$-fifo queues, which
provides strong (but still relaxed with regard to a sequential specification)
guarantees. The parameter $k$ allows to dynamically configure the trade-off
between scalability and the required ordering guarantee. Third, and finally, we
combine both data structures into a hybrid, $k$-priority data structure, which
provides scalability similar to the work-stealing based approach for larger
$k$, while giving strong ordering guarantees for smaller $k$. We argue for
using the hybrid data structure as the best compromise for generic,
priority-based task-scheduling.
  We analyze the behavior and trade-offs of our data structures in the context
of a simple parallelization of Dijkstra's single-source shortest path
algorithm. Our theoretical analysis and simulations show that both the
centralized and the hybrid $k$-priority based data structures can give strong
guarantees on the useful work performed by the parallel Dijkstra algorithm. We
support our results with experimental evidence on an 80-core Intel Xeon system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2502</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2502</id><created>2013-12-09</created><updated>2015-03-26</updated><authors><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>M&#xf6;mke</keyname><forenames>Tobias</forenames></author></authors><title>Improved integrality gap upper bounds for TSP with distances one and two</title><categories>cs.DS</categories><comments>36 pages, 12 figures</comments><msc-class>90C05, 68W25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of solutions to linear programming formulations for
the traveling salesperson problem (TSP).
  We perform a detailed analysis of the support of the subtour elimination
linear programming relaxation, which leads to algorithms that find 2-matchings
with few components in polynomial time. The number of components directly leads
to integrality gap upper bounds for the TSP with distances one and two, for
both undirected and directed graphs.
  Our main results concern the subtour elimination relaxation with one
additional cutting plane inequality:
  - For undirected instances we obtain an integrality gap upper bound of 5/4
without any further restrictions, of 7/6 if the optimal LP solution is
half-integral.
  - For instances of order n where the fractional LP value has a cost of n, we
obtain a tight integrality gap upper bound of 10/9 if there is an optimal
solution with subcubic support graph. The latter property that the graph is
subcubic is implied if the solution is a basic solution in the fractional
2-matching polytope.
  - For directed instances we obtain an integrality gap upper bound of 3/2, and
of 4/3 if given an optimal 1/2-integral solution. In the case of undirected
graphs, we can avoid to add the cutting plane inequality if we accept slightly
increased values. For the tight result, the cutting plane is not required.
  Additionally, we show that relying on the structure of the support is not an
artefact of our algorithm, but is necessary under standard complexity-theoretic
assumptions: we show that finding improved solutions via local search is
W[1]-hard for k-edge change neighborhoods even for the TSP with distances one
and two, which strengthens a result of D\'aniel Marx.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2506</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2506</id><created>2013-12-09</created><authors><author><keyname>Inclezan</keyname><forenames>Daniela</forenames></author></authors><title>An Application of Answer Set Programming to the Field of Second Language
  Acquisition</title><categories>cs.AI</categories><comments>17 pages, 3 tables, to appear in Theory and Practice of Logic
  Programming (TPLP)</comments><doi>10.1017/S1471068413000653</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the contributions of Answer Set Programming (ASP) to the
study of an established theory from the field of Second Language Acquisition:
Input Processing. The theory describes default strategies that learners of a
second language use in extracting meaning out of a text, based on their
knowledge of the second language and their background knowledge about the
world. We formalized this theory in ASP, and as a result we were able to
determine opportunities for refining its natural language description, as well
as directions for future theory development. We applied our model to automating
the prediction of how learners of English would interpret sentences containing
the passive voice. We present a system, PIas, that uses these predictions to
assist language instructors in designing teaching materials. To appear in
Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2526</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2526</id><created>2013-12-09</created><authors><author><keyname>Mehta</keyname><forenames>Vaibhav Kumar</forenames></author><author><keyname>Arrichiello</keyname><forenames>Filippo</forenames></author></authors><title>Connectivity maintenance by robotic Mobile Ad-hoc NETwork</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maintaining a wireless communication link between a fixed base
station and an autonomous agent by means of a team of mobile robots is
addressed in this work. Such problem can be of interest for search and rescue
missions in post disaster scenario where the autonomous agent can be used for
remote monitoring and first hand knowledge of the aftermath, while the mobile
robots can be used to provide the agent the possibility to dynamically send its
collected information to an external base station. To study the problem, a
distributed multi-robot system with wifi communication capabilities has been
developed and used to implement a Mobile Ad-hoc NETwork (MANET) to guarantee
the required multi-hop communication. None of the robots of the team possess
the knowledge of agent's movement, neither they hold a pre-assigned position in
the ad-hoc network but they adapt with respect to the dynamic environmental
situations. This adaptation only requires the robots to have the knowledge of
their position and the possibility to exchange such information with their
one-hop neighbours. Robots' motion is achieved by implementing a behavioural
control, namely the Null-Space based Behavioural control, embedding the
collective mission to achieve the required self-configuration. Validation of
the approach is performed by means of demanding experimental tests involving
five ground mobile robots capable of self localization and dynamic obstacle
avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2530</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2530</id><created>2013-12-06</created><authors><author><keyname>Owens</keyname><forenames>Trevor</forenames></author></authors><title>Mr. Moo's First RPG: Rules, Discussion and the Instructional
  Implications of Collective Intelligence on the Open Web</title><categories>cs.CY</categories><comments>Published as a chapter in Rhetoric/Composition/Play: How Electronic
  Games Mediate Composition Theory and Practice (and Vice Versa). (2013) New
  York, NY: Palgrave Macmillan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a moment, imagine an active online learning community of writers,
artists, and designers, many spending more than eight hours a week composing
projects. In this community, young people, primarily between the ages of 18-26,
regularly critique, facilitate, and support each other in their composition
activities. They are motivated to participate by their shared interest and
affinity for their creative work. In the age of Wikipedia, this might not seem
particularly novel, but what I am actually describing is an online discussion
board, RPGmakerVX.net. Elsewhere, I have presented a general outline of the
kinds of individuals involved in this community and the way that the site as a
whole functions as an open learning environment (Owens, 2010). In this essay, I
present a case study of one participant in this community. His user name is Mr.
Moo, and at the time I interviewed him, he was a 19 year old college student
from Calgary, Canada. When he created his first role-playing game, Prelude of
Identity, he was eighteen. After providing a conceptual context for this case
study in work on collective intelligence, I draw out the relationship between
the technical system of the discussion boards and the creative process of
engaging with peers in the production of a video game. I suggest that the
discussion board rules and interaction enable a dialogue around composition
that ultimately leaves Mr. Moo with a valuable learning experience while also
producing a role-playing game. Thinking about this system from the perspective
of collective intelligence enables us to use these kinds of interest-driven,
online affinity communities as tools in an open education tool kit for
educators in more formal learning environments. Ultimately, discussion boards
in gaming communities, both the technical and social systems they represent,
could be thought of as instructional tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2539</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2539</id><created>2013-12-09</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>A Key Set Cipher for Wireless Sensor Networks</title><categories>cs.CR</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the use of sets of keys, together with corresponding
identifiers, for use in wireless sensor networks and similar
resource-constrained applications. A specific cryptographic scheme described in
the paper is based on the use of a family of self-inverting matrices derived
from the number theoretic Hilbert transform in conjunction with the Blom's
scheme. In a randomized version of this scheme, the users change their
published IDs at will but the parties can still reach agreement on the key by
using individual scaling factors. The random protocol increases the security of
the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2544</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2544</id><created>2013-12-09</created><updated>2014-07-30</updated><authors><author><keyname>Moritz</keyname><forenames>Guilherme Luiz</forenames></author><author><keyname>Rebelatto</keyname><forenames>Jo&#xe3;o Luiz</forenames></author><author><keyname>Souza</keyname><forenames>Richard Demo</forenames></author><author><keyname>Uch&#xf4;a-Filho</keyname><forenames>Bartolomeu F.</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author></authors><title>Time-Switching Uplink Network-Coded Cooperative Communication with
  Downlink Energy Transfer</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. Signal Process., to be published</comments><doi>10.1109/TSP.2014.2345332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a multiuser cooperative wireless network where the
energy-constrained sources have independent information to transmit to a common
destination, which is assumed to be externally powered and responsible for
transferring energy wirelessly to the sources. The source nodes may cooperate,
under either decode-and-forward or network coding-based protocols. Taking into
account the fact that the energy harvested by the source nodes is a function of
the fading realization of inter-user channels and user-destination channels, we
obtain a closed-form approximation for the system outage probability, as well
as an approximation for the optimal energy transfer period that minimizes such
outage probability. It is also shown that, even though the achievable diversity
order is reduced due to wireless energy transfer process, it is very close to
the one achieved for a network without energy constraints. Numerical results
are also presented to validate the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2549</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2549</id><created>2013-12-09</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Jiang</keyname><forenames>Minghui</forenames></author></authors><title>On the approximability of covering points by lines and related problems</title><categories>cs.CG</categories><comments>20 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ points in the plane, {\sc Covering Points by Lines} is
the problem of finding a minimum-cardinality set $\L$ of lines such that every
point $p \in P$ is incident to some line $\ell \in \L$. As a geometric variant
of {\sc Set Cover}, {\sc Covering Points by Lines} is still NP-hard. Moreover,
it has been proved to be APX-hard, and hence does not admit any polynomial-time
approximation scheme unless P $=$ NP\@. In contrast to the small constant
approximation lower bound implied by APX-hardness, the current best
approximation ratio for {\sc Covering Points by Lines} is still $O(\log n)$,
namely the ratio achieved by the greedy algorithm for {\sc Set Cover}.
  In this paper, we give a lower bound of $\Omega(\log n)$ on the approximation
ratio of the greedy algorithm for {\sc Covering Points by Lines}. We also study
several related problems including {\sc Maximum Point Coverage by Lines}, {\sc
Minimum-Link Covering Tour}, {\sc Minimum-Link Spanning Tour}, and {\sc
Min-Max-Turn Hamiltonian Tour}. We show that all these problems are either
APX-hard or at least NP-hard. In particular, our proof of APX-hardness of {\sc
Min-Max-Turn Hamiltonian Tour} sheds light on the difficulty of {\sc
Bounded-Turn-Minimum-Length Hamiltonian Tour}, a problem proposed by Aggarwal
et al.\ at SODA 1997.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2551</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2551</id><created>2013-12-09</created><updated>2015-11-17</updated><authors><author><keyname>Lesnik</keyname><forenames>Dmitry</forenames></author><author><keyname>Schaefer</keyname><forenames>Tobias</forenames></author></authors><title>A state vector algebra for algorithmic implementation of second-order
  logic</title><categories>cs.AI cs.LO</categories><comments>This paper has been withdrawn by the author due to numerous errors
  found</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mathematical framework for mapping second-order logic relations
onto a simple state vector algebra. Using this algebra, basic theorems of set
theory can be proven in an algorithmic way, hence by an expert system. We
illustrate the use of the algebra with simple examples and show that, in
principle, all theorems of basic set theory can be recovered in an elementary
way. The developed technique can be used for an automated theorem proving in
the 1st and 2nd order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2552</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2552</id><created>2013-12-09</created><authors><author><keyname>Falaschi</keyname><forenames>Moreno</forenames></author><author><keyname>Olarte</keyname><forenames>Carlos</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>Abstract Interpretation of Temporal Concurrent Constraint Programs</title><categories>cs.LO</categories><acm-class>F.3.1; D.3.2</acm-class><journal-ref>Theory and Practice of Logic Programming, 15(3), 312-357, 2015</journal-ref><doi>10.1017/S1471068413000641</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Timed Concurrent Constraint Programming (tcc) is a declarative model for
concurrency offering a logic for specifying reactive systems, i.e. systems that
continuously interact with the environment. The universal tcc formalism (utcc)
is an extension of tcc with the ability to express mobility. Here mobility is
understood as communication of private names as typically done for mobile
systems and security protocols. In this paper we consider the denotational
semantics for tcc, and we extend it to a &quot;collecting&quot; semantics for utcc based
on closure operators over sequences of constraints. Relying on this semantics,
we formalize a general framework for data flow analyses of tcc and utcc
programs by abstract interpretation techniques. The concrete and abstract
semantics we propose are compositional, thus allowing us to reduce the
complexity of data flow analyses. We show that our method is sound and
parametric with respect to the abstract domain. Thus, different analyses can be
performed by instantiating the framework. We illustrate how it is possible to
reuse abstract domains previously defined for logic programming to perform, for
instance, a groundness analysis for tcc programs. We show the applicability of
this analysis in the context of reactive systems. Furthermore, we make also use
of the abstract semantics to exhibit a secrecy flaw in a security protocol. We
also show how it is possible to make an analysis which may show that tcc
programs are suspension free. This can be useful for several purposes, such as
for optimizing compilation or for debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2570</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2570</id><created>2013-12-09</created><authors><author><keyname>Sedjelmaci</keyname><forenames>Sidi Mohamed</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>A New Modular Division Algorithm and Applications</title><categories>cs.DC cs.DM</categories><comments>12 pages</comments><proxy>ccsd</proxy><journal-ref>International Conference on Theoretical Computer Science
  (ICTCS98), Pisa : Italy (1998)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper proposes a new parallel algorithm for the modular division
$u/v\bmod \beta^s$, where $u,\; v,\; \beta$ and $s$ are positive integers
$(\beta\ge 2)$. The algorithm combines the classical add-and-shift
multiplication scheme with a new propagation carry technique. This &quot;Pen and
Paper Inverse&quot; ({\em PPI}) algorithm, is better suited for systolic
parallelization in a &quot;least-significant digit first&quot; pipelined manner. Although
it is equivalent to Jebelean's modular division algorithm~\cite{jeb2} in terms
of performance (time complexity, work, efficiency), the linear parallelization
of the {\em PPI} algorithm improves on the latter when the input size is large.
The parallelized versions of the {\em PPI} algorithm leads to various
applications, such as the exact division and the digit modulus operation (dmod)
of two long integers. It is also applied to the determination of the periods of
rational numbers as well as their $p$-adic expansion in any radix $\beta \ge
2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2574</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2574</id><created>2013-12-09</created><updated>2014-10-25</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Backing off from Infinity: Performance Bounds via Concentration of
  Spectral Measure for Random MIMO Channels</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>accepted to IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 61, No. 1, pp.
  366-387, January 2015</journal-ref><doi>10.1109/TIT.2014.2365497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance analysis of random vector channels, particularly
multiple-input-multiple-output (MIMO) channels, has largely been established in
the asymptotic regime of large channel dimensions, due to the analytical
intractability of characterizing the exact distribution of the objective
performance metrics. This paper exposes a new non-asymptotic framework that
allows the characterization of many canonical MIMO system performance metrics
to within a narrow interval under moderate-to-large channel dimensionality,
provided that these metrics can be expressed as a separable function of the
singular values of the matrix. The effectiveness of our framework is
illustrated through two canonical examples. Specifically, we characterize the
mutual information and power offset of random MIMO channels, as well as the
minimum mean squared estimation error of MIMO channel inputs from the channel
outputs. Our results lead to simple, informative, and reasonably accurate
control of various performance metrics in the finite-dimensional regime, as
corroborated by the numerical simulations. Our analysis framework is
established via the concentration of spectral measure phenomenon for random
matrices uncovered by Guionnet and Zeitouni, which arises in a variety of
random matrix ensembles irrespective of the precise distributions of the matrix
entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2578</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2578</id><created>2013-12-09</created><updated>2014-04-28</updated><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>Kernel-based Distance Metric Learning in the Output Space</title><categories>cs.LG</categories><comments>11 pages, 7 figures, appeared in the Proceedings of 2013
  International Joint Conference on Neural Networks (IJCNN)</comments><doi>10.1109/IJCNN.2013.6706862</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present two related, kernel-based Distance Metric Learning
(DML) methods. Their respective models non-linearly map data from their
original space to an output space, and subsequent distance measurements are
performed in the output space via a Mahalanobis metric. The dimensionality of
the output space can be directly controlled to facilitate the learning of a
low-rank metric. Both methods allow for simultaneous inference of the
associated metric and the mapping to the output space, which can be used to
visualize the data, when the output space is 2- or 3-dimensional. Experimental
results for a collection of classification tasks illustrate the advantages of
the proposed methods over other traditional and kernel-based DML approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2581</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2581</id><created>2013-12-08</created><authors><author><keyname>Lazzez</keyname><forenames>Amor</forenames></author><author><keyname>Slimani</keyname><forenames>Thabet</forenames></author></authors><title>Deployment of VoIP Technology: QoS Concerns</title><categories>cs.NI</categories><comments>8 pages, 3 figures</comments><journal-ref>International Journal of Advanced Research in Computer and
  Communication Engineering (IJARCCE), Vol. 2, Issue 9, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over IP (VoIP) is an emerging communication service allowing voice
transmission over a private or a public IP network. VoIP allows significant
benefits for customers and service providers including cost savings, phone and
service portability, mobility, and the integration with other applications.
Nevertheless, the deployment of the VoIP technology encounters many challenges
such as interoperability issues, security issues, and QoS concerns. Among these
disadvantages, QoS issues are considered the most serious due to the QoS
problems that may arise on IP networks, and the stringent QoS requirements of
voice traffic. The aim of this paper is carry out a deep analysis of the QoS
concerns of the VoIP technology. Firstly, we present a brief overview about the
VoIP technology. Then, we discuss the QoS issues related to the use of the IP
networking technology for voice traffic transmission. After that, we present
the QoS concerns related voice clarity. Finally, we present the QoS mechanisms
proposed to make the IP technology able to support voice traffic QoS
requirements in terms of voice clarity, voice packet delay, packet delay
variation, and packet loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2585</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2585</id><created>2013-12-09</created><authors><author><keyname>Nurassyl</keyname><forenames>Kerimbayev</forenames></author><author><keyname>Aliya</keyname><forenames>Akramova</forenames></author><author><keyname>Jarkynbike</keyname><forenames>Suleimenova</forenames></author></authors><title>E-learning for ungraded schools of Kazakhstan: experience,
  implementation, and innovation</title><categories>cs.CY</categories><comments>6 pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modernization of the educational process in the ungraded schools of the
Republic of Kazakhstan requires the provision of affordable quality education
for students in rural areas on information technology, the creation of
e-learning. It was important to consider two points: how does e-learning
influence the educational process in ungraded schools, and directly on the
quality of teaching, and what results can thus be achieved. The significance of
the work is also to explore innovative approaches to e-learning system of
ungraded schools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2598</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2598</id><created>2013-12-09</created><updated>2014-02-18</updated><authors><author><keyname>Ramirez</keyname><forenames>Lina</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author></authors><title>Monitoring voltage collapse margin by measuring the area voltage across
  several transmission lines with synchrophasors</title><categories>cs.SY</categories><comments>IEEE Power and Energy Society General Meeting, July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fast monitoring of voltage collapse margin using
synchrophasor measurements at both ends of transmission lines that transfer
power from two generators to two loads. This shows a way to extend the
monitoring of a radial transmission line to multiple transmission lines. The
synchrophasor voltages are combined into a single complex voltage difference
across an area containing the transmission lines that can be monitored in the
same way as a single transmission line. We identify ideal conditions under
which this reduction to the single line case perfectly preserves the margin to
voltage collapse, and give an example that shows that the error under practical
non-ideal conditions is reasonably small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2606</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2606</id><created>2013-12-09</created><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Georgiopoulos</keyname><forenames>Michael</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Georgios C.</forenames></author></authors><title>Multi-Task Classification Hypothesis Space with Improved Generalization
  Bounds</title><categories>cs.LG</categories><comments>18 pages, 4 figures, submitted to IEEE Transactions on Neural
  Networks and Learning Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a RKHS, in general, of vector-valued functions intended
to be used as hypothesis space for multi-task classification. It extends
similar hypothesis spaces that have previously considered in the literature.
Assuming this space, an improved Empirical Rademacher Complexity-based
generalization bound is derived. The analysis is itself extended to an MKL
setting. The connection between the proposed hypothesis space and a Group-Lasso
type regularizer is discussed. Finally, experimental results, with some
SVM-based Multi-Task Learning problems, underline the quality of the derived
bounds and validate the paper's analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2612</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2612</id><created>2013-12-09</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L</forenames></author></authors><title>A New Variable Step-size Zero-point Attracting Projection Algorithm</title><categories>cs.OH</categories><comments>5 pages, Asilomar 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new variable step-size (VSS) scheme for the recently
introduced zero-point attracting projection (ZAP) algorithm. The proposed
variable step-size ZAPs are based on the gradient of the estimated filter
coefficients sparseness that is approximated by the difference between the
sparseness measure of current filter coefficients and an averaged sparseness
measure. Simulation results demonstrate that the proposed approach provides
both faster convergence rate and better tracking ability than previous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2625</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2625</id><created>2013-12-09</created><authors><author><keyname>Thompson</keyname><forenames>Carlton A.</forenames></author><author><keyname>Latchman</keyname><forenames>Haniph A.</forenames></author><author><keyname>Angelacos</keyname><forenames>Nathan</forenames></author><author><keyname>Pareek</keyname><forenames>Bharath Kumar</forenames></author></authors><title>A Distributed IP-Based Telecommunication System using SIP</title><categories>cs.NI</categories><comments>16 pages, 10 figures</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.5, No.6, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over Internet Protocol (VoIP) technologies are integral to modern
telecommunications because of their advanced features, flexibility, and
economic benefits. Internet Service Providers initially promoted these
technologies by providing low cost local and international calling. At present,
there is also a great deal of interest in using IP-based technologies to
replace traditional small and large office telephone systems that use
traditional PBXs (Private Branch eXchange). Unfortunately, the large majority
of the emerging VoIP based office telephone systems have followed the
centralized design of traditional public and private telephone systems in which
all the intelligence in the system is at the core, with quite expensive
hardware and software components and appropriate redundancy for adequate levels
of reliability. In this paper, it is argued that a centralized model for an
IP-based telecommunications system fails to exploit the full capabilities of
Internet-inspired communications and that, very simple, inexpensive, elegant
and flexible solutions are possible by deliberately avoiding the centralized
approach. This paper describes the design, philosophy and implementation of a
prototype for a fully distributed IP-based Telecommunication System (IPTS) that
provides the essential feature set for office and home telecommunications,
including IP-based long-distance and local calling, and with the support for
video as well as data and text. The prototype system was implemented with an
Internet-inspired distributed design using open source software, with
appropriate customizations and configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2627</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2627</id><created>2013-12-09</created><authors><author><keyname>Flagg</keyname><forenames>Garret</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author></authors><title>Multipoint Volterra Series Interpolation and H2 Optimal Model Reduction
  of Bilinear Systems</title><categories>math.NA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on model reduction of large-scale bilinear systems.
The main contributions are threefold. First, we introduce a new framework for
interpolatory model reduction of bilinear systems. In contrast to the existing
methods where interpolation is forced on some of the leading subsystem transfer
functions, the new framework shows how to enforce multipoint interpolation of
the underlying Volterra series. Then, we show that the first-order conditions
for optimal H2 model reduction of bilinear systems require multivariate Hermite
interpolation in terms of the new Volterra series interpolation framework; and
thus we extend the interpolation-based first-order necessary conditions for H2
optimality of LTI systems to the bilinear case. Finally, we show that
multipoint interpolation on the truncated Volterra series representation of a
bilinear system leads to an asymptotically optimal approach to H2 optimal model
reduction, leading to an efficient model reduction algorithm. Several numerical
examples illustrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2628</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2628</id><created>2013-12-09</created><authors><author><keyname>Umar</keyname><forenames>Ibrahim</forenames><affiliation>University of Troms&#xf8;</affiliation></author><author><keyname>Anshus</keyname><forenames>Otto</forenames><affiliation>University of Troms&#xf8;</affiliation></author><author><keyname>Ha</keyname><forenames>Phuong</forenames><affiliation>University of Troms&#xf8;</affiliation></author></authors><title>DeltaTree: A Practical Locality-aware Concurrent Search Tree</title><categories>cs.DC</categories><report-no>IFI-UIT Technical Report 2013-74</report-no><msc-class>68W10</msc-class><acm-class>D.1.3; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As other fundamental programming abstractions in energy-efficient computing,
search trees are expected to support both high parallelism and data locality.
However, existing highly-concurrent search trees such as red-black trees and
AVL trees do not consider data locality while existing locality-aware search
trees such as those based on the van Emde Boas layout (vEB-based trees), poorly
support concurrent (update) operations.
  This paper presents DeltaTree, a practical locality-aware concurrent search
tree that combines both locality-optimisation techniques from vEB-based trees
and concurrency-optimisation techniques from non-blocking highly-concurrent
search trees. DeltaTree is a $k$-ary leaf-oriented tree of DeltaNodes in which
each DeltaNode is a size-fixed tree-container with the van Emde Boas layout.
The expected memory transfer costs of DeltaTree's Search, Insert, and Delete
operations are $O(\log_B N)$, where $N, B$ are the tree size and the unknown
memory block size in the ideal cache model, respectively. DeltaTree's Search
operation is wait-free, providing prioritised lanes for Search operations, the
dominant operation in search trees. Its Insert and {\em Delete} operations are
non-blocking to other Search, Insert, and Delete operations, but they may be
occasionally blocked by maintenance operations that are sometimes triggered to
keep DeltaTree in good shape. Our experimental evaluation using the latest
implementation of AVL, red-black, and speculation friendly trees from the
Synchrobench benchmark has shown that DeltaTree is up to 5 times faster than
all of the three concurrent search trees for searching operations and up to 1.6
times faster for update operations when the update contention is not too high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2629</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2629</id><created>2013-12-09</created><authors><author><keyname>Wang</keyname><forenames>Yongcai</forenames></author><author><keyname>Feng</keyname><forenames>Haoran</forenames></author><author><keyname>Xi</keyname><forenames>Xiangyu</forenames></author></authors><title>Sense, Model and Identify the Load Signatures of HVAC Systems in Metro
  Stations</title><categories>cs.SY</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The HVAC systems in subway stations are energy consuming giants, each of
which may consume over 10, 000 Kilowatts per day for cooling and ventilation.
To save energy for the HVAC systems, it is critically important to firstly know
the &quot;load signatures&quot; of the HVAC system, i.e., the quantity of heat imported
from the outdoor environments and by the passengers respectively in different
periods of a day, which will significantly benefit the design of control
policies. In this paper, we present a novel sensing and learning approach to
identify the load signature of the HVAC system in the subway stations. In
particular, sensors and smart meters were deployed to monitor the indoor,
outdoor temperatures, and the energy consumptions of the HVAC system in
real-time. The number of passengers was counted by the ticket checking system.
At the same time, the cooling supply provided by the HVAC system was inferred
via the energy consumption logs of the HVAC system. Since the indoor
temperature variations are driven by the difference of the loads and the
cooling supply, linear regression model was proposed for the load signature,
whose coefficients are derived via a proposed algorithm . We collected real
sensing data and energy log data from HaiDianHuangZhuang Subway station, which
is in line 4 of Beijing from the duration of July 2012 to Sept. 2012. The data
was used to evaluate the coefficients of the regression model. The experiment
results show typical variation signatures of the loads from the passengers and
from the outdoor environments respectively, which provide important contexts
for smart control policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2631</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2631</id><created>2013-12-09</created><authors><author><keyname>Memon</keyname><forenames>Abdul Basit</forenames></author><author><keyname>Verriest</keyname><forenames>Erik I.</forenames></author></authors><title>Kernel representation approach to persistence of behavior</title><categories>math.OC cs.SY</categories><comments>Submitted to the 19th IFAC World Congress 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal control problem of connecting any two trajectories in a behavior
B with maximal persistence of that behavior is put forth and a compact solution
is obtained for a general class of behaviors. The behavior B is understood in
the context of Willems's behavioral theory and its representation is given by
the kernel of some operator. In general the solution to the problem will not
lie in the same behavior and so a maximally persistent solution is defined as
one that will be as close as possible to the behavior. A vast number of
behaviors can be treated in this framework such as stationary solutions, limit
cycles etc. The problem is linked to the ideas of controllability presented by
Willems. It draws its roots from quasi-static transitions in thermodynamics and
bears connections to morphing theory. The problem has practical applications in
finite time thermodynamics, deployment of tensigrity structures and legged
locomotion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2632</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2632</id><created>2013-12-09</created><authors><author><keyname>Wang</keyname><forenames>Yongcai</forenames></author><author><keyname>Feng</keyname><forenames>Haoran</forenames></author><author><keyname>Qi</keyname><forenames>Xiao</forenames></author></authors><title>SEED: Public Energy and Environment Dataset for Optimizing HVAC
  Operation in Subway Stations</title><categories>cs.SY</categories><comments>5 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For sustainability and energy saving, the problem to optimize the control of
heating, ventilating, and air-conditioning (HVAC) systems has attracted great
attentions, but analyzing the signatures of thermal environments and HVAC
systems and the evaluation of the optimization policies has encountered
inefficiency and inconvenient problems due to the lack of public dataset. In
this paper, we present the Subway station Energy and Environment Dataset
(SEED), which was collected from a line of Beijing subway stations, providing
minute-resolution data regarding the environment dynamics (temperature,
humidity, CO2, etc.) working states and energy consumptions of the HVAC systems
(ventilators, refrigerators, pumps), and hour-resolution data of passenger
flows. We describe the sensor deployments and the HVAC systems for data
collection and for environment control, and also present initial investigation
for the energy disaggregation of HVAC system, the signatures of the thermal
load, cooling supply, and the passenger flow using the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2637</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2637</id><created>2013-12-09</created><updated>2015-07-28</updated><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>The Throughput-Outage Tradeoff of Wireless One-Hop Caching Networks</title><categories>cs.IT math.IT</categories><comments>58 pages, 8 figures, Revised version of the manuscript submitted to
  IEEE Transactions on Information Theory, This is the extended version of the
  conference (ISIT) paper arXiv:1302.2168</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless device-to-device (D2D) network where the nodes have
pre-cached information from a library of available files. Nodes request files
at random. If the requested file is not in the on-board cache, then it is
downloaded from some neighboring node via one-hop &quot;local&quot; communication. An
outage event occurs when a requested file is not found in the neighborhood of
the requesting node, or if the network admission control policy decides not to
serve the request. We characterize the optimal throughput-outage tradeoff in
terms of tight scaling laws for various regimes of the system parameters, when
both the number of nodes and the number of files in the library grow to
infinity. Our analysis is based on Gupta and Kumar {\em protocol model} for the
underlying D2D wireless network, widely used in the literature on capacity
scaling laws of wireless networks without caching. Our results show that the
combination of D2D spectrum reuse and caching at the user nodes yields a
per-user throughput independent of the number of users, for any fixed outage
probability in $(0,1)$. This implies that the D2D caching network is
&quot;scalable&quot;: even though the number of users increases, each user achieves
constant throughput. This behavior is very different from the classical Gupta
and Kumar result on ad-hoc wireless networks, for which the per-user throughput
vanishes as the number of users increases. Furthermore, we show that the user
throughput is directly proportional to the fraction of cached information over
the whole file library size. Therefore, we can conclude that D2D caching
networks can turn &quot;memory&quot; into &quot;bandwidth&quot; (i.e., doubling the on-board cache
memory on the user devices yields a 100\% increase of the user throughout).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2641</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2641</id><created>2013-12-09</created><authors><author><keyname>Shin</keyname><forenames>Wiroy</forenames></author></authors><title>Simultaneous auctions for complementary goods</title><categories>q-fin.TR cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies an environment of simultaneous, separate, first-price
auctions for complementary goods. Agents observe private values of each good
before making bids, and the complementarity between goods is explicitly
incorporated in their utility. For simplicity, a model is presented with two
first-price auctions and two bidders. We show that a monotone pure-strategy
Bayesian Nash Equilibrium exists in the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2642</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2642</id><created>2013-12-09</created><authors><author><keyname>Sree</keyname><forenames>P. Kiran</forenames></author><author><keyname>Raju</keyname><forenames>G. V. S.</forenames></author><author><keyname>Raju</keyname><forenames>S. Viswandha</forenames></author><author><keyname>Devi</keyname><forenames>N. S. S. S. N Usha</forenames></author></authors><title>Cellular Automata based Feedback Mechanism in Strengthening biological
  Sequence Analysis Approach to Robotic Soccer</title><categories>cs.MA cs.RO</categories><journal-ref>Artificial Intelligence and Machine Learning Journal(ICGST-AIML),
  Volume 8, Issue I, June 2008,pp 29-37</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the application of sequence analysis algorithms for
agents in robotic soccer and a suitable representation is proposed to achieve
this mapping. The objective of this research is to generate novel better
in-game strategies with the aim of faster adaptation to the changing
environment. A homogeneous non-communicating multi-agent architecture using the
representation is presented. To achieve real-time learning during a game, a
bucket brigade algorithm is used to reinforce Cellular Automata Based
Classifier. A technique for selecting strategies based on sequence analysis is
adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2650</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2650</id><created>2013-12-09</created><updated>2014-05-12</updated><authors><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Author Impact Factor: tracking the dynamics of individual scientific
  impact</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>Published version. 6 pages, 5 figures + Appendix</comments><journal-ref>Sci. Rep. 4, 4880 (2014)</journal-ref><doi>10.1038/srep04880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact factor (IF) of scientific journals has acquired a major role in
the evaluations of the output of scholars, departments and whole institutions.
Typically papers appearing in journals with large values of the IF receive a
high weight in such evaluations. However, at the end of the day one is
interested in assessing the impact of individuals, rather than papers. Here we
introduce Author Impact Factor (AIF), which is the extension of the IF to
authors. The AIF of an author A in year $t$ is the average number of citations
given by papers published in year $t$ to papers published by A in a period of
$\Delta t$ years before year $t$. Due to its intrinsic dynamic character, AIF
is capable to capture trends and variations of the impact of the scientific
output of scholars in time, unlike the $h$-index, which is a growing measure
taking into account the whole career path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2658</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2658</id><created>2013-12-09</created><authors><author><keyname>Otsuki</keyname><forenames>Akira</forenames></author><author><keyname>Kawamura</keyname><forenames>Masayoshi</forenames></author></authors><title>The Study about the Analysis of Responsiveness Pair Clustering to Social
  Network Bipartite Graph</title><categories>cs.CY</categories><comments>14 pages, 8 figures, 3 tables</comments><journal-ref>Advanced Computing: An International Journal (ACIJ), Vol.4, No.6,
  November 2013</journal-ref><doi>10.5121/acij.2013.4601</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this study, regional (cities, towns and villages) data and tweet data are
obtained from Twitter, and extract information of purchase information (Where
and what bought) from the tweet data by morphological analysis and rule-based
dependency analysis. Then, the &quot;The regional information&quot; and &quot;The information
of purchase history (Where and what bought information)&quot; are captured as
bipartite graph, and Responsiveness Pair Clustering analysis (a clustering
using correspondence analysis as similarity measure) is conducted. In this
study, since it was found to be difficult to analyze a network such as
bipartite graph having limitations in links by using modularity Q,
responsiveness is used instead of modularity Q as similarity measure. As a
result of this analysis, &quot;regional information cluster&quot; which refers to similar
&quot;The information of purchase history&quot; nodes group is generated. Finally,
similar regions are visualized by mapping the regional information cluster on
the map. This visualization system is expected to contribute as an analytical
tool for customers purchasing behavior and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2668</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2668</id><created>2013-12-09</created><updated>2014-09-15</updated><authors><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Fisher</keyname><forenames>Michael W.</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Pan</keyname><forenames>Feng</forenames></author></authors><title>Optimal compression in natural gas networks: a geometric programming
  approach</title><categories>cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural gas transmission pipelines are complex systems whose flow
characteristics are governed by challenging non-linear physical behavior. These
pipelines extend over hundreds and even thousands of miles. Gas is typically
injected into the system at a constant rate, and a series of compressors are
distributed along the pipeline to boost the gas pressure to maintain system
pressure and throughput. These compressors consume a portion of the gas, and
one goal of the operator is to control the compressor operation to minimize
this consumption while satisfying pressure constraints at the gas load points.
The optimization of these operations is computationally challenging. Many
pipelines simply rely on the intuition and prior experience of operators to
make these decisions. Here, we present a new geometric programming approach for
optimizing compressor operation in natural gas pipelines. Using models of real
natural gas pipelines, we show that the geometric programming algorithm
consistently outperforms approaches that mimic existing state of practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2669</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2669</id><created>2013-12-10</created><authors><author><keyname>Vishwanath</keyname><forenames>R H</forenames></author><author><keyname>Samartha</keyname><forenames>T V</forenames></author><author><keyname>Srikantaiah</keyname><forenames>K C</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>DRSP : Dimension Reduction For Similarity Matching And Pruning Of Time
  Series Data Streams</title><categories>cs.DB</categories><comments>20 pages,8 figures, 6 Tables</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.3, No.6,pp.107-126, November 2013</journal-ref><doi>10.5121/ijdkp.2013.3607</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Similarity matching and join of time series data streams has gained a lot of
relevance in today's world that has large streaming data. This process finds
wide scale application in the areas of location tracking, sensor networks,
object positioning and monitoring to name a few. However, as the size of the
data stream increases, the cost involved to retain all the data in order to aid
the process of similarity matching also increases. We develop a novel framework
to addresses the following objectives. Firstly, Dimension reduction is
performed in the preprocessing stage, where large stream data is segmented and
reduced into a compact representation such that it retains all the crucial
information by a technique called Multi-level Segment Means (MSM). This reduces
the space complexity associated with the storage of large time-series data
streams. Secondly, it incorporates effective Similarity Matching technique to
analyze if the new data objects are symmetric to the existing data stream. And
finally, the Pruning Technique that filters out the pseudo data object pairs
and join only the relevant pairs. The computational cost for MSM is O(l*ni) and
the cost for pruning is O(DRF*wsize*d), where DRF is the Dimension Reduction
Factor. We have performed exhaustive experimental trials to show that the
proposed framework is both efficient and competent in comparison with earlier
works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2674</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2674</id><created>2013-12-10</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Schmit</keyname><forenames>Sven</forenames></author><author><keyname>Schreiber</keyname><forenames>Robert</forenames></author></authors><title>Silent error detection in numerical time-stepping schemes</title><categories>cs.NA cs.MS math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Errors due to hardware or low level software problems, if detected, can be
fixed by various schemes, such as recomputation from a checkpoint. Silent
errors are errors in application state that have escaped low-level error
detection. At extreme scale, where machines can perform astronomically many
operations per second, silent errors threaten the validity of computed results.
  We propose a new paradigm for detecting silent errors at the application
level. Our central idea is to frequently compare computed values to those
provided by a cheap checking computation, and to build error detectors based on
the difference between the two output sequences. Numerical analysis provides us
with usable checking computations for the solution of initial-value problems in
ODEs and PDEs, arguably the most common problems in computational science.
Here, we provide, optimize, and test methods based on Runge-Kutta and linear
multistep methods for ODEs, and on implicit and explicit finite difference
schemes for PDEs. We take the heat equation and Navier-Stokes equations as
examples. In tests with artificially injected errors, this approach effectively
detects almost all meaningful errors, without significant slowdown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2678</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2678</id><created>2013-12-10</created><authors><author><keyname>Sastry</keyname><forenames>S. Hanumanth</forenames></author><author><keyname>Babu</keyname><forenames>Prof. M. S. Prasada</forenames></author></authors><title>Analysis &amp; Prediction of Sales Data in SAP-ERP System using Clustering
  Algorithms</title><categories>cs.DB</categories><comments>AIRCC-IJCSITY Journal Publication.
  http://airccse.org/journal/ijcsity/Paper.html</comments><journal-ref>International Journal of Computational Science and Information
  Technology (IJCSITY) Vol.1, No.4, November 2013</journal-ref><doi>10.5121/ijcsity.2013.1407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an important data mining technique where we will be interested
in maximizing intracluster distance and also minimizing intercluster distance.
We have utilized clustering techniques for detecting deviation in product sales
and also to identify and compare sales over a particular period of time.
Clustering is suited to group items that seem to fall naturally together, when
there is no specified class for any new item. We have utilizedannual sales data
of a steel major to analyze Sales Volume &amp; Value with respect to dependent
attributes like products, customers and quantities sold. The demand for steel
products is cyclical and depends on many factors like customer profile,
price,Discounts and tax issues. In this paper, we have analyzed sales data with
clustering algorithms like K-Means&amp;EMwhichrevealed many interesting
patternsuseful for improving sales revenue and achieving higher sales volume.
Our study confirms that partition methods like K-Means &amp; EM algorithms are
better suited to analyze our sales data in comparison to Density based methods
like DBSCAN &amp; OPTICS or Hierarchical methods like COBWEB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2681</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2681</id><created>2013-12-10</created><authors><author><keyname>Sridharan</keyname><forenames>Gokul</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Degrees of Freedom of MIMO Cellular Networks: Decomposition and Linear
  Beamforming Design</title><categories>cs.IT math.IT</categories><comments>25 pages, 14 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the symmetric degrees of freedom (DoF) of MIMO
cellular networks with G cells and K users per cell, having N antennas at each
base station and M antennas at each user. In particular, we investigate
achievability techniques based on either decomposition with asymptotic
interference alignment (IA) or linear beamforming schemes, and show that there
are distinct regimes of (G,K,M,N) where one outperforms the other. We first
note that both one-sided and two-sided decomposition with asymptotic IA achieve
the same degrees of freedom. We then establish specific antenna configurations
under which the DoF achieved using decomposition based schemes is optimal by
deriving a set of outer bounds on the symmetric DoF. For linear beamforming
schemes, we first focus on small networks and propose a structured approach to
linear beamforming based on a notion called packing ratios. Packing ratio
describes the interference footprint or shadow cast by a set of transmit
beamformers and enables us to identify the underlying structures for aligning
interference. Such a structured beamforming design can be shown to achieve the
optimal spatially normalized DoF (sDoF) of two-cell two-user/cell network and
the two-cell three-user/cell network. For larger networks, we develop an
unstructured approach to linear interference alignment, where transmit
beamformers are designed to satisfy conditions for IA without explicitly
identifying the underlying structures for IA. The main numerical insight of
this paper is that such an approach appears to be capable of achieving the
optimal sDoF for MIMO cellular networks in regimes where linear beamforming
dominates asymptotic decomposition, and a significant portion of sDoF
elsewhere. Remarkably, polynomial identity test appears to play a key role in
identifying the boundary of the achievable sDoF region in the former case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2688</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2688</id><created>2013-12-10</created><updated>2014-01-08</updated><authors><author><keyname>Song</keyname><forenames>Xiaoshi</forenames></author><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author><author><keyname>Liu</keyname><forenames>Danpu</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Spatial Throughput Characterization in Cognitive Radio Networks with
  Threshold-Based Opportunistic Spectrum Access</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Journal on Selected Areas in Communications,
  Cognitive Radio Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the opportunistic spectrum access (OSA) of the secondary
users in a large-scale overlay cognitive radio (CR) network. Two
threshold-based OSA schemes, namely the primary receiver assisted (PRA)
protocol and the primary transmitter assisted (PTA) protocol, are investigated.
Under the PRA/PTA protocol, a secondary transmitter (ST) is allowed to access
the spectrum only when the maximum signal power of the received beacons/pilots
sent from the active primary receivers/transmitters (PRs/PTs) is lower than a
certain threshold. To measure the resulting transmission opportunity for the
secondary users by the proposed OSA protocols, the concept of spatial
opportunity, which is defined as the probability that an arbitrary location in
the primary network is detected as a spatial spectrum hole, is introduced and
then evaluated by applying tools from stochastic geometry. Based on spatial
opportunity, the coverage (non-outage transmission) performance in the overlay
CR network is analyzed. With the obtained results of spatial opportunity and
coverage probability, we finally characterize the spatial throughput, which is
defined as the average spatial density of successful transmissions in the
primary/secondary network, under the PRA and PTA protocols, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2696</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2696</id><created>2013-12-10</created><authors><author><keyname>Caldwell</keyname><forenames>James</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>Structural Induction Principles for Functional Programmers</title><categories>cs.PL cs.LO</categories><comments>In Proceedings TFPIE 2013, arXiv:1312.2216</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 136, 2013, pp. 16-26</journal-ref><doi>10.4204/EPTCS.136.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User defined recursive types are a fundamental feature of modern functional
programming languages like Haskell, Clean, and the ML family of languages.
Properties of programs defined by recursion on the structure of recursive types
are generally proved by structural induction on the type. It is well known in
the theorem proving community how to generate structural induction principles
from data type declarations. These methods deserve to be better know in the
functional programming community. Existing functional programming textbooks
gloss over this material. And yet, if functional programmers do not know how to
write down the structural induction principle for a new type - how are they
supposed to reason about it? In this paper we describe an algorithm to generate
structural induction principles from data type declarations. We also discuss
how these methods are taught in the functional programming course at the
University of Wyoming. A Haskell implementation of the algorithm is included in
an appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2698</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2698</id><created>2013-12-10</created><authors><author><keyname>Padovani</keyname><forenames>Luca</forenames><affiliation>Dipartimento di Informatica, Universit&#xe0; di Torino</affiliation></author></authors><title>From Lock Freedom to Progress Using Session Types</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><acm-class>F.3.3; D.3.3; F.1.2</acm-class><journal-ref>EPTCS 137, 2013, pp. 3-19</journal-ref><doi>10.4204/EPTCS.137.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by Kobayashi's type system for lock freedom, we define a behavioral
type system for ensuring progress in a language of binary sessions. The key
idea is to annotate actions in session types with priorities representing the
urgency with which such actions must be performed and to verify that processes
perform such actions with the required priority. Compared to related systems
for session-based languages, the presented type system is relatively simpler
and establishes progress for a wider range of processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2699</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2699</id><created>2013-12-10</created><authors><author><keyname>Di Giusto</keyname><forenames>Cinzia</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames></author></authors><title>Session Types with Runtime Adaptation: Overview and Examples</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.3.1; F.3.2</acm-class><journal-ref>EPTCS 137, 2013, pp. 21-32</journal-ref><doi>10.4204/EPTCS.137.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, we have developed a session types discipline for a calculus
that features the usual constructs for session establishment and communication,
but also two novel constructs that enable communicating processes to be
stopped, duplicated, or discarded at runtime. The aim is to understand whether
known techniques for the static analysis of structured communications scale up
to the challenging context of context-aware, adaptable distributed systems, in
which disciplined interaction and runtime adaptation are intertwined concerns.
In this short note, we summarize the main features of our session-typed
framework with runtime adaptation, and recall its basic correctness properties.
We illustrate our framework by means of examples. In particular, we present a
session representation of supervision trees, a mechanism for enforcing
fault-tolerant applications in the Erlang language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2700</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2700</id><created>2013-12-10</created><authors><author><keyname>Hirai</keyname><forenames>Yoichi</forenames><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author></authors><title>Session Types in Abelian Logic</title><categories>cs.LO cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><acm-class>F.4.1, D.3.1</acm-class><journal-ref>EPTCS 137, 2013, pp. 33-52</journal-ref><doi>10.4204/EPTCS.137.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There was a PhD student who says &quot;I found a pair of wooden shoes. I put a
coin in the left and a key in the right. Next morning, I found those objects in
the opposite shoes.&quot; We do not claim existence of such shoes, but propose a
similar programming abstraction in the context of typed lambda calculi. The
result, which we call the Amida calculus, extends Abramsky's linear lambda
calculus LF and characterizes Abelian logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2701</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2701</id><created>2013-12-10</created><authors><author><keyname>Bocchi</keyname><forenames>Laura</forenames><affiliation>Imperial College, London</affiliation></author><author><keyname>Demangeon</keyname><forenames>Romain</forenames><affiliation>Imperial College, London</affiliation></author></authors><title>Embedding Session Types in HML</title><categories>cs.DC cs.LO</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013, pp. 53-62</journal-ref><doi>10.4204/EPTCS.137.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on the enhancement of multiparty session types with logical
annotations enable the effective verification of properties on (1) the
structure of the conversations, (2) the sorts of the messages, and (3) the
actual values exchanged. In [3] we extend this work to enable the specification
and verification of mutual effects of multiple cross-session interactions. Here
we give a sound and complete embedding into the Hennessy-Milner logic to
justify the expressiveness of the approach in [3] and to provide it with a
logical background that will enable us to compare it with similar approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2702</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2702</id><created>2013-12-10</created><authors><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames></author></authors><title>Coinductive Big-Step Semantics for Concurrency</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><acm-class>F.3.2; F.1.2</acm-class><journal-ref>EPTCS 137, 2013, pp. 63-78</journal-ref><doi>10.4204/EPTCS.137.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a paper presented at SOS 2010, we developed a framework for big-step
semantics for interactive input-output in combination with divergence, based on
coinductive and mixed inductive-coinductive notions of resumptions, evaluation
and termination-sensitive weak bisimilarity. In contrast to standard
inductively defined big-step semantics, this framework handles divergence
properly; in particular, runs that produce some observable effects and then
diverge, are not &quot;lost&quot;. Here we scale this approach for shared-variable
concurrency on a simple example language. We develop the metatheory of our
semantics in a constructive logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2703</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2703</id><created>2013-12-10</created><authors><author><keyname>Tousimojarad</keyname><forenames>Ashkan</forenames></author><author><keyname>Vanderbauwhede</keyname><forenames>Wim</forenames></author></authors><title>The Glasgow Parallel Reduction Machine: Programming Shared-memory
  Many-core Systems using Parallel Task Composition</title><categories>cs.DC cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013, pp. 79-94</journal-ref><doi>10.4204/EPTCS.137.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Glasgow Parallel Reduction Machine (GPRM), a novel, flexible
framework for parallel task-composition based many-core programming. We allow
the programmer to structure programs into task code, written as C++ classes,
and communication code, written in a restricted subset of C++ with functional
semantics and parallel evaluation. In this paper we discuss the GPRM, the
virtual machine framework that enables the parallel task composition approach.
We focus the discussion on GPIR, the functional language used as the
intermediate representation of the bytecode running on the GPRM. Using examples
in this language we show the flexibility and power of our task composition
framework. We demonstrate the potential using an implementation of a merge sort
algorithm on a 64-core Tilera processor, as well as on a conventional Intel
quad-core processor and an AMD 48-core processor system. We also compare our
framework with OpenMP tasks in a parallel pointer chasing algorithm running on
the Tilera processor. Our results show that the GPRM programs outperform the
corresponding OpenMP codes on all test platforms, and can greatly facilitate
writing of parallel programs, in particular non-data parallel algorithms such
as reductions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2704</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2704</id><created>2013-12-10</created><authors><author><keyname>Neykova</keyname><forenames>Rumyana</forenames><affiliation>Imperial College</affiliation></author></authors><title>Session Types Go Dynamic or How to Verify Your Python Conversations</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013, pp. 95-102</journal-ref><doi>10.4204/EPTCS.137.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first implementation of session types in a
dynamically-typed language - Python. Communication safety of the whole system
is guaranteed at runtime by monitors that check the execution traces comply
with an associated protocol. Protocols are written in Scribble, a choreography
description language based on multiparty session types, with addition of logic
formulas for more precise behaviour properties. The presented framework
overcomes the limitations of previous works on the session types where all
endpoints should be statically typed so that they do not permit
interoperability with untyped participants. The advantages, expressiveness and
performance of dynamic protocol checking are demonstrated through use case and
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2705</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2705</id><created>2013-12-10</created><authors><author><keyname>Marques</keyname><forenames>Eduardo R. B.</forenames><affiliation>LASIGE/FCUL, Universidade of Lisbon</affiliation></author><author><keyname>Martins</keyname><forenames>Francisco</forenames><affiliation>LASIGE/FCUL, Universidade of Lisbon</affiliation></author><author><keyname>Vasconcelos</keyname><forenames>Vasco T.</forenames><affiliation>LASIGE/FCUL, Universidade of Lisbon</affiliation></author><author><keyname>Ng</keyname><forenames>Nicholas</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Martins</keyname><forenames>Nuno</forenames><affiliation>LASIGE/FCUL, Universidade of Lisbon</affiliation></author></authors><title>Towards deductive verification of MPI programs against session types</title><categories>cs.DC cs.LO cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013, pp. 103-113</journal-ref><doi>10.4204/EPTCS.137.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Message Passing Interface (MPI) is the de facto standard message-passing
infrastructure for developing parallel applications. Two decades after the
first version of the library specification, MPI-based applications are nowadays
routinely deployed on super and cluster computers. These applications, written
in C or Fortran, exhibit intricate message passing behaviours, making it hard
to statically verify important properties such as the absence of deadlocks. Our
work builds on session types, a theory for describing protocols that provides
for correct-by-construction guarantees in this regard. We annotate MPI
primitives and C code with session type contracts, written in the language of a
software verifier for C. Annotated code is then checked for correctness with
the software verifier. We present preliminary results and discuss the
challenges that lie ahead for verifying realistic MPI program compliance
against session types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2706</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2706</id><created>2013-12-10</created><authors><author><keyname>Demeyer</keyname><forenames>Romain</forenames></author><author><keyname>Vanhoof</keyname><forenames>Wim</forenames></author></authors><title>Static Application-Level Race Detection in STM Haskell using Contracts</title><categories>cs.LO cs.DC cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218. rde@info.fundp.ac.be;
  wim.vanhoof@unamur.be</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 137, 2013, pp. 115-134</journal-ref><doi>10.4204/EPTCS.137.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing concurrent programs is a hard task, even when using high-level
synchronization primitives such as transactional memories together with a
functional language with well-controlled side-effects such as Haskell, because
the interferences generated by the processes to each other can occur at
different levels and in a very subtle way. The problem occurs when a thread
leaves or exposes the shared data in an inconsistent state with respect to the
application logic or the real meaning of the data. In this paper, we propose to
associate contracts to transactions and we define a program transformation that
makes it possible to extend static contract checking in the context of STM
Haskell. As a result, we are able to check statically that each transaction of
a STM Haskell program handles the shared data in a such way that a given
consistency property, expressed in the form of a user-defined boolean function,
is preserved. This ensures that bad interference will not occur during the
execution of the concurrent program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2707</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2707</id><created>2013-12-10</created><authors><author><keyname>Dobson</keyname><forenames>Simon</forenames><affiliation>University of St Andrews</affiliation></author><author><keyname>Dearle</keyname><forenames>Alan</forenames><affiliation>University of St Andrews</affiliation></author><author><keyname>Porter</keyname><forenames>Barry</forenames><affiliation>University of St Andrews</affiliation></author></authors><title>Minimising virtual machine support for concurrency</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2013, arXiv:1312.2218</comments><proxy>EPTCS</proxy><acm-class>D3.4</acm-class><journal-ref>EPTCS 137, 2013, pp. 135-141</journal-ref><doi>10.4204/EPTCS.137.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Co-operative and pre-emptive scheduling are usually considered to be
complementary models of threading. In the case of virtual machines, we show
that they can be unified using a single concept, the bounded execution of a
thread of control, essentially providing a first-class representation of a
computation as it is reduced. Furthermore this technique can be used to surface
the thread scheduler of a language into the language itself, allowing programs
to provide their own schedulers without any additional support in the virtual
machine, and allowing the same virtual machine to support different thread
models simultaneously and without re-compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2709</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2709</id><created>2013-12-10</created><authors><author><keyname>Kumar</keyname><forenames>Anugrah</forenames></author><author><keyname>Roy</keyname><forenames>Sanjiban Shekar</forenames></author><author><keyname>Rawat</keyname><forenames>Sarvesh SS</forenames></author><author><keyname>Saxena</keyname><forenames>Sanklan</forenames></author></authors><title>Phishing Detection by determining reliability factor using rough set
  theory</title><categories>cs.AI</categories><comments>The International Conference on Machine Intelligence Research and
  Advancement, ICMIRA-2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Phishing is a common online weapon, used against users, by Phishers for
acquiring a confidential information through deception. Since the inception of
internet, nearly everything, ranging from money transaction to sharing
information, is done online in most parts of the world. This has also given
rise to malicious activities such as Phishing. Detecting Phishing is an
intricate process due to complexity, ambiguity and copious amount of
possibilities of factors responsible for phishing . Rough sets can be a
powerful tool, when working on such kind of Applications containing vague or
imprecise data. This paper proposes an approach towards Phishing Detection
Using Rough Set Theory. The Thirteen basic factors, directly responsible
towards Phishing, are grouped into four Strata. Reliability Factor is
determined on the basis of the outcome of these strata, using Rough Set Theory
. Reliability Factor determines the possibility of a suspected site to be Valid
or Fake. Using Rough set Theory most and the least influential factors towards
Phishing are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2710</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2710</id><created>2013-12-10</created><authors><author><keyname>Rawat</keyname><forenames>Sarvesh SS</forenames></author><author><keyname>Mor</keyname><forenames>Dheeraj Dilip</forenames></author><author><keyname>Kumar</keyname><forenames>Anugrah</forenames></author><author><keyname>Roy</keyname><forenames>Sanjiban Shekar</forenames></author><author><keyname>kumar</keyname><forenames>Rohit</forenames></author></authors><title>Improving circuit miniaturization and its efficiency using Rough Set
  Theory</title><categories>cs.LG cs.AI</categories><comments>The International Conference on Machine Intelligence Research and
  Advancement,ICMIRA-2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  High-speed, accuracy, meticulousness and quick response are notion of the
vital necessities for modern digital world. An efficient electronic circuit
unswervingly affects the maneuver of the whole system. Different tools are
required to unravel different types of engineering tribulations. Improving the
efficiency, accuracy and low power consumption in an electronic circuit is
always been a bottle neck problem. So the need of circuit miniaturization is
always there. It saves a lot of time and power that is wasted in switching of
gates, the wiring-crises is reduced, cross-sectional area of chip is reduced,
the number of transistors that can implemented in chip is multiplied many
folds. Therefore to trounce with this problem we have proposed an Artificial
intelligence (AI) based approach that make use of Rough Set Theory for its
implementation. Theory of rough set has been proposed by Z Pawlak in the year
1982. Rough set theory is a new mathematical tool which deals with uncertainty
and vagueness. Decisions can be generated using rough set theory by reducing
the unwanted and superfluous data. We have condensed the number of gates
without upsetting the productivity of the given circuit. This paper proposes an
approach with the help of rough set theory which basically lessens the number
of gates in the circuit, based on decision rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2730</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2730</id><created>2013-12-10</created><updated>2016-02-15</updated><authors><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Trunck</keyname><forenames>Th&#xe9;ophile</forenames></author></authors><title>Clique-Stable Set separation in perfect graphs with no balanced
  skew-partitions</title><categories>cs.DM math.CO</categories><comments>arXiv admin note: text overlap with arXiv:1308.6444</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a question of Yannakakis on the Vertex Packing polytope of
perfect graphs, we study the Clique-Stable Set Separation in a non-hereditary
subclass of perfect graphs. A cut (B,W) of G (a bipartition of V(G)) separates
a clique K and a stable set S if $K\subseteq B$ and $S\subseteq W$. A
Clique-Stable Set Separator is a family of cuts such that for every clique K,
and for every stable set S disjoint from K, there exists a cut in the family
that separates K and S. Given a class of graphs, the question is to know
whether every graph of the class admits a Clique-Stable Set Separator
containing only polynomially many cuts. It is open for the class of all graphs,
and also for perfect graphs, which was Yannakakis' original question. Here we
investigate on perfect graphs with no balanced skew-partition; the balanced
skew-partition was introduced in the proof of the Strong Perfect Graph Theorem.
Recently, Chudnovsky, Trotignon, Trunck and Vuskovic proved that forbidding
this unfriendly decomposition permits to recursively decompose Berge graphs
using 2-join and complement 2-join until reaching a basic graph, and they found
an efficient combinatorial algorithm to color those graphs. We apply their
decomposition result to prove that perfect graphs with no balanced
skew-partition admit a quadratic-size Clique-Stable Set Separator, by taking
advantage of the good behavior of 2-join with respect to this property. We then
generalize this result and prove that the Strong Erdos-Hajnal property holds in
this class, which means that every such graph has a linear-size biclique or
complement biclique. This property does not hold for all perfect graphs (Fox
2006), and moreover when the Strong Erdos-Hajnal property holds in a hereditary
class of graphs, then both the Erdos-Hajnal property and the polynomial
Clique-Stable Set Separation hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2738</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2738</id><created>2013-12-10</created><updated>2014-01-10</updated><authors><author><keyname>&#x130;leri</keyname><forenames>Atalay Mert</forenames></author><author><keyname>K&#xfc;lekci</keyname><forenames>M. O&#x11f;uzhan</forenames></author><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>Shortest Unique Substring Query Revisited</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of finding shortest unique substring (SUS) proposed
recently by [6]. We propose an optimal $O(n)$ time and space algorithm that can
find an SUS for every location of a string of size $n$. Our algorithm
significantly improves the $O(n^2)$ time complexity needed by [6]. We also
support finding all the SUSes covering every location, whereas the solution in
[6] can find only one SUS for every location. Further, our solution is simpler
and easier to implement and can also be more space efficient in practice, since
we only use the inverse suffix array and longest common prefix array of the
string, while the algorithm in [6] uses the suffix tree of the string and other
auxiliary data structures. Our theoretical results are validated by an
empirical study that shows our algorithm is much faster and more space-saving
than the one in [6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2784</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2784</id><created>2013-12-10</created><authors><author><keyname>Gupta</keyname><forenames>Srishti</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>OCEAN: Open-source Collation of eGovernment data And Networks -
  Understanding Privacy Leaks in Open Government Data</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The awareness and sense of privacy has increased in the minds of people over
the past few years. Earlier, people were not very restrictive in sharing their
personal information, but now they are more cautious in sharing it with
strangers, either in person or online. With such privacy expectations and
attitude of people, it is di?fficult to embrace the fact that a lot of
information is publicly available on the web. Information portals in the form
of the e-governance websites run by Delhi Government in India provide access to
such PII without any anonymization. Several databases e.g., Voterrolls, Driving
Licence number, MTNL phone directory, PAN card serve as repositories of
personal information of Delhi residents. This large amount of available
personal information can be exploited due to the absence of proper written law
on privacy in India. PII can also be collected from various social networking
sites like Facebook, Twitter, GooglePlus etc. where the users share some
information about them. Since users themselves put this information, it may not
be considered as a privacy breach, but if the information is aggregated, it may
give out much more information resulting in a bigger threat. To bring such
issues to public notice, we developed Open-source Collation of eGovernment data
And Networks (OCEAN), a system where the user enters little information (e.g.
Name) about a person and gets large amount of personal information about him /
her like name, age, address, date of birth, mother's name, father's name, voter
ID, driving licence number, PAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2785</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2785</id><created>2013-12-10</created><authors><author><keyname>Seidl</keyname><forenames>Mathis</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>An efficient length- and rate-preserving concatenation of polar and
  repetition codes</title><categories>cs.IT math.IT</categories><comments>to be presented at International Zurich Seminar (IZS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve the method in \cite{Seidl:10} for increasing the finite-lengh
performance of polar codes by protecting specific, less reliable symbols with
simple outer repetition codes. Decoding of the scheme integrates easily in the
known successive decoding algorithms for polar codes. Overall rate and block
length remain unchanged, the decoding complexity is at most doubled. A
comparison to related methods for performance improvement of polar codes is
drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2789</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2789</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya . M.</forenames></author></authors><title>Performance Analysis Of Regularized Linear Regression Models For
  Oxazolines And Oxazoles Derivitive Descriptor Dataset</title><categories>cs.LG</categories><journal-ref>published International Journal of Computational Science and
  Information Technology (IJCSITY) Vol.1, No.4, November 2013</journal-ref><doi>10.5121/ijcsity.2013.1408</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Regularized regression techniques for linear regression have been created the
last few ten years to reduce the flaws of ordinary least squares regression
with regard to prediction accuracy. In this paper, new methods for using
regularized regression in model choice are introduced, and we distinguish the
conditions in which regularized regression develops our ability to discriminate
models. We applied all the five methods that use penalty-based (regularization)
shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with
far more predictors than observations. The lasso, ridge, elasticnet, lars and
relaxed lasso further possess the desirable property that they simultaneously
select relevant predictive descriptors and optimally estimate their effects.
Here, we comparatively evaluate the performance of five regularized linear
regression methods The assessment of the performance of each model by means of
benchmark experiments is an established exercise. Cross-validation and
resampling methods are generally used to arrive point evaluates the
efficiencies which are compared to recognize methods with acceptable features.
Predictive accuracy was evaluated using the root mean squared error (RMSE) and
Square of usual correlation between predictors and observed mean inhibitory
concentration of antitubercular activity (R square). We found that all five
regularized regression models were able to produce feasible models and
efficient capturing the linearity in the data. The elastic net and lars had
similar accuracies as well as lasso and relaxed lasso had similar accuracies
but outperformed ridge regression in terms of the RMSE and R square metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2791</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2791</id><created>2013-12-10</created><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Robinson-Garc&#xed;a</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Campanario</keyname><forenames>J. M.</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Coverage, field specialization and impact of scientific publishers
  indexed in the 'Book Citation Index'</title><categories>cs.DL</categories><comments>Online Information Review, 38(1)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: The aim of this study is to analyze the disciplinary coverage of the
Thomson Reuters' Book Citation Index database focusing on publisher presence,
impact and specialization. Design/Methodology/approach: We conduct a
descriptive study in which we examine coverage by discipline, publisher
distribution by field and country of publication, and publisher impact. For
this the Thomson Reuters' Subject Categories were aggregated into 15
disciplines. Findings: 30% of the total share of this database belongs to the
fields of Humanities and Social Sciences. Most of the disciplines are covered
by very few publishers mainly from the UK and USA (75.05% of the books), in
fact 33 publishers concentrate 90% of the whole share. Regarding publisher
impact, 80.5% of the books and chapters remained uncited. Two serious errors
were found in this database. Firstly, the Book Citation Index does not retrieve
all citations for books and chapters. Secondly, book citations do not include
citations to their chapters. Research limitations/implications: The Book
Citation Index is still underdeveloped and has serious limitations which call
into caution when using it for bibliometric purposes. Practical implications:
The results obtained from this study warn against the use of this database for
bibliometric purposes, but opens a new window of opportunities for covering
long neglected areas such as Humanities and Social Sciences. The target
audience of this study is librarians, bibliometricians, researchers, scientific
publishers, prospective authors and evaluation agencies. Originality/Value:
There are currently no studies analyzing in depth the coverage of this novel
database which covers monographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2795</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2795</id><created>2013-12-10</created><updated>2013-12-11</updated><authors><author><keyname>Arberet</keyname><forenames>Simon</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Reverberant Audio Source Separation via Sparse and Low-Rank Modeling</title><categories>cs.SD</categories><doi>10.1109/LSP.2014.2303135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of audio source separation from underdetermined convolutive
mixture assuming known mixing filters can be significantly improved by using an
analysis sparse prior optimized by a reweighting l1 scheme and a wideband
datafidelity term, as demonstrated by a recent article. In this letter, we show
that the performance can be improved even more significantly by exploiting a
low-rank prior on the source spectrograms.We present a new algorithm to
estimate the sources based on i) an analysis sparse prior, ii) a reweighting
scheme so as to increase the sparsity, iii) a wideband data-fidelity term in a
constrained form, and iv) a low-rank constraint on the source spectrograms.
Evaluation on reverberant music mixtures shows that the resulting algorithm
improves state-of-the-art methods by more than 2 dB of signal-to-distortion
ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2798</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2798</id><created>2013-12-10</created><authors><author><keyname>Liang</keyname><forenames>Shao Fen</forenames></author><author><keyname>Scott</keyname><forenames>Donia</forenames></author><author><keyname>Stevens</keyname><forenames>Robert</forenames></author><author><keyname>Rector</keyname><forenames>Alan</forenames></author></authors><title>OntoVerbal: a Generic Tool and Practical Application to SNOMED CT</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology development is a non-trivial task requiring expertise in the chosen
ontological language. We propose a method for making the content of ontologies
more transparent by presenting, through the use of natural language generation,
naturalistic descriptions of ontology classes as textual paragraphs. The method
has been implemented in a proof-of- concept system, OntoVerbal, that
automatically generates paragraph-sized textual descriptions of ontological
classes expressed in OWL. OntoVerbal has been applied to ontologies that can be
loaded into Prot\'eg\'e and been evaluated with SNOMED CT, showing that it
provides coherent, well-structured and accurate textual descriptions of
ontology classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2804</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2804</id><created>2013-12-10</created><authors><author><keyname>Parkinson</keyname><forenames>Simon</forenames></author><author><keyname>Crampton</keyname><forenames>Andrew</forenames></author></authors><title>A Novel Software Tool for Analysing NT File System Permissions</title><categories>cs.SE</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 4, No. 6, 2013</journal-ref><doi>10.14569/IJACSA.2013.040635</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Administrating and monitoring New Technology File System (NTFS) permissions
can be a cumbersome and convoluted task. In today's data rich world there has
never been a more important time to ensure that data is secured against
unwanted access. This paper identifies the essential and fundamental
requirements of access control, highlighting the main causes of their
misconfiguration within the NTFS. In response, a number of features are
identified and an efficient, informative and intuitive software-based solution
is proposed for examining file system permissions. In the first year that the
software has been made freely available it has been downloaded and installed by
over four thousand users
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2806</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2806</id><created>2013-12-10</created><authors><author><keyname>Matsumae</keyname><forenames>Susumu</forenames></author><author><keyname>Ooshita</keyname><forenames>Fukuhito</forenames></author></authors><title>Hierarchical Low Power Consumption Technique with Location Information
  for Sensor Networks</title><categories>cs.NI cs.DC</categories><comments>6 pages, 9 figures, 2 tables</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 4 Issue 4, 2013</journal-ref><doi>10.14569/IJACSA.2013.040412</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the wireless sensor networks composed of battery-powered sensor nodes, one
of the main issues is how to save power consumption at each node. The usual
approach to this problem is to activate only necessary nodes (e.g., those nodes
which compose a backbone network), and to put other nodes to sleep. One such
algorithm using location information is GAF (Geographical Adaptive Fidelity),
and the GAF is enhanced to HGAF (Hierarchical Geographical Adaptive Fidelity).
In this paper, we show that we can further improve the energy efficiency of
HGAF by modifying the manner of dividing sensor-field. We also provide a
theoretical bound on this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2807</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2807</id><created>2013-12-10</created><authors><author><keyname>Matsumae</keyname><forenames>Susumu</forenames></author></authors><title>Polylogarithmic Gap between Meshes with Reconfigurable Row/Column Buses
  and Meshes with Statically Partitioned Buses</title><categories>cs.DC</categories><comments>6 pages, 5 figures</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 3 Issue 2, 2012</journal-ref><doi>10.14569/IJACSA.2012.030216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the difference in computational power between the
mesh-connected parallel computers equipped with dynamically reconfigurable bus
systems and those with static ones. The mesh with separable buses (MSB) is the
mesh-connected parallel computer with dynamically reconfigurable row/column
buses. The broadcast buses of the MSB can be dynamically sectioned into smaller
bus segments by program control. We show that the MSB of size $n \times n$ can
work with$O(\log^2 n)$ step even if its dynamic reconfigurable function is
disabled. Here, we assume the word-model broadcast buses, and use the relation
between the word-model bus and the bit-model bus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2808</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2808</id><created>2013-12-10</created><authors><author><keyname>SIngh</keyname><forenames>Abhishek Kumar</forenames></author><author><keyname>Sharma</keyname><forenames>Aditi</forenames></author><author><keyname>Mishra</keyname><forenames>Rahul</forenames></author></authors><title>Personalized real time weather forecasting</title><categories>cs.OH</categories><comments>Published in IJACSA 5 pages Weather Forcasting. Paper 26</comments><journal-ref>IJACSA 2013 vol 4 issue 11</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temperature forecasting and rain forecasting in today's environment is
playing a major role in many fields like transportation, tour planning and
agriculture. The purpose of this paper is to provide a real time forecasting to
the user according to their current position and requirement. The simplest
method of forecasting the weather, persistence, relies upon today's conditions
to forecast the conditions tomorrow i.e. analyzing historical data for
predicting future weather conditions. The weather data used for the DM research
include daily temperature, daily pressure and monthly rainfall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2813</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2813</id><created>2013-12-10</created><authors><author><keyname>Matsumae</keyname><forenames>Susumu</forenames></author></authors><title>Energy-Efficient Cell Partition of 3D Space for Sensor Networks with
  Location Information</title><categories>cs.DC cs.NI</categories><comments>14 pages, 9 figures, 2 tables</comments><journal-ref>International Journal of Network Protocols and Algorithms, Vol. 1,
  No. 2 (2009)</journal-ref><doi>10.5296/npa.v1i2.270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the wireless sensor networks composed of battery-powered sensor nodes, one
of the main issues is how to save power consumption on each node. The usual
approach to this problem is to activate only necessary nodes (e.g., those nodes
which compose a backbone network), and to put other nodes to sleep. One such
algorithm using location information is GAF (Geographical Adaptive Fidelity),
and GAF is enhanced to HGAF (Hierarchical Geographical Adaptive Fidelity). In
this paper, we study the energy-efficient partition of a 3 dimensional sensor
field into cells. Further, we give a theoretical upper bound on cell size for
this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2816</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2816</id><created>2013-12-10</created><authors><author><keyname>Heuvel</keyname><forenames>Jan van den</forenames></author></authors><title>The Complexity of Change</title><categories>cs.DM math.CO</categories><comments>28 pages, 6 figures</comments><journal-ref>In: S.R. Blackburn, S. Gerke and M. Wildon (eds.), &quot;Surveys in
  Combinatorics 2013&quot;. Cambridge UP, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many combinatorial problems can be formulated as &quot;Can I transform
configuration 1 into configuration 2, if certain transformations only are
allowed?&quot;. An example of such a question is: given two k-colourings of a graph,
can I transform the first k-colouring into the second one, by recolouring one
vertex at a time, and always maintaining a proper k-colouring? Another example
is: given two solutions of a SAT-instance, can I transform the first solution
into the second one, by changing the truth value one variable at a time, and
always maintaining a solution of the SAT-instance? Other examples can be found
in many classical puzzles, such as the 15-Puzzle and Rubik's Cube.
  In this survey we shall give an overview of some older and more recent work
on this type of problem. The emphasis will be on the computational complexity
of the problems: how hard is it to decide if a certain transformation is
possible or not?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2818</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2818</id><created>2013-12-10</created><updated>2014-08-08</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Bright</keyname><forenames>Jonathan</forenames></author></authors><title>Can electoral popularity be predicted using socially generated big data?</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>To appear in Information Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, our more-than-ever digital lives leave significant footprints in
cyberspace. Large scale collections of these socially generated footprints,
often known as big data, could help us to re-investigate different aspects of
our social collective behaviour in a quantitative framework. In this
contribution we discuss one such possibility: the monitoring and predicting of
popularity dynamics of candidates and parties through the analysis of socially
generated data on the web during electoral campaigns. Such data offer
considerable possibility for improving our awareness of popularity dynamics.
However they also suffer from significant drawbacks in terms of
representativeness and generalisability. In this paper we discuss potential
ways around such problems, suggesting the nature of different political systems
and contexts might lend differing levels of predictive power to certain types
of data source. We offer an initial exploratory test of these ideas, focussing
on two data streams, Wikipedia page views and Google search queries. On the
basis of this data, we present popularity dynamics from real case examples of
recent elections in three different countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2819</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2819</id><created>2013-12-10</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author></authors><title>Covering Partial Cubes with Zones</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial cube is a graph having an isometric embedding in a hypercube.
Partial cubes are characterized by a natural equivalence relation on the edges,
whose classes are called zones. The number of zones determines the minimal
dimension of a hypercube in which the graph can be embedded. We consider the
problem of covering the vertices of a partial cube with the minimum number of
zones. The problem admits several special cases, among which are the problem of
covering the cells of a line arrangement with a minimum number of lines, and
the problem of finding a minimum-size fibre in a bipartite poset. For several
such special cases, we give upper and lower bounds on the minimum size of a
covering by zones. We also consider the computational complexity of those
problems, and establish some hardness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2822</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2822</id><created>2013-12-10</created><authors><author><keyname>Charalampous</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kostavelis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chrysostomou</keyname><forenames>Dimitrios</forenames></author><author><keyname>Amanatiadis</keyname><forenames>Angelos</forenames></author><author><keyname>Gasteratos</keyname><forenames>Antonios</forenames></author></authors><title>3D Maps Registration and Path Planning for Autonomous Robot Navigation</title><categories>cs.RO</categories><comments>3 pages, 6 figures, IROS'13 Workshop on Robots and Sensors
  integration in future rescue INformation system (ROSIN'13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile robots dedicated in security tasks should be capable of clearly
perceiving their environment to competently navigate within cluttered areas, so
as to accomplish their assigned mission. The paper in hand describes such an
autonomous agent designed to deploy competently in hazardous environments
equipped with a laser scanner sensor. During the robot's motion, consecutive
scans are obtained to produce dense 3D maps of the area. A 3D point cloud
registration technique is exploited to merge the successively created maps
during the robot's motion followed by an ICP refinement step. The reconstructed
3D area is then top-down projected with great resolution, to be fed in a path
planning algorithm suitable to trace obstacle-free trajectories in the explored
area. The main characteristic of the path planner is that the robot's
embodiment is considered for producing detailed and safe trajectories of $1$
$cm$ resolution. The proposed method has been evaluated with our mobile robot
in several outdoor scenarios revealing remarkable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2828</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2828</id><created>2013-12-10</created><authors><author><keyname>Pourghomi</keyname><forenames>Pardis</forenames></author><author><keyname>saeed</keyname><forenames>Muhammad Qasim</forenames></author><author><keyname>Ghinea</keyname><forenames>Gheorghita</forenames></author></authors><title>A Proposed NFC Payment Application</title><categories>cs.CR</categories><comments>9 pages, 4 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 8, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Near Field Communication (NFC) technology is based on a short range radio
communication channel which enables users to exchange data between devices.
With NFC technology, mobile services establish a contactless transaction system
to make the payment methods easier for people. Although NFC mobile services
have great potential for growth, they have raised several issues which have
concerned the researches and prevented the adoption of this technology within
societies. Reorganizing and describing what is required for the success of this
technology have motivated us to extend the current NFC ecosystem models to
accelerate the development of this business area. In this paper, we introduce a
new NFC payment application, which is based on our previous NFC Cloud Wallet
model to demonstrate a reliable structure of NFC ecosystem. We also describe
the step by step execution of the proposed protocol in order to carefully
analyse the payment application and our main focus will be on the Mobile
Network Operator (MNO) as the main player within the ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2841</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2841</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya . M.</forenames></author></authors><title>Predictive Comparative QSAR Analysis Of As 5-Nitofuran-2-YL Derivatives
  Myco bacterium tuberculosis H37RV Inhibitors Bacterium Tuberculosis H37RV
  Inhibitors</title><categories>cs.CE</categories><journal-ref>published Health Informatics- An International Journal (HIIJ)
  Vol.2, No.4, November 2013</journal-ref><doi>10.5121/hiij.2013.2404</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Antitubercular activity of 5-nitrofuran-2-yl Derivatives series were
subjected to Quantitative Structure Activity Relationship (QSAR) Analysis with
an effort to derive and understand a correlation between the biological
activity as response variable and different molecular descriptors as
independent variables. QSAR models are built using 40 molecular descriptor
dataset. Different statistical regression expressions were got using Partial
Least Squares (PLS),Multiple Linear Regression (MLR) and Principal Component
Regression (PCR) techniques. The among these technique, Partial Least Square
Regression (PLS) technique has shown very promising result as compared to MLR
technique A QSAR model was build by a training set of 30 molecules with
correlation coefficient ($r^2$) of 0.8484, significant cross validated
correlation coefficient ($q^2$) is 0.0939, F test is 48.5187, ($r^2$) for
external test set (pred$_r^2$) is -0.5604, coefficient of correlation of
predicted data set (pred$_r^2se$) is 0.7252 and degree of freedom is 26 by
Partial Least Squares Regression technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2844</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2844</id><created>2013-12-10</created><authors><author><keyname>Rimoux</keyname><forenames>Norbert</forenames></author><author><keyname>Descourt</keyname><forenames>Patrice</forenames></author></authors><title>mARC: Memory by Association and Reinforcement of Contexts</title><categories>cs.IR cs.CL nlin.AO nlin.CD</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper introduces the memory by Association and Reinforcement of Contexts
(mARC). mARC is a novel data modeling technology rooted in the second
quantization formulation of quantum mechanics. It is an all-purpose incremental
and unsupervised data storage and retrieval system which can be applied to all
types of signal or data, structured or unstructured, textual or not. mARC can
be applied to a wide range of information clas-sification and retrieval
problems like e-Discovery or contextual navigation. It can also for-mulated in
the artificial life framework a.k.a Conway &quot;Game Of Life&quot; Theory. In contrast
to Conway approach, the objects evolve in a massively multidimensional space.
In order to start evaluating the potential of mARC we have built a mARC-based
Internet search en-gine demonstrator with contextual functionality. We compare
the behavior of the mARC demonstrator with Google search both in terms of
performance and relevance. In the study we find that the mARC search engine
demonstrator outperforms Google search by an order of magnitude in response
time while providing more relevant results for some classes of queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2853</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2853</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya . M.</forenames></author></authors><title>Performance Analysis Of Neural Network Models For Oxazolines And
  Oxazoles Derivatives Descriptor Dataset</title><categories>cs.CE cs.NE</categories><journal-ref>published International Journal of Information Sciences and
  Techniques (IJIST) Vol.3, No.6, November 2013</journal-ref><doi>10.5121/ijist.2013.3601</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Neural networks have been used successfully to a broad range of areas such as
business, data mining, drug discovery and biology. In medicine, neural networks
have been applied widely in medical diagnosis, detection and evaluation of new
drugs and treatment cost estimation. In addition, neural networks have begin
practice in data mining strategies for the aim of prediction, knowledge
discovery. This paper will present the application of neural networks for the
prediction and analysis of antitubercular activity of Oxazolines and Oxazoles
derivatives. This study presents techniques based on the development of Single
hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural
network (GDBPNN), Gradient Descent Back propagation with momentum neural
network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)
and Quantile regression neural network (QRNN) of artificial neural network
(ANN) models Here, we comparatively evaluate the performance of five neural
network techniques. The evaluation of the efficiency of each model by ways of
benchmark experiments is an accepted application. Cross-validation and
resampling techniques are commonly used to derive point estimates of the
performances which are compared to identify methods with good properties.
Predictive accuracy was evaluated using the root mean squared error (RMSE),
Coefficient determination(???), mean absolute error(MAE), mean percentage
error(MPE) and relative square error(RSE). We found that all five neural
network models were able to produce feasible models. QRNN model is outperforms
with all statistical tests amongst other four models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2859</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2859</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya . M.</forenames></author></authors><title>A Robust Missing Value Imputation Method MifImpute For Incomplete
  Molecular Descriptor Data And Comparative Analysis With Other Missing Value
  Imputation Methods</title><categories>cs.CE</categories><comments>arXiv admin note: text overlap with arXiv:1105.0828 by other authors
  without attribution</comments><journal-ref>Published International Journal on Computational Sciences &amp;
  Applications (IJCSA) Vol.3, No4, August 2013</journal-ref><doi>10.5121/ijcsa.2013.3406</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Missing data imputation is an important research topic in data mining.
Large-scale Molecular descriptor data may contains missing values (MVs).
However, some methods for downstream analyses, including some prediction tools,
require a complete descriptor data matrix. We propose and evaluate an iterative
imputation method MiFoImpute based on a random forest. By averaging over many
unpruned regression trees, random forest intrinsically constitutes a multiple
imputation scheme. Using the NRMSE and NMAE estimates of random forest, we are
able to estimate the imputation error. Evaluation is performed on two molecular
descriptor datasets generated from a diverse selection of pharmaceutical fields
with artificially introduced missing values ranging from 10% to 30%. The
experimental result demonstrates that missing values has a great impact on the
effectiveness of imputation techniques and our method MiFoImpute is more robust
to missing value than the other ten imputation methods used as benchmark.
Additionally, MiFoImpute exhibits attractive computational efficiency and can
cope with high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2861</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2861</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya . M.</forenames></author></authors><title>Identification Of Outliers In Oxazolines AND Oxazoles High Dimension
  Molecular Descriptor Dataset Using Principal Component Outlier Detection
  Algorithm And Comparative Numerical Study Of Other Robust Estimators</title><categories>cs.CE</categories><journal-ref>Published International Journal of Data Mining &amp; Knowledge
  Management Process (IJDKP) Vol.3, No.4, July 2013</journal-ref><doi>10.5121/ijdkp.2013.3405</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  From the past decade outlier detection has been in use. Detection of outliers
is an emerging topic and is having robust applications in medical sciences and
pharmaceutical sciences. Outlier detection is used to detect anomalous
behaviour of data. Typical problems in Bioinformatics can be addressed by
outlier detection. A computationally fast method for detecting outliers is
shown, that is particularly effective in high dimensions. PrCmpOut algorithm
make use of simple properties of principal components to detect outliers in the
transformed space, leading to significant computational advantages for high
dimensional data. This procedure requires considerably less computational time
than existing methods for outlier detection. The properties of this estimator
(Outlier error rate (FN), Non-Outlier error rate(FP) and computational costs)
are analyzed and compared with those of other robust estimators described in
the literature through simulation studies. Numerical evidence based Oxazolines
and Oxazoles molecular descriptor dataset shows that the proposed method
performs well in a variety of situations of practical interest. It is thus a
valuable companion to the existing outlier detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2865</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2865</id><created>2013-12-10</created><authors><author><keyname>Volovoi</keyname><forenames>Vitali</forenames></author></authors><title>Abridged Petri Nets</title><categories>cs.OH</categories><comments>17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new graphical framework, Abridged Petri Nets (APNs) is introduced for
bottom-up modeling of complex stochastic systems. APNs are similar to
Stochastic Petri Nets (SPNs) in as much as they both rely on component-based
representation of system state space, in contrast to Markov chains that
explicitly model the states of an entire system. In both frameworks, so-called
tokens (denoted as small circles) represent individual entities comprising the
system; however, SPN graphs contain two distinct types of nodes (called places
and transitions) with transitions serving the purpose of routing tokens among
places. As a result, a pair of place nodes in SPNs can be linked to each other
only via a transient stop, a transition node. In contrast, APN graphs link
place nodes directly by arcs (transitions), similar to state space diagrams for
Markov chains, and separate transition nodes are not needed.
  Tokens in APN are distinct and have labels that can assume both discrete
values (&quot;colors&quot;) and continuous values (&quot;ages&quot;), both of which can change
during simulation. Component interactions are modeled in APNs using triggers,
which are either inhibitors or enablers (the inhibitors' opposites).
Hierarchical construction of APNs rely on using stacks (layers) of submodels
with automatically matching color policies. As a result, APNs provide at least
the same modeling power as SPNs, but, as demonstrated by means of several
examples, the resulting models are often more compact and transparent,
therefore facilitating more efficient performance evaluation of complex
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2867</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2867</id><created>2013-12-10</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya M.</forenames></author></authors><title>Study Of E-Smooth Support Vector Regression And Comparison With E-
  Support Vector Regression And Potential Support Vector Machines For
  Prediction For The Antitubercular Activity Of Oxazolines And Oxazoles
  Derivatives</title><categories>cs.CE cs.LO</categories><journal-ref>Published International Journal on Soft Computing, Artificial
  Intelligence and Applications (IJSCAI), Vol.2, No.2, April 2013</journal-ref><doi>10.5121/ijscai.2013.2204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A new smoothing method for solving ? -support vector regression (?-SVR),
tolerating a small error in fitting a given data sets nonlinearly is proposed
in this study. Which is a smooth unconstrained optimization reformulation of
the traditional linear programming associated with a ?-insensitive support
vector regression. We term this redeveloped problem as ?-smooth support vector
regression (?-SSVR). The performance and predictive ability of ?-SSVR are
investigated and compared with other methods such as LIBSVM (?-SVR) and P-SVM
methods. In the present study, two Oxazolines and Oxazoles molecular descriptor
data sets were evaluated. We demonstrate the merits of our algorithm in a
series of experiments. Primary experimental results illustrate that our
proposed approach improves the regression performance and the learning
efficiency. In both studied cases, the predictive ability of the ?- SSVR model
is comparable or superior to those obtained by LIBSVM and P-SVM. The results
indicate that ?-SSVR can be used as an alternative powerful modeling method for
regression studies. The experimental results show that the presented algorithm
?-SSVR, plays better precisely and effectively than LIBSVMand P-SVM in
predicting antitubercular activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2868</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2868</id><created>2013-12-10</created><authors><author><keyname>Garc&#xed;a</keyname><forenames>Victoriano Valencia</forenames></author><author><keyname>Vicente</keyname><forenames>Eugenio J. Fern&#xe1;ndez</forenames></author><author><keyname>Aragon&#xe9;s</keyname><forenames>Luis Usero</forenames></author></authors><title>Maturity Model for IT Service Outsourcing in Higher Education
  Institutions</title><categories>cs.OH</categories><comments>7 pages, 3 tables, 1 figure</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA) Vol. 4, No. 10, 2013</journal-ref><doi>10.14569/IJACSA.2013.041007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current success of organizations depends on the successful implementation
of Information and Comunication Technologies (ICTs). Good governance and ICT
management are essential for delivering value, managing technological risks,
managing resources and performance measurement. In addition, outsourcing is a
strategic option which complements IT services provided internally in
organizations. This paper proposes the design of a new holistic maturity model
based on standards ISO/IEC 20000 and ISO/IEC 38500, the frameworks and best
practices of ITIL and COBIT, with a specific focus on IT outsourcing. This
model is validated by practices in the field of higher education, using a
questionnaire and a metrics table among other measurement tools. Models,
standards and guidelines are proposed in the model for facilitating adaptation
to universities and achieving excellence in the outsourcing of IT services. The
applicability of the model allows an effective transition to a model of good
governance and management of outsourced IT services which, aligned with the
core business of universities (teaching, research and innovation), affect the
effectiveness and efficiency of its management, optimizes its value and
minimizes risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2873</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2873</id><created>2013-12-10</created><updated>2014-03-29</updated><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Fisikopoulos</keyname><forenames>Vissarion</forenames></author></authors><title>Efficient Random-Walk Methods for Approximating Polytope Volume</title><categories>cs.CG cs.MS</categories><comments>15 pages, 2 figures, 8 tables, in Proc. of SoCG'14</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We experimentally study the fundamental problem of computing the volume of a
convex polytope given as an intersection of linear inequalities. We implement
and evaluate practical randomized algorithms for accurately approximating the
polytope's volume in high dimensions (e.g. one hundred). To carry out this
efficiently we experimentally correlate the effect of parameters, such as
random walk length and number of sample points, on accuracy and runtime.
Moreover, we exploit the problem's geometry by implementing an iterative
rounding procedure, computing partial generations of random points and
designing fast polytope boundary oracles. Our publicly available code is
significantly faster than exact computation and more accurate than existing
approximation methods. We provide volume approximations for the Birkhoff
polytopes B_11,...,B_15, whereas exact methods have only computed that of B_10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2877</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2877</id><created>2013-12-10</created><authors><author><keyname>Alomari</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Samaha</keyname><forenames>Aya</forenames></author><author><keyname>AlKamha</keyname><forenames>Khaled</forenames></author></authors><title>Automated Classification of L/R Hand Movement EEG Signals using Advanced
  Feature Extraction and Machine Learning</title><categories>cs.NE cs.CV cs.HC</categories><comments>6 pages, 4 figures</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (ijacsa) 07/2013; 4(6):207-212</journal-ref><doi>10.14569/IJACSA.2013.040628</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an automated computer platform for the purpose of
classifying Electroencephalography (EEG) signals associated with left and right
hand movements using a hybrid system that uses advanced feature extraction
techniques and machine learning algorithms. It is known that EEG represents the
brain activity by the electrical voltage fluctuations along the scalp, and
Brain-Computer Interface (BCI) is a device that enables the use of the brain
neural activity to communicate with others or to control machines, artificial
limbs, or robots without direct physical movements. In our research work, we
aspired to find the best feature extraction method that enables the
differentiation between left and right executed fist movements through various
classification algorithms. The EEG dataset used in this research was created
and contributed to PhysioNet by the developers of the BCI2000 instrumentation
system. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts
removal was done using AAR. Data was epoched on the basis of Event-Related (De)
Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)
features. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta
rhythms were isolated for the MRCP analysis. The Independent Component Analysis
(ICA) spatial filter was applied on related channels for noise reduction and
isolation of both artifactually and neutrally generated EEG sources. The final
feature vector included the ERD, ERS, and MRCP features in addition to the
mean, power and energy of the activations of the resulting independent
components of the epoched feature datasets. The datasets were inputted into two
machine-learning algorithms: Neural Networks (NNs) and Support Vector Machines
(SVMs). Intensive experiments were carried out and optimum classification
performances of 89.8 and 97.1 were obtained using NN and SVM, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2884</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2884</id><created>2013-12-10</created><authors><author><keyname>Sheikh</keyname><forenames>Muhammad Usman</forenames></author><author><keyname>Lempiainen</keyname><forenames>Jukka</forenames></author></authors><title>Advanced Antenna Techniques and High Order Sectorization with Novel
  Network Tessellation for Enhancing Macro Cell Capacity in DC-HSDPA Network</title><categories>cs.NI cs.PF</categories><comments>20 pages, 18 Figures, and 5 Tables</comments><journal-ref>International Journal of Wireless and Mobile Networks (IJWMN).
  Vol.5, No.5, October 2013</journal-ref><doi>10.5121/ijwmn.2013.5505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile operators commonly use macro cells with traditional wide beam antennas
for wider coverage in the cell, but future capacity demands cannot be achieved
by using them only. It is required to achieve maximum practical capacity from
macro cells by employing higher order sectorization and by utilizing all
possible antenna solutions including smart antennas. This paper presents
enhanced tessellation for 6-sector sites and proposes novel layout for
12-sector sites. The main target of this paper is to compare the performance of
conventional wide beam antenna, switched beam smart antenna, adaptive beam
antenna and different network layouts in terms of offering better received
signal quality and user throughput. Splitting macro cell into smaller micro or
pico cells can improve the capacity of network, but this paper highlights the
importance of higher order sectorization and advance antenna techniques to
attain high Signal to Interference plus Noise Ratio (SINR), along with improved
network capacity. Monte Carlo simulations at system level were done for Dual
Cell High Speed Downlink Packet Access (DC-HSDPA) technology with multiple
(five) users per Transmission Time Interval (TTI) at different Intersite
Distance (ISD). The obtained results validate and estimate the gain of using
smart antennas and higher order sectorization with proposed network layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2889</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2889</id><created>2013-12-10</created><authors><author><keyname>Baste</keyname><forenames>Julien</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author></authors><title>The role of planarity in connectivity problems parameterized by
  treewidth</title><categories>cs.DS cs.DM math.CO</categories><comments>23 pages</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For some years it was believed that for &quot;connectivity&quot; problems such as
Hamiltonian Cycle, algorithms running in time 2^{O(tw)}n^{O(1)} -called
single-exponential- existed only on planar and other sparse graph classes,
where tw stands for the treewidth of the n-vertex input graph. This was
recently disproved by Cygan et al. [FOCS 2011], Bodlaender et al. [ICALP 2013],
and Fomin et al. [SODA 2014], who provided single-exponential algorithms on
general graphs for essentially all connectivity problems that were known to be
solvable in single-exponential time on sparse graphs. In this article we
further investigate the role of planarity in connectivity problems
parameterized by treewidth, and convey that several problems can indeed be
distinguished according to their behavior on planar graphs. Known results from
the literature imply that there exist problems, like Cycle Packing, that cannot
be solved in time 2^{o(tw logtw)}n^{O(1)} on general graphs but that can be
solved in time 2^{O(tw)}n^{O(1)} when restricted to planar graphs. Our main
contribution is to show that there exist problems that can be solved in time
2^{O(tw logtw)}n^{O(1)} on general graphs but that cannot be solved in time
2^{o(tw logtw)}n^{O(1)} even when restricted to planar graphs. Furthermore, we
prove that Planar Cycle Packing and Planar Disjoint Paths cannot be solved in
time 2^{o(tw)}n^{O(1)}. The mentioned negative results hold unless the ETH
fails. We feel that our results constitute a first step in a subject that can
be further exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2894</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2894</id><created>2013-12-10</created><authors><author><keyname>Mayer</keyname><forenames>Marta Cialdea</forenames></author></authors><title>A Proof Procedure for Hybrid Logic with Binders, Transitivity and
  Relation Hierarchies (extended version)</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1210.5734</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous works, a tableau calculus has been defined, which constitutes a
decision procedure for hybrid logic with the converse and global modalities and
a restricted use of the binder. This work shows how to extend such a calculus
to multi-modal logic enriched with features largely used in description logics:
transitivity and relation inclusion assertions.
  The separate addition of either transitive relations or relation hierarchies
to the considered decidable fragment of multi-modal hybrid logic can easily be
shown to stay decidable, by resorting to results already proved in the
literature. However, such results do not directly allow for concluding whether
the logic including both features is still decidable. The existence of a
terminating, sound and complete calculus for the considered logic proves that
the addition of transitive relations and relation hierarchies to such an
expressive decidable fragment of hybrid logic does not endanger decidability.
  A further result proved in this work is that the logic extending the
considered fragment with the addition of graded modalities (the modal
counterpart of number restrictions of description logics) has an undecidable
satisfiability problem, unless further syntactical restrictions are placed on
the universal graded modality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2903</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2903</id><created>2013-12-10</created><authors><author><keyname>Oliveira</keyname><forenames>Roberto Imbuzeiro</forenames></author></authors><title>The lower tail of random quadratic forms, with applications to ordinary
  least squares and restricted eigenvalue properties</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>36 pages</comments><msc-class>60F99, 94A15, 62J05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite sample properties of random covariance-type matrices have been the
subject of much research. In this paper we focus on the &quot;lower tail&quot; of such a
matrix, and prove that it is subgaussian under a simple fourth moment
assumption on the one-dimensional marginals of the random vectors. A similar
result holds for more general sums of random positive semidefinite matrices,
and the (relatively simple) proof uses a variant of the so-called PAC-Bayesian
method for bounding empirical processes.
  We give two applications of the main result. In the first one we obtain a new
finite-sample bound for ordinary least squares estimator in linear regression
with random design. Our result is model-free, requires fairly weak moment
assumptions and is almost optimal. Our second application is to bounding
restricted eigenvalue constants of certain random ensembles with &quot;heavy tails&quot;.
These constants are important in the analysis of problems in Compressed Sensing
and High Dimensional Statistics, where one recovers a sparse vector from a
small umber of linear measurements. Our result implies that heavy tails still
allow for the fast recovery rates found in efficient methods such as the LASSO
and the Dantzig selector. Along the way we strengthen, with a fairly short
argument, a recent result of Rudelson and Zhou on the restricted eigenvalue
property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2915</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2915</id><created>2013-12-10</created><authors><author><keyname>Saket</keyname><forenames>Rishi</forenames></author></authors><title>Hardness of Finding Independent Sets in 2-Colorable Hypergraphs and of
  Satisfiable CSPs</title><categories>cs.CC</categories><comments>23 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work revisits the PCP Verifiers used in the works of Hastad [Has01],
Guruswami et al.[GHS02], Holmerin[Hol02] and Guruswami[Gur00] for satisfiable
Max-E3-SAT and Max-Ek-Set-Splitting, and independent set in 2-colorable
4-uniform hypergraphs. We provide simpler and more efficient PCP Verifiers to
prove the following improved hardness results: Assuming that NP\not\subseteq
DTIME(N^{O(loglog N)}),
  There is no polynomial time algorithm that, given an n-vertex 2-colorable
4-uniform hypergraph, finds an independent set of n/(log n)^c vertices, for
some constant c &gt; 0.
  There is no polynomial time algorithm that satisfies 7/8 + 1/(log n)^c
fraction of the clauses of a satisfiable Max-E3-SAT instance of size n, for
some constant c &gt; 0.
  For any fixed k &gt;= 4, there is no polynomial time algorithm that finds a
partition splitting (1 - 2^{-k+1}) + 1/(log n)^c fraction of the k-sets of a
satisfiable Max-Ek-Set-Splitting instance of size n, for some constant c &gt; 0.
  Our hardness factor for independent set in 2-colorable 4-uniform hypergraphs
is an exponential improvement over the previous results of Guruswami et
al.[GHS02] and Holmerin[Hol02]. Similarly, our inapproximability of (log
n)^{-c} beyond the random assignment threshold for Max-E3-SAT and
Max-Ek-Set-Splitting is an exponential improvement over the previous bounds
proved in [Has01], [Hol02] and [Gur00]. The PCP Verifiers used in our results
avoid the use of a variable bias parameter used in previous works, which leads
to the improved hardness thresholds in addition to simplifying the analysis
substantially. Apart from standard techniques from Fourier Analysis, for the
first mentioned result we use a mixing estimate of Markov Chains based on
uniform reverse hypercontractivity over general product spaces from the work of
Mossel et al.[MOS13].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2919</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2919</id><created>2013-12-10</created><authors><author><keyname>Zinn</keyname><forenames>Daniel</forenames></author><author><keyname>Green</keyname><forenames>Todd J</forenames></author><author><keyname>Lud&#xe4;scher</keyname><forenames>Bertram</forenames></author></authors><title>Win-Move is Coordination-Free (Sometimes)</title><categories>cs.DB</categories><comments>Proceedings of the 15th International Conference on Database Theory.
  Pages 99-113. March 26-30, 2012, Berlin, Germany</comments><acm-class>H.2.4</acm-class><doi>10.1145/2274576.2274588</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper by Hellerstein [15], a tight relationship was conjectured
between the number of strata of a Datalog${}^\neg$ program and the number of
&quot;coordination stages&quot; required for its distributed computation. Indeed, Ameloot
et al. [9] showed that a query can be computed by a coordination-free
relational transducer network iff it is monotone, thus answering in the
affirmative a variant of Hellerstein's CALM conjecture, based on a particular
definition of coordination-free computation. In this paper, we present three
additional models for declarative networking. In these variants, relational
transducers have limited access to the way data is distributed. This variation
allows transducer networks to compute more queries in a coordination-free
manner: e.g., a transducer can check whether a ground atom $A$ over the input
schema is in the &quot;scope&quot; of the local node, and then send either $A$ or $\neg
A$ to other nodes.
  We show the surprising result that the query given by the well-founded
semantics of the unstratifiable win-move program is coordination-free in some
of the models we consider. We also show that the original transducer network
model [9] and our variants form a strict hierarchy of classes of
coordination-free queries. Finally, we identify different syntactic fragments
of Datalog${}^{\neg\neg}_{\forall}$, called semi-monotone programs, which can
be used as declarative network programming languages, whose distributed
computation is guaranteed to be eventually consistent and coordination-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2936</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2936</id><created>2013-12-10</created><authors><author><keyname>Togelius</keyname><forenames>Julian</forenames></author><author><keyname>Shaker</keyname><forenames>Noor</forenames></author><author><keyname>Yannakakis</keyname><forenames>Georgios N.</forenames></author></authors><title>Active Player Modelling</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue for the use of active learning methods for player modelling. In
active learning, the learning algorithm chooses where to sample the search
space so as to optimise learning progress. We hypothesise that player modelling
based on active learning could result in vastly more efficient learning, but
will require big changes in how data is collected. Some example active player
modelling scenarios are described. A particular form of active learning is also
equivalent to an influential formalisation of (human and machine) curiosity,
and games with active learning could therefore be seen as being curious about
the player. We further hypothesise that this form of curiosity is symmetric,
and therefore that games that explore their players based on the principles of
active learning will turn out to select game configurations that are
interesting to the player that is being explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2949</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2949</id><created>2013-12-10</created><authors><author><keyname>Patel</keyname><forenames>Rajendra</forenames></author><author><keyname>Rajwat</keyname><forenames>Arvind</forenames></author></authors><title>A Survey of Embedded Software Profiling Methodologies</title><categories>cs.PF</categories><comments>22 Pages, 14 Figures, 2 Tables, International Journal of Embedded
  Systems and Applications (IJESA)</comments><doi>10.5121/ijesa.2011.1203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded Systems combine one or more processor cores with dedicated logic
running on an ASIC or FPGA to meet design goals at reasonable cost. It is
achieved by profiling the application with variety of aspects like performance,
memory usage, cache hit versus cache miss, energy consumption, etc. Out of
these, performance estimation is more important than others. With ever
increasing system complexities, it becomes quite necessary to carry out
performance estimation of embedded software implemented in a particular
processor for fast design space exploration. Such profiled data also guides the
designer how to partition the system for Hardware (HW) and Software (SW)
environments. In this paper, we propose a classification for currently
available Embedded Software Profiling Tools, and we present different academic
and industrial approaches in this context. Based on these observations, it will
be easy to identify such common principles and needs which are required for a
true Software Profiling Tool for a particular application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2976</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2976</id><created>2013-12-10</created><authors><author><keyname>Achballah</keyname><forenames>Ahmed Ben</forenames></author><author><keyname>Saoud</keyname><forenames>Slim Ben</forenames></author></authors><title>A Survey of Network-On-Chip Tools</title><categories>cs.AR</categories><comments>7 pages, 1 figure, 4 tables</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), 4(9), 2013</journal-ref><doi>10.14569/IJACSA.2013.040910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays System-On-Chips (SoCs) have evolved considerably in term of
performances, reliability and integration capacity. The last advantage has
induced the growth of the number of cores or Intellectual Properties (IPs) in a
same chip. Unfortunately, this important number of IPs has caused a new issue
which is the intra-communication between the elements of a same chip. To
resolve this problem, a new paradigm has been introduced which is the
Network-On-Chip (NoC). Since the introduction of the NoC paradigm in the last
decade, new methodologies and approaches have been presented by research
community and many of them have been adopted by industrials. The literature
contains many relevant studies and surveys discussing NoC proposals and
contributions. However, few of them have discussed or proposed a comparative
study of NoC tools. The objective of this work is to establish a reliable
survey about available design, simulation or implementation NoC tools. We
collected an important amount of information and characteristics about NoC
dedicated tools that we will present throughout this survey. This study is
built around a respectable amount of references and we hope it will help
scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2983</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2983</id><created>2013-12-10</created><authors><author><keyname>Seyedmehdi</keyname><forenames>S. Hossein</forenames></author><author><keyname>Boudreau</keyname><forenames>Gary</forenames></author></authors><title>An Efficient Clustering Algorithm for Device-to-Device Assisted Virtual
  MIMO</title><categories>cs.IT math.IT</categories><comments>in press, to appear in the IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the utilization of mobile devices (MDs) as decode-and-forward
relays in a device-to-device assisted virtual MIMO (VMIMO) system is studied.
Single antenna MDs are randomly distributed on a 2D plane according to a
Poisson point process, and only a subset of them are sources leaving other idle
MDs available to assist them (relays). Our goal is to develop an efficient
algorithm to cluster each source with a subset of available relays to form a
VMIMO system under a limited feedback assumption. We first show that the NP-
hard optimization problem of precoding in our scenario can be approximately
solved by semidefinite relaxation. We investigate a special case with a single
source and analytically derive an upper bound on the average spectral
efficiency of the VMIMO system. Then, we propose an optimal greedy algorithm
that achieves this bound. We further exploit these results to obtain a
polynomial time clustering algorithm for the general case with multiple
sources. Finally, numerical simulations are performed to compare the
performance of our algorithm with that of an exhaustive clustering algorithm,
and it shown that these numerical results corroborate the efficiency of our
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2984</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2984</id><created>2013-12-10</created><updated>2014-07-20</updated><authors><author><keyname>Darvishi</keyname><forenames>Atena</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author></authors><title>Synchrophasor monitoring of single line outages via area angle and
  susceptance</title><categories>cs.SY</categories><comments>adjusted version accepted at North American Power Symposium (NAPS),
  Pullman WA USA, September 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The area angle is a scalar measure of power system area stress that responds
to line outages within the area and is a combination of synchrophasor
measurements of voltage angles around the border of the area. Both idealized
and practical examples are given to show that the variation of the area angle
for single line outages can be approximately related to changes in the overall
susceptance of the area and the line outage severity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2986</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2986</id><created>2013-12-10</created><updated>2014-06-20</updated><authors><author><keyname>Ku&#x142;akowski</keyname><forenames>Konrad</forenames></author></authors><title>Notes on discrepancy in the pairwise comparisons method</title><categories>cs.DM cs.IR</categories><comments>8 pages</comments><journal-ref>EJOR, Vol. 245, Issue 1, Pages 333 - 337, 2015</journal-ref><doi>10.1016/j.ejor.2015.03.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pairwise comparisons method is a convenient tool used when the relative
order among different concepts (alternatives) needs to be determined. One
popular implementation of the method is based on solving an eigenvalue problem
for the pairwise comparisons matrix. In such cases the ranking result the
principal eigenvector of the pairwise comparison matrix is adopted, whilst the
eigenvalue is used to determine the index of inconsistency. A lot of research
has been devoted to the critical analysis of the eigenvalue based approach. One
of them is the work (Bana e Costa and Vansnick, 2008). In their work authors
define the conditions of order preservation (COP) and show that even for a
sufficiently consistent pairwise comparisons matrices, this condition can not
be met. The present work defines a more precise criteria for determining when
the COP is met. To formulate the criteria a discrepancy factor is used
describing how far the input to the ranking procedure is from the ranking
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2988</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2988</id><created>2013-12-10</created><updated>2015-04-08</updated><authors><author><keyname>Ma</keyname><forenames>Jianzhu</forenames></author><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Protein Contact Prediction by Integrating Joint Evolutionary Coupling
  Analysis and Supervised Learning</title><categories>q-bio.QM cs.LG math.OC q-bio.BM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein contacts contain important information for protein structure and
functional study, but contact prediction from sequence remains very
challenging. Both evolutionary coupling (EC) analysis and supervised machine
learning methods are developed to predict contacts, making use of different
types of information, respectively. This paper presents a group graphical lasso
(GGL) method for contact prediction that integrates joint multi-family EC
analysis and supervised learning. Different from existing single-family EC
analysis that uses residue co-evolution information in only the target protein
family, our joint EC analysis uses residue co-evolution in both the target
family and its related families, which may have divergent sequences but similar
folds. To implement joint EC analysis, we model a set of related protein
families using Gaussian graphical models (GGM) and then co-estimate their
precision matrices by maximum-likelihood, subject to the constraint that the
precision matrices shall share similar residue co-evolution patterns. To
further improve the accuracy of the estimated precision matrices, we employ a
supervised learning method to predict contact probability from a variety of
evolutionary and non-evolutionary information and then incorporate the
predicted probability as prior into our GGL framework. Experiments show that
our method can predict contacts much more accurately than existing methods, and
that our method performs better on both conserved and family-specific contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2990</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2990</id><created>2013-12-10</created><updated>2014-06-09</updated><authors><author><keyname>Afrati</keyname><forenames>Foto N.</forenames></author><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Vasilakopoulos</keyname><forenames>Angelos</forenames></author></authors><title>Efficient Lineage for SUM Aggregate Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI systems typically make decisions and find patterns in data based on the
computation of aggregate and specifically sum functions, expressed as queries,
on data's attributes. This computation can become costly or even inefficient
when these queries concern the whole or big parts of the data and especially
when we are dealing with big data. New types of intelligent analytics require
also the explanation of why something happened. In this paper we present a
randomised algorithm that constructs a small summary of the data, called
Aggregate Lineage, which can approximate well and explain all sums with large
values in time that depends only on its size. The size of Aggregate Lineage is
practically independent on the size of the original data. Our algorithm does
not assume any knowledge on the set of sum queries to be approximated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.2998</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.2998</id><created>2013-12-10</created><authors><author><keyname>Marinescu</keyname><forenames>Dan C.</forenames></author><author><keyname>Paya</keyname><forenames>Ashkan</forenames></author><author><keyname>Morrison</keyname><forenames>John P.</forenames></author><author><keyname>Healy</keyname><forenames>Philip</forenames></author></authors><title>An Auction-driven Self-organizing Cloud Delivery Model</title><categories>cs.DC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three traditional cloud delivery models -- IaaS, PaaS, and SaaS --
constrain access to cloud resources by hiding their raw functionality and
forcing us to use them indirectly via a restricted set of actions. Can we
introduce a new delivery model, and, at the same time, support improved
security, a higher degree of assurance, find relatively simple solutions to the
hard cloud resource management problems, eliminate some of the inefficiencies
related to resource virtualization, allow the assembly of clouds of clouds,
and, last but not least, minimize the number of interoperability standards?
  We sketch a self-organizing architecture for very large compute clouds
composed of many-core processors and heterogeneous coprocessors. We discuss how
self-organization will address each of the challenges described above. The
approach is {\em bid-centric}. The system of heterogeneous cloud resources is
dynamically, and autonomically, configured to bid to meet the needs identified
in a high-level task or service specification. When the task is completed, or
the service is retired, the resources are released for subsequent reuse.
  Our approach mimics the process followed by individual researchers who, in
response to a call for proposals released by a funding agency, organize
themselves in groups of various sizes and specialities. If the bid is
successful, then the group carries out the proposed work and releases the
results. After the work is completed, individual researchers in the group
disperse, possibly joining other groups or submitting individual bids in
response to other proposals. Similar protocols are common to other human
activities such as procurement management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3003</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3003</id><created>2013-12-10</created><authors><author><keyname>Wan</keyname><forenames>Andrew</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Wu</keyname><forenames>Chenggang</forenames></author></authors><title>Decision Trees, Protocols, and the Fourier Entropy-Influence Conjecture</title><categories>cs.CC</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $f:\{-1, 1\}^n \rightarrow \{-1, 1\}$, define the \emph{spectral
distribution} of $f$ to be the distribution on subsets of $[n]$ in which the
set $S$ is sampled with probability $\widehat{f}(S)^2$. Then the Fourier
Entropy-Influence (FEI) conjecture of Friedgut and Kalai (1996) states that
there is some absolute constant $C$ such that $\operatorname{H}[\widehat{f}^2]
\leq C\cdot\operatorname{Inf}[f]$. Here, $\operatorname{H}[\widehat{f}^2]$
denotes the Shannon entropy of $f$'s spectral distribution, and
$\operatorname{Inf}[f]$ is the total influence of $f$. This conjecture is one
of the major open problems in the analysis of Boolean functions, and settling
it would have several interesting consequences.
  Previous results on the FEI conjecture have been largely through direct
calculation. In this paper we study a natural interpretation of the conjecture,
which states that there exists a communication protocol which, given subset $S$
of $[n]$ distributed as $\widehat{f}^2$, can communicate the value of $S$ using
at most $C\cdot\operatorname{Inf}[f]$ bits in expectation.
  Using this interpretation, we are able show the following results:
  1. First, if $f$ is computable by a read-$k$ decision tree, then
$\operatorname{H}[\widehat{f}^2] \leq 9k\cdot \operatorname{Inf}[f]$.
  2. Next, if $f$ has $\operatorname{Inf}[f] \geq 1$ and is computable by a
decision tree with expected depth $d$, then $\operatorname{H}[\widehat{f}^2]
\leq 12d\cdot \operatorname{Inf}[f]$.
  3. Finally, we give a new proof of the main theorem of O'Donnell and Tan
(ICALP 2013), i.e. that their FEI$^+$ conjecture composes.
  In addition, we show that natural improvements to our decision tree results
would be sufficient to prove the FEI conjecture in its entirety. We believe
that our methods give more illuminating proofs than previous results about the
FEI conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3005</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3005</id><created>2013-12-10</created><updated>2014-03-04</updated><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Schuster</keyname><forenames>Mike</forenames></author><author><keyname>Ge</keyname><forenames>Qi</forenames></author><author><keyname>Brants</keyname><forenames>Thorsten</forenames></author><author><keyname>Koehn</keyname><forenames>Phillipp</forenames></author><author><keyname>Robinson</keyname><forenames>Tony</forenames></author></authors><title>One Billion Word Benchmark for Measuring Progress in Statistical
  Language Modeling</title><categories>cs.CL</categories><comments>Accompanied by a code.google.com project allowing anyone to generate
  the benchmark data, and use it to compare their language model against the
  ones described in the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new benchmark corpus to be used for measuring progress in
statistical language modeling. With almost one billion words of training data,
we hope this benchmark will be useful to quickly evaluate novel language
modeling techniques, and to compare their contribution when combined with other
advanced techniques. We show performance of several well-known types of
language models, with the best results achieved with a recurrent neural network
based language model. The baseline unpruned Kneser-Ney 5-gram model achieves
perplexity 67.6; a combination of techniques leads to 35% reduction in
perplexity, or 10% reduction in cross-entropy (bits), over that baseline.
  The benchmark is available as a code.google.com project; besides the scripts
needed to rebuild the training/held-out data, it also makes available
log-probability values for each word in each of ten held-out data sets, for
each of the baseline n-gram models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3009</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3009</id><created>2013-12-10</created><updated>2015-01-06</updated><authors><author><keyname>Rivin</keyname><forenames>Igor</forenames></author></authors><title>Large Galois groups with applications to Zariski density</title><categories>math.NT cs.SC</categories><comments>25 pages</comments><msc-class>14L35, 15B36, 14Q99, 12F10, 11Y40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the first provably efficient algorithm to check if a finitely
generated subgroup of an almost simple semi-simple group over the rationals is
Zariski-dense. We reduce this question to one of computing Galois groups, and
to this end we describe efficient algorithms to check if the Galois group of a
polynomial $p$ with integer coefficients is &quot;generic&quot; (which, for arbitrary
polynomials of degree $n$ means the full symmetric group $S_n,$ while for
reciprocal polynomials of degree $2n$ it means the hyperoctahedral group $C_2
\wr S_n.$). We give efficient algorithms to verify that a polynomial has Galois
group $S_n,$ and that a reciprocal polynomial has Galois group $C_2 \wr S_n.$
We show how these algorithms give efficient algorithms to check if a set of
matrices $\mathcal{G}$ in $\mathop{SL}(n, \mathbb{Z})$ or $\mathop{Sp}(2n,
\mathbb{Z})$ generate a \emph{Zariski dense} subgroup.
  The complexity of doing this in$\mathop{SL}(n, \mathbb{Z})$ is of order
$O(n^4 \log n \log \|\mathcal{G}\|)\log \epsilon$ and in $\mathop{Sp}(2n,
\mathbb{Z})$ the complexity is of order $O(n^8 \log n\log \|\mathcal{G}\|)\log
\epsilon$ In general semisimple groups we show that Zariski density can be
confirmed or denied in time of order $O(n^14 \log \|\mathcal{G}\|\log
\epsilon),$ where $\epsilon$ is the probability of a wrong &quot;NO&quot; answer, while
$\|\mathcal{G}\|$ is the measure of complexity of the input (the maximum of the
Frobenius norms of the generating matrices). The algorithms work essentially
without change over algebraic number fields, and in other semi-simple groups.
However, we restrict to the case of the special linear and symplectic groups
and rational coefficients in the interest of clarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3018</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3018</id><created>2013-12-10</created><updated>2014-12-05</updated><authors><author><keyname>Gharaibeh</keyname><forenames>Abdullah</forenames></author><author><keyname>Reza</keyname><forenames>Tahsin</forenames></author><author><keyname>Santos-Neto</keyname><forenames>Elizeu</forenames></author><author><keyname>Costa</keyname><forenames>Lauro Beltrao</forenames></author><author><keyname>Sallinen</keyname><forenames>Scott</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Efficient Large-Scale Graph Processing on Hybrid CPU and GPU Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing scale and wealth of inter-connected data, such as those
accrued by social network applications, demand the design of new techniques and
platforms to efficiently derive actionable knowledge from large-scale graphs.
However, real-world graphs are famously difficult to process efficiently. Not
only they have a large memory footprint, but also most graph algorithms entail
memory access patterns with poor locality, data-dependent parallelism and a low
compute-to-memory access ratio. Moreover, most real-world graphs have a highly
heterogeneous node degree distribution, hence partitioning these graphs for
parallel processing and simultaneously achieving access locality and
load-balancing is difficult.
  This work starts from the hypothesis that hybrid platforms (e.g.,
GPU-accelerated systems) have both the potential to cope with the heterogeneous
structure of real graphs and to offer a cost-effective platform for
high-performance graph processing. This work assesses this hypothesis and
presents an extensive exploration of the opportunity to harness hybrid systems
to process large-scale graphs efficiently. In particular, (i) we present a
performance model that estimates the achievable performance on hybrid
platforms; (ii) informed by the performance model, we design and develop TOTEM
- a processing engine that provides a convenient environment to implement graph
algorithms on hybrid platforms; (iii) we show that further performance gains
can be extracted using partitioning strategies that aim to produce partitions
that each matches the strengths of the processing element it is allocated to,
finally, (iv) we demonstrate the performance advantages of the hybrid system
through a comprehensive evaluation that uses real and synthetic workloads (as
large as 16 billion edges), multiple graph algorithms that stress the system in
various ways, and a variety of hardware configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3020</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3020</id><created>2013-12-10</created><authors><author><keyname>Zhao</keyname><forenames>Huasha</forenames></author><author><keyname>Canny</keyname><forenames>John</forenames></author></authors><title>Sparse Allreduce: Efficient Scalable Communication for Power-Law Data</title><categories>cs.DC cs.AI cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many large datasets exhibit power-law statistics: The web graph, social
networks, text data, click through data etc. Their adjacency graphs are termed
natural graphs, and are known to be difficult to partition. As a consequence
most distributed algorithms on these graphs are communication intensive. Many
algorithms on natural graphs involve an Allreduce: a sum or average of
partitioned data which is then shared back to the cluster nodes. Examples
include PageRank, spectral partitioning, and many machine learning algorithms
including regression, factor (topic) models, and clustering. In this paper we
describe an efficient and scalable Allreduce primitive for power-law data. We
point out scaling problems with existing butterfly and round-robin networks for
Sparse Allreduce, and show that a hybrid approach improves on both.
Furthermore, we show that Sparse Allreduce stages should be nested instead of
cascaded (as in the dense case). And that the optimum throughput Allreduce
network should be a butterfly of heterogeneous degree where degree decreases
with depth into the network. Finally, a simple replication scheme is introduced
to deal with node failures. We present experiments showing significant
improvements over existing systems such as PowerGraph and Hadoop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3024</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3024</id><created>2013-12-10</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Sinop</keyname><forenames>Ali Kemal</forenames></author></authors><title>Rounding Lasserre SDPs using column selection and spectrum-based
  approximation schemes for graph partitioning and Quadratic IPs</title><categories>cs.DS cs.CC</categories><comments>This manuscript is a merged and definitive version of (Guruswami,
  Sinop: FOCS 2011) and (Guruswami, Sinop: SODA 2013), with a significantly
  revised presentation. arXiv admin note: substantial text overlap with
  arXiv:1104.4746</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approximation scheme for minimizing certain Quadratic Integer
Programming problems with positive semidefinite objective functions and global
linear constraints. This framework includes well known graph problems such as
Minimum graph bisection, Edge expansion, Sparsest Cut, and Small Set expansion,
as well as the Unique Games problem. These problems are notorious for the
existence of huge gaps between the known algorithmic results and NP-hardness
results. Our algorithm is based on rounding semidefinite programs from the
Lasserre hierarchy, and the analysis uses bounds for low-rank approximations of
a matrix in Frobenius norm using columns of the matrix.
  For all the above graph problems, we give an algorithm running in time
$n^{O(r/\epsilon^2)}$ with approximation ratio
$\frac{1+\epsilon}{\min\{1,\lambda_r\}}$, where $\lambda_r$ is the $r$'th
smallest eigenvalue of the normalized graph Laplacian $\mathcal{L}$. In the
case of graph bisection and small set expansion, the number of vertices in the
cut is within lower-order terms of the stipulated bound. Our results imply
$(1+O(\epsilon))$ factor approximation in time $n^{O(r^\ast/\epsilon^2)}$ where
is the number of eigenvalues of $\mathcal{L}$ smaller than $1-\epsilon$ (for
variants of sparsest cut, $\lambda_{r^\ast} \ge \mathrm{OPT}/\epsilon$ also
suffices, and as $\mathrm{OPT}$ is usually $o(1)$ on interesting instances of
these problems, this requirement on $r^\ast$ is typically weaker). For Unique
Games, we give a factor $(1+\frac{2+\epsilon}{\lambda_r})$ approximation for
minimizing the number of unsatisfied constraints in $n^{O(r/\epsilon)}$ time,
improving upon an earlier bound for solving Unique Games on expanders. We also
give an algorithm for independent sets in graphs that performs well when the
Laplacian does not have too many eigenvalues bigger than $1+o(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3035</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3035</id><created>2013-12-10</created><authors><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Glashoff</keyname><forenames>Klaus</forenames></author></authors><title>Heat kernel coupling for multiple graph analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce heat kernel coupling (HKC) as a method of
constructing multimodal spectral geometry on weighted graphs of different size
without vertex-wise bijective correspondence. We show that Laplacian averaging
can be derived as a limit case of HKC, and demonstrate its applications on
several problems from the manifold learning and pattern recognition domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3041</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3041</id><created>2013-12-11</created><authors><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Cross-Layer MIMO Transceiver Optimization for Multimedia Streaming in
  Interference Networks</title><categories>cs.IT cs.MM math.IT</categories><comments>16 pages, 12 figures, 1 table. This paper has been accepted by the
  IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2296878</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider dynamic precoder/decorrelator optimization for
multimedia streaming in MIMO interference networks. We propose a truly
cross-layer framework in the sense that the optimization objective is the
application level performance metrics for multimedia streaming, namely the
playback interruption and buffer overflow probabilities. The optimization
variables are the MIMO precoders/decorrelators at the transmitters and the
receivers, which are adaptive to both the instantaneous channel condition and
the playback queue length. The problem is a challenging multi-dimensional
stochastic optimization problem and brute-force solution has exponential
complexity. By exploiting the underlying timescale separation and special
structure in the problem, we derive a closed-form approximation of the value
function based on continuous time perturbation. Using this approximation, we
propose a low complexity dynamic MIMO precoder/decorrelator control algorithm
by solving an equivalent weighted MMSE problem. We also establish the technical
conditions for asymptotic optimality of the low complexity control algorithm.
Finally, the proposed scheme is compared with various baselines through
simulations and it is shown that significant performance gain can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3045</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3045</id><created>2013-12-11</created><authors><author><keyname>Lamersdorf</keyname><forenames>Ansgar</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Rombach</keyname><forenames>Dieter</forenames></author></authors><title>A Decision Model for Supporting Task Allocation Processes in Global
  Software Development</title><categories>cs.SE</categories><comments>15 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-02152-7_25</comments><journal-ref>Product-Focused Software Process Improvement, volume 32 of Lecture
  Notes in Business Information Processing, pages 332-346. Springer Berlin
  Heidelberg, 2009</journal-ref><doi>10.1007/978-3-642-02152-7_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, software-intensive systems are increasingly being developed in a
globally distributed way. However, besides its benefit, global development also
bears a set of risks and problems. One critical factor for successful project
management of distributed software development is the allocation of tasks to
sites, as this is assumed to have a major influence on the benefits and risks.
We introduce a model that aims at improving management processes in globally
distributed projects by giving decision support for task allocation that
systematically regards multiple criteria. The criteria and causal relationships
were identified in a literature study and refined in a qualitative interview
study. The model uses existing approaches from distributed systems and
statistical modeling. The article gives an overview of the problem and related
work, introduces the empirical and theoretical foundations of the model, and
shows the use of the model in an example scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3048</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3048</id><created>2013-12-11</created><authors><author><keyname>Duong</keyname><forenames>Pham Luu Trung</forenames></author><author><keyname>Lee</keyname><forenames>Moonyong</forenames></author></authors><title>Deterministic and stochastic analysis of distributed order systems using
  operational matrix</title><categories>cs.SY</categories><comments>Submitted to Probabilistic Engineering Mechanics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fractional order system, which is described by the fractional order
derivative and integral, has been studied in many engineering areas. Recently,
the concept of fractional order has been generalized to the distributed order
concept, which is a parallel connection of fractional order integrals and
derivatives taken to the infinitesimal limit in delta order. On the other hand,
there are very few numerical methods available for the analysis of distributed
order systems, particularly under stochastic forcing. This paper first proposes
a numerical scheme for analyzing the behavior of a SISO linear system with a
single term distributed order differentiator/integrator using an operational
matrix in the time domain under both deterministic and random forcing. To
assess the stochastic distributed order system, the existing Monte-Carlo,
polynomial chaos and frequency methods are first adopted to the stochastic
distributed order system for comparison. The numerical examples demonstrate the
accuracy and computational efficiency of the proposed method for analyzing
stochastic distributed order systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3059</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3059</id><created>2013-12-11</created><authors><author><keyname>Arai</keyname><forenames>Toshiyasu</forenames></author></authors><title>A polynomial time complete disjunction property in intuitionistic
  propositional logic</title><categories>math.LO cs.LO</categories><msc-class>03F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the polynomial time algorithms due to Buss and Mints(APAL 1999) and
Ferrari, Fiorentini and Fiorino(LPAR 2002) to yield a polynomial time complete
disjunction property in intuitionistic propositional logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3060</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3060</id><created>2013-12-11</created><authors><author><keyname>Istiadi</keyname></author><author><keyname>Sulistiarini</keyname><forenames>Emma Budi</forenames></author></authors><title>Representing Knowledge Base into Database for WAP and Web-based Expert
  System</title><categories>cs.AI cs.CY</categories><journal-ref>International Conference on Information Systems for Business
  Competitiveness (ICISBC 2013), Semarang, Indonesia, December 5, 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Expert System is developed as consulting service for users spread or public
requires affordable access. The Internet has become a medium for such services,
but presence of mobile devices make the access becomes more widespread by
utilizing mobile web and WAP (Wireless Application Protocol). Applying expert
systems applications over the web and WAP requires a knowledge base
representation that can be accessed simultaneously. This paper proposes single
database to accommodate the knowledge representation with decision tree mapping
approach. Because of the database exist, consulting application through both
web and WAP can access it to provide expert system services options for more
affordable for public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3061</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3061</id><created>2013-12-11</created><authors><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Ke</keyname><forenames>Qifa</forenames></author><author><keyname>Zeng</keyname><forenames>Gang</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Fast Approximate $K$-Means via Cluster Closures</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $K$-means, a simple and effective clustering algorithm, is one of the most
widely used algorithms in multimedia and computer vision community. Traditional
$k$-means is an iterative algorithm---in each iteration new cluster centers are
computed and each data point is re-assigned to its nearest center. The cluster
re-assignment step becomes prohibitively expensive when the number of data
points and cluster centers are large.
  In this paper, we propose a novel approximate $k$-means algorithm to greatly
reduce the computational complexity in the assignment step. Our approach is
motivated by the observation that most active points changing their cluster
assignments at each iteration are located on or near cluster boundaries. The
idea is to efficiently identify those active points by pre-assembling the data
into groups of neighboring points using multiple random spatial partition
trees, and to use the neighborhood information to construct a closure for each
cluster, in such a way only a small number of cluster candidates need to be
considered when assigning a data point to its nearest cluster. Using complexity
analysis, image data clustering, and applications to image retrieval, we show
that our approach out-performs state-of-the-art approximate $k$-means
algorithms in terms of clustering quality and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3062</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3062</id><created>2013-12-11</created><authors><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Zeng</keyname><forenames>Gang</forenames></author><author><keyname>Gan</keyname><forenames>Rui</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author><author><keyname>Guo</keyname><forenames>Baining</forenames></author></authors><title>Fast Neighborhood Graph Search using Cartesian Concatenation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new data structure for approximate nearest
neighbor search. This structure augments the neighborhood graph with a bridge
graph. We propose to exploit Cartesian concatenation to produce a large set of
vectors, called bridge vectors, from several small sets of subvectors. Each
bridge vector is connected with a few reference vectors near to it, forming a
bridge graph. Our approach finds nearest neighbors by simultaneously traversing
the neighborhood graph and the bridge graph in the best-first strategy. The
success of our approach stems from two factors: the exact nearest neighbor
search over a large number of bridge vectors can be done quickly, and the
reference vectors connected to a bridge (reference) vector near the query are
also likely to be near the query. Experimental results on searching over large
scale datasets (SIFT, GIST and HOG) show that our approach outperforms
state-of-the-art ANN search algorithms in terms of efficiency and accuracy. The
combination of our approach with the IVFADC system also shows superior
performance over the BIGANN dataset of $1$ billion SIFT features compared with
the best previously published result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3077</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3077</id><created>2013-12-11</created><authors><author><keyname>Kozak</keyname><forenames>Marcin</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>How have the Eastern European countries of the former Warsaw Pact
  developed since 1990? A bibliometric study</title><categories>cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Did the demise of the Soviet Union in 1991 influence the scientific
performance of the researchers in Eastern European countries? Did this
historical event affect international collaboration by researchers from the
Eastern European countries with those of Western countries? Did it also change
international collaboration among researchers from the Eastern European
countries? Trying to answer these questions, this study aims to shed light on
international collaboration by researchers from the Eastern European countries
(Russia, Ukraine, Belarus, Moldova, Bulgaria, the Czech Republic, Hungary,
Poland, Romania and Slovakia). The number of publications and normalized
citation impact values are compared for these countries based on InCites
(Thomson Reuters), from 1981 up to 2011. The international collaboration by
researchers affiliated to institutions in Eastern European countries at the
time points of 1990, 2000 and 2011 was studied with the help of Pajek and
VOSviewer software, based on data from the Science Citation Index (Thomson
Reuters). Our results show that the breakdown of the communist regime did not
lead, on average, to a huge improvement in the publication performance of the
Eastern European countries and that the increase in international co-authorship
relations by the researchers affiliated to institutions in these countries was
smaller than expected. Most of the Eastern European countries are still subject
to changes and are still awaiting their boost in scientific development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3092</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3092</id><created>2013-12-11</created><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Beygi</keyname><forenames>Lotfollah</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Johannisson</keyname><forenames>Pontus</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author></authors><title>A Low-Complexity Detector for Memoryless Polarization-Multiplexed
  Fiber-Optical Channels</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Comm. Lett</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low-complexity detector is introduced for polarization-multiplexed M-ary
phase shift keying modulation in a fiber-optical channel impaired by nonlinear
phase noise, generalizing a previous result by Lau and Kahn for
single-polarization signals. The proposed detector uses phase compensation
based on both received signal amplitudes in conjunction with simple
straight-line rather than four-dimensional maximum-likelihood decision
boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3116</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3116</id><created>2013-12-11</created><authors><author><keyname>Mayer</keyname><forenames>R. V.</forenames></author></authors><title>Various models of process of the learning, based on the numerical
  solution of the differential equations</title><categories>cs.OH</categories><comments>8 pages, 5 figures, on russian; Modern scientific researches and
  innovations 2013 oktober 10</comments><msc-class>68U20</msc-class><acm-class>I.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principles on which can be based computer model of process of training
are formulated. Are considered: 1) the unicomponent model, which is recognizing
that educational information consists of equal elements; 2) the multicomponent
model, which is considering that knowledge is assimilate with a various
strength, and on lesson weak knowledge becomes strong; 3) the generalized
multicomponent model which considers change of working capacity of the pupil
and various complexity of studied elements of a training material. Typical
results of imitating modeling of learning process are presented in article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3134</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3134</id><created>2013-12-11</created><authors><author><keyname>Lunglmayr</keyname><forenames>Michael</forenames></author><author><keyname>Unterrieder</keyname><forenames>Christoph</forenames></author><author><keyname>Huemer</keyname><forenames>Mario</forenames></author></authors><title>Approximate Least Squares</title><categories>cs.DS</categories><comments>Preprint of the paper submitted to IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel iterative algorithm for approximating the linear least
squares solution with low complexity. After a motivation of the algorithm we
discuss the algorithm's properties including its complexity, and we present
theoretical results as well as simulation based performance results. We
describe the analysis of its convergence behavior and show that in the noise
free case the algorithm converges to the least squares solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3139</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3139</id><created>2013-12-11</created><authors><author><keyname>Bellingeri</keyname><forenames>Michele</forenames></author><author><keyname>Cassi</keyname><forenames>Davide</forenames></author><author><keyname>Vincenzi</keyname><forenames>Simone</forenames></author></authors><title>Efficiency of attack strategies on complex model and real-world networks</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>18 pages, 4 figures</comments><doi>10.1016/j.physa.2014.06.079</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigated the efficiency of attack strategies to network nodes when
targeting several complex model and real-world networks. We tested 5 attack
strategies, 3 of which were introduced in this work for the first time, to
attack 3 model (Erdos and Renyi, Barabasi and Albert preferential attachment
network, and scale-free network configuration models) and 3 real networks
(Gnutella peer-to-peer network, email network of the University of Rovira i
Virgili, and immunoglobulin interaction network). Nodes were removed
sequentially according to the importance criterion defined by the attack
strategy. We used the size of the largest connected component (LCC) as a
measure of network damage. We found that the efficiency of attack strategies
(fraction of nodes to be deleted for a given reduction of LCC size) depends on
the topology of the network, although attacks based on the number of
connections of a node and betweenness centrality were often the most efficient
strategies. Sequential deletion of nodes in decreasing order of betweenness
centrality was the most efficient attack strategy when targeting real-world
networks. In particular for networks with power-law degree distribution, we
observed that most efficient strategy change during the sequential removal of
nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3141</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3141</id><created>2013-12-11</created><authors><author><keyname>Couto</keyname><forenames>Joao Pedro</forenames></author><author><keyname>Tiago</keyname><forenames>Teresa</forenames></author><author><keyname>Tiago</keyname><forenames>Flavio</forenames></author></authors><title>An analysis of Internet Banking in Portugal: the antecedents of mobile
  banking adoption</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications, Volume 4 Issue 11(2013), 117-123</journal-ref><doi>10.14569/IJACSA.2013.041116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, mobile operations have gained wide popularity among
mainstream users, and banks tend to follow this trend. But are bank customers
ready to move forward? Mobile banking appears to be a natural extension of
Internet banking. Thus, to predict consumer decisions to adopt mobile banking,
it is useful to understand the pattern of adoption of Internet banking (IB).
This investigation seeks contribute to an expansion of the knowledge regarding
this matter by researching Portuguese consumers patterns and behaviors
concerning the acceptance and use intention of IB as a foundation for
establishing growth strategies of mobile banking. For data collection, we used
an online snowball process. The statistical treatment used included a factor
analysis in order to allow examination of the interrelationships between the
original variables. The analysis was made possible by developing a set of
factors that expresses the common traits between them. The results revealed
that the majority of respondents did not identify problems with the use of and
interaction with the IB service. The study generated some interesting findings.
First, the data generally supports the conceptual framework presented. However,
some points need to be made: trust and convenience, from all the elements
referenced in the literature as relevant from the client perspective, continue
to be a very important elements; the results did not support the paradigm that
the characteristics of individuals affect their behavior as consumers,
individual technological characteristics affect consumer adoption of IB
service; consumer perceptions about the IB service affect their use, as reveal
by the existence of three types of customers that show different practices and
perceptions of IB; and intention to use IB is dependent upon attitudes and
subjective norms regarding the use of IB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3146</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3146</id><created>2013-12-11</created><authors><author><keyname>Rass</keyname><forenames>Stefan</forenames></author></authors><title>Blind Turing-Machines: Arbitrary Private Computations from Group
  Homomorphic Encryption</title><categories>cs.CR</categories><comments>10 pages</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 11, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure function evaluation (SFE) is the process of computing a function (or
running an algorithm) on some data, while keeping the input, output and
intermediate results hidden from the environment in which the function is
evaluated. This can be done using fully homomorphic encryption, Yao's garbled
circuits or secure multiparty computation. Applications are manifold, most
prominently the outsourcing of computations to cloud service providers, where
data is to be manipulated and processed in full confidentiality. Today, one of
the most intensively studied solutions to SFE is fully homomorphic encryption
(FHE). Ever since the first such systems have been discovered in 2009, and
despite much progress, FHE still remains inefficient and difficult to implement
practically. Similar concerns apply to garbled circuits and (generic)
multiparty computation protocols. In this work, we introduce the concept of a
blind Turing-machine, which uses simple homomorphic encryption (an extension of
ElGamal encryption) to process ciphertexts in the way as standard
Turing-machines do, thus achieving computability of any function in total
privacy. Remarkably, this shows that fully homomorphic encryption is indeed an
overly strong primitive to do SFE, as group homomorphic encryption with
equality check is already sufficient. Moreover, the technique is easy to
implement and perhaps opens the door to efficient private computations on
nowadays computing machinery, requiring only simple changes to well-established
computer architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3158</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3158</id><created>2013-12-11</created><authors><author><keyname>Murphy</keyname><forenames>James</forenames></author></authors><title>Benders, Nested Benders and Stochastic Programming: An Intuitive
  Introduction</title><categories>math.OC cs.DS</categories><comments>57 pages, 21 figures</comments><report-no>CUED/F-INFENG/TR.675</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article aims to explain the Nested Benders algorithm for the solution of
large-scale stochastic programming problems in a way that is intelligible to
someone coming to it for the first time. In doing so it gives an explanation of
Benders decomposition and of its application to two-stage stochastic
programming problems (also known in this context as the L-shaped method), then
extends this to multi-stage problems as the Nested Benders algorithm. The
article is aimed at readers with some knowledge of linear and possibly
stochastic programming but aims to develop most concepts from simple principles
in an understandable way. The focus is on intuitive understanding rather than
rigorous proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3168</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3168</id><created>2013-12-11</created><authors><author><keyname>Mery</keyname><forenames>Bruno</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI</affiliation></author></authors><title>Semantic Types, Lexical Sorts and Classifiers</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>NLPCS '10- 10th International Workshop on Natural Language
  Processing and Computer Science - 2013 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a cognitively and linguistically motivated set of sorts for
lexical semantics in a compositional setting: the classifiers in languages that
do have such pronouns. These sorts are needed to include lexical considerations
in a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical
extensions of usual Montague semantics to model restriction of selection,
felicitous and infelicitous copredication require a rich and refined type
system whose base types are the lexical sorts, the basis of the many-sorted
logic in which semantical representations of sentences are stated. However,
none of those approaches define precisely the actual base types or sorts to be
used in the lexicon. In this article, we shall discuss some of the options
commonly adopted by researchers in formal lexical semantics, and defend the
view that classifiers in the languages which have such pronouns are an
appealing solution, both linguistically and cognitively motivated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3182</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3182</id><created>2013-12-11</created><authors><author><keyname>R</keyname><forenames>Ram Kumar.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Kannan</forenames></author><author><keyname>Changat</keyname><forenames>Manoj</forenames></author><author><keyname>Sreekumar</keyname><forenames>A.</forenames></author><author><keyname>Narasimha-Shenoi</keyname><forenames>Prasanth G.</forenames></author></authors><title>On The Center Sets and Center Numbers of Some Graph Classes</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a set $S$ of vertices and the vertex $v$ in a connected graph $G$,
$\displaystyle\max_{x \in S}d(x,v)$ is called the $S$-eccentricity of $v$ in
$G$. The set of vertices with minimum $S$-eccentricity is called the $S$-center
of $G$. Any set $A$ of vertices of $G$ such that $A$ is an $S$-center for some
set $S$ of vertices of $G$ is called a center set. We identify the center sets
of certain classes of graphs namely, Block graphs, $K_{m,n}$, $K_n-e$, wheel
graphs, odd cycles and symmetric even graphs and enumerate them for many of
these graph classes. We also introduce the concept of center number which is
defined as the number of distinct center sets of a graph and determine the
center number of some graph classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3183</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3183</id><created>2013-12-11</created><authors><author><keyname>Balogun</keyname><forenames>Adedayo M.</forenames></author><author><keyname>Zhu</keyname><forenames>Shao Ying</forenames></author></authors><title>Privacy Impacts of Data Encryption on the Efficiency of Digital
  Forensics Technology</title><categories>cs.CR</categories><comments>5 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to a number of reasons, the deployment of encryption solutions are
beginning to be ubiquitous at both organizational and individual levels. The
most emphasized reason is the necessity to ensure confidentiality of privileged
information. Unfortunately, it is also popular as cyber-criminals' escape route
from the grasp of digital forensic investigations. The direct encryption of
data or indirect encryption of storage devices, more often than not, prevents
access to such information contained therein. This consequently leaves the
forensics investigation team, and subsequently the prosecution, little or no
evidence to work with, in sixty percent of such cases. However, it is
unthinkable to jeopardize the successes brought by encryption technology to
information security, in favour of digital forensics technology. This paper
examines what data encryption contributes to information security, and then
highlights its contributions to digital forensics of disk drives. The paper
also discusses the available ways and tools, in digital forensics, to get
around the problems constituted by encryption. A particular attention is paid
to the Truecrypt encryption solution to illustrate ideas being discussed. It
then compares encryption's contributions in both realms, to justify the need
for introduction of new technologies to forensically defeat data encryption as
the only solution, whilst maintaining the privacy goal of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3184</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3184</id><created>2013-12-11</created><authors><author><keyname>Keil</keyname><forenames>Matthias</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>Efficient Dynamic Access Analysis Using JavaScript Proxies</title><categories>cs.PL</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  JSConTest introduced the notions of effect monitoring and dynamic effect
inference for JavaScript. It enables the description of effects with path
specifications resembling regular expressions. It is implemented by an offline
source code transformation.
  To overcome the limitations of the JSConTest implementation, we redesigned
and reimplemented effect monitoring by taking advantange of JavaScript proxies.
Our new design avoids all drawbacks of the prior implementation. It guarantees
full interposition; it is not restricted to a subset of JavaScript; it is
self-maintaining; and its scalability to large programs is significantly better
than with JSConTest.
  The improved scalability has two sources. First, the reimplementation is
significantly faster than the original, transformation-based implementation.
Second, the reimplementation relies on the fly-weight pattern and on trace
reduction to conserve memory. Only the combination of these techniques enables
monitoring and inference for large programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3188</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3188</id><created>2013-12-11</created><authors><author><keyname>Alvarez</keyname><forenames>Victor</forenames></author><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author></authors><title>A Simple Sweep Line Algorithm for Counting Triangulations and
  Pseudo-triangulations</title><categories>cs.CG cs.DS math.CO</categories><comments>38 pages, 48 figures. Submitted to journal</comments><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P\subset\mathbb{R}^{2}$ be a set of $n$ points. In this paper we show
two new algorithms, one to compute the number of triangulations of $P$, and one
to compute the number of pseudo-triangulations of $P$. We show that our
algorithms run in time $O^{*}(t(P))$ and $O^{*}(pt(P))$ respectively, where
$t(P)$ and $pt(P)$ are the largest number of triangulation paths (T-paths) and
pseudo-triangulations paths (PT-paths), respectively, that the algorithms
encounter during their execution. Moreover, we show that $t(P) = O^{*}(9^{n})$,
which is the first non-trivial bound on $t(P)$ to be known.
  While there already are algorithms that count triangulations in
$O^{*}\left(2^n\right)$, and $O^{*}\left(3.1414^{n}\right)$, there are sets of
points where the number of T-paths is $O(2^{n})$. In such cases the algorithm
herein presented could potentially be faster. Furthermore, it is not clear
whether the already-known algorithms can be modified to count
pseudo-triangulations so that their running times remain $O^{*}(c^n)$, for some
small constant $c\in\mathbb{R}$. Therefore, for counting pseudo-triangulations
(and possibly other similar structures) our approach seems better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3193</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3193</id><created>2013-12-11</created><authors><author><keyname>Miles</keyname><forenames>Eric</forenames></author></authors><title>Iterated group products and leakage resilience against NC^1</title><categories>cs.CC</categories><comments>ITCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if NC$^1 \neq$ L, then for every element $\alpha$ of the
alternating group $A_t$, circuits of depth $O(\log t)$ cannot distinguish
between a uniform vector over $(A_t)^t$ with product $= \alpha$ and one with
product $=$ identity. Combined with a recent construction by the author and
Viola in the setting of leakage-resilient cryptography [STOC '13], this gives a
compiler that produces circuits withstanding leakage from NC$^1$ (assuming
NC$^1 \neq$ L). For context, leakage from NC$^1$ breaks nearly all previous
constructions, and security against leakage from P is impossible. %In the
multi-query setting, circuits produced by this compiler use a simple secure
hardware component.
  We build on work by Cook and McKenzie [J.\ Algorithms '87] establishing the
relationship between L $=$ logarithmic space and the symmetric group $S_t$. Our
techniques include a novel algorithmic use of commutators to manipulate the
cycle structure of permutations in $A_t$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3194</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3194</id><created>2013-12-11</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Error-Correcting Regenerating and Locally Repairable Codes via
  Rank-Metric Codes</title><categories>cs.IT math.IT</categories><comments>This paper was presented in part at the 2012 50th Annual Allerton
  Conference on Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and analyzes a novel concatenated coding scheme for
enabling error resilience in two distributed storage settings: one being
storage using existing regenerating codes and the second being storage using
locally repairable codes. The concatenated coding scheme brings together a
maximum rank distance (MRD) code as an outer code and either a globally
regenerating or a locally repairable code as an inner code. Also, error
resilience for combination of locally repairable codes with regenerating codes
is considered. This concatenated coding system is designed to handle two
different types of adversarial errors: the first type includes an adversary
that can replace the content of an affected node only once; while the second
type studies an adversary that is capable of polluting data an unbounded number
of times. The paper establishes an upper bound on the resilience capacity for a
locally repairable code and proves that this concatenated coding coding scheme
attains the upper bound on resilience capacity for the first type of adversary.
Further, the paper presents mechanisms that combine the presented concatenated
coding scheme with subspace signatures to achieve error resilience for the
second type of errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3198</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3198</id><created>2013-12-11</created><updated>2015-02-28</updated><authors><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panagiotis</forenames></author></authors><title>Secrecy Capacity Scaling in Large Cooperative Wireless Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>The material in this paper has been presented in part in the IEEE
  INFOCOM, Toronto, Canada, April-May 2014 and in the Iran Workshop on
  Communication and Information Theory (IWCIT), Tehran, Iran, May 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate large wireless networks subject to security constraints. In
contrast to point-to-point, interference-limited communications considered in
prior works, we propose active cooperative relaying based schemes. We consider
a network with $n_l$ legitimate nodes, $n_e$ eavesdroppers, and path loss
exponent $\alpha\geq 2$. As long as $n_e^2(\log(n_e))^{\gamma}=o(n_l)$, for
some positive $\gamma$, we show one can obtain unbounded secure aggregate rate.
This means zero-cost secure communication, given fixed total power constraint
for the entire network. We achieve this result through (i) the source using
Wyner randomized encoder and a serial (multi-stage) block Markov scheme, to
cooperate with the relays and (ii) the relays acting as a virtual multi-antenna
to apply beamforming against the eavesdroppers. Our simpler parallel
(two-stage) relaying scheme can achieve the same unbounded secure aggregate
rate when
$n_e^{\frac{\alpha}{2}+1}(\log(n_e))^{\gamma+\delta(\frac{\alpha}{2}+1)}=o(n_l)$
holds, for some positive $\gamma,\delta$. Finally, we study the improvement (to
the detriment of legitimate nodes) the eavesdroppers achieve in terms of the
information leakage rate in a large cooperative network in case of collusion.
We show that again the zero-cost secure communication is possible, if
$n_e^{(2+\frac{2}{\alpha})}(\log n_e)^{\gamma}=o(n_l)$ holds, for some positive
$\gamma$; i.e., in case of collusion slightly fewer eavesdroppers can be
tolerated compared to the non-colluding case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3199</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3199</id><created>2013-12-11</created><authors><author><keyname>Kafieh</keyname><forenames>Raheleh</forenames></author><author><keyname>Rabbani</keyname><forenames>Hossein</forenames></author><author><keyname>Hajizadeh</keyname><forenames>Fedra</forenames></author><author><keyname>Abramoff</keyname><forenames>Michael D.</forenames></author><author><keyname>Sonka</keyname><forenames>Milan</forenames></author></authors><title>Thickness Mapping of Eleven Retinal Layers in Normal Eyes Using Spectral
  Domain Optical Coherence Tomography</title><categories>cs.CV</categories><comments>32 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose. This study was conducted to determine the thickness map of eleven
retinal layers in normal subjects by spectral domain optical coherence
tomography (SD-OCT) and evaluate their association with sex and age. Methods.
Mean regional retinal thickness of 11 retinal layers were obtained by automatic
three-dimensional diffusion-map-based method in 112 normal eyes of 76 Iranian
subjects. Results. The thickness map of central foveal area in layer 1, 3, and
4 displayed the minimum thickness (P&lt;0.005 for all). Maximum thickness was
observed in nasal to the fovea of layer 1 (P&lt;0.001) and in a circular pattern
in the parafoveal retinal area of layers 2, 3 and 4 and in central foveal area
of layer 6 (P&lt;0.001). Temporal and inferior quadrants of the total retinal
thickness and most of other quadrants of layer 1 were significantly greater in
the men than in the women. Surrounding eight sectors of total retinal thickness
and a limited number of sectors in layer 1 and 4 significantly correlated with
age. Conclusion. SD-OCT demonstrated the three-dimensional thickness
distribution of retinal layers in normal eyes. Thickness of layers varied with
sex and age and in different sectors. These variables should be considered
while evaluating macular thickness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3200</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3200</id><created>2013-12-11</created><authors><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panagiotis</forenames></author></authors><title>Constrained Colluding Eavesdroppers: An Information-Theoretic Model</title><categories>cs.IT cs.CR math.IT</categories><comments>A shorter version of this paper was accepted to International Zurich
  Seminar (IZS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the secrecy capacity in the vicinity of colluding eavesdroppers.
Contrary to the perfect collusion assumption in previous works, our new
information-theoretic model considers constraints in collusion. We derive the
achievable secure rates (lower bounds on the perfect secrecy capacity), both
for the discrete memoryless and Gaussian channels. We also compare the proposed
rates to the non-colluding and perfect colluding cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3203</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3203</id><created>2013-12-04</created><authors><author><keyname>Aldroubi</keyname><forenames>Akram</forenames></author><author><keyname>Davis</keyname><forenames>Jacqueline</forenames></author><author><keyname>Krishtal</keyname><forenames>Ilya</forenames></author></authors><title>Exact Reconstruction of Spatially Undersampled Signals in Evolutionary
  Systems</title><categories>cs.OH math.FA</categories><doi>10.1007/s00041-014-9359-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of spatiotemporal sampling in which an initial state
$f$ of an evolution process $f_t=A_tf$ is to be recovered from a combined set
of coarse samples from varying time levels $\{t_1,\dots,t_N\}$. This new way of
sampling, which we call dynamical sampling, differs from standard sampling
since at any fixed time $t_i$ there are not enough samples to recover the
function $f$ or the state $f_{t_i}$. Although dynamical sampling is an inverse
problem, it differs from the typical inverse problems in which $f$ is to be
recovered from $A_Tf$ for a single time $T$. In this paper, we consider signals
that are modeled by $\ell^2(\mathbb Z)$ or a shift invariant space $V\subset
L^2(\mathbb R)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3213</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3213</id><created>2013-12-08</created><authors><author><keyname>Abiteboul</keyname><forenames>Serge</forenames><affiliation>LSV, INRIA Saclay - Ile de France</affiliation></author></authors><title>Les connaissances de la toile</title><categories>cs.GL</categories><comments>in French; Cultures num\'eriques, \'education aux m\'edias et \`a
  l'information (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to manage knowledge on the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3214</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3214</id><created>2013-12-09</created><updated>2014-07-05</updated><authors><author><keyname>Aboulker</keyname><forenames>Pierre</forenames></author><author><keyname>Kapadia</keyname><forenames>Rohan</forenames></author></authors><title>The Chen-Chv\'atal conjecture for metric spaces induced by
  distance-hereditary graphs</title><categories>math.MG cs.DM math.CO</categories><comments>11 pages, 0 figures</comments><msc-class>05C99, 05D99, 51G99</msc-class><journal-ref>European J. Combin. 43 (2015), 1-7</journal-ref><doi>10.1016/j.ejc.2014.06.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A special case of a theorem of De Bruijn and Erd\H{o}s asserts that any
noncollinear set of $n$ points in the plane determines at least $n$ distinct
lines. Chen and Chv\'atal conjectured a generalization of this result to
arbitrary finite metric spaces, with a particular definition of lines in a
metric space. We prove it for metric spaces induced by connected
distance-hereditary graphs -- a graph $G$ is called distance-hereditary if the
distance between two vertices $u$ and $v$ in any connected induced subgraph $H$
of $G$ is equal to the distance between $u$ and $v$ in $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3215</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3215</id><created>2013-12-09</created><updated>2016-01-05</updated><authors><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Decomposition of multiple packings with subquadratic union complexity</title><categories>math.MG cs.CG cs.DM math.CO</categories><comments>Small generalization of the main result, improvements in the proofs,
  minor corrections</comments><msc-class>52C15, 05B40</msc-class><journal-ref>Combinator. Probab. Comp. 25 (2016) 145-153</journal-ref><doi>10.1017/S0963548315000280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $k$ is a positive integer and $\mathcal{X}$ is a $k$-fold packing of
the plane by infinitely many arc-connected compact sets, which means that every
point of the plane belongs to at most $k$ sets. Suppose there is a function
$f(n)=o(n^2)$ with the property that any $n$ members of $\mathcal{X}$ determine
at most $f(n)$ holes, which means that the complement of their union has at
most $f(n)$ bounded connected components. We use tools from extremal graph
theory and the topological Helly theorem to prove that $\mathcal{X}$ can be
decomposed into at most $p$ ($1$-fold) packings, where $p$ is a constant
depending only on $k$ and $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3222</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3222</id><created>2013-12-10</created><authors><author><keyname>Pasztor</keyname><forenames>Attila</forenames></author><author><keyname>Pap-Szigeti</keyname><forenames>Robert</forenames></author><author><keyname>Torok</keyname><forenames>Erika</forenames></author></authors><title>Mobile Robots in Teaching Programming for IT Engineers and its Effects</title><categories>cs.CY cs.RO</categories><comments>Article Published in International Journal of Advanced Computer
  Science and Applications(IJACSA), Volume 4 Issue 11, 2013</comments><doi>10.14569/IJACSA.2013.041123</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the new methods and devices introduced into the learning
process of programming for IT engineers at our college is described. Based on
our previous research results we supposed that project methods and some new
devices can reduce programming problems during the first term. These problems
are rooted in the difficulties of abstract thinking and they can cause the
decrease of programming self-concept and other learning motives. We redesigned
the traditional learning environment. As a constructive approach project method
was used. Our students worked in groups of two or three; small problems were
solved after every lesson. In the problem solving process students use
programmable robots (e.g. Surveyor, LEGO NXT and RCX). They had to plan their
program, solve some technical problems and test their solution. The usability
of mobile robots in the learning process and the short-term efficiency of our
teaching method were checked with a control group after a semester (n = 149).
We examined the effects on our students' programming skills and on their
motives, mainly on their attitudes and programming self-concept. After a
two-year-long period we could measure some positive long-term effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3230</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3230</id><created>2013-12-11</created><authors><author><keyname>Andrychowicz</keyname><forenames>Marcin</forenames></author><author><keyname>Dziembowski</keyname><forenames>Stefan</forenames></author><author><keyname>Malinowski</keyname><forenames>Daniel</forenames></author><author><keyname>Mazurek</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>How to deal with malleability of BitCoin transactions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BitCoin transactions are malleable in a sense that given a transaction an
adversary can easily construct an equivalent transaction which has a different
hash. This can pose a serious problem in some BitCoin distributed contracts in
which changing a transaction's hash may result in the protocol disruption and a
financial loss. The problem mostly concerns protocols, which use a &quot;refund&quot;
transaction to withdraw a deposit in a case of the protocol interruption. In
this short note, we show a general technique for creating
malleability-resilient &quot;refund&quot; transactions, which does not require any
modification of the BitCoin protocol. Applying our technique to our previous
paper &quot;Fair Two-Party Computations via the BitCoin Deposits&quot; (Cryptology ePrint
Archive, 2013) allows to achieve fairness in any Two-Party Computation using
the BitCoin protocol in its current version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3234</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3234</id><created>2013-12-11</created><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames></author><author><keyname>Gomez-Gardenes</keyname><forenames>Jesus</forenames></author></authors><title>Communicability reveals a transition to coordinated behavior in
  multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 2 figures, 3 tables</comments><doi>10.1103/PhysRevE.89.042819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the flow of information in multiplex networks by means of the
communicability function. First, we generalize this measure from its definition
from simple graphs to multiplex networks. Then, we study its relevance for the
analysis of real-world systems by studying a social multiplex where information
flows using formal/informal channels and an air transportation system where the
layers represent different air companies. Accordingly, the communicability,
which is essential for the good performance of these complex systems, emerges
at a systemic operation point in the multiplex where the performance of the
layers operates in a coordinated way very differently from the state
represented by a collection of unconnected networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3240</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3240</id><created>2013-12-11</created><updated>2014-07-30</updated><authors><author><keyname>Vezhnevets</keyname><forenames>Alexander</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Associative embeddings for large-scale knowledge transfer with
  self-assessment</title><categories>cs.CV</categories><comments>A final CVPR version with a correction in (1). IEEE Computer Vision
  and Pattern Recognition, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for knowledge transfer between semantically related
classes in ImageNet. By transferring knowledge from the images that have
bounding-box annotations to the others, our method is capable of automatically
populating ImageNet with many more bounding-boxes and even pixel-level
segmentations. The underlying assumption that objects from semantically related
classes look alike is formalized in our novel Associative Embedding (AE)
representation. AE recovers the latent low-dimensional space of appearance
variations among image windows. The dimensions of AE space tend to correspond
to aspects of window appearance (e.g. side view, close up, background). We
model the overlap of a window with an object using Gaussian Processes (GP)
regression, which spreads annotation smoothly through AE space. The
probabilistic nature of GP allows our method to perform self-assessment, i.e.
assigning a quality estimate to its own output. It enables trading off the
amount of returned annotations for their quality. A large scale experiment on
219 classes and 0.5 million images demonstrates that our method outperforms
state-of-the-art methods and baselines for both object localization and
segmentation. Using self-assessment we can automatically return bounding-box
annotations for 30% of all images with high localization accuracy (i.e.~73%
average overlap with ground-truth).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3245</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3245</id><created>2013-12-11</created><updated>2014-02-27</updated><authors><author><keyname>Truong</keyname><forenames>Hien Thi Thu</forenames></author><author><keyname>Lagerspetz</keyname><forenames>Eemil</forenames></author><author><keyname>Nurmi</keyname><forenames>Petteri</forenames></author><author><keyname>Oliner</keyname><forenames>Adam J.</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourav</forenames></author></authors><title>The Company You Keep: Mobile Malware Infection Rates and Inexpensive
  Risk Indicators</title><categories>cs.CR</categories><acm-class>D.4.6</acm-class><doi>10.1145/2566486.2568046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is little information from independent sources in the public domain
about mobile malware infection rates. The only previous independent estimate
(0.0009%) [12], was based on indirect measurements obtained from domain name
resolution traces. In this paper, we present the first independent study of
malware infection rates and associated risk factors using data collected
directly from over 55,000 Android devices. We find that the malware infection
rates in Android devices estimated using two malware datasets (0.28% and
0.26%), though small, are significantly higher than the previous independent
estimate. Using our datasets, we investigate how indicators extracted
inexpensively from the devices correlate with malware infection. Based on the
hypothesis that some application stores have a greater density of malicious
applications and that advertising within applications and cross-promotional
deals may act as infection vectors, we investigate whether the set of
applications used on a device can serve as an indicator for infection of that
device. Our analysis indicates that this alone is not an accurate indicator for
pinpointing infection. However, it is a very inexpensive but surprisingly
useful way for significantly narrowing down the pool of devices on which
expensive monitoring and analysis mechanisms must be deployed. Using our two
malware datasets we show that this indicator performs 4.8 and 4.6 times
(respectively) better at identifying infected devices than the baseline of
random checks. Such indicators can be used, for example, in the search for new
or previously undetected malware. It is therefore a technique that can
complement standard malware scanning by anti-malware tools. Our analysis also
demonstrates a marginally significant difference in battery use between
infected and clean devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3248</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3248</id><created>2013-12-11</created><updated>2013-12-16</updated><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author><author><keyname>Amsterdamer</keyname><forenames>Yael</forenames></author><author><keyname>Milo</keyname><forenames>Tova</forenames></author></authors><title>On the Complexity of Mining Itemsets from the Crowd Using Taxonomies</title><categories>cs.DB cs.CC cs.IR</categories><comments>18 pages, 2 figures. To be published to ICDT'13. Added missing
  acknowledgement</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of frequent itemset mining in domains where data is not
recorded in a conventional database but only exists in human knowledge. We
provide examples of such scenarios, and present a crowdsourcing model for them.
The model uses the crowd as an oracle to find out whether an itemset is
frequent or not, and relies on a known taxonomy of the item domain to guide the
search for frequent itemsets. In the spirit of data mining with oracles, we
analyze the complexity of this problem in terms of (i) crowd complexity, that
measures the number of crowd questions required to identify the frequent
itemsets; and (ii) computational complexity, that measures the computational
effort required to choose the questions. We provide lower and upper complexity
bounds in terms of the size and structure of the input taxonomy, as well as the
size of a concise description of the output itemsets. We also provide
constructive algorithms that achieve the upper bounds, and consider more
efficient variants for practical situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3251</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3251</id><created>2013-12-11</created><authors><author><keyname>Kalita</keyname><forenames>Nayan Jyoti</forenames></author><author><keyname>Saharia</keyname><forenames>Navanath</forenames></author><author><keyname>Sinha</keyname><forenames>Smriti Kumar</forenames></author></authors><title>Towards The Development of a Bishnupriya Manipuri Corpus</title><categories>cs.CL</categories><comments>5 pages, conference at National Conference on Recent Trends in
  Computer Sciences at Bodoland University, 25th-26th March, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any deep computational processing of language we need evidences, and one
such set of evidences is corpus. This paper describes the development of a
text-based corpus for the Bishnupriya Manipuri language. A Corpus is considered
as a building block for any language processing tasks. Due to the lack of
awareness like other Indian languages, it is also studied less frequently. As a
result the language still lacks a good corpus and basic language processing
tools. As per our knowledge this is the first effort to develop a corpus for
Bishnupriya Manipuri language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3258</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3258</id><created>2013-12-11</created><authors><author><keyname>Ouertani</keyname><forenames>Henda Chorfi</forenames></author></authors><title>Implicit Sensitive Text Summarization based on Data Conveyed by
  Connectives</title><categories>cs.CL</categories><comments>4 pages, 2 figures, journal IJACSA; (IJACSA) International Journal of
  Advanced Computer Science and Applications, Vol. 4, No. 11, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far and trying to reach human capabilities, research in automatic
summarization has been based on hypothesis that are both enabling and limiting.
Some of these limitations are: how to take into account and reflect (in the
generated summary) the implicit information conveyed in the text, the author
intention, the reader intention, the context influence, the general world
knowledge. Thus, if we want machines to mimic human abilities, then they will
need access to this same large variety of knowledge. The implicit is affecting
the orientation and the argumentation of the text and consequently its summary.
Most of Text Summarizers (TS) are processing as compressing the initial data
and they necessarily suffer from information loss. TS are focusing on features
of the text only, not on what the author intended or why the reader is reading
the text. In this paper, we address this problem and we present a system
focusing on acquiring knowledge that is implicit. We principally spotlight the
implicit information conveyed by the argumentative connectives such as: but,
even, yet and their effect on the summary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3263</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3263</id><created>2013-12-11</created><updated>2014-02-20</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>Stable Embedding of Grassmann Manifold via Gaussian Random matrices</title><categories>cs.IT math.IT</categories><comments>To be submitted to Journals</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore a volume-based stable embedding of
multi-dimensional signals based on Grassmann manifold, via Gaussian random
measurement matrices. The Grassmann manifold is a topological space in which
each point is a linear vector subspace, and is widely regarded as an ideal
model for multi-dimensional signals. In this paper, we formulate the linear
subspace spanned by multi-dimensional signal vectors as points on the Grassmann
manifold, and use the volume and the product of sines of principal angles (also
known as the product of principal sines) as the generalized norm and distance
measure for the space of Grassmann manifold. We prove a volume-preserving
embedding property for points on the Grassmann manifold via Gaussian random
measurement matrices, i.e., the volumes of all parallelotopes from a finite set
in Grassmann manifold are preserved upon compression. This volume-preserving
embedding property is a multi-dimensional generalization of the conventional
stable embedding properties, which only concern the approximate preservation of
lengths of vectors in certain unions of subspaces. Additionally, we use the
volume-preserving embedding property to explore the stable embedding effect on
a generalized distance measure of Grassmann manifold induced from volume. It is
proved that the generalized distance measure, i.e., the product of principal
sines between different points on the Grassmann manifold, is well preserved in
the compressed domain via Gaussian random measurement matrices.Numerical
simulations are also provided for validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3269</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3269</id><created>2013-12-11</created><updated>2014-03-14</updated><authors><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Cai</keyname><forenames>Yongjian</forenames></author></authors><title>Power Scheduling of Kalman Filtering in Wireless Sensor Networks with
  Data Packet Drops</title><categories>cs.SY</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a wireless sensor network (WSN) with a large number of low-cost,
battery-driven, multiple transmission power leveled sensor nodes of limited
transmission bandwidth, then conservation of transmission resources (power and
bandwidth) is of paramount importance. Towards this end, this paper considers
the problem of power scheduling of Kalman filtering for general linear
stochastic systems subject to data packet drops (over a packet-dropping
wireless network). The transmission of the acquired measurement from the sensor
to the remote estimator is realized by sequentially transmitting every single
component of the measurement to the remote estimator in one time period. The
sensor node decides separately whether to use a high or low transmission power
to communicate every component to the estimator across a packet-dropping
wireless network based on the rule that promotes the power scheduling with the
least impact on the estimator mean squared error. Under the customary
assumption that the predicted density is (approximately) Gaussian, leveraging
the statistical distribution of sensor data, the mechanism of power scheduling,
the wireless network effect and the received data, the minimum mean squared
error estimator is derived. By investigating the statistical convergence
properties of the estimation error covariance, we establish, for general linear
systems, both the sufficient condition and the necessary condition guaranteeing
the stability of the estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3270</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3270</id><created>2013-12-11</created><updated>2013-12-15</updated><authors><author><keyname>Dur&#xe1;n</keyname><forenames>Antonio J.</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Mario</forenames></author><author><keyname>Varona</keyname><forenames>Juan L.</forenames></author></authors><title>Misfortunes of a mathematicians' trio using Computer Algebra Systems:
  Can we trust?</title><categories>cs.SC cs.MS</categories><comments>4 pages</comments><msc-class>68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer algebra systems are a great help for mathematical research but
sometimes unexpected errors in the software can also badly affect it. As an
example, we show how we have detected an error of Mathematica computing
determinants of matrices of integer numbers: not only it computes the
determinants wrongly, but also it produces different results if one evaluates
the same determinant twice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3288</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3288</id><created>2013-12-11</created><authors><author><keyname>Pinheiro</keyname><forenames>Rian G. S.</forenames></author><author><keyname>Martins</keyname><forenames>Ivan C.</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Ochi</keyname><forenames>Luiz S.</forenames></author><author><keyname>Simonetti</keyname><forenames>Luidi G.</forenames></author><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author></authors><title>On Solving Manufacturing Cell Formation via Bicluster Editing</title><categories>math.OC cs.DS</categories><comments>19 pages</comments><msc-class>90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the Bicluster Graph Editing Problem (BGEP) and how it
can be applied to solve the Manufacturing Cell Formation Problem (MCFP). We
develop an exact method for the BGEP that consists of a Branch-and-Cut approach
combined with a special separation algorithm based on dynamic programming. We
also describe a new preprocessing procedure for the BGEP derived from
theoretical results on vertex distances in the input graph. Computational
experiments performed on randomly generated instances with various levels of
difficulty show that our separation algorithm accelerates the convergence
speed, and our preprocessing procedure is effective for low density instances.
Other contribution of this work is to reveal the similarities between the BGEP
and the MCFP. We show that the BGEP and the MCFP have the same solution space.
This fact leads to the proposal of two new exact approaches for the MCFP based
on mathematical formulations for the BGEP. Both approaches use the grouping
efficacy measure as the objective function. Up to the authors' knowledge, these
are the first exact methods that employ such a measure to optimally solve
instances of the MCFP. The first approach consists of iteratively running
several calls to a parameterized version of the BGEP, and the second is a
linearization of a new fractional-linear model for the MCFP. Computational
experiments performed on instances of the MCFP found in the literature show
that our exact methods for the MCFP are able to prove several previously
unknown optima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3295</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3295</id><created>2013-12-11</created><updated>2014-02-06</updated><authors><author><keyname>Sarkar</keyname><forenames>Chayan</forenames></author><author><keyname>Rao</keyname><forenames>Vijay S.</forenames></author><author><keyname>Prasad</keyname><forenames>R. Venkatesha</forenames></author></authors><title>No-Sense: Sense with Dormant Sensors</title><categories>cs.NI</categories><comments>Accepted for publication in IEEE Twentieth National Conference on
  Communications (NCC-2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have enabled continuous monitoring of an area
of interest (body, room, region, etc.) while eliminating expensive wired
infrastructure. Typically in such applications, wireless sensor nodes report
the sensed values to a sink node, where the information is required for the
end-user. WSNs also provide the flexibility to the end-user for choosing
several parameters for the monitoring application. For example, placement of
sensors, frequency of sensing and transmission of those sensed data. Over the
years, the advancement in embedded technology has led to increased processing
power and memory capacity of these battery powered devices. However, batteries
can only supply limited energy, thus limiting the lifetime of the network. In
order to prolong the lifetime of the deployment, various efforts have been made
to improve the battery technologies and also reduce the energy consumption of
the sensor node at various layers in the networking stack. Of all the
operations in the network stack, wireless data transmission and reception have
found to consume most of the energy. Hence many proposals found in the
literature target reducing them through intelligent schemes like power control,
reducing retransmissions, etc. In this article we propose a new framework
called Virtual Sensing Framework (VSF), which aims to sufficiently satisfy
application requirements while conserving energy at the sensor nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3300</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3300</id><created>2013-12-11</created><authors><author><keyname>Revol</keyname><forenames>Nathalie</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Th&#xe9;veny</keyname><forenames>Philippe</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme, LIP</affiliation></author></authors><title>Numerical Reproducibility and Parallel Computations: Issues for Interval
  Algorithms</title><categories>cs.NA cs.DC</categories><comments>submitted to IEEE Transactions on Computers</comments><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Computers (2014)</journal-ref><doi>10.1109/TC.2014.2322593</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is called &quot;numerical reproducibility&quot; is the problem of getting the same
result when the scientific computation is run several times, either on the same
machine or on different machines, with different types and numbers of
processing units, execution environments, computational loads etc. This problem
is especially stringent for HPC numerical simulations. In what follows, the
focus is on parallel implementations of interval arithmetic using
floating-point arithmetic. For interval computations, numerical reproducibility
is of course an issue for testing and debugging purposes. However, as long as
the computed result encloses the exact and unknown result, the inclusion
property, which is the main property of interval arithmetic, is satisfied and
getting bit for bit identical results may not be crucial. Still, implementation
issues may invalidate the inclusion property. Several ways to preserve the
inclusion property are presented, on the example of the product of matrices
with interval coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3303</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3303</id><created>2013-12-11</created><authors><author><keyname>Butelle</keyname><forenames>Franck</forenames><affiliation>LIPN</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author><author><keyname>Bui</keyname><forenames>Marc</forenames><affiliation>CHART</affiliation></author></authors><title>A Uniform Self-Stabilizing Minimum Diameter Spanning Tree Algorithm</title><categories>cs.DC cs.DS</categories><comments>14 pages; International conf\'erence; Uniform self-stabilizing
  variant of the problem, 9th International Workshop on Distributed Algorithms
  (WDAG'95), Mont-Saint-Michel : France (1995)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a uniform self-stabilizing algorithm, which solves the problem of
distributively finding a minimum diameter spanning tree of an arbitrary
positively real-weighted graph. Our algorithm consists in two stages of
stabilizing protocols. The first stage is a uniform randomized stabilizing {\em
unique naming} protocol, and the second stage is a stabilizing {\em MDST}
protocol, designed as a {\em fair composition} of Merlin--Segall's stabilizing
protocol and a distributed deterministic stabilizing protocol solving the
(MDST) problem. The resulting randomized distributed algorithm presented herein
is a composition of the two stages; it stabilizes in $O(n\Delta+{\cal D}^2 + n
\log\log n)$ expected time, and uses $O(n^2\log n + n \log W)$ memory bits
(where $n$ is the order of the graph, $\Delta$ is the maximum degree of the
network, $\cal D$ is the diameter in terms of hops, and $W$ is the largest edge
weight). To our knowledge, our protocol is the very first distributed algorithm
for the (MDST) problem. Moreover, it is fault-tolerant and works for any
anonymous arbitrary network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3304</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3304</id><created>2013-12-11</created><updated>2014-06-17</updated><authors><author><keyname>Pierrot</keyname><forenames>Alexandre J.</forenames></author><author><keyname>Chou</keyname><forenames>R&#xe9;mi A.</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>The Effect of Eavesdropper's Statistics in Experimental Wireless
  Secret-Key Generation</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Forensics and
  Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the role of the eavesdropper's statistics in the
implementation of a practical secret-key generation system. We carefully
conduct the information-theoretic analysis of a secret-key generation system
from wireless channel gains measured with software-defined radios. In
particular, we show that it is inaccurate to assume that the eavesdropper gets
no information because of decorrelation with distance. We also provide a bound
for the achievable secret-key rate in the finite key-length regime that takes
into account the presence of correlated eavesdropper's observations. We
evaluate this bound with our experimental gain measurements to show that
operating with a finite number of samples incurs a loss in secret-key rate on
the order of 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3345</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3345</id><created>2013-12-11</created><authors><author><keyname>Ravi</keyname><forenames>Peruvemba Sundaram</forenames></author><author><keyname>Tuncel</keyname><forenames>Levent</forenames></author><author><keyname>Huang</keyname><forenames>Michael</forenames></author></authors><title>Worst-Case Performance Analysis of Some Approximation Algorithms for
  Minimizing Makespan and Flow-Time</title><categories>cs.DS cs.DM math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1976, Coffman and Sethi conjectured that a natural extension of LPT list
scheduling to the bicriteria scheduling problem of minimizing makespan over
flowtime optimal schedules, called LD algorithm, has a simple worst-case
performance bound: (5m-2)/(4m-1), where m is the number of machines. We study
structure of potential minimal counterexamples to this conjecture and prove
that the conjecture holds for the cases (i) n &gt; 5m, (ii) m = 2, (iii) m = 3,
and (iv) m greater than or equal to 4, n less than or equal to 3m, where n is
the number of jobs. We further conclude that to verify the conjecture, it
suffices to analyze the following case: for every m greater than or equal to 4,
n is either equal to 4m or 5m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3347</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3347</id><created>2013-12-11</created><authors><author><keyname>Naimi</keyname><forenames>M.</forenames></author><author><keyname>Thiare</keyname><forenames>O.</forenames></author></authors><title>A Distributed Deadlock Free Quorum Based Algorithm for Mutual Exclusion</title><categories>cs.DC</categories><comments>7 pages, 9 figures</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 11, No. 8, August 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Quorum based mutual exclusion algorithms enjoy many advantages such as low
message complexity and high failure resiliency. The use of quorums is a well
known approach to achieving mutual exclusion in distributed environments.
Several distributed based quorum mutual exclusion was presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3368</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3368</id><created>2013-12-11</created><authors><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>New Codes on Graphs Constructed by Connecting Spatially Coupled Chains</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel code construction based on spatially coupled low-density parity-check
(SC-LDPC) codes is presented. The proposed code ensembles are described by
protographs, comprised of several protograph-based chains characterizing
individual SC-LDPC codes. We demonstrate that code ensembles obtained by
connecting appropriately chosen SC-LDPC code chains at specific points have
improved iterative decoding thresholds compared to those of single SC-LDPC
coupled chains. In addition, it is shown that the improved decoding properties
of the connected ensembles result in reduced decoding complexity required to
achieve a specific bit error probability. The constructed ensembles are also
asymptotically good, in the sense that the minimum distance grows linearly with
the block length. Finally, we show that the improved asymptotic properties of
the connected chain ensembles also translate into improved finite length
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3372</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3372</id><created>2013-12-11</created><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>On resources and tasks</title><categories>cs.LO</categories><msc-class>03B70</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Essentially being an extended abstract of the author's 1998 PhD thesis, this
paper introduces an extension of the language of linear logic with a semantics
which treats sentences as tasks rather than true/false statements. A resource
is understood as an agent capable of accomplishing the task expressed by such a
sentence. It is argued that the corresponding logic can be used as a planning
logic, whose advantage over the traditional comprehensive planning logics is
that it avoids the representationalframe problem and significantly alleviates
the inferential frame problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3379</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3379</id><created>2013-12-11</created><authors><author><keyname>Hsia</keyname><forenames>Yong</forenames></author><author><keyname>Sheu</keyname><forenames>Ruey-Lin</forenames></author></authors><title>On RIC bounds of Compressed Sensing Matrices for Approximating Sparse
  Solutions Using $\ell_q$ Quasi Norms</title><categories>cs.IT math.IT math.OC</categories><comments>16pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper follows the recent discussion on the sparse solution recovery with
quasi-norms $\ell_q,~q\in(0,1)$ when the sensing matrix possesses a Restricted
Isometry Constant $\delta_{2k}$ (RIC). Our key tool is an improvement on a
version of &quot;the converse of a generalized Cauchy-Schwarz inequality&quot; extended
to the setting of quasi-norm. We show that, if $\delta_{2k}\le 1/2$, any
minimizer of the $l_q$ minimization, at least for those $q\in(0,0.9181]$, is
the sparse solution of the corresponding underdetermined linear system.
Moreover, if $\delta_{2k}\le0.4931$, the sparse solution can be recovered by
any $l_q, q\in(0,1)$ minimization. The values $0.9181$ and $0.4931$ improves
those reported previously in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3386</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3386</id><created>2013-12-11</created><updated>2013-12-25</updated><authors><author><keyname>Terada</keyname><forenames>Yoshikazu</forenames></author></authors><title>Clustering for high-dimension, low-sample size data using distance
  vectors</title><categories>stat.ML cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high-dimension, low-sample size (HDLSS) data, it is not always true that
closeness of two objects reflects a hidden cluster structure. We point out the
important fact that it is not the closeness, but the &quot;values&quot; of distance that
contain information of the cluster structure in high-dimensional space. Based
on this fact, we propose an efficient and simple clustering approach, called
distance vector clustering, for HDLSS data. Under the assumptions given in the
work of Hall et al. (2005), we show the proposed approach provides a true
cluster label under milder conditions when the dimension tends to infinity with
the sample size fixed. The effectiveness of the distance vector clustering
approach is illustrated through a numerical experiment and real data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3387</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3387</id><created>2013-12-11</created><authors><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Neal</keyname><forenames>Zachary P.</forenames></author></authors><title>Navigating the massive world of reddit: Using backbone networks to map
  user interests in social media</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 3 figures, 1 table, 1 supplementary information page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the massive online worlds of social media, users frequently rely on
organizing themselves around specific topics of interest to find and engage
with like-minded people. However, navigating these massive worlds and finding
topics of specific interest often proves difficult because the worlds are
mostly organized haphazardly, leaving users to find relevant interests by word
of mouth or using a basic search feature. Here, we report on a method using the
backbone of a network to create a map of the primary topics of interest in any
social network. To demonstrate the method, we build an interest map for the
social news web site reddit and show how such a map could be used to navigate a
social media world. Moreover, we analyze the network properties of the reddit
social network and find that it has a scale-free, small-world, and modular
community structure, much like other online social networks such as Facebook
and Twitter. We suggest that the integration of interest maps into popular
social media platforms will assist users in organizing themselves into more
specific interest groups, which will help alleviate the overcrowding effect
often observed in large online communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3388</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3388</id><created>2013-12-11</created><authors><author><keyname>Shi</keyname><forenames>Tianlin</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Online Bayesian Passive-Aggressive Learning</title><categories>cs.LG</categories><comments>10 Pages. ICML 2014, Beijing, China</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Online Passive-Aggressive (PA) learning is an effective framework for
performing max-margin online learning. But the deterministic formulation and
estimated single large-margin model could limit its capability in discovering
descriptive structures underlying complex data. This pa- per presents online
Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA
and extends naturally to incorporate latent variables and perform nonparametric
Bayesian inference, thus providing great flexibility for explorative analysis.
We apply BayesPA to topic modeling and derive efficient online learning
algorithms for max-margin topic models. We further develop nonparametric
methods to resolve the number of topics. Experimental results on real datasets
show that our approaches significantly improve time efficiency while
maintaining comparable results with the batch counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3389</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3389</id><created>2013-12-11</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>Matrix Product Codes over Finite Commutative Frobenius Rings</title><categories>cs.IT math.IT</categories><comments>Des. Codes Cryptogr. published online: 19 Jul 2012</comments><doi>10.1007/s10623-012-9726-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Properties of matrix product codes over finite commutative Frobenius rings
are investigated. The minimum distance of matrix product codes constructed with
several types of matrices is bounded in different ways. The duals of matrix
product codes are also explicitly described in terms of matrix product codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3393</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3393</id><created>2013-12-11</created><updated>2013-12-17</updated><authors><author><keyname>Zoghi</keyname><forenames>Masrour</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author><author><keyname>de Rijke</keyname><forenames>Maarten</forenames></author></authors><title>Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem</title><categories>cs.LG</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method for the K-armed dueling bandit problem, a
variation on the regular K-armed bandit problem that offers only relative
feedback about pairs of arms. Our approach extends the Upper Confidence Bound
algorithm to the relative setting by using estimates of the pairwise
probabilities to select a promising arm and applying Upper Confidence Bound
with the winner as a benchmark. We prove a finite-time regret bound of order
O(log t). In addition, our empirical results using real data from an
information retrieval application show that it greatly outperforms the state of
the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3394</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3394</id><created>2013-12-11</created><authors><author><keyname>Zwillinger</keyname><forenames>Daniel</forenames></author></authors><title>Voting Power of Teams Working Together</title><categories>cs.GT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting power determines the &quot;power&quot; of individuals who cast votes; their
power is based on their ability to influence the winning-ness of a coalition.
Usually each individual acts alone, casting either all or none of their votes
and is equally likely to do either. This paper extends this standard &quot;random
voting&quot; model to allow probabilistic voting, partial voting, and correlated
team voting. We extend the standard Banzhaf metric to account for these cases;
our generalization reduces to the standard metric under &quot;random voting&quot;, This
new paradigm allows us to answer questions such as &quot;In the 2013 US Senate, how
much more unified would the Republicans have to be in order to have the same
power as the Democrats in attaining cloture?&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3399</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3399</id><created>2013-12-11</created><authors><author><keyname>Kaynama</keyname><forenames>Shahab</forenames></author><author><keyname>Mitchell</keyname><forenames>Ian M.</forenames></author><author><keyname>Oishi</keyname><forenames>Meeko</forenames></author><author><keyname>Dumont</keyname><forenames>Guy A.</forenames></author></authors><title>Scalable Safety-Preserving Robust Control Synthesis for Continuous-Time
  Linear Systems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable set-valued safety-preserving controller for constrained
continuous-time linear time-invariant (LTI) systems subject to additive,
unknown but bounded disturbance or uncertainty. The approach relies upon a
conservative approximation of the discriminating kernel using robust maximal
reachable sets---an extension of our earlier work on computation of the
viability kernel for high-dimensional systems. Based on ellipsoidal techniques
for reachability, a piecewise ellipsoidal algorithm with polynomial complexity
is described that under-approximates the discriminating kernel under LTI
dynamics. This precomputed piecewise ellipsoidal set is then used online to
synthesize a permissive state-feedback safety-preserving controller. The
controller is modeled as a hybrid automaton and can be formulated such that
under certain conditions the resulting control signal is continuous across its
transitions. We show the performance of the controller on a twelve-dimensional
flight envelope protection problem for a quadrotor with actuation saturation
and unknown wind disturbances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3412</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3412</id><created>2013-12-12</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM, IMJ</affiliation></author></authors><title>The Determinacy of Context-Free Games</title><categories>cs.LO cs.GT math.LO</categories><comments>This paper is an extended version of a STACS 2012 conference paper
  [arXiv:1112.1186]. It will appear in the Journal of Symbolic Logic</comments><proxy>ccsd</proxy><report-no>EXT-JSL-STACS12</report-no><journal-ref>Journal of Symbolic Logic 78, 4 (2013) 1115-1134</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the determinacy of Gale-Stewart games whose winning sets are
accepted by real-time 1-counter B\&quot;uchi automata is equivalent to the
determinacy of (effective) analytic Gale-Stewart games which is known to be a
large cardinal assumption. We show also that the determinacy of Wadge games
between two players in charge of omega-languages accepted by 1-counter B\&quot;uchi
automata is equivalent to the (effective) analytic Wadge determinacy. Using
some results of set theory we prove that one can effectively construct a
1-counter B\&quot;uchi automaton A and a B\&quot;uchi automaton B such that: (1) There
exists a model of ZFC in which Player 2 has a winning strategy in the Wadge
game W(L(A), L(B)); (2) There exists a model of ZFC in which the Wadge game
W(L(A), L(B)) is not determined. Moreover these are the only two possibilities,
i.e. there are no models of ZFC in which Player 1 has a winning strategy in the
Wadge game W(L(A), L(B)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3416</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3416</id><created>2013-12-12</created><authors><author><keyname>Latella</keyname><forenames>Diego</forenames></author><author><keyname>Loreti</keyname><forenames>Michele</forenames></author><author><keyname>Massink</keyname><forenames>Mieke</forenames></author></authors><title>On-the-fly Fast Mean-Field Model-Checking: Extended Version</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel, scalable, on-the-fly model-checking procedure is presented to verify
bounded PCTL properties of selected individuals in the context of very large
systems of independent interacting objects. The proposed procedure combines
on-the-fly model checking techniques with deterministic mean-field
approximation in discrete time. The asymptotic correctness of the procedure is
shown and some results of the application of a prototype implementation of the
FlyFast model-checker are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3417</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3417</id><created>2013-12-12</created><updated>2016-01-19</updated><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Asymptotic MMSE Analysis Under Sparse Representation Modeling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a signal processing technique in which data is acquired
directly in a compressed form. There are two modeling approaches that can be
considered: the worst-case (Hamming) approach and a statistical mechanism, in
which the signals are modeled as random processes rather than as individual
sequences. In this paper, the second approach is studied. In particular, we
consider a model of the form $\boldsymbol{Y} =
\boldsymbol{H}\boldsymbol{X}+\boldsymbol{W}$, where each comportment of
$\boldsymbol{X}$ is given by $X_i = S_iU_i$, where $\left\{U_i\right\}$ are
i.i.d. Gaussian random variables, and $\left\{S_i\right\}$ are binary random
variables independent of $\left\{U_i\right\}$, and not necessarily independent
and identically distributed (i.i.d.), $\boldsymbol{H}\in\mathbb{R}^{k\times n}$
is a random matrix with i.i.d. entries, and $\boldsymbol{W}$ is white Gaussian
noise. Using a direct relationship between optimum estimation and certain
partition functions, and by invoking methods from statistical mechanics and
from random matrix theory (RMT), we derive an asymptotic formula for the
minimum mean-square error (MMSE) of estimating the input vector
$\boldsymbol{X}$ given $\boldsymbol{Y}$ and $\boldsymbol{H}$, as
$k,n\to\infty$, keeping the measurement rate, $R = k/n$, fixed. In contrast to
previous derivations, which are based on the replica method, the analysis
carried out in this paper is rigorous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3418</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3418</id><created>2013-12-12</created><authors><author><keyname>Liu</keyname><forenames>Wenhui</forenames></author><author><keyname>Gong</keyname><forenames>Da</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>One-Bit Compressed Sensing by Greedy Algorithms</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sign truncated matching pursuit (STrMP) algorithm is presented in this paper.
STrMP is a new greedy algorithm for the recovery of sparse signals from the
sign measurement, which combines the principle of consistent reconstruction
with orthogonal matching pursuit (OMP). The main part of STrMP is as concise as
OMP and hence STrMP is simple to implement. In contrast to previous greedy
algorithms for one-bit compressed sensing, STrMP only need to solve a convex
and unconstraint subproblem at each iteration. Numerical experiments show that
STrMP is fast and accurate for one-bit compressed sensing compared with other
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3420</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3420</id><created>2013-12-12</created><authors><author><keyname>Lei</keyname><forenames>Xinyu</forenames></author><author><keyname>Liao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Xiong</keyname><forenames>Yonghong</forenames></author></authors><title>Group Key Agreement Protocol for MANETs Based on HSK Scheme</title><categories>cs.CR cs.NI</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first provide a spanning tree (ST)-based centralized group
key agreement protocol for unbalanced mobile Ad Hoc networks (MANETs). Based on
the centralized solution, a local spanning tree (LST)-based distributed
protocol for general MANETs is subsequently presented. Both protocols follow
the basic features of the HSK scheme: 1) H means that a hybrid approach, which
is the combination of key agreement and key distribution via symmetric
encryption, is exploited; 2) S indicates that a ST or LSTs are adopted to form
a connected network topology; and 3) K implies that the extended Kruskal
algorithm is employed to handle dynamic events. It is shown that the HSK scheme
is a uniform approach to handle the initial key establishment process as well
as all kinds of dynamic events in group key agreement protocol for MANETs.
Additionally, the extended Kruskal algorithm enables to realize the reusability
of the precomputed secure links to reduce the overhead. Moreover, some other
aspects, such as the network topology connectivity and security, are well
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3422</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3422</id><created>2013-12-12</created><updated>2014-03-09</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Manzini</keyname><forenames>Giovanni</forenames></author><author><keyname>Valenzuela</keyname><forenames>Daniel</forenames></author></authors><title>Compressed Spaced Suffix Arrays</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spaced seeds are important tools for similarity search in bioinformatics, and
using several seeds together often significantly improves their performance.
With existing approaches, however, for each seed we keep a separate linear-size
data structure, either a hash table or a spaced suffix array (SSA). In this
paper we show how to compress SSAs relative to normal suffix arrays (SAs) and
still support fast random access to them. We first prove a theoretical upper
bound on the space needed to store an SSA when we already have the SA. We then
present experiments indicating that our approach works even better in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3429</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3429</id><created>2013-12-12</created><updated>2013-12-16</updated><authors><author><keyname>Konda</keyname><forenames>Kishore</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Unsupervised learning of depth and motion</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model for the joint estimation of disparity and motion. The
model is based on learning about the interrelations between images from
multiple cameras, multiple frames in a video, or the combination of both. We
show that learning depth and motion cues, as well as their combinations, from
data is possible within a single type of architecture and a single type of
learning algorithm, by using biologically inspired &quot;complex cell&quot; like units,
which encode correlations between the pixels across image pairs. Our
experimental results show that the learning of depth and motion makes it
possible to achieve state-of-the-art performance in 3-D activity analysis, and
to outperform existing hand-engineered 3-D motion features by a very large
margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3441</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3441</id><created>2013-12-12</created><authors><author><keyname>Jain</keyname><forenames>Prachi</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Call Me MayBe: Understanding Nature and Risks of Sharing Mobile Numbers
  on Online Social Networks</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a great concern about the potential for people to leak private
information on OSNs, but few quantitative studies on this. This research
explores the activity of sharing mobile numbers on OSNs, via public profiles
and posts. We attempt to understand the characteristics and risks of mobile
numbers sharing behaviour on OSNs and focus on Indian mobile numbers. We
collected 76,347 unique mobile numbers posted by 85905 users on Twitter and
Facebook and analysed 2997 numbers, prefixed with +91. We observed, most users
shared their own mobile numbers to spread urgent information; and to market
products and escort business. Fewer female users shared mobile numbers on OSNs.
Users utilized other OSN platforms and third party applications like
Twitterfeed, to post mobile numbers on multiple OSNs. In contrast to the user's
perception of numbers spreading quickly on OSN, we observed that except for
emergency, most numbers did not diffuse deep. To assess risks associated with
mobile numbers exposed on OSNs, we used numbers to gain sensitive information
about their owners (e.g. name, Voter ID) by collating publicly available data
from OSNs, Truecaller, OCEAN. On using the numbers on WhatApp, we obtained a
myriad of sensitive details (relationship status, BBM pins) of the number
owner. We communicated the observed risks to the owners by calling. Few users
were surprised to know about the online presence of their number, while a few
others intentionally posted it online for business purposes. We observed, 38.3%
of users who were unaware of the online presence of their number have posted
their number themselves on the social network. With these observations, we
highlight that there is a need to monitor leakage of mobile numbers via profile
and public posts. To the best of our knowledge, this is the first exploratory
study to critically investigate the exposure of Indian mobile numbers on OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3444</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3444</id><created>2013-12-12</created><authors><author><keyname>Debnath</keyname><forenames>Pradip</forenames></author></authors><title>Some results of domination and total domination in the direct product of
  two fuzzy graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we give a new de?nition of direct product of two arbitrary
fuzzy graphs. We define the concepts of domination and total domination in this
new product graph. We obtain an upper bound for the total domination number of
the product fuzzy graph. Further we define the concept of total
alpha-domination number and derive a lower bound for the total domination
number of the product fuzzy graph in terms of the total alpha-domination number
of the component graphs. A lower bound for the domination number of the same
has also been found.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3455</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3455</id><created>2013-12-12</created><updated>2014-07-31</updated><authors><author><keyname>Liu</keyname><forenames>Shuhao</forenames></author><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Cai</keyname><forenames>Zhiping</forenames></author></authors><title>Low Latency Datacenter Networking: A Short Survey</title><categories>cs.NI cs.DC cs.PF</categories><comments>6 pages</comments><acm-class>C.2.1; C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datacenters are the cornerstone of the big data infrastructure supporting
numerous online services. The demand for interactivity, which significantly
impacts user experience and provider revenue, is translated into stringent
timing requirements for flows in datacenter networks. Thus low latency
networking is becoming a major concern of both industry and academia.
  We provide a short survey of recent progress made by the networking community
for low latency datacenter networks. We propose a taxonomy to categorize
existing work based on four main techniques, reducing queue length,
accelerating retransmissions, prioritizing mice flows, and exploiting
multi-path. Then we review select papers, highlight the principal ideas, and
discuss their pros and cons. We also present our perspectives of the research
challenges and opportunities, hoping to aspire more future work in this space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3478</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3478</id><created>2013-12-12</created><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Ebrahim</forenames></author><author><keyname>Orlin</keyname><forenames>James B.</forenames></author></authors><title>On the power of randomization in network interdiction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interdiction can be viewed as a game between two players, an
&quot;interdictor&quot; and a &quot;flow player&quot;. The flow player wishes to send as much
material as possible through a network, while the interdictor attempts to
minimize the amount of transported material by removing a certain number of
arcs, say $\Gamma$ arcs. We introduce the randomized network interdiction
problem that allows the interdictor to use randomness to select arcs to be
removed. We model the problem in two different ways: arc-based and path-based
formulations, depending on whether flows are defined on arcs or paths,
respectively. We present insights into the modeling power, complexity, and
approximability of both formulations. In particular, we prove that
$Z_{\text{NI}}/Z_{\text{RNI}}\leq \Gamma+1$,
$Z_{\text{NI}}/Z_{\text{RNI}}^{\text{Path}}\leq \Gamma+1$,
$Z_{\text{RNI}}/Z_{\text{RNI}}^{\text{Path}}\leq \Gamma$, where
$Z_{\text{NI}}$, $Z_{\text{RNI}}$, and $Z_{\text{RNI}}^{\text{Path}}$ are the
optimal values of the network interdiction problem and its randomized versions
in arc-based and path-based formulations, respectively. We also show that these
bounds are tight. We show that it is NP-hard to compute the values
$Z_{\text{RNI}}$ and $Z_{\text{RNI}}^{\text{Path}}$ for a general $\Gamma$, but
they are computable in polynomial time when $\Gamma=1$. Further, we provide a
$(\Gamma+1)$-approximation for $Z_{\text{NI}}$, a $\Gamma$-approximation for
$Z_{\text{RNI}}$, and a $\big(1+\lfloor \Gamma/2\rfloor \cdot \lceil
\Gamma/2\rceil/(\Gamma+1)\big)$-approximation for
$Z_{\text{RNI}}^{\text{Path}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3491</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3491</id><created>2013-12-12</created><authors><author><keyname>Aruliah</keyname><forenames>Dhavide</forenames></author><author><keyname>van Veen</keyname><forenames>Lennaert</forenames></author><author><keyname>Dubitski</keyname><forenames>Alex</forenames></author></authors><title>PAMPAC: A Parallel Adaptive Method for Pseudo-Arclength Continuation</title><categories>math.NA cs.DC</categories><comments>21 pages, 7 figures</comments><msc-class>65Y20</msc-class><acm-class>G.1.5; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudo-arclength continuation is a well-established method for generating a
numerical curve approximating the solution of an underdetermined system of
nonlinear equations. It is an inherently sequential predictor-corrector method
in which new approximate solutions are extrapolated from previously converged
results and then iteratively refined. Convergence of the iterative corrections
is guaranteed only for sufficiently small prediction steps. In high-dimensional
systems, corrector steps are extremely costly to compute and the prediction
step-length must be adapted carefully to avoid failed steps or unnecessarily
slow progress. We describe a parallel method for adapting the step-length
employing several predictor-corrector sequences of different step lengths
computed concurrently. In addition, the algorithm permits intermediate results
of unconverged correction sequences to seed new predictions. This strategy
results in an aggressive optimization of the step length at the cost of
redundancy in the concurrent computation. We present two examples of convoluted
solution curves of high-dimensional systems showing that speed-up by a factor
of two can be attained on a multi-core CPU while a factor of three is
attainable on a small cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3496</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3496</id><created>2013-12-10</created><updated>2014-04-18</updated><authors><author><keyname>Medus</keyname><forenames>A. D.</forenames></author><author><keyname>Dorso</keyname><forenames>C. O.</forenames></author></authors><title>Memory effects induce structure in social networks with activity-driven
  agents</title><categories>physics.soc-ph cs.SI</categories><comments>19 pages, 12 figures, Major changes. Re-written work</comments><doi>10.1088/1742-5468/2014/09/P09009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity-driven modeling has been recently proposed as an alternative growth
mechanism for time varying networks, displaying power-law degree distribution
in time-aggregated representation. This approach assumes memoryless agents
developing random connections, thus leading to random networks that fail to
reproduce two-nodes degree correlations and the high clustering coefficient
widely observed in real social networks. In this work we introduce these
missing topological features by accounting for memory effects on the dynamic
evolution of time-aggregated networks. To this end, we propose an
activity-driven network growth model including a triadic-closure step as main
connectivity mechanism. We show that this mechanism provides some of the
fundamental topological features expected for social networks. We derive
analytical results and perform extensive numerical simulations in regimes with
and without population growth. Finally, we present two cases of study, one
comprising face-to-face encounters in a closed gathering, while the other one
from an online social friendship network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3504</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3504</id><created>2013-12-12</created><authors><author><keyname>Smith</keyname><forenames>Warren</forenames></author><author><keyname>Smallen</keyname><forenames>Shava</forenames></author></authors><title>Building An Information System for a Distributed Testbed</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an information system designed to support the large
volume of monitoring information generated by a distributed testbed. This
monitoring information is produced by several subsystems and consists of status
and performance data that needs to be federated, distributed, and stored in a
timely and easy to use manner. Our approach differs from existing approaches
because it federates and distributes information at a low architectural level
via messaging; a natural match to many of the producers and consumers of
information. In addition, a database is easily layered atop the messaging layer
for consumers that want to query and search the information. Finally, a common
language to represent information in all layers of the information system makes
it significantly easier for users to consume information. Performance data
shows that this approach meets the significant needs of FutureGrid and would
meet the needs of an experimental infrastructure twice the size of FutureGrid.
In addition, this design also meets the needs of existing distributed
scientific infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3507</identifier>
 <datestamp>2015-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3507</id><created>2013-12-12</created><updated>2015-01-21</updated><authors><author><keyname>Dokuchaev</keyname><forenames>Nikolai</forenames></author></authors><title>Transmission of a continuous signal via one-bit capacity channel</title><categories>cs.IT math.IT math.OC</categories><msc-class>94A12, 94A40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of the transmission of currently observed time variable
signals via a channel that is capable of sending a single binary signal only
for each measurement of the underlying process. For encoding and decoding, we
suggest a modification othe adaptive delta modulation algorithm. This
modification ensures tracking of time variable signals. We obtained upper
estimates for the error for the case of noiseless transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3522</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3522</id><created>2013-12-12</created><updated>2014-10-12</updated><authors><author><keyname>Lu</keyname><forenames>Weizhi</forenames></author><author><keyname>Li</keyname><forenames>Weiyu</forenames></author><author><keyname>Kpalma</keyname><forenames>Kidiyo</forenames></author><author><keyname>Ronsin</keyname><forenames>Joseph</forenames></author></authors><title>Sparse Matrix-based Random Projection for Classification</title><categories>cs.LG cs.CV stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As a typical dimensionality reduction technique, random projection can be
simply implemented with linear projection, while maintaining the pairwise
distances of high-dimensional data with high probability. Considering this
technique is mainly exploited for the task of classification, this paper is
developed to study the construction of random matrix from the viewpoint of
feature selection, rather than of traditional distance preservation. This
yields a somewhat surprising theoretical result, that is, the sparse random
matrix with exactly one nonzero element per column, can present better feature
selection performance than other more dense matrices, if the projection
dimension is sufficiently large (namely, not much smaller than the number of
feature elements); otherwise, it will perform comparably to others. For random
projection, this theoretical result implies considerable improvement on both
complexity and performance, which is widely confirmed with the classification
experiments on both synthetic data and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3532</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3532</id><created>2013-12-12</created><authors><author><keyname>Rahadi</keyname><forenames>Dedi Rianto</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>The utilization of social networking as promotion media (Case study:
  Handicraft business in Palembang)</title><categories>cs.SI cs.CY</categories><comments>SESINDO2013 conference. Seminar Nasional Sistem Informasi Indonesia
  (SESINDO), Bali, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays social media (Twitter, Facebook, etc.), not only simply as
communication media, but also for promotion. Social networking media offers
many business benefits for companies and organizations. Research purposes is to
determine the model of social network media utilization as a promotional media
for handicraft business in Palembang city. Qualitative and quantitative
research design are used to know how handicraft business in Palembang city
utilizing social media networking as a promotional media. The research results
show 35% craft businesses already utilizing social media as a promotional
media. The social media used are blog development 15%, facebook 46%, and
twitter etc. are 39%. The reasons they use social media such as, 1) minimal
cost, 2) easily recognizable, 3) global distribution areas. Social media
emphasis on direct engagement with customers better. So that the marketing
method could be more personal through direct communication with customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3537</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3537</id><created>2013-12-12</created><updated>2015-06-11</updated><authors><author><keyname>Morton</keyname><forenames>Jason</forenames></author><author><keyname>Turner</keyname><forenames>Jacob</forenames></author></authors><title>Computing the Tutte Polynomial of Lattice Path Matroids Using
  Determinantal Circuits</title><categories>math.CO cs.CC</categories><doi>10.1016/j.tcs.2015.07.042</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a quantum-inspired $O(n^4)$ algorithm computing the Tutte polynomial
of a lattice path matroid, where $n$ is the size of the ground set of the
matroid. Furthermore, this can be improved to $O(n^2)$ arithmetic operations if
we evaluate the Tutte polynomial on a given input, fixing the values of the
variables. The best existing algorithm, found in 2004, was $O(n^5)$, and the
problem has only been known to be polynomial time since 2003. Conceptually, our
algorithm embeds the computation in a determinant using a recently demonstrated
equivalence of categories useful for counting problems such as those that
appear in simulating quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3538</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3538</id><created>2013-12-12</created><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Smooth Orthogonal Drawings of Planar Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In \emph{smooth orthogonal layouts} of planar graphs, every edge is an
alternating sequence of axis-aligned segments and circular arcs with common
axis-aligned tangents. In this paper, we study the problem of finding smooth
orthogonal layouts of low \emph{edge complexity}, that is, with few segments
per edge. We say that a graph has \emph{smooth complexity} k---for short, an
SC_k-layout---if it admits a smooth orthogonal drawing of edge complexity at
most $k$.
  Our main result is that every 4-planar graph has an SC_2-layout. While our
drawings may have super-polynomial area, we show that, for 3-planar graphs,
cubic area suffices. Further, we show that every biconnected 4-outerplane graph
admits an SC_1-layout. On the negative side, we demonstrate an infinite family
of biconnected 4-planar graphs that requires exponential area for an
SC_1-layout. Finally, we present an infinite family of biconnected 4-planar
graphs that does not admit an SC_1-layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3543</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3543</id><created>2013-12-12</created><authors><author><keyname>Wang</keyname><forenames>Zhuwei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Optimal Distributed Control for Networked Control Systems with Delays</title><categories>cs.SY cs.GT</categories><comments>25 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In networked control systems (NCS), sensing and control signals between the
plant and controllers are typically transmitted wirelessly. Thus, the time
delay plays an important role for the stability of NCS, especially with
distributed controllers. In this paper, the optimal control strategy is derived
for distributed control networks with time delays. In particular, we form the
optimal control problem as a non-cooperative linear quadratic game (LQG). Then,
the optimal control strategy of each controller is obtained that is based on
the current state and the last control strategies. The proposed optimal
distributed controller reduces to some known controllers under certain
conditions. Moreover, we illustrate the application of the proposed distributed
controller to load frequency control in power grid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3550</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3550</id><created>2013-12-12</created><authors><author><keyname>Graben</keyname><forenames>Peter beim</forenames></author><author><keyname>Potthast</keyname><forenames>Roland</forenames></author></authors><title>Universal neural field computation</title><categories>cs.FL</categories><comments>21 pages; 6 figures. arXiv admin note: text overlap with
  arXiv:1204.5462</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turing machines and G\&quot;odel numbers are important pillars of the theory of
computation. Thus, any computational architecture needs to show how it could
relate to Turing machines and how stable implementations of Turing computation
are possible. In this chapter, we implement universal Turing computation in a
neural field environment. To this end, we employ the canonical symbologram
representation of a Turing machine obtained from a G\&quot;odel encoding of its
symbolic repertoire and generalized shifts. The resulting nonlinear dynamical
automaton (NDA) is a piecewise affine-linear map acting on the unit square that
is partitioned into rectangular domains. Instead of looking at point dynamics
in phase space, we then consider functional dynamics of probability
distributions functions (p.d.f.s) over phase space. This is generally described
by a Frobenius-Perron integral transformation that can be regarded as a neural
field equation over the unit square as feature space of a dynamic field theory
(DFT). Solving the Frobenius-Perron equation yields that uniform p.d.f.s with
rectangular support are mapped onto uniform p.d.f.s with rectangular support,
again. We call the resulting representation \emph{dynamic field automaton}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3552</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3552</id><created>2013-12-12</created><authors><author><keyname>Acharyya</keyname><forenames>Rupam</forenames></author><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author><author><keyname>Jha</keyname><forenames>Nitesh</forenames></author></authors><title>Counting Popular Matchings in House Allocation Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of counting the number of popular matchings in a given
instance. A popular matching instance consists of agents A and houses H, where
each agent ranks a subset of houses according to their preferences. A matching
is an assignment of agents to houses. A matching M is more popular than
matching M' if the number of agents that prefer M to M' is more than the number
of people that prefer M' to M. A matching M is called popular if there exists
no matching more popular than M. McDermid and Irving gave a poly-time algorithm
for counting the number of popular matchings when the preference lists are
strictly ordered.
  We first consider the case of ties in preference lists. Nasre proved that the
problem of counting the number of popular matching is #P-hard when there are
ties. We give an FPRAS for this problem.
  We then consider the popular matching problem where preference lists are
strictly ordered but each house has a capacity associated with it. We give a
switching graph characterization of popular matchings in this case. Such
characterizations were studied earlier for the case of strictly ordered
preference lists (McDermid and Irving) and for preference lists with ties
(Nasre). We use our characterization to prove that counting popular matchings
in capacitated case is #P-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3582</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3582</id><created>2013-12-12</created><updated>2015-01-07</updated><authors><author><keyname>Jo</keyname><forenames>Jason</forenames></author></authors><title>Iterative Hard Thresholding for Weighted Sparse Approximation</title><categories>cs.IT math.IT math.NA</categories><comments>Fixed a minor typo in the proof of Theorem 3.4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work by Rauhut and Ward developed a notion of weighted sparsity and a
corresponding notion of Restricted Isometry Property for the space of weighted
sparse signals. Using these notions, we pose a best weighted sparse
approximation problem, i.e. we seek structured sparse solutions to
underdetermined systems of linear equations. Many computationally efficient
greedy algorithms have been developed to solve the problem of best $s$-sparse
approximation. The design of all of these algorithms employ a similar template
of exploiting the RIP and computing projections onto the space of sparse
vectors. We present an extension of the Iterative Hard Thresholding (IHT)
algorithm to solve the weighted sparse approximation problem. This IHT
extension employs a weighted analogue of the template employed by all greedy
sparse approximation algorithms. Theoretical guarantees are presented and much
of the original analysis remains unchanged and extends quite naturally.
However, not all the theoretical analysis extends. To this end, we identify and
discuss the barrier to extension. Much like IHT, our IHT extension requires
computing a projection onto a non-convex space. However unlike IHT and other
greedy methods which deal with the classical notion of sparsity, no simple
method is known for computing projections onto these weighted sparse spaces.
Therefore we employ a surrogate for the projection and analyze its empirical
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3590</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3590</id><created>2013-12-12</created><authors><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author><author><keyname>Napp</keyname><forenames>John</forenames></author></authors><title>Quantum computation and real multiplication</title><categories>math-ph cs.IT math.IT math.MP</categories><comments>21 pages, LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a construction of anyon systems associated to quantum tori with
real multiplication and the embedding of quantum tori in AF algebras. These
systems generalize the Fibonacci anyons, with weaker categorical properties,
and are obtained from the basic modules and the real multiplication structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3604</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3604</id><created>2013-12-12</created><authors><author><keyname>May</keyname><forenames>Michael P.</forenames></author></authors><title>A closed-form solution for the flat-state geometry of cylindrical
  surface intersections bounded on all sides by orthogonal planes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A closed-form solution for the boundary of the flat state of an orthogonal
cross section of contiguous surface geometry formed by the intersection of two
cylinders of equal radii oriented in dual directions of rotation about their
intersecting axes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3613</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3613</id><created>2013-12-12</created><updated>2014-06-10</updated><authors><author><keyname>Tristan</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Huang</keyname><forenames>Daniel</forenames></author><author><keyname>Tassarotti</keyname><forenames>Joseph</forenames></author><author><keyname>Pocock</keyname><forenames>Adam</forenames></author><author><keyname>Green</keyname><forenames>Stephen J.</forenames></author><author><keyname>Steele</keyname><forenames>Guy L.</forenames><suffix>Jr</suffix></author></authors><title>Augur: a Modeling Language for Data-Parallel Probabilistic Inference</title><categories>stat.ML cs.AI cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is time-consuming and error-prone to implement inference procedures for
each new probabilistic model. Probabilistic programming addresses this problem
by allowing a user to specify the model and having a compiler automatically
generate an inference procedure for it. For this approach to be practical, it
is important to generate inference code that has reasonable performance. In
this paper, we present a probabilistic programming language and compiler for
Bayesian networks designed to make effective use of data-parallel architectures
such as GPUs. Our language is fully integrated within the Scala programming
language and benefits from tools such as IDE support, type-checking, and code
completion. We show that the compiler can generate data-parallel inference code
scalable to thousands of GPU cores by making use of the conditional
independence relationships in the Bayesian network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3614</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3614</id><created>2013-12-12</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Multiuser Quadrature Allocation for Continuous-Variable Quantum Key
  Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>41 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the adaptive multicarrier quadrature division-multiuser quadrature
allocation (AMQD-MQA) multiple access technique for continuous-variable quantum
key distribution (CVQKD). The MQA scheme is based on the AMQD modulation, which
granulates the inputs of the users into Gaussian subcarrier
continuous-variables (CVs). The subcarrier coherent states formulate Gaussian
sub-channels from the physical link with diverse transmittance coefficients. In
an AMQD-MQA multiple access scenario, the simultaneous reliable transmission of
the users is handled by the dynamic allocation of the Gaussian subcarrier CVs.
We propose two different settings of AMQD-MQA for multiple input-multiple
output communication. We introduce a rate-selection strategy that tunes the
modulation variances and allocates adaptively the quadratures of the users over
the sub-channels. We also prove the rate formulas if only partial channel side
information is available for the users of the sub-channel conditions. In an
experimental CVQKD scenario, an ideal Gaussian input modulation can only be
approximated, which affects the quadrature adaption and the efficiency of the
transmission. We show a technique for the compensation of a nonideal Gaussian
input modulation, which allows the users to overwhelm the modulation
imperfections to reach optimal capacity-achieving communication over the
Gaussian sub-channels. We investigate the diversity amplification of the
sub-channel transmittance coefficients and reveal that a strong diversity can
be exploited by opportunistic Gaussian modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3618</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3618</id><created>2013-12-12</created><authors><author><keyname>Bellamy</keyname><forenames>James</forenames></author></authors><title>Randomness of D Sequences via Diehard Testing</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparison of the quality of randomness of D sequences
based on diehard tests. Since D sequences can model any random sequence, this
comparison is of value beyond this specific class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3631</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3631</id><created>2013-12-12</created><updated>2015-04-07</updated><authors><author><keyname>Sefidgaran</keyname><forenames>Milad</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>Distributed Function Computation Over a Rooted Directed Tree</title><categories>cs.IT math.IT</categories><comments>36 pages, Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes the rate region for a class of source coding function
computation setups where sources of information are available at the nodes of a
tree and where a function of these sources must be computed at the root. The
rate region holds for any function as long as the sources' joint distribution
satisfies a certain Markov criterion. This criterion is met, in particular,
when the sources are independent.
  This result recovers the rate regions of several function computation setups.
These include the point-to-point communication setting with arbitrary sources,
the noiseless multiple access network with &quot;conditionally independent sources,&quot;
and the cascade network with Markovian sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3662</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3662</id><created>2013-12-12</created><authors><author><keyname>Sahin</keyname><forenames>Alphan</forenames></author><author><keyname>Bala</keyname><forenames>Erdem</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Yang</keyname><forenames>Rui</forenames></author><author><keyname>Arslan</keyname><forenames>Huseyin</forenames></author></authors><title>Partially Overlapping Tones for Uncoordinated Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications, 11 pages, 12
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an uncoordinated network, the link performance between the devices might
degrade significantly due to the interference from other links in the network
sharing the same spectrum. As a solution, in this study, the concept of
partially overlapping tones (POT) is introduced. The interference energy
observed at the victim receiver is mitigated by partially overlapping the
individual subcarriers via an intentional carrier frequency offset between the
links. Also, it is shown that while orthogonal transformations at the receiver
cannot mitigate the other-user interference without losing spectral efficiency,
non-orthogonal transformations are able to mitigate the other-user interference
without any spectral efficiency loss at the expense of self-interference. Using
spatial Poisson point process, a tractable bit error rate analysis is provided
to demonstrate potential benefits emerging from POT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3665</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3665</id><created>2013-12-12</created><updated>2014-05-05</updated><authors><author><keyname>Wolinsky</keyname><forenames>David Isaac</forenames></author><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Managing NymBoxes for Identity and Tracking Protection</title><categories>cs.OS cs.CR</categories><comments>16 pages, 7 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the attempts of well-designed anonymous communication tools to
protect users from tracking or identification, flaws in surrounding software
(such as web browsers) and mistakes in configuration may leak the user's
identity. We introduce Nymix, an anonymity-centric operating system
architecture designed &quot;top-to-bottom&quot; to strengthen identity- and
tracking-protection. Nymix's core contribution is OS support for nym-browsing:
independent, parallel, and ephemeral web sessions. Each web session, or
pseudonym, runs in a unique virtual machine (VM) instance evolving from a
common base state with support for long-lived sessions which can be anonymously
stored to the cloud, avoiding de-anonymization despite potential confiscation
or theft. Nymix allows a user to safely browse the Web using various different
transports simultaneously through a pluggable communication model that supports
Tor, Dissent, and a private browsing mode. In evaluations, Nymix consumes 600
MB per nymbox and loads within 15 to 25 seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3679</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3679</id><created>2013-12-12</created><updated>2014-08-09</updated><authors><author><keyname>Balko</keyname><forenames>Martin</forenames></author><author><keyname>Fulek</keyname><forenames>Radoslav</forenames></author><author><keyname>Kyn&#x10d;l</keyname><forenames>Jan</forenames></author></authors><title>Crossing numbers and combinatorial characterization of monotone drawings
  of $K_n$</title><categories>math.CO cs.DM</categories><comments>43 pages, 30 figures; minor changes, three additional references</comments><msc-class>05C10</msc-class><journal-ref>Discrete and Computational Geometry 53 (2015), Issue 1, 107-143</journal-ref><doi>10.1007/s00454-014-9644-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1958, Hill conjectured that the minimum number of crossings in a drawing
of $K_n$ is exactly $Z(n) = \frac{1}{4} \lfloor\frac{n}{2}\rfloor
\left\lfloor\frac{n-1}{2}\right\rfloor
\left\lfloor\frac{n-2}{2}\right\rfloor\left\lfloor\frac{n-3}{2}\right\rfloor$.
Generalizing the result by \'{A}brego et al. for 2-page book drawings, we prove
this conjecture for plane drawings in which edges are represented by
$x$-monotone curves. In fact, our proof shows that the conjecture remains true
for $x$-monotone drawings of $K_n$ in which adjacent edges may cross an even
number of times, and instead of the crossing number we count the pairs of edges
which cross an odd number of times. We further discuss a generalization of this
result to shellable drawings, a notion introduced by \'{A}brego et al. We also
give a combinatorial characterization of several classes of $x$-monotone
drawings of complete graphs using a small set of forbidden configurations. For
a similar local characterization of shellable drawings, we generalize
Carath\'eodory's theorem to simple drawings of complete graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3681</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3681</id><created>2013-12-12</created><updated>2014-03-11</updated><authors><author><keyname>Lichtman</keyname><forenames>Marc</forenames></author><author><keyname>Reed</keyname><forenames>Jeffrey H.</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author><author><keyname>Norton</keyname><forenames>Mark</forenames></author></authors><title>Vulnerability of LTE to Hostile Interference</title><categories>cs.OH</categories><comments>4 pages, see below for citation. M. Lichtman, J. Reed, M. Norton, T.
  Clancy, &quot;Vulnerability of LTE to Hostile Interference'', IEEE Global
  Conference on Signal and Information Processing (GlobalSIP), Dec 2013</comments><doi>10.1109/GlobalSIP.2013.6736871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE is well on its way to becoming the primary cellular standard, due to its
performance and low cost. Over the next decade we will become dependent on LTE,
which is why we must ensure it is secure and available when we need it.
Unfortunately, like any wireless technology, disruption through radio jamming
is possible. This paper investigates the extent to which LTE is vulnerable to
intentional jamming, by analyzing the components of the LTE downlink and uplink
signals. The LTE physical layer consists of several physical channels and
signals, most of which are vital to the operation of the link. By taking into
account the density of these physical channels and signals with respect to the
entire frame, as well as the modulation and coding schemes involved, we come up
with a series of vulnerability metrics in the form of jammer to signal ratios.
The ``weakest links'' of the LTE signals are then identified, and used to
establish the overall vulnerability of LTE to hostile interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3683</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3683</id><created>2013-12-12</created><updated>2014-07-24</updated><authors><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Non-linear growth and condensation in multiplex networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>15 pages, 9 figures</comments><journal-ref>Phys. Rev. E 90, 042807 (2014)</journal-ref><doi>10.1103/PhysRevE.90.042807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different types of interactions coexist and coevolve to shape the structure
and function of a multiplex network. We propose here a general class of growth
models in which the various layers of a multiplex network coevolve through a
set of non-linear preferential attachment rules. We show, both numerically and
analytically, that by tuning the level of non-linearity these models allow to
reproduce either homogeneous or heterogeneous degree distributions, together
with positive or negative degree correlations across layers. In particular, we
derive the condition for the appearance of a condensed state in which one node
in each layer attracts an extensive fraction of all the edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3692</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3692</id><created>2013-12-12</created><authors><author><keyname>Lam</keyname><forenames>Hoai Bao</forenames></author><author><keyname>Phan</keyname><forenames>Tai Tan</forenames></author><author><keyname>Vuong</keyname><forenames>Long Huynh</forenames></author><author><keyname>Huynh</keyname><forenames>Hiep Xuan</forenames></author><author><keyname>Pottier</keyname><forenames>Bernard</forenames></author></authors><title>Designing a brown planthoppers surveillance network based on wireless
  sensor network approach</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach for monitoring brown planthoppers (BPH)
swarms using a surveillance network at provincial scale. The topology of this
network is identified to a wireless sensor network (WSN), where each node is a
real light trap and each edge describes the influence between two nodes,
allowing gathering BPH information. Different communication ranges are
evaluated to choose a suitable network. The experiments are performed on the
light traps surveillance network of Hau Giang province, a typical rice province
in the Mekong Delta region of Vietnam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3693</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3693</id><created>2013-12-12</created><authors><author><keyname>Kim</keyname><forenames>Kwang Deok</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author></authors><title>Policy Network Approach to Coordinated Disaster Response</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>8 pages, 5 figures, and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the formation of network relationships among
disaster relief agencies during the process of responding to an unexpected
event. The relationship is investigated through variables derived from the
policy network theory, and four cases from three developed countries such as
(i) Hurricane Katrina in the US; (ii) Typhoon Maemi in South Korea; (iii) Kobe;
and, (iv) Tohoku Earthquake in Japan that failed to cope with extreme events
forms the basis for case study presented here. We argue that structural
characteristics of multi-jurisdictional coordination may facilitate or impede
in responding to a complex nature of recent disaster. We further highlight the
promise of policy network approach in facilitating the development of
multi-jurisdictional coordination process which may provide new avenue to
improve the communication and coordination of hierarchical command control
driven organizations with the local community. Our proposed novel approach in
investigating the usefulness of network approach through media content analysis
for emergency may provide opportunity as a countermeasure to a traditional
hierarchical coordination, which may give further insights in establishing a
more effective network for emergency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3695</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3695</id><created>2013-12-12</created><authors><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author></authors><title>Secure Beamforming for MIMO Two-Way Communications with an Untrusted
  Relay</title><categories>cs.IT math.IT</categories><comments>10 figures, Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2307276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the secure beamforming design in a multiple-antenna
three-node system where two source nodes exchange messages with the help of an
untrusted relay node. The relay acts as both an essential signal forwarder and
a potential eavesdropper. Both two-phase and three-phase two-way relay
strategies are considered. Our goal is to jointly optimize the source and relay
beamformers for maximizing the secrecy sum rate of the two-way communications.
We first derive the optimal relay beamformer structures. Then, iterative
algorithms are proposed to find source and relay beamformers jointly based on
alternating optimization. Furthermore, we conduct asymptotic analysis on the
maximum secrecy sum-rate. Our analysis shows that when all transmit powers
approach infinity, the two-phase two-way relay scheme achieves the maximum
secrecy sum rate if the source beamformers are designed such that the received
signals at the relay align in the same direction. This reveals an important
advantage of signal alignment technique in against eavesdropping. It is also
shown that if the source powers approach zero the three-phase scheme performs
the best while the two-phase scheme is even worse than direct transmission.
Simulation results have verified the efficiency of the secure beamforming
algorithms as well as the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3700</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3700</id><created>2013-12-12</created><authors><author><keyname>Mayer</keyname><forenames>Robert V</forenames></author></authors><title>The solution of complex problems on calculation of the electrostatic
  fields on lessons on computer modeling</title><categories>cs.OH</categories><comments>in Russian, 10 pages, 5 figures, 3 programs on Pascal. Modern
  scientific researches and innovations 2013 12 December</comments><msc-class>68U20</msc-class><acm-class>I.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In article the following tasks on computer modeling of electric fields are
analyzed: 1) calculation of distribution of potential for the field created by
two parallel plates and charged bodies in the non-uniform environment; 2)
calculation of distribution of potential and force lines of electric field in
which are brought the cylinder, a pipe, a plate, a rectangular parallelepiped
from dielectric, and also the metal cylinder; 3) calculation of distribution of
potential in the one-dimensional nonuniform environment; 4) the solution of the
equation of Poisson in spherical coordinates; 5) calculation of distribution of
potential in cylindrical coordinates with the subsequent creation of
equipotential surfaces and force lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3702</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3702</id><created>2013-12-12</created><updated>2014-08-05</updated><authors><author><keyname>Zeinalpour-Yazdi</keyname><forenames>Zolfa</forenames></author><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author></authors><title>Outage Analysis of Uplink Two-tier Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Employing multi-tier networks is among the most promising approaches to
address the rapid growth of the data demand in cellular networks. In this
paper, we study a two-tier uplink cellular network consisting of femtocells and
a macrocell. Femto base stations, and femto and macro users are assumed to be
spatially deployed based on independent Poisson point processes. We consider an
open access assignment policy, where each macro user based on the ratio between
its distances from its nearest femto access point (FAP) and from the macro base
station (MBS) is assigned to either of them. By tuning the threshold, this
policy allows controlling the coverage areas of FAPs. For a fixed threshold,
femtocells coverage areas depend on their distances from the MBS; Those closest
to the fringes will have the largest coverage areas. Under this open-access
policy, ignoring the additive noise, we derive analytical upper and lower
bounds on the outage probabilities of femto users and macro users that are
subject to fading and path loss. We also study the effect of the distance from
the MBS on the outage probability experienced by the users of a femtocell. In
all cases, our simulation results comply with our analytical bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3711</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3711</id><created>2013-12-13</created><updated>2015-01-12</updated><authors><author><keyname>Bae</keyname><forenames>Sang Won</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Computing the $L_1$ Geodesic Diameter and Center of a Simple Polygon in
  Linear Time</title><categories>cs.CG</categories><comments>10-page Abstract appeared in the proceedings of LATIN 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the $L_1$ geodesic diameter and center of a
simple polygon can be computed in linear time. For the purpose, we focus on
revealing basic geometric properties of the $L_1$ geodesic balls, that is, the
metric balls with respect to the $L_1$ geodesic distance. More specifically, in
this paper we show that any family of $L_1$ geodesic balls in any simple
polygon has Helly number two, and the $L_1$ geodesic center consists of
midpoints of shortest paths between diametral pairs. These properties are
crucial for our linear-time algorithms, and do not hold for the Euclidean case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3717</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3717</id><created>2013-12-13</created><authors><author><keyname>Ben-Or</keyname><forenames>Michael</forenames></author><author><keyname>Eldar</keyname><forenames>Lior</forenames></author></authors><title>Optimal algorithms for linear algebra by quantum inspiration</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results by Harrow et. al. and by Ta-Shma, suggest that quantum
computers may have an exponential advantage in solving a wealth of linear
algebraic problems, over classical algorithms. Building on the quantum
intuition of these results, we step back into the classical domain, and explore
its usefulness in designing classical algorithms. We achieve an algorithm for
solving the major linear-algebraic problems in time $O(n^{\omega+\nu})$ for any
$\nu&gt;0$, where $\omega$ is the optimal matrix-product constant. Thus our
algorithm is optimal w.r.t. matrix multiplication, and comparable to the
state-of-the-art algorithm for these problems due to Demmel et. al. Being
derived from quantum intuition, our proposed algorithm is completely disjoint
from all previous classical algorithms, and builds on a combination of
low-discrepancy sequences and perturbation analysis. As such, we hope it
motivates further exploration of quantum techniques in this respect, hopefully
leading to improvements in our understanding of space complexity and numerical
stability of these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3724</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3724</id><created>2013-12-13</created><authors><author><keyname>Gallo</keyname><forenames>Pierluigi</forenames></author><author><keyname>Tinnirello</keyname><forenames>Ilenia</forenames></author><author><keyname>Giarr&#xe9;</keyname><forenames>Laura</forenames></author><author><keyname>Garlisi</keyname><forenames>Domenico</forenames></author><author><keyname>Croce</keyname><forenames>Daniele</forenames></author><author><keyname>Fagiolini</keyname><forenames>Adriano</forenames></author></authors><title>ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented
  perception</title><categories>cs.CV cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with
Augmented perception. It is a flexible and low cost navigation system for vi-
sually impaired people. Arianna permits to navigate colored paths painted or
sticked on the floor revealing their directions through vibrational feedback on
commercial smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3735</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3735</id><created>2013-12-13</created><updated>2014-05-02</updated><authors><author><keyname>Bunte</keyname><forenames>Christoph</forenames></author><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author></authors><title>Codes for Tasks and R\'enyi Entropy Rate</title><categories>cs.IT math.IT</categories><comments>5 pages, to be presented at ISIT 2014; minor changes in the
  presentation, added a reference</comments><doi>10.1109/ISIT.2014.6874852</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A task is randomly drawn from a finite set of tasks and is described using a
fixed number of bits. All the tasks that share its description must be
performed. Upper and lower bounds on the minimum $\rho$-th moment of the number
of performed tasks are derived. The key is an analog of the Kraft Inequality
for partitions of finite sets. When a sequence of tasks is produced by a source
of a given R\'enyi entropy rate of order $1/(1+\rho)$ and $n$ tasks are jointly
described using $nR$ bits, it is shown that for $R$ larger than the R\'enyi
entropy rate, the $\rho$-th moment of the ratio of performed tasks to $n$ can
be driven to one as $n$ tends to infinity, and that for $R$ less than the
R\'enyi entropy rate it tends to infinity. This generalizes a recent result for
IID sources by the same authors. A mismatched version of the direct part is
also considered, where the code is designed according to the wrong law. The
penalty incurred by the mismatch can be expressed in terms of a divergence
measure that was shown by Sundaresan to play a similar role in the
Massey-Arikan guessing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3738</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3738</id><created>2013-12-13</created><authors><author><keyname>Dhawan</keyname><forenames>Amiraj</forenames></author><author><keyname>Oak</keyname><forenames>Parag</forenames></author><author><keyname>Mishra</keyname><forenames>Rahul</forenames></author><author><keyname>Puthanpurackal</keyname><forenames>George</forenames></author></authors><title>Path Based Mapping Technique for Robots</title><categories>cs.RO</categories><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence, Vol. 2, No.5, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to explore a new way of autonomous mapping.
Current systems using perception techniques like LAZER or SONAR use
probabilistic methods and have a drawback of allowing considerable uncertainty
in the mapping process. Our approach is to break down the environment,
specifically indoor, into reachable areas and objects, separated by boundaries,
and identifying their shape, to render various navigable paths around them.
This is a novel method to do away with uncertainties, as far as possible, at
the cost of temporal efficiency. Also this system demands only minimum and
cheap hardware, as it relies on only Infra-Red sensors to do the job.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3739</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3739</id><created>2013-12-13</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author><author><keyname>Cunningham</keyname><forenames>David</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author><author><keyname>Shinnar</keyname><forenames>Avraham</forenames></author><author><keyname>Tardieu</keyname><forenames>Olivier</forenames></author></authors><title>Semantics of (Resilient) X10</title><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a formal small-step structural operational semantics for a large
fragment of X10, unifying past work. The fragment covers multiple places,
mutable objects on the heap, sequencing, \code{try/catch}, \code{async},
\code{finish}, and \code{at} constructs. This model accurately captures the
behavior of a large class of concurrent, multi-place X10 programs. Further, we
introduce a formal model of resilience in X10. During execution of an X10
program, a place may fail for many reasons. Resilient X10 permits the program
to continue executing, losing the data at the failed place, and most of the
control state, and repairing the global control state in such a way that key
semantic principles hold, the Invariant Happens Before Principle, and the
Failure Masking Principle. These principles permit an X10 programmer to write
clean code that continues to work in the presence of place failure. The given
semantics have additionally been mechanized in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3740</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3740</id><created>2013-12-13</created><updated>2013-12-31</updated><authors><author><keyname>Jain</keyname><forenames>Ashish</forenames></author><author><keyname>Chaudhari</keyname><forenames>Narendra S.</forenames></author></authors><title>Analytical Observations on Knapsack Cipher 0/255</title><categories>cs.CR</categories><comments>article submitted in the reputed journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observed few important facts that concerns with the new proposal of
knapsack cipher 0/255, recently published by Pham [1]. The author claimed that
the time complexity for solving new improved trapdoor knapsack is O(256^N). In
this paper, we show that the knapsack cipher 0/255 can be solved in the same
time that is required for solving the basic knapsack-cipher proposed by Merkle
and Hellman [2]. In other words we claim that the improved version proposed by
Pham [1] is technically same as the basic Merkle and Hellman Knapsack-based
cryptosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3748</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3748</id><created>2013-12-13</created><authors><author><keyname>Zhang</keyname><forenames>Yuanyu</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Shen</keyname><forenames>Yulong</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Wang</keyname><forenames>Hua</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames><affiliation>Senior Member, IEEE</affiliation></author></authors><title>On Eavesdropper-Tolerance Capability of Two-Hop Wireless Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-hop wireless network serves as the basic net-work model for the study of
general wireless networks, while cooperative jamming is a promising scheme to
achieve the physi-cal layer security. This paper establishes a theoretical
framework for the study of eavesdropper-tolerance capability (i.e., the exact
maximum number of eavesdroppers that can be tolerated) in a two-hop wireless
network, where the cooperative jamming is adopted to ensure security defined by
secrecy outage probability (SOP) and opportunistic relaying is adopted to
guarantee relia-bility defined by transmission outage probability (TOP). For
the concerned network, closed form modeling for both SOP and TOP is first
conducted based on the Central Limit Theorem. With the help of SOP and TOP
models and also the Stochastic Ordering Theory, the model for
eavesdropper-tolerance capability analysis is then developed. Finally,
extensive simulation and numerical results are provided to illustrate the
efficiency of our theoretical framework as well as the eavesdropper-tolerance
capability of the concerned network from adopting cooperative jamming and
opportunistic relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3749</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3749</id><created>2013-12-13</created><updated>2014-02-22</updated><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Fibonacci Binning</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note argues that when dot-plotting distributions typically found in
papers about web and social networks (degree distributions, component-size
distributions, etc.), and more generally distributions that have high
variability in their tail, an exponentially binned version should always be
plotted, too, and suggests Fibonacci binning as a visually appealing,
easy-to-use and practical choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3766</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3766</id><created>2013-12-13</created><authors><author><keyname>Basilico</keyname><forenames>Nicola</forenames></author><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Goffredi</keyname><forenames>Luca</forenames></author><author><keyname>Cesana</keyname><forenames>Matteo</forenames></author></authors><title>Beaconing-Aware Optimal Policies for Two-Hop Routing in Multi-Class
  Delay Tolerant Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Delay Tolerant Networks (DTNs), two-hop routing compromises energy versus
delay more conveniently than epidemic routing. Literature provides
comprehensive results on optimal routing policies for mobile nodes with
homogeneous mobility, often neglecting signaling costs. Routing policies are
customarily computed by means of fluid approximation techniques, which assure
solutions to be optimal only when the number of nodes is infinite, while they
provide a coarse approximation otherwise. This work addresses heterogeneous
mobility patterns and multiple wireless transmission technologies; moreover, we
explicitly consider the beaconing/signaling costs to support routing and the
possibility for nodes to discard packets after a local time. We theoretically
characterize the optimal policies by deriving their formal properties. Such
analysis is leveraged to define two algorithmic approaches which allow to trade
off optimality with computational efficiency. Theoretical bounds on the
approximation guarantees of the proposed algorithms are derived. We then
experimentally evaluated them in realistic scenarios of multi-class DTNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3779</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3779</id><created>2013-12-13</created><updated>2014-01-14</updated><authors><author><keyname>Mishra</keyname><forenames>Sounaka</forenames></author><author><keyname>Pananjady</keyname><forenames>Ashwin</forenames></author><author><keyname>Devi</keyname><forenames>N Safina</forenames></author></authors><title>On the Complexity of Making a Distinguished Vertex Minimum or Maximum
  Degree by Vertex Deletion</title><categories>cs.DS cs.DM math.OC</categories><comments>16 pages, 4 figures, submitted to Elsevier's Journal of Discrete
  Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the approximability of two node deletion
problems. Given a vertex weighted graph $G=(V,E)$ and a specified, or
&quot;distinguished&quot; vertex $p \in V$, MDD(min) is the problem of finding a minimum
weight vertex set $S \subseteq V\setminus \{p\}$ such that $p$ becomes the
minimum degree vertex in $G[V \setminus S]$; and MDD(max) is the problem of
finding a minimum weight vertex set $S \subseteq V\setminus \{p\}$ such that
$p$ becomes the maximum degree vertex in $G[V \setminus S]$. These are known
$NP$-complete problems and have been studied from the parameterized complexity
point of view in previous work. Here, we prove that for any $\epsilon &gt; 0$,
both the problems cannot be approximated within a factor $(1 - \epsilon)\log
n$, unless $NP \subseteq DTIME(n^{\log\log n})$. We also show that for any
$\epsilon &gt; 0$, MDD(min) cannot be approximated within a factor $(1
-\epsilon)\log n$ on bipartite graphs, unless $NP \subseteq DTIME(n^{\log\log
n})$, and that for any $\epsilon &gt; 0$, MDD(max) cannot be approximated within a
factor $(1/2 - \epsilon)\log n$ on bipartite graphs, unless $NP \subseteq
DTIME(n^{\log\log n})$. We give an $O(\log n)$ factor approximation algorithm
for MDD(max) on general graphs, provided the degree of $p$ is $O(\log n)$. We
then show that if the degree of $p$ is $n-O(\log n)$, a similar result holds
for MDD(min). We prove that MDD(max) is $APX$-complete on 3-regular unweighted
graphs and provide an approximation algorithm with ratio $1.583$ when $G$ is a
3-regular unweighted graph. In addition, we show that MDD(min) can be solved in
polynomial time when $G$ is a regular graph of constant degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3787</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3787</id><created>2013-12-13</created><updated>2015-02-14</updated><authors><author><keyname>S.</keyname><forenames>Dharini</forenames></author><author><keyname>M.</keyname><forenames>Guru Prasad</forenames></author><author><keyname>V.</keyname><forenames>Hari haran.</forenames></author><author><keyname>L.</keyname><forenames>Kiran Tej J.</forenames></author><author><keyname>Ghosh</keyname><forenames>Kunal</forenames></author></authors><title>Analysis and Understanding of Various Models for Efficient
  Representation and Accurate Recognition of Human Faces</title><categories>cs.CV</categories><comments>Proceedings of National Conference on &quot;Emerging Trends in IT&quot; -
  eit10, March 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we have tried to compare the various face recognition models
against their classical problems. We look at the methods followed by these
approaches and evaluate to what extent they are able to solve the problems. All
methods proposed have some drawbacks under certain conditions. To overcome
these drawbacks we propose a multi-model approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3790</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3790</id><created>2013-12-13</created><updated>2015-04-09</updated><authors><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>INRIA Paris - Rocquencourt, CMAP</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames><affiliation>TUM</affiliation></author><author><keyname>Seibert</keyname><forenames>Matthias</forenames><affiliation>TUM</affiliation></author></authors><title>Sample Complexity of Dictionary Learning and other Matrix Factorizations</title><categories>stat.ML cs.IT math.IT</categories><comments>to appear</comments><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Information Theory, Institute of Electrical
  and Electronics Engineers (IEEE), 2015, pp.18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern tools in machine learning and signal processing, such as sparse
dictionary learning, principal component analysis (PCA), non-negative matrix
factorization (NMF), $K$-means clustering, etc., rely on the factorization of a
matrix obtained by concatenating high-dimensional vectors from a training
collection. While the idealized task would be to optimize the expected quality
of the factors over the underlying distribution of training vectors, it is
achieved in practice by minimizing an empirical average over the considered
collection. The focus of this paper is to provide sample complexity estimates
to uniformly control how much the empirical average deviates from the expected
cost function. Standard arguments imply that the performance of the empirical
predictor also exhibit such guarantees. The level of genericity of the approach
encompasses several possible constraints on the factors (tensor product
structure, shift-invariance, sparsity \ldots), thus providing a unified
perspective on the sample complexity of several widely used matrix
factorization schemes. The derived generalization bounds behave proportional to
$\sqrt{\log(n)/n}$ w.r.t.\ the number of samples $n$ for the considered matrix
factorization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3794</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3794</id><created>2013-12-13</created><authors><author><keyname>Dugu&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>LIFO</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIFO</affiliation></author><author><keyname>Perez</keyname><forenames>Anthony</forenames><affiliation>LIFO</affiliation></author></authors><title>Identification de r\^oles communautaires dans des r\'eseaux orient\'es
  appliqu\'ee \`a Twitter</title><categories>cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1309.2132</comments><proxy>ccsd</proxy><journal-ref>14\`eme conf\'erence Extraction et Gestion des Connaissances,
  Rennes : France (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of community structure is particularly useful when analyzing
complex networks, because it provides an intermediate level, compared to the
more classic global (whole network) and local (node neighborhood) approaches.
The concept of community role of a node was derived from this base, in order to
describe the position of a node in a network depending on its connectivity at
the community level. However, the existing approaches are restricted to
undirected networks, use topological measures which do not consider all aspects
of community-related connectivity, and their role identification methods are
not generalizable to all networks. We tackle these limitations by generalizing
and extending the measures, and using an unsupervised approach to determine the
roles. We then illustrate the applicability of our method by analyzing a
Twitter network.We show how our modifications allow discovering the fact some
particular users called social capitalists occupy very specific roles in this
system.
  ---
  La notion de structure de communaut\'es est particuli\`erement utile pour
\'etudier les r\'eseaux complexes, car elle am\`ene un niveau d'analyse
interm\'ediaire, par opposition aux plus classiques niveaux local (voisinage
des noeuds) et global (r\'eseau entier). Le concept de r\^ole communautaire
permet de d\'ecrire le positionnement d'un noeud en fonction de sa
connectivit\'e communautaire. Cependant, les approches existantes sont
restreintes aux r\'eseaux non-orient\'es, utilisent des mesures topologiques ne
consid\'erant pas tous les aspects de la connectivit\'e communautaire, et des
m\'ethodes d'identification des r\^oles non-g\'en\'eralisables \`a tous les
r\'eseaux. Nous proposons de r\'esoudre ces probl\`emes en g\'en\'eralisant les
mesures existantes, et en utilisant une m\'ethode non-supervis\'ee pour
d\'eterminer les r\^oles. Nous illustrons l'int\'er\^et de notre m\'ethode en
l'appliquant au r\'eseau de Twitter. Nous montrons que nos modifications
mettent en \'evidence les r\^oles sp\'ecifiques d'utilisateurs particuliers du
r\'eseau, nomm\'es capitalistes sociaux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3797</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3797</id><created>2013-12-13</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM, IMJ</affiliation></author></authors><title>Infinite Games Specified by 2-Tape Automata</title><categories>cs.LO cs.GT math.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.3412</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the determinacy of Gale-Stewart games whose winning sets are
infinitary rational relations accepted by 2-tape B\&quot;uchi automata is equivalent
to the determinacy of (effective) analytic Gale-Stewart games which is known to
be a large cardinal assumption. Then we prove that winning strategies, when
they exist, can be very complex, i.e. highly non-effective, in these games. We
prove the same results for Gale-Stewart games with winning sets accepted by
real-time 1-counter B\&quot;uchi automata, then extending previous results obtained
about these games. Then we consider the strenghs of determinacy for these
games, and we prove that there is a transfinite sequence of 2-tape B\&quot;uchi
automata (respectively, of real-time 1-counter B\&quot;uchi automata) $A_\alpha$,
indexed by recursive ordinals, such that the games $G(L(A_\alpha))$ have
strictly increasing strenghs of determinacy. Moreover there is a 2-tape B\&quot;uchi
automaton (respectively, a real-time 1-counter B\&quot;uchi automaton) B such that
the determinacy of G(L(B)) is equivalent to the (effective) analytic
determinacy and thus has the maximal strength of determinacy. We show also that
the determinacy of Wadge games between two players in charge of infinitary
rational relations accepted by 2-tape B\&quot;uchi automata is equivalent to the
(effective) analytic determinacy, and thus not provable in ZFC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3808</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3808</id><created>2013-12-13</created><authors><author><keyname>wilking</keyname><forenames>Benjamin</forenames></author><author><keyname>Meissner</keyname><forenames>Daniel</forenames></author><author><keyname>Reuter</keyname><forenames>Stephan</forenames></author><author><keyname>Dietmayer</keyname><forenames>Klaus</forenames></author></authors><title>Information Maps: A Practical Approach to Position Dependent
  Parameterization</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution a practical approach to determine and store position
dependent parameters is presented. These parameters can be obtained, among
others, using experimental results or expert knowledge and are stored in
'Information Maps'. Each Information Map can be interpreted as a kind of static
grid map and the framework allows to link different maps hierarchically. The
Information Maps can be local or global, with static and dynamic information in
it. One application of Information Maps is the representation of position
dependent characteristics of a sensor. Thus, for instance, it is feasible to
store arbitrary attributes of a sensor's preprocessing in an Information Map
and utilize them by simply taking the map value at the current position. This
procedure is much more efficient than using the attributes of the sensor
itself. Some examples where and how Information Maps can be used are presented
in this publication. The Information Map is meant to be a simple and practical
approach to the problem of position dependent parameterization in all kind of
algorithms when the analytical description is not possible or can not be
implemented efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3811</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3811</id><created>2013-12-13</created><authors><author><keyname>Sehnke</keyname><forenames>Frank</forenames></author></authors><title>Efficient Baseline-free Sampling in Parameter Exploring Policy
  Gradients: Super Symmetric PGPE</title><categories>cs.LG</categories><comments>Artificial Neural Networks and Machine Learning - ICANN 2013 Springer
  Berlin Heidelberg 2013. 130-137</comments><doi>10.1007/978-3-642-40728-4_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy Gradient methods that explore directly in parameter space are among
the most effective and robust direct policy search methods and have drawn a lot
of attention lately. The basic method from this field, Policy Gradients with
Parameter-based Exploration, uses two samples that are symmetric around the
current hypothesis to circumvent misleading reward in \emph{asymmetrical}
reward distributed problems gathered with the usual baseline approach. The
exploration parameters are still updated by a baseline approach - leaving the
exploration prone to asymmetric reward distributions. In this paper we will
show how the exploration parameters can be sampled quasi symmetric despite
having limited instead of free parameters for exploration. We give a
transformation approximation to get quasi symmetric samples with respect to the
exploration without changing the overall sampling distribution. Finally we will
demonstrate that sampling symmetrically also for the exploration parameters is
superior in needs of samples and robustness than the original sampling
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3814</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3814</id><created>2013-12-13</created><updated>2014-04-13</updated><authors><author><keyname>Baxter</keyname><forenames>Gareth J.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>Sergey N.</forenames></author><author><keyname>Mendes</keyname><forenames>Jos&#xe9; F. F.</forenames></author><author><keyname>Cellai</keyname><forenames>Davide</forenames></author></authors><title>Weak percolation on multiplex networks</title><categories>cond-mat.dis-nn cs.CR math.PR physics.soc-ph</categories><comments>14 pages, 12 figures</comments><journal-ref>Phys. Rev. E 89, 042801 (2014)</journal-ref><doi>10.1103/PhysRevE.89.042801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bootstrap percolation is a simple but non-trivial model. It has applications
in many areas of science and has been explored on random networks for several
decades. In single layer (simplex) networks, it has been recently observed that
bootstrap percolation, which is defined as an incremental process, can be seen
as the opposite of pruning percolation, where nodes are removed according to a
connectivity rule. Here we propose models of both bootstrap and pruning
percolation for multiplex networks. We collectively refer to these two models
with the concept of &quot;weak&quot; percolation, to distinguish them from the somewhat
classical concept of ordinary (&quot;strong&quot;) percolation. While the two models
coincide in simplex networks, we show that they decouple when considering
multiplexes, giving rise to a wealth of critical phenomena. Our bootstrap model
constitutes the simplest example of a contagion process on a multiplex network
and has potential applications in critical infrastructure recovery and
information security. Moreover, we show that our pruning percolation model may
provide a way to diagnose missing layers in a multiplex network. Finally, our
analytical approach allows us to calculate critical behavior and characterize
critical clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3822</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3822</id><created>2013-12-13</created><updated>2014-01-09</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Quantum Achievability Proof via Collision Relative Entropy</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a simple framework for deriving one-shot achievable
bounds for some problems in quantum information theory. Our framework is based
on the joint convexity of the exponential of the collision relative entropy,
and is a (partial) quantum generalization of the technique of Yassaee et al.
(2013) from classical information theory. Based on this framework, we derive
one-shot achievable bounds for the problems of communication over
classical-quantum channels, quantum hypothesis testing, and classical data
compression with quantum side information. We argue that our one-shot
achievable bounds are strong enough to give the asymptotic achievable rates of
these problems even up to the second order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3823</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3823</id><created>2013-12-13</created><updated>2014-01-31</updated><authors><author><keyname>Yang</keyname><forenames>Yanbo</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Huang</keyname><forenames>Wentao</forenames></author></authors><title>Network error correction with limited feedback capacity</title><categories>cs.IT math.IT</categories><comments>13 pages, reconstruct papers content</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We consider the problem of characterizing network capacity in the presence of
adversarial errors on network links,focusing in particular on the effect of
low-capacity feedback links cross network cuts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3825</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3825</id><created>2013-12-13</created><authors><author><keyname>Ahlrichs</keyname><forenames>Claas</forenames></author><author><keyname>Lawo</keyname><forenames>Michael</forenames></author></authors><title>Parkinson's Disease Motor Symptoms in Machine Learning: A Review</title><categories>cs.AI</categories><comments>Health Informatics: An International Journal (HIIJ), November 2013,
  Volume 2, Number 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews related work and state-of-the-art publications for
recognizing motor symptoms of Parkinson's Disease (PD). It presents research
efforts that were undertaken to inform on how well traditional machine learning
algorithms can handle this task. In particular, four PD related motor symptoms
are highlighted (i.e. tremor, bradykinesia, freezing of gait and dyskinesia)
and their details summarized. Thus the primary objective of this research is to
provide a literary foundation for development and improvement of algorithms for
detecting PD related motor symptoms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3829</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3829</id><created>2013-12-13</created><updated>2015-02-05</updated><authors><author><keyname>Bubenik</keyname><forenames>Peter</forenames></author><author><keyname>de Silva</keyname><forenames>Vin</forenames></author><author><keyname>Scott</keyname><forenames>Jonathan</forenames></author></authors><title>Metrics for generalized persistence modules</title><categories>math.AT cs.CG</categories><comments>Final version; no changes from previous version. Published online Oct
  2014 in Foundations of Computational Mathematics. Print version to appear</comments><msc-class>55U99, 68U05</msc-class><doi>10.1007/s10208-014-9229-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of defining interleaving metrics on generalized
persistence modules over arbitrary preordered sets. Our constructions are
functorial, which implies a form of stability for these metrics. We describe a
large class of examples, inverse-image persistence modules, which occur
whenever a topological space is mapped to a metric space. Several standard
theories of persistence and their stability can be described in this framework.
This includes the classical case of sublevelset persistent homology. We
introduce a distinction between `soft' and `hard' stability theorems. While our
treatment is direct and elementary, the approach can be explained abstractly in
terms of monoidal functors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3836</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3836</id><created>2013-12-13</created><authors><author><keyname>Brand&#xe3;o</keyname><forenames>Filipe</forenames></author><author><keyname>Pedroso</keyname><forenames>Jo&#xe3;o Pedro</forenames></author></authors><title>Multiple-choice Vector Bin Packing: Arc-flow Formulation with Graph
  Compression</title><categories>math.OC cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1310.6887 by other authors</comments><report-no>DCC-2013-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vector bin packing problem (VBP) is a generalization of bin packing with
multiple constraints. In this problem we are required to pack items,
represented by p-dimensional vectors, into as few bins as possible. The
multiple-choice vector bin packing (MVBP) is a variant of the VBP in which bins
have several types and items have several incarnations. We present an exact
method, based on an arc-flow formulation with graph compression, for solving
MVBP by simply representing all the patterns in a very compact graph. As a
proof of concept we report computational results on a variable-sized bin
packing data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3837</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3837</id><created>2013-12-13</created><updated>2014-09-23</updated><authors><author><keyname>Davydov</keyname><forenames>Alexander A.</forenames></author><author><keyname>Faina</keyname><forenames>Giorgio</forenames></author><author><keyname>Giulietti</keyname><forenames>Massimo</forenames></author><author><keyname>Marcugini</keyname><forenames>Stefano</forenames></author><author><keyname>Pambianco</keyname><forenames>Fernanda</forenames></author></authors><title>Tables of parameters of symmetric configurations $v_{k}$</title><categories>math.CO cs.IT math.IT</categories><comments>38 pages, 7 tables, 61 references; data in tables are updated,
  references are added, connections with new references are discussed; this
  paper develops the topic of arXiv:1203.0709 using new methods giving rise to
  new results</comments><msc-class>05C10 (Primary), 05B25, 94B05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tables of the currently known parameters of symmetric configurations are
given. Formulas for parameters of the known infinite families of symmetric
configurations are presented as well. The results of the recent paper [18] are
used. This work can be viewed as an appendix to [18], in the sense that the
tables given here cover a much larger set of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3838</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3838</id><created>2013-12-12</created><authors><author><keyname>Kuperman</keyname><forenames>Marcelo N.</forenames></author></authors><title>Invited review: Epidemics on social networks</title><categories>nlin.AO cs.SI physics.soc-ph q-bio.PE</categories><comments>17 pages, 13 figures</comments><proxy>Luis Pugnaloni</proxy><journal-ref>Papers in Physics 5, 050003 (2010)</journal-ref><doi>10.4279/PIP.050003</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Since its first formulations almost a century ago, mathematical models for
disease spreading contributed to understand, evaluate and control the epidemic
processes.They promoted a dramatic change in how epidemiologists thought of the
propagation of infectious diseases.In the last decade, when the traditional
epidemiological models seemed to be exhausted, new types of models were
developed.These new models incorporated concepts from graph theory to describe
and model the underlying social structure.Many of these works merely produced a
more detailed extension of the previous results, but some others triggered a
completely new paradigm in the mathematical study of epidemic processes. In
this review, we will introduce the basic concepts of epidemiology, epidemic
modeling and networks, to finally provide a brief description of the most
relevant results in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3847</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3847</id><created>2013-12-13</created><authors><author><keyname>Qi</keyname><forenames>Qi</forenames></author><author><keyname>Cao</keyname><forenames>Yufei</forenames></author></authors><title>Cloud Service-Aware Location Update in Mobile Cloud Computing</title><categories>cs.NI</categories><comments>11 pages, 11 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices are becoming the primary platforms for many users who always
roam around when accessing the cloud computing services. From this, the cloud
computing is integrated into the mobile environment by introducing a new
paradigm, mobile cloud computing. In the context of mobile computing, the
battery life of mobile device is limited, and it is important to balance the
mobility performance and energy consumption. Fortunately, cloud services
provide both opportunities and challenges for mobility management. Taking the
activities of cloud services accessing into consideration, we propose a
service-aware location update mechanism, which can detect the presence and
location of the mobile device without traditional periodic registration update.
Analytic model and simulation are developed to investigate the new mechanism.
The results demonstrate that the service-aware location update management can
reduce the location update times and handoff signaling, which can efficiently
save power consumption for mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3858</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3858</id><created>2013-12-13</created><authors><author><keyname>Pandey</keyname><forenames>Geetika Silakari</forenames></author><author><keyname>Jain</keyname><forenames>R. C.</forenames></author></authors><title>Computational impact of hydrophobicity in protein stability</title><categories>cs.CE</categories><journal-ref>(ijcsis)international journal of computer science and information
  security, vol.11, no. 10, october 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the various features of amino acids, the hydrophobic property has most
visible impact on stability of a sequence folding. This is mentioned in many
protein folding related work, in this paper we more elaborately discuss the
computational impact of the well defined hydrophobic aspect in determining
stability, approach with the help of a developed free energy computing
algorithm covering various aspects preprocessing of an amino acid sequence,
generating the folding and calculating free energy. Later discussing its use in
protein structure related research work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3860</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3860</id><created>2013-12-13</created><updated>2013-12-16</updated><authors><author><keyname>Upadhyay</keyname><forenames>Rahul R</forenames></author></authors><title>Data Hiding and Retrieval Using Permutation Index Method</title><categories>cs.CR</categories><comments>6 pages, 4 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel approach for matrix manipulation and indexing is
proposed .Here the elements in a row of matrix are designated by numeric value
called permutation index followed by the elements of the row being randomised.
This is done for all the rows of the matrix and in the end the set of
permutation indices are put in the parent matrix and random locations depending
on a pre decided scheme called passkey. This passkey is used to put back the
elements of all the rows back in the correct sequence. This approach finds
application in data encapsulation and hiding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3872</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3872</id><created>2013-12-13</created><authors><author><keyname>Bensman</keyname><forenames>Stephen J.</forenames></author></authors><title>Eugene Garfield, Francis Narin, and PageRank: The Theoretical Bases of
  the Google Search Engine</title><categories>cs.IR cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a test of the validity of using Google Scholar to
evaluate the publications of researchers by comparing the premises on which its
search engine, PageRank, is based, to those of Garfield's theory of citation
indexing. It finds that the premises are identical and that PageRank and
Garfield's theory of citation indexing validate each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3876</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3876</id><created>2013-12-13</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>A novel partial order for the information sets of polar codes over
  B-DMCs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study partial orders on the information sets of polar codes designed for
binary discrete memoryless channels. We show that the basic polarization
transformations defined by Ar{\i}kan preserve `symmetric convex/concave
orders'. While for symmetric channels this ordering turns out to be equivalent
to the stochastic degradation ordering already known to order the information
sets of polar codes, we show that a strictly weaker partial order is obtained
when at least one of the channels is asymmetric. We also discuss two tools
which can be useful for verifying `symmetric convex/concave ordering': a
criterion known as the cut criterion and channel symmetrization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3889</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3889</id><created>2013-12-13</created><updated>2015-04-02</updated><authors><author><keyname>Aubry</keyname><forenames>Yves</forenames></author><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Langevin</keyname><forenames>Philippe</forenames></author></authors><title>Cyclotomy of Weil Sums of Binomials</title><categories>math.NT cs.IT math.CO math.IT</categories><comments>18 pages</comments><msc-class>11T23, 11L05, 11T22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Weil sum $W_{K,d}(a)=\sum_{x \in K} \psi(x^d + a x)$ where $K$ is a
finite field, $\psi$ is an additive character of $K$, $d$ is coprime to
$|K^\times|$, and $a \in K^\times$ arises often in number-theoretic
calculations, and in applications to finite geometry, cryptography, digital
sequence design, and coding theory. Researchers are especially interested in
the case where $W_{K,d}(a)$ assumes three distinct values as $a$ runs through
$K^\times$. A Galois-theoretic approach, combined with $p$-divisibility results
on Gauss sums, is used here to prove a variety of new results that constrain
which fields $K$ and exponents $d$ support three-valued Weil sums, and restrict
the values that such Weil sums may assume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3891</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3891</id><created>2013-12-13</created><authors><author><keyname>Stewart</keyname><forenames>Michael</forenames></author></authors><title>Algorithmic Diversity for Software Security</title><categories>cs.PL cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software diversity protects against a modern-day exploits such as code-reuse
attacks. When an attacker designs a code-reuse attack on an example executable,
it relies on replicating the target environment. With software diversity, the
attacker cannot reliably replicate their target. This is a security benefit
which can be applied to massive-scale software distribution. When applied to
large-scale communities, an invested attacker may perform analysis of samples
to improve the chances of a successful attack (M. Franz).
  We present a general NOP-insertion algorithm which can be expanded and
customized for security, performance, or other costs. We demonstrate an
improvement in security so that a code-reuse attack based on any one variant
has minimal chances of success on another and analyse the costs of this method.
Alternately, the variants may be customized to meet performance or memory
overhead constraints. Deterministic diversification allows for the flexibility
to balance these needs in a way that doesn't exist in a random online method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3892</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3892</id><created>2013-12-13</created><authors><author><keyname>Bil&#xf2;</keyname><forenames>Vittorio</forenames></author><author><keyname>Flammini</keyname><forenames>Michele</forenames></author><author><keyname>Monaco</keyname><forenames>Gianpiero</forenames></author></authors><title>Approximating the Revenue Maximization Problem with Sharp Demands</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the revenue maximization problem with sharp multi-demand, in
which $m$ indivisible items have to be sold to $n$ potential buyers. Each buyer
$i$ is interested in getting exactly $d_i$ items, and each item $j$ gives a
benefit $v_{ij}$ to buyer $i$. We distinguish between unrelated and related
valuations. In the former case, the benefit $v_{ij}$ is completely arbitrary,
while, in the latter, each item $j$ has a quality $q_j$, each buyer $i$ has a
value $v_i$ and the benefit $v_{ij}$ is defined as the product $v_i q_j$. The
problem asks to determine a price for each item and an allocation of bundles of
items to buyers with the aim of maximizing the total revenue, that is, the sum
of the prices of all the sold items. The allocation must be envy-free, that is,
each buyer must be happy with her assigned bundle and cannot improve her
utility. We first prove that, for related valuations, the problem cannot be
approximated to a factor $O(m^{1-\epsilon})$, for any $\epsilon&gt;0$, unless {\sf
P} = {\sf NP} and that such result is asymptotically tight. In fact we provide
a simple $m$-approximation algorithm even for unrelated valuations. We then
focus on an interesting subclass of &quot;proper&quot; instances, that do not contain
buyers a priori known not being able to receive any item. For such instances,
we design an interesting $2$-approximation algorithm and show that no
$(2-\epsilon)$-approximation is possible for any $0&lt;\epsilon\leq 1$, unless
{\sf P} $=$ {\sf NP}. We observe that it is possible to efficiently check if an
instance is proper, and if discarding useless buyers is allowed, an instance
can be made proper in polynomial time, without worsening the value of its
optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3903</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3903</id><created>2013-12-13</created><authors><author><keyname>Machado</keyname><forenames>Marlos C.</forenames></author></authors><title>A Methodology for Player Modeling based on Machine Learning</title><categories>cs.AI cs.LG</categories><comments>Thesis presented by Marlos C. Machado as part of the requirements for
  the degree or Master of Science in Computer Science granted by the
  Universidade Federal de Minas Gerais. February, 18th, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI is gradually receiving more attention as a fundamental feature to increase
the immersion in digital games. Among the several AI approaches, player
modeling is becoming an important one. The main idea is to understand and model
the player characteristics and behaviors in order to develop a better AI. In
this work, we discuss several aspects of this new field. We proposed a taxonomy
to organize the area, discussing several facets of this topic, ranging from
implementation decisions up to what a model attempts to describe. We then
classify, in our taxonomy, some of the most important works in this field. We
also presented a generic approach to deal with player modeling using ML, and we
instantiated this approach to model players' preferences in the game
Civilization IV. The instantiation of this approach has several steps. We first
discuss a generic representation, regardless of what is being modeled, and
evaluate it performing experiments with the strategy game Civilization IV.
Continuing the instantiation of the proposed approach we evaluated the
applicability of using game score information to distinguish different
preferences. We presented a characterization of virtual agents in the game,
comparing their behavior with their stated preferences. Once we have
characterized these agents, we were able to observe that different preferences
generate different behaviors, measured by several game indicators. We then
tackled the preference modeling problem as a binary classification task, with a
supervised learning approach. We compared four different methods, based on
different paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a
set of matches played by different virtual agents. We conclude our work using
the learned models to infer human players' preferences. Using some of the
evaluated classifiers we obtained accuracies over 60% for most of the inferred
preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3904</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3904</id><created>2013-12-13</created><updated>2016-03-05</updated><authors><author><keyname>Cheilaris</keyname><forenames>Panagiotis</forenames></author><author><keyname>Khramtcova</keyname><forenames>Elena</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Papadopoulou</keyname><forenames>Evanthia</forenames></author></authors><title>A Randomized Incremental Algorithm for the Hausdorff Voronoi Diagram of
  Non-crossing Clusters</title><categories>cs.CG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.5838</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Hausdorff Voronoi diagram of a family of \emph{clusters of points} in
the plane, the distance between a point $t$ and a cluster $P$ is measured as
the maximum distance between $t$ and any point in $P$, and the diagram is
defined in a nearest-neighbor sense for the input clusters. In this paper we
consider %El.&quot;non-crossing&quot; \emph{non-crossing} clusters in the plane, for
which the combinatorial complexity of the Hausdorff Voronoi diagram is linear
in the total number of points, $n$, on the convex hulls of all clusters. We
present a randomized incremental construction, based on point location, that
computes this diagram in expected $O(n\log^2{n})$ time and expected $O(n)$
space. Our techniques efficiently handle non-standard characteristics of
generalized Voronoi diagrams, such as sites of non-constant complexity, sites
that are not enclosed in their Voronoi regions, and empty Voronoi regions. The
diagram finds direct applications in VLSI computer-aided design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3905</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3905</id><created>2013-12-13</created><updated>2014-02-18</updated><authors><author><keyname>Becker</keyname><forenames>Ruben</forenames></author><author><keyname>Karrenbauer</keyname><forenames>Andreas</forenames></author></authors><title>A Combinatorial $\tilde{O}(m^{3/2})$-time Algorithm for the Min-Cost
  Flow Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a combinatorial method for the min-cost flow problem and prove
that its expected running time is bounded by $\tilde O(m^{3/2})$. This matches
the best known bounds, which previously have only been achieved by numerical
algorithms or for special cases. Our contribution contains three parts that
might be interesting in their own right: (1) We provide a construction of an
equivalent auxiliary network and interior primal and dual points with potential
$P_0=\tilde{O}(\sqrt{m})$ in linear time. (2) We present a combinatorial
potential reduction algorithm that transforms initial solutions of potential
$P_0$ to ones with duality gap below $1$ in $\tilde O(P_0\cdot
\mbox{CEF}(n,m,\epsilon))$ time, where $\epsilon^{-1}=O(m^2)$ and
$\mbox{CEF}(n,m,\epsilon)$ denotes the running time of any combinatorial
algorithm that computes an $\epsilon$-approximate electrical flow. (3) We show
that solutions with duality gap less than $1$ suffice to compute optimal
integral potentials in $O(m+n\log n)$ time with our novel crossover procedure.
All in all, using a variant of a state-of-the-art $\epsilon$-electrical flow
solver, we obtain an algorithm for the min-cost flow problem running in $\tilde
O(m^{3/2})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3910</identifier>
 <datestamp>2013-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3910</id><created>2013-12-13</created><authors><author><keyname>Jancar</keyname><forenames>Petr</forenames></author></authors><title>Bisimulation equivalence of first-order grammars is Ackermann-hard</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bisimulation equivalence (or bisimilarity) of first-order grammars is
decidable, as follows from the decidability result by Senizergues (1998, 2005)
that has been given in an equivalent framework of equational graphs with finite
out-degree, or of pushdown automata (PDA) with only deterministic and popping
epsilon-transitions. Benedikt, Goeller, Kiefer, and Murawski (2013) have shown
that the bisimilarity problem for PDA (even) without epsilon-transitions is
nonelementary. Here we show Ackermann-hardness for bisimilarity of first-order
grammars. The grammars do not use explicit epsilon-transitions, but they
correspond to the above mentioned PDA with (deterministic and popping)
epsilon-transitions, and this feature is substantial in the presented
lower-bound proof. The proof is based on a (polynomial) reduction from the
reachability problem of reset (or lossy) counter machines, for which the
Ackermann-hardness has been shown by Schnoebelen (2010); in fact, this
reachability problem is known to be Ackermann-complete in the hierarchy of
fast-growing complexity classes defined by Schmitz (2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3913</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3913</id><created>2013-12-13</created><updated>2014-06-23</updated><authors><author><keyname>He</keyname><forenames>Xi</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Ding</keyname><forenames>Bolin</forenames></author></authors><title>Blowfish Privacy: Tuning Privacy-Utility Trade-offs using Policies</title><categories>cs.DB</categories><comments>Full version of the paper at SIGMOD'14 Snowbird, Utah USA</comments><doi>10.1145/2588555.2588581</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy definitions provide ways for trading-off the privacy of individuals
in a statistical database for the utility of downstream analysis of the data.
In this paper, we present Blowfish, a class of privacy definitions inspired by
the Pufferfish framework, that provides a rich interface for this trade-off. In
particular, we allow data publishers to extend differential privacy using a
policy, which specifies (a) secrets, or information that must be kept secret,
and (b) constraints that may be known about the data. While the secret
specification allows increased utility by lessening protection for certain
individual properties, the constraint specification provides added protection
against an adversary who knows correlations in the data (arising from
constraints). We formalize policies and present novel algorithms that can
handle general specifications of sensitive information and certain count
constraints. We show that there are reasonable policies under which our privacy
mechanisms for k-means clustering, histograms and range queries introduce
significantly lesser noise than their differentially private counterparts. We
quantify the privacy-utility trade-offs for various policies analytically and
empirically on real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3927</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3927</id><created>2013-12-13</created><updated>2013-12-23</updated><authors><author><keyname>ner</keyname><forenames>Christine Ga&#xdf;</forenames><affiliation>Universit&#xe4;t Greifswald</affiliation></author></authors><title>Strong Turing Degrees for Additive BSS RAM's</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (December
  25, 2013) lmcs:732</journal-ref><doi>10.2168/LMCS-9(4:25)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the additive real BSS machines using only constants 0 and 1 and order
tests we consider the corresponding Turing reducibility and characterize some
semi-decidable decision problems over the reals. In order to refine,
step-by-step, a linear hierarchy of Turing degrees with respect to this model,
we define several halting problems for classes of additive machines with
different abilities and construct further suitable decision problems. In the
construction we use methods of the classical recursion theory as well as
techniques for proving bounds resulting from algebraic properties. In this way
we extend a known hierarchy of problems below the halting problem for the
additive machines using only equality tests and we present a further
subhierarchy of semi-decidable problems between the halting problems for the
additive machines using only equality tests and using order tests,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3938</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3938</id><created>2013-12-13</created><updated>2014-01-30</updated><authors><author><keyname>Cao</keyname><forenames>Jiajun</forenames></author><author><keyname>Kerr</keyname><forenames>Gregory</forenames></author><author><keyname>Arya</keyname><forenames>Kapil</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author></authors><title>Transparent Checkpoint-Restart over InfiniBand</title><categories>cs.OS cs.DC</categories><comments>22 pages, 2 figures, 9 tables</comments><acm-class>D.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  InfiniBand is widely used for low-latency, high-throughput cluster computing.
Saving the state of the InfiniBand network as part of distributed checkpointing
has been a long-standing challenge for researchers. Because of a lack of a
solution, typical MPI implementations have included custom checkpoint-restart
services that &quot;tear down&quot; the network, checkpoint each node as if the node were
a standalone computer, and then re-connect the network again. We present the
first example of transparent, system-initiated checkpoint-restart that directly
supports InfiniBand. The new approach is independent of any particular Linux
kernel, thus simplifying the current practice of using a kernel-based module,
such as BLCR. This direct approach results in checkpoints that are found to be
faster than with the use of a checkpoint-restart service. The generality of
this approach is shown not only by checkpointing an MPI computation, but also a
native UPC computation (Berkeley Unified Parallel C), which does not use MPI.
Scalability is shown by checkpointing 2,048 MPI processes across 128 nodes
(with 16 cores per node). In addition, a cost-effective debugging approach is
also enabled, in which a checkpoint image from an InfiniBand-based production
cluster is copied to a local Ethernet-based cluster, where it can be restarted
and an interactive debugger can be attached to it. This work is based on a
plugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)
checkpoint-restart package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3961</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3961</id><created>2013-12-13</created><updated>2014-12-10</updated><authors><author><keyname>Sengupta</keyname><forenames>Avik</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Fundamental Limits of Caching with Secure Delivery</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>IEEE Transactions on Information Forensics and Security, 02
  December 2014</journal-ref><doi>10.1109/TIFS.2014.2375553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is emerging as a vital tool for alleviating the severe capacity
crunch in modern content-centric wireless networks. The main idea behind
caching is to store parts of popular content in end-users' memory and leverage
the locally stored content to reduce peak data rates. By jointly designing
content placement and delivery mechanisms, recent works have shown order-wise
reduction in transmission rates in contrast to traditional methods. In this
work, we consider the secure caching problem with the additional goal of
minimizing information leakage to an external wiretapper. The fundamental cache
memory vs. transmission rate trade-off for the secure caching problem is
characterized. Rather surprisingly, these results show that security can be
introduced at a negligible cost, particularly for large number of files and
users. It is also shown that the rate achieved by the proposed caching scheme
with secure delivery is within a constant multiplicative factor from the
information-theoretic optimal rate for almost all parameter values of practical
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3968</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3968</id><created>2013-12-13</created><updated>2014-10-19</updated><authors><author><keyname>Borgerding</keyname><forenames>Mark</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Generalized Approximate Message Passing for Cosparse Analysis
  Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cosparse analysis compressive sensing (CS), one seeks to estimate a
non-sparse signal vector from noisy sub-Nyquist linear measurements by
exploiting the knowledge that a given linear transform of the signal is
cosparse, i.e., has sufficiently many zeros. We propose a novel approach to
cosparse analysis CS based on the generalized approximate message passing
(GAMP) algorithm. Unlike other AMP-based approaches to this problem, ours works
with a wide range of analysis operators and regularizers. In addition, we
propose a novel $\ell_0$-like soft-thresholder based on MMSE denoising for a
spike-and-slab distribution with an infinite-variance slab. Numerical
demonstrations on synthetic and practical datasets demonstrate advantages over
existing AMP-based, greedy, and reweighted-$\ell_1$ approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3970</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3970</id><created>2013-12-13</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>An Extensive Evaluation of Filtering Misclassified Instances in
  Supervised Classification Tasks</title><categories>cs.LG stat.ML</categories><comments>29 pages, 3 Figures, 20 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Removing or filtering outliers and mislabeled instances prior to training a
learning algorithm has been shown to increase classification accuracy. A
popular approach for handling outliers and mislabeled instances is to remove
any instance that is misclassified by a learning algorithm. However, an
examination of which learning algorithms to use for filtering as well as their
effects on multiple learning algorithms over a large set of data sets has not
been done. Previous work has generally been limited due to the large
computational requirements to run such an experiment, and, thus, the
examination has generally been limited to learning algorithms that are
computationally inexpensive and using a small number of data sets. In this
paper, we examine 9 learning algorithms as filtering algorithms as well as
examining the effects of filtering in the 9 chosen learning algorithms on a set
of 54 data sets. In addition to using each learning algorithm individually as a
filter, we also use the set of learning algorithms as an ensemble filter and
use an adaptive algorithm that selects a subset of the learning algorithms for
filtering for a specific task and learning algorithm. We find that for most
cases, using an ensemble of learning algorithms for filtering produces the
greatest increase in classification accuracy. We also compare filtering with a
majority voting ensemble. The voting ensemble significantly outperforms
filtering unless there are high amounts of noise present in the data set.
Additionally, we find that a majority voting ensemble is robust to noise as
filtering with a voting ensemble does not increase the classification accuracy
of the voting ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3971</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3971</id><created>2013-12-13</created><authors><author><keyname>Urli</keyname><forenames>Tommaso</forenames></author></authors><title>Balancing bike sharing systems (BBSS): instance generation from the
  CitiBike NYC data</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bike sharing systems are a very popular means to provide bikes to citizens in
a simple and cheap way. The idea is to install bike stations at various points
in the city, from which a registered user can easily loan a bike by removing it
from a specialized rack. After the ride, the user may return the bike at any
station (if there is a free rack). Services of this kind are mainly public or
semi-public, often aimed at increasing the attractiveness of non-motorized
means of transportation, and are usually free, or almost free, of charge for
the users. Depending on their location, bike stations have specific patterns
regarding when they are empty or full. For instance, in cities where most jobs
are located near the city centre, the commuters cause certain peaks in the
morning: the central bike stations are filled, while the stations in the
outskirts are emptied. Furthermore, stations located on top of a hill are more
likely to be empty, since users are less keen on cycling uphill to return the
bike, and often leave their bike at a more reachable station. These issues
result in substantial user dissatisfaction which may eventually cause the users
to abandon the service. This is why nowadays most bike sharing system providers
take measures to rebalance them. Over the last few years, balancing bike
sharing systems (BBSS) has become increasingly studied in optimization. As
such, generating meaningful instance to serve as a benchmark for the proposed
approaches is an important task. In this technical report we describe the
procedure we used to generate BBSS problem instances from data of the CitiBike
NYC bike sharing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3979</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3979</id><created>2013-12-13</created><updated>2014-03-27</updated><authors><author><keyname>Jansen</keyname><forenames>Nils</forenames></author><author><keyname>Corzilius</keyname><forenames>Florian</forenames></author><author><keyname>Volk</keyname><forenames>Matthias</forenames></author><author><keyname>Wimmer</keyname><forenames>Ralf</forenames></author><author><keyname>&#xc1;brah&#xe1;m</keyname><forenames>Erika</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author><author><keyname>Becker</keyname><forenames>Bernd</forenames></author></authors><title>Accelerating Parametric Probabilistic Verification</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for computing reachability probabilities of
parametric discrete-time Markov chains whose transition probabilities are
fractions of polynomials over a set of parameters. Our algorithm is based on
two key ingredients: a graph decomposition into strongly connected subgraphs
combined with a novel factorization strategy for polynomials. Experimental
evaluations show that these approaches can lead to a speed-up of up to several
orders of magnitude in comparison to existing approaches
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3981</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3981</id><created>2013-12-13</created><authors><author><keyname>Bose</keyname><forenames>Sandip</forenames></author><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Valero</keyname><forenames>Henri-Pierre</forenames></author></authors><title>Joint multi-mode dispersion extraction in Fourier and space time domains</title><categories>physics.geo-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel broadband approach for the extraction of
dispersion curves of multiple time frequency overlapped dispersive modes such
as in borehole acoustic data. The new approach works jointly in the Fourier and
space time domains and, in contrast to existing space time approaches that
mainly work for time frequency separated signals, efficiently handles multiple
signals with significant time frequency overlap. The proposed method begins by
exploiting the slowness (phase and group) and time location estimates based on
frequency-wavenumber (f-k) domain sparsity penalized broadband dispersion
extraction method as presented in \cite{AeronTSP2011}. In this context we first
present a Cramer Rao Bound (CRB) analysis for slowness estimation in the (f-k)
domain and show that for the f-k domain broadband processing, group slowness
estimates have more variance than the phase slowness estimates and time
location estimates. In order to improve the group slowness estimates we exploit
the time compactness property of the modes to effectively represent the data as
a linear superposition of time compact space time propagators parameterized by
the phase and group slowness. A linear least squares estimation algorithm in
the space time domain is then used to obtain improved group slowness estimates.
The performance of the method is demonstrated on real borehole acoustic data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3986</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3986</id><created>2013-12-13</created><updated>2014-01-16</updated><authors><author><keyname>Wasserman</keyname><forenames>Max</forenames></author><author><keyname>Mukherjee</keyname><forenames>Satyam</forenames></author><author><keyname>Scott</keyname><forenames>Konner</forenames></author><author><keyname>Zeng</keyname><forenames>Xiao Han T.</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Amaral</keyname><forenames>Lu&#xed;s A. N.</forenames></author></authors><title>Correlations between user voting data, budget, and box office for films
  in the Internet Movie Database</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 8 figures, 3 tables, accepted for publication to JASIST</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet Movie Database (IMDb) is one of the most-visited websites in the
world and the premier source for information on films. Like Wikipedia, much of
IMDb's information is user contributed. IMDb also allows users to voice their
opinion on the quality of films through voting. We investigate whether there is
a connection between this user voting data and certain economic film
characteristics. To this end, we perform distribution and correlation analysis
on a set of films chosen to mitigate effects of bias due to the language and
country of origin of films. We show that production budget, box office gross,
and total number of user votes for films are consistent with double-log normal
distributions for certain time periods. Both total gross and user votes are
consistent with a double-log normal distribution from the late 1980s onward,
while for budget, it extends from 1935 to 1979. In addition, we find a strong
correlation between number of user votes and the economic statistics,
particularly budget. Remarkably, we find no evidence for a correlation between
number of votes and average user rating. As previous studies have found a
strong correlation between production budget and marketing expenses, our
results suggest that total user votes is an indicator of a film's prominence or
notability, which can be quantified by its promotional costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3989</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3989</id><created>2013-12-13</created><authors><author><keyname>Hatami</keyname><forenames>Nima</forenames></author><author><keyname>Chira</keyname><forenames>Camelia</forenames></author></authors><title>Classifiers With a Reject Option for Early Time-Series Classification</title><categories>cs.CV cs.LG</categories><journal-ref>Computational Intelligence and Ensemble Learning (CIEL), IEEE
  Symposium on, 9-16, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early classification of time-series data in a dynamic environment is a
challenging problem of great importance in signal processing. This paper
proposes a classifier architecture with a reject option capable of online
decision making without the need to wait for the entire time series signal to
be present. The main idea is to classify an odor/gas signal with an acceptable
accuracy as early as possible. Instead of using posterior probability of a
classifier, the proposed method uses the &quot;agreement&quot; of an ensemble to decide
whether to accept or reject the candidate label. The introduced algorithm is
applied to the bio-chemistry problem of odor classification to build a novel
Electronic-Nose called Forefront-Nose. Experimental results on wind tunnel
test-bed facility confirms the robustness of the forefront-nose compared to the
standard classifiers from both earliness and recognition perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.3990</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.3990</id><created>2013-12-13</created><authors><author><keyname>Hatami</keyname><forenames>Nima</forenames></author><author><keyname>Ebrahimpour</keyname><forenames>Reza</forenames></author><author><keyname>Ghaderi</keyname><forenames>Reza</forenames></author></authors><title>ECOC-Based Training of Neural Networks for Face Recognition</title><categories>cs.CV cs.LG</categories><journal-ref>Cybernetics and Intelligent Systems, IEEE Conference on, 450-454,
  2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error Correcting Output Codes, ECOC, is an output representation method
capable of discovering some of the errors produced in classification tasks.
This paper describes the application of ECOC to the training of feed forward
neural networks, FFNN, for improving the overall accuracy of classification
systems. Indeed, to improve the generalization of FFNN classifiers, this paper
proposes an ECOC-Based training method for Neural Networks that use ECOC as the
output representation, and adopts the traditional Back-Propagation algorithm,
BP, to adjust weights of the network. Experimental results for face recognition
problem on Yale database demonstrate the effectiveness of our method. With a
rejection scheme defined by a simple robustness rate, high reliability is
achieved in this application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4003</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4003</id><created>2013-12-13</created><updated>2014-11-14</updated><authors><author><keyname>Wang</keyname><forenames>Ping-Chung</forenames></author><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Asynchronous Physical-Layer Network Coding with Quasi-Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>21 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication in the presence of bounded timing asynchronism which is known
to the receiver but cannot be easily compensated is studied. Examples of such
situations include point-to-point communication over inter-symbol interference
(ISI) channels and asynchronous wireless networks. In these scenarios, although
the receiver may know all the delays, it is often not be an easy task for the
receiver to compensate the delays as the signals are mixed together. A novel
framework called interleave/deinterleave transform (IDT) is proposed to deal
with this problem. It is shown that the IDT allows one to design the delays so
that quasi-cyclic (QC) codes with a proper shifting constraint can be used
accordingly. When used in conjunction with QC codes, IDT provides significantly
better performance than existing schemes relying solely on cyclic codes. Two
instances of asynchronous physical-layer network coding, namely the
integer-forcing equalization for ISI channels and asynchronous
compute-and-forward, are then studied. For integer-forcing equalization, the
proposed scheme provides improved performance over using cyclic codes. For
asynchronous compute-and-forward, the proposed scheme shows that there is no
loss in the achievable information due to delays which are integer multiples of
the symbol duration. Further, the proposed approach shows that delays
introduced by the channel can sometimes be exploited to obtain higher
information rates than those obtainable in the synchronous case. The proposed
IDT can be thought of as a generalization of the interleaving/deinterleaving
idea proposed by Wang et al. which allows the use of QC codes thereby
substantially increasing the design space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4012</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4012</id><created>2013-12-14</created><authors><author><keyname>Arasu</keyname><forenames>Arvind</forenames></author><author><keyname>Kaushik</keyname><forenames>Raghav</forenames></author></authors><title>Oblivious Query Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by cloud security concerns, there is an increasing interest in
database systems that can store and support queries over encrypted data. A
common architecture for such systems is to use a trusted component such as a
cryptographic co-processor for query processing that is used to securely
decrypt data and perform computations in plaintext. The trusted component has
limited memory, so most of the (input and intermediate) data is kept encrypted
in an untrusted storage and moved to the trusted component on ``demand.''
  In this setting, even with strong encryption, the data access pattern from
untrusted storage has the potential to reveal sensitive information; indeed,
all existing systems that use a trusted component for query processing over
encrypted data have this vulnerability. In this paper, we undertake the first
formal study of secure query processing, where an adversary having full
knowledge of the query (text) and observing the query execution learns nothing
about the underlying database other than the result size of the query on the
database. We introduce a simpler notion, oblivious query processing, and show
formally that a query admits secure query processing iff it admits oblivious
query processing. We present oblivious query processing algorithms for a rich
class of database queries involving selections, joins, grouping and
aggregation. For queries not handled by our algorithms, we provide some initial
evidence that designing oblivious (and therefore secure) algorithms would be
hard via reductions from two simple, well-studied problems that are generally
believed to be hard. Our study of oblivious query processing also reveals
interesting connections to database join theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4014</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4014</id><created>2013-12-14</created><authors><author><keyname>Tzitzikas</keyname><forenames>Yannis</forenames></author></authors><title>A Simple Method to Produce Algorithmic MIDI Music based on Randomness,
  Simple Probabilities and Multi-Threading</title><categories>cs.SD</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a simple method for producing multichannel MIDI music
that is based on randomness and simple probabilities. One distinctive feature
of the method is that it produces and sends in parallel to the sound card more
than one unsynchronized channels by exploiting the multi-threading capabilities
of general purpose programming languages. As consequence the derived sound
offers a quite ``full&quot; and ``unpredictable&quot; acoustic experience to the
listener. Subsequently the paper reports the results of an evaluation with
users. The results were very surprising: the majority of users responded that
they could tolerate this music in various occasions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4016</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4016</id><created>2013-12-14</created><authors><author><keyname>Geng</keyname><forenames>Xiongfeng</forenames></author><author><keyname>Wang</keyname><forenames>Yongcai</forenames></author><author><keyname>Feng</keyname><forenames>Haoran</forenames></author><author><keyname>Chen</keyname><forenames>Zhoufeng</forenames></author></authors><title>Hybrid Radio-map for Noise Tolerant Wireless Indoor Localization</title><categories>cs.NI</categories><comments>6 pages, 11th IEEE International Conference on Networking, Sensing
  and Control, April 7-9, 2014, Miami, FL, USA</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In wireless networks, radio-map based locating techniques are commonly used
to cope the complex fading feature of radio signal, in which a radio-map is
built by calibrating received signal strength (RSS) signatures at training
locations in the offline phase. However, in severe hostile environments, such
as in ship cabins where severe shadowing, blocking and multi-path fading
effects are posed by ubiquitous metallic architecture, even radio-map cannot
capture the dynamics of RSS. In this paper, we introduced multiple feature
radio-map location method for severely noisy environments. We proposed to add
low variance signature into radio map. Since the low variance signatures are
generally expensive to obtain, we focus on the scenario when the low variance
signatures are sparse. We studied efficient construction of multi-feature
radio-map in offline phase, and proposed feasible region narrowing down and
particle based algorithm for online tracking. Simulation results show the
remarkably performance improvement in terms of positioning accuracy and
robustness against RSS noises than the traditional radio-map method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4026</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4026</id><created>2013-12-14</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Achieving Fully Proportional Representation: Approximability Results</title><categories>cs.AI cs.GT cs.MA</categories><comments>arXiv admin note: substantial text overlap with arXiv:1208.1661,
  arXiv:1301.6400</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of (approximate) winner determination under the
Monroe and Chamberlin--Courant multiwinner voting rules, which determine the
set of representatives by optimizing the total (dis)satisfaction of the voters
with their representatives. The total (dis)satisfaction is calculated either as
the sum of individual (dis)satisfactions (the utilitarian case) or as the
(dis)satisfaction of the worst off voter (the egalitarian case). We provide
good approximation algorithms for the satisfaction-based utilitarian versions
of the Monroe and Chamberlin--Courant rules, and inapproximability results for
the dissatisfaction-based utilitarian versions of them and also for all
egalitarian cases. Our algorithms are applicable and particularly appealing
when voters submit truncated ballots. We provide experimental evaluation of the
algorithms both on real-life preference-aggregation data and on synthetic data.
These experiments show that our simple and fast algorithms can in many cases
find near-perfect solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4036</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4036</id><created>2013-12-14</created><authors><author><keyname>Narang</keyname><forenames>Apoorv</forenames></author><author><keyname>Bedathur</keyname><forenames>Srikanta</forenames></author></authors><title>Mind Your Language: Effects of Spoken Query Formulation on Retrieval
  Effectiveness</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice search is becoming a popular mode for interacting with search engines.
As a result, research has gone into building better voice transcription
engines, interfaces, and search engines that better handle inherent verbosity
of queries. However, when one considers its use by non- native speakers of
English, another aspect that becomes important is the formulation of the query
by users. In this paper, we present the results of a preliminary study that we
conducted with non-native English speakers who formulate queries for given
retrieval tasks. Our results show that the current search engines are sensitive
in their rankings to the query formulation, and thus highlights the need for
developing more robust ranking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4042</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4042</id><created>2013-12-14</created><authors><author><keyname>Mishra</keyname><forenames>Mina</forenames></author><author><keyname>Mankar</keyname><forenames>Vijay H.</forenames></author></authors><title>Chaotic Encryption Scheme Using 1-D Chaotic Map</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes three different chaotic encryption methods using 1-D
chaotic map known as Logistic map named as Logistic, NLFSR and Modified NLFSR
according to the name of chaotic map and non-linear function involved in the
scheme. The designed schemes have been crypt analyzed for five different
methods for testing its strength. Cryptanalysis has been performed for various
texts using various keys selected from domain of key space. Logistic and NLFSR
methods are found to resist known plaintext attack for available first two
characters of plaintext. Plaintext sensitivity of both methods is within small
range along with medium key sensitivity. Identifiability for keys of first two
of the scheme has not been derived concluding that methods may prove to be weak
against brute-force attack. In the last modified scheme avalanche effect found
to be improved compared to the previous ones and method is found to resist
brute-force attack as it derives the conclusion for identifiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4043</identifier>
 <datestamp>2014-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4043</id><created>2013-12-14</created><updated>2014-01-09</updated><authors><author><keyname>S&#xe1;nchez</keyname><forenames>Alejandro</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>C&#xe9;sar</forenames></author></authors><title>Parametrized Invariance for Infinite State Processes</title><categories>cs.LO</categories><comments>15 pages plus appendix. Typos fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the uniform verification problem for infinite state processes, which
consists of proving that the parallel composition of an arbitrary number of
processes satisfies a temporal property. Our practical motivation is to build a
general framework for the temporal verification of concurrent datatypes. The
contribution of this paper is a general method for the verification of safety
properties of parametrized programs that manipulate complex local and global
data, including mutable state in the heap. This method is based on the
separation between two concerns: (1) the interaction between executing
threads---handled by novel parametrized invariance rules---,and the data being
manipulated---handled by specialized decision procedures. The proof rules
discharge automatically a finite collection of verification conditions (VCs),
the number depending only on the size of the program description and the
specification, but not on the number of processes in any given instance or on
the kind of data manipulated. Moreover, all VCs are quantifier free, which
eases the development of decision procedures for complex data-types on top of
off-the-shelf SMT solvers. We discuss the practical verification (of shape and
also functional correctness properties) of a concurrent list implementation
based on the method presented in this paper. Our tool also all VCs using a
decision procedure for a theory of list layouts in the heap built on top of
state-of-the-art SMT solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4044</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4044</id><created>2013-12-14</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author></authors><title>CACO : Competitive Ant Colony Optimization, A Nature-Inspired
  Metaheuristic For Large-Scale Global Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale problems are nonlinear problems that need metaheuristics, or
global optimization algorithms. This paper reviews nature-inspired
metaheuristics, then it introduces a framework named Competitive Ant Colony
Optimization inspired by the chemical communications among insects. Then a case
study is presented to investigate the proposed framework for large-scale global
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4048</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4048</id><created>2013-12-14</created><authors><author><keyname>Bui</keyname><forenames>Lam Thu</forenames></author><author><keyname>Mac</keyname><forenames>Van Vien</forenames></author></authors><title>Toward an agent based distillation approach for protesting crowd
  simulation</title><categories>cs.MA</categories><comments>3 pages, ISCRAM-VN 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of protesting crowd simulation. It
considers CROCADILE, an agent based distillation system, for this purpose. A
model of protesting crowd was determined and then a CROCADILE model of
protesting crowd was engineered and demonstrated. We validated the model by
using two scenarios where protesters are varied with different personalities.
The results indicated that CROCADILE served well as the platform for protesting
crowd modeling simulation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4066</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4066</id><created>2013-12-14</created><authors><author><keyname>Perrot</keyname><forenames>K&#xe9;vin</forenames></author><author><keyname>R&#xe9;mila</keyname><forenames>Eric</forenames></author></authors><title>Emergence of wave patterns on Kadanoff Sandpiles</title><categories>cs.DM</categories><comments>21 pages (including 9 pages of annexes). LATIN 2014. arXiv admin
  note: substantial text overlap with arXiv:1301.0997</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergence is a concept that is easy to exhibit, but very hard to formally
handle. This paper is about cubic sand grains moving around on nicely packed
columns in one dimension (the physical sandpile is two dimensional, but the
support of sand columns is one dimensional). The Kadanoff Sandpile Model is a
discrete dynamical system describing the evolution of a finite number of
stacked grains --as they would fall from an hourglass-- to a stable
configuration (fixed point). Grains move according to the repeated application
of a simple local rule until reaching a fixed point. The main interest of the
model relies in the difficulty of understanding its behavior, despite the
simplicity of the rule. In this paper we prove the emergence of wave patterns
periodically repeated on fixed points. Remarkably, those regular patterns do
not cover the entire fixed point, but eventually emerge from a seemingly highly
disordered segment. The proof technique we set up associates arguments of
linear algebra and combinatorics, which interestingly allow to formally state
the emergence of regular patterns without requiring a precise understanding of
the chaotic initial segment's dynamic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4071</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4071</id><created>2013-12-14</created><authors><author><keyname>Ganguly</keyname><forenames>Srinjoy</forenames></author><author><keyname>Chakraborty</keyname><forenames>Arpita</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal Kanti</forenames></author></authors><title>A Trust-based Framework for Congestion-aware Energy Efficient Routing in
  Wireless Multimedia Sensor Networks</title><categories>cs.NI</categories><comments>5 pages, 3 figures and 0 tables. Poster Paper at the Student Research
  Symposium of the International Conference on High Performance Computing
  (HiPC), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new era in wireless sensor network technology has been ushered in through
the introduction of multimedia sensor networks, which has a major bottleneck in
the form of network congestion. Congestion occurs when resources are in high
demand during the active period while the data processing and transmission
speeds lag behind the speed of the incoming traffic. This may disrupt normal
network operations by buffer overflow, packet loss, increased latency,
excessive energy consumption and even worse, a collapse of the entire
operation. In this paper we propose a novel Trust Integrated Congestion-aware
Energy Efficient Routing algorithm (TCEER) in which the potential of a node is
computed using its trust value, congestion status, residual energy, distance
from the current packet-forwarding node and the distance from the base station
using a Fuzzy Logic Controller. The source node selects the node of highest
potential in its one hop radio range for data transmission. Hop by hop data
routing from source to base station is obtained which is light-weight as well
as energy-efficient. Finally, the merits of the proposed scheme are discussed
by comparing it with the existing protocols and the study shows promising
improvements in network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4074</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4074</id><created>2013-12-14</created><authors><author><keyname>Ganguly</keyname><forenames>Srinjoy</forenames></author><author><keyname>Bose</keyname><forenames>Digbalay</forenames></author><author><keyname>Konar</keyname><forenames>Amit</forenames></author></authors><title>Clustering using Vector Membership: An Extension of the Fuzzy C-Means
  Algorithm</title><categories>cs.CV</categories><comments>6 pages, 8 figures and 1 table (Conference Paper)</comments><journal-ref>Proceedings of the IEEE International Conference on Advanced
  Computing (ICoAC)-2013, pp.XX-XX,Chennai, India, 18 - 20 December (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is an important facet of explorative data mining and finds
extensive use in several fields. In this paper, we propose an extension of the
classical Fuzzy C-Means clustering algorithm. The proposed algorithm,
abbreviated as VFC, adopts a multi-dimensional membership vector for each data
point instead of the traditional, scalar membership value defined in the
original algorithm. The membership vector for each point is obtained by
considering each feature of that point separately and obtaining individual
membership values for the same. We also propose an algorithm to efficiently
allocate the initial cluster centers close to the actual centers, so as to
facilitate rapid convergence. Further, we propose a scheme to achieve crisp
clustering using the VFC algorithm. The proposed, novel clustering scheme has
been tested on two standard data sets in order to analyze its performance. We
also examine the efficacy of the proposed scheme by analyzing its performance
on image segmentation examples and comparing it with the classical Fuzzy
C-means clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4075</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4075</id><created>2013-12-14</created><authors><author><keyname>Nasrabadi</keyname><forenames>Ebrahim</forenames></author><author><keyname>Orlin</keyname><forenames>James B.</forenames></author></authors><title>Robust optimization with incremental recourse</title><categories>cs.CC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an adaptive approach to address optimization
problems with uncertain cost parameters. Here, the decision maker selects an
initial decision, observes the realization of the uncertain cost parameters,
and then is permitted to modify the initial decision. We treat the uncertainty
using the framework of robust optimization in which uncertain parameters lie
within a given set. The decision maker optimizes so as to develop the best cost
guarantee in terms of the worst-case analysis. The recourse decision is
``incremental&quot;; that is, the decision maker is permitted to change the initial
solution by a small fixed amount. We refer to the resulting problem as the
robust incremental problem. We study robust incremental variants of several
optimization problems. We show that the robust incremental counterpart of a
linear program is itself a linear program if the uncertainty set is polyhedral.
Hence, it is solvable in polynomial time. We establish the NP-hardness for
robust incremental linear programming for the case of a discrete uncertainty
set. We show that the robust incremental shortest path problem is NP-complete
when costs are chosen from a polyhedral uncertainty set, even in the case that
only one new arc may be added to the initial path. We also address the
complexity of several special cases of the robust incremental shortest path
problem and the robust incremental minimum spanning tree problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4076</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4076</id><created>2013-12-14</created><authors><author><keyname>Lopez-Pablos</keyname><forenames>Rodrigo</forenames><affiliation>Universidad Nacional de La Matanza y Universidad Tecnol&#xf3;gica Nacional</affiliation></author></authors><title>Elementos de ingenier\'ia de explotaci\'on de la informaci\'on:
  r\'eplica y algunos trazos sobre teor\'ia inform\'atica</title><categories>cs.IT math.IT</categories><comments>4 pages, 1 figure, written in castilian, Anales de la AAEP, ISSN
  1852-0022</comments><acm-class>H.1.1; I.2.4</acm-class><journal-ref>Asoc.Arg.Econ.Polit. XLVIII RA (2013): 1-4</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A reply to the commentaries of Yana (2013), and some jots on information
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4077</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4077</id><created>2013-12-14</created><authors><author><keyname>Chakraborty</keyname><forenames>Arpita</forenames></author><author><keyname>Ganguly</keyname><forenames>Srinjoy</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal Kanti</forenames></author><author><keyname>Karmakar</keyname><forenames>Anupam</forenames></author></authors><title>A Trust Based Congestion Aware Hybrid Ant Colony Optimization Algorithm
  for Energy Efficient Routing in Wireless Sensor Networks (TC-ACO)</title><categories>cs.NI</categories><comments>6 pages, 5 figures and 2 tables (Conference Paper)</comments><journal-ref>Proceedings of the IEEE International Conference on Advanced
  Computing (ICoAC)-2013, pp.XX-XX,Chennai, India, 18 - 20 December (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Congestion is a problem of paramount importance in resource constrained
Wireless Sensor Networks, especially for large networks, where the traffic
loads exceed the available capacity of the resources. Sensor nodes are prone to
failure and the misbehavior of these faulty nodes creates further congestion.
The resulting effect is a degradation in network performance, additional
computation and increased energy consumption, which in turn decreases network
lifetime. Hence, the data packet routing algorithm should consider congestion
as one of the parameters, in addition to the role of the faulty nodes and not
merely energy efficient protocols. Unfortunately most of the researchers have
tried to make the routing schemes energy efficient without considering
congestion factor and the effect of the faulty nodes. In this paper we have
proposed a congestion aware, energy efficient, routing approach that utilizes
Ant Colony Optimization algorithm, in which faulty nodes are isolated by means
of the concept of trust. The merits of the proposed scheme are verified through
simulations where they are compared with other protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4078</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4078</id><created>2013-12-14</created><authors><author><keyname>Mozaffari</keyname><forenames>Ahmad</forenames></author><author><keyname>Fathi</keyname><forenames>Alireza</forenames></author></authors><title>A natural-inspired optimization machine based on the annual migration of
  salmons in nature</title><categories>cs.NE</categories><comments>12 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio inspiration is a branch of artificial simulation science that shows
pervasive contributions to variety of engineering fields such as automate
pattern recognition, systematic fault detection and applied optimization. In
this paper, a new metaheuristic optimizing algorithm that is the simulation of
The Great Salmon Run(TGSR) is developed. The obtained results imply on the
acceptable performance of implemented method in optimization of complex non
convex, multi dimensional and multi-modal problems. To prove the superiority of
TGSR in both robustness and quality, it is also compared with most of the well
known proposed optimizing techniques such as Simulated Annealing (SA), Parallel
Migrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA),
Particle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony
(ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained results
confirm the acceptable performance of the proposed method in both robustness
and quality for different bench-mark optimizing problems and also prove the
authors claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4082</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4082</id><created>2013-12-14</created><authors><author><keyname>Mesmoudi</keyname><forenames>Asma</forenames></author><author><keyname>Feham</keyname><forenames>Mohammed</forenames></author><author><keyname>Labraoui</keyname><forenames>Nabila</forenames></author></authors><title>Wireless sensor networks localization algorithms: a comprehensive survey</title><categories>cs.NI</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSNs) have recently gained a lot of attention by
scientific community. Small and inexpensive devices with low energy consumption
and limited computing resources are increasingly being adopted in different
application scenarios including environmental monitoring, target tracking and
biomedical health monitoring. In many such applications, node localization is
inherently one of the system parameters. Localization process is necessary to
report the origin of events, routing and to answer questions on the network
coverage,assist group querying of sensors. In general, localization schemes are
classified into two broad categories: range-based and range-free. However, it
is difficult to classify hybrid solutions as range-based or range-free. In this
paper we make this classification easy, where range-based schemes and
range-free schemes are divided into two types: fully schemes and hybrid
schemes. Moreover, we compare the most relevant localization algorithms and
discuss the future research directions for wireless sensor networks
localization schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4091</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4091</id><created>2013-12-14</created><updated>2014-03-31</updated><authors><author><keyname>Ashrafi</keyname><forenames>Shwan</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author><author><keyname>Firooz</keyname><forenames>Hamed</forenames></author></authors><title>On Dissemination Time of Random Linear Network Coding in Ad-hoc Networks</title><categories>cs.IT math.IT</categories><comments>An example needs correction. No replacement available</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random linear network coding (RLNC) unicast protocol is analyzed over a
rapidly-changing network topology. We model the probability mass function (pmf)
of the dissemination time as a sequence of independent geometric random
variables whose success probability changes with every successful reception of
an innovative packet. We derive a tight approximation of the average networked
innovation probability conditioned on network dimension increase. We show
through simulations that our approximations for the average dissemination time
and its pmf are tight. We then propose to use a RLNC-based broadcast
dissemination protocol over a general dynamic topology where nodes are chosen
for transmission based on average innovative information that they can provided
to the rest of the network. Simulation results show that information
disseminates considerably faster as opposed to standard RLNC algorithm where
nodes are chosen uniformly at random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4092</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4092</id><created>2013-12-14</created><authors><author><keyname>Grave</keyname><forenames>Edouard</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>LIGM</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Domain adaptation for sequence labeling using hidden Markov models</title><categories>cs.CL cs.LG</categories><comments>New Directions in Transfer and Multi-Task: Learning Across Domains
  and Tasks (NIPS Workshop) (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most natural language processing systems based on machine learning are not
robust to domain shift. For example, a state-of-the-art syntactic dependency
parser trained on Wall Street Journal sentences has an absolute drop in
performance of more than ten points when tested on textual data from the Web.
An efficient solution to make these methods more robust to domain shift is to
first learn a word representation using large amounts of unlabeled data from
both domains, and then use this representation as features in a supervised
learning algorithm. In this paper, we propose to use hidden Markov models to
learn word representations for part-of-speech tagging. In particular, we study
the influence of using data from the source, the target or both domains to
learn the representation and the different ways to represent words using an
HMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4101</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4101</id><created>2013-12-14</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author></authors><title>Trees and co-trees with constant maximum degree in planar 3-connected
  graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the conjecture by Gr\&quot;unbaum that every planar
3-connected graph has a spanning tree $T$ such that both $T$ and its co-tree
have maximum degree at most 3. Here, the co-tree of $T$ is the spanning tree of
the dual obtained by taking the duals of the non-tree edges. While Gr\&quot;unbaum's
conjecture remains open, we show that every planar 3-connected graph has a
spanning tree $T$ such that both $T$ and its co-tree have maximum degree at
most 5. It can be found in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4106</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4106</id><created>2013-12-14</created><authors><author><keyname>Nakaizumi</keyname><forenames>Chisaki</forenames></author><author><keyname>Mori</keyname><forenames>Koichi</forenames></author><author><keyname>Matsui</keyname><forenames>Toshie</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Auditory Brain-Computer Interface Paradigm with Head Related Impulse
  Response-based Spatial Cues</title><categories>q-bio.NC cs.HC</categories><comments>The final publication is available at IEEE Xplore
  http://ieeexplore.ieee.org and the copyright of the final version has been
  transferred to IEEE (c)2013</comments><journal-ref>Proceedings of the 9th International Conference on Signal Image
  Technology and Internet Based Systems. Kyoto, Japan: IEEE Computer Society;
  2013. p. 806-811</journal-ref><doi>10.1109/SITIS.2013.131</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The aim of this study is to provide a comprehensive test of head related
impulse response (HRIR) for an auditory spatial speller brain-computer
interface (BCI) paradigm. The study is conducted with six users in an
experimental set up based on five Japanese hiragana vowels. Auditory evoked
potentials resulted with encouragingly good and stable &quot;aha-&quot; or P300-responses
in real-world online BCI experiments. Our case study indicated that the
auditory HRIR spatial sound reproduction paradigm could be a viable alternative
to the established multi-loudspeaker surround sound BCI-speller applications,
as far as healthy pilot study users are concerned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4108</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4108</id><created>2013-12-15</created><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author><author><keyname>Balaban</keyname><forenames>Mehmet Erdal</forenames></author></authors><title>A MapReduce based distributed SVM algorithm for binary classification</title><categories>cs.LG cs.DC</categories><comments>19 Pages. arXiv admin note: text overlap with arXiv:1301.0082</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Support Vector Machine (SVM) algorithm has a high generalization
property to classify for unseen examples after training phase and it has small
loss value, the algorithm is not suitable for real-life classification and
regression problems. SVMs cannot solve hundreds of thousands examples in
training dataset. In previous studies on distributed machine learning
algorithms, SVM is trained over a costly and preconfigured computer
environment. In this research, we present a MapReduce based distributed
parallel SVM training algorithm for binary classification problems. This work
shows how to distribute optimization problem over cloud computing systems with
MapReduce technique. In the second step of this work, we used statistical
learning theory to find the predictive hypothesis that minimize our empirical
risks from hypothesis spaces that created with reduce function of MapReduce.
The results of this research are important for training of big datasets for SVM
algorithm based classification problems. We provided that iterative training of
split dataset with MapReduce technique; accuracy of the classifier function
will converge to global optimal classifier function's accuracy in finite
iteration size. The algorithm performance was measured on samples from letter
recognition and pen-based recognition of handwritten digits dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4113</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4113</id><created>2013-12-15</created><updated>2014-05-29</updated><authors><author><keyname>Mattei</keyname><forenames>Nicholas</forenames></author><author><keyname>Dodson</keyname><forenames>Thomas</forenames></author><author><keyname>Guerin</keyname><forenames>Joshua T.</forenames></author><author><keyname>Goldsmith</keyname><forenames>Judy</forenames></author><author><keyname>Mazur</keyname><forenames>Joan M.</forenames></author></authors><title>Lessons Learned from Development of a Software Tool to Support Academic
  Advising</title><categories>cs.CY physics.ed-ph</categories><comments>5 Figures, revised version including more figures and
  cross-referencing</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We detail some lessons learned while designing and testing a
decision-theoretic advising support tool for undergraduates at a large state
university. Between 2009 and 2011 we conducted two surveys of over 500 students
in multiple majors and colleges. These surveys asked students detailed
questions about their preferences concerning course selection, advising, and
career paths. We present data from this study which may be helpful for faculty
and staff who advise undergraduate students. We find that advising support
software tools can augment the student-advisor relationship, particularly in
terms of course planning, but cannot and should not replace in-person advising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4114</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4114</id><created>2013-12-15</created><authors><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author><author><keyname>Ong</keyname><forenames>Luke</forenames></author></authors><title>EGuaranteeNash for Boolean Games is NEXP-Hard</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean games are an expressive and natural formalism through which to
investigate problems of strategic interaction in multiagent systems. Although
they have been widely studied, almost all previous work on Nash equilibria in
Boolean games has focused on the restricted setting of pure strategies. This is
a shortcoming as finite games are guaranteed to have at least one equilibrium
in mixed strategies, but many simple games fail to have pure strategy
equilibria at all. We address this by showing that a natural decision problem
about mixed equilibria: determining whether a Boolean game has a mixed strategy
equilibrium that guarantees every player a given payoff, is NEXP-hard.
Accordingly, the $\epsilon$ variety of the problem is NEXP-complete. The proof
can be adapted to show coNEXP-hardness of a similar question: whether all Nash
equilibria of a Boolean game guarantee every player at least the given payoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4116</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4116</id><created>2013-12-15</created><authors><author><keyname>Kumar</keyname><forenames>Niraj</forenames></author><author><keyname>Goswami</keyname><forenames>Debabrata</forenames></author></authors><title>Quantum Algorithm to Solve a Maze: Converting the Maze Problem into a
  Search Problem</title><categories>quant-ph cs.DS</categories><comments>5 pages, 5 figures,Appeared in Asian Quantum Information
  Science(AQIS'13) Conference, Chennai, India, August 2013.
  http://www.imsc.res.in/ aqis13/submissions/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a different methodology towards approaching a Maze problem. We
convert the problem into a Quantum Search Problem (QSP), and its solutions are
sought for using the iterative Grover's Search Algorithm. Though the category
of mazes we are looking at are of the NP complete class, we have redirected
such a NP complete problem into a QSP. Our solution deals with two dimensional
perfect mazes with no closed loops. We encode all possible individual paths
from the starting point of the maze into a quantum register. A quantum fitness
operator applied on the register encodes each individual with its fitness
value. We propose an oracle design which marks all the individuals above a
certain fitness value and use the Grover search algorithm to find one of the
marked states. Iterating over this method, we approach towards the optimum
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4124</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4124</id><created>2013-12-15</created><authors><author><keyname>Khalili</keyname><forenames>Maryam Soltanali</forenames></author><author><keyname>Sadjedi</keyname><forenames>Hamed</forenames></author></authors><title>A robust Iris recognition method on adverse conditions</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.3,No.5,October 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As a stable biometric system, iris has recently attracted great attention
among the researchers. However, research is still needed to provide appropriate
solutions to ensure the resistance of the system against error factors. The
present study has tried to apply a mask to the image so that the unexpected
factors affecting the location of the iris can be removed. So, pupil
localization will be faster and robust. Then to locate the exact location of
the iris, a simple stage of boundary displacement due to the Canny edge
detector has been applied. Then, with searching left and right IRIS edge point,
outer radios of IRIS will be detect. Through the process of extracting the iris
features, it has been sought to obtain the distinctive iris texture features by
using a discrete stationary wavelets transform 2-D (DSWT2). Using DSWT2 tool
and symlet 4 wavelet, distinctive features are extracted. To reduce the
computational cost, the features obtained from the application of the wavelet
have been investigated and a feature selection procedure, using similarity
criteria, has been implemented. Finally, the iris matching has been performed
using a semi-correlation criterion. The accuracy of the proposed method for
localization on CASIA-v1, CASIA-v3 is 99.73%, 98.24% and 97.04%, respectively.
The accuracy of the feature extraction proposed method for CASIA3 iris images
database is 97.82%, which confirms the efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4125</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4125</id><created>2013-12-15</created><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Li</keyname><forenames>Jerry</forenames></author><author><keyname>Roy</keyname><forenames>Sudeepa</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>Model Counting of Query Expressions: Limitations of Propositional
  Methods</title><categories>cs.DB cs.CC</categories><comments>To appear in International Conference on Database Theory (ICDT) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query evaluation in tuple-independent probabilistic databases is the problem
of computing the probability of an answer to a query given independent
probabilities of the individual tuples in a database instance. There are two
main approaches to this problem: (1) in `grounded inference' one first obtains
the lineage for the query and database instance as a Boolean formula, then
performs weighted model counting on the lineage (i.e., computes the probability
of the lineage given probabilities of its independent Boolean variables); (2)
in methods known as `lifted inference' or `extensional query evaluation', one
exploits the high-level structure of the query as a first-order formula.
Although it is widely believed that lifted inference is strictly more powerful
than grounded inference on the lineage alone, no formal separation has
previously been shown for query evaluation. In this paper we show such a formal
separation for the first time.
  We exhibit a class of queries for which model counting can be done in
polynomial time using extensional query evaluation, whereas the algorithms used
in state-of-the-art exact model counters on their lineages provably require
exponential time. Our lower bounds on the running times of these exact model
counters follow from new exponential size lower bounds on the kinds of d-DNNF
representations of the lineages that these model counters (either explicitly or
implicitly) produce. Though some of these queries have been studied before, no
non-trivial lower bounds on the sizes of these representations for these
queries were previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4126</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4126</id><created>2013-12-15</created><updated>2013-12-17</updated><authors><author><keyname>Durand</keyname><forenames>Bruno</forenames></author><author><keyname>Gamard</keyname><forenames>Guilhem</forenames></author><author><keyname>Grandjean</keyname><forenames>Anael</forenames></author></authors><title>Aperiodic tilings and entropy</title><categories>cs.LO math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a construction of Kari-Culik aperiodic tile set -
the smallest known until now. With the help of this construction, we prove that
this tileset has positive entropy. We also explain why this result was not
expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4127</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4127</id><created>2013-12-15</created><authors><author><keyname>Eldin</keyname><forenames>Ashraf M. Mohy</forenames></author><author><keyname>Youssif</keyname><forenames>Aliaa A. A.</forenames></author></authors><title>A Hybrid Approach for Co-Channel Speech Segregation based on CASA, HMM
  Multipitch Tracking, and Medium Frame Harmonic Model</title><categories>cs.SD</categories><comments>Keywords: CASA (computational auditory scene analysis); co-channel
  speech segregation; HMM (hidden Markov model) tracking; hybrid speech
  segregation approach; medium frame harmonic model; multipitch tracking;
  prominent pitch</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA)Volume 4 Issue 7, 2013</journal-ref><doi>10.14569/IJACSA.2013.040721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a hybrid approach for co-channel speech segregation. HMM
(hidden Markov model) is used to track the pitches of 2 talkers. The resulting
pitch tracks are then enriched with the prominent pitch. The enriched tracks
are correctly grouped using pitch continuity. Medium frame harmonics are used
to extract the second pitch for frames with only one pitch deduced using the
previous steps. Finally, the pitch tracks are input to CASA (computational
auditory scene analysis) to segregate the mixed speech. The center frequency
range of the gamma tone filter banks is maximized to reduce the overlap between
the channels filtered for better segregation. Experiments were conducted using
this hybrid approach on the speech separation challenge database and compared
to the single (non-hybrid) approaches, i.e. signal processing and CASA. Results
show that using the hybrid approach outperforms the single approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4132</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4132</id><created>2013-12-15</created><authors><author><keyname>Mozaffari</keyname><forenames>Ahmad</forenames></author><author><keyname>Fathi</keyname><forenames>Alireza</forenames></author></authors><title>An introduction to synchronous self-learning Pareto strategy</title><categories>cs.NE</categories><comments>17 pages, 7 figure, 3 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In last decades optimization and control of complex systems that possessed
various conflicted objectives simultaneously attracted an incremental interest
of scientists. This is because of the vast applications of these systems in
various fields of real life engineering phenomena that are generally multi
modal, non convex and multi criterion. Hence, many researchers utilized
versatile intelligent models such as Pareto based techniques, game theory
(cooperative and non cooperative games), neuro evolutionary systems, fuzzy
logic and advanced neural networks for handling these types of problems. In
this paper a novel method called Synchronous Self Learning Pareto Strategy
Algorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),
Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map
(CSOM) simultaneously incorporating with a data shuffling behavior.
Evolutionary Algorithms (EA) which attempt to simulate the phenomenon of
natural evolution are powerful numerical optimization algorithms that reach an
approximate global maximum of a complex multi variable function over a wide
search space and swarm base technique can improved the intensity and the
robustness in EA. CSOM is a neural network capable of learning and can improve
the quality of obtained optimal Pareto front. To prove the efficient
performance of proposed algorithm, authors utilized some well known benchmark
test functions. Obtained results indicate that the cited method is best suit in
the case of vector optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4134</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4134</id><created>2013-12-15</created><authors><author><keyname>Brodskaya</keyname><forenames>Julia</forenames></author></authors><title>The Generation of Minimal Tests Sets and Some Minimal Tests</title><categories>cs.DM</categories><comments>16 pages, in Russian</comments><report-no>Saratov State University 001</report-no><msc-class>68R05 (Primary), 68R99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method for a problem solution of expenditures reduction of computing
resources and time is developed at a pattern recognition, with the way of
construction of the minimum tests sets or separate minimum tests on Boolean
matrixes is suggested
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4139</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4139</id><created>2013-12-15</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards slime mould colour sensor: Recognition of colours by Physarum
  polycephalum</title><categories>cs.ET physics.bio-ph q-bio.CB</categories><journal-ref>Adamatzky A. Towards slime mould colour sensor: Recognition of
  colours by Physarum polycephalum. Organic Electronics 14 (2013) 12, 3355-3361</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acellular slime mould Physarum polycephalum is a popular now user-friendly
living substrate for designing of future and emergent sensing and computing
devices. P. polycephalum exhibits regular patterns of oscillations of its
surface electrical potential. The oscillation patterns are changed when the
slime mould is subjected to mechanical, chemical, electrical or optical
stimuli. We evaluate feasibility of slime-mould based colour sensors by
illuminating Physarum with red, green, blue and white colours and analysing
patterns of the slime mould's electrical potential oscillations. We define that
the slime mould recognises a colour if it reacts to illumination with the
colour by a unique changes in amplitude and periods of oscillatory activity. In
laboratory experiments we found that the slime mould recognises red and blue
colour. The slime mould does not differentiate between green and white colours.
The slime mould also recognises when red colour is switched off. We also map
colours to diversity of the oscillations: illumination with a white colour
increases diversity of amplitudes and periods of oscillations, other colours
studied increase diversity either of amplitude or period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4149</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4149</id><created>2013-12-15</created><authors><author><keyname>Sagheer</keyname><forenames>Alaa</forenames></author><author><keyname>Zidan</keyname><forenames>Mohammed</forenames></author></authors><title>Autonomous Quantum Perceptron Neural Network</title><categories>cs.NE</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, with the rapid development of technology, there are a lot of
applications require to achieve low-cost learning. However the computational
power of classical artificial neural networks, they are not capable to provide
low-cost learning. In contrast, quantum neural networks may be representing a
good computational alternate to classical neural network approaches, based on
the computational power of quantum bit (qubit) over the classical bit. In this
paper we present a new computational approach to the quantum perceptron neural
network can achieve learning in low-cost computation. The proposed approach has
only one neuron can construct self-adaptive activation operators capable to
accomplish the learning process in a limited number of iterations and, thereby,
reduce the overall computational cost. The proposed approach is capable to
construct its own set of activation operators to be applied widely in both
quantum and classical applications to overcome the linearity limitation of
classical perceptron. The computational power of the proposed approach is
illustrated via solving variety of problems where promising and comparable
results are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4162</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4162</id><created>2013-12-15</created><authors><author><keyname>Esp&#xe8;s</keyname><forenames>David</forenames></author><author><keyname>Pistea</keyname><forenames>Ana Maria</forenames></author><author><keyname>Canaff</keyname><forenames>Charles</forenames></author><author><keyname>Iordache</keyname><forenames>Ioana</forenames></author><author><keyname>Parc</keyname><forenames>Philippe Le</forenames></author><author><keyname>Radoi</keyname><forenames>Emanuel</forenames></author></authors><title>New Method for Localization and Human Being Detection using UWB
  Technology: Helpful Solution for Rescue Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two challenges for rescue robots are to detect human beings and to have an
accurate positioning system. In indoor positioning, GPS receivers cannot be
used due to the reflections or attenuation caused by obstacles. To detect human
beings, sensors such as thermal camera, ultrasonic and microphone can be
embedded on the rescue robot. The drawback of these sensors is the detection
range. These sensors have to be in close proximity to the victim in order to
detect it. UWB technology is then very helpful to ensure precise localization
of the rescue robot inside the disaster site and detect human beings.
  We propose a new method to both detect human beings and locate the rescue
robot at the same time. To achieve these goals we optimize the design of UWB
pulses based on B-splines. The spectral effectiveness is optimized so the
symbols are easier to detect and the mitigation with noise is reduced. Our
positioning system performs to locate the rescue robot with an accuracy about 2
centimeters. During some tests we discover that UWB signal characteristics
abruptly change after passing through a human body. Our system uses this
particular signature to detect human body.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4176</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4176</id><created>2013-12-15</created><updated>2014-11-10</updated><authors><author><keyname>Oliva</keyname><forenames>Gabriele</forenames></author><author><keyname>Setola</keyname><forenames>Roberto</forenames></author><author><keyname>Hadjicostis</keyname><forenames>Christoforos N.</forenames></author></authors><title>Distributed k-means algorithm</title><categories>cs.LG cs.DC</categories><comments>preprint submitted to IEEE transactions on mobile computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a fully distributed implementation of the k-means
clustering algorithm, intended for wireless sensor networks where each agent is
endowed with a possibly high-dimensional observation (e.g., position, humidity,
temperature, etc.) The proposed algorithm, by means of one-hop communication,
partitions the agents into measure-dependent groups that have small in-group
and large out-group &quot;distances&quot;. Since the partitions may not have a relation
with the topology of the network--members of the same clusters may not be
spatially close--the algorithm is provided with a mechanism to compute the
clusters'centroids even when the clusters are disconnected in several
sub-clusters.The results of the proposed distributed algorithm coincide, in
terms of minimization of the objective function, with the centralized k-means
algorithm. Some numerical examples illustrate the capabilities of the proposed
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4177</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4177</id><created>2013-12-15</created><authors><author><keyname>Pham</keyname><forenames>Congduc</forenames></author><author><keyname>Diop</keyname><forenames>Mamour</forenames></author><author><keyname>Thiare</keyname><forenames>Ousmane</forenames></author></authors><title>Selecting source image sensor nodes based on 2-hop information to
  improve image transmissions to mobile robot sinks in search \&amp; rescue
  operations</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Robot-assisted Search $\&amp;$ Rescue operations enhanced with some
fixed image sensor nodes capable of capturing and sending visual information to
a robot sink. In order to increase the performance of image transfer from image
sensor nodes to the robot sinks we propose a 2-hop neighborhood
information-based cover set selection to determine the most relevant image
sensor nodes to activate. Then, in order to be consistent with our proposed
approach, a multi-path extension of Greedy Perimeter Stateless Routing (called
T-GPSR) wherein routing decisions are also based on 2-hop neighborhood
information is proposed. Simulation results show that our proposal reduces
packet losses, enabling fast packet delivery and higher visual quality of
received images at the robot sink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4179</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4179</id><created>2013-12-15</created><updated>2014-08-01</updated><authors><author><keyname>Rath</keyname><forenames>Satyajit</forenames></author><author><keyname>Sahoo</keyname><forenames>B. P. S.</forenames></author><author><keyname>Pandey</keyname><forenames>S. K.</forenames></author><author><keyname>Sandha</keyname><forenames>D. P.</forenames></author></authors><title>Multi-Parameter Decision Support with Data Transmission over GSM/GPRS
  Network: a Case Study of Landslide Monitoring</title><categories>cs.NI</categories><comments>6 pages, 2 figures, MODSOLVE 2013. Rath, Satyajit, et al.
  &quot;Multi-Parameter Decision Support with Data Transmission over GSM/GPRS
  Network: a Case Study of Landslide Monitoring.&quot; in: Proceedings of MODSOLVE
  2013, CSIR-NISCAIR, New Delhi, India. May 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The planet Earth has hundreds of impact events, with some occurrences causing
both in terms of human casualty as well as economic losses. Such attitudes of
earth pushed the frontiers to develop innovative monitoring strategies for the
earth system. To make that real, although, will require coherent and real-time
data by observing the earth behavior contiguously. Wireless Sensor Network
(WSN) appears to be the best suitable infrastructure to sense environmental
parameters of our interests. In this event of earth observation, another
important issue is the monitoring system with high level of precision. There
are different types of sensors to measure the behavioral aspects of earth. The
sensors integrated with WSN, provide an accurate and contiguous data for
analysis and interpretation. This paper briefly addresses earth observation and
areas of critical importance to people and society. A case study has also been
carried out for disaster like Landslide in the North Eastern region of India.
Application software has been developed for the said study for online data
acquisition and analysis with pre-disaster early warning system. The system
monitors the changing geotechnical condition of this region using various
geo-technical sensors like Rain gauge, In-place Inclinometer, Tilt-meter,
Piezo-meter and Crack meter. This paper also touches upon the aspects of data
transmission over Global System for Mobile Communication (GSM) / General Packet
Radio Service (GPRS) to a remote data center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4182</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4182</id><created>2013-12-15</created><updated>2015-08-07</updated><authors><author><keyname>Agrawal</keyname><forenames>Shweta</forenames></author><author><keyname>Gelles</keyname><forenames>Ran</forenames></author><author><keyname>Sahai</keyname><forenames>Amit</forenames></author></authors><title>Adaptive Protocols for Interactive Communication</title><categories>cs.DS cs.IT math.IT</categories><comments>Content is similar to previous version yet with an improved
  presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How much adversarial noise can protocols for interactive communication
tolerate? This question was examined by Braverman and Rao (IEEE Trans. Inf.
Theory, 2014) for the case of &quot;robust&quot; protocols, where each party sends
messages only in fixed and predetermined rounds. We consider a new class of
non-robust protocols for Interactive Communication, which we call adaptive
protocols. Such protocols adapt structurally to the noise induced by the
channel in the sense that both the order of speaking, and the length of the
protocol may vary depending on observed noise.
  We define models that capture adaptive protocols and study upper and lower
bounds on the permissible noise rate in these models. When the length of the
protocol may adaptively change according to the noise, we demonstrate a
protocol that tolerates noise rates up to $1/3$. When the order of speaking may
adaptively change as well, we demonstrate a protocol that tolerates noise rates
up to $2/3$. Hence, adaptivity circumvents an impossibility result of $1/4$ on
the fraction of tolerable noise (Braverman and Rao, 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4185</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4185</id><created>2013-12-15</created><authors><author><keyname>Kappen</keyname><forenames>H. J.</forenames></author></authors><title>Comment: Causal entropic forces</title><categories>cond-mat.stat-mech cs.SY</categories><comments>2 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this comment I argue that the causal entropy proposed in [1] is
state-independent and the entropic force is zero for state-independent noise in
a discrete time formulation and that the causal entropy description is
incomplete in the continuous time case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4187</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4187</id><created>2013-12-15</created><authors><author><keyname>Seeman</keyname><forenames>Lior</forenames></author></authors><title>I'd Rather Stay Stupid: The Advantage of Having Low Utility</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by cost of computation in game theory, we explore how changing the
utilities of players (changing their complexity costs) affects the outcome of a
game. We show that even if we improve a player's utility in every action
profile, his payoff in equilibrium might be lower than in the equilibrium
before the change. We provide some conditions on games that are sufficient to
ensure this does not occur. We then show how this counter-intuitive phenomenon
can explain real life phenomena such as free riding, and why this might cause
people to give signals indicating that they are not as good as they really are.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4188</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4188</id><created>2013-12-15</created><authors><author><keyname>Reddy</keyname><forenames>Kamal Chandra</forenames></author><author><keyname>Tharwani</keyname><forenames>Ankit</forenames></author><author><keyname>Krishna</keyname><forenames>Ch. Vamshi</forenames></author><author><keyname>V</keyname><forenames>Lakshminarayanan.</forenames></author></authors><title>Parallel Firewalls on General-Purpose Graphics Processing Units</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Firewalls use a rule database to decide which packets will be allowed from
one network onto another thereby implementing a security policy. In high-speed
networks as the inter-arrival rate of packets decreases, the latency incurred
by a firewall increases. In such a scenario, a single firewall become a
bottleneck and reduces the overall throughput of the network.A firewall with
heavy load, which is supposed to be a first line of defense against attacks,
becomes susceptible to Denial of Service (DoS) attacks. Many works are being
done to optimize firewalls.This paper presents our implementation of different
parallel firewall models on General-Purpose Graphics Processing Unit (GPGPU).
We implemented the parallel firewall architecture proposed in and introduced a
new model that can effectively exploit the massively parallel computing
capabilities of GPGPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4189</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4189</id><created>2013-12-15</created><authors><author><keyname>Whiting</keyname><forenames>James G. H.</forenames></author><author><keyname>Costello</keyname><forenames>Ben P. J. de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards slime mould chemical sensor: Mapping chemical inputs onto
  electrical potential dynamics of Physarum Polycephalum</title><categories>cs.ET physics.bio-ph q-bio.CB</categories><journal-ref>Sensors and Actuators B: Chemical 191 (2014) 844-853</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We experimentally derived a unique one-to-one mapping between a range of
selected bioactive chemicals and patterns of oscillations of the slime mould's
extacellular electrical potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4190</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4190</id><created>2013-12-15</created><updated>2014-02-15</updated><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Hagara</keyname><forenames>Michal</forenames></author></authors><title>One-Shot-Learning Gesture Recognition using HOG-HOF Features</title><categories>cs.CV</categories><comments>20 pages, 10 figures, 2 tables To appear in Journal of Machine
  Learning Research subject to minor revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to describe one-shot-learning gesture
recognition systems developed on the \textit{ChaLearn Gesture Dataset}. We use
RGB and depth images and combine appearance (Histograms of Oriented Gradients)
and motion descriptors (Histogram of Optical Flow) for parallel temporal
segmentation and recognition. The Quadratic-Chi distance family is used to
measure differences between histograms to capture cross-bin relationships. We
also propose a new algorithm for trimming videos --- to remove all the
unimportant frames from videos. We present two methods that use combination of
HOG-HOF descriptors together with variants of Dynamic Time Warping technique.
Both methods outperform other published methods and help narrow down the gap
between human performance and algorithms on this task. The code has been made
publicly available in the MLOSS repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4203</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4203</id><created>2013-12-15</created><updated>2014-06-24</updated><authors><author><keyname>Fotakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Milis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zampetakis</keyname><forenames>Emmanouil</forenames></author><author><keyname>Zois</keyname><forenames>Georgios</forenames></author></authors><title>Scheduling MapReduce Jobs and Data Shuffle on Unrelated Processors</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose constant approximation algorithms for generalizations of the
Flexible Flow Shop (FFS) problem which form a realistic model for
non-preemptive scheduling in MapReduce systems. Our results concern the
minimization of the total weighted completion time of a set of MapReduce jobs
on unrelated processors and improve substantially on the model proposed by
Moseley et al. (SPAA 2011) in two directions. First, we consider each job
consisting of multiple Map and Reduce tasks, as this is the key idea behind
MapReduce computations, and we propose a constant approximation algorithm.
Then, we introduce into our model the crucial cost of data shuffle phase, i.e.,
the cost for the transmission of intermediate data from Map to Reduce tasks. In
fact, we model this phase by an additional set of Shuffle tasks for each job
and we manage to keep the same approximation ratio when they are scheduled on
the same processors with the corresponding Reduce tasks and to provide also a
constant ratio when they are scheduled on different processors. This is the
most general setting of the FFS problem (with a special third stage) for which
a constant approximation ratio is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4207</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4207</id><created>2013-12-15</created><updated>2015-01-31</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Wassell</keyname><forenames>Ian J.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Unlocking Energy Neutrality in Energy Harvesting Wireless Sensor
  Networks: An Approach Based on Distributed Compressed Sensing</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper advocates the use of the emerging distributed compressed sensing
(DCS) paradigm to deploy energy harvesting (EH) wireless sensor networks (WSN)
with practical network lifetime and data gathering rates that are substantially
higher than the state-of-the-art. The basis of our work is a centralized EH WSN
architecture where the sensors convey data to a fusion center, using stylized
models that capture the fact that the signals collected by different nodes can
exhibit correlation and that the energy harvested by different nodes can also
exhibit some degree of correlation. Via the probability of incorrect data
reconstruction, we characterize the performance of both a compressive sensing
(CS) and a DCS based approach to data acquisition and reconstruction. Moreover,
we perform an in-depth comparison of the proposed DCS based approach against a
state-of-the-art distributed source coding (DSC) system in terms of decoded
data distortion versus harvested energy. These performance characterizations
and comparisons embody the effect of various system phenomena and parameters
such as signal correlation, EH correlation, network size, and energy
availability level. Our results unveil that, for an EH WSN consisting of eight
SNs with our simple signal correlation and EH models, a target probability of
incorrect reconstruction of $10^{-2}$, and under the same EH capability as CS,
the proposed approach allows for a six-fold increase in data gathering
capability with respect to the baseline CS-based approach. Moreover, under the
same energy harvested level, the proposed solution offers a substantial
reduction of the mean-squared error distortion (up to 66.67\%) with respect to
the state-of-the-art DSC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4209</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4209</id><created>2013-12-15</created><authors><author><keyname>Davis</keyname><forenames>Richard</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Leong</keyname><forenames>Philip</forenames></author></authors><title>Feature Graph Architectures</title><categories>cs.LG</categories><comments>9 pages, with 5 pages of supplementary material (appendices)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we propose feature graph architectures (FGA), which are deep
learning systems employing a structured initialisation and training method
based on a feature graph which facilitates improved generalisation performance
compared with a standard shallow architecture. The goal is to explore
alternative perspectives on the problem of deep network training. We evaluate
FGA performance for deep SVMs on some experimental datasets, and show how
generalisation and stability results may be derived for these models. We
describe the effect of permutations on the model accuracy, and give a criterion
for the optimal permutation in terms of feature correlations. The experimental
results show that the algorithm produces robust and significant test set
improvements over a standard shallow SVM training method for a range of
datasets. These gains are achieved with a moderate increase in time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4224</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4224</id><created>2013-12-15</created><updated>2014-05-06</updated><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author></authors><title>A paradox in community detection</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 3 figures</comments><journal-ref>EPL 106, 38001 (2014)</journal-ref><doi>10.1209/0295-5075/106/38001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that virtually all algorithms aimed at the
identification of communities in networks are affected by the same main
limitation: the impossibility to detect communities, even when these are
well-defined, if the average value of the difference between internal and
external node degrees does not exceed a strictly positive value, in literature
known as detectability threshold. Here, we counterintuitively show that the
value of this threshold is inversely proportional to the intrinsic quality of
communities: the detection of well-defined modules is thus more difficult than
the identification of ill-defined communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4231</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4231</id><created>2013-12-15</created><updated>2015-03-04</updated><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Dependence space of matroids and its application to attribute reduction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute reduction is a basic issue in knowledge representation and data
mining. Rough sets provide a theoretical foundation for the issue. Matroids
generalized from matrices have been widely used in many fields, particularly
greedy algorithm design, which plays an important role in attribute reduction.
Therefore, it is meaningful to combine matroids with rough sets to solve the
optimization problems. In this paper, we introduce an existing algebraic
structure called dependence space to study the reduction problem in terms of
matroids. First, a dependence space of matroids is constructed. Second, the
characterizations for the space such as consistent sets and reducts are studied
through matroids. Finally, we investigate matroids by the means of the space
and present two expressions for their bases. In a word, this paper provides new
approaches to study attribute reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4232</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4232</id><created>2013-12-15</created><updated>2014-01-04</updated><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Geometric lattice structure of covering and its application to attribute
  reduction through matroids</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reduction of covering decision systems is an important problem in data
mining, and covering-based rough sets serve as an efficient technique to
process the problem. Geometric lattices have been widely used in many fields,
especially greedy algorithm design which plays an important role in the
reduction problems. Therefore, it is meaningful to combine coverings with
geometric lattices to solve the optimization problems. In this paper, we obtain
geometric lattices from coverings through matroids and then apply them to the
issue of attribute reduction. First, a geometric lattice structure of a
covering is constructed through transversal matroids. Then its atoms are
studied and used to describe the lattice. Second, considering that all the
closed sets of a finite matroid form a geometric lattice, we propose a
dependence space through matroids and study the attribute reduction issues of
the space, which realizes the application of geometric lattices to attribute
reduction. Furthermore, a special type of information system is taken as an
example to illustrate the application. In a word, this work points out an
interesting view, namely, geometric lattice to study the attribute reduction
issues of information systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4234</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4234</id><created>2013-12-15</created><updated>2015-03-04</updated><authors><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Connectedness of graphs and its application to connected matroids
  through covering-based rough sets</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph theoretical ideas are highly utilized by computer science fields
especially data mining. In this field, a data structure can be designed in the
form of tree. Covering is a widely used form of data representation in data
mining and covering-based rough sets provide a systematic approach to this type
of representation. In this paper, we study the connectedness of graphs through
covering-based rough sets and apply it to connected matroids. First, we present
an approach to inducing a covering by a graph, and then study the connectedness
of the graph from the viewpoint of the covering approximation operators.
Second, we construct a graph from a matroid, and find the matroid and the graph
have the same connectedness, which makes us to use covering-based rough sets to
study connected matroids. In summary, this paper provides a new approach to
studying graph theory and matroid theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4239</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4239</id><created>2013-12-15</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The zeta function for circular graphs</title><categories>math.SP cs.DM math.CV</categories><comments>24 pages and 7 figures</comments><msc-class>11M99, 68R10, 30D20, 11M26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the entire function zeta(n,s) which is the sum of l to the power -s,
where l runs over the positive eigenvalues of the Laplacian of the circular
graph C(n) with n vertices. We prove that the roots of zeta(n,s) converge for n
to infinity to the line Re(s)=1/2 in the sense that for every compact subset K
in the complement of this line, and large enough n, no root of the zeta
function zeta(n,s) is in K. To prove this, we look at the Dirac zeta function,
which uses the positive eigenvalues of the Dirac operator D=d+d^* of the
circular graph, the square root of the Laplacian. We extend a
Newton-Coates-Rolle type analysis for Riemann sums and use a derivative which
has similarities with the Schwarzian derivative. As the zeta functions
zeta(n,s) of the circular graphs are entire functions, the result does not say
anything about the roots of the classical Riemann zeta function zeta(s), which
is also the Dirac zeta function for the circle. Only for Re(s)&gt;1, the values of
zeta(n,s) converge suitably scaled to zeta(s). We also give a new solution to
the discrete Basel problem which is giving expressions like zeta_n(2) =
(n^2-1)/12 or zeta_n(4) = (n^2-1)(n^2+11)/45 which allows to re-derive the
values of the classical Basel problem zeta(2) = pi^2/6 or zeta(4)=pi^4/90 in
the continuum limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4252</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4252</id><created>2013-12-16</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author></authors><title>Three New Families of Zero-difference Balanced Functions with
  Applications</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><msc-class>05B10, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-difference balanced (ZDB) functions integrate a number of subjects in
combinatorics and algebra, and have many applications in coding theory,
cryptography and communications engineering. In this paper, three new families
of ZDB functions are presented. The first construction, inspired by the recent
work \cite{Cai13}, gives ZDB functions defined on the abelian groups $(\gf(q_1)
\times \cdots \times \gf(q_k), +)$ with new and flexible parameters. The other
two constructions are based on $2$-cyclotomic cosets and yield ZDB functions on
$\Z_n$ with new parameters. The parameters of optimal constant composition
codes, optimal and perfect difference systems of sets obtained from these new
families of ZDB functions are also summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4259</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4259</id><created>2013-12-16</created><authors><author><keyname>Kaur</keyname><forenames>Sandeep</forenames></author><author><keyname>Kaur</keyname><forenames>Harjot</forenames></author><author><keyname>Sehra</keyname><forenames>Sumeet Kaur</forenames></author></authors><title>Modification of Contract Net Protocol(CNP) : A Rule-Updation Approach</title><categories>cs.MA</categories><comments>7 pages, 6 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 11, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordination in multi-agent system is very essential, in order to perform
complex tasks and lead MAS towards its goal. Also, the member agents of
multi-agent system should be autonomous as well as collaborative to accomplish
the complex task for which multi-agent system is designed specifically.
Contract-Net Protocol (CNP) is one of the coordination mechanisms which is used
by multi-agent systems which prefer coordination through interaction protocols.
In order to overcome the limitations of conventional CNP, this paper proposes a
modification in conventional CNP called updated-CNP. Updated-CNP is an effort
towards updating of a CNP in terms of its limitations of modifiability and
communication overhead. The limitation of the modification of tasks, if the
task requirements change at any instance, corresponding to tasks which are
allocated to contractor agents by manager agents is possible in our updated-CNP
version, which was not possible in the case of conventional-CNP, as it has to
be restarted in the case of task modification. This in turn will be reducing
the communication overhead of CNP, which is time taken by various agents using
CNP to pass messages to each other. For the illustration of the updated CNP, we
have used a sound predator-prey case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4265</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4265</id><created>2013-12-16</created><authors><author><keyname>Cayrel</keyname><forenames>Pierre-Louis</forenames></author><author><keyname>Meziani</keyname><forenames>Mohammed</forenames></author></authors><title>Post-Quantum Cryptography: Code-based Signatures</title><categories>cs.CR</categories><journal-ref>Proceedings of ISA 2010, LNCS 6059, pages 82-99, Springer-Verlag,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey provides a comparative overview of code-based signature schemes
with respect to security and performance. Furthermore, we explicitly describe
serveral code-based signature schemes with additional properties such as
identity-based, threshold ring and blind signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4280</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4280</id><created>2013-12-16</created><authors><author><keyname>Xu</keyname><forenames>Chunlei</forenames></author><author><keyname>Zhao</keyname><forenames>Yun-Bin</forenames></author></authors><title>Uniqueness Conditions for A Class of l0-Minimization Problems</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of l0-minimization problems, which is to search for the
partial sparsest solution to an underdetermined linear system with additional
constraints. We introduce several concepts, including lp-induced norm (0 &lt; p &lt;
1), maximal scaled spark and scaled mutual coherence, to develop several new
uniqueness conditions for the partial sparsest solution to this class of
l0-minimization problems. A further improvement of some of these uniqueness
criteria have been also achieved through the so-called concepts such as maximal
scaled (sub)coherence rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4283</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4283</id><created>2013-12-16</created><authors><author><keyname>He</keyname><forenames>Yeye</forenames></author><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Naughton</keyname><forenames>Jeffrey F.</forenames></author></authors><title>On Load Shedding in Complex Event Processing</title><categories>cs.DB</categories><comments>The conference version of this work to appear in the International
  Conference on Database Theory (ICDT), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex Event Processing (CEP) is a stream processing model that focuses on
detecting event patterns in continuous event streams. While the CEP model has
gained popularity in the research communities and commercial technologies, the
problem of gracefully degrading performance under heavy load in the presence of
resource constraints, or load shedding, has been largely overlooked. CEP is
similar to &quot;classical&quot; stream data management, but addresses a substantially
different class of queries. This unfortunately renders the load shedding
algorithms developed for stream data processing inapplicable. In this paper we
study CEP load shedding under various resource constraints. We formalize broad
classes of CEP load-shedding scenarios as different optimization problems. We
demonstrate an array of complexity results that reveal the hardness of these
problems and construct shedding algorithms with performance guarantees. Our
results shed some light on the difficulty of developing load-shedding
algorithms that maximize utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4287</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4287</id><created>2013-12-16</created><authors><author><keyname>Governatori</keyname><forenames>Guido</forenames></author><author><keyname>Olivieri</keyname><forenames>Francesco</forenames></author><author><keyname>Scannapieco</keyname><forenames>Simone</forenames></author><author><keyname>Rotolo</keyname><forenames>Antonino</forenames></author><author><keyname>Cristani</keyname><forenames>Matteo</forenames></author></authors><title>Strategic Argumentation is NP-Complete</title><categories>cs.LO cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the complexity of strategic argumentation for dialogue
games. A dialogue game is a 2-player game where the parties play arguments. We
show how to model dialogue games in a skeptical, non-monotonic formalism, and
we show that the problem of deciding what move (set of rules) to play at each
turn is an NP-complete problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4314</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4314</id><created>2013-12-16</created><updated>2014-03-09</updated><authors><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author></authors><title>Learning Factored Representations in a Deep Mixture of Experts</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixtures of Experts combine the outputs of several &quot;expert&quot; networks, each of
which specializes in a different part of the input space. This is achieved by
training a &quot;gating&quot; network that maps each input to a distribution over the
experts. Such models show promise for building larger networks that are still
cheap to compute at test time, and more parallelizable at training time. In
this this work, we extend the Mixture of Experts to a stacked model, the Deep
Mixture of Experts, with multiple sets of gating and experts. This
exponentially increases the number of effective experts by associating each
input with a combination of experts at each layer, yet maintains a modest model
size. On a randomly translated version of the MNIST dataset, we find that the
Deep Mixture of Experts automatically learns to develop location-dependent
(&quot;where&quot;) experts at the first layer, and class-specific (&quot;what&quot;) experts at
the second layer. In addition, we see that the different combinations are in
use when the model is applied to a dataset of speech monophones. These
demonstrate effective use of all expert combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4318</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4318</id><created>2013-12-16</created><authors><author><keyname>Mhembere</keyname><forenames>Disa</forenames></author><author><keyname>Roncal</keyname><forenames>William Gray</forenames></author><author><keyname>Sussman</keyname><forenames>Daniel</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author><author><keyname>Jung</keyname><forenames>Rex</forenames></author><author><keyname>Ryman</keyname><forenames>Sephira</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author></authors><title>Computing Scalable Multivariate Glocal Invariants of Large (Brain-)
  Graphs</title><categories>cs.SY q-bio.QM</categories><comments>Published as part of 2013 IEEE GlobalSIP conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are quickly emerging as a leading abstraction for the representation
of data. One important application domain originates from an emerging
discipline called &quot;connectomics&quot;. Connectomics studies the brain as a graph;
vertices correspond to neurons (or collections thereof) and edges correspond to
structural or functional connections between them. To explore the variability
of connectomes---to address both basic science questions regarding the
structure of the brain, and medical health questions about psychiatry and
neurology---one can study the topological properties of these brain-graphs. We
define multivariate glocal graph invariants: these are features of the graph
that capture various local and global topological properties of the graphs. We
show that the collection of features can collectively be computed via a
combination of daisy-chaining, sparse matrix representation and computations,
and efficient approximations. Our custom open-source Python package serves as a
back-end to a Web-service that we have created to enable researchers to upload
graphs, and download the corresponding invariants in a number of different
formats. Moreover, we built this package to support distributed processing on
multicore machines. This is therefore an enabling technology for network
science, lowering the barrier of entry by providing tools to biologists and
analysts who otherwise lack these capabilities. As a demonstration, we run our
code on 120 brain-graphs, each with approximately 16M vertices and up to 90M
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4322</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4322</id><created>2013-12-16</created><authors><author><keyname>Bryson</keyname><forenames>Steve</forenames></author></authors><title>Virtual Reality: A Definition History - A Personal Essay</title><categories>cs.HC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This essay, written in 1998 by an active participant in both virtual reality
development and the virtual reality definition debate, discusses the definition
of the phrase &quot;Virtual Reality&quot; (VR). I start with history from a personal
perspective, concentrating on the debate between the &quot;Virtual Reality&quot; and
&quot;Virtual Environment&quot; labels in the late 1980's and early 1990's. Definitions
of VR based on specific technologies are shown to be unsatisfactory. I propose
the following definition of VR, based on the striking effects of a good VR
system: &quot;Virtual Reality is the use of computer technology to create the effect
of an interactive three-dimensional world in which the objects have a sense of
spatial presence.&quot; The justification for this definition is discussed in
detail, and is favorably compared with the dictionary definitions of &quot;virtual&quot;
and &quot;reality&quot;. The implications of this definition for virtual reality
technology are briefly examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4328</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4328</id><created>2013-12-16</created><authors><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author><author><keyname>Kimmig</keyname><forenames>Angelika</forenames></author></authors><title>Probabilistic Programming Concepts</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multitude of different probabilistic programming languages exists today,
all extending a traditional programming language with primitives to support
modeling of complex, structured probability distributions. Each of these
languages employs its own probabilistic primitives, and comes with a particular
syntax, semantics and inference procedure. This makes it hard to understand the
underlying programming concepts and appreciate the differences between the
different languages. To obtain a better understanding of probabilistic
programming, we identify a number of core programming concepts underlying the
primitives used by various probabilistic languages, discuss the execution
mechanisms that they require and use these to position state-of-the-art
probabilistic languages and their implementation. While doing so, we focus on
probabilistic extensions of logic programming languages such as Prolog, which
have been developed since more than 20 years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4333</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4333</id><created>2013-12-16</created><authors><author><keyname>Buliga</keyname><forenames>Marius</forenames></author><author><keyname>Kauffman</keyname><forenames>Louis H.</forenames></author></authors><title>GLC actors, artificial chemical connectomes, topological issues and
  knots</title><categories>cs.DC math.GT math.LO</categories><msc-class>68Q85, 57M25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on graphic lambda calculus, we propose a program for a new model of
asynchronous distributed computing, inspired from Hewitt Actor Model, as well
as several investigation paths, concerning how one may graft lambda calculus
and knot diagrammatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4345</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4345</id><created>2013-12-16</created><authors><author><keyname>Figueiredo</keyname><forenames>Rosa</forenames></author><author><keyname>Frota</keyname><forenames>Yuri</forenames></author></authors><title>An improved Branch-and-cut code for the maximum balanced subgraph of a
  signed graph</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Balanced Subgraph Problem (MBSP) is the problem of finding a
subgraph of a signed graph that is balanced and maximizes the cardinality of
its vertex set. We are interested in the exact solution of the problem: an
improved version of a branch-and-cut algorithm is proposed. Extensive
computational experiments are carried out on a set of instances from three
applications previously discussed in the literature as well as on a set of
random instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1312.4346</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1312.4346</id><created>2013-12-16</created><authors><author><keyname>Sato</keyname><forenames>Noritaka</forenames></author><author><keyname>Ito</keyname><forenames>Masataka</forenames></author><author><keyname>Morita</keyname><forenames>Yoshifumi</forenames></author><author><keyname>Matsuno</keyname><forenames>Fumitoshi</forenames></author></authors><title>Teleoperation System Using Past Image Records Considering Narrow
  Communication Band</title><categories>cs.RO cs.CV</categories><comments>ROSIN2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teleoperation is necessary when the robot is applied to real missions, for
example surveillance, search and rescue. We proposed teleoperation system using
past image records (SPIR). SPIR virtually generates the bird's-eye view image
by overlaying the CG model of the robot at the corresponding current position
on the background image which is captured from the camera mounted on the robot
at a past time. The problem for SPIR is that the communication bandwidth is
often narrow in some teleoperation tasks. In this case, the candidates of
background image of SPIR are few and the position of the robot is often
delayed. In this study, we propose zoom function for insufficiency of
candidates of the background image and additional interpolation lines for the
delay of the position data of the robot. To evaluate proposed system, an
outdoor experiments are carried out. The outdoor experiment is conducted on a
training course of a driving school.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="53000" completeListSize="102538">1122234|54001</resumptionToken>
</ListRecords>
</OAI-PMH>
