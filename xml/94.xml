<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:04:01Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|93001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00132</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00132</id><created>2016-02-29</created><authors><author><keyname>Hu</keyname><forenames>Zexi</forenames></author><author><keyname>Gao</keyname><forenames>Yuefang</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Tian</keyname><forenames>Xuhong</forenames></author></authors><title>A Universal Update-pacing Framework For Visual Tracking</title><categories>cs.CV</categories><comments>Submitted to ICIP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework to alleviate the model drift problem in
visual tracking, which is based on paced updates and trajectory selection.
Given a base tracker, an ensemble of trackers is generated, in which each
tracker's update behavior will be paced and then traces the target object
forward and backward to generate a pair of trajectories in an interval. Then,
we implicitly perform self-examination based on trajectory pair of each tracker
and select the most robust tracker. The proposed framework can effectively
leverage temporal context of sequential frames and avoid to learn corrupted
information. Extensive experiments on the standard benchmark suggest that the
proposed framework achieves superior performance against state-of-the-art
trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00133</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00133</id><created>2016-02-29</created><authors><author><keyname>Qian</keyname><forenames>Rongrong</forenames></author><author><keyname>Qi</keyname><forenames>Yuan</forenames></author></authors><title>Asymptotic Analysis of Random Lattices in High Dimensions</title><categories>cs.IT math.IT</categories><comments>22 pages, 2 figures</comments><msc-class>94A24, 94A13</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the asymptotic analysis of random lattices in high
dimensions to clarify the distance properties of the considered lattices. These
properties not only indicate the asymptotic value for the distance between any
pair of lattice points in high-dimension random lattices, but also describe the
convergence behavior of how the asymptotic value approaches the exact distance.
The asymptotic analysis further prompts new insights into the asymptotic
behavior of sphere-decoding complexity and the pairwise error probability (PEP)
with maximum-likelihood (ML) detector for a large number of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00145</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00145</id><created>2016-03-01</created><authors><author><keyname>Liu</keyname><forenames>Shifeng</forenames></author><author><keyname>Hu</keyname><forenames>Zheng</forenames></author><author><keyname>Dey</keyname><forenames>Sujit</forenames></author><author><keyname>Ke</keyname><forenames>Xin</forenames></author></authors><title>On Tie Strength Augmented Social Correlation for Inferring Preference of
  Mobile Telco Users</title><categories>cs.SI cs.IR cs.LG</categories><comments>10 pages,9 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For mobile telecom operators, it is critical to build preference profiles of
their customers and connected users, which can help operators make better
marketing strategies, and provide more personalized services. With the
deployment of deep packet inspection (DPI) in telecom networks, it is possible
for the telco operators to obtain user online preference. However, DPI has its
limitations and user preference derived only from DPI faces sparsity and cold
start problems. To better infer the user preference, social correlation in
telco users network derived from Call Detailed Records (CDRs) with regard to
online preference is investigated. Though widely verified in several online
social networks, social correlation between online preference of users in
mobile telco networks, where the CDRs derived relationship are of less social
properties and user mobile internet surfing activities are not visible to
neighbourhood, has not been explored at a large scale. Based on a real world
telecom dataset including CDRs and preference of more than $550K$ users for
several months, we verified that correlation does exist between online
preference in such \textit{ambiguous} social network. Furthermore, we found
that the stronger ties that users build, the more similarity between their
preference may have. After defining the preference inferring task as a Top-$K$
recommendation problem, we incorporated Matrix Factorization Collaborative
Filtering model with social correlation and tie strength based on call patterns
to generate Top-$K$ preferred categories for users. The proposed Tie Strength
Augmented Social Recommendation (TSASoRec) model takes data sparsity and cold
start user problems into account, considering both the recorded and missing
recorded category entries. The experiment on real dataset shows the proposed
model can better infer user preference, especially for cold start users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00146</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00146</id><created>2016-03-01</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Wistar</keyname><forenames>Stephen</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author><author><keyname>Steinberg</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>James Z.</forenames></author></authors><title>Storm Detection by Visual Learning Using Satellite Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computers are widely utilized in today's weather forecasting as a powerful
tool to leverage an enormous amount of data. Yet, despite the availability of
such data, current techniques often fall short of producing reliable detailed
storm forecasts. Each year severe thunderstorms cause significant damage and
loss of life, some of which could be avoided if better forecasts were
available. We propose a computer algorithm that analyzes satellite images from
historical archives to locate visual signatures of severe thunderstorms for
short-term predictions. While computers are involved in weather forecasts to
solve numerical models based on sensory data, they are less competent in
forecasting based on visual patterns from satellite images. In our system, we
extract and summarize important visual storm evidence from satellite image
sequences in the way that meteorologists interpret the images. In particular,
the algorithm extracts and fits local cloud motion from image sequences to
model the storm-related cloud patches. Image data from the year 2008 have been
adopted to train the model, and historical thunderstorm reports in continental
US from 2000 through 2013 have been used as the ground-truth and priors in the
modeling process. Experiments demonstrate the usefulness and potential of the
algorithm for producing more accurate thunderstorm forecasts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00149</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00149</id><created>2016-03-01</created><authors><author><keyname>Mhanna</keyname><forenames>Sleiman</forenames></author><author><keyname>Chapman</keyname><forenames>Archie</forenames></author><author><keyname>Verbic</keyname><forenames>Gregor</forenames></author></authors><title>A Fast Distributed Algorithm for Large-Scale Demand Response Aggregation</title><categories>cs.DC math.OC</categories><comments>Accepted in IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge to implementing residential demand response is that of
aligning the objectives of many households, each of which aims to minimize its
payments and maximize its comfort level, while balancing this with the
objectives of an aggregator that aims to minimize the cost of electricity
purchased in a pooled wholesale market. This paper presents a fast distributed
algorithm for aggregating a large number of households with a mixture of
discrete and continuous energy levels. A distinctive feature of the method in
this paper is that the nonconvex DR problem is decomposed in terms of
households as opposed to devices, which allows incorporating more intricate
couplings between energy storage devices, appliances and distributed energy
resources. The proposed method is a fast distributed algorithm applied to the
double smoothed dual function of the adopted DR model. The method is tested on
systems with up to 2560 households, each with 10 devices on average. The
proposed algorithm is designed to terminate in 60 iterations irrespective of
system size, which can be ideal for an on-line version of this problem.
Moreover, numerical results show that with minimal parameter tuning, the
algorithm exhibits a very similar convergence behavior throughout the studied
systems and converges to near-optimal solutions, which corroborates its
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00150</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00150</id><created>2016-03-01</created><authors><author><keyname>Campbell</keyname><forenames>Dylan</forenames></author><author><keyname>Petersson</keyname><forenames>Lars</forenames></author></authors><title>GOGMA: Globally-Optimal Gaussian Mixture Alignment</title><categories>cs.CV cs.RO</categories><comments>Manuscript in press 2016 IEEE Conference on Computer Vision and
  Pattern Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian mixture alignment is a family of approaches that are frequently used
for robustly solving the point-set registration problem. However, since they
use local optimisation, they are susceptible to local minima and can only
guarantee local optimality. Consequently, their accuracy is strongly dependent
on the quality of the initialisation. This paper presents the first
globally-optimal solution to the 3D rigid Gaussian mixture alignment problem
under the L2 distance between mixtures. The algorithm, named GOGMA, employs a
branch-and-bound approach to search the space of 3D rigid motions SE(3),
guaranteeing global optimality regardless of the initialisation. The geometry
of SE(3) was used to find novel upper and lower bounds for the objective
function and local optimisation was integrated into the scheme to accelerate
convergence without voiding the optimality guarantee. The evaluation
empirically supported the optimality proof and showed that the method performed
much more robustly on two challenging datasets than an existing
globally-optimal registration solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00154</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00154</id><created>2016-03-01</created><authors><author><keyname>Hu</keyname><forenames>Ping</forenames></author><author><keyname>Sung</keyname><forenames>Chi Wan</forenames></author><author><keyname>Chan</keyname><forenames>Terence H.</forenames></author></authors><title>Broadcast Repair for Wireless Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, in Proc. International Conference on Information,
  Communications and Signal Processing (ICICS), Singapore, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless distributed storage systems, storage nodes are connected by
wireless channels, which are broadcast in nature. This paper exploits this
unique feature to design an efficient repair mechanism, called broadcast
repair, for wireless distributed storage systems with multiple-node failures.
Since wireless channels are typically bandwidth limited, we advocate a new
measure on repair performance called repair-transmission bandwidth, which
measures the average number of packets transmitted by helper nodes per failed
node. The fundamental tradeoff between storage amount and repair-transmission
bandwidth is obtained. It is shown that broadcast repair outperforms
cooperative repair, which is the basic repair method for wired distributed
storage systems with multiple-node failures, in terms of storage efficiency and
repair-transmission bandwidth, thus yielding a better tradeoff curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00160</identifier>
 <datestamp>2016-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00160</id><created>2016-03-01</created><authors><author><keyname>Al-Abbasi</keyname><forenames>Abubakr O.</forenames></author><author><keyname>Hamila</keyname><forenames>Ridha</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Al-Dhahir</keyname><forenames>Naofal</forenames></author></authors><title>Design and Analysis Framework for Sparse FIR Channel Shortening</title><categories>cs.IT math.IT</categories><comments>7 pages, 7 pages, ICC 2016. arXiv admin note: text overlap with
  arXiv:1511.06971</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major performance and complexity limitation in broadband communications is
the long channel delay spread which results in a highly-frequency-selective
channel frequency response. Channel shortening equalizers (CSEs) are used to
ensure that the cascade of a long channel impulse response (CIR) and the CSE is
approximately equivalent to a target impulse response (TIR) with much shorter
delay spread. In this paper, we propose a general framework that transforms the
problems of design of sparse CSE and TIR finite impulse response (FIR) filters
into the problem of sparsest-approximation of a vector in different
dictionaries. In addition, we compare several choices of sparsifying
dictionaries under this framework. Furthermore, the worst-case coherence of
these dictionaries, which determines their sparsifying effectiveness, are
analytically and/or numerically evaluated. Finally, the usefulness of the
proposed framework for the design of sparse CSE and TIR filters is validated
through numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00162</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00162</id><created>2016-03-01</created><authors><author><keyname>Cohen</keyname><forenames>Nadav</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>Convolutional Rectifier Networks as Generalized Tensor Decompositions</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional rectifier networks, i.e. convolutional neural networks with
rectified linear activation and max or average pooling, are the cornerstone of
modern deep learning. However, despite their wide use and success, our
theoretical understanding of the expressive properties that drive these
networks is partial at best. On other hand, we have a much firmer grasp of
these issues in the world of arithmetic circuits. Specifically, it is known
that convolutional arithmetic circuits posses the property of &quot;complete depth
efficiency&quot;, meaning that besides a negligible set, all functions that can be
implemented by a deep network of polynomial size, require exponential size in
order to be implemented (or even approximated) by a shallow network. In this
paper we describe a construction based on generalized tensor decompositions,
that transforms convolutional arithmetic circuits into convolutional rectifier
networks. We then use mathematical tools available from the world of arithmetic
circuits to prove new results. First, we show that convolutional rectifier
networks are universal with max pooling but not with average pooling. Second,
and more importantly, we show that depth efficiency is weaker with
convolutional rectifier networks than it is with convolutional arithmetic
circuits. This leads us to believe that developing effective methods for
training convolutional arithmetic circuits, thereby fulfilling their expressive
potential, may give rise to a deep learning architecture that is provably
superior to convolutional rectifier networks but has so far been overlooked by
practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00169</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00169</id><created>2016-03-01</created><authors><author><keyname>Ren</keyname><forenames>Hong</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author><author><keyname>Pan</keyname><forenames>Cunhua</forenames></author></authors><title>Energy Efficient Transmission for Multicast Services in MISO Distributed
  Antenna Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to solve the energy efficiency (EE) maximization problem for
multicast services in a multiple-input single-output (MISO) distributed antenna
system (DAS). A novel iterative algorithm is proposed, which consists of
solving two subproblems iteratively: the power allocation problem and the beam
direction updating problem. The former subproblem can be equivalently
transformed into a one-dimension quasi-concave problem that is solved by the
golden search method. The latter problem can be efficiently solved by the
existing method. Simulation results show that the proposed algorithm achieves
significant EE performance gains over the existing rate maximization method. In
addition, when the backhaul power consumption is low, the EE performance of the
DAS is better than that of the centralized antenna system (CAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00175</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00175</id><created>2016-03-01</created><authors><author><keyname>Itoh</keyname><forenames>Shoji</forenames></author><author><keyname>Sugihara</keyname><forenames>Masaaki</forenames></author></authors><title>The structure of the polynomials in preconditioned BiCG algorithms and
  the switching direction of preconditioned systems</title><categories>math.NA cs.NA</categories><msc-class>15A06, 65F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a theorem that defines the direction of a preconditioned system
for the BiCG method, and we extend it to preconditioned bi-Lanczos-type
algorithms. We show that the direction of a preconditioned system is switched
by construction and by the settings of the initial shadow residual vector. We
analyze and compare the polynomial structures of four preconditioned BiCG
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00176</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00176</id><created>2016-03-01</created><authors><author><keyname>Itoh</keyname><forenames>Shoji</forenames></author><author><keyname>Sugihara</keyname><forenames>Masaaki</forenames></author></authors><title>Analysis of the structure of the Krylov subspace in various
  preconditioned CGS algorithms</title><categories>math.NA cs.NA</categories><msc-class>15A06, 65F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An improved preconditioned CGS (PCGS) algorithm has recently been proposed,
and it performs much better than the conventional PCGS algorithm. In this
paper, the improved PCGS algorithm is verified as a coordinative to the
left-preconditioned system; this is done by comparing, analyzing, and executing
numerical examinations of various PCGS algorithms, including the most recently
proposed one. We show that the direction of the preconditioned system for the
CGS method is determined by the operations of $\alpha_k$ and $\beta_k$ in the
PCGS algorithm. By comparing the logical structures of these algorithms, we
show that the direction can be switched by the construction and setting of the
initial shadow residual vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00182</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00182</id><created>2016-03-01</created><authors><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author></authors><title>Protecting suppliers' private information: the case of stock levels and
  the impact of correlated items</title><categories>cs.CR</categories><comments>13 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1509.06524</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A marketplace is defined where the private data of suppliers (e.g.,
prosumers) are protected, so that neither their identity nor their level of
stock is made known to end customers, while they can sell their products at a
reduced price. A broker acts as an intermediary, which takes care of providing
the items missing to meet the customers' demand and allows end customers to
take advantages of reduced prices through the subscription of option contracts.
Formulas are provided for the option price under three different probability
models for the availability of items. Option pricing allows the broker to
partially transfer its risk on end customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00203</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00203</id><created>2016-03-01</created><authors><author><keyname>Kariminezhad</keyname><forenames>Ali</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Harvesting the (Self-)Interference in Heterogeneous Full-Duplex Networks
  For Joint Rate-Energy Optimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless nodes in future communication systems are expected to overcome three
barriers when compared to their transitional counterparts, namely to support
significantly higher data rates, have long-lasting energy supplies and remain
fully operational in interference-limited heterogeneous networks. This could be
achieved by providing three promising features, which are radio frequency (RF)
energy harvesting, improper Gaussian signaling and operating in full-duplex
communication mode, i.e., transmit and receive at the same time within the same
frequency band. In this paper, we consider these aspects jointly in a
multi-antenna heterogeneous two-tier-network. Thus, the users in the femto-cell
are sharing the scarce resources with the cellular users in the macro-cell and
have to cope with the interference from the macro-cell base station as well as
the transmitter noise and residual self-interference (RSI) due to imperfect
full-duplex operation. Interestingly enough, while these impairments are
detrimental from the achievable rate perspective, they are beneficial from the
energy harvesting aspect as they carry RF energy. In this paper, we consider
this natural trade-off jointly and propose appropriate optimization problems
for beamforming and optimal resource allocation. Various receiver structures
are employed for both information detection (ID) and energy harvesting (EH)
capabilities. The paper aims at characterizing the trade-off between the
achievable rates and harvested energies. Rate and energy maximization problems
are thoroughly investigated. Finally, the numerical illustrations demonstrate
the impact of the energy harvesting on the achievable rate performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00210</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00210</id><created>2016-03-01</created><authors><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Liu</keyname><forenames>Jieyuan</forenames></author><author><keyname>Kong</keyname><forenames>Youyong</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Phase-only signal reconstruction by MagnitudeCut</title><categories>cs.NA cs.IT math.IT</categories><comments>10 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new algorithm, called MagnitudeCut, for
recovering a signal from the phase of its Fourier transform. We casted our
recovering problem into a new convex optimization problem, and then solved it
by the block coordinate descent algorithm and the interior point algorithm, in
which the iteration process consists of matrix vector product and inner
product. We used the new method for reconstruction of a set of signal/image.
The simulation results reveal that the proposed MagnitudeCut method can
reconstruct the original signal with fewer sampling number of the phase
information than that of the Greedy algorithm and iterative method under the
same reconstruction error. Moreover, our algorithm can also reconstruct the
symmetric image from its Fourier phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00213</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00213</id><created>2016-03-01</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>An Optimal Algorithm for l1-Heavy Hitters in Insertion Streams and
  Related Problems</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first optimal bounds for returning the $\ell_1$-heavy hitters in
a data stream of insertions, together with their approximate frequencies,
closing a long line of work on this problem. For a stream of $m$ items in $\{1,
2, \dots, n\}$ and parameters $0 &lt; \epsilon &lt; \phi \leq 1$, let $f_i$ denote
the frequency of item $i$, i.e., the number of times item $i$ occurs in the
stream. With arbitrarily large constant probability, our algorithm returns all
items $i$ for which $f_i \geq \phi m$, returns no items $j$ for which $f_j \leq
(\phi -\epsilon)m$, and returns approximations $\tilde{f}_i$ with $|\tilde{f}_i
- f_i| \leq \epsilon m$ for each item $i$ that it returns. Our algorithm uses
$O(\epsilon^{-1} \log\phi^{-1} + \phi^{-1} \log n + \log \log m)$ bits of
space, processes each stream update in $O(1)$ worst-case time, and can report
its output in time linear in the output size. We also prove a lower bound,
which implies that our algorithm is optimal up to a constant factor in its
space complexity. A modification of our algorithm can be used to estimate the
maximum frequency up to an additive $\epsilon m$ error in the above amount of
space, resolving Question 3 in the IITK 2006 Workshop on Algorithms for Data
Streams for the case of $\ell_1$-heavy hitters. We also introduce several
variants of the heavy hitters and maximum frequency problems, inspired by rank
aggregation and voting schemes, and show how our techniques can be applied in
such settings. Unlike the traditional heavy hitters problem, some of these
variants look at comparisons between items rather than numerical values to
determine the frequency of an item.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00223</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00223</id><created>2016-03-01</created><authors><author><keyname>Lu</keyname><forenames>Liang</forenames></author><author><keyname>Kong</keyname><forenames>Lingpeng</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Segmental Recurrent Neural Networks for End-to-end Speech Recognition</title><categories>cs.CL cs.LG cs.NE</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the segmental recurrent neural network for end-to-end acoustic
modelling. This model connects the segmental conditional random field (CRF)
with a recurrent neural network (RNN) used for feature extraction. Compared to
most previous CRF-based acoustic models, it does not rely on an external system
to provide features or segmentation boundaries. Instead, this model
marginalises out all the possible segmentations, and features are extracted
from the RNN trained together with the segmental CRF. In essence, this model is
self-contained and can be trained end-to-end. In this paper, we discuss
practical training and decoding issues as well as the method to speed up the
training in the context of speech recognition. We performed experiments on the
TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass
decoding --- the best reported result using CRFs, despite the fact that we only
used a zeroth-order CRF and without using any language model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00226</identifier>
 <datestamp>2016-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00226</id><created>2016-03-01</created><authors><author><keyname>Meinhardt</keyname><forenames>Holger Ingmar</forenames></author></authors><title>Finding the Nucleoli of Large Cooperative Games: A Disproof with
  Counter-Example</title><categories>cs.GT</categories><comments>5 pages, 1 figure, 1 table. arXiv admin note: text overlap with
  arXiv:1509.05883</comments><msc-class>03B05, 91A12, 91B24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nguyen and Thomas (2016) claimed that they have found a method to compute the
nucleoli of games with more than $50$ players using nested linear programs
(LP). Unfortunately, this claim is false. They incorrectly applied the indirect
proof by &quot;$A \land \neg B$ implies $A \land \neg A$&quot; to conclude that &quot;if $A$
then $B$&quot; is valid. In fact, they prove that a truth implies a falsehood. As
established by Meinhardt (2015a), this is a wrong statement. Therefore, instead
of giving a proof of their main Theorem 4b, they give a disproof. It comes as
no surprise to us that the flow game example presented by these authors to
support their arguments is obviously a counter-example of their algorithm. We
show that the computed solution by this algorithm is neither the nucleolus nor
a core element of the flow game. Moreover, the stopping criterion of all
proposed methods is wrong, since it does not satisfy one of Kohlberg's
properties (cf. Kohlberg (1971)). As a consequence, none of these algorithms is
robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00229</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00229</id><created>2016-03-01</created><updated>2016-03-02</updated><authors><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Abbar</keyname><forenames>Sofiane</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author></authors><title>Fetishizing Food in Digital Age: #foodporn Around the World</title><categories>cs.SI</categories><comments>International AAAI Conference on Web and Social Media (ICWSM), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What food is so good as to be considered pornographic? Worldwide, the popular
#foodporn hashtag has been used to share appetizing pictures of peoples'
favorite culinary experiences. But social scientists ask whether #foodporn
promotes an unhealthy relationship with food, as pornography would contribute
to an unrealistic view of sexuality. In this study, we examine nearly 10
million Instagram posts by 1.7 million users worldwide. An overwhelming (and
uniform across the nations) obsession with chocolate and cake shows the
domination of sugary dessert over local cuisines. Yet, we find encouraging
traits in the association of emotion and health-related topics with #foodporn,
suggesting food can serve as motivation for a healthy lifestyle. Social
approval also favors the healthy posts, with users posting with healthy
hashtags having an average of 1,000 more followers than those with unhealthy
ones. Finally, we perform a demographic analysis which shows nation-wide trends
of behavior, such as a strong relationship (r=0.51) between the GDP per capita
and the attention to healthiness of their favorite food. Our results expose a
new facet of food &quot;pornography&quot;, revealing potential avenues for utilizing this
precarious notion for promoting healthy lifestyles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00244</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00244</id><created>2016-03-01</created><authors><author><keyname>Chen</keyname><forenames>Zhuoqun</forenames></author><author><keyname>Liu</keyname><forenames>Yangyang</forenames></author><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>Caching Incentive Design in Wireless D2D Networks: A Stackelberg Game
  Approach</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>Accepted to IEEE International Conference on Communications (ICC)
  2016, Kuala Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching in wireless device-to-device (D2D) networks can be utilized to
offload data traffic during peak times. However, the design of incentive
mechanisms is challenging due to the heterogeneous preference and selfish
nature of user terminals (UTs). In this paper, we propose an incentive
mechanism in which the base station (BS) rewards those UTs that share contents
with others using D2D communication. We study the cost minimization problem for
the BS and the utility maximization problem for each UT. In particular, the BS
determines the rewarding policy to minimize his total cost, while each UT aims
to maximize his utility by choosing his caching policy. We formulate the
conflict among UTs and the tension between the BS and the UTs as a Stackelberg
game. We show the existence of the equilibrium and propose an iterative
gradient algorithm (IGA) to obtain the Stackelberg Equilibrium. Extensive
simulations are carried out to evaluate the performance of the proposed caching
scheme and comparisons are drawn with several baseline caching schemes with no
incentives. Numerical results show that the caching scheme under our incentive
mechanism outperforms other schemes in terms of the BS serving cost and the
utilities of the UTs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00254</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00254</id><created>2016-03-01</created><authors><author><keyname>Jel&#xed;nek</keyname><forenames>V&#xed;t</forenames></author><author><keyname>Jel&#xed;nkov&#xe1;</keyname><forenames>Eva</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author></authors><title>On the hardness of switching to a small number of edges</title><categories>cs.CC cs.DM</categories><comments>19 pages, 7 figures. An extended abstract submitted to COCOON 2016</comments><msc-class>05C76</msc-class><acm-class>G.2.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seidel's switching is a graph operation which makes a given vertex adjacent
to precisely those vertices to which it was non-adjacent before, while keeping
the rest of the graph unchanged. Two graphs are called switching-equivalent if
one can be made isomorphic to the other one by a sequence of switches.
  Jel\'inkov\'a et al. [DMTCS 13, no. 2, 2011] presented a proof that it is
NP-complete to decide if the input graph can be switched to contain at most a
given number of edges. There turns out to be a flaw in their proof. We present
a correct proof.
  Furthermore, we prove that the problem remains NP-complete even when
restricted to graphs whose density is bounded from above by an arbitrary fixed
constant. This partially answers a question of Matou\v{s}ek and Wagner
[Discrete Comput. Geom. 52, no. 1, 2014].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00260</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00260</id><created>2016-03-01</created><authors><author><keyname>Gupta</keyname><forenames>Dhruv</forenames></author></authors><title>Event Search and Analytics: Detecting Events in Semantically Annotated
  Corpora for Search and Analytics</title><categories>cs.IR cs.CL</categories><comments>Extended research report of an extended abstract published at WSDM
  2016 Doctoral Consortium. in WSDM 2016 Proceedings of the Ninth ACM
  International Conference on Web Search and Data Mining</comments><doi>10.1145/2835776.2855083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, I present the questions that I seek to answer in my PhD
research. I posit to analyze natural language text with the help of semantic
annotations and mine important events for navigating large text corpora.
Semantic annotations such as named entities, geographic locations, and temporal
expressions can help us mine events from the given corpora. These events thus
provide us with useful means to discover the locked knowledge in them. I pose
three problems that can help unlock this knowledge vault in semantically
annotated text corpora: i. identifying important events; ii. semantic search;
and iii. event analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00273</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00273</id><created>2016-03-01</created><authors><author><keyname>Yankelevsky</keyname><forenames>Yael</forenames></author><author><keyname>Friedman</keyname><forenames>Zvi</forenames></author><author><keyname>Feuer</keyname><forenames>Arie</forenames></author></authors><title>Component Based Modeling of Ultrasound Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a component based model for the raw ultrasound signals
acquired by the transducer elements. Based on this approach, before undergoing
the standard digital processing chain, every sampled raw signal is first
decomposed into a smooth background signal and a strong reflectors component.
The decomposition allows for a suited processing scheme to be adjusted for each
component individually. We demonstrate the potential benefit of this approach
in image enhancement by suppressing side lobe artifacts, and in improvement of
digital data compression. Applying our proposed processing schemes to real
cardiac ultrasound data, we show that by separating the two components and
compressing them individually, over twenty-fold reduction of the data size is
achieved while retaining the image contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00275</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00275</id><created>2016-03-01</created><authors><author><keyname>Sirinukunwattana</keyname><forenames>Korsuk</forenames></author><author><keyname>Pluim</keyname><forenames>Josien P. W.</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Qi</keyname><forenames>Xiaojuan</forenames></author><author><keyname>Heng</keyname><forenames>Pheng-Ann</forenames></author><author><keyname>Guo</keyname><forenames>Yun Bo</forenames></author><author><keyname>Wang</keyname><forenames>Li Yang</forenames></author><author><keyname>Matuszewski</keyname><forenames>Bogdan J.</forenames></author><author><keyname>Bruni</keyname><forenames>Elia</forenames></author><author><keyname>Sanchez</keyname><forenames>Urko</forenames></author><author><keyname>B&#xf6;hm</keyname><forenames>Anton</forenames></author><author><keyname>Ronneberger</keyname><forenames>Olaf</forenames></author><author><keyname>Cheikh</keyname><forenames>Bassem Ben</forenames></author><author><keyname>Racoceanu</keyname><forenames>Daniel</forenames></author><author><keyname>Kainz</keyname><forenames>Philipp</forenames></author><author><keyname>Pfeiffer</keyname><forenames>Michael</forenames></author><author><keyname>Urschler</keyname><forenames>Martin</forenames></author><author><keyname>Snead</keyname><forenames>David R. J.</forenames></author><author><keyname>Rajpoot</keyname><forenames>Nasir M.</forenames></author></authors><title>Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Colorectal adenocarcinoma originating in intestinal glandular structures is
the most common form of colon cancer. In clinical practice, the morphology of
intestinal glands, including architectural appearance and glandular formation,
is used by pathologists to inform prognosis and plan the treatment of
individual patients. However, achieving good inter-observer as well as
intra-observer reproducibility of cancer grading is still a major challenge in
modern pathology. An automated approach which quantifies the morphology of
glands is a solution to the problem. This paper provides an overview to the
Gland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at
MICCAI'2015. Details of the challenge, including organization, dataset and
evaluation criteria, are presented, along with the method descriptions and
evaluation results from the top performing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00284</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00284</id><created>2016-03-01</created><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Becker</keyname><forenames>Stephen</forenames></author></authors><title>Dual Smoothing and Level Set Techniques for Variational Matrix
  Decomposition</title><categories>stat.ML cs.CV math.OC</categories><comments>38 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1406.1089</comments><msc-class>90C06, 81P50, 65K10, 62F35, 47N30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the robust principal component analysis (RPCA) problem, and
review a range of old and new convex formulations for the problem and its
variants. We then review dual smoothing and level set techniques in convex
optimization, present several novel theoretical results, and apply the
techniques on the RPCA problem. In the final sections, we show a range of
numerical experiments for simulated and real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00286</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00286</id><created>2016-03-01</created><authors><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author></authors><title>How to Re-Divide a Cake Fairly</title><categories>cs.GT</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classic problem of fairly dividing a heterogeneous resource
(&quot;cake&quot;) among several agents with different preferences. In our setting, the
resource is already divided among several &quot;old&quot; agents. When &quot;young&quot; agents
come, the resource should be re-divided in a way that is fair both for the
young and for the old agents. Motivated by land redivision and other
two-dimensional division problems, we assume that the cake is a rectilinear
polygon and require that the allotted pieces be rectangles. Our re-division
protocol has an implication on another problem: the price-of-fairness - the
loss of social welfare caused by fairness requirements. Our protocol allows us
to prove upper bounds on the price-of-fairness in two-dimensional cake-cutting
with rectangular pieces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00293</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00293</id><created>2016-03-01</created><authors><author><keyname>Matter</keyname><forenames>Ulrich</forenames></author></authors><title>RWebData: A High-Level Interface to the Programmable Web</title><categories>stat.CO cs.MS cs.SE</categories><comments>Working Paper. Keywords: R, programmable web, big public data, web
  api, rest</comments><acm-class>K.8.1; G.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of the programmable web offers new opportunities for the empirically
driven social sciences. The access, compilation and preparation of data from
the programmable web for statistical analysis can, however, involve substantial
up-front costs for the practical researcher. The R-package RWebData provides a
high-level framework that allows data to be easily collected from the
programmable web in a format that can directly be used for statistical analysis
in R (R Core Team 2013) without bothering about the data's initial format and
nesting structure. It was developed specifically for users who have no
experience with web technologies and merely use R as a statistical software.
The core idea and methodological contribution of the package are the
disentangling of parsing web data and mapping them with a generic algorithm
(independent of the initial data structure) to a flat table-like
representation. This paper provides an overview of the high-level functions for
R-users, explains the basic architecture of the package, and illustrates the
implemented data mapping algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00295</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00295</id><created>2016-03-01</created><authors><author><keyname>Divan</keyname><forenames>Stefano</forenames></author><author><keyname>Fontanelli</keyname><forenames>Daniele</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>Hybrid Feedback Path Following for Robotic Walkers via Bang-Bang Control
  Actions</title><categories>cs.RO</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a control algorithm to guide a robotic walking assistant along a
planned path. The control strategy exploits the electromechanical brakes
mounted on the back wheels of the walker. In order to reduce the hardware
requirements we adopt a Bang Bang approach relying of four actions (with
saturated value for the braking torques).When the platform is far away from the
path, we execute an approach phase in which the walker converges toward the
platform with a specified angle. When it comes in proximity of the platform,
the control strategy switches to a path tracking mode, which uses the four
control actions to converge toward the path with an angle which is a function
of the state. This way it is possible to control the vehicle in feedback,
secure a gentle convergence of the user to the planned path and her steady
progress towards the destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00300</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00300</id><created>2016-02-26</created><authors><author><keyname>Yaliniz</keyname><forenames>R. Irem Bor</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author></authors><title>Efficient 3-D Placement of an Aerial Base Station in Next Generation
  Cellular Networks</title><categories>math.OC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agility and resilience requirements of future cellular networks may not be
fully satisfied by terrestrial base stations in cases of unexpected or
temporary events. A promising solution is assisting the cellular network via
low-altitude unmanned aerial vehicles equipped with base stations, i.e.,
drone-cells. Although drone-cells provide a quick deployment opportunity as
aerial base stations, efficient placement becomes one of the key issues. In
addition to mobility of the drone-cells in the vertical dimension as well as
the horizontal dimension, the differences between the air-to-ground and
terrestrial channels cause the placement of the drone-cells to diverge from
placement of terrestrial base stations. In this paper, we first highlight the
properties of the dronecell placement problem, and formulate it as a 3-D
placement problem with the objective of maximizing the revenue of the network.
After some mathematical manipulations, we formulate an equivalent
quadratically-constrained mixed integer non-linear optimization problem and
propose a computationally efficient numerical solution for this problem. We
verify our analytical derivations with numerical simulations and enrich them
with discussions which could serve as guidelines for researchers, mobile
network operators, and policy makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00302</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00302</id><created>2016-03-01</created><authors><author><keyname>Ding</keyname><forenames>Z.</forenames></author><author><keyname>Dai</keyname><forenames>L.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>MIMO-NOMA Design for Small Packet Transmission in the Internet of Things</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A feature of the Internet of Things (IoT) is that some users in the system
need to be served quickly for small packet transmission. To address this
requirement, a new multiple-input multiple-output non-orthogonal multiple
access (MIMO-NOMA) scheme is designed in this paper, where one user is served
with its quality of service (QoS) requirement strictly met, and the other user
is served opportunistically by using the NOMA concept. The novelty of this new
scheme is that it confronts the challenge that most existing MIMO-NOMA schemes
rely on the assumption that users' channel conditions are different, a strong
assumption which may not be valid in practice. The developed precoding and
detection strategies can effectively create a significant difference between
the users' effective channel gains, and therefore the potential of NOMA can be
realized even if the users' original channel conditions are similar. Analytical
and numerical results are provided to demonstrate the performance of the
proposed MIMO-NOMA scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00307</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00307</id><created>2016-03-01</created><authors><author><keyname>Corrodi</keyname><forenames>Claudio</forenames></author><author><keyname>Heu&#xdf;ner</keyname><forenames>Alexander</forenames></author><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author></authors><title>A Graph-Based Semantics Workbench for Concurrent Asynchronous Programs</title><categories>cs.SE cs.DC cs.LO cs.PL</categories><comments>Accepted for publication in the proceedings of FASE 2016 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of novel programming languages and libraries have been proposed that
offer simpler-to-use models of concurrency than threads. It is challenging,
however, to devise execution models that successfully realise their
abstractions without forfeiting performance or introducing unintended
behaviours. This is exemplified by SCOOP---a concurrent object-oriented
message-passing language---which has seen multiple semantics proposed and
implemented over its evolution. We propose a &quot;semantics workbench&quot; with fully
and semi-automatic tools for SCOOP, that can be used to analyse and compare
programs with respect to different execution models. We demonstrate its use in
checking the consistency of semantics by applying it to a set of representative
programs, and highlighting a deadlock-related discrepancy between the principal
execution models of the language. Our workbench is based on a modular and
parameterisable graph transformation semantics implemented in the GROOVE tool.
We discuss how graph transformations are leveraged to atomically model
intricate language abstractions, and how the visual yet algebraic nature of the
model can be used to ascertain soundness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00312</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00312</id><created>2016-03-01</created><authors><author><keyname>Axenovich</keyname><forenames>Maria</forenames></author><author><keyname>Rollin</keyname><forenames>Jonathan</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Chromatic number of ordered graphs with forbidden ordered subgraphs</title><categories>math.CO cs.DM</categories><comments>21 pages, 9 figures</comments><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the graphs not containing a given graph H as a subgraph
have bounded chromatic number if and only if H is acyclic. Here we consider
ordered graphs, i.e., graphs with a linear ordering on their vertex set, and
the function f(H) = sup{chi(G) | G in Forb(H)} where Forb(H) denotes the set of
all ordered graphs that do not contain a copy of H. If H contains a cycle, then
as in the case of unordered graphs, f(H) is infinity. However, in contrast to
the unordered graphs, we describe an infinite family of ordered forests H with
infinite f(H). An ordered graph is crossing if there are two edges uv and u'v'
with u &lt; u' &lt; v &lt; v'. For connected crossing ordered graphs H we reduce the
problem of determining whether f(H) is finite to a family of so-called
monotonically alternating trees. For non-crossing H we prove that f(H) is
finite if and only if H is acyclic and does not contain a copy of any of the
five special ordered forests on four or five vertices, which we call bonnets.
For such forests H, we show that f(H) &lt;= 2^|V(H)| and that f(H) &lt;= 2|V(H)|-3 if
H is connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00322</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00322</id><created>2016-03-01</created><authors><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author><author><keyname>Fiore</keyname><forenames>Davide</forenames></author><author><keyname>Russo</keyname><forenames>Giovanni</forenames></author></authors><title>On synchronization and consensus patterns in complex networks: from
  analysis to control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, the study of the mechanisms to achieve consensus and
synchronization in complex networks has attracted the interest of the
Scientific Community. Most of the results on synchronization and consensus are
obtained under the assumption that some form cooperation occurs between the
nodes. Unfortunately, this assumption is not satisfied in many instances of
systems from Nature and Technology. For example, biochemical and social
networks are often characterized by some form of antagonism between nodes. The
same happens for complex networked control systems where agents need to
optimize conflicting utility functions. In this paper, we present new
conditions for the onset of synchronization and consensus patterns in complex
networks. Essentially, we show that if network nodes exhibit some symmetry,
then this can be potentially translated into a synchronization/consensus
pattern, where two groups of nodes emerge. Interestingly, the symmetry at the
node level is also reflected at the network level as the trajectories of the
two groups are related with each other via this symmetry. We also show that our
results can be turned into a design tool and offer a systematic methodology to
address the problem of designing a network controller that drives the network
towards a desired synchronization/consensus pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00329</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00329</id><created>2016-03-01</created><authors><author><keyname>Freixas</keyname><forenames>Josep</forenames></author><author><keyname>Freixas</keyname><forenames>Marc</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Characterization of threshold functions: state of the art, some new
  contributions and open problems</title><categories>cs.GT</categories><comments>23 pages, 5 tables</comments><msc-class>91A12, 06E30, 94C10, 68T27, 92B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the characterization of threshold functions within the class of
switching functions is an important problem that goes back at least to the
mid--20th century. Due to different motivations switching and threshold
functions have been investigated in a variety of different mathematical
contexts: Boolean or switching functions, neural networks, hypergraphs,
coherent structures, Sperner families, clutters, secret sharing and simple
games or binary voting systems.
  The paper revises the state of the art about this significant problem and
proposes some new contributions concerning asummability and invariant
asummability, a refinement of asummability. It also includes several questions
and conjectures for future research whose solution would mean a new
breakthrough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00361</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00361</id><created>2016-03-01</created><authors><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Piecewise Testable Languages and Nondeterministic Automata</title><categories>cs.FL</categories><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A regular language is $k$-piecewise testable if it is a finite boolean
combination of languages of the form $\Sigma^* a_1 \Sigma^* \cdots \Sigma^* a_n
\Sigma^*$, where $a_i\in\Sigma$ and $0\le n \le k$. Given a DFA $A$ and $k\ge
0$, it is an NL-complete problem to decide whether the language $L(A)$ is
piecewise testable and, for $k\ge 4$, it is coNP-complete to decide whether the
language $L(A)$ is $k$-piecewise testable. It is known that the depth of the
minimal DFA serves as an upper bound on $k$. Namely, if $L(A)$ is piecewise
testable, then it is $k$-piecewise testable for $k$ equal to the depth of $A$.
In this paper, we show that some form of nondeterminism does not violate this
upper bound result. Specifically, we define a class of NFAs, called ptNFAs,
that recognize piecewise testable languages and show that the depth of a ptNFA
provides an (up to exponentially better) upper bound on $k$ than the minimal
DFA. We provide an application of our result, discuss the relationship between
$k$-piecewise testability and the depth of NFAs, and study the complexity of
$k$-piecewise testability for ptNFAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00370</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00370</id><created>2016-03-01</created><authors><author><keyname>Jose</keyname><forenames>Cijo</forenames></author><author><keyname>Fleuret</keyname><forenames>Francois</forenames></author></authors><title>Scalable Metric Learning via Weighted Approximate Rank Component
  Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to learn a Mahalanobis distance by minimizing a loss defined on
the weighted sum of the precision at different ranks. Our core motivation is
that minimizing a weighted rank loss is a natural criterion for many problems
in computer vision such as person re-identification. We propose a novel metric
learning formulation called Weighted Approximate Rank Component Analysis
(WARCA). We then derive a scalable stochastic gradient descent algorithm for
the resulting learning problem. We also derive an efficient non-linear
extension of WARCA by using the kernel trick. Kernel space embedding decouples
the training and prediction costs from the data dimension and enables us to
plug inarbitrary distance measures which are more natural for the features. We
also address a more general problem of matrix rank degeneration $\&amp;$
non-isolated minima in the low-rank matrix optimization by using new type of
regularizer which approximately enforces the orthonormality of the learned
matrix very efficiently. We validate this new method on nine standard person
re-identification datasets including two large scale Market-1501 and CUHK03
datasets and show that we improve upon the current state-of-the-art methods on
all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00375</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00375</id><created>2016-03-01</created><authors><author><keyname>Kiperwasser</keyname><forenames>Eliyahu</forenames></author><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author></authors><title>Easy-First Dependency Parsing with Hierarchical Tree LSTMs</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest a compositional vector representation of parse trees that relies
on a recursive combination of recurrent-neural network encoders. To demonstrate
its effectiveness, we use the representation as the backbone of a greedy,
bottom-up dependency parser, achieving state-of-the-art accuracies for English
and Chinese, without relying on external word embeddings. The parser's
implementation is available for download at the first author's webpage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00391</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00391</id><created>2016-03-01</created><updated>2016-03-06</updated><authors><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Moczulski</keyname><forenames>Marcin</forenames></author><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Noisy Activation Functions</title><categories>cs.LG cs.NE stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Common activation functions used in neural networks can yield to training
difficulties due to the saturation behavior of the activation function, which
may hide dependencies which are not visible to vanilla-SGD (using first order
gradients only). Gating mechanisms that use softly saturating activation
functions to emulate the discrete switching of digital logic circuits are good
examples of this. We propose to exploit the injection of appropriate noise so
that some gradients may sometimes flow, even if the noiseless application of
the activation function would yield zero gradient. Large noise will dominate
the noise-free gradient and allow stochastic gradient descent to explore more.
By adding noise only to the problematic parts of the activation function, we
allow the optimization procedure to explore the boundary between the degenerate
(saturating) and the well-behaved parts of the activation function. We also
establish connections to simulated annealing, when the amount of noise is
annealed down, making it easier to optimize hard objective functions. We find
experimentally that replacing such saturating activation functions by noisy
variants helps training in many contexts, yielding state-of-the-art or
competitive results on different datasets and task, especially when training
seems to be the most difficult, e.g., when curriculum learning is necessary to
obtain good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00395</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00395</id><created>2016-03-01</created><authors><author><keyname>Wu</keyname><forenames>Tao</forenames></author><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>General Tensor Spectral Co-clustering for Higher-Order Data</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering and co-clustering are well-known techniques in data
analysis, and recent work has extended spectral clustering to square, symmetric
tensors and hypermatrices derived from a network. We develop a new tensor
spectral co-clustering method that applies to any non-negative tensor of data.
The result of applying our method is a simultaneous clustering of the rows,
columns, and slices of a three-mode tensor, and the idea generalizes to any
number of modes. The algorithm we design works by recursively bisecting the
tensor into two pieces. We also design a new measure to understand the role of
each cluster in the tensor. Our new algorithm and pipeline are demonstrated in
both synthetic and real-world problems. On synthetic problems with a planted
higher-order cluster structure, our method is the only one that can reliably
identify the planted structure in all cases. On tensors based on n-gram text
data, we identify stop-words and semantically independent sets; on tensors from
an airline-airport multimodal network, we find worldwide and regional
co-clusters of airlines and airports; and on tensors from an email network, we
identify daily-spam and focused-topic sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00400</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00400</id><created>2016-03-01</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>A Fast Randomized Algorithm for Multi-Objective Query Optimization</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query plans are compared according to multiple cost metrics in
multi-objective query optimization. The goal is to find the set of Pareto plans
realizing optimal cost tradeoffs for a given query. So far, only algorithms
with exponential complexity in the number of query tables have been proposed
for multi-objective query optimization. In this work, we present the first
algorithm with polynomial complexity in the query size.
  Our algorithm is randomized and iterative. It improves query plans via a
multi-objective version of hill climbing that applies multiple transformations
in each climbing step for maximal efficiency. Based on a locally optimal plan,
we approximate the Pareto plan set within the restricted space of plans with
similar join orders. We maintain a cache of Pareto-optimal plans for each
potentially useful intermediate result to share partial plans that were
discovered in different iterations. We show that each iteration of our
algorithm performs in expected polynomial time based on an analysis of the
expected path length between a random plan and local optima reached by hill
climbing. We experimentally show that our algorithm can optimize queries with
hundreds of tables and outperforms other randomized algorithms such as the
NSGA-II genetic algorithm over a wide range of scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00406</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00406</id><created>2016-03-01</created><authors><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Mani</keyname><forenames>Pradeepkumar</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Flavel</keyname><forenames>Ashley</forenames></author><author><keyname>Maltz</keyname><forenames>Dave</forenames></author></authors><title>Distributed Load Management Algorithms in Anycast-based CDNs</title><categories>cs.NI</categories><comments>Submitted to the journal : Computer Networks. Part of the paper
  appeared in the 53rd Annual Allerton Conference on Communication, Control and
  Computing 2015. arXiv admin note: substantial text overlap with
  arXiv:1509.08194</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anycast is an internet addressing protocol where multiple hosts share the
same IP-address. A popular architecture for modern Content Distribution
Networks (CDNs) for geo-replicated services consists of multiple layers of
proxy nodes for service and co-located DNS-servers for load-balancing among
different proxies. Both the proxies and the DNS-servers use anycast addressing,
which offers simplicity of design and high availability of service at the cost
of partial loss of routing control. Due to the very nature of anycast,
redirection actions by a DNS-server also affects loads at nearby proxies in the
network. This makes the problem of optimal distributed load management highly
challenging. In this paper, we propose and evaluate an analytical framework to
formulate and solve the load-management problem in this context. We consider
two distinct algorithms. In the first half of the paper, we pose the
load-management problem as a convex optimization problem. Following a
Kelly-type dual decomposition technique, we propose a fully-distributed
load-management algorithm by introducing FastControl packets. This algorithm
utilizes the underlying anycast mechanism itself to enable effective
coordination among the nodes, thus obviating the need for any external control
channel. In the second half of the paper, we consider an alternative greedy
load-management heuristic, currently in production in a major commercial CDN.
We study its dynamical characteristics and analytically identify its
operational and stability properties. Finally, we critically evaluate both the
algorithms and explore their optimality-vs-complexity trade-off using
trace-driven simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00418</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00418</id><created>2016-03-01</created><authors><author><keyname>Shamailh</keyname><forenames>Ahmad Al-</forenames></author></authors><title>Visualizing source code in 3D Maya software</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper is clarify the summaries codes for programmers through
three-dimensional shapes, and clearly programmers and developers, scholars and
researchers in the field of software engineering, as well as researchers from
the representative about threedimensional forms. Through a three-dimensional
drawing on a Maya scripts which are based on drawing shapes and
three-dimensional stereoscopic show every part of the code, for example,
classes, methods, coherence and homogeneity , In these drawings and show
clearly and useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00423</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00423</id><created>2016-03-01</created><authors><author><keyname>Le</keyname><forenames>Phong</forenames></author><author><keyname>Zuidema</keyname><forenames>Willem</forenames></author></authors><title>Quantifying the vanishing gradient and long distance dependency problem
  in recursive neural networks and recursive LSTMs</title><categories>cs.AI cs.CL cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive neural networks (RNN) and their recently proposed extension
recursive long short term memory networks (RLSTM) are models that compute
representations for sentences, by recursively combining word embeddings
according to an externally provided parse tree. Both models thus, unlike
recurrent networks, explicitly make use of the hierarchical structure of a
sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the
vanishing gradient and long distance dependency problem, and that RLSTMs
greatly improve over RNN's on these problems. We present an artificial learning
task that allows us to quantify the severity of these problems for both models.
We further show that a ratio of gradients (at the root node and a focal leaf
node) is highly indicative of the success of backpropagation at optimizing the
relevant weights low in the tree. This paper thus provides an explanation for
existing, superior results of RLSTMs on tasks such as sentiment analysis, and
suggests that the benefits of including hierarchical structure and of including
LSTM-style gating are complementary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00427</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00427</id><created>2016-03-01</created><authors><author><keyname>Pinheiro</keyname><forenames>Felipe C.</forenames></author><author><keyname>Lopes</keyname><forenames>C&#xe1;ssio G.</forenames></author></authors><title>A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear
  Functionals</title><categories>cs.SY cs.LG</categories><comments>5 pages, one of references, plus extra page attached</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear adaptive filtering allows for modeling of some additional aspects
of a general system and usually relies on highly complex algorithms, such as
those based on the Volterra series. Through the use of the Kronecker product
and some basic facts of tensor algebra, we propose a simple model of
nonlinearity, one that can be interpreted as a product of the outputs of K FIR
linear filters, and compute its cost function together with its gradient, which
allows for some analysis of the optimization problem. We use these results it
in a stochastic gradient framework, from which we derive an LMS-like algorithm
and investigate the problems of multi-modality in the mean-square error surface
and the choice of adequate initial conditions. Its computational complexity is
calculated. The new algorithm is tested in a system identification setup and is
compared with other polynomial algorithms from the literature, presenting
favorable convergence and/or computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00431</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00431</id><created>2016-03-01</created><authors><author><keyname>Sanatinia</keyname><forenames>Amirali</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author></authors><title>On GitHub's Programming Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GitHub is the most widely used social, distributed version control system. It
has around 10 million registered users and hosts over 16 million public
repositories. Its user base is also very active as GitHub ranks in the top 100
Alexa most popular websites. In this study, we collect GitHub's state in its
entirety. Doing so, allows us to study new aspects of the ecosystem. Although
GitHub is the home to millions of users and repositories, the analysis of
users' activity time-series reveals that only around 10% of them can be
considered active. The collected dataset allows us to investigate the
popularity of programming languages and existence of pattens in the relations
between users, repositories, and programming languages.
  By, applying a k-means clustering method to the users-repositories commits
matrix, we find that two clear clusters of programming languages separate from
the remaining. One cluster forms for &quot;web programming&quot; languages (Java Script,
Ruby, PHP, CSS), and a second for &quot;system oriented programming&quot; languages (C,
C++, Python). Further classification, allow us to build a phylogenetic tree of
the use of programming languages in GitHub. Additionally, we study the main and
the auxiliary programming languages of the top 1000 repositories in more
detail. We provide a ranking of these auxiliary programming languages using
various metrics, such as percentage of lines of code, and PageRank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00437</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00437</id><created>2016-03-01</created><authors><author><keyname>Imbiriba</keyname><forenames>Tales</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Bermudez</keyname><forenames>Jos&#xe9; Carlos Moreira</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames><affiliation>Universit&#xe9; de Nice Sophia-Antipolis, CNRS, Nice, France</affiliation></author></authors><title>Technical Report: Band selection for nonlinear unmixing of hyperspectral
  images as a maximal click problem</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based nonlinear mixing models have been applied to unmix spectral
information of hyperspectral images when the type of mixing occurring in the
scene is too complex or unknown. Such methods, however, usually require the
inversion of matrices of sizes equal to the number of spectral bands. Reducing
the computational load of these methods remains a challenge in large scale
applications. This paper proposes a centralized method for band selection (BS)
in the reproducing kernel Hilbert space (RKHS). It is based upon the coherence
criterion, which sets the largest value allowed for correlations between the
basis kernel functions characterizing the unmixing model. We show that the
proposed BS approach is equivalent to solving a maximum clique problem (MCP),
that is, searching for the biggest complete subgraph in a graph. Furthermore,
we devise a strategy for selecting the coherence threshold and the Gaussian
kernel bandwidth using coherence bounds for linearly independent bases.
Simulation results illustrate the efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00438</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00438</id><created>2016-03-01</created><authors><author><keyname>Paulin</keyname><forenames>Mattis</forenames><affiliation>LEAR</affiliation></author><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>LEAR</affiliation></author><author><keyname>Douze</keyname><forenames>Matthijs</forenames><affiliation>LEAR</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>NYU</affiliation></author><author><keyname>Perronnin</keyname><forenames>Florent</forenames><affiliation>LEAR</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>LEAR</affiliation></author></authors><title>Convolutional Patch Representations for Image Retrieval: an Unsupervised
  Approach</title><categories>cs.CV</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) have recently received a lot of
attention due to their ability to model local stationary structures in natural
images in a multi-scale fashion, when learning all model parameters with
supervision. While excellent performance was achieved for image classification
when large amounts of labeled visual data are available, their success for
un-supervised tasks such as image retrieval has been moderate so far. Our paper
focuses on this latter setting and explores several methods for learning patch
descriptors without supervision with application to matching and instance-level
retrieval. To that effect, we propose a new family of convolutional descriptors
for patch representation , based on the recently introduced convolutional
kernel networks. We show that our descriptor, named Patch-CKN, performs better
than SIFT as well as other convolutional networks learned by artificially
introducing supervision and is significantly faster to train. To demonstrate
its effectiveness, we perform an extensive evaluation on standard benchmarks
for patch and image retrieval where we obtain state-of-the-art results. We also
introduce a new dataset called RomePatches, which allows to simultaneously
study descriptor performance for patch and image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00441</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00441</id><created>2016-03-01</created><authors><author><keyname>Liao</keyname><forenames>Ruizhi</forenames></author><author><keyname>Roman</keyname><forenames>Cristian</forenames></author><author><keyname>Ball</keyname><forenames>Peter</forenames></author><author><keyname>Ou</keyname><forenames>Shumao</forenames></author><author><keyname>Chen</keyname><forenames>Liping</forenames></author></authors><title>Crowdsourcing On-street Parking Space Detection</title><categories>cs.HC cs.LG</categories><comments>8 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the number of vehicles continues to grow, parking spaces are at a premium
in city streets. Additionally, due to the lack of knowledge about street
parking spaces, heuristic circling the blocks not only costs drivers' time and
fuel, but also increases city congestion. In the wake of recent trend to build
convenient, green and energy-efficient smart cities, we rethink common
techniques adopted by high-profile smart parking systems, and present a
user-engaged (crowdsourcing) and sonar-based prototype to identify urban
on-street parking spaces. The prototype includes an ultrasonic sensor, a GPS
receiver and associated Arduino micro-controllers. It is mounted on the
passenger side of a car to measure the distance from the vehicle to the nearest
roadside obstacle. Multiple road tests are conducted around Wheatley, Oxford to
gather results and emulate the crowdsourcing approach. By extracting parked
vehicles' features from the collected trace, a supervised learning algorithm is
developed to estimate roadside parking occupancy and spot illegal parking
vehicles. A quantity estimation model is derived to calculate the required
number of sensing units to cover urban streets. The estimation is
quantitatively compared to a fixed sensing solution. The results show that the
crowdsourcing way would need substantially fewer sensors compared to the fixed
sensing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00448</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00448</id><created>2016-03-01</created><updated>2016-03-03</updated><authors><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Guided Cost Learning: Deep Inverse Optimal Control via Policy
  Optimization</title><categories>cs.LG cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning can acquire complex behaviors from high-level
specifications. However, defining a cost function that can be optimized
effectively and encodes the correct task is challenging in practice. We explore
how inverse optimal control (IOC) can be used to learn behaviors from
demonstrations, with applications to torque control of high-dimensional robotic
systems. Our method addresses two key challenges in inverse optimal control:
first, the need for informative features and effective regularization to impose
structure on the cost, and second, the difficulty of learning the cost function
under unknown dynamics for high-dimensional continuous systems. To address the
former challenge, we present an algorithm capable of learning arbitrary
nonlinear cost functions, such as neural networks, without meticulous feature
engineering. To address the latter challenge, we formulate an efficient
sample-based approximation for MaxEnt IOC. We evaluate our method on a series
of simulated tasks and real-world robotic manipulation problems, demonstrating
substantial improvement over prior methods both in terms of task complexity and
sample efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00489</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00489</id><created>2016-03-01</created><authors><author><keyname>Bency</keyname><forenames>Archith J.</forenames></author><author><keyname>Kwon</keyname><forenames>Heesung</forenames></author><author><keyname>Lee</keyname><forenames>Hyungtae</forenames></author><author><keyname>Karthikeyan</keyname><forenames>S.</forenames></author><author><keyname>Manjunath</keyname><forenames>B. S.</forenames></author></authors><title>Weakly Supervised Localization using Deep Feature Maps</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object localization is an important computer vision problem with a variety of
applications. The lack of large scale object-level annotations and the relative
abundance of image-level labels makes a compelling case for weak supervision in
the object localization task. Deep Convolutional Neural Networks are a class of
state-of-the-art methods for the related problem of object recognition. In this
paper, we describe a novel object localization algorithm which uses
classification networks trained on only image labels. This weakly supervised
method leverages local spatial and semantic patterns captured in the
convolutional layers of classification networks. We propose an efficient beam
search based approach to detect and localize multiple objects in images. The
proposed method significantly outperforms the state-of-the-art in standard
object detection data-sets with a 8 point increase in mAP scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00491</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00491</id><created>2016-03-01</created><authors><author><keyname>Dukhan</keyname><forenames>Marat</forenames></author><author><keyname>Vuduc</keyname><forenames>Richard</forenames></author><author><keyname>Riedy</keyname><forenames>Jason</forenames></author></authors><title>Wanted: Floating-Point Add Round-off Error instruction</title><categories>cs.NA cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new instruction (FPADDRE) that computes the round-off error in
floating-point addition. We explain how this instruction benefits
high-precision arithmetic operations in applications where double precision is
not sufficient. Performance estimates on Intel Haswell, Intel Skylake, and AMD
Steamroller processors, as well as Intel Knights Corner co-processor,
demonstrate that such an instruction would improve the latency of double-double
addition by up to 55% and increase double-double addition throughput by up to
103%, with smaller, but non-negligible benefits for double-double
multiplication. The new instruction delivers up to 2x speedups on three
benchmarks that use high-precision floating-point arithmetic: double-double
matrix-matrix multiplication, compensated dot product, and polynomial
evaluation via the compensated Horner scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00502</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00502</id><created>2016-03-01</created><authors><author><keyname>Turner</keyname><forenames>JT</forenames></author><author><keyname>Gupta</keyname><forenames>Kalyan</forenames></author><author><keyname>Morris</keyname><forenames>Brendan</forenames></author><author><keyname>Aha</keyname><forenames>David W.</forenames></author></authors><title>Keypoint Density-based Region Proposal for Fine-Grained Object Detection
  and Classification using Regions with Convolutional Neural Network Features</title><categories>cs.CV</categories><comments>9 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although recent advances in regional Convolutional Neural Networks (CNNs)
enable them to outperform conventional techniques on standard object detection
and classification tasks, their response time is still slow for real-time
performance. To address this issue, we propose a method for region proposal as
an alternative to selective search, which is used in current state-of-the art
object detection algorithms. We evaluate our Keypoint Density-based Region
Proposal (KDRP) approach and show that it speeds up detection and
classification on fine-grained tasks by 100% versus the existing selective
search region proposal technique without compromising classification accuracy.
KDRP makes the application of CNNs to real-time detection and classification
feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00509</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00509</id><created>2016-03-01</created><authors><author><keyname>Havvaei</keyname><forenames>Elham</forenames></author><author><keyname>Deo</keyname><forenames>Narsingh</forenames></author></authors><title>A Game-Theoretic Approach for Detection of Overlapping Communities in
  Dynamic Complex Networks</title><categories>cs.GT cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks tend to display communities which are groups of nodes
cohesively connected among themselves in one group and sparsely connected to
the remainder of the network. Detecting such communities is an important
computational problem, since it provides an insight into the functionality of
networks. Further, investigating community structure in a dynamic network,
where the network is subject to change, is even more challenging. This paper
presents a game-theoretical technique for detecting community structures in
dynamic as well as static complex networks. In our method, each node takes the
role of a player that attempts to gain a higher payoff by joining one or more
communities or switching between them. The goal of the game is to reveal
community structure formed by these players by finding a Nash-equilibrium point
among them. To the best of our knowledge, this is the first game-theoretic
algorithm which is able to extract overlapping communities from either static
or dynamic networks. We present the experimental results illustrating the
effectiveness of the proposed method on both synthetic and real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00522</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00522</id><created>2016-03-01</created><authors><author><keyname>Gupta</keyname><forenames>Swati</forenames></author><author><keyname>Goemans</keyname><forenames>Michel</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Solving Combinatorial Games using Products, Projections and
  Lexicographically Optimal Bases</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to find Nash-equilibria for two-player zero-sum games where each
player plays combinatorial objects like spanning trees, matchings etc, we
consider two online learning algorithms: the online mirror descent (OMD)
algorithm and the multiplicative weights update (MWU) algorithm. The OMD
algorithm requires the computation of a certain Bregman projection, that has
closed form solutions for simple convex sets like the Euclidean ball or the
simplex. However, for general polyhedra one often needs to exploit the general
machinery of convex optimization. We give a novel primal-style algorithm for
computing Bregman projections on the base polytopes of polymatroids. Next, in
the case of the MWU algorithm, although it scales logarithmically in the number
of pure strategies or experts $N$ in terms of regret, the algorithm takes time
polynomial in $N$; this especially becomes a problem when learning
combinatorial objects. We give a general recipe to simulate the multiplicative
weights update algorithm in time polynomial in their natural dimension. This is
useful whenever there exists a polynomial time generalized counting oracle
(even if approximate) over these objects. Finally, using the combinatorial
structure of symmetric Nash-equilibria (SNE) when both players play bases of
matroids, we show that these can be found with a single projection or convex
minimization (without using online learning).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00531</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00531</id><created>2016-03-01</created><authors><author><keyname>Yu</keyname><forenames>Kui</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Xindong</forenames></author></authors><title>LOFS: Library of Online Streaming Feature Selection</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an emerging research direction, online streaming feature selection deals
with sequentially added dimensions in a feature space while the number of data
instances is fixed. Online streaming feature selection provides a new,
complementary algorithmic methodology to enrich online feature selection,
especially targets to high dimensionality in big data analytics. This paper
introduces the first comprehensive open-source library for use in MATLAB that
implements the state-of-the-art algorithms of online streaming feature
selection. The library is designed to facilitate the development of new
algorithms in this exciting research direction and make comparisons between the
new methods and existing ones available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00532</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00532</id><created>2016-03-01</created><authors><author><keyname>Ghorashi</keyname><forenames>Soroush</forenames></author><author><keyname>Jensen</keyname><forenames>Carlos</forenames></author></authors><title>Jimbo: A Collaborative IDE with Live Preview</title><categories>cs.HC</categories><comments>4 pages</comments><acm-class>D.2.6; D.2.2</acm-class><doi>10.1145/2897586.2897613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Team collaboration plays a key role in the success of any multi-user
activity. Software engineering is a highly collaborative activity, where
multiple developers and designers work together to solve a common problem.
Meaningful and effective designer-developer collaboration improves the user
experience, which can improve the chances of success for the project. Learning
to program is another activity that can be implemented in a more collaborative
way, students can learn in an active style by working with others. The growth
of online classes, from small structured seminars to massive open online
courses (MOOCs), and the isolation and impoverished learning experience some
students report in these, points to an urgent need for tools that support
remote pair programming in a distributed educational setting. In this paper, we
describe Jimbo, a collaborative integrated development environment (IDE) that
we believe is beneficial and effective in both aforementioned activities. Jimbo
integrates many features that support better collaboration and communication
between designers and developers, to bridge communication gaps and develop
mutual understanding. These novel features can improve today's CS education by
bringing students closer to each other and their instructors as well as
training them to collaborate which is consistent with current practices in
software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00536</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00536</id><created>2016-03-01</created><authors><author><keyname>Mu&#xf1;oz</keyname><forenames>C&#xe9;sar A.</forenames><affiliation>NASA Langley Research Center</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen</affiliation></author></authors><title>Proceedings of the Eleventh International Workshop on Developments in
  Computational Models</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 204, 2016</journal-ref><doi>10.4204/EPTCS.204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of DCM 2015, the 11th International
Workshop on Developments in Computational Models held on October 28, 2015 in
Cali, Colombia. DCM 2015 was organized as a one-day satellite event of the 12th
International Colloquium on Theoretical Aspects of Computing (ICTAC 2015).
  Several new models of computation have emerged in the last few years, and
many developments of traditional computational models have been proposed with
the aim of taking into account the new demands of computer systems users and
the new capabilities of computation engines. A new computational model, or a
new feature in a traditional one, usually is reflected in a new family of
programming languages, and new paradigms of software development.
  The aim of the DCM workshop series is to bring together researchers who are
currently developing new computational models or new features for traditional
computational models, in order to foster their interaction, to provide a forum
for presenting new ideas and work in progress, and to enable newcomers to learn
about current activities in this area. Topics of interest include all abstract
models of computation and their applications to the development of programming
languages and systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00542</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00542</id><created>2016-03-01</created><authors><author><keyname>Dashti</keyname><forenames>Mohammad</forenames></author><author><keyname>John</keyname><forenames>Sachin Basil</forenames></author><author><keyname>Shaikhha</keyname><forenames>Amir</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Repairing Conflicts among MVCC Transactions</title><categories>cs.DB</categories><comments>12 pages, 9 figures</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimistic variants of MVCC (Multi-Version Concurrency Control) avoid
blocking concurrent transactions at the cost of having a validation phase. Upon
failure in the validation phase, the transaction is usually aborted and
restarted from scratch. The &quot;abort and restart&quot; approach becomes a performance
bottleneck for the use cases with high contention objects or long running
transactions. In addition, restarting from scratch creates a negative feedback
loop in the system, because the system incurs additional overhead that may
create even further conflicts.
  In this paper, we propose a novel approach for conflict resolution in MVCC
for in-memory databases. This low overhead approach summarizes the transaction
programs in the form of a dependency graph. The dependency graph also contains
the constructs used in the validation phase of the MVCC algorithm. Then, in the
case of encountering conflicts among transactions, the conflict locations in
the program are quickly detected, and the conflicting transactions are
partially re-executed. This approach maximizes the reuse of the computations
done in the initial execution round, and increases the transaction processing
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00544</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00544</id><created>2016-03-01</created><authors><author><keyname>Massoulie</keyname><forenames>Laurent</forenames></author><author><keyname>Xu</keyname><forenames>Kuang</forenames></author></authors><title>On the capacity of information processing systems</title><categories>math.PR cs.IT math.IT stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a family of information processing systems, where a
finite set of experts or servers are employed to extract information about a
stream of incoming jobs. Each job is associated with a hidden label drawn from
some prior distribution. An inspection by an expert produces a noisy outcome
that depends both on the job's hidden label and the type of the expert, and
occupies the expert for a finite time duration. A decision maker's task is to
dynamically assign inspections so that the resulting outcomes can be used to
accurately recover the labels of all jobs, while keeping the system stable.
Among our chief motivations are applications in crowd-sourcing, diagnostics,
and experiment designs, where one wishes to efficiently learn the nature of a
large number of items, using a finite pool of computational resources or human
agents.
  We focus on the capacity of such an information processing system. Given a
level of accuracy guarantee, we ask how many experts are needed in order to
stabilize the system, and through what inspection architecture. Our main result
provides an adaptive inspection policy that is asymptotically optimal in the
following sense: the ratio between the required number of experts under our
policy and the theoretical optimal converges to one, as the probability of
error in label recovery tends to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00546</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00546</id><created>2016-03-01</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Voglreiter</keyname><forenames>Philip</forenames></author><author><keyname>Dokter</keyname><forenames>Mark</forenames></author><author><keyname>Hofmann</keyname><forenames>Michael</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zoller</keyname><forenames>Wolfram G.</forenames></author><author><keyname>Schmalstieg</keyname><forenames>Dieter</forenames></author><author><keyname>Hann</keyname><forenames>Alexander</forenames></author></authors><title>US-Cut: Interactive Algorithm for rapid Detection and Segmentation of
  Liver Tumors in Ultrasound Acquisitions</title><categories>cs.CV cs.CE cs.CG cs.GR</categories><comments>6 pages, 6 figures, 1 table, 32 references</comments><journal-ref>SPIE Medical Imaging Conference 2016, Paper 9790-47</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultrasound (US) is the most commonly used liver imaging modality worldwide.
It plays an important role in follow-up of cancer patients with liver
metastases. We present an interactive segmentation approach for liver tumors in
US acquisitions. Due to the low image quality and the low contrast between the
tumors and the surrounding tissue in US images, the segmentation is very
challenging. Thus, the clinical practice still relies on manual measurement and
outlining of the tumors in the US images. We target this problem by applying an
interactive segmentation algorithm to the US data, allowing the user to get
real-time feedback of the segmentation results. The algorithm has been
developed and tested hand-in-hand by physicians and computer scientists to make
sure a future practical usage in a clinical setting is feasible. To cover
typical acquisitions from the clinical routine, the approach has been evaluated
with dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic
(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.
Due to the interactive real-time behavior of the approach, it was possible even
in difficult cases to find satisfying segmentations of the tumors within
seconds and without parameter settings, and the average tumor deviation was
only 1.4mm compared with manual measurements. However, the long term goal is to
ease the volumetric acquisition of liver tumors in order to evaluate for
treatment response. Additional aim is the registration of intraoperative US
images via the interactive segmentations to the patient's pre-interventional CT
acquisitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00550</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00550</id><created>2016-03-01</created><authors><author><keyname>Changpinyo</keyname><forenames>Soravit</forenames></author><author><keyname>Chao</keyname><forenames>Wei-Lun</forenames></author><author><keyname>Gong</keyname><forenames>Boqing</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>Synthesized Classifiers for Zero-Shot Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given semantic descriptions of object classes, zero-shot learning aims to
accurately recognize objects of the unseen classes, from which no examples are
available at the training stage, by associating them to the seen classes, from
which labeled examples are provided. We propose to tackle this problem from the
perspective of manifold learning. Our main idea is to align the semantic space
that is derived from external information to the model space that concerns
itself with recognizing visual features. To this end, we introduce a set of
&quot;phantom&quot; object classes whose coordinates live in both the semantic space and
the model space. Serving as bases in a dictionary, they can be optimized from
labeled data such that the synthesized real object classifiers achieve optimal
discriminative performance. We demonstrate superior accuracy of our approach
over the state of the art on four benchmark datasets for zero-shot learning,
including the full ImageNet Fall 2011 dataset with more than 20,000 unseen
classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00560</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00560</id><created>2016-03-01</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Learnt quasi-transitive similarity for retrieval from large collections
  of faces</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in identity-based retrieval of face sets from large
unlabelled collections acquired in uncontrolled environments. Given a baseline
algorithm for measuring the similarity of two face sets, the meta-algorithm
introduced in this paper seeks to leverage the structure of the data corpus to
make the best use of the available baseline. In particular, we show how partial
transitivity of inter-personal similarity can be exploited to improve the
retrieval of particularly challenging sets which poorly match the query under
the baseline measure. We: (i) describe the use of proxy sets as a means of
computing the similarity between two sets, (ii) introduce transitivity
meta-features based on the similarity of salient modes of appearance variation
between sets, (iii) show how quasi-transitivity can be learnt from such
features without any labelling or manual intervention, and (iv) demonstrate the
effectiveness of the proposed methodology through experiments on the
notoriously challenging YouTube database and two successful baselines from the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00562</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00562</id><created>2016-03-01</created><updated>2016-03-02</updated><authors><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Wang</keyname><forenames>Zihe</forenames></author></authors><title>Ironing the Border: Optimal Auctions for Negatively Correlated Items</title><categories>cs.GT</categories><comments>26 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing revenue-optimal auctions for selling two
items and bidders' valuations are independent among bidders but negatively
correlated among items.
  In this paper, we obtain the closed-form optimal auction for this setting, by
directly addressing the two difficulties above. In particular, the first
difficulty is that when pointwise maximizing virtual surplus under
multi-dimensional feasibility (i.e., the Border feasibility), (1) neither the
optimal interim allocation is trivially monotone in the virtual value, (2) nor
the virtual value is monotone in the bidder's type. As a result, the optimal
interim allocations resulting from virtual surplus maximization no longer
guarantees BIC. To address (1), we prove a generalization of Border's theorem
and show that optimal interim allocation is indeed monotone in the virtual
value. To address (2), we adapt Myerson's ironing procedure to this setting by
redefining the (ironed) virtual value as a function of the lowest utility
point. The second difficulty, perhaps a more challenging one, is that the
lowest utility type in general is no longer at the endpoints of the type
interval. To address this difficulty, we show by construction that there exist
an allocation rule and an induced lowest utility type such that they form a
solution of the virtual surplus maximization and in the meanwhile guarantees
IIR. In the single bidder case, the optimal auction consists of a randomized
bundle menu and a deterministic bundle menu; while in the multiple bidder case,
the optimal auction is a randomization between two extreme mechanisms. The
optimal solutions of our setting can be implemented by a Bayesian IC and IR
auction, however, perhaps surprisingly, the revenue of this auction cannot be
achieved by any (dominant-strategy) IC and IR auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00564</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00564</id><created>2016-03-01</created><authors><author><keyname>Alaoui</keyname><forenames>Ahmed El</forenames></author><author><keyname>Cheng</keyname><forenames>Xiang</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Asymptotic behavior of $\ell_p$-based Laplacian regularization in
  semi-supervised learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a weighted graph with $N$ vertices, consider a real-valued regression
problem in a semi-supervised setting, where one observes $n$ labeled vertices,
and the task is to label the remaining ones. We present a theoretical study of
$\ell_p$-based Laplacian regularization under a $d$-dimensional geometric
random graph model. We provide a variational characterization of the
performance of this regularized learner as $N$ grows to infinity while $n$
stays constant, the associated optimality conditions lead to a partial
differential equation that must be satisfied by the associated function
estimate $\hat{f}$. From this formulation we derive several predictions on the
limiting behavior the $d$-dimensional function $\hat{f}$, including (a) a phase
transition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoff
between smoothness and sensitivity to the underlying unlabeled data
distribution $P$. Thus, over the range $p \leq d$, the function estimate
$\hat{f}$ is degenerate and &quot;spiky,&quot; whereas for $p\geq d+1$, the function
estimate $\hat{f}$ is smooth. We show that the effect of the underlying density
vanishes monotonically with $p$, such that in the limit $p = \infty$,
corresponding to the so-called Absolutely Minimal Lipschitz Extension, the
estimate $\hat{f}$ is independent of the distribution $P$. Under the assumption
of semi-supervised smoothness, ignoring $P$ can lead to poor statistical
performance, in particular, we construct a specific example for $d=1$ to
demonstrate that $p=2$ has lower risk than $p=\infty$ due to the former penalty
adapting to $P$ and the latter ignoring it. We also provide simulations that
verify the accuracy of our predictions for finite sample sizes. Together, these
properties show that $p = d+1$ is an optimal choice, yielding a function
estimate $\hat{f}$ that is both smooth and non-degenerate, while remaining
maximally sensitive to $P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00567</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00567</id><created>2016-03-01</created><authors><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Narayanan</keyname><forenames>Deepak</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>MacroBase: Analytic Monitoring for the Internet of Things</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  An increasing proportion of data today is generated by automated processes,
sensors, and systems---collectively, the Internet of Things (IoT). A key
challenge in IoT data management and a core aspect of many IoT applications
(e.g., industrial diagnostics, predictive maintenance, and equipment auditing)
is in identifying and highlighting unusual and surprising data (e.g., poor
driving behavior, equipment failures, physical intrusion). We call this task,
which is often statistical in nature and time-sensitive, analytic monitoring.
To facilitate rapid development and scalable deployment of analytic monitoring
queries, we have developed MacroBase, a data analytics engine that performs
analytic monitoring of IoT data streams. MacroBase implements a customizable
pipeline of outlier detection, summarization, and ranking operators. For
efficient and accurate execution, MacroBase implements several cross-layer
optimizations across robust estimation, pattern mining, and sketching
procedures, allowing order-of-magnitude speedups. As a result, MacroBase can
analyze up to 1M events per second on a single core. MacroBase has already
delivered meaningful analytic monitoring results in production at a
medium-scale IoT startup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00570</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00570</id><created>2016-03-01</created><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Without-Replacement Sampling for Stochastic Gradient Methods:
  Convergence Results and Application to Distributed Optimization</title><categories>cs.LG math.OC stat.ML</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient methods for machine learning and optimization problems
are usually analyzed assuming data points are sampled with replacement. In
practice, however, sampling without replacement is very common, easier to
implement in many cases, and often performs better. In this paper, we provide
competitive convergence guarantees for without-replacement sampling, under
various scenarios, for three types of algorithms: Any algorithm with online
regret guarantees, stochastic gradient descent, and SVRG. A useful application
of our SVRG analysis is a nearly-optimal algorithm for regularized least
squares in a distributed setting, in terms of both communication complexity and
runtime complexity, when the data is randomly partitioned and the condition
number can be as large as the data size (up to logarithmic factors). Our proof
techniques combine ideas from stochastic optimization, adversarial online
learning, and transductive learning theory, and can potentially be applied to
other stochastic optimization and learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00572</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00572</id><created>2016-03-01</created><authors><author><keyname>Hingwe</keyname><forenames>Kamlesh Kumar</forenames></author><author><keyname>Bhanu</keyname><forenames>S. Mary Saira</forenames></author></authors><title>Hierarchical Role-Based Access Control with Homomorphic Encryption for
  Database as a Service</title><categories>cs.CR</categories><comments>11 Pages,4 figures, Proceedings of International Conference on ICT
  for Sustainable Development</comments><doi>10.1007/978-981-10-0135-2_43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database as a service provides services for accessing and managing customers
data which provides ease of access, and the cost is less for these services.
There is a possibility that the DBaaS service provider may not be trusted, and
data may be stored on untrusted server. The access control mechanism can
restrict users from unauthorized access, but in cloud environment access
control policies are more flexible. However, an attacker can gather sensitive
information for a malicious purpose by abusing the privileges as another user
and so database security is compromised. The other problems associated with the
DBaaS are to manage role hierarchy and secure session management for query
transaction in the database. In this paper, a role-based access control for the
multitenant database with role hierarchy is proposed. The query is granted with
least access privileges, and a session key is used for session management. The
proposed work protects data from privilege escalation and SQL injection. It
uses the partial homomorphic encryption (Paillier Encryption) for the
encrypting the sensitive data. If a query is to perform any operation on
sensitive data, then extra permissions are required for accessing sensitive
data. Data confidentiality and integrity are achieved using the role-based
access control with partial homomorphic encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00573</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00573</id><created>2016-03-01</created><authors><author><keyname>Srikant</keyname><forenames>Sukumar</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>A jammer's perspective of reachability and LQ optimal control</title><categories>cs.SY math.OC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article treats two problems dealing with control of linear systems in
the presence of a jammer that can sporadically turn off the control signal. The
first problem treats the standard reachability problem, and the second treats
the standard linear quadratic regulator problem under the above class of
jamming signals. We provide necessary and sufficient conditions for optimality
based on a nonsmooth Pontryagin maximum principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00576</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00576</id><created>2016-03-01</created><authors><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Distributed Estimation of Dynamic Parameters : Regret Analysis</title><categories>math.OC cs.LG cs.SI</categories><comments>6 pages, To appear in American Control Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the estimation of a time- varying parameter in a
network. A group of agents sequentially receive noisy signals about the
parameter (or moving target), which does not follow any particular dynamics.
The parameter is not observable to an individual agent, but it is globally
identifiable for the whole network. Viewing the problem with an online
optimization lens, we aim to provide the finite-time or non-asymptotic analysis
of the problem. To this end, we use a notion of dynamic regret which suits the
online, non-stationary nature of the problem. In our setting, dynamic regret
can be recognized as a finite-time counterpart of stability in the mean- square
sense. We develop a distributed, online algorithm for tracking the moving
target. Defining the path-length as the consecutive differences between target
locations, we express an upper bound on regret in terms of the path-length of
the target and network errors. We further show the consistency of the result
with static setting and noiseless observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00580</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00580</id><created>2016-03-02</created><authors><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>van Kreveld</keyname><forenames>Marc</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Sacrist&#xe1;n</keyname><forenames>Vera</forenames></author><author><keyname>Shioura</keyname><forenames>Akiyoshi</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo I.</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author></authors><title>Colored Spanning Graphs for Set Visualization</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an algorithmic problem that is motivated by ink minimization for
sparse set visualizations. Our input is a set of points in the plane which are
either blue, red, or purple. Blue points belong exclusively to the blue set,
red points belong exclusively to the red set, and purple points belong to both
sets. A \emph{red-blue-purple spanning graph} (RBP spanning graph) is a set of
edges connecting the points such that the subgraph induced by the red and
purple points is connected, and the subgraph induced by the blue and purple
points is connected.
  We study the geometric properties of minimum RBP spanning graphs and the
algorithmic problems associated with computing them. Specifically, we show that
the general problem can be solved in polynomial time using matroid techniques.
In addition, we discuss more efficient algorithms for the case in which points
are located on a line or a circle, and also describe a fast $(\frac
12\rho+1)$-approximation algorithm, where $\rho$ is the Steiner ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00583</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00583</id><created>2016-03-02</created><authors><author><keyname>Devin</keyname><forenames>Sandra</forenames></author><author><keyname>Milliez</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Fiore</keyname><forenames>Michelangelo</forenames></author><author><keyname>Clodic</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Alami</keyname><forenames>Rachid</forenames></author></authors><title>Some essential skills and their combination in an architecture for a
  cognitive and interactive robot</title><categories>cs.RO</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)&quot;</comments><report-no>CogArch4sHRI/2016/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topic of joint actions has been deeply studied in the context of
Human-Human interaction in order to understand how humans cooperate. Creating
autonomous robots that collaborate with humans is a complex problem, where it
is relevant to apply what has been learned in the context of Human-Human
interaction. The question is what skills to implement and how to integrate them
in order to build a cognitive architecture, allowing a robot to collaborate
efficiently and naturally with humans. In this paper, we first list a set of
skills that we consider essential for Joint Action, then we analyze the problem
from the robot's point of view and discuss how they can be instantiated in
human-robot scenarios. Finally, we open the discussion on how to integrate such
skills into a cognitive architecture for human-robot collaborative problem
solving and task achievement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00585</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00585</id><created>2016-03-02</created><authors><author><keyname>Gao</keyname><forenames>Juntao</forenames></author><author><keyname>Ito</keyname><forenames>Minoru</forenames></author><author><keyname>Shiratori</keyname><forenames>Norio</forenames></author></authors><title>Optimal Scheduling for Incentive WiFi Offloading under Energy Constraint</title><categories>cs.NI cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi offloading, where mobile device users (e.g., smart phone users) transmit
packets through WiFi networks rather than cellular networks, is a promising
solution to alleviating the heavy traffic burden of cellular networks due to
data explosion. However, since WiFi networks are intermittently available, a
mobile device user in WiFi offloading usually needs to wait for WiFi connection
and thus experiences longer delay of packet transmission. To motivate users to
participate in WiFi offloading, cellular network operators give incentives
(rewards like coupons, e-coins) to users who wait for WiFi connection and
transmit packets through WiFi networks.
  In this paper, we aim at maximizing users' rewards while meeting constraints
on queue stability and energy consumption. However, we face scheduling
challenges from random packet arrivals, intermittent WiFi connection and time
varying wireless link states. To address these challenges, we first formulate
the problem as a stochastic optimization problem. We then propose an optimal
scheduling policy, named Optimal scheduling Policy under Energy Constraint
(OPEC), which makes online decisions as to when to delay packet transmission to
wait for WiFi connection and which wireless link (WiFi link or cellular link)
to transmit packets on. OPEC automatically adapts to random packet arrivals and
time varying wireless link states, not requiring a priori knowledge of packet
arrival and wireless link probabilities. As verified by simulations, OPEC
scheduling policy can achieve the maximum rewards while keeping queue stable
and meeting energy consumption constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00587</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00587</id><created>2016-03-02</created><authors><author><keyname>Hwang</keyname><forenames>Wen-Liang</forenames></author></authors><title>A Theorem on Multi-Objective Optimization Approach for Bit Allocation of
  Scalable Coding</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the current work, we have formulated the optimal bit-allocation problem
for a scalable codec of images or videos as a constrained vector-valued
optimization problem and demonstrated that there can be many optimal solutions,
called Pareto optimal points. In practice, the Pareto points are derived via
the weighted sum scalarization approach. An important question which arises is
whether all the Pareto optimal points can be derived using the scalarization
approach? The present paper provides a sufficient condition on the
rate-distortion function of each resolution of a scalable codec to address the
above question. The result indicated that if the rate-distortion function of
each resolution is strictly decreasing and convex and the Pareto points form a
continuous curve, then all the optimal Pareto points can be derived by using
the scalarization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00588</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00588</id><created>2016-03-02</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Lin</keyname><forenames>Ching-Chao</forenames></author><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author><author><keyname>Hsiao</keyname><forenames>Hsu-Chun</forenames></author><author><keyname>Huang</keyname><forenames>Chun-Ying</forenames></author></authors><title>Decapitation via Digital Epidemics: A Bio-Inspired Transmissive Attack</title><categories>cs.CR cs.NI cs.SI</categories><comments>To appear in June 2016 IEEE Communications Magazine, feature topic on
  &quot;Bio-inspired Cyber Security for Communications and Networking&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of communication technology and the proliferation of electronic
devices have rendered adversaries powerful means for targeted attacks via all
sorts of accessible resources. In particular, owing to the intrinsic
interdependency and ubiquitous connectivity of modern communication systems,
adversaries can devise malware that propagates through intermediate hosts to
approach the target, which we refer to as transmissive attacks. Inspired by
biology, the transmission pattern of such an attack in the digital space much
resembles the spread of an epidemic in real life. This paper elaborates
transmissive attacks, summarizes the utility of epidemic models in
communication systems, and draws connections between transmissive attacks and
epidemic models. Simulations, experiments, and ongoing research challenges on
transmissive attacks are also addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00589</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00589</id><created>2016-03-02</created><authors><author><keyname>Awasthi</keyname><forenames>Sateesh Kumar</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author></authors><title>Generalized Analysis of Convergence of Absolute Trust in Peer to Peer
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open and anonymous nature of peer to peer networks provides an opportunity to
malicious peers to behave unpredictably in the network. This leads the lack of
trust among the peers. To control the behavior of peers in the network,
reputation system can be used. In a reputation system, aggregation of trust is
a primary issue. Algorithm for aggregation of trust should be designed such
that, it can converge to a certain finite value. Absolute Trust is one of the
algorithm, which is used for the aggregation of trust in peer to peer networks.
In this letter, we present the generalized analysis of convergence of the
Absolute Trust algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00600</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00600</id><created>2016-03-02</created><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author><author><keyname>Jalden</keyname><forenames>Joakim</forenames></author></authors><title>Decentralized Detection in Energy Harvesting Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a decentralized hypothesis testing problem in which several
peripheral energy harvesting sensors are arranged in parallel. Each sensor
makes a noisy observation of a time varying phenomenon, and sends a message
about the present hypothesis towards a fusion center at each time instance t.
The fusion center, using the aggregate of the received messages during the time
instance t, makes a decision about the state of the present hypothesis. We
assume that each sensor is an energy harvesting device and is capable of
harvesting all the energy it needs to communicate from its environment. Our
contribution is to formulate and analyze the decentralized detection problem
when the energy harvesting sensors are allowed to form a long term energy usage
policy. Our analysis is based on a queuing-theoretic model for the battery.
Then, by using numerical simulations, we show how the resulting performance
differs from the energy unconstrained case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00611</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00611</id><created>2016-03-02</created><authors><author><keyname>L&#xf6;ber</keyname><forenames>Jakob</forenames></author></authors><title>Exactly realizable desired trajectories</title><categories>math.OC cs.SY math.DS nlin.SI</categories><comments>10 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trajectory tracking of nonlinear dynamical systems with affine open-loop
controls is investigated. The control task is to enforce the system state to
follow a prescribed desired trajectory as closely as possible. We introduce
exactly realizable desired trajectories as these trajectories which can be
tracked exactly by an appropriate control. Exactly realizable trajectories are
characterized mathematically by means of Moore-Penrose projectors constructed
from the input matrix. The approach leads to differential-algebraic systems of
equations and is considerably simpler than the related concept of system
inversion. Furthermore, we identify a particularly simple class of nonlinear
affine control systems. Systems in this class satisfy the so-called linearizing
assumption and share many properties with linear control systems. For example,
conditions for controllability can be formulated in terms of a rank condition
for a controllability matrix analogously to the Kalman rank condition for
linear time-invariant systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00619</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00619</id><created>2016-03-02</created><authors><author><keyname>Lin</keyname><forenames>Yixiao</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Li</keyname><forenames>Shuting</forenames></author></authors><title>Porting Code Across Simple Mobile Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The StarL programming framework aims to simplify development of distributed
robotic applications by providing easy-to-use language constructs for
communication and control. It has been used to develop applications such as
formation control, distributed tracking, and collaborative search. In this
paper, we present a complete redesign of the StarL language and its runtime
system which enables us to achieve portability of robot programs across
platforms. Thus, the same application program, say, for distributed tracking,
can now be compiled and deployed on multiple, heterogeneous robotic platforms.
Towards portability, this we first define the semantics of StarL programs in a
way that is largely platform independent, except for a few key
platform-dependent parameters that capture the worst-case execution and sensing
delays and resolution of sensors. Next, we present a design of the StarL
runtime system, including a robot controller, that meets the above semantics.
The controller consists of a platform-independent path planner implemented
using RRTs and a platform-dependent way-point tracker that is implemented using
the control commands available for the platform. We demonstrate portability of
StarL applications using simulation results for two different robotic
platforms, and several applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00621</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00621</id><created>2016-03-02</created><authors><author><keyname>Browet</keyname><forenames>Arnaud</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Sarlette</keyname><forenames>Alain</forenames></author></authors><title>Incompatibility boundaries for properties of community partitions</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the incompatibility of certain desirable properties of community
partition quality functions. Our results generalize the impossibility result of
[Kleinberg 2003] by considering sets of weaker properties. In particular, we
use an alternative notion to solve the central issue of the consistency
property. (The latter means that modifying the graph in a way consistent with a
partition should not have counterintuitive effects). Our results clearly show
that community partition methods should not be expected to perfectly satisfy
all ideally desired properties.
  We then proceed to show that this incompatibility no longer holds when
slightly relaxed versions of the properties are considered, and we provide in
fact examples of simple quality functions satisfying these relaxed properties.
An experimental study of these quality functions shows a behavior comparable to
established methods in some situations, but more debatable results in others.
This suggests that defining a notion of good partition in communities probably
requires imposing additional properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00622</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00622</id><created>2016-03-02</created><authors><author><keyname>Kahn</keyname><forenames>Gregory</forenames></author><author><keyname>Zhang</keyname><forenames>Tianhao</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>PLATO: Policy Learning using Adaptive Trajectory Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy search can in principle acquire complex strategies for control of
robots, self-driving vehicles, and other autonomous systems. When the policy is
trained to process raw sensory inputs, such as images and depth maps, it can
acquire a strategy that combines perception and control. However, effectively
processing such complex inputs requires an expressive policy class, such as a
large neural network. These high-dimensional policies are difficult to train,
especially when training must be done for safety-critical systems. We propose
PLATO, an algorithm that trains complex control policies with supervised
learning, using model-predictive control (MPC) to generate the supervision.
PLATO uses an adaptive training method to modify the behavior of MPC to
gradually match the learned policy, in order to generate training samples at
states that are likely to be visited by the policy while avoiding highly
undesirable on-policy actions. We prove that this type of adaptive MPC expert
produces supervision that leads to good long-horizon performance of the
resulting policy, and empirically demonstrate that MPC can still avoid
dangerous on-policy actions in unexpected situations during training. Compared
to prior methods, our empirical results demonstrate that PLATO learns faster
and often converges to a better solution on a set of challenging simulated
experiments involving autonomous aerial vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00636</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00636</id><created>2016-03-02</created><authors><author><keyname>Grov</keyname><forenames>Gudmund</forenames></author><author><keyname>Ireland</keyname><forenames>Andrew</forenames></author><author><keyname>Llano</keyname><forenames>Maria Teresa</forenames></author><author><keyname>Kovacs</keyname><forenames>Peter</forenames></author><author><keyname>Colton</keyname><forenames>Simon</forenames></author><author><keyname>Gow</keyname><forenames>Jeremy</forenames></author></authors><title>Semi-Automated Design Space Exploration for Formal Modelling</title><categories>cs.SE cs.LO</categories><comments>14 pages. Long version of a short paper to be published at ABZ 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refinement based formal methods allow the modelling of systems through
incremental steps via abstraction. Discovering the right levels of abstraction,
formulating correct and meaningful invariants, and analysing faulty models are
some of the challenges faced when using this technique. Here, we propose Design
Space Exploration, an approach that aims to assist a designer by automatically
providing high-level modelling guidance in real-time. More specifically,
through the combination of common patterns of modelling with techniques from
automated theory formation and automated reasoning, different design
alternatives are explored and suitable models that deal with faults are
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00644</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00644</id><created>2016-03-02</created><authors><author><keyname>Meng</keyname><forenames>Ya</forenames></author><author><keyname>Li</keyname><forenames>Liping</forenames></author><author><keyname>Hu</keyname><forenames>Yanjun</forenames></author></authors><title>A Novel Interleaving Scheme for Polar Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It's known that the bit errors of polar codes with successive cancellation
(SC) decoding are coupled. We call the coupled information bits the correlated
bits. In this paper, concatenation schemes are studied for polar codes (as
inner codes) and LDPC codes (as outer codes). In a conventional concatenation
scheme, to achieve a better BER performance, one can divide all $N_l$ bits in a
LDPC block into $N_l$ polar blocks to completely de-correlate the possible
coupled errors. In this paper, we propose a novel interleaving scheme between a
LDPC code and a polar code which breaks the correlation of the errors among the
correlated bits. This interleaving scheme still keeps the simple SC decoding of
polar codes while achieves a comparable BER performance at a much smaller delay
compared with a $N_l$-block delay scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00646</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00646</id><created>2016-03-02</created><authors><author><keyname>An</keyname><forenames>Jisun</forenames></author><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>De Oger</keyname><forenames>Sonia Alonso Saenz</forenames></author><author><keyname>Fortes</keyname><forenames>Braulio Gomez</forenames></author></authors><title>Are you Charlie or Ahmed? Cultural pluralism in Charlie Hebdo response
  on Twitter</title><categories>cs.CY</categories><comments>International AAAI Conference on Web and Social Media (ICWSM), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the response to the Charlie Hebdo shootings of January 7, 2015 on
Twitter across the globe. We ask whether the stances on the issue of freedom of
speech can be modeled using established sociological theories, including
Huntington's culturalist Clash of Civilizations, and those taking into
consideration social context, including Density and Interdependence theories.
We find support for Huntington's culturalist explanation, in that the
established traditions and norms of one's &quot;civilization&quot; predetermine some of
one's opinion. However, at an individual level, we also find social context to
play a significant role, with non-Arabs living in Arab countries using
#JeSuisAhmed (&quot;I am Ahmed&quot;) five times more often when they are embedded in a
mixed Arab/non-Arab (mention) network. Among Arabs living in the West, we find
a great variety of responses, not altogether associated with the size of their
expatriate community, suggesting other variables to be at play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00648</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00648</id><created>2016-03-02</created><authors><author><keyname>Upadhya</keyname><forenames>Karthik</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Vehkapera</keyname><forenames>Mikko</forenames></author></authors><title>Superimposed Pilots are Superior for Mitigating Pilot Contamination in
  Massive MIMO - Part I: Theory and Channel Estimation</title><categories>cs.IT math.IT</categories><comments>31 pages, 5 figures, Submitted to IEEE Trans. Signal Processing on
  November 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superimposed pilots are shown, in this two-part paper, to be a superior
alternative to time-multiplexed pilot and data symbols for mitigating pilot
contamination in massive multiple-input multiple-output (MIMO) systems.
Provided that the number of symbols transmitted in the uplink time-slot is
larger than the total number of users in the system, superimposed pilots allow
for each user to be assigned a unique pilot sequence. This property, along with
larger time-averaging owing to longer length pilotsequences, allows for a
significant reduction in pilot contamination. In this part of the two-part
paper, superimposed pilots are introduced as an approach for mitigating pilot
contamination and it is shown that a hybrid system with users utilizing both
time-multiplexed pilots and superimposed pilots is superior to an optimally
designed system that only employs conventional pilots. In addition, we describe
an iterative data-aided method for uplink channel estimation that offers a
significantly better performance. Approximate expressions for the uplink
signal-to-interference-plus-noise ratio are also provided for this method.
Numerical simulations validate the approximations and the performance of the
proposed method while demonstrating its superiority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00649</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00649</id><created>2016-03-02</created><updated>2016-03-03</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Peter</forenames></author><author><keyname>Schwerhoff</keyname><forenames>Malte</forenames></author><author><keyname>Summers</keyname><forenames>Alexander J.</forenames></author></authors><title>Automatic Verification of Iterated Separating Conjunctions using
  Symbolic Execution</title><categories>cs.PL cs.LO</categories><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In permission logics such as separation logic, the iterated separating
conjunction is a quantifier denoting access permission to an unbounded set of
heap locations. In contrast to recursive predicates, iterated separating
conjunctions do not prescribe a structure on the locations they range over, and
so do not restrict how to traverse and modify these locations. This flexibility
is important for the verification of random-access data structures such as
arrays and data structures that can be traversed in multiple ways such as
graphs. Despite its usefulness, no automatic program verifier natively supports
iterated separating conjunctions; they are especially difficult to incorporate
into symbolic execution engines, the prevalent technique for building verifiers
for these logics.
  In this paper, we present the first symbolic execution technique to support
general iterated separating conjunctions. We propose a novel representation of
symbolic heaps and flexible support for logical specifications that quantify
over heap locations. Our technique exhibits predictable and fast performance
despite employing quantifiers at the SMT level, by carefully controlling
quantifier instantiations. It is compatible with other features of permission
logics such as fractional permissions, abstract predicates, and abstraction
functions. Our technique is implemented as an extension of the Viper
verification infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00652</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00652</id><created>2016-03-02</created><authors><author><keyname>Fromm</keyname><forenames>Tobias</forenames></author><author><keyname>Birk</keyname><forenames>Andreas</forenames></author></authors><title>Physics-Based Damage-Aware Manipulation Strategy Planning Using Scene
  Dynamics Anticipation</title><categories>cs.RO</categories><comments>submitted to IROS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a damage-aware planning approach which determines the best
sequence to manipulate a number of objects in a scene. This works on
task-planning level, abstracts from motion planning and anticipates the
dynamics of the scene using a physics simulation. Instead of avoiding
interaction with the environment, we take unintended motion of other objects
into account and plan manipulation sequences which minimize the potential
damage. Our method can also be used as a validation measure to judge planned
motions for their feasibility in terms of damage avoidance. We evaluate our
approach on one industrial scenario (autonomous container unloading) and one
retail scenario (shelf replenishment).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00656</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00656</id><created>2016-03-02</created><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Pipe</keyname><forenames>Tony</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Model-Based Testing, Using Belief-Desire-Intentions Agents, of Control
  Code for Robots in Collaborative Human-Robot Interactions</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The software of robotic assistants needs to be verified, to ensure its safety
and functional correctness. Testing in simulation allows a high degree of
realism in the verification. However, generating tests that cover both
interesting foreseen and unforeseen scenarios in human-robot interaction (HRI)
tasks, while executing most of the code, remains a challenge. We propose the
use of belief-desire-intention (BDI) agents in the test environment, to
increase the level of realism and human-like stimulation of simulated robots.
Artificial intelligence, such as agent theory, can be exploited for more
intelligent test generation. An automated testbench was implemented for a
simulation in Robot Operating System (ROS) and Gazebo, of a cooperative table
assembly task between a humanoid robot and a person. Requirements were verified
for this task, and some unexpected design issues were discovered, leading to
possible code improvements. Our results highlight the practicality of BDI
agents to automatically generate valid and human-like tests to get high code
coverage, compared to hand-written directed tests, pseudorandom generation, and
other variants of model-based test generation. Also, BDI agents allow the
coverage of combined behaviours of the HRI system with more ease than writing
temporal logic properties for model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00658</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00658</id><created>2016-03-02</created><authors><author><keyname>Praveen</keyname><forenames>M.</forenames></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames></author></authors><title>Nesting Depth of Operators in Graph Database Queries: Expressiveness Vs.
  Evaluation Complexity</title><categories>cs.LO cs.DB cs.FL</categories><acm-class>F.1.3; F.4.3; H.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing query languages for graph structured data is an active field of
research, where expressiveness and efficient algorithms for query evaluation
are conflicting goals. To better handle dynamically changing data, recent work
has been done on designing query languages that can compare values stored in
the graph database, without hard coding the values in the query. The main idea
is to allow variables in the query and bind the variables to values when
evaluating the query. For query languages that bind variables only once, query
evaluation is usually NP-complete. There are query languages that allow binding
inside the scope of Kleene star operators, which can themselves be in the scope
of bindings and so on. Uncontrolled nesting of binding and iteration within one
another results in query evaluation being PSPACE-complete.
  We define a way to syntactically control the nesting depth of iterated
bindings, and study how this affects expressiveness and efficiency of query
evaluation. The result is an infinite, syntactically defined hierarchy of
expressions. We prove that the corresponding language hierarchy is strict.
Given an expression in the hierarchy, we prove that it undecidable to check if
there is a language equivalent expression at lower levels. We prove that
evaluating a query based on an expression at level i can be done in $\Sigma_i$
in the polynomial time hierarchy. Satisfiability of quantified Boolean formulas
can be reduced to query evaluation; we study the relationship between
alternations in Boolean quantifiers and the depth of nesting of iterated
bindings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00663</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00663</id><created>2016-03-02</created><authors><author><keyname>Fromm</keyname><forenames>Tobias</forenames></author><author><keyname>Mueller</keyname><forenames>Christian A.</forenames></author><author><keyname>Birk</keyname><forenames>Andreas</forenames></author></authors><title>Unsupervised Watertight Mesh Generation for Physics Simulation
  Applications Using Growing Neural Gas on Noisy Free-Form Object Models</title><categories>cs.RO</categories><comments>submitted to IROS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework to generate watertight mesh representations in an
unsupervised manner from noisy point clouds of complex, heterogeneous objects
with free-form surfaces. The resulting meshes are ready to use in applications
like kinematics and dynamics simulation where watertightness and fast
processing are the main quality criteria. This works with no necessity of user
interaction, mainly by utilizing a modified Growing Neural Gas technique for
surface reconstruction combined with several post-processing steps. In contrast
to existing methods, the proposed framework is able to cope with input point
clouds generated by consumer-grade RGBD sensors and works even if the input
data features large holes, e.g. a missing bottom which was not covered by the
sensor. Additionally, we explain a method to unsupervisedly optimize the
parameters of our framework in order to improve generalization quality and, at
the same time, keep the resulting meshes as coherent as possible to the
original object regarding visual and geometric properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00671</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00671</id><created>2016-03-02</created><updated>2016-03-06</updated><authors><author><keyname>Silva</keyname><forenames>Jose Mairton B. da</forenames><suffix>Jr</suffix></author><author><keyname>Xu</keyname><forenames>Yuzhe</forenames></author><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Distributed Spectral Efficiency Maximization in Full-Duplex Cellular
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 3 figures, accepted in IEEE ICC 2016 - Workshop on Novel
  Medium Access and Resource Allocation for 5G Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-node full-duplex is a promising new transmission mode between a
full-duplex capable wireless node and two other wireless nodes that use
half-duplex transmission and reception respectively. Although three-node
full-duplex transmissions can increase the spectral efficiency without
requiring full-duplex capability of user devices, inter-node interference - in
addition to the inherent self-interference - can severely degrade the
performance. Therefore, as methods that provide effective self-interference
mitigation evolve, the management of inter-node interference is becoming
increasingly important. This paper considers a cellular system in which a
full-duplex capable base station serves a set of half-duplex capable users. As
the spectral efficiencies achieved by the uplink and downlink transmissions are
inherently intertwined, the objective is to device channel assignment and power
control algorithms that maximize the weighted sum of the uplink-downlink
transmissions. To this end a distributed auction based channel assignment
algorithm is proposed, in which the scheduled uplink users and the base station
jointly determine the set of downlink users for full-duplex transmission.
Realistic system simulations indicate that the spectral efficiency can be up to
89% better than using the traditional half-duplex mode. Furthermore, when the
self-interference cancelling level is high, the impact of the user-to-user
interference is severe unless properly managed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00682</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00682</id><created>2016-03-02</created><authors><author><keyname>Shu</keyname><forenames>Liangsuo</forenames></author><author><keyname>Liu</keyname><forenames>Xiaokang</forenames></author><author><keyname>Jin</keyname><forenames>Shiping</forenames></author><author><keyname>Huang</keyname><forenames>Suyi</forenames></author></authors><title>Maxwell's demon and information channel width of a black hole</title><categories>quant-ph cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a new generalized second law of thermodynamics developed in the
thermodynamics of information, the information and entropy of a black hole and
its accretion disk was analyzed respectively. We find a bound of the
information channel width of black hole, which is determined by the variation
rate of the horizon temperature and the mass of the black hole's shell (the
accretion disk close to the horizon).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00696</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00696</id><created>2016-03-02</created><updated>2016-03-03</updated><authors><author><keyname>Paruma-Pab&#xf3;n</keyname><forenames>Oscar Hern&#xe1;n</forenames></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Fabio A.</forenames></author><author><keyname>Aponte</keyname><forenames>Jairo</forenames></author><author><keyname>Camargo</keyname><forenames>Jorge E.</forenames></author><author><keyname>Restrepo-Calle</keyname><forenames>Felipe</forenames></author></authors><title>Finding Relationships between Socio-Technical Aspects and Personality
  Traits by Mining Developer E-mails</title><categories>cs.SE</categories><comments>7 pages, ICSE 2016 - Workshop on Cooperative and Human Aspects of
  Software Engineering (CHASE)</comments><doi>10.1145/2897586.2897611</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personality traits influence most, if not all, of the human activities, from
those as natural as the way people walk, talk, dress and write to those most
complex as the way they interact with others. Most importantly, personality
influences the way people make decisions including, in the case of developers,
the criteria they consider when selecting a software project they want to
participate. Most of the works that study the influence of social, technical
and human factors in software development projects have been focused on the
impact of communications in software quality. For instance, on identifying
predictors to detect files that may contain bugs before releasing an enhanced
version of a software product. Only a few of these works focus on the analysis
of personality traits of developers with commit permissions (committers) in
Free/Libre and Open-Source Software projects and their relationship with the
software artifacts they interact with. This paper presents an approach, based
on the automatic recognition of personality traits from e-mails sent by
committers in FLOSS projects, to uncover relationships between the social and
technical aspects that occur during the software development process. Our
experimental results suggest the existence of some relationships among
personality traits projected by the committers through their e-mails and the
social (communication) and technical activities they undertake. This work is a
preliminary study aimed at supporting the setting up of efficient work teams in
software development projects based on an appropriate mix of stakeholders
taking into account their personality traits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00707</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00707</id><created>2016-03-02</created><authors><author><keyname>Itkin</keyname><forenames>Eyal</forenames></author><author><keyname>Wool</keyname><forenames>Avishai</forenames></author></authors><title>A Security Analysis and Revised Security Extension for the Precision
  Time Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Precision Time Protocol (PTP) aims to provide highly accurate and
synchronised clocks. Its defining standard, IEEE 1588, has a security section
(&quot;Annex K&quot;) which relies on symmetric-key secrecy. In this paper we present a
detailed threat analysis of the PTP standard, in which we highlight the
security properties that should be addressed by any security extension. During
this analysis we identify a sequence of new attacks and non-cryptographic
network-based defenses that mitigate them. We then suggest to replace Annex K's
symmetric cryptography by an efficient elliptic-curve Public-Key signatures. We
implemented all our attacks to demonstrate their effectiveness, and also
implemented and evaluated both the network and cryptographic defenses. Our
results show that the proposed schemes are extremely practical, and much more
secure than previous suggestions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00709</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00709</id><created>2016-03-02</created><authors><author><keyname>Ishak</keyname><forenames>Mouna Ben</forenames><affiliation>LARODEC</affiliation></author><author><keyname>Chulyadyo</keyname><forenames>Rajani</forenames><affiliation>LINA</affiliation></author><author><keyname>Leray</keyname><forenames>Philippe</forenames><affiliation>LINA</affiliation></author></authors><title>Probabilistic Relational Model Benchmark Generation</title><categories>cs.LG cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The validation of any database mining methodology goes through an evaluation
process where benchmarks availability is essential. In this paper, we aim to
randomly generate relational database benchmarks that allow to check
probabilistic dependencies among the attributes. We are particularly interested
in Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)
to a relational data mining context and enable effective and robust reasoning
over relational data. Even though a panoply of works have focused, separately ,
on the generation of random Bayesian networks and relational databases, no work
has been identified for PRMs on that track. This paper provides an algorithmic
approach for generating random PRMs from scratch to fill this gap. The proposed
method allows to generate PRMs as well as synthetic relational data from a
randomly generated relational schema and a random set of probabilistic
dependencies. This can be of interest not only for machine learning researchers
to evaluate their proposals in a common framework, but also for databases
designers to evaluate the effectiveness of the components of a database
management system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00713</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00713</id><created>2016-03-02</created><authors><author><keyname>Santoni</keyname><forenames>Christian</forenames></author><author><keyname>Salvati</keyname><forenames>Gabriele</forenames></author><author><keyname>Tibaldo</keyname><forenames>Valentina</forenames></author><author><keyname>Pellacini</keyname><forenames>Fabio</forenames></author></authors><title>LevelMerge: Collaborative Game Level Editing by Merging Labeled Graphs</title><categories>cs.GR</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game level editing is the process of constructing a full game level starting
from 3D asset libraries, e.g. 3d models, textures, shaders, scripts. In level
editing, designers define the look and behavior of the whole level by placing
objects, assigning materials and lighting parameters, setting animations and
physics properties and customizing the objects AI and behavior by editing
scripts. The heterogeneity of the task usually translates to a workflow where a
team of people, experts on separate aspects, cooperate to edit the game level,
often working on the same objects (e.g.: a programmer working on the AI of a
character, while an artist works on its 3D model or its materials). Today this
collaboration is established by using version control systems designed for text
documents, such as Git, to manage different versions and share them amongst
users. The merge algorithms used in these systems though does not perform well
in our case since it does not respect the relations between game objects
necessary to maintain the semantic of the game level behavior and look. This is
a known problem and commercial systems for game level merging exists, e.g.
PlasticSCM, but these are only slightly more robust than text-based ones. This
causes designers to often merge scenes manually, essentially reapplying others
edits in the game level editor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00740</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00740</id><created>2016-02-29</created><authors><author><keyname>Raz</keyname><forenames>Orit E.</forenames></author></authors><title>A note on distinct distances</title><categories>math.MG cs.CG math.CO</categories><comments>14 pages</comments><msc-class>52C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, for a constant-degree algebraic curve $\gamma$ in
$\mathbb{R}^D$, every set of $n$ points on $\gamma$ spans at least
$\Omega(n^{4/3})$ distinct distances, unless $\gamma$ is an {\it algebraic
helix} (see Definition 1.1). This improves the earlier bound $\Omega(n^{5/4})$
of Charalambides [Discrete Comput. Geom. (2014)].
  We also show that, for every set $P$ of $n$ points that lie on a
$d$-dimensional constant-degree algebraic variety $V$ in $\mathbb{R}^D$, there
exists a subset $S\subset P$ of size at least
$\Omega(n^{\frac{4}{9+12(d-1)}})$, such that $S$ spans $\binom{|S|}{2}$
distinct distances. This improves the earlier bound of
$\Omega(n^{\frac{1}{3d}})$ of Conlon et al. [SIAM J. Discrete Math. (2015)].
  Both results are consequences of a common technical tool, given in Lemma 2.7
below.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00747</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00747</id><created>2016-02-18</created><authors><author><keyname>Kim</keyname><forenames>Yoongu</forenames></author><author><keyname>Daly</keyname><forenames>Ross</forenames></author><author><keyname>Kim</keyname><forenames>Jeremie</forenames></author><author><keyname>Fallin</keyname><forenames>Chris</forenames></author><author><keyname>Lee</keyname><forenames>Ji Hye</forenames></author><author><keyname>Lee</keyname><forenames>Donghyuk</forenames></author><author><keyname>Wilkerson</keyname><forenames>Chris</forenames></author><author><keyname>Lai</keyname><forenames>Konrad</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>RowHammer: Reliability Analysis and Security Implications</title><categories>cs.DC cs.CR</categories><comments>This is the summary of the paper titled &quot;Flipping Bits in Memory
  Without Accessing Them: An Experimental Study of DRAM Disturbance Errors&quot;
  which appeared in ISCA in June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As process technology scales down to smaller dimensions, DRAM chips become
more vulnerable to disturbance, a phenomenon in which different DRAM cells
interfere with each other's operation. For the first time in academic
literature, our ISCA paper exposes the existence of disturbance errors in
commodity DRAM chips that are sold and used today. We show that repeatedly
reading from the same address could corrupt data in nearby addresses. More
specifically: When a DRAM row is opened (i.e., activated) and closed (i.e.,
precharged) repeatedly (i.e., hammered), it can induce disturbance errors in
adjacent DRAM rows. This failure mode is popularly called RowHammer. We tested
129 DRAM modules manufactured within the past six years (2008-2014) and found
110 of them to exhibit RowHammer disturbance errors, the earliest of which
dates back to 2010. In particular, all modules from the past two years
(2012-2013) were vulnerable, which implies that the errors are a recent
phenomenon affecting more advanced generations of process technology.
Importantly, disturbance errors pose an easily-exploitable security threat
since they are a breach of memory protection, wherein accesses to one page
(mapped to one row) modifies the data stored in another page (mapped to an
adjacent row).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00748</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00748</id><created>2016-03-02</created><authors><author><keyname>Gu</keyname><forenames>Shixiang</forenames></author><author><keyname>Lillicrap</keyname><forenames>Timothy</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author></authors><title>Continuous Deep Q-Learning with Model-based Acceleration</title><categories>cs.LG cs.AI cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-free reinforcement learning has been successfully applied to a range of
challenging problems, and has recently been extended to handle large neural
network policies and value functions. However, the sample complexity of
model-free algorithms, particularly when using high-dimensional function
approximators, tends to limit their applicability to physical systems. In this
paper, we explore algorithms and representations to reduce the sample
complexity of deep reinforcement learning for continuous control tasks. We
propose two complementary techniques for improving the efficiency of such
algorithms. First, we derive a continuous variant of the Q-learning algorithm,
which we call normalized adantage functions (NAF), as an alternative to the
more commonly used policy gradient and actor-critic methods. NAF representation
allows us to apply Q-learning with experience replay to continuous tasks, and
substantially improves performance on a set of simulated robotic control tasks.
To further improve the efficiency of our approach, we explore the use of
learned models for accelerating model-free reinforcement learning. We show that
iteratively refitted local linear models are especially effective for this, and
demonstrate substantially faster learning on domains where such models are
applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00749</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00749</id><created>2016-03-02</created><updated>2016-03-03</updated><authors><author><keyname>Wang</keyname><forenames>Sinong</forenames></author><author><keyname>Liu</keyname><forenames>Fang</forenames></author><author><keyname>Shroff</keyname><forenames>Ness</forenames></author></authors><title>Non-additive Security Game</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strategically allocating resources to protect targets against potential
threats in an efficient way is a key challenge facing our society. From the
classical interdiction game to the recently proposed Stackelberg Security Game,
applying computational game models to the security domain has made a real-world
impact on numerous fields including military attack and defense, financial
system security, political campaign and civil safeguarding. However, existing
methods assume additive utility functions, which are unable to capture the
inherent dependencies that exist among different targets in current complex
networks.
  In this paper, we introduce a new security game model, called Non-additive
Security Game (NASG). It adopts a non-additive set function to assign the
utility to every subset of targets, which completely describes the internal
linkage structure of the game. However, both the number of utility functions
and the strategies exponentially increase in the number of targets, which poses
a significant challenge in developing algorithms to determine the equilibrium
strategy. To tackle this problem, we first reduce the NASG to an equivalent
zero-sum game and construct a low-rank perturbed game via matrix decomposition
and random low-dimensional embedding. Then, we incorporate the above low-rank
perturbed game into the Augmented Lagrangian and Coordinate Descent method.
Using a series of careful constructions, we show that our method has a total
complexity that is nearly linear in the number of utility functions and
achieves asymptotic zero error. To the best of our knowledge, the NASG is the
first computational game model to investigate dependencies among different
targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00751</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00751</id><created>2016-03-02</created><authors><author><keyname>Milosevic</keyname><forenames>Nikola</forenames></author></authors><title>Equity forecast: Predicting long term stock price movement using machine
  learning</title><categories>cs.LG q-fin.GN</categories><comments>8 pages, 3 tables, computational finance, algorithmic finance</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Long term investment is one of the major investment strategies. However,
calculating intrinsic value of some company and evaluating shares for long term
investment is not easy, since analyst have to care about a large number of
financial indicators and evaluate them in a right manner. So far, little help
in predicting the direction of the company value over the longer period of time
has been provided from the machines. In this paper we present a machine
learning aided approach to evaluate the equity's future price over the long
time. Our method is able to correctly predict whether some company's value will
be 10% higher or not over the period of one year in 76.5% of cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00759</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00759</id><created>2016-03-02</created><updated>2016-03-08</updated><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Ivkin</keyname><forenames>Nikita</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author><author><keyname>Wang</keyname><forenames>Zhengyu</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>BPTree: an $\ell_2$ heavy hitters algorithm using constant memory</title><categories>cs.DS</categories><comments>v3: fixed accidental mis-sorting of author last names; v2: added
  section explaining why pick-and-drop sampling fails for l2 heavy hitters, and
  fixed minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of finding heavy hitters is one of the best known and well studied
problems in the area of data streams. In sub-polynomial space, the strongest
guarantee available is the $\ell_2$ guarantee, which requires finding all items
that occur at least $\varepsilon\|f\|_2$ times in the stream, where the $i$th
coordinate of the vector $f$ is the number of occurrences of $i$ in the stream.
The first algorithm to achieve the $\ell_2$ guarantee was the CountSketch of
[CCF04], which for constant $\varepsilon$ requires $O(\log n)$ words of memory
and $O(\log n)$ update time, and is known to be space-optimal if the stream
allows for deletions. The recent work of [BCIW16] gave an improved algorithm
for insertion-only streams, using only $O(\log\log n)$ words of memory.
  In this work, we give an algorithm &quot;BPTree&quot; for $\ell_2$ heavy hitters in
insertion-only streams that achieves $O(1)$ words of memory and $O(1)$ update
time for constant $\varepsilon$, which is optimal. In addition, we describe an
algorithm for tracking $\|f\|_2$ at all times with $O(1)$ memory and update
time. Our analyses rely on bounding the expected supremum of a Bernoulli
process involving Rademachers with limited independence, which we accomplish
via a Dudley-like chaining argument that may have applications elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00762</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00762</id><created>2016-03-02</created><authors><author><keyname>Alahmadi</keyname><forenames>Adel</forenames></author><author><keyname>&#xd6;zdemir</keyname><forenames>Funda</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>On self-dual double circulant codes</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-dual double circulant codes of odd dimension are shown to be dihedral in
even characteristic and consta-dihedral in odd characteristic. Exact counting
formulae are derived for them and used to show they contain families of codes
with relative distance satisfying a modified Gilbert-Varshamov bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00770</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00770</id><created>2016-03-02</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Str&#xf8;mme</keyname><forenames>Torstein J. F.</forenames></author></authors><title>Vertex Cover Structural Parameterization Revisited</title><categories>cs.DS cs.CC cs.DM</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudoforest is a graph whose connected components have at most one cycle.
Let X be a pseudoforest modulator of graph G, i. e. a vertex subset of G such
that G-X is a pseudoforest. We show that Vertex Cover admits a polynomial
kernel being parameterized by the size of the pseudoforest modulator. In other
words, we provide a polynomial time algorithm that for an input graph G and
integer k, outputs a graph G' and integer k', such that G' has O(|X|12)
vertices and G has a vertex cover of size k if and only if G' has vertex cover
of size k'. We complement our findings by proving that there is no polynomial
kernel for Vertex Cover parameterized by the size of a modulator to a mock
forest (a graph where no cycles share a vertex) unless NP is a subset of
coNP/poly. In particular, this also rules out polynomial kernels when
parameterized by the size of a modulator to outerplanar and cactus graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00772</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00772</id><created>2016-03-02</created><authors><author><keyname>Naik</keyname><forenames>Azad</forenames></author><author><keyname>Rangwala</keyname><forenames>Huzefa</forenames></author></authors><title>Filter based Taxonomy Modification for Improving Hierarchical
  Classification</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale classification of data organized as a hierarchy of classes has
received significant attention in the literature. Top-Down (TD) Hierarchical
Classification (HC), which exploits the hierarchical structure during the
learning process is an effective method for dealing with problems at scale due
to its computational benefits. However, its accuracy suffers due to error
propagation i.e., prediction errors made at higher levels in the hierarchy
cannot be corrected at lower levels. One of the main reasons behind errors at
the higher levels is the presence of inconsistent nodes and links that are
introduced due to the arbitrary process of creating these hierarchies by domain
experts. In this paper, we propose two efficient data driven filter based
approaches for hierarchical structure modification: (i) Flattening (local and
global) approach that identifies and removes inconsistent nodes present within
the hierarchy and (ii) Rewiring approach modifies parent-child relationships to
improve the classification performance of learned models. Our extensive
empirical evaluation of the proposed approaches on several image and text
datasets shows improved performance over competing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00773</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00773</id><created>2016-03-01</created><authors><author><keyname>Mhanna</keyname><forenames>Sleiman</forenames></author><author><keyname>Verbic</keyname><forenames>Gregor</forenames></author><author><keyname>Chapman</keyname><forenames>Archie</forenames></author></authors><title>Tight LP Approximations for the Optimal Power Flow Problem</title><categories>math.OC cs.CE cs.SY</categories><comments>7 pages. To appear in Proc. 19th Power Syst. Comput. Conf. (PSCC),
  Genoa, Italy, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DC power flow approximations are ubiquitous in the electricity industry.
However, these linear approximations fail to capture important physical aspects
of power flow, such as the reactive power and voltage magnitude, which are
crucial in many applications to ensure voltage stability and AC solution
feasibility. This paper proposes two LP approximations of the AC optimal power
flow problem, founded on tight polyhedral approximations of the SOC
constraints, in the aim of retaining the good lower bounds of the SOCP
relaxation and relishing the computational efficiency of LP solvers. The high
accuracy of the two LP approximations is corroborated by rigorous computational
evaluations on systems with up to 9241 buses and different operating
conditions. The computational efficiency of the two proposed LP models is shown
to be comparable to, if not better than, that of the SOCP models in most
instances. This performance is ideal for MILP extensions of these LP models
since MILP is computationally more efficient than MIQCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00786</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00786</id><created>2016-03-02</created><authors><author><keyname>Peng</keyname><forenames>Nanyun</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author></authors><title>Learning Word Segmentation Representations to Improve Named Entity
  Recognition for Chinese Social Media</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named entity recognition, and other information extraction tasks, frequently
use linguistic features such as part of speech tags or chunkings. For languages
where word boundaries are not readily identified in text, word segmentation is
a key first step to generating features for an NER system. While using word
boundary tags as features are helpful, the signals that aid in identifying
these boundaries may provide richer information for an NER system. New
state-of-the-art word segmentation systems use neural models to learn
representations for predicting word boundaries. We show that these same
representations, jointly trained with an NER system, yield significant
improvements in NER for Chinese social media. In our experiments, jointly
training NER and word segmentation with an LSTM-CRF model yields nearly 5%
absolute improvement over previously published results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00788</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00788</id><created>2016-03-02</created><authors><author><keyname>Kucukelbir</keyname><forenames>Alp</forenames></author><author><keyname>Tran</keyname><forenames>Dustin</forenames></author><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Automatic Differentiation Variational Inference</title><categories>stat.ML cs.AI cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic modeling is iterative. A scientist posits a simple model, fits
it to her data, refines it according to her analysis, and repeats. However,
fitting complex models to large data is a bottleneck in this process. Deriving
algorithms for new models can be both mathematically and computationally
challenging, which makes it difficult to efficiently cycle through the steps.
To this end, we develop automatic differentiation variational inference (ADVI).
Using our method, the scientist only provides a probabilistic model and a
dataset, nothing else. ADVI automatically derives an efficient variational
inference algorithm, freeing the scientist to refine and explore many models.
ADVI supports a broad class of models-no conjugacy assumptions are required. We
study ADVI across ten different models and apply it to a dataset with millions
of observations. ADVI is integrated into Stan, a probabilistic programming
system; it is available for immediate use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00794</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00794</id><created>2016-03-02</created><authors><author><keyname>Hangl</keyname><forenames>Simon</forenames></author><author><keyname>Ugur</keyname><forenames>Emre</forenames></author><author><keyname>Szedmak</keyname><forenames>Sandor</forenames></author><author><keyname>Piater</keyname><forenames>Justus</forenames></author></authors><title>Hierarchical Haptic Manipulation for Complex Skill Learning</title><categories>cs.RO</categories><comments>submitted to iros2016 (http://www.iros2016.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex manipulation scenarios (e.g. tasks requiring complex interaction
of two hands or in-hand manipulation), generaliza- tion is a hard problem.
Current methods still either require a substantial amount of (supervised)
training data and / or strong assumptions on both the environment and the task.
In this paradigm, controllers solving these tasks tend to be complex. We
propose a paradigm of maintaining simpler controllers solving the task in a
small number of specific situations. In order to generalize to novel
situations, the robot transforms the environment from novel situations to a
situation where the solution of the task is already known. Our solution to this
problem is to play with objects and use previously trained skills (basis
skills). These skills can either be used for estimating or for changing the
current state of the environment and are organized in skill hierarchies. The
approach is evaluated in complex pick-and-place scenarios that involve complex
manipulation. We further show that these skills can be learned by autonomous
playing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00797</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00797</id><created>2016-03-02</created><updated>2016-03-07</updated><authors><author><keyname>Twomey</keyname><forenames>Niall</forenames></author><author><keyname>Diethe</keyname><forenames>Tom</forenames></author><author><keyname>Kull</keyname><forenames>Meelis</forenames></author><author><keyname>Song</keyname><forenames>Hao</forenames></author><author><keyname>Camplani</keyname><forenames>Massimo</forenames></author><author><keyname>Hannuna</keyname><forenames>Sion</forenames></author><author><keyname>Fafoutis</keyname><forenames>Xenofon</forenames></author><author><keyname>Zhu</keyname><forenames>Ni</forenames></author><author><keyname>Woznowski</keyname><forenames>Pete</forenames></author><author><keyname>Flach</keyname><forenames>Peter</forenames></author><author><keyname>Craddock</keyname><forenames>Ian</forenames></author></authors><title>The SPHERE Challenge: Activity Recognition with Multimodal Sensor Data</title><categories>cs.CY cs.HC</categories><comments>Paper describing dataset. 11 pages; 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines the Sensor Platform for HEalthcare in Residential
Environment (SPHERE) project and details the SPHERE challenge that will take
place in conjunction with European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery (ECML-PKDD) between March and
July 2016. The SPHERE challenge is an activity recognition competition where
predictions are made from video, accelerometer and environmental sensors.
Monetary prizes will be awarded to the top three entrants, with Euro 1,000
being awarded to the winner, Euro 600 being awarded to the first runner up, and
Euro 400 being awarded to the second runner up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00802</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00802</id><created>2016-03-02</created><authors><author><keyname>Tehrani-Saleh</keyname><forenames>Ali</forenames></author><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Flies as Ship Captains? Digital Evolution Unravels Selective Pressures
  to Avoid Collision in Drosophila</title><categories>q-bio.PE cs.CV nlin.AO q-bio.NC</categories><comments>8 pages, 10 figures, submitted to 15th Artificial Life conference
  (ALife 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flies that walk in a covered planar arena on straight paths avoid colliding
with each other, but which of the two flies stops is not random.
High-throughput video observations, coupled with dedicated experiments with
controlled robot flies have revealed that flies utilize the type of optic flow
on their retina as a determinant of who should stop, a strategy also used by
ship captains to determine which of two ships on a collision course should
throw engines in reverse. We use digital evolution to test whether this
strategy evolves when collision avoidance is the sole penalty. We find that the
strategy does indeed evolve in a narrow range of cost/benefit ratios, for
experiments in which the &quot;regressive motion&quot; cue is error free. We speculate
that these stringent conditions may not be sufficient to evolve the strategy in
real flies, pointing perhaps to auxiliary costs and benefits not modeled in our
study
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00806</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00806</id><created>2016-03-02</created><authors><author><keyname>Strub</keyname><forenames>Florian</forenames><affiliation>SEQUEL, CRIStAL</affiliation></author><author><keyname>Mary</keyname><forenames>Jeremie</forenames><affiliation>CRIStAL, SEQUEL</affiliation></author><author><keyname>Gaudel</keyname><forenames>Romaric</forenames><affiliation>LIFL</affiliation></author></authors><title>Hybrid Collaborative Filtering with Neural Networks Romaric Gaudel</title><categories>cs.IR cs.AI cs.NE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative Filtering aims at exploiting the feedback of users to provide
personalised recommendations. Such algorithms look for latent variables in a
large sparse matrix of ratings. They can be enhanced by adding side information
to tackle the well-known cold start problem. While Neu-ral Networks have
tremendous success in image and speech recognition, they have received less
attention in Collaborative Filtering. This is all the more surprising that
Neural Networks are able to discover latent variables in large and
heterogeneous datasets. In this paper, we introduce a Collaborative Filtering
Neural network architecture aka CFN which computes a non-linear Matrix
Factorization from sparse rating inputs and side information. We show
experimentally on the MovieLens and Douban dataset that CFN outper-forms the
state of the art and benefits from side information. We provide an
implementation of the algorithm as a reusable plugin for Torch, a popular
Neural Network framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00810</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00810</id><created>2016-03-02</created><authors><author><keyname>Costa-Juss&#xe0;</keyname><forenames>Marta R.</forenames></author><author><keyname>Fonollosa</keyname><forenames>Jos&#xe9; A. R.</forenames></author></authors><title>Character-based Neural Machine Translation</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Machine Translation (MT) has reached state-of-the-art results.
However, one of the main challenges that neural MT still faces is dealing with
very large vocabularies and morphologically rich languages. In this paper, we
propose a neural MT system using character-based embeddings in combination with
convolutional and highway layers to replace the standard lookup-based word
representations. The resulting unlimited-vocabulary and affix aware source word
embeddings are tested in a state-of-the-art neural MT based on an
attention-based bidirectional recurrent neural network. The proposed MT scheme
completely avoids the problem of unknown source words and provides improved
results even when the source language is not morphologically rich. The number
of target words is still limited by the standard word-based softmax output
layer. However the number of unknowns at the output of the translation network
is dramatically reduced (by a relative 66%) with a significant overall
improvement over both neural and phrase-based baselines. Improvements up to 3
BLEU points are obtained in the German-English WMT task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00812</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00812</id><created>2016-03-02</created><authors><author><keyname>Chen</keyname><forenames>Chaomei</forenames></author></authors><title>Grand Challenges in Measuring and Characterizing Scholarly Impact</title><categories>cs.DL cs.CY</categories><comments>6 pages</comments><acm-class>H.3; H.4; H.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The constantly growing body of scholarly knowledge of science, technology,
and humanities is an asset of the mankind. While new discoveries expand the
existing knowledge, they may simultaneously render some of it obsolete. It is
crucial for scientists and other stakeholders to keep their knowledge up to
date. Policy makers, decision makers, and the general public also need an
efficient communication of scientific knowledge. Several grand challenges
concerning the creation, adaptation, and diffusion of scholarly knowledge, and
advance quantitative and qualitative approaches to the study of scholarly
knowledge are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00814</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00814</id><created>2016-03-02</created><authors><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Sabato</keyname><forenames>Zachary</forenames></author><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author></authors><title>Active Requirement Mining of Bounded-Time Temporal Properties of
  Cyber-Physical Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses active learning to solve the problem of mining bounded-time
signal temporal requirements of cyber-physical systems or simply the
requirement mining problem. By utilizing robustness degree, we formulates the
requirement mining problem into two optimization problems, a parameter
synthesis problem and a falsification problem. We then propose a new active
learning algorithm called Gaussian Process Adaptive Confidence Bound (GP-ACB)
to help solving the falsification problem. We show theoretically that the
GP-ACB algorithm has a lower regret bound thus a larger convergence rate than
some existing active learning algorithms, such as GP-UCB. We finally illustrate
and apply our requirement mining algorithm on two case studies, the Ackley's
function and a real world automatic transmission model. The case studies show
that our mining algorithm with GP-ACB outperforms others, such as those based
on Nelder-Mead, by an average of 30% to 40%. Our results demonstrate that there
is a principled and efficient way of extracting requirements for complex
cyber-physical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00816</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00816</id><created>2016-03-02</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Holland</keyname><forenames>Daniel</forenames></author></authors><title>A Nonlinear Weighted Total Variation Image Reconstruction Algorithm for
  Electrical Capacitance Tomography</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the techniques of iterative soft thresholding on total variation
penalty and adaptive reweighted compressive sensing, a new iterative
reconstruction algorithm for electrical capacitance tomography (ECT) is
proposed. This algorithm encourages sharp changes in the ECT image and
overcomes the disadvantage of the $l_1$ minimization by equipping the total
variation an adaptive weighted depending on the reconstructed image. Moreover,
the nonlinear effect is also partially reduced due to the adoption of the
updated accurate sensitivity matrix. Simulation results show that it recovers
ECT images more precisely and therefore suitable for the imaging of multiphase
systems in industrial or medical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00831</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00831</id><created>2016-03-02</created><authors><author><keyname>Milan</keyname><forenames>Anton</forenames></author><author><keyname>Leal-Taixe</keyname><forenames>Laura</forenames></author><author><keyname>Reid</keyname><forenames>Ian</forenames></author><author><keyname>Roth</keyname><forenames>Stefan</forenames></author><author><keyname>Schindler</keyname><forenames>Konrad</forenames></author></authors><title>MOT16: A Benchmark for Multi-Object Tracking</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1504.01942</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standardized benchmarks are crucial for the majority of computer vision
applications. Although leaderboards and ranking tables should not be
over-claimed, benchmarks often provide the most objective measure of
performance and are therefore important guides for reseach.
  Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was
launched with the goal of collecting existing and new data and creating a
framework for the standardized evaluation of multiple object tracking methods.
The first release of the benchmark focuses on multiple people tracking, since
pedestrians are by far the most studied object in the tracking community. This
paper accompanies a new release of the MOTChallenge benchmark. Unlike the
initial release, all videos of MOT16 have been carefully annotated following a
consistent protocol. Moreover, it not only offers a significant increase in the
number of labeled boxes, but also provides multiple object classes beside
pedestrians and the level of visibility for every single object of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00838</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00838</id><created>2016-03-02</created><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author><author><keyname>Goncharov</keyname><forenames>Sergey</forenames></author></authors><title>Some Remarks on Conway and Iteration Theories</title><categories>cs.LO</categories><msc-class>68Q55, 18C10</msc-class><acm-class>F.3.2; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an axiomatization of Conway theories which yields,as a corollary,
a very concise axiomatization of iteration theories satisfying the functorial
implication for base morphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00841</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00841</id><created>2016-03-02</created><authors><author><keyname>Giraldo</keyname><forenames>Jhony</forenames></author><author><keyname>Salazar</keyname><forenames>Augusto</forenames></author></authors><title>Automatic segmentation of lizard spots using an active contour model</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animal biometrics is a challenging task. In the literature, many algorithms
have been used, e.g. penguin chest recognition, elephant ears recognition and
leopard stripes pattern recognition, but to use technology to a large extent in
this area of research, still a lot of work has to be done. One important target
in animal biometrics is to automate the segmentation process, so in this paper
we propose a segmentation algorithm for extracting the spots of Diploglossus
millepunctatus, an endangered lizard species. The automatic segmentation is
achieved with a combination of preprocessing, active contours and morphology.
The parameters of each stage of the segmentation algorithm are found using an
optimization procedure, which is guided by the ground truth. The results show
that automatic segmentation of spots is possible. A 78.37 % of correct
segmentation in average is reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00845</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00845</id><created>2016-03-02</created><authors><author><keyname>Pan</keyname><forenames>Junting</forenames></author><author><keyname>McGuinness</keyname><forenames>Kevin</forenames></author><author><keyname>Sayrol</keyname><forenames>Elisa</forenames></author><author><keyname>O'Connor</keyname><forenames>Noel</forenames></author><author><keyname>Giro-i-Nieto</keyname><forenames>Xavier</forenames></author></authors><title>Shallow and Deep Convolutional Networks for Saliency Prediction</title><categories>cs.CV cs.LG</categories><comments>Preprint of the paper accepted at 2016 IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR). Source code and models available at
  https://github.com/imatge-upc/saliency-2016-cvpr. Junting Pan and Kevin
  McGuinness contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prediction of salient areas in images has been traditionally addressed
with hand-crafted features based on neuroscience principles. This paper,
however, addresses the problem with a completely data-driven approach by
training a convolutional neural network (convnet). The learning process is
formulated as a minimization of a loss function that measures the Euclidean
distance of the predicted saliency map with the provided ground truth. The
recent publication of large datasets of saliency prediction has provided enough
data to train end-to-end architectures that are both fast and accurate. Two
designs are proposed: a shallow convnet trained from scratch, and a another
deeper solution whose first three layers are adapted from another network
trained for classification. To the authors knowledge, these are the first
end-to-end CNNs trained and tested for the purpose of saliency prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00847</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00847</id><created>2016-03-02</created><authors><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Maftuleac</keyname><forenames>Daniela</forenames></author><author><keyname>Owen</keyname><forenames>Megan</forenames></author></authors><title>Shortest Paths and Convex Hulls in 2D Complexes with Non-Positive
  Curvature</title><categories>cs.CG math.CO math.MG</categories><comments>26 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Globally non-positively curved, or CAT(0), polyhedral complexes arise in a
number of applications, including evolutionary biology and robotics. These
spaces have unique shortest paths and are composed of Euclidean polyhedra, yet
many properties of convex hulls in Euclidean space fail to transfer over. We
give examples of some such properties. For 2-dimensional CAT(0) polyhedral
complexes, we give polynomial-time algorithms for computing convex hulls using
linear programming and for answering shortest path queries from a given source
point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00856</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00856</id><created>2016-03-02</created><authors><author><keyname>Kearnes</keyname><forenames>Steven</forenames></author><author><keyname>McCloskey</keyname><forenames>Kevin</forenames></author><author><keyname>Berndl</keyname><forenames>Marc</forenames></author><author><keyname>Pande</keyname><forenames>Vijay</forenames></author><author><keyname>Riley</keyname><forenames>Patrick</forenames></author></authors><title>Molecular Graph Convolutions: Moving Beyond Fingerprints</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular &quot;fingerprints&quot; encoding structural information are the workhorse of
cheminformatics and machine learning in drug discovery applications. However,
fingerprint representations necessarily emphasize particular aspects of the
molecular structure while ignoring others, rather than allowing the model to
make data-driven decisions. We describe molecular graph convolutions, a novel
machine learning architecture for learning from undirected graphs, specifically
small molecules. Graph convolutions use a simple encoding of the molecular
graph (atoms, bonds, distances, etc.), allowing the model to take greater
advantage of information in the graph structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00858</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00858</id><created>2016-03-02</created><authors><author><keyname>Cechlarova</keyname><forenames>Katarina</forenames></author><author><keyname>Klaus</keyname><forenames>Bettina</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author></authors><title>Pareto optimal matchings of students to courses in the presence of
  prerequisites</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of allocating applicants to courses, where each
applicant has a subset of acceptable courses that she ranks in strict order of
preference. Each applicant and course has a capacity, indicating the maximum
number of courses and applicants they can be assigned to, respectively. We thus
essentially have a many-to-many bipartite matching problem with one-sided
preferences, which has applications to the assignment of students to optional
courses at a university.
  We consider additive preferences and lexicographic preferences as two means
of extending preferences over individual courses to preferences over bundles of
courses. We additionally focus on the case that courses have prerequisite
constraints: we will mainly treat these constraints as compulsory, but we also
allow alternative prerequisites. We further study the case where courses may be
corequisites.
  For these extensions to the basic problem, we present the following
algorithmic results, which are mainly concerned with the computation of Pareto
optimal matchings (POMs). Firstly, we consider compulsory prerequisites. For
additive preferences, we show that the problem of finding a POM is NP-hard. On
the other hand, in the case of lexicographic preferences we give a
polynomial-time algorithm for finding a POM. However we show that the problem
of deciding whether a given matching is Pareto optimal is co-NP-complete. We
further prove that finding a maximum cardinality (Pareto optimal) matching is
NP-hard. Under alternative prerequisites, we show that finding a POM is
NP-hard. Finally we consider corequisites. We prove that, as in the case of
compulsory prerequisites, finding a POM is NP-hard for additive preferences,
though solvable in polynomial time for lexicographic preferences. In the latter
case, the problem of finding a maximum cardinality POM is NP-hard and very
difficult to approximate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00859</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00859</id><created>2016-03-02</created><updated>2016-03-04</updated><authors><author><keyname>Miller</keyname><forenames>Konstantin</forenames></author><author><keyname>Al-Tamimi</keyname><forenames>Abdel-Karim</forenames></author><author><keyname>Wolisz</keyname><forenames>Adam</forenames></author></authors><title>QoE-Based Low-Delay Live Streaming Using Throughput Predictions</title><categories>cs.MM cs.NI</categories><comments>Technical Report TKN-16-001, Telecommunication Networks Group,
  Technische Universitaet Berlin. This TR updated TR TKN-15-001</comments><report-no>TKN-16-001</report-no><acm-class>C.2.1; C.2.4; C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, HTTP-based adaptive streaming has become the de facto standard for
video streaming over the Internet. It allows clients to dynamically adapt media
characteristics to network conditions in order to ensure a high quality of
experience, that is, minimize playback interruptions, while maximizing video
quality at a reasonable level of quality changes. In the case of live
streaming, this task becomes particularly challenging due to the latency
constraints. The challenge further increases if a client uses a wireless
network, where the throughput is subject to considerable fluctuations.
Consequently, live streams often exhibit latencies of up to 30 seconds. In the
present work, we introduce an adaptation algorithm for HTTP-based live
streaming called LOLYPOP (Low-Latency Prediction-Based Adaptation) that is
designed to operate with a transport latency of few seconds. To reach this
goal, LOLYPOP leverages TCP throughput predictions on multiple time scales,
from 1 to 10 seconds, along with an estimate of the prediction error
distribution. In addition to satisfying the latency constraint, the algorithm
heuristically maximizes the quality of experience by maximizing the average
video quality as a function of the number of skipped segments and quality
transitions. In order to select an efficient prediction method, we studied the
performance of several time series prediction methods in IEEE 802.11 wireless
access networks. We evaluated LOLYPOP under a large set of experimental
conditions limiting the transport latency to 3 seconds, against a
state-of-the-art adaptation algorithm from the literature, called FESTIVE. We
observed that the average video quality is by up to a factor of 3 higher than
with FESTIVE. We also observed that LOLYPOP is able to reach a broader region
in the quality of experience space, and thus it is better adjustable to the
user profile or service provider requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00892</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00892</id><created>2016-03-02</created><authors><author><keyname>Mrk&#x161;i&#x107;</keyname><forenames>Nikola</forenames></author><author><keyname>S&#xe9;aghdha</keyname><forenames>Diarmuid &#xd3;</forenames></author><author><keyname>Thomson</keyname><forenames>Blaise</forenames></author><author><keyname>Ga&#x161;i&#x107;</keyname><forenames>Milica</forenames></author><author><keyname>Rojas-Barahona</keyname><forenames>Lina</forenames></author><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Counter-fitting Word Vectors to Linguistic Constraints</title><categories>cs.CL cs.LG</categories><comments>Paper accepted for the 15th Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a novel counter-fitting method which injects
antonymy and synonymy constraints into vector space representations in order to
improve the vectors' capability for judging semantic similarity. Applying this
method to publicly available pre-trained word vectors leads to a new state of
the art performance on the SimLex-999 dataset. We also show how the method can
be used to tailor the word vector space for the downstream task of dialogue
state tracking, resulting in robust improvements across different dialogue
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00893</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00893</id><created>2016-03-02</created><updated>2016-03-08</updated><authors><author><keyname>Dong</keyname><forenames>Boxiang</forenames></author><author><keyname>Wang</keyname><forenames>Hui Wendy</forenames></author></authors><title>Frequency-hiding Dependency-preserving Encryption for Outsourced
  Databases</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud paradigm enables users to outsource their data to computationally
powerful third-party service providers for data management. Many data
management tasks rely on the data dependencies in the outsourced data. This
raises an important issue of how the data owner can protect the sensitive
information in the outsourced data while preserving the data dependencies. In
this paper, we consider functional dependency FD, an important type of data
dependency. We design a FD-preserving encryption scheme, named F2, that enables
the service provider to discover the FDs from the encrypted dataset. We
consider the frequency analysis attack, and show that the F2 encryption scheme
can defend against the attack under Kerckhoff's principle with provable
guarantee. Our empirical study demonstrates the efficiency and effectiveness of
F2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00897</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00897</id><created>2016-03-02</created><authors><author><keyname>Liaw</keyname><forenames>Christopher</forenames></author><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>A simple tool for bounding the deviation of random matrices on geometric
  sets</title><categories>math.PR cs.IT math.IT</categories><comments>17 pages</comments><msc-class>60B20 (primary), 60D05, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be an isotropic, sub-gaussian $m \times n$ matrix. We prove that the
process $Z_x := \|Ax\|_2 - \sqrt m \|x\|_2$ has sub-gaussian increments. Using
this, we show that for any bounded set $T\subseteq \mathbb{R}^n$, the deviation
of $\|Ax\|_2$ around its mean is uniformly bounded by the Gaussian complexity
of $T$. We also prove a local version of this theorem, which allows for
unbounded sets. These theorems have various applications, some of which are
reviewed in this paper. In particular, we give a new result regarding model
selection in the constrained linear model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00898</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00898</id><created>2016-03-02</created><authors><author><keyname>Gual&#xe0;</keyname><forenames>Luciano</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Tauraso</keyname><forenames>Roberto</forenames></author></authors><title>Large Peg-Army Maneuvers</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its long history, the classical game of peg solitaire continues to
attract the attention of the scientific community. In this paper, we consider
two problems with an algorithmic flavour which are related with this game,
namely Solitaire-Reachability and Solitaire-Army. In the first one, we show
that deciding whether there is a sequence of jumps which allows a given initial
configuration of pegs to reach a target position is NP-complete. Regarding
Solitaire-Army, the aim is to successfully deploy an army of pegs in a given
region of the board in order to reach a target position. By solving an
auxiliary problem with relaxed constraints, we are able to answer some open
questions raised by Cs\'ak\'any and Juh\'asz (Mathematics Magazine, 2000).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00910</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00910</id><created>2016-03-02</created><authors><author><keyname>Mones</keyname><forenames>Enys</forenames></author><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author><author><keyname>Hupert</keyname><forenames>Nathaniel</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>Vaccination and Complex Social Dynamics</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vaccination and outbreak monitoring are essential tools for preventing and
minimizing outbreaks of infectious diseases. Targeted strategies, where the
individuals most important for monitoring or preventing outbreaks are selected
for intervention, offer a possibility to significantly improve these measures.
Although targeted strategies carry a strong potential, identifying optimal
target groups remains a challenge. Here we consider the problem of identifying
target groups based on digital communication networks (telecommunication,
online social media) in order to predict and contain an infectious disease
spreading on a real-world person-to-person network of more than 500
individuals. We show that target groups for efficient outbreak monitoring can
be determined based on both telecommunication and online social network
information. In case of vaccination the information regarding the digital
communication networks improves the efficacy for short-range disease
transmissions but, surprisingly, performance is severely reduced in the case of
long-range transmission. These results are robust with respect to the strategy
used to identify targeted individuals and time-gap between identification of
targets and the intervention. Thus, we demonstrate that data available from
telecommunication and online social networks can greatly improve epidemic
control measures, but it is important to consider the details of the pathogen
spreading mechanism when such policies are applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00912</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00912</id><created>2016-03-02</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Yongun</forenames></author></authors><title>LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based
  Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the task of separating ground points from airborne LiDAR
point cloud data in urban areas. A novel ground filtering method using scan
line segmentation is proposed here, which we call SLSGF. It utilizes the scan
line information in LiDAR data to segment the LiDAR data. The similarity
measurements are designed to make it possible to segment complex roof
structures into a single segment as much as possible so the topological
relationships between the roof and the ground are simpler, which will benefit
the labeling process. In the labeling process, the initial ground segments are
detected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011
are used to test the accuracy of SLSGF; and our analytical and experimental
results show that this method is computationally-efficient and
noise-insensitive, thereby making a denoising process unnecessary before
filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00913</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00913</id><created>2016-03-02</created><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Sridhar</keyname><forenames>Anirudh</forenames></author></authors><title>Client-CASH: Protecting Master Passwords against Offline Attacks</title><categories>cs.CR</categories><comments>ASIA CCS 2016. Full Version</comments><doi>10.1145/2897845.2897876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline attacks on passwords are increasingly commonplace and dangerous. An
offline adversary is limited only by the amount of computational resources he
or she is willing to invest to crack a user's password. The danger is
compounded by the existence of authentication servers who fail to adopt proper
password storage practices like key-stretching. Password managers can help
mitigate these risks by adopting key stretching procedures like hash iteration
or memory hard functions to derive site specific passwords from the user's
master password on the client-side. While key stretching can reduce the offline
adversary's success rate, these procedures also increase computational costs
for a legitimate user. Motivated by the observation that most of the password
guesses of the offline adversary will be incorrect, we propose a client side
cost asymmetric secure hashing scheme (Client-CASH). Client-CASH randomizes the
runtime of client-side key stretching procedure in a way that the expected
computational cost of our key derivation function is greater when run with an
incorrect master password. We make several contributions. First, we show how to
introduce randomness into a client-side key stretching algorithms through the
use of halting predicates which are selected randomly at the time of account
creation. Second, we formalize the problem of finding the optimal running time
distribution subject to certain cost constraints for the client and certain
security constrains on the halting predicates. Finally, we demonstrate that
Client-CASH can reduce the adversary's success rate by up to $21\%$. These
results demonstrate the promise of the Client-CASH mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00928</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00928</id><created>2016-03-02</created><authors><author><keyname>Almanza</keyname><forenames>Matteo</forenames></author><author><keyname>Leucci</keyname><forenames>Stefano</forenames></author><author><keyname>Panconesi</keyname><forenames>Alessandro</forenames></author></authors><title>Trainyard is NP-Hard</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, due to the widespread diffusion of smart-phones, mobile puzzle
games have experienced a huge increase in their popularity. A successful puzzle
has to be both captivating and challenging, and it has been suggested that this
features are somehow related to their computational complexity \cite{Eppstein}.
Indeed, many puzzle games --such as Mah-Jongg, Sokoban, Candy Crush, and 2048,
to name a few-- are known to be NP-hard \cite{CondonFLS97,
culberson1999sokoban, GualaLN14, Mehta14a}. In this paper we consider
Trainyard: a popular mobile puzzle game whose goal is to get colored trains
from their initial stations to suitable destination stations. We prove that the
problem of determining whether there exists a solution to a given Trainyard
level is NP-hard. We also \href{http://trainyard.isnphard.com}{provide} an
implementation of our hardness reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00930</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00930</id><created>2016-03-02</created><authors><author><keyname>Summerville</keyname><forenames>Adam</forenames></author><author><keyname>Mateas</keyname><forenames>Michael</forenames></author></authors><title>Super Mario as a String: Platformer Level Generation Via LSTMs</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The procedural generation of video game levels has existed for at least 30
years, but only recently have machine learning approaches been used to generate
levels without specifying the rules for generation. A number of these have
looked at platformer levels as a sequence of characters and performed
generation using Markov chains. In this paper we examine the use of Long
Short-Term Memory recurrent neural networks (LSTMs) for the purpose of
generating levels trained from a corpus of Super Mario Brothers levels. We
analyze a number of different data representations and how the generated levels
fit into the space of human authored Super Mario Brothers levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00939</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00939</id><created>2016-03-02</created><authors><author><keyname>Rossi</keyname><forenames>Federico</forenames></author><author><keyname>Zhang</keyname><forenames>Rick</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Autonomous Vehicle Routing in Congested Road Networks</title><categories>cs.MA</categories><comments>11 pages, 3 figures. Working paper, in preparation for journal
  submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of routing and rebalancing a shared fleet of
autonomous (i.e., self-driving) vehicles providing on-demand mobility within a
capacitated transportation network, where congestion might disrupt throughput.
We model the problem within a network flow framework and show that under
relatively mild assumptions the rebalancing vehicles, if properly coordinated,
do not lead to an increase in congestion (in stark contrast to common belief).
From an algorithmic standpoint, such theoretical insight suggests that the
problem of routing customers and rebalancing vehicles can be decoupled, which
leads to a computationally-efficient routing and rebalancing algorithm for the
autonomous vehicles. Numerical experiments and case studies corroborate our
theoretical insights and show that the proposed algorithm outperforms
state-of-the-art point-to-point methods by avoiding excess congestion on the
road. Collectively, this paper provides a rigorous approach to the problem of
congestion-aware, system-wide coordination of autonomously driving vehicles,
and to the characterization of the sustainability of such robotic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00944</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00944</id><created>2016-03-02</created><authors><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Qiu</keyname><forenames>Shijie</forenames></author><author><keyname>Kong</keyname><forenames>Youyong</forenames></author><author><keyname>Jiang</keyname><forenames>Longyu</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>PCANet: An energy perspective</title><categories>cs.CV</categories><comments>37 pages, 23 figures, 16 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principal component analysis network (PCANet), which is one of the
recently proposed deep learning architectures, achieves the state-of-the-art
classification accuracy in various databases. However, the explanation of the
PCANet is lacked. In this paper, we try to explain why PCANet works well from
energy perspective point of view based on a set of experiments. The impact of
various parameters on the error rate of PCANet is analyzed in depth. It was
found that this error rate is correlated with the logarithm of energy of image.
The proposed energy explanation approach can be used as a testing method for
checking if every step of the constructed networks is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00954</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00954</id><created>2016-03-02</created><updated>2016-03-03</updated><authors><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Training Input-Output Recurrent Neural Networks through Spectral Methods</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of training input-output recurrent neural networks
(RNN) for sequence labeling tasks. We propose a novel spectral approach for
learning the network parameters. It is based on decomposition of the
cross-moment tensor between the output and a non-linear transformation of the
input, based on score functions. We guarantee consistent learning with
polynomial sample and computational complexity under transparent conditions
such as non-degeneracy of model parameters, polynomial activations for the
neurons, and a Markovian evolution of the input sequence. We also extend our
results to Bidirectional RNN which uses both previous and future information to
output the label at each time point, and is employed in many NLP tasks such as
POS tagging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00955</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00955</id><created>2016-03-02</created><authors><author><keyname>Tamjidi</keyname><forenames>Amirhossein</forenames></author><author><keyname>Chakravorty</keyname><forenames>Suman</forenames></author><author><keyname>Shell</keyname><forenames>Dylan</forenames></author></authors><title>Decentralized State Estimation via a Hybrid of Consensus and Covariance
  intersection</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new recursive information consensus filter for
decentralized dynamic-state estimation. No structure is assumed about the
topology of the network and local estimators are assumed to have access only to
local information. The network need not be connected at all times. Consensus
over priors which might become correlated is performed through Covariance
Intersection (CI) and consensus over new information is handled using weights
based on a Metropolis Hastings Markov Chains. We establish bounds for
estimation performance and show that our method produces unbiased conservative
estimates that are better than CI. The performance of the proposed method is
evaluated and compared with competing algorithms on an atmospheric dispersion
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00957</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00957</id><created>2016-03-02</created><authors><author><keyname>Xu</keyname><forenames>Kun</forenames></author><author><keyname>Feng</keyname><forenames>Yansong</forenames></author><author><keyname>Reddy</keyname><forenames>Siva</forenames></author><author><keyname>Huang</keyname><forenames>Songfang</forenames></author><author><keyname>Zhao</keyname><forenames>Dongyan</forenames></author></authors><title>Enhancing Freebase Question Answering Using Textual Evidence</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing knowledge-based question answering systems often rely on small
annotated training data. While shallow methods like information extraction
techniques are robust to data scarcity, they are less expressive than deep
understanding methods, thereby failing at answering questions involving
multiple constraints. Here we alleviate this problem by empowering a relation
extraction method with additional evidence from Wikipedia. We first present a
novel neural network based relation extractor to retrieve the candidate answers
from Freebase, and then develop a refinement model to validate answers using
Wikipedia. We achieve 53.3 F1 on WebQuestions, a substantial improvement over
the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00960</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00960</id><created>2016-03-02</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author></authors><title>Cellular Automata Segmentation of the Boundary between the Compacta of
  Vertebral Bodies and Surrounding Structures</title><categories>cs.CV cs.CG cs.GR</categories><comments>6 pages, 5 figures, 1 table, 42 references</comments><journal-ref>SPIE Medical Imaging Conference 2016, Paper 9787-52</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the aging population, spinal diseases get more and more common
nowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white women
and 13% for white men in the United States. Thus the numbers of surgical spinal
procedures are also increasing with the aging population and precise diagnosis
plays a vital role in reducing complication and recurrence of symptoms. Spinal
imaging of vertebral column is a tedious process subjected to interpretation
errors. In this contribution, we aim to reduce time and error for vertebral
interpretation by applying and studying the GrowCut-algorithm for boundary
segmentation between vertebral body compacta and surrounding structures.
GrowCut is a competitive region growing algorithm using cellular automata. For
our study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans were
first manually outlined by neurosurgeons. Then, the vertebral bodies were
segmented in the medical images by a GrowCut-trained physician using the
semi-automated GrowCut-algorithm. Afterwards, results of both segmentation
processes were compared using the Dice Similarity Coefficient (DSC) and the
Hausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of
18.91+/-7.2 voxel, respectively. In addition, the times have been measured
during the manual and the GrowCut segmentations, showing that a
GrowCut-segmentation - with an average time of less than six minutes
(5.77+/-0.73) - is significantly shorter than a pure manual outlining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00961</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00961</id><created>2016-03-02</created><authors><author><keyname>L&#xfc;ddemann</keyname><forenames>Tobias</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via
  User-Defined Templates</title><categories>cs.CV cs.GR</categories><comments>6 pages, 4 figures, 1 table, 43 references</comments><journal-ref>SPIE Medical Imaging Conference 2016, Paper 9784-113</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among all types of cancer, gynecological malignancies belong to the 4th most
frequent type of cancer among women. Besides chemotherapy and external beam
radiation, brachytherapy is the standard procedure for the treatment of these
malignancies. In the progress of treatment planning, localization of the tumor
as the target volume and adjacent organs of risks by segmentation is crucial to
accomplish an optimal radiation distribution to the tumor while simultaneously
preserving healthy tissue. Segmentation is performed manually and represents a
time-consuming task in clinical daily routine. This study focuses on the
segmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological
brachytherapy. The proposed segmentation method uses an interactive,
graph-based segmentation scheme with a user-defined template. The scheme
creates a directed two dimensional graph, followed by the minimal cost closed
set computation on the graph, resulting in an outlining of the rectum. The
graphs outline is dynamically adapted to the last calculated cut. Evaluation
was performed by comparing manual segmentations of the rectum/sigmoid colon to
results achieved with the proposed method. The comparison of the algorithmic to
manual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,
in comparison to 83.97+/-8.08% for the comparison of two manual segmentations
of the same physician. Utilizing the proposed methodology resulted in a median
time of 128 seconds per dataset, compared to 300 seconds needed for pure manual
segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00963</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00963</id><created>2016-03-02</created><authors><author><keyname>Campbell</keyname><forenames>Newton</forenames><suffix>Jr</suffix></author></authors><title>Using Quadrilaterals to Compute the Shortest Path</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new heuristic for the A* algorithm that references a data
structure significantly smaller than that of ALT. We characterize the behavior
of this new heuristic based on a dual landmark configuration that leverages
quadrilateral inequalities to identify the lower bound for shortest path. Using
this approach, we demonstrate both the utility and detriments of using polygon
inequalities aside from the triangle inequality to establish lower bounds for
shortest path queries. While this new heuristic does not dominate previous
heuristics based on triangle inequalities, the inverse is true, as well.
Further, we demonstrate that an A* heuristic function does not necessarily
outperform another heuristic that it dominates. In comparison to other landmark
methods, the new heuristic maintains a larger average search space while
commonly decreasing the number of computed arithmetic operations. The new
heuristic can significantly outperform previous methods, particularly in graphs
with larger path lengths. The characterization of the use of these inequalities
for bounding offers insight into its applications in other theoretical spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00964</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00964</id><created>2016-03-02</created><authors><author><keyname>Zeng</keyname><forenames>Zhen</forenames></author><author><keyname>Kuipers</keyname><forenames>Benjamin</forenames></author></authors><title>Learning Tabletop Object Manipulation by Imitation</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to enable robot to learn tabletop object manipulation by imitation.
Given external observations of demonstrations on object manipulations, we
believe that two underlying problems to address in learning by imitation is 1)
segment a given demonstration into skills that can be individually learned and
reused, and 2) formulate the correct RL (Reinforcement Learning) problem that
only considers the relevant aspects of each skill so that the policy for each
skill can be effectively learned. Previous works made certain progress in this
direction, but none has taken private information into account. The public
information is the information that is available in the external observations
of demonstration, and the private information is the information that are only
available to the agent that executes the actions, such as tactile sensations.
Our contribution is that we provide a method for the robot to automatically
segment the demonstration into multiple skills, and formulate the correct RL
problem for each skill, and automatically decide whether the private
information is an important aspect of each skill based on interaction with the
world. Our motivating example is for a real robot to play the shape sorter game
by imitating other's behavior, and we will show the results in a simulated 2D
environment that captures the important properties of the shape sorter game.
The evaluation is based on whether the demonstration is reasonably segmented,
and whether the correct RL problems are formulated. In the end, we will show
that robot can imitate the demonstrated behavior based on learned policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00968</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00968</id><created>2016-03-02</created><authors><author><keyname>Zhang</keyname><forenames>Ye</forenames></author><author><keyname>Roller</keyname><forenames>Stephen</forenames></author><author><keyname>Wallace</keyname><forenames>Byron</forenames></author></authors><title>MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for
  Sentence Classification</title><categories>cs.CL</categories><comments>This paper got accepted by NAACL 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel, simple convolution neural network (CNN) architecture -
multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of
word embeddings for sentence classification. MGNC-CNN extracts features from
input embedding sets independently and then joins these at the penultimate
layer in the network to form a final feature vector. We then adopt a group
regularization strategy that differentially penalizes weights associated with
the subcomponents generated from the respective embedding sets. This model is
much simpler than comparable alternative architectures and requires
substantially less training time. Furthermore, it is flexible in that it does
not require input word embeddings to be of the same dimensionality. We show
that MGNC-CNN consistently outperforms baseline models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00973</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00973</id><created>2016-03-03</created><authors><author><keyname>Friggstad</keyname><forenames>Zachary</forenames></author><author><keyname>Zhang</keyname><forenames>Yifeng</forenames></author></authors><title>Tight Analysis of a Multiple-Swap Heuristic for Budgeted Red-Blue Median</title><categories>cs.DS</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Budgeted Red-Blue Median is a generalization of classic $k$-Median in that
there are two sets of facilities, say $\mathcal{R}$ and $\mathcal{B}$, that can
be used to serve clients located in some metric space. The goal is to open
$k_r$ facilities in $\mathcal{R}$ and $k_b$ facilities in $\mathcal{B}$ for
some given bounds $k_r, k_b$ and connect each client to their nearest open
facility in a way that minimizes the total connection cost.
  We extend work by Hajiaghayi, Khandekar, and Kortsarz [2012] and show that a
multiple-swap local search heuristic can be used to obtain a
$(5+\epsilon)$-approximation for Budgeted Red-Blue Median for any constant
$\epsilon &gt; 0$. This is an improvement over their single swap analysis and
beats the previous best approximation guarantee of 8 by Swamy [2014].
  We also present a matching lower bound showing that for every $p \geq 1$,
there are instances of Budgeted Red-Blue Median with local optimum solutions
for the $p$-swap heuristic whose cost is $5 + \Omega\left(\frac{1}{p}\right)$
times the optimum solution cost. Thus, our analysis is tight up to the lower
order terms. In particular, for any $\epsilon &gt; 0$ we show the single-swap
heuristic admits local optima whose cost can be as bad as $7-\epsilon$ times
the optimum solution cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00975</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00975</id><created>2016-03-03</created><authors><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author></authors><title>Formalising Confluence in PVS</title><categories>cs.LO cs.PL</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><acm-class>F.4.2;D.3.1</acm-class><journal-ref>EPTCS 204, 2016, pp. 11-17</journal-ref><doi>10.4204/EPTCS.204.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confluence is a critical property of computational systems which is related
with determinism and non ambiguity and thus with other relevant computational
attributes of functional specifications and rewriting system as termination and
completion. Several criteria have been explored that guarantee confluence and
their formalisations provide further interesting information. This work
discusses topics and presents personal positions and views related with the
formalisation of confluence properties in the Prototype Verification System PVS
developed at our research group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00976</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00976</id><created>2016-03-03</created><authors><author><keyname>Soboci&#x144;ski</keyname><forenames>Pawe&#x142;</forenames><affiliation>University of Southampton</affiliation></author></authors><title>Compositional model checking of concurrent systems, with Petri nets</title><categories>cs.LO cs.FL cs.SE</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 204, 2016, pp. 19-30</journal-ref><doi>10.4204/EPTCS.204.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compositionality and process equivalence are both standard concepts of
process algebra. Compositionality means that the behaviour of a compound system
relies only on the behaviour of its components, i.e. there is no emergent
behaviour. Process equivalence means that the explicit statespace of a system
takes a back seat to its interaction patterns: the information that an
environment can obtain though interaction.
  Petri nets are a classical, yet widely used and understood, model of
concurrency. Nevertheless, they have often been described as a
non-compositional model, and tools tend to deal with monolithic,
globally-specified models.
  This tutorial paper concentrates on Petri Nets with Boundaries (PNB): a
compositional, graphical algebra of 1-safe nets, and its applications to
reachability checking within the tool Penrose. The algorithms feature the use
of compositionality and process equivalence, a powerful combination that can be
harnessed to improve the performance of checking reachability and coverability
in several common examples where Petri nets model realistic concurrent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00977</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00977</id><created>2016-03-03</created><authors><author><keyname>Amani</keyname><forenames>Mahdi</forenames><affiliation>Universit&#xe0; di Pisa, Pisa, Italy</affiliation></author><author><keyname>Nowzari-Dalini</keyname><forenames>Abbas</forenames><affiliation>UT, Tehran, Iran</affiliation></author></authors><title>Generation, Ranking and Unranking of Ordered Trees with Degree Bounds</title><categories>cs.CC cs.DM cs.DS</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><acm-class>G.2.2</acm-class><journal-ref>EPTCS 204, 2016, pp. 31-45</journal-ref><doi>10.4204/EPTCS.204.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of generating, ranking and unranking of unlabeled
ordered trees whose nodes have maximum degree of &#xce;&#x94;. This class of trees
represents a generalization of chemical trees. A chemical tree is an unlabeled
tree in which no node has degree greater than 4. By allowing up to &#xce;&#x94; children
for each node of chemical tree instead of 4, we will have a generalization of
chemical trees. Here, we introduce a new encoding over an alphabet of size 4
for representing unlabeled ordered trees with maximum degree of &#xce;&#x94;. We use this
encoding for generating these trees in A-order with constant average time and
O(n) worst case time. Due to the given encoding, with a precomputation of size
and time O(n^2) (assuming &#xce;&#x94; is constant), both ranking and unranking algorithms
are also designed taking O(n) and O(nlogn) time complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00978</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00978</id><created>2016-03-03</created><authors><author><keyname>Haeusler</keyname><forenames>Edward Hermann</forenames><affiliation>Puc-Rio</affiliation></author></authors><title>Finiteness and Computation in Toposes</title><categories>cs.LO</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><acm-class>F.1.1;F.4.1</acm-class><journal-ref>EPTCS 204, 2016, pp. 61-77</journal-ref><doi>10.4204/EPTCS.204.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some notions in mathematics can be considered relative. Relative is a term
used to denote when the variation in the position of an observer implies
variation in properties or measures on the observed object. We know, from
Skolem theorem, that there are first-order models where the set of real numbers
is countable and some where it is not. This fact depends on the position of the
observer and on the instrument/language the obserevr uses as well, i.e., it
depends on whether he/she is inside the model or not and in this particular
case the use of first-order logic. In this article, we assume that computation
is based on finiteness rather than natural numbers and discuss Turing machines
computable morphisms defined on top of the sole notion finiteness. We explore
the relativity of finiteness in models provided by toposes where the Axiom of
Choice (AC) does not hold, since Tarski proved that if AC holds then all
finiteness notions are equivalent. Our toposes do not have natural numbers
object (NNO) either, since in a topos with a NNO these finiteness notions are
equivalent to Peano finiteness going back to computation on top of Natural
Numbers. The main contribution of this article is to show that although from
inside every topos, with the properties previously stated, the computation
model is standard, from outside some of these toposes, unexpected properties on
the computation arise, e.g., infinitely long programs, finite computations
containing infinitely long ones, infinitely branching computations. We mainly
consider Dedekind and Kuratowski notions of finiteness in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00979</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00979</id><created>2016-03-03</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames><affiliation>Universidad Eafit, Colombia</affiliation></author><author><keyname>Philippou</keyname><forenames>Anna</forenames><affiliation>University of Cyprus, Cyprus</affiliation></author><author><keyname>Arboleda</keyname><forenames>Sair</forenames><affiliation>Universidad de Antioquia, Colombia</affiliation></author><author><keyname>Puerta</keyname><forenames>Mar&#xed;a</forenames><affiliation>Universidad Eafit, Colombia</affiliation></author><author><keyname>S.</keyname><forenames>Carlos M. V&#xe9;lez</forenames><affiliation>Universidad Eafit, Colombia</affiliation></author></authors><title>Mean-Field Semantics for a Process Calculus for Spatially-Explicit
  Ecological Models</title><categories>cs.LO</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 204, 2016, pp. 79-94</journal-ref><doi>10.4204/EPTCS.204.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a mean-field semantics for S-PALPS, a process calculus for
spatially-explicit, individual-based modeling of ecological systems. The new
semantics of S-PALPS allows an interpretation of the average behavior of a
system as a set of recurrence equations. Recurrence equations are a useful
approximation when dealing with a large number of individuals, as it is the
case in epidemiological studies. As a case study, we compute a set of
recurrence equations capturing the dynamics of an individual-based model of the
transmission of dengue in Bello (Antioquia), Colombia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00980</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00980</id><created>2016-03-03</created><authors><author><keyname>Kanji</keyname><forenames>Tanaka</forenames></author></authors><title>Local Map Descriptor for Compressive Change Retrieval</title><categories>cs.RO</categories><comments>8 pages, 7 figures, Draft of a paper submitted to an International
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change detection, i.e., anomaly detection from local maps built by a mobile
robot at multiple different times, is a challenging problem to solve in
practice. Most previous work either cannot be applied to scenarios where the
size of the map collection is large, or simply assumed that the robot
self-location is globally known. In this paper, we tackle the problem of
simultaneous self-localization and change detection, by reformulating the
problem as a map retrieval problem, and propose a local map descriptor with a
compressed bag-of-words (BoW) structure as a scalable solution. We make the
following contributions. (1) To enable a direct comparison of the spatial
layout of visual features between different local maps, the origin of the local
map coordinate (termed &quot;viewpoint&quot;) is planned by scene parsing and determined
by our &quot;viewpoint planner&quot; to be invariant against small variations in
self-location and changes, aiming at providing similar viewpoints for similar
scenes (i.e., the relevant map pair). (2) We extend the BoW model to enable the
use of not only the appearance (e.g., polestar) but also the spatial layout
(e.g., spatial pyramid) of visual features with respect to the planned
viewpoint. The key observation is that the planned viewpoint (i.e., the origin
of local map coordinate) acts as a pseudo viewpoint that is usually required by
spatial BoW (e.g., SPM) and also by anomaly detection (e.g., NN-d, LOF). (3)
Experimental results on a challenging &quot;loop-closing&quot; scenario show that the
proposed method outperforms previous BoW methods in self-localization, and
furthermore, that the use of both appearance and pose information in change
detection produces better results than the use of either information alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00982</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00982</id><created>2016-03-03</created><authors><author><keyname>Chung</keyname><forenames>Yu-An</forenames></author><author><keyname>Wu</keyname><forenames>Chao-Chung</forenames></author><author><keyname>Shen</keyname><forenames>Chia-Hao</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author></authors><title>Unsupervised Learning of Audio Segment Representations using
  Sequence-to-sequence Recurrent Neural Networks</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing audio segments expressed with variable-length acoustic feature
sequences as fixed-length feature vectors is usually needed in many speech
applications, including speaker identification, audio emotion classification
and spoken term detection (STD). In this paper, we apply and extend
sequence-to-sequence learning framework to learn representations for audio
segments without any supervision. The model we used is called
Sequence-to-sequence Autoencoder (SA), which consists of two RNNs equipped with
Long Short-Term Memory (LSTM) units: the first RNN acts as an encoder that maps
the input sequence into a vector representation of fixed dimensionality, and
the second RNN acts as a decoder that maps the representation back to the input
sequence. The two RNNs are then jointly trained by minimizing the
reconstruction error. We further propose Denoising Sequence-to-sequence
Autoencoder (DSA) that improves the learned representations. The vector
representations learned by SA and DSA are shown to be very helpful for
query-by-example STD. The experimental results have shown that the proposed
models achieved better retrieval performance than using audio segment
representation designed heuristically and the classical Dynamic Time Warping
(DTW) approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00988</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00988</id><created>2016-03-03</created><updated>2016-03-05</updated><authors><author><keyname>Mhaskar</keyname><forenames>Hrushikesh</forenames></author><author><keyname>Liao</keyname><forenames>Qianli</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Learning Real and Boolean Functions: When Is Deep Better Than Shallow</title><categories>cs.LG</categories><comments>Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe computational tasks - especially in vision - that correspond to
compositional/hierarchical functions. While the universal approximation
property holds both for hierarchical and shallow networks, we prove that deep
(hierarchical) networks can approximate the class of compositional functions
with the same accuracy as shallow networks but with exponentially lower
VC-dimension as well as the number of training parameters. This leads to the
question of approximation by sparse polynomials (in the number of independent
parameters) and, as a consequence, by deep networks. We also discuss
connections between our results and learnability of sparse Boolean functions,
settling an old conjecture by Bengio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.00993</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.00993</id><created>2016-03-03</created><authors><author><keyname>Kanji</keyname><forenames>Tanaka</forenames></author></authors><title>Self-localization from Images with Small Overlap</title><categories>cs.CV</categories><comments>8 pages, 9 figures, Draft of a paper submitted to an International
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent success of visual features from deep convolutional neural
networks (DCNN) in visual robot self-localization, it has become important and
practical to address more general self-localization scenarios. In this paper,
we address the scenario of self-localization from images with small overlap. We
explicitly introduce a localization difficulty index as a decreasing function
of view overlap between query and relevant database images and investigate
performance versus difficulty for challenging cross-view self-localization
tasks. We then reformulate the self-localization as a scalable
bag-of-visual-features (BoVF) scene retrieval and present an efficient solution
called PCA-NBNN, aiming to facilitate fast and yet discriminative
correspondence between partially overlapping images. The proposed approach
adopts recent findings in discriminativity preserving encoding of DCNN features
using principal component analysis (PCA) and cross-domain scene matching using
naive Bayes nearest neighbor distance metric (NBNN). We experimentally
demonstrate that the proposed PCA-NBNN framework frequently achieves comparable
results to previous DCNN features and that the BoVF model is significantly more
efficient. We further address an important alternative scenario of
&quot;self-localization from images with NO overlap&quot; and report the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01006</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01006</id><created>2016-03-03</created><authors><author><keyname>Castro</keyname><forenames>F. M.</forenames></author><author><keyname>Marin-Jimenez</keyname><forenames>M. J.</forenames></author><author><keyname>Guil</keyname><forenames>N.</forenames></author><author><keyname>de la Blanca</keyname><forenames>N. Perez</forenames></author></authors><title>Automatic learning of gait signatures for people identification</title><categories>cs.CV cs.AI</categories><comments>Proof of concept paper. Technical report on the use of ConvNets (CNN)
  for gait recognition</comments><report-no>2016-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work targets people identification in video based on the way they walk
(i.e. gait). While classical methods typically derive gait signatures from
sequences of binary silhouettes, in this work we explore the use of
convolutional neural networks (CNN) for learning high-level descriptors from
low-level motion features (i.e. optical flow components). We carry out a
thorough experimental evaluation of the proposed CNN architecture on the
challenging TUM-GAID dataset. The experimental results indicate that using
spatio-temporal cuboids of optical flow as input data for CNN allows to obtain
state-of-the-art results on the gait task with an image resolution eight times
lower than the previously reported results (i.e. 80x60 pixels).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01016</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01016</id><created>2016-03-03</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Xu</keyname><forenames>Bangteng</forenames></author></authors><title>Nonlinear functions and difference sets on group actions</title><categories>math.CO cs.DM</categories><msc-class>05B10, 05E18, 65T50, 94E18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$, $H$ be finite groups and let $X$ be a finite $G$-set. $G$-perfect
nonlinear functions from $X$ to $H$ have been studied in several papers. They
have more interesting properties than perfect nonlinear functions from $G$
itself to $H$. By introducing the concept of a $(G, H)$-related difference
family of $X$, we obtain a characterization of $G$-perfect nonlinear functions
on $X$. When $G$ is abelian, we characterize a $G$-difference set of $X$ by the
Fourier transform on a normalized $G$-dual set $\widehat X$. We will also
investigate the existence and constructions of $G$-perfect nonlinear functions
and $G$-bent functions. Several known results in [2,6,10,17] are direct
consequences of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01022</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01022</id><created>2016-03-03</created><authors><author><keyname>Wu</keyname><forenames>Shanai</forenames></author><author><keyname>Shin</keyname><forenames>Yoan</forenames></author><author><keyname>Kim</keyname><forenames>Jin Young</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Analysis of the Packet Loss Probability in Energy Harvesting Cognitive
  Radio Networks</title><categories>math.PR cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Markovian battery model is proposed to provide the variation of energy
states for energy harvesting (EH) secondary users (SUs) in the EH cognitive
radio networks (CRN). Based on the proposed battery model, we derive the packet
loss probability in the EH SUs due to sensing inaccuracy and energy outage.
With the proposed analysis, the packet loss probability can easily be predicted
and utilized to optimize the transmission policy (i.e., opportunities for
successful transmission and EH) of EH SUs to improve their throughput.
Especially, the proposed method can be applied to upper layer (scheduling and
routing) optimization. To this end, we validate the proposed analysis through
Monte-Carlo simulation and show an agreement between the analysis and
simulations results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01025</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01025</id><created>2016-03-03</created><authors><author><keyname>Miyashita</keyname><forenames>Daisuke</forenames></author><author><keyname>Lee</keyname><forenames>Edward H.</forenames></author><author><keyname>Murmann</keyname><forenames>Boris</forenames></author></authors><title>Convolutional Neural Networks using Logarithmic Data Representation</title><categories>cs.NE cs.LG</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in convolutional neural networks have considered model
complexity and hardware efficiency to enable deployment onto embedded systems
and mobile devices. For example, it is now well-known that the arithmetic
operations of deep networks can be encoded down to 8-bit fixed-point without
significant deterioration in performance. However, further reduction in
precision down to as low as 3-bit fixed-point results in significant losses in
performance. In this paper we propose a new data representation that enables
state-of-the-art networks to be encoded to 3 bits with negligible loss in
classification performance. To perform this, we take advantage of the fact that
the weights and activations in a trained network naturally have non-uniform
distributions. Using non-uniform, base-2 logarithmic representation to encode
weights, communicate activations, and perform dot-products enables networks to
1) achieve higher classification accuracies than fixed-point at the same
resolution and 2) eliminate bulky digital multipliers. Finally, we propose an
end-to-end training procedure that uses log representation at 5-bits, which
achieves higher final test accuracy than linear at 5-bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01032</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01032</id><created>2016-03-03</created><updated>2016-03-04</updated><authors><author><keyname>Navarro</keyname><forenames>Javier Arias</forenames></author></authors><title>Right Ideals of a Ring and Sublanguages of Science</title><categories>cs.CL</categories><comments>Keywords: Zellig Sabbetai Harris, Information Structure of Language,
  Sublanguages of Science, Ideal Numbers, Ernst Kummer, Ideals, Richard
  Dedekind, Ring Theory, Right Ideals, Emmy Noether, Order Theory, Marshall
  Harvey Stone</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among Zellig Harris's numerous contributions to linguistics his theory of the
sublanguages of science probably ranks among the most underrated. However, not
only has this theory led to some exhaustive and meaningful applications in the
study of the grammar of immunology language and its changes over time, but it
also illustrates the nature of mathematical relations between chunks or subsets
of a grammar and the language as a whole. This becomes most clear when dealing
with the connection between metalanguage and language, as well as when
reflecting on operators.
  This paper tries to justify the claim that the sublanguages of science stand
in a particular algebraic relation to the rest of the language they are
embedded in, namely, that of right ideals in a ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01046</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01046</id><created>2016-03-03</created><authors><author><keyname>Helenius</keyname><forenames>Teemu</forenames></author><author><keyname>Siltanen</keyname><forenames>Samuli</forenames></author></authors><title>Photographic dataset: random peppercorns</title><categories>physics.data-an cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a photographic dataset collected for testing image processing
algorithms. The idea is to have sets of different but statistically similar
images. In this work the images show randomly distributed peppercorns. The
dataset is made available at www.fips.fi/photographic_dataset.php .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01055</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01055</id><created>2016-03-03</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Rong</keyname><forenames>Yifei</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Zhu</keyname><forenames>Tianchi</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofan</forenames></author></authors><title>Feedback Control of Real-Time Display Advertising</title><categories>cs.GT cs.SY</categories><comments>WSDM 2016</comments><doi>10.1145/2835776.2835843</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-Time Bidding (RTB) is revolutionising display advertising by
facilitating per-impression auctions to buy ad impressions as they are being
generated. Being able to use impression-level data, such as user cookies,
encourages user behaviour targeting, and hence has significantly improved the
effectiveness of ad campaigns. However, a fundamental drawback of RTB is its
instability because the bid decision is made per impression and there are
enormous fluctuations in campaigns' key performance indicators (KPIs). As such,
advertisers face great difficulty in controlling their campaign performance
against the associated costs. In this paper, we propose a feedback control
mechanism for RTB which helps advertisers dynamically adjust the bids to
effectively control the KPIs, e.g., the auction winning ratio and the effective
cost per click. We further formulate an optimisation framework to show that the
proposed feedback control mechanism also has the ability of optimising campaign
performance. By settling the effective cost per click at an optimal reference
value, the number of campaign's ad clicks can be maximised with the budget
constraint. Our empirical study based on real-world data verifies the
effectiveness and robustness of our RTB control system in various situations.
The proposed feedback control mechanism has also been deployed on a commercial
RTB platform and the online test has shown its success in generating
controllable advertising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01056</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01056</id><created>2016-03-03</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>A novel and automatic pectoral muscle identification algorithm for
  mediolateral oblique (MLO) view mammograms using ImageJ</title><categories>cs.CV</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pectoral muscle identification is often required for breast cancer risk
analysis, such as estimating breast density. Traditional methods are
overwhelmingly based on manual visual assessment or straight line fitting for
the pectoral muscle boundary, which are inefficient and inaccurate since
pectoral muscle in mammograms can have curved boundaries.
  This paper proposes a novel and automatic pectoral muscle identification
algorithm for MLO view mammograms. It is suitable for both scanned film and
full field digital mammograms. This algorithm is demonstrated using a public
domain software ImageJ. A validation of this algorithm has been performed using
real-world data and it shows promising result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01058</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01058</id><created>2016-03-03</created><authors><author><keyname>Vesti</keyname><forenames>Jetro</forenames></author></authors><title>Rich square-free words</title><categories>math.CO cs.FL</categories><comments>25 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A word w is rich if it has |w|+1 many distinct palindromic factors, including
the empty word. A word is square-free if it does not have a factor uu, where u
is a non-empty word.
  Pelantov\'a and Starosta (Discrete Math. 313 (2013)) proved that every
infinite rich word contains a square. We will give another proof for that
result. Pelantov\'a and Starosta denoted by r(n) the length of a longest rich
square-free word on an alphabet of size n. The exact value of r(n) was left as
an open question. We will give an upper and a lower bound for r(n), and make a
conjecture that our lower bound is exact.
  We will also generalize the notion of repetition threshold for a limited
class of infinite words. The repetition thresholds for episturmian and rich
words are left as an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01060</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01060</id><created>2016-03-03</created><authors><author><keyname>Carrea</keyname><forenames>Laura</forenames></author><author><keyname>Vernitski</keyname><forenames>Alexei</forenames></author><author><keyname>Reed</keyname><forenames>Martin</forenames></author></authors><title>Yes-no Bloom filter: A way of representing sets with fewer false
  positives</title><categories>cs.DS cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bloom filter (BF) is a space efficient randomized data structure
particularly suitable to represent a set supporting approximate membership
queries. BFs have been extensively used in many applications especially in
networking due to their simplicity and flexibility. The performances of BFs
mainly depends on query overhead, space requirements and false positives. The
aim of this paper is to focus on false positives. Inspired by the recent
application of the BF in a novel multicast forwarding fabric for information
centric networks, this paper proposes the yes-no BF, a new way of representing
a set, based on the BF, but with significantly lower false positives and no
false negatives. Although it requires slightly more processing at the stage of
its formation, it offers the same processing requirements for membership
queries as the BF. After introducing the yes-no BF, we show through
simulations, that it has better false positive performance than the BF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01067</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01067</id><created>2016-03-03</created><authors><author><keyname>Onal</keyname><forenames>Itir</forenames></author><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Mizrak</keyname><forenames>Eda</forenames></author><author><keyname>Oztekin</keyname><forenames>Ilke</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author></authors><title>Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain
  Decoding</title><categories>cs.LG cs.AI cs.CV</categories><comments>13 pages, 10 figures, submitted to JSTSP Special Issue on Advanced
  Signal Processing in Brain Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)
brain volumes recorded during a cognitive stimulus by a graph which consists of
a set of local meshes. The corresponding cognitive process, encoded in the
brain, is then represented by these meshes each of which is estimated assuming
a linear relationship among the voxel time series in a predefined locality.
First, we define the concept of locality in two neighborhood systems, namely,
the spatial and functional neighborhoods. Then, we construct spatially and
functionally local meshes around each voxel, called seed voxel, by connecting
it either to its spatial or functional p-nearest neighbors. The mesh formed
around a voxel is a directed sub-graph with a star topology, where the
direction of the edges is taken towards the seed voxel at the center of the
mesh. We represent the time series recorded at each seed voxel in terms of
linear combination of the time series of its p-nearest neighbors in the mesh.
The relationships between a seed voxel and its neighbors are represented by the
edge weights of each mesh, and are estimated by solving a linear regression
equation. The estimated mesh edge weights lead to a better representation of
information in the brain for encoding and decoding of the cognitive tasks. We
test our model on a visual object recognition and emotional memory retrieval
experiments using Support Vector Machines that are trained using the mesh edge
weights as features. In the experimental analysis, we observe that the edge
weights of the spatial and functional meshes perform better than the
state-of-the-art brain decoding models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01068</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01068</id><created>2016-03-03</created><authors><author><keyname>Baroffio</keyname><forenames>Luca</forenames></author><author><keyname>Bondi</keyname><forenames>Luca</forenames></author><author><keyname>Bestagini</keyname><forenames>Paolo</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>Camera identification with deep convolutional networks</title><categories>cs.CV cs.MM</categories><comments>submitted to ICIP '16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility of detecting which camera has been used to shoot a specific
picture is of paramount importance for many forensics tasks. This is extremely
useful for copyright infringement cases, ownership attribution, as well as for
detecting the authors of distributed illicit material (e.g., pedo-pornographic
shots). Due to its importance, the forensics community has developed a series
of robust detectors that exploit characteristic traces left by each camera on
the acquired images during the acquisition pipeline. These traces are
reverse-engineered in order to attribute a picture to a camera. In this paper,
we investigate an alternative approach to solve camera identification problem.
Indeed, we propose a data-driven algorithm based on convolutional neural
networks, which learns features characterizing each camera directly from the
acquired pictures. The proposed approach is tested on both instance-attribution
and model-attribution, providing an accuracy greater than 94% in discriminating
27 camera models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01076</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01076</id><created>2016-03-03</created><authors><author><keyname>Csurka</keyname><forenames>Gabriela</forenames></author><author><keyname>Larlus</keyname><forenames>Diane</forenames></author><author><keyname>Gordo</keyname><forenames>Albert</forenames></author><author><keyname>Almazan</keyname><forenames>Jon</forenames></author></authors><title>What is the right way to represent document images?</title><categories>cs.CV</categories><comments>Preprint of our Pattern Recognition submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the problem of document image representation based
on visual features. We propose a comprehensive experimental study that compares
three types of visual document image representations: (1) traditional so-called
shallow features, such as the RunLength and the Fisher-Vector descriptors, (2)
deep features based on Convolutional Neural Networks, and (3) features
extracted from hybrid architectures that take inspiration from the two previous
ones.
  We evaluate these features in several tasks (i.e. classification, clustering,
and retrieval) and in different setups (e.g. domain transfer) using several
public and in-house datasets. Our results show that deep features generally
outperform other types of features when there is no domain shift and the new
task is closely related to the one used to train the model. However, when a
large domain or task shift is present, the Fisher-Vector shallow features
generalize better and often obtain the best results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01080</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01080</id><created>2016-03-03</created><authors><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Zorzi</keyname><forenames>and Michele</forenames></author></authors><title>Spectrum Pooling in MmWave Networks: Opportunities, Challenges, and
  Enablers</title><categories>cs.IT cs.NI cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the intrinsic characteristics of mmWave technologies, we discuss
the possibility of an authorization regime that allows spectrum sharing between
multiple operators, also referred to as spectrum pooling. In particular,
considering user rate as the performance measure, we assess the benefit of
coordination among the networks of different operators, study the impact of
beamforming both at the base stations and at the user terminals, and analyze
the pooling performance at different frequency carriers. We also discuss the
enabling spectrum mechanisms, architectures, and protocols required to make
spectrum pooling work in real networks. Our initial results show that, from a
technical perspective, spectrum pooling at mmWave has the potential for a more
efficient spectrum use than a traditional exclusive spectrum allocation to a
single operator. However, further studies are needed in order to reach a
thorough understanding of this matter, and we hope that this paper will help
stimulate further research in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01082</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01082</id><created>2016-03-03</created><authors><author><keyname>Morse</keyname><forenames>Jeremy</forenames></author><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Lawry</keyname><forenames>Jonathan</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Towards the Specification of Adaptive Robotic Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread adoption of autonomous adaptive systems depends on provided
guarantees of safety and functional correctness, at both design time and
runtime. Specifying adaptive systems is cognitively difficult when their
aspects are in a large number and have complicated dependencies.
  We present a technique to construct and automatically explore a specification
for systems that can degrade and/or adapt, towards analysis at design time for
verification and validation. This technique combines and constructs sections of
a lattice (or Hasse diagram) of all the possible ordered system
degradations/adaptations of interest, limited by desirability or risk
thresholds. The lattice allows the designer to understand the different levels
and combinations of system degradations/adaptations. We use the lattices (or
sections) to systematically explore whether a system is able to fulfil its task
goals under a dynamic and uncertain environment, through probabilistic model
checking.
  We illustrate the proposed specification technique through a domestic robotic
assistant example. Systematically exploring the lattice allowed comparing the
probabilities of task success/failure, to determine which
degradation/adaptation combinations can be allowed in the final system
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01090</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01090</id><created>2016-03-03</created><authors><author><keyname>Kaljun</keyname><forenames>David</forenames></author><author><keyname>Petri&#x161;i&#x10d;</keyname><forenames>Joze</forenames></author><author><keyname>&#x17d;erovnik</keyname><forenames>Janez</forenames></author></authors><title>Using Newton's method to model a spatial light distribution of a LED
  with attached secondary optics</title><categories>cs.OH</categories><comments>submitted to Journal of Mecanical enginering (Strojni\v{s}ki vestnik,
  Ljubljana)</comments><msc-class>90C59</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In design of optical systems based on LED (Light emitting diode) technology,
a crucial task is to handle the unstructured data describing properties of
optical elements in standard formats. This leads to the problem of data fitting
within an appropriate model. Newton's method is used as an upgrade of
previously developed most promising discrete optimization heuristics showing
improvement of both performance and quality of solutions. Experiment also
indicates that a combination of an algorithm that finds promising initial
solutions as a preprocessor to Newton's method may be a winning idea, at least
on some datasets of instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01096</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01096</id><created>2016-03-03</created><authors><author><keyname>Liu</keyname><forenames>Qingshan</forenames></author><author><keyname>Sun</keyname><forenames>Yubao</forenames></author><author><keyname>Wang</keyname><forenames>Cantian</forenames></author><author><keyname>Liu</keyname><forenames>Tongliang</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised
  Classification</title><categories>cs.CV</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph model is emerging as a very effective tool for learning the complex
structures and relationships hidden in data. Generally, the critical purpose of
graph-oriented learning algorithms is to construct an informative graph for
image clustering and classification tasks. In addition to the classical
$K$-nearest-neighbor and $r$-neighborhood methods for graph construction,
$l_1$-graph and its variants are emerging methods for finding the neighboring
samples of a center datum, where the corresponding ingoing edge weights are
simultaneously derived by the sparse reconstruction coefficients of the
remaining samples. However, the pair-wise links of $l_1$-graph are not capable
of capturing the high order relationships between the center datum and its
prominent data in sparse reconstruction. Meanwhile, from the perspective of
variable selection, the $l_1$ norm sparse constraint, regarded as a LASSO
model, tends to select only one datum from a group of data that are highly
correlated and ignore the others. To simultaneously cope with these drawbacks,
we propose a new elastic net hypergraph learning model, which consists of two
steps. In the first step, the Robust Matrix Elastic Net model is constructed to
find the canonically related samples in a somewhat greedy way, achieving the
grouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In the
second step, hypergraph is used to represent the high order relationships
between each datum and its prominent samples by regarding them as a hyperedge.
Subsequently, hypergraph Laplacian matrix is constructed for further analysis.
New hypergraph learning algorithms, including unsupervised clustering and
multi-class semi-supervised classification, are then derived. Extensive
experiments on face and handwriting databases demonstrate the effectiveness of
the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01102</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01102</id><created>2016-03-03</created><authors><author><keyname>Ponomarev</keyname><forenames>I. N.</forenames><affiliation>Moscow Institute of Physics and Technology</affiliation></author></authors><title>Implementation of the fast table grid user interface element for working
  with large database tables</title><categories>cs.DB</categories><comments>10 pages, 3 figures, in Russian</comments><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Table grid user interface element with a vertical scrollbar is a standard way
of working with database table records. There are two basic operations each
grid should support: scrolling records with vertical scrollbar and positioning
to a record with a given primary key. This paper addresses the case when the
number of records is so large that it is not feasible to load them all into
memory, and database functions like &quot;select..offset&quot; work insufficiently fast
and put undue load on RDBMS. Our main idea is to use only queries that involve
index lookup (a O(Log(N)-fast operation) and to use statistic properties of
hypergeometric distribution to &quot;guess&quot; primary keys of records given their
ordinal numbers. The proposed method allows us to implement a grid with
O(Log(N))-fast scrolling and positioning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01107</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01107</id><created>2016-03-03</created><authors><author><keyname>Tischner</keyname><forenames>Daniel</forenames></author></authors><title>Minimization of B\&quot;uchi Automata using Fair Simulation</title><categories>cs.FL cs.DS</categories><comments>Bachelor's thesis, 67 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an algorithm, which reduces the size of B\&quot;uchi automata using
fair simulation. Its time complexity is $\mathcal{O}(|Q|^4 \cdot |\Delta|^2)$,
the space complexity is $\mathcal{O}(|Q| \cdot |\Delta|)$.
  Simulation is a common approach for minimizing $\omega$-automata such as
B\&quot;uchi automata. Direct simulation, delayed simulation and fair simulation are
different types of simulation. As we will show, minimization based on direct or
delayed simulation is conceptually simple. Whereas the algorithm based on fair
simulation is more complex. However, fair simulation allows a stronger
minimization of the automaton.
  Further, we illustrate the theory behind the algorithm, cover optimizations
useful in practice, give experimental results and compare our technique to
other minimization strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01112</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01112</id><created>2016-03-03</created><authors><author><keyname>Elkhouly</keyname><forenames>Reem</forenames></author><author><keyname>Kimura</keyname><forenames>Keiji</forenames></author><author><keyname>El-Mahdy</keyname><forenames>Ahmed</forenames></author></authors><title>If-Conversion Optimization using Neuro Evolution of Augmenting
  Topologies</title><categories>cs.DC</categories><comments>Part of the Program Transformation for Programmability in
  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March
  2016, 6 pages, LaTeX, 2 PDF figures</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control-flow dependence is an intrinsic limiting factor for pro- gram
acceleration. With the availability of instruction-level par- allel
architectures, if-conversion optimization has, therefore, be- come pivotal for
extracting parallelism from serial programs. While many if-conversion
optimization heuristics have been proposed in the literature, most of them
consider rigid criteria regardless of the underlying hardware and input
programs. In this paper, we propose a novel if-conversion scheme that preforms
an efficient if-conversion transformation using a machine learning technique
(NEAT). This method enables if-conversion customization overall branches within
a program unlike the literature that considered in- dividual branches. Our
technique also provides flexibility required when compiling for heterogeneous
systems. The efficacy of our approach is shown by experiments and reported
results which il- lustrate that the programs can be accelerated on the same
archi- tecture and without modifying the original code. Our technique applies
for general purpose programming languages (e.g. C/C++) and is transparent for
the programmer. We implemented our tech- nique in LLVM 3.6.1 compilation
infrastructure and experimented on the kernels of SPEC-CPU2006 v1.1 benchmarks
suite running on a multicore system of Intel(R) Xeon(R) 3.50GHz processors. Our
findings show a performance gain up to 8.6% over the stan- dard optimized code
(LLVM -O2 with if-conversion included), in- dicating the need for If-conversion
compilation optimization that can adapt to the unique characteristics of every
individual branch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01113</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01113</id><created>2016-03-03</created><authors><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author><author><keyname>Janciak</keyname><forenames>Ivan</forenames></author><author><keyname>Brezany</keyname><forenames>Peter</forenames></author><author><keyname>W&#xf6;hrer</keyname><forenames>Alexander</forenames></author></authors><title>A Survey of the State of the Art in Data Mining and Integration Query
  Languages</title><categories>cs.DB</categories><comments>2011 International Conference on Network-Based Information Systems</comments><doi>10.1109/NBiS.2011.58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The major aim of this survey is to identify the strengths and weaknesses of a
representative set of Data-Mining and Integration (DMI) query languages. We
describe a set of properties of DMI-related languages that we use for a
systematic evaluation of these languages. In addition, we introduce a scoring
system that we use to quantify our opinion on how well a DMI-related language
supports a property. The languages surveyed in this paper include: DMQL,
MineSQL, MSQL, M2MQL, dmFSQL, OLEDB for DM, MINE RULE, and Oracle Data Mining.
This survey may help researchers to propose a DMI language that is beyond the
state-of-the-art, or it may help practitioners to select an existing language
that fits well a purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01115</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01115</id><created>2016-03-03</created><authors><author><keyname>Abd-Elmagid</keyname><forenames>Mohamed A.</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author></authors><title>A Generalized Optimization Framework for Wireless Powered Communication
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study wireless networks where nodes have two energy
sources, namely a battery and radio frequency (RF) energy harvesting circuitry.
We formulate two optimization problems with different objective functions,
namely maximizing the sum throughput and maximizing the minimum throughput, for
enhanced fairness. Furthermore, we show the generality of the proposed system
model through characterizing the conditions under which the two formulated
optimization problems can be reduced to the corresponding problems of different
known wireless networks, namely, conventional wireless networks
(battery-powered) and wireless powered communications networks (WPCNs) with
only RF energy harvesting nodes. In addition, we introduce WPCNs with two types
of nodes, with and without RF energy harvesting capability, in which the nodes
without RF energy harvesting are utilized to enhance the sum throughput, even
beyond WPCNs with all energy harvesting nodes. We establish the convexity of
all formulated problems which opens room for efficient solution using standard
techniques. Our numerical results show that the two types of wireless networks,
namely WPCNs with only RF energy harvesting nodes and conventional wireless
networks, are considered, respectively, as lower and upper bounds on the
performance of the generalized problem setting in terms of the maximum sum
throughput and the maxmin throughput. Moreover, the results reveal new insights
and throughput-fairness trade-offs unique to our new problem setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01121</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01121</id><created>2016-03-03</created><authors><author><keyname>Heinrich</keyname><forenames>Johannes</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Deep Reinforcement Learning from Self-Play in Imperfect-Information
  Games</title><categories>cs.LG cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world applications can be described as large-scale games of
imperfect information. To deal with these challenging domains, prior work has
focused on computing Nash equilibria in a handcrafted abstraction of the
domain. In this paper we introduce the first scalable end-to-end approach to
learning approximate Nash equilibria without any prior knowledge. Our method
combines fictitious self-play with deep reinforcement learning. When applied to
Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,
whereas common reinforcement learning methods diverged. In Limit Texas Holdem,
a poker game of real-world scale, NFSP learnt a competitive strategy that
approached the performance of human experts and state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01179</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01179</id><created>2016-03-03</created><authors><author><keyname>V&#xf6;lkel</keyname><forenames>Finn</forenames></author><author><keyname>bapteste</keyname><forenames>Eric</forenames></author><author><keyname>Habib</keyname><forenames>Michel</forenames></author><author><keyname>Lopez</keyname><forenames>Philippe</forenames></author><author><keyname>Vigliotti</keyname><forenames>Chloe</forenames></author></authors><title>Read networks and k-laminar graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce k-laminar graphs a new class of graphs which
extends the idea of Asteroidal triple free graphs. Indeed a graph is k-laminar
if it admits a diametral path that is k-dominating. This bio-inspired class of
graphs was motivated by a biological application dealing with sequence
similarity networks of reads (called hereafter read networks for short). We
briefly develop the context of the biological application in which this graph
class appeared and then we consider the relationships of this new graph class
among known graph classes and then we study its recognition problem. For the
recognition of k-laminar graphs, we develop polynomial algorithms when k is
fixed. For k=1, our algorithm improves a Deogun and Krastch's algorithm (1999).
We finish by an NP-completeness result when k is unbounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01181</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01181</id><created>2016-03-03</created><authors><author><keyname>Marcus</keyname><forenames>Neta</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>The Domination Game: Proving the 3/5 Conjecture on Isolate-Free Forests</title><categories>cs.DS</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the domination game, where two players, Dominator and Staller,
construct together a dominating set M in a given graph, by alternately
selecting vertices into M. Each move must increase the size of the dominated
set. The players have opposing goals: Dominator wishes M to be as small as
possible, and Staller has the opposite goal. Kinnersley, West and Zamani
conjectured that when both players play optimally on an isolate-free forest,
there is a guaranteed upper bound for the size of the dominating set that
depends only on the size n of the forest. This bound is 3n/5 when the first
player is Dominator, and (3n+2)/5 when the first player is Staller. The
conjecture was proved for specific families of forests by Kinnersley et al. and
later extended by Bujtas. Here we prove it for all isolate-free forests, by
supplying an algorithm for Dominator that guarantees the desired bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01182</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01182</id><created>2016-03-03</created><authors><author><keyname>Verri</keyname><forenames>Filipe Alves Neto</forenames></author><author><keyname>Urio</keyname><forenames>Paulo Roberto</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>Network Unfolding Map by Edge Dynamics Modeling</title><categories>cs.AI</categories><comments>14 pages, 7 figures, 1 appendix, submitted to IEEE Transactions on
  Neural Networks and Learning Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of collective dynamics in neural networks is a mechanism of the
animal and human brain for information processing. In this paper, we develop a
computational technique of distributed processing elements, which are called
particles. We observe the collective dynamics of particles in a complex network
for transductive inference on semi-supervised learning problems. Three actions
govern the particles' dynamics: walking, absorption, and generation. Labeled
vertices generate new particles that compete against rival particles for edge
domination. Active particles randomly walk in the network until they are
absorbed by either a rival vertex or an edge currently dominated by rival
particles. The result from the model simulation consists of sets of edges
sorted by the label dominance. Each set tends to form a connected subnetwork to
represent a data class. Although the intrinsic dynamics of the model is a
stochastic one, we prove there exists a deterministic version with largely
reduced computational complexity; specifically, with subquadratic growth.
Furthermore, the edge domination process corresponds to an unfolding map.
Intuitively, edges &quot;stretch&quot; and &quot;shrink&quot; according to edge dynamics.
Consequently, such effect summarizes the relevant relationships between
vertices and uncovered data classes. The proposed model captures important
details of connectivity patterns over the edge dynamics evolution, which
contrasts with previous approaches focused on vertex dynamics. Computer
simulations reveal that our model can identify nonlinear features in both real
and artificial data, including boundaries between distinct classes and the
overlapping structure of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01185</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01185</id><created>2016-03-02</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Evolving Boolean Regulatory Networks with Variable Gene Expression Times</title><categories>q-bio.BM cs.NE q-bio.MN</categories><comments>arXiv admin note: text overlap with arXiv:1505.01980,
  arXiv:1306.4793, arXiv:1303.7220, arXiv:1310.5568</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The time taken for gene expression varies not least because proteins vary in
length considerably. This paper uses an abstract, tuneable Boolean regulatory
network model to explore gene expression time variation. In particular, it is
shown how non-uniform expression times can emerge under certain conditions
through simulated evolution. That is, gene expression time variance appears
beneficial in the shaping of the dynamical behaviour of the regulatory network
without explicit consideration of protein function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01187</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01187</id><created>2016-03-03</created><authors><author><keyname>Aklah</keyname><forenames>Zeyad</forenames></author><author><keyname>Ma</keyname><forenames>Sen</forenames></author><author><keyname>Andrews</keyname><forenames>David</forenames></author></authors><title>A Dynamic Overlay Supporting Just-In-Time Assembly to Construct
  Customized Hardware Accelerators</title><categories>cs.AR</categories><comments>2 pages, extended abstract, 2nd International Workshop on Overlay
  Architectures for FPGAs (OLAF),Feburary 21, 2016 - Monterey, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barriers that prevent programmers from using FPGAs include the need to work
within vendor specific CAD tools, knowledge of hardware programming models, and
the requirement to pass each design through synthesis, place and route. In this
work, a dynamic overlay is designed to support Just- In-Time assembly by
composing hardware operators to construct full accelerators. The hardware
operators are pre-synthesized bit- streams and can be downloaded to Partially
Reconfigurable(PR) regions at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01191</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01191</id><created>2016-03-03</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Pyatkin</keyname><forenames>Artem V.</forenames></author></authors><title>Completing partial schedules for Open Shop with unit processing times
  and routing</title><categories>cs.DM cs.DS</categories><comments>15 pages, plus 6 pages appendix</comments><msc-class>90B35</msc-class><acm-class>F.2.2; I.2.8; G.2.1; G.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open Shop is a classical scheduling problem: given a set $J$ of jobs and a
set $M$ of machines, find a minimum-makespan schedule to process each job
$J_i\in J$ on each machine $M_q\in M$ for a given amount $p_{iq}$ of time such
that each machine processes only one job at a time and each job is processed by
only one machine at a time. In Routing Open Shop, the jobs are located in the
vertices of an edge-weighted graph $G=(V,E)$ whose edge weights determine the
time needed for the machines to travel between jobs. The travel times also have
a natural interpretation as sequence-dependent family setup times. Routing Open
Shop is NP-hard for $|V|=|M|=2$. For the special case with unit processing
times $p_{iq}=1$, we exploit Galvin's theorem about list-coloring edges of
bipartite graphs to prove a theorem that gives a sufficient condition for the
completability of partial schedules. Exploiting this schedule completion
theorem and integer linear programming, we show that Routing Open Shop with
unit processing times is solvable in
$2^{O(|V||M|^2\log|V||M|)}\cdot\mathrm{poly}(|J|)$ time, that is,
fixed-parameter tractable parameterized by $|V|+|M|$. Various upper bounds
shown using the schedule completion theorem suggest it to be likewise
beneficial for the development of approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01202</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01202</id><created>2016-03-03</created><authors><author><keyname>Izzo</keyname><forenames>Paolo</forenames></author><author><keyname>Qu</keyname><forenames>Hongyang</forenames></author><author><keyname>Veres</keyname><forenames>Sandor M.</forenames></author></authors><title>Reducing complexity of autonomous control agents for verifiability</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AgentSpeak type of languages are considered for decision making in
autonomous control systems. To reduce the complexity and increase the
verifiability of decision making, a limited instruction set agent (LISA) is
introduced. The new decision method is structurally simpler than its
predecessors and easily lends itself to both design time and runtime
verification methods. The process of converting a control agent in LISA into a
model in a probabilistic model checker is described. Due to the practical
complexity of design time verification the feasibility of runtime probabilistic
verification is investigated and illustrated in the LISA agent programming
system for verifying symbolic plans of the agent using a probabilistic model
checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01203</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01203</id><created>2016-03-03</created><authors><author><keyname>Kumar</keyname><forenames>Praveen</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author><author><keyname>Yu</keyname><forenames>Chris</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author><author><keyname>Soul&#xe9;</keyname><forenames>Robert</forenames></author></authors><title>Kulfi: Robust Traffic Engineering Using Semi-Oblivious Routing</title><categories>cs.NI</categories><comments>23 pages, 13 figures</comments><acm-class>C.2.2; C.2.3; C.2.5; C.2.6; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wide-area network traffic engineering enables network operators to reduce
congestion and improve utilization by balancing load across multiple paths.
Current approaches to traffic engineering can be modeled in terms of a routing
component that computes forwarding paths, and a load balancing component that
maps incoming flows onto those paths dynamically, adjusting sending rates to
fit current conditions. Unfortunately, existing systems rely on simple
strategies for one or both of these components, which leads to poor performance
or requires making frequent updates to forwarding paths, significantly
increasing management complexity. This paper explores a different approach
based on semi-oblivious routing, a natural extension of oblivious routing in
which the system computes a diverse set of paths independent of demands, but
also dynamically adapts sending rates as conditions change. Semi-oblivious
routing has a number of important advantages over competing approaches
including low overhead, nearly optimal performance, and built-in protection
against unexpected bursts of traffic and failures. Through in-depth simulations
and a deployment on SDN hardware, we show that these benefits are robust, and
hold across a wide range of topologies, demands, resource budgets, and failure
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01204</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01204</id><created>2016-03-03</created><authors><author><keyname>Backovi&#x107;</keyname><forenames>Mihailo</forenames></author></authors><title>A Theory of Ambulance Chasing</title><categories>physics.soc-ph cs.DL hep-ph physics.data-an</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ambulance chasing is a common socio-scientific phenomenon in particle
physics. I argue that despite the seeming complexity, it is possible to gain
insight into both the qualitative and quantitative features of ambulance
chasing dynamics. Compound-Poisson statistics suffices to accommodate the time
evolution of the cumulative number of papers on a topic, where basic
assumptions that the interest in the topic as well as the number of available
ideas decrease with time appear to drive the time evolution. It follows that if
the interest scales as an inverse power law in time, the cumulative number of
papers on a topic is well described by a di-gamma function, with a distinct
logarithmic behavior at large times. In cases where the interest decreases
exponentially with time, the model predicts that the total number of papers on
the topic will converge to a fixed value as time goes to infinity. I
demonstrate that the two models are able to fit at least 9 specific instances
of ambulance chasing in particle physics using only two free parameters. In
case of the most recent ambulance chasing instance, the ATLAS {\gamma}{\gamma}
excess, fits to the current data predict that the total number of papers on the
topic will not exceed 310 papers by the June 1. 2016, and prior to the natural
cut-off for the validity of the theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01207</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01207</id><created>2016-03-03</created><authors><author><keyname>Gibson</keyname><forenames>Nathan P.</forenames></author><author><keyname>Michelson</keyname><forenames>David A.</forenames></author><author><keyname>Schwartz</keyname><forenames>Daniel L.</forenames></author></authors><title>From manuscript catalogues to a handbook of Syriac literature: Modeling
  an infrastructure for Syriaca.org</title><categories>cs.DL</categories><comments>Part of special issue: Computer-Aided Processing of Intertextuality
  in Ancient Languages. 15 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite increasing interest in Syriac studies and growing digital
availability of Syriac texts, there is currently no up-to-date infrastructure
for discovering, identifying, classifying, and referencing works of Syriac
literature. The standard reference work (Baumstark's Geschichte) is over ninety
years old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be
accessed only through disparate catalogues and databases. The present article
proposes a tentative data model for Syriaca.org's New Handbook of Syriac
Literature, an open-access digital publication that will serve as both an
authority file for Syriac works and a guide to accessing their manuscript
representations, editions, and translations. The authors hope that by
publishing a draft data model they can receive feedback and incorporate
suggestions into the next stage of the project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01213</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01213</id><created>2016-03-03</created><authors><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Optimal Rebuilding of Multiple Erasures in MDS Codes</title><categories>cs.IT math.IT</categories><comments>There is an overlap of this work with our two previous submissions:
  Zigzag Codes: MDS Array Codes with Optimal Rebuilding; On Codes for Optimal
  Rebuilding Access. arXiv admin note: text overlap with arXiv:1112.0371</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MDS array codes are widely used in storage systems due to their
computationally efficient encoding and decoding procedures. An MDS code with
$r$ redundancy nodes can correct any $r$ node erasures by accessing all the
remaining information in the surviving nodes. However, in practice, $e$
erasures is a more likely failure event, for $1\le e&lt;r$. Hence, a natural
question is how much information do we need to access in order to rebuild $e$
storage nodes? We define the rebuilding ratio as the fraction of remaining
information accessed during the rebuilding of $e$ erasures. In our previous
work we constructed MDS codes, called zigzag codes, that achieve the optimal
rebuilding ratio of $1/r$ for the rebuilding of any systematic node when $e=1$,
however, all the information needs to be accessed for the rebuilding of the
parity node erasure.
  The (normalized) repair bandwidth is defined as the fraction of information
transmitted from the remaining nodes during the rebuilding process. For codes
that are not necessarily MDS, Dimakis et al. proposed the regenerating codes
framework where any $r$ erasures can be corrected by accessing some of the
remaining information, and any $e=1$ erasure can be rebuilt from some subsets
of surviving nodes with optimal repair bandwidth.
  In this work, we study 3 questions on rebuilding of codes: (i) We show a
fundamental trade-off between the storage size of the node and the repair
bandwidth similar to the regenerating codes framework, and show that zigzag
codes achieve the optimal rebuilding ratio of $e/r$ for MDS codes, for any
$1\le e\le r$. (ii) We construct systematic codes that achieve optimal
rebuilding ratio of $1/r$, for any systematic or parity node erasure. (iii) We
present error correction algorithms for zigzag codes, and in particular
demonstrate how these codes can be corrected beyond their minimum Hamming
distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01214</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01214</id><created>2016-03-03</created><authors><author><keyname>Franke</keyname><forenames>Beate</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Network modularity in the presence of covariates</title><categories>math.ST cs.SI stat.ME stat.TH</categories><comments>56 pages, 4 figures; submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the large-sample properties of network modularity in the
presence of covariates, under a natural and flexible nonparametric null model.
This provides for the first time an objective measure of whether or not a
particular value of modularity is meaningful. In particular, our results
quantify the strength of the relation between observed community structure and
the interactions in a network. Our technical contribution is to provide limit
theorems for modularity when a community assignment is given by nodal features
or covariates. These theorems hold for a broad class of network models over a
range of sparsity regimes, as well as weighted, multi-edge, and power-law
networks. This allows us to assign $p$-values to observed community structure,
which we validate using several benchmark examples in the literature. We
conclude by applying this methodology to investigate a multi-edge network of
corporate email interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01215</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01215</id><created>2016-03-03</created><authors><author><keyname>Marigo</keyname><forenames>Francesco</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>Remarks on Frankl's conjecture</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First a few reformulations of Frankl's conjecture are given, in terms of
reduced families or matrices, or analogously in terms of lattices. These lead
naturally to a stronger conjecture with a neat formulation which might be
easier to attack than Frankl's. To this end we prove an inequality which might
help in proving the stronger conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01217</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01217</id><created>2016-03-03</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Dai</keyname><forenames>Mingbo</forenames></author><author><keyname>Rassouli</keyname><forenames>Borzoo</forenames></author></authors><title>Rate Splitting for MIMO Wireless Networks: A Promising PHY-Layer
  Strategy for LTE Evolution</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted to IEEE Communication Magazine, special issue on LTE
  Evolution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIMO processing plays a central part towards the recent increase in spectral
and energy efficiencies of wireless networks. MIMO has grown beyond the
original point-to-point channel and nowadays refers to a diverse range of
centralized and distributed deployments. The fundamental bottleneck towards
enormous spectral and energy efficiency benefits in multiuser MIMO networks
lies in a huge demand for accurate channel state information at the transmitter
(CSIT). This has become increasingly difficult to satisfy due to the increasing
number of antennas and access points in next generation wireless networks
relying on dense heterogeneous networks and transmitters equipped with a large
number of antennas. CSIT inaccuracy results in a multi-user interference
problem that is the primary bottleneck of MIMO wireless networks. Looking
backward, the problem has been to strive to apply techniques designed for
perfect CSIT to scenarios with imperfect CSIT. In this paper, we depart from
this conventional approach and introduce the readers to a promising strategy
based on rate-splitting. Rate-splitting relies on the transmission of common
and private messages and is shown to provide significant benefits in terms of
spectral and energy efficiencies, reliability and CSI feedback overhead
reduction over conventional strategies used in LTE-A and exclusively relying on
private message transmissions. Open problems, impact on standard specifications
and operational challenges are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01228</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01228</id><created>2016-03-03</created><authors><author><keyname>Kov&#xe1;cs</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>S&#xf3;lyom-Gecse</keyname><forenames>Csilla</forenames></author></authors><title>GeoGebra Tools with Proof Capabilities</title><categories>cs.AI</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report about significant enhancements of the complex algebraic geometry
theorem proving subsystem in GeoGebra for automated proofs in Euclidean
geometry, concerning the extension of numerous GeoGebra tools with proof
capabilities. As a result, a number of elementary theorems can be proven by
using GeoGebra's intuitive user interface on various computer architectures
including native Java and web based systems with JavaScript. We also provide a
test suite for benchmarking our results with 200 test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01232</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01232</id><created>2016-03-03</created><authors><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Gasic</keyname><forenames>Milica</forenames></author><author><keyname>Mrksic</keyname><forenames>Nikola</forenames></author><author><keyname>Rojas-Barahona</keyname><forenames>Lina M.</forenames></author><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Multi-domain Neural Network Language Generation for Spoken Dialogue
  Systems</title><categories>cs.CL</categories><comments>Accepted as a long paper in NAACL-HLT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moving from limited-domain natural language generation (NLG) to open domain
is difficult because the number of semantic input combinations grows
exponentially with the number of domains. Therefore, it is important to
leverage existing resources and exploit similarities between domains to
facilitate domain adaptation. In this paper, we propose a procedure to train
multi-domain, Recurrent Neural Network-based (RNN) language generators via
multiple adaptation steps. In this procedure, a model is first trained on
counterfeited data synthesised from an out-of-domain dataset, and then fine
tuned on a small set of in-domain utterances with a discriminative objective
function. Corpus-based evaluation results show that the proposed procedure can
achieve competitive performance in terms of BLEU score and slot error rate
while significantly reducing the data needed to train generators in new, unseen
domains. In subjective testing, human judges confirm that the procedure greatly
improves generator performance when only a small amount of data is available in
the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01244</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01244</id><created>2016-03-03</created><authors><author><keyname>Rowe</keyname><forenames>Paul D.</forenames></author></authors><title>Principles of Layered Attestation</title><categories>cs.CR cs.MA</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems designed with measurement and attestation in mind are often layered,
with the lower layers measuring the layers above them. Attestations of such
systems, which we call layered attestations, must bundle together the results
of a diverse set of application-specific measurements of various parts of the
system. Some methods of layered attestation are more trustworthy than others,
so it is important for system designers to understand the trust consequences of
different system configurations. This paper presents a formal framework for
reasoning about layered attestations, and provides generic reusable principles
for achieving trustworthy results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01249</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01249</id><created>2016-03-03</created><authors><author><keyname>Ranjan</keyname><forenames>Rajeev</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>HyperFace: A Deep Multi-task Learning Framework for Face Detection,
  Landmark Localization, Pose Estimation, and Gender Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for simultaneous face detection, landmarks
localization, pose estimation and gender recognition using deep convolutional
neural networks (CNN). The proposed method called, Hyperface, fuses the
intermediate layers of a deep CNN using a separate CNN and trains multi-task
loss on the fused features. It exploits the synergy among the tasks which
boosts up their individual performances. Extensive experiments show that the
proposed method is able to capture both global and local information of faces
and performs significantly better than many competitive algorithms for each of
these four tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01250</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01250</id><created>2016-03-03</created><authors><author><keyname>Ioannou</keyname><forenames>Yani</forenames></author><author><keyname>Robertson</keyname><forenames>Duncan</forenames></author><author><keyname>Zikic</keyname><forenames>Darko</forenames></author><author><keyname>Kontschieder</keyname><forenames>Peter</forenames></author><author><keyname>Shotton</keyname><forenames>Jamie</forenames></author><author><keyname>Brown</keyname><forenames>Matthew</forenames></author><author><keyname>Criminisi</keyname><forenames>Antonio</forenames></author></authors><title>Decision Forests, Convolutional Networks and the Models in-Between</title><categories>cs.CV cs.AI</categories><comments>Microsoft Research Technical Report</comments><report-no>MSR-TR-2015-58</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the connections between two state of the art
classifiers: decision forests (DFs, including decision jungles) and
convolutional neural networks (CNNs). Decision forests are computationally
efficient thanks to their conditional computation property (computation is
confined to only a small region of the tree, the nodes along a single branch).
CNNs achieve state of the art accuracy, thanks to their representation learning
capabilities. We present a systematic analysis of how to fuse conditional
computation with representation learning and achieve a continuum of hybrid
models with different ratios of accuracy vs. efficiency. We call this new
family of hybrid models conditional networks. Conditional networks can be
thought of as: i) decision trees augmented with data transformation operators,
or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data
routing functions. Experimental validation is performed on the common task of
image classification on both the CIFAR and Imagenet datasets. Compared to state
of the art CNNs, our hybrid models yield the same accuracy with a fraction of
the compute cost and much smaller number of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01257</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01257</id><created>2016-03-03</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Jain</keyname><forenames>Kamal</forenames></author><author><keyname>Mai</keyname><forenames>Tung</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author><author><keyname>Yazdanbod</keyname><forenames>Sadra</forenames></author></authors><title>New Convex Programs for Fisher's Market Model and its Generalizations</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the following results pertaining to Fisher's market model:
  -We give two natural generalizations of Fisher's market model: In model M_1,
sellers can declare an upper bound on the money they wish to earn (and take
back their unsold good), and in model M_2, buyers can declare an upper bound on
the amount to utility they wish to derive (and take back the unused part of
their money).
  -We derive convex programs for the linear case of these two models by
generalizing a convex program due to Shmyrev and the Eisenberg-Gale program,
respectively.
  -We generalize the Arrow-Hurwicz theorem to the linear case of these two
models, hence deriving alternate convex programs.
  -For the special class of convex programs having convex objective functions
and linear constraints, we derive a simple set of rules for constructing the
dual program (as simple as obtaining the dual of an LP). Using these rules we
show a formal relationship between the two seemingly different convex programs
for linear Fisher markets, due to Eisenberg-Gale and Shmyrev; the duals of
these are the same, upto a change of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01292</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01292</id><created>2016-03-03</created><authors><author><keyname>Singh</keyname><forenames>Abhineet</forenames></author><author><keyname>Roy</keyname><forenames>Ankush</forenames></author><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Jagersand</keyname><forenames>Martin</forenames></author></authors><title>Modular Decomposition and Analysis of Registration based Trackers</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new way to study registration based trackers by
decomposing them into three constituent sub modules: appearance model, state
space model and search method. It is often the case that when a new tracker is
introduced in literature, it only contributes to one or two of these sub
modules while using existing methods for the rest. Since these are often
selected arbitrarily by the authors, they may not be optimal for the new
method. In such cases, our breakdown can help to experimentally find the best
combination of methods for these sub modules while also providing a framework
within which the contributions of the new tracker can be clearly demarcated and
thus studied better. We show how existing trackers can be broken down using the
suggested methodology and compare the performance of the default configuration
chosen by the authors against other possible combinations to demonstrate the
new insights that can be gained by such an approach. We also present an open
source system that provides a convenient interface to plug in a new method for
any sub module and test it against all possible combinations of methods for the
other two sub modules while also serving as a fast and efficient solution for
practical tracking requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01303</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01303</id><created>2016-03-03</created><authors><author><keyname>Yoon</keyname><forenames>Wonjun</forenames></author><author><keyname>Kim</keyname><forenames>Sol-A</forenames></author><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author></authors><title>A Robot Learning to Play a Mobile Game Under Unknown Dynamics</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advance in robotic hardware and intelligent software, humanoid robot
could play an important role in various fields including service for human
assistance and heavy job for hazardous industry. Under unknown dynamics
operating smart devices with a humanoid robot is a even more challenging task
because a robot needs to learn both swipe actions and complex state transitions
inside the smart devices in a long time horizon. Recent advances in task
learning enable humanoid robots to conduct dexterous manipulation tasks such as
grasping objects and assembling parts of furniture. In this paper, we explore
another step further toward building a human-like robot by introducing an
architecture which enables humanoid robots to learn operating smart devices
requiring complex tasks. We implement our learning architecture in the Baxter
Research Robot and experimentally demonstrate that the robot with our
architecture could play a challenging mobile game, the 2048 game, as accurate
as in a simulated environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01312</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01312</id><created>2016-03-03</created><authors><author><keyname>Lerer</keyname><forenames>Adam</forenames></author><author><keyname>Gross</keyname><forenames>Sam</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Learning Physical Intuition of Block Towers by Example</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wooden blocks are a common toy for infants, allowing them to develop motor
skills and gain intuition about the physical behavior of the world. In this
paper, we explore the ability of deep feed-forward models to learn such
intuitive physics. Using a 3D game engine, we create small towers of wooden
blocks whose stability is randomized and render them collapsing (or remaining
upright). This data allows us to train large convolutional network models which
can accurately predict the outcome, as well as estimating the block
trajectories. The models are also able to generalize in two important ways: (i)
to new physical scenarios, e.g. towers with an additional block and (ii) to
images of real wooden blocks, where it obtains a performance comparable to
human subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01313</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01313</id><created>2016-03-03</created><authors><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author><author><keyname>Cox</keyname><forenames>Guilherme</forenames></author><author><keyname>Deng</keyname><forenames>Qingyuan</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Bianchini</keyname><forenames>Ricardo</forenames></author></authors><title>FastCap: An Efficient and Fair Algorithm for Power Capping in Many-Core
  Systems</title><categories>cs.PF</categories><comments>Accepted by ISPASS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future servers will incorporate many active lowpower modes for different
system components, such as cores and memory. Though these modes provide
flexibility for power management via Dynamic Voltage and Frequency Scaling
(DVFS), they must be operated in a coordinated manner. Such coordinated control
creates a combinatorial space of possible power mode configurations. Given the
rapid growth of the number of cores, it is becoming increasingly challenging to
quickly select the configuration that maximizes the performance under a given
power budget. Prior power capping techniques do not scale well to large numbers
of cores, and none of those works has considered memory DVFS. In this paper, we
present FastCap, our optimization approach for system-wide power capping, using
both CPU and memory DVFS. Based on a queuing model, FastCap formulates power
capping as a non-linear optimization problem where we seek to maximize the
system performance under a power budget, while promoting fairness across
applications. Our FastCap algorithm solves the optimization online and
efficiently (low complexity on the number of cores), using a small set of
performance counters as input. To evaluate FastCap, we simulate it for a
many-core server running different types of workloads. Our results show that
FastCap caps power draw accurately, while producing better application
performance and fairness than many existing CPU power capping methods (even
after they are extended to use of memory DVFS as well).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01315</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01315</id><created>2016-03-03</created><authors><author><keyname>Cheng</keyname><forenames>Shin-Ming</forenames></author><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author></authors><title>Ecology-Based DoS Attack in Cognitive Radio Networks</title><categories>cs.CR cs.GT cs.NI</categories><comments>to appear in IEEE Symposium on Security and Privacy (IEEE S&amp;P) 2016
  Workshop on Bio-inspired Security, Trust, Assurance and Resilience</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio technology, which is designed to enhance spectrum
utilization, depends on the success of opportunistic access, where secondary
users (SUs) exploit spectrum void unoccupied by primary users (PUs) for
transmissions. We note that the system behaviors are very similar to the
interactions among different species coexisting in an ecosystem. However, SUs
of a selfish nature or of misleading information may make concurrent
transmissions with PUs for additional incentives, and thus disrupt the entire
ecosystem. By exploiting this vulnerability, this paper proposes a novel
distributed denial-of-service (DoS) attack where invasive species, i.e.,
malicious users (MUs), induce originally normal-behaved SUs to execute
concurrent transmissions with PUs and thus collapse the cognitive radio
network. We adopt stochastic geometry to model the spatial distributions of
PUs, SUs, and MUs for the analysis of the mutual interference among them. The
access strategy of each SU in the spectrum sharing ecosystem, which evolves
with the experienced payoffs and interference, is modeled by an evolutionary
game. Based on the evolutionary stable strategy concept, we could efficiently
identify the fragile operating region at which normal-behaved SUs are
eventually evolved to conduct concurrent transmissions and thus to cause the
ruin of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01316</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01316</id><created>2016-03-03</created><authors><author><keyname>de Oliveira</keyname><forenames>R. C.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Ramalho</keyname><forenames>R. A.</forenames></author><author><keyname>Viana</keyname><forenames>L. P. S.</forenames></author></authors><title>Performance Assessment of WhatsApp and IMO on Android Operating System
  (Lollipop and KitKat) during VoIP calls using 3G or WiFi</title><categories>cs.PF</categories><comments>8 pages, Number of floats/tables/figures: 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper assesses the performance of mobile messaging and VoIP connections.
We investigate the CPU requirements of WhatsApp and IMO under different
scenarios. This analysis also enabled a comparison of the performance of these
applications on two Android operating system (OS) versions: KitKat or Lollipop.
Two models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4.
The applications behavior was statistically investigated for both sending and
receiving VoIP calls. Connections have been examined over 3G and WiFi. The
handset model plays a decisive role in CPU requirements of the application.
t-tests showed that IMO has a statistical better performance that WhatsApp
whatever be the Android at a significance level 1%, on Galaxy Note 4. In
contrast, WhatsApp requires less CPU than IMO on Galaxy S4 whatever be the OS
and access (3G/WiFi). Galaxy Note 4 using WiFi always outperformed S4 in terms
of processing efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01318</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01318</id><created>2016-03-03</created><authors><author><keyname>Ziani</keyname><forenames>Juba</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author></authors><title>Recovering Games from Perturbed Equilibrium Observations Using Convex
  Optimization</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of reconstructing a game that is consistent with
observed equilibrium play, a fundamental problem in econometrics. Our
contribution is to develop and analyze a new methodology based on convex
optimization to address this problem for many classes of games and observation
models of interest. Our approach provides the flexibility to solve a number of
variants and specializations of this problem, such as an evaluation of the
power of games from a particular class (e.g., zero-sum, potential, linearly
parameterized) to explain player behavior or the extent to which a particular
set of observations tightly constrains the space of consistent explanations; it
can also simply provide a compact summary of observed behavior. The framework
underlying the development in this paper also differs from much of the
literature on econometrics, as we do not make strong distributional assumptions
on the observations of player actions.
  We illustrate our approach with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01324</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01324</id><created>2016-03-03</created><authors><author><keyname>Hollis</keyname><forenames>Brayden</forenames></author><author><keyname>Patterson</keyname><forenames>Stacy</forenames></author><author><keyname>Trinkle</keyname><forenames>Jeff</forenames></author></authors><title>Compressed Sensing for Tactile Skins</title><categories>cs.RO</categories><comments>8 pages, 4 figures, submitted to ICRA16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole body tactile perception via tactile skins offers large benefits for
robots in unstructured environments. To fully realize this benefit, tactile
systems must support real-time data acquisition over a massive number of
tactile sensor elements. We present a novel approach for scalable tactile data
acquisition using compressed sensing. We first demonstrate that the tactile
data is amenable to compressed sensing techniques. We then develop a solution
for fast data sampling, compression, and reconstruction that is suited for
tactile system hardware and has potential for reducing the wiring complexity.
Finally, we evaluate the performance of our technique on simulated tactile
sensor networks. Our evaluations show that compressed sensing, with a
compression ratio of 3 to 1, can achieve higher signal acquisition accuracy
than full data acquisition of noisy sensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01333</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01333</id><created>2016-03-03</created><authors><author><keyname>Sha</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Sujian</forenames></author><author><keyname>Chang</keyname><forenames>Baobao</forenames></author><author><keyname>Sui</keyname><forenames>Zhifang</forenames></author></authors><title>Joint Learning Templates and Slots for Event Schema Induction</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic event schema induction (AESI) means to extract meta-event from raw
text, in other words, to find out what types (templates) of event may exist in
the raw text and what roles (slots) may exist in each event type. In this
paper, we propose a joint entity-driven model to learn templates and slots
simultaneously based on the constraints of templates and slots in the same
sentence. In addition, the entities' semantic information is also considered
for the inner connectivity of the entities. We borrow the normalized cut
criteria in image segmentation to divide the entities into more accurate
template clusters and slot clusters. The experiment shows that our model gains
a relatively higher result than previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01335</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01335</id><created>2016-03-03</created><authors><author><keyname>Choi</keyname><forenames>Jaeyoung</forenames></author><author><keyname>Larson</keyname><forenames>Martha</forenames></author><author><keyname>Li</keyname><forenames>Xinchao</forenames></author><author><keyname>Friedland</keyname><forenames>Gerald</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Where to be wary: The impact of widespread photo-taking and image
  enhancement practices on users' geo-privacy</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's geo-location estimation approaches are able to infer the location of
a target image using its visual content alone. These approaches exploit visual
matching techniques, applied to a large collection of background images with
known geo-locations. Users who are unaware that visual retrieval approaches can
compromise their geo-privacy, unwittingly open themselves to risks of crime or
other unintended consequences. Private photo sharing is not able to protect
users effectively, since its inconvenience is a barrier to consistent use, and
photos can still fall into the wrong hands if they are re-shared. This paper
lays the groundwork for a new approach to geo-privacy of social images: Instead
of requiring a complete change of user behavior, we investigate the protection
potential latent in users existing practices. We carry out a series of
retrieval experiments using a large collection of social images (8.5M) to
systematically analyze where users should be wary, and how both photo taking
and editing practices impact the performance of geo-location estimation. We
find that practices that are currently widespread are already sufficient to
protect single-handedly the geo-location ('geo-cloak') up to more than 50% of
images whose location would otherwise be automatically predictable. Our
conclusion is that protecting users against the unwanted effects of visual
retrieval is a viable research field, and should take as its starting point
existing user practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01336</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01336</id><created>2016-03-03</created><authors><author><keyname>Ribas</keyname><forenames>Sabir</forenames></author><author><keyname>Ueda</keyname><forenames>Alberto</forenames></author><author><keyname>Santos</keyname><forenames>Rodrygo L. T.</forenames></author><author><keyname>Ribeiro-Neto</keyname><forenames>Berthier</forenames></author><author><keyname>Ziviani</keyname><forenames>Nivio</forenames></author></authors><title>Simplified Relative Citation Ratio for Static Paper Ranking: UFMG/LATIN
  at WSDM Cup 2016</title><categories>cs.IR cs.DL</categories><comments>WSDM Cup. The 9th ACM International Conference on Web Search and Data
  Mining San Francisco, California, USA. February 22-25, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static rankings of papers play a key role in the academic search setting.
Many features are commonly used in the literature to produce such rankings,
some examples are citation-based metrics, distinct applications of PageRank,
among others. More recently, learning to rank techniques have been successfully
applied to combine sets of features producing effective results. In this work,
we propose the metric S-RCR, which is a simplified version of a metric called
Relative Citation Ratio --- both based on the idea of a co-citation network.
When compared to the classical version, our simplification S-RCR leads to
improved efficiency with a reasonable effectiveness. We use S-RCR to rank over
120 million papers in the Microsoft Academic Graph dataset. By using this
single feature, which has no parameters and does not need to be tuned, our team
was able to reach the 3rd position in the first phase of the WSDM Cup 2016.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01338</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01338</id><created>2016-03-03</created><authors><author><keyname>Yang</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Ju</forenames></author></authors><title>Finding best possible constant for a polynomial inequality</title><categories>cs.SC</categories><comments>Proceedings of the 20th Asian Technology Conference in Mathematics
  (Leshan, China, 2015) 178-187</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a multi-variant polynomial inequality with a parameter, how to find the
best possible value of this parameter that satisfies the inequality? For
instance, find the greatest number $k$ that satisfies $ a^3+b^3+c^3+
k(a^2b+b^2c+c^2a)-(k+1)(ab^2+bc^2+ca^2)\geq 0 $ for all nonnegative real
numbers $ a,b,c $. Analogues problems often appeared in studies of inequalities
and were dealt with by various methods. In this paper, a general algorithm is
proposed for finding the required best possible constant. The algorithm can be
easily implemented by computer algebra tools such as Maple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01340</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01340</id><created>2016-03-03</created><authors><author><keyname>Yin</keyname><forenames>Yanling</forenames></author><author><keyname>Liu</keyname><forenames>Songzuo</forenames></author><author><keyname>Qiao</keyname><forenames>Gang</forenames></author><author><keyname>Yang</keyname><forenames>Yue</forenames></author></authors><title>OFDM demodulation using virtual time reversal processing in underwater
  acoustic communication</title><categories>cs.IT cs.SD math.IT</categories><journal-ref>Journal of Computational Acoustics, Vol. 23, No. 4, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extremely long underwater channel delay spread causes severe inter-symbol
interference (ISI) for underwater acoustic communications. Passive time
reversal processing (PTRP) can effectively reduce the channel time dispersion
in a simple way via convolving the received packet with a time reversed probe
signal. However the probe signal itself may introduce extra noise and
interference (self-correlation of the probe signal). In this paper, we propose
a virtual time reversal processing (VTRP) for single input single output (SISO)
Orthogonal Frequency Division Multiplexing (OFDM) systems. It convolves the
received packet with the reversed estimated channel, instead of the probe
signal to reduce the interference. Two sparse channel estimation methods,
matching pursuit (MP), and basis pursuit de-noising (BPDN), are adopted to
estimate the channel impulse response (CIR). We compare the performance of VTRP
with the PTRP and without any time reversal processing through MATLAB
simulations and the pool experiments. The results reveal that VTRP has
outstanding performance over time-invariant channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01352</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01352</id><created>2016-03-04</created><authors><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Fangfei</forenames></author><author><keyname>Lee</keyname><forenames>Ruby B.</forenames></author></authors><title>Cloud Server Benchmarks for Performance Evaluation of New Hardware
  Architecture</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adding new hardware features to a cloud computing server requires testing
both the functionalities and the performance of the new hardware mechanisms.
However, commonly used cloud computing server workloads are not
well-represented by the SPEC integer and floating-point benchmark and Parsec
suites typically used by the computer architecture community. Existing cloud
benchmark suites for scale-out or scale-up computing are not representative of
the most common cloud usage, and are very difficult to run on a cycle-accurate
simulator that can accurately model new hardware, like gem5. In this paper, we
present PALMScloud, a suite of cloud computing benchmarks for performance
evaluation of cloud servers, that is ready to run on the gem5 cycle-accurate
simulator. We demonstrate how our cloud computing benchmarks are used in
evaluating the cache performance of a new secure cache called Newcache as a
case study. We hope that these cloud benchmarks, ready to run on a dual-machine
gem5 simulator or on real machines, can be useful to other researchers
interested in improving hardware micro-architecture and cloud server
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01354</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01354</id><created>2016-03-04</created><updated>2016-03-08</updated><authors><author><keyname>Ma</keyname><forenames>Xuezhe</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title><categories>cs.LG cs.CL</categories><comments>10 pages, 3 figures. Submission on ACL 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art sequence labeling systems traditionally require large
amounts of task-specific knowledge in the form of hand-crafted features and
data pre-processing. In this paper, we introduce a novel neutral network
architecture that benefits from both word- and character-level representations
automatically, by using combination of bidirectional LSTM, CNN and CRF. Our
system is truly end-to-end, requiring no feature engineering or data
pre-processing, thus making it applicable to a wide range of sequence labeling
tasks on different languages. We evaluate our system on two data sets for two
sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS)
tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain
state-of-the-art performance on both the two data --- 97.55\% accuracy for POS
tagging and 91.21\% F1 for NER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01356</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01356</id><created>2016-03-04</created><authors><author><keyname>Yang</keyname><forenames>Yue</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>PCF Scheme for Periodic Data Transmission in Smart Metering Network with
  Cognitive Radio</title><categories>cs.NI</categories><journal-ref>Proceeding in IEEE Globecom, Dec. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The next generation Advanced Metering Infrastructure (AMI), with the aid of
two-way Smart Metering Network (SMN), is expected to support many advanced
functions. In this work, we focus on the application of remote periodic energy
consumption reporting, which is a fundamental and significant component of
Demand Response and Load Management. In order to support this periodic
application with satisfactory communication performance, a well-suited Media
Access Control (MAC) protocol needs to be designed. Because the number of Smart
Meters (communication nodes) involved in SMN are much larger than that in
today's local area networks, the traditional taking-turns MAC protocol, such as
Point Coordination Function (PCF) in WiFi is unlikely to perform well. In order
to solve this problem, we propose a modified PCF scheme with the combination of
Cognitive Radio technology, in which the Smart Meters may use the free channels
(white space) to report energy consumption data to the Local Collector when the
Primary Users are not occupying the channels. We also conduct comprehensive
throughput analysis on the proposed scheme. The numerical results and
simulation results through NS-3 show that the PCF scheme with Cognitive Radio
significantly outperform the traditional one in a densely populated network
like SMN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01359</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01359</id><created>2016-03-04</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Learning deep representation of multityped objects and tasks</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a deep multitask architecture to integrate multityped
representations of multimodal objects. This multitype exposition is less
abstract than the multimodal characterization, but more machine-friendly, and
thus is more precise to model. For example, an image can be described by
multiple visual views, which can be in the forms of bag-of-words (counts) or
color/texture histograms (real-valued). At the same time, the image may have
several social tags, which are best described using a sparse binary vector. Our
deep model takes as input multiple type-specific features, narrows the
cross-modality semantic gaps, learns cross-type correlation, and produces a
high-level homogeneous representation. At the same time, the model supports
heterogeneously typed tasks. We demonstrate the capacity of the model on two
applications: social image retrieval and multiple concept prediction. The deep
architecture produces more compact representation, naturally integrates
multiviews and multimodalities, exploits better side information, and most
importantly, performs competitively against baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01360</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01360</id><created>2016-03-04</created><authors><author><keyname>Lample</keyname><forenames>Guillaume</forenames></author><author><keyname>Ballesteros</keyname><forenames>Miguel</forenames></author><author><keyname>Subramanian</keyname><forenames>Sandeep</forenames></author><author><keyname>Kawakami</keyname><forenames>Kazuya</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Neural Architectures for Named Entity Recognition</title><categories>cs.CL</categories><comments>Proceedings of NAACL 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art named entity recognition systems rely heavily on
hand-crafted features and domain-specific knowledge in order to learn
effectively from the small, supervised training corpora that are available. In
this paper, we introduce two new neural architectures---one based on
bidirectional LSTMs and conditional random fields, and the other that
constructs and labels segments using a transition-based approach inspired by
shift-reduce parsers. Our models rely on two sources of information about
words: character-based word representations learned from the supervised corpus
and unsupervised word representations learned from unannotated corpora. Our
models obtain state-of-the-art performance in NER in four languages without
resorting to any language-specific knowledge or resources such as gazetteers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01367</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01367</id><created>2016-03-04</created><authors><author><keyname>Neves</keyname><forenames>Davide</forenames></author><author><keyname>Costa</keyname><forenames>Donovan</forenames></author><author><keyname>Oliveira</keyname><forenames>Marcio</forenames></author><author><keyname>Jardim</keyname><forenames>Ruben</forenames></author><author><keyname>Gouveia</keyname><forenames>Ruben</forenames></author><author><keyname>Karapanos</keyname><forenames>Evangelos</forenames></author></authors><title>Motivating Healthy Water Intake through Prompting, Historical
  Information, and Implicit Feedback</title><categories>cs.HC</categories><comments>In Adjunct Proceedings of Persuasive Technology 2016, Salzburg,
  Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Hydroprompt, a prototype for sensing and motivating healthy water
intake in work environments. In a 3-week field deployment of Hydroprompt, we
evaluate the effectiveness of three approaches to behavior change: historical
information enabling users to compare their water intake lev- els across
different times of day and days of week, implicit feedback providing subtle
cues to users on the current hydration levels, and explicit prompting at-
tempting to remind participants when hydration falls below acceptable levels or
when substantial amount of time has elapsed since the last sip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01369</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01369</id><created>2016-03-04</created><authors><author><keyname>Karapanos</keyname><forenames>Evangelos</forenames></author></authors><title>Designing for Different Stages in Behavior Change</title><categories>cs.HC</categories><comments>In Proceedings of the workshop &quot;Personalization in Persuasive
  Technology&quot;, Persuasive Technology 2016, Salzburg, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior change process is a dynamic journey with different informational
and motivational needs across its different stages; yet current technologies
for behavior change are static. In our recent deployment of Habito, an activity
tracking mobile app, we found individuals &quot;readiness&quot; to behavior change (or
the stage of behavior change they were in) to be a strong predictor of
adoption. Individuals in the contemplation and preparation stages had an
adoption rate of 56%, whereas individuals in precontemplation, action or
maintenance stages had an adoption rate of only 20%. In this position paper we
argue for behavior change technologies that are tailored to the different
stages of behavior change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01372</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01372</id><created>2016-03-04</created><authors><author><keyname>Tichavsky</keyname><forenames>Petr</forenames></author><author><keyname>Phan</keyname><forenames>Anh Huy</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Numerical CP Decomposition of Some Difficult Tensors</title><categories>math.NA cs.NA stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a numerical method is proposed for canonical polyadic (CP)
decomposition of small size tensors. The focus is primarily on decomposition of
tensors that correspond to small matrix multiplications. Here, rank of the
tensors is equal to the smallest number of scalar multiplications that are
necessary to accomplish the matrix multiplication. The proposed method is based
on a constrained Levenberg-Marquardt optimization. Numerical results indicate
the rank and border ranks of tensors that correspond to multiplication of
matrices of the size 2x3 and 3x2, 3x3 and 3x2, 3x3 and 3x3, and 3x4 and 4x3.
The ranks are 11, 15, 23 and 29, respectively. In particular, a novel algorithm
for multiplying the matrices of the sizes 3x3 and 3x2 with 15 multiplications
is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01374</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01374</id><created>2016-03-04</created><authors><author><keyname>Moeller</keyname><forenames>John</forenames></author><author><keyname>Swaminathan</keyname><forenames>Sarathkrishna</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>A Unified View of Localized Kernel Learning</title><categories>cs.LG stat.ML</categories><comments>14 pages, 2 figures, 4 tables. Reformatted version of the accepted
  SDM 2016 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to
learn not only a classifier/regressor but also the best kernel for the training
task, usually from a combination of existing kernel functions. Most MKL methods
seek the combined kernel that performs best over every training example,
sacrificing performance in some areas to seek a global optimum. Localized
kernel learning (LKL) overcomes this limitation by allowing the training
algorithm to match a component kernel to the examples that can exploit it best.
Several approaches to the localized kernel learning problem have been explored
in the last several years. We unify many of these approaches under one simple
system and design a new algorithm with improved performance. We also develop
enhanced versions of existing algorithms, with an eye on scalability and
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01384</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01384</id><created>2016-03-04</created><authors><author><keyname>Gramoli</keyname><forenames>Vincent</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>In the Search of Optimal Concurrency</title><categories>cs.DC</categories><comments>Extended version of results in arXiv:1203.4751</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing a concurrent data structure typically begins with defining its
sequential specification. However, when used \emph{as is}, a nontrivial
sequential data structure, such as a linked list, a search tree, or a hash
table, may expose incorrect behavior: lost updates, inconsistent responses,
etc. To ensure correctness, portions of the sequential code operating on the
shared data must be &quot;protected&quot; from data races using synchronization
primitives and, thus, certain schedules of the steps of concurrent operations
must be rejected. But can we ensure that we do not &quot;overuse&quot; synchronization,
i.e., that we reject a concurrent schedule only if it violates correctness?
  In this paper, we treat this question formally by introducing the notion of a
\emph{concurrency-optimal} implementation. A program's concurrency is defined
here as its ability to accept concurrent schedules, i.e., interleavings of
steps of its sequential implementation. An implementation is
concurrency-optimal if it accepts all interleavings that do not violate the
program's correctness. We explore the concurrency properties of \emph{search}
data structures which can be represented in the form of directed acyclic graphs
exporting insert, delete and search operations. We prove, for the first time,
that \emph{pessimistic} e.g., based on conservative locking) and
\emph{optimistic serializable} e.g., based on serializable transactional
memory) implementations of search data-structures are incomparable in terms of
concurrency. Specifically, there exist simple interleavings of sequential code
that cannot be accepted by \emph{any} pessimistic (and \emph{resp.},
serializable optimistic) implementation, but accepted by a serializable
optimistic one (and \emph{resp.}, pessimistic). Thus, neither of these two
implementation classes is concurrency-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01392</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01392</id><created>2016-03-04</created><authors><author><keyname>Feghhi</keyname><forenames>Saman</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author><author><keyname>Karzand</keyname><forenames>Mohammad</forenames></author></authors><title>Proportional Fair Rate Allocation for Private Shared Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider fair privacy in a shared network subject to
traffic analysis attacks by an eavesdropper. We initiate the study of the joint
trade-off between privacy, throughput and delay in such a shared network as a
utility fairness problem and derive the proportional fair rate allocation for
networks of flows subject to privacy constraints and delay deadlines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01393</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01393</id><created>2016-03-04</created><authors><author><keyname>Duarte</keyname><forenames>Melissa</forenames></author><author><keyname>Feki</keyname><forenames>Afef</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author></authors><title>Inter-User Interference Coordination in Full-Duplex Systems Based on
  Geographical Context Information</title><categories>cs.IT math.IT</categories><comments>ICC2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a coordination scheme to minimize the interference between users
in a cellular network with full-duplex base stations and half-duplex user
devices. Our scheme exploits signal attenuation from obstacles between the
users by (i) extracting spatially isolated regions from a radio map and (ii)
assigning simultaneous co-channel uplink and downlink transmissions to users in
these regions such that inter-user interference is minimized. While adding low
computational complexity and insignificant signaling overhead to existing
deployments, evaluating our solution with real coverage data shows impressive
gains compared to conventional half-duplex and full-duplex operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01399</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01399</id><created>2016-03-04</created><authors><author><keyname>Obuchi</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Sampling approach to sparse approximation problem: determining degrees
  of freedom by simulated annealing</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT stat.ME</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approximation of a high-dimensional vector by a small combination of
column vectors selected from a fixed matrix has been actively debated in
several different disciplines. In this paper, a sampling approach based on the
Monte Carlo method is presented as an efficient solver for such problems.
Especially, the use of simulated annealing (SA), a metaheuristic optimization
algorithm, for determining degrees of freedom (the number of used columns) by
cross validation is focused on and tested. Test on a synthetic model indicates
that our SA-based approach can find a nearly optimal solution for the
approximation problem and, when combined with the CV framework, it can optimize
the generalization ability. Its utility is also confirmed by application to a
real-world supernova data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01404</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01404</id><created>2016-03-04</created><authors><author><keyname>Adan</keyname><forenames>Ivo</forenames></author><author><keyname>Boon</keyname><forenames>Marko</forenames></author><author><keyname>Weiss</keyname><forenames>Gideon</forenames></author></authors><title>Design Heuristic for Parallel Many Server Systems under FCFS-ALIS</title><categories>math.PR cs.PF</categories><msc-class>60J10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a parallel service queueing system with servers of types
$s_1,\ldots,s_J$, customers of types $c_1,\ldots,c_I$, bipartite compatibility
graph $\mathcal{G}$, where arc $(c_i, s_j)$ indicates that server type $s_j$
can serve customer type $c_i$, and service policy of first come first served
FCFS, assign longest idle server ALIS. For a general renewal stream of arriving
customers and general service time distributions, the behavior of such systems
is very complicated, in particular the calculation of matching rates
$r_{c_i,s_j}$, the fraction of services of customers of type $c_i$ by servers
of type $s_j$, is intractable. We suggest through a heuristic argument that if
the number of servers becomes large, the matching rates are well approximated
by matching rates calculated from the tractable FCFS bipartite infinite
matching model. We present simulation evidence to support this heuristic
argument, and show how this can be used to design systems for given performance
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01407</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01407</id><created>2016-03-04</created><authors><author><keyname>Djemame</keyname><forenames>Karim</forenames></author><author><keyname>Armstrong</keyname><forenames>Django</forenames></author><author><keyname>Kavanagh</keyname><forenames>Richard</forenames></author><author><keyname>Deprez</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Ferrer</keyname><forenames>Ana Juan</forenames></author><author><keyname>Perez</keyname><forenames>David Garcia</forenames></author><author><keyname>Badia</keyname><forenames>Rosa</forenames></author><author><keyname>Sirvent</keyname><forenames>Raul</forenames></author><author><keyname>Ejarque</keyname><forenames>Jorge</forenames></author><author><keyname>Georgiou</keyname><forenames>Yiannis</forenames></author></authors><title>TANGO: Transparent heterogeneous hardware Architecture deployment for
  eNergy Gain in Operation</title><categories>cs.SE cs.DC</categories><comments>Part of the Program Transformation for Programmability in
  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March
  2016, 7 pages, LaTeX, 3 PNG figures</comments><acm-class>C.1.4; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with the issue of how software systems actually use
Heterogeneous Parallel Architectures (HPAs), with the goal of optimizing power
consumption on these resources. It argues the need for novel methods and tools
to support software developers aiming to optimise power consumption resulting
from designing, developing, deploying and running software on HPAs, while
maintaining other quality aspects of software to adequate and agreed levels. To
do so, a reference architecture to support energy efficiency at application
construction, deployment, and operation is discussed, as well as its
implementation and evaluation plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01412</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01412</id><created>2016-03-04</created><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Groves</keyname><forenames>Lindsay</forenames></author></authors><title>Contextual trace refinement for concurrent objects: Safety and progress</title><categories>cs.DC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correctness of concurrent objects is defined in terms of safety properties
such as linearizability, sequential consistency, and quiescent consistency, and
progress properties such as wait-, lock-, and obstruction-freedom. These
properties, however, only refer to the behaviours of the object in isolation,
which does not tell us what guarantees these correctness conditions on
concurrent objects provide to their client programs. This paper investigates
the links between safety and progress properties of concurrent objects and a
form of trace refinement for client programs, called contextual trace
refinement. In particular, we show that linearizability together with a minimal
notion of progress are sufficient properties of concurrent objects to ensure
contextual trace refinement, but sequential consistency and quiescent
consistency are both too weak. Our reasoning is carried out in the action
systems framework with procedure calls, which we extend to cope with non-atomic
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01417</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01417</id><created>2016-03-04</created><authors><author><keyname>Xiong</keyname><forenames>Caiming</forenames></author><author><keyname>Merity</keyname><forenames>Stephen</forenames></author><author><keyname>Socher</keyname><forenames>Richard</forenames></author></authors><title>Dynamic Memory Networks for Visual and Textual Question Answering</title><categories>cs.NE cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network architectures with memory and attention mechanisms exhibit
certain reasoning capabilities required for question answering. One such
architecture, the dynamic memory network (DMN), obtained high accuracy on a
variety of language tasks. However, it was not shown whether the architecture
achieves strong results for question answering when supporting facts are not
marked during training or whether it could be applied to other modalities such
as images. Based on an analysis of the DMN, we propose several improvements to
its memory and input modules. Together with these changes we introduce a novel
input module for images in order to be able to answer visual questions. Our new
DMN+ model improves the state of the art on both the Visual Question Answering
dataset and the \babi-10k text question-answering dataset without supporting
fact supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01419</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01419</id><created>2016-03-04</created><authors><author><keyname>Palmer</keyname><forenames>Andrew W.</forenames></author><author><keyname>Hill</keyname><forenames>Andrew J.</forenames></author><author><keyname>Scheding</keyname><forenames>Steven J.</forenames></author></authors><title>Methods for Stochastic Collection and Replenishment (SCAR) optimisation
  for persistent autonomy</title><categories>cs.RO cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consideration of resources such as fuel, battery charge, and storage space,
is a crucial requirement for the successful persistent operation of autonomous
systems. The Stochastic Collection and Replenishment (SCAR) scenario is
motivated by mining and agricultural scenarios where a dedicated replenishment
agent transports a resource between a centralised replenishment point to agents
using the resource in the field. The agents in the field typically operate
within fixed areas (for example, benches in mining applications, and fields or
orchards in agricultural scenarios), and the motion of the replenishment agent
may be restricted by a road network. Existing research has typically approached
the problem of scheduling the actions of the dedicated replenishment agent from
a short-term and deterministic angle. This paper introduces a method of
incorporating uncertainty in the schedule optimisation through a novel
prediction framework, and a branch and bound optimisation method which uses the
prediction framework to minimise the downtime of the agents. The prediction
framework makes use of several Gaussian approximations to quickly calculate the
risk-weighted cost of a schedule. The anytime nature of the branch and bound
method is exploited within an MPC-like framework to outperform existing
optimisation methods while providing reasonable calculation times in large
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01420</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01420</id><created>2016-03-04</created><authors><author><keyname>Benammar</keyname><forenames>Meryem</forenames><affiliation>Shitz</affiliation></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Capacity Results for the Multicast Cognitive Interference Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity region of the Multicast Cognitive Interference Channel (CIFC) is
investigated. This channel consists of two independent transmitters that wish
to multicast two different messages, each of them to a different set of users.
In addition, one of the transmitters --commonly referred to as the cognitive
transmitter-- has prior non-causal knowledge of both messages to be
transmitted. This scenario combines difficulties and challenges arising in the
Interference Channel, the Broadcast Channel and multicasting communications.
Our aim concerns the derivation of optimal interference mitigation techniques
in such a challenging communication setup. We investigate to this end the
multi-primary CIFC and its dual multi-secondary CIFC under various interference
regimes as an attempt to build a thorough understanding for the more general
setting. It is shown that, for some regimes, well-known coding techniques for
the conventional CIFC remain still optimal in the presence of multicasting.
While in other regimes, evolved encoding and/or decoding strategies are
crucial. A careful use of these coding schemes and new outer bounding
techniques allows us to characterize the capacity region for several classes of
discrete memoryless and Gaussian channels in different interference regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01431</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01431</id><created>2016-03-04</created><authors><author><keyname>Arpit</keyname><forenames>Devansh</forenames></author><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Kota</keyname><forenames>Bhargava U.</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Normalization Propagation: A Parametric Technique for Removing Internal
  Covariate Shift in Deep Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the authors of Batch Normalization (BN) identify and address an
important problem involved in training deep networks-- \textit{Internal
Covariate Shift}-- the current solution has multiple drawbacks. For instance,
BN depends on batch statistics for layerwise input normalization during
training which makes the estimates of mean and standard deviation of input
(distribution) to hidden layers inaccurate due to shifting parameter values
(specially during initial training epochs). Another fundamental problem with BN
is that it cannot be used with batch-size $ 1 $ during training. We address
these (and other) drawbacks of BN by proposing a non-adaptive normalization
technique for removing covariate shift, that we call \textit{Normalization
Propagation}. Our approach does not depend on batch statistics, but rather uses
a data-independent parametric estimate of mean and standard-deviation in every
layer; thus being computationally cheaper. We exploit the observation that the
pre-activation before Rectified Linear Units follow a Gaussian distribution in
deep networks, and that once the first and second order statistics of any given
dataset are normalized, we can forward propagate this normalization without the
need for recalculating the approximate statistics (using data) for any of the
hidden layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01436</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01436</id><created>2016-03-04</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Implementation of a Direct Coupling Coherent Quantum Observer including
  Observer Measurements</title><categories>quant-ph cs.SY math.OC</categories><comments>A version of this paper has been accepted by the 2016 American
  Control Conference. arXiv admin note: text overlap with arXiv:1509.01898</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of constructing a direct coupling quantum
observer for a quantum harmonic oscillator system. The proposed observer is
shown to be able to estimate one but not both of the plant variables and
produces a measureable output using homodyne detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01443</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01443</id><created>2016-03-04</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Computing the Power Distribution in the IMF</title><categories>cs.GT</categories><comments>19 pages, 2 figures, 13 tables</comments><msc-class>91B12, 91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The International Monetary Fund is one of the largest international
organizations using a weighted voting system. The weights of its 188 members
are determined by a fixed amount of basic votes plus some extra votes for
so-called Special Drawing Rights (SDR). On January 26, 2016, the conditions for
the SDRs were increased at the 14th General Quota Review, which drastically
changed the corresponding voting weights. However, since the share of voting
weights in general is not equal to the influence, of a committee member on the
committees overall decision, so-called power indices were introduced. So far
the power distribution of the IMF was only computed by either approximation
procedures or smaller games than then entire Board of Governors consisting of
188 members. We improve existing algorithms, based on dynamic programming, for
the computation of power indices and provide the exact results for the IMF
Board of Governors before and after the increase of voting weights. Tuned
low-level details of the algorithms allow the repeated routine with sparse
computational resources and can of course be applied to other large voting
bodies. It turned out that the Banzhaf power shares are rather sensitive to
changes of the quota.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01444</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01444</id><created>2016-03-04</created><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author></authors><title>Computational Aspects of Private Bayesian Persuasion</title><categories>cs.GT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational questions in a game-theoretic model that, in
particular, aims to capture advertising/persuasion applications such as viral
marketing. Specifically, we consider a multi-agent Bayesian persuasion model
where an informed sender (marketer) tries to persuade a group of agents
(consumers) to adopt a certain product. The quality of the product is known to
the sender, but it is unknown to the agents. The sender is allowed to commit to
a signaling policy where she sends a private signal---say, a viral marketing
ad---to every agent. This work studies the computation aspects of finding a
signaling policy that maximizes the sender's revenue.
  We show that if the sender's utility is a submodular function of the set of
agents that adopt the product, then we can efficiently find a signaling policy
whose revenue is at least (1-1/e) times the optimal. We also prove that
approximating the sender's optimal revenue by a factor better than (1-1/e) is
NP-hard and, hence, the developed approximation guarantee is essentially tight.
When the senders' utility is a function of the number of agents that adopt the
product (i.e., the utility function is anonymous), we show that an optimal
signaling policy can be computed in polynomial time. Our results are based on
an interesting connection between the Bayesian persuasion problem and the
evaluation of the concave closure of a set function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01445</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01445</id><created>2016-03-04</created><authors><author><keyname>Sato</keyname><forenames>Tetsuya</forenames></author></authors><title>Approximate Relational Hoare Logic for Continuous Random Samplings</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate relational Hoare logic (apRHL) is a logic for formal verification
of the differential privacy of databases written in the programming language
pWHILE. Strictly speaking, however, this logic deals only with discrete random
samplings. In this paper, we define the graded relational lifting of the
subprobabilistic variant of Giry monad, which described differential privacy.
We extend the logic apRHL with this graded lifting to deal with continuous
random samplings. We give a generic method to give proof rules of apRHL for
continuous random samplings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01450</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01450</id><created>2016-03-04</created><authors><author><keyname>Vahabi</keyname><forenames>Hossein</forenames></author><author><keyname>Lagr&#xe9;e</keyname><forenames>Paul</forenames></author><author><keyname>Vernade</keyname><forenames>Claire</forenames></author><author><keyname>Capp&#xe9;</keyname><forenames>Olivier</forenames></author></authors><title>Sequential ranking under random semi-bandit feedback</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many web applications, a recommendation is not a single item suggested to
a user but a list of possibly interesting contents that may be ranked in some
contexts. The combinatorial bandit problem has been studied quite extensively
these last two years and many theoretical results now exist : lower bounds on
the regret or asymptotically optimal algorithms. However, because of the
variety of situations that can be considered, results are designed to solve the
problem for a specific reward structure such as the Cascade Model. The present
work focuses on the problem of ranking items when the user is allowed to click
on several items while scanning the list from top to bottom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01455</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01455</id><created>2016-03-04</created><authors><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author><author><keyname>Kozen</keyname><forenames>Dexter</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Wi&#xdf;mann</keyname><forenames>Thorsten</forenames></author></authors><title>Nominal Automata with Name Binding</title><categories>cs.FL cs.LO</categories><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata models for data languages (i.e. languages over infinite alphabets)
often feature either global or local freshness operators. We show that Bollig
et al.'s session automata, which focus on global freshness, are equivalent to
regular nondeterministic nominal automata (RNNA), a natural nominal automaton
model with explicit name binding that has appeared implicitly in the semantics
of nominal Kleene algebra (NKA), an extension of Kleene algebra with name
binding. The expected Kleene theorem for NKA is known to fail in one direction,
i.e. there are nominal languages that can be accepted by an RNNA but are not
definable in NKA; via session automata, we obtain a full Kleene theorem for
RNNAs for an expression language that extends NKA with unscoped name binding.
Based on the equivalence with RNNAs, we then slightly rephrase the known
equivalence checking algorithm for session automata. Reinterpreting the data
language semantics of name binding by unrestricted instead of clean
alpha-equivalence, we obtain a local freshness semantics as a quotient of the
global freshness semantics. Under local freshness semantics, RNNAs turn out to
be equivalent to a natural subclass of Bojanczyk et al.'s nondeterministic
orbit-finite automata. We establish decidability of inclusion under local
freshness by modifying the RNNA-based algorithm; in summary, we obtain a
formalism for local freshness in data languages that is reasonably expressive
and has a decidable inclusion problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01468</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01468</id><created>2016-03-04</created><authors><author><keyname>Jardel</keyname><forenames>Fanny</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph J.</forenames></author></authors><title>Edge Coloring and Stopping Sets Analysis in Product Codes with MDS
  components</title><categories>cs.IT math.IT</categories><comments>82 pages, 14 figures, and 4 tables, Submitted to the IEEE
  Transactions on Information Theory, Dec. 2015, paper IT-15-1104</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider non-binary product codes with MDS components and their iterative
row-column algebraic decoding on the erasure channel. Both independent and
block erasures are considered in this paper. A compact graph representation is
introduced on which we define double-diversity edge colorings via the rootcheck
concept. An upper bound of the number of decoding iterations is given as a
function of the graph size and the color palette size $M$. Stopping sets are
defined in the context of MDS components and a relationship is established with
the graph representation. A full characterization of these stopping sets is
given up to a size $(d_1+1)(d_2+1)$, where $d_1$ and $d_2$ are the minimum
Hamming distances of the column and row MDS components respectively. Then, we
propose a differential evolution edge coloring algorithm that produces
colorings with a large population of minimal rootcheck order symbols. The
complexity of this algorithm per iteration is $o(M^{\aleph})$, for a given
differential evolution parameter $\aleph$, where $M^{\aleph}$ itself is small
with respect to the huge cardinality of the coloring ensemble. The performance
of MDS-based product codes with and without double-diversity coloring is
analyzed in presence of both block and independent erasures. In the latter
case, ML and iterative decoding are proven to coincide at small channel erasure
probability. Furthermore, numerical results show excellent performance in
presence of unequal erasure probability due to double-diversity colorings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01472</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01472</id><created>2016-03-04</created><authors><author><keyname>Wang</keyname><forenames>Yinan</forenames></author><author><keyname>Johansson</keyname><forenames>Hakan</forenames></author><author><keyname>Xu</keyname><forenames>Hui</forenames></author><author><keyname>Diao</keyname><forenames>Jietao</forenames></author></authors><title>Minimax Design and Order Estimation of FIR Filters for Extending the
  Bandwidth of ADCs</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, IEEE Int. Symp. Circuits Syst. (to appear),
  Montreal, Canada, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bandwidth of the sampling systems, especially for time-interleaved
analog-to-digital converters, needs to be extended along with the rapid
increase of the sampling rate. A digitally assisted technique becomes a
feasible approach to extend the analog bandwidth, as it is impractical to
implement the extension in analog circuits. This paper derives accurate order
estimation formulas for the bandwidth extension filter, which is designed in
the minimax sense with the ripple constraints as the design criteria. The
derived filter order estimation is significant in evaluating the computational
complexity from the viewpoint of the top-level system design. Moreover, with
the proposed order estimates, one can conveniently obtain the minimal order
that satisfies the given ripple constraints, which contributes to reducing the
design time. Both the performance of the extension filter and its order
estimation are illustrated and demonstrated through simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01486</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01486</id><created>2016-03-04</created><authors><author><keyname>Harris</keyname><forenames>David G.</forenames></author><author><keyname>Schneider</keyname><forenames>Johannes</forenames></author><author><keyname>Su</keyname><forenames>Hsin-Hao</forenames></author></authors><title>Distributed $(\Delta+1)$-Coloring in Sublogarithmic Rounds</title><categories>cs.DS cs.DC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $(\Delta+1)$-coloring problem is a fundamental symmetry breaking problem
in distributed computing. We give a new randomized coloring algorithm for
$(\Delta+1)$-coloring running in $O(\sqrt{\log \Delta})+ 2^{O(\sqrt{\log \log
n})}$ rounds with probability $1-1/n^{\Omega(1)}$ in a graph with $n$ nodes and
maximum degree $\Delta$. This implies that the $(\Delta+1)$-coloring problem is
easier than the maximal independent set problem and the maximal matching
problem, due to their lower bounds by Kuhn, Moscibroda, and Wattenhofer
[PODC'04]. Our algorithm also extends to the list-coloring problem where the
palette of each node contains $\Delta+1$ colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01488</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01488</id><created>2016-03-03</created><authors><author><keyname>Basso-Blandin</keyname><forenames>Adrien</forenames><affiliation>LIP, ENS Lyon</affiliation></author><author><keyname>Fontana</keyname><forenames>Walter</forenames><affiliation>Harvard Medical School</affiliation></author><author><keyname>Harmer</keyname><forenames>Russ</forenames><affiliation>CNRS &amp; LIP, ENS Lyon</affiliation></author></authors><title>A knowledge representation meta-model for rule-based modelling of
  signalling networks</title><categories>cs.AI q-bio.MN</categories><comments>In Proceedings DCM 2015, arXiv:1603.00536</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 204, 2016, pp. 47-59</journal-ref><doi>10.4204/EPTCS.204.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of cellular signalling pathways and their deregulation in disease
states, such as cancer, is a large and extremely complex task. Indeed, these
systems involve many parts and processes but are studied piecewise and their
literatures and data are consequently fragmented, distributed and sometimes&#xe2;&#x80;&#x94;at
least apparently&#xe2;&#x80;&#x94;inconsistent. This makes it extremely difficult to build
significant explanatory models with the result that effects in these systems
that are brought about by many interacting factors are poorly understood.
  The rule-based approach to modelling has shown some promise for the
representation of the highly combinatorial systems typically found in
signalling where many of the proteins are composed of multiple binding domains,
capable of simultaneous interactions, and/or peptide motifs controlled by
post-translational modifications. However, the rule-based approach requires
highly detailed information about the precise conditions for each and every
interaction which is rarely available from any one single source. Rather, these
conditions must be painstakingly inferred and curated, by hand, from
information contained in many papers&#xe2;&#x80;&#x94;each of which contains only part of the
story.
  In this paper, we introduce a graph-based meta-model, attuned to the
representation of cellular signalling networks, which aims to ease this massive
cognitive burden on the rule-based curation process. This meta-model is a
generalization of that used by Kappa and BNGL which allows for the flexible
representation of knowledge at various levels of granularity. In particular, it
allows us to deal with information which has either too little, or too much,
detail with respect to the strict rule-based meta-model. Our approach provides
a basis for the gradual aggregation of fragmented biological knowledge
extracted from the literature into an instance of the meta-model from which we
can define an automated translation into executable Kappa programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01489</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01489</id><created>2016-03-04</created><authors><author><keyname>Cody-Kenny</keyname><forenames>Brendan</forenames></author><author><keyname>Barrett</keyname><forenames>Stephen</forenames></author></authors><title>Performance Localisation</title><categories>cs.SE cs.NE cs.PF</categories><comments>In Preparation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profiling is a prominent technique for finding the location of performance
&quot;bottlenecks&quot; in code. Profiling can be performed by adding code to a program
which increments a counter for each line of code each time it is executed. Any
lines of code which have a large execution count relative to other lines in the
program can be considered a bottleneck. Though code profiling can determine the
location of a performance issue or bottleneck, we posit that the code change
required to improve performance may not always be found at the same location.
Developers must frequently trace back through a program to understand what code
is contributing to a bottleneck. We seek to highlight code which is likely
causing or has the most effect on the overall execution cost of a program. In
this document we compare different methods for localising potential performance
improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01507</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01507</id><created>2016-03-04</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Li</keyname><forenames>Dongfang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author></authors><title>Improved Sufficient Conditions for Sparse Recovery with Generalized
  Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized orthogonal matching pursuit (gOMP), also called the
orthogonal multi-matching pursuit (OMMP), is a natural generation of the
well-known orthogonal matching pursuit (OMP) in the sense that multiple $N
(N\geq1)$ indices are identified per iteration. Sufficient conditions for
sparse recovery with OMP and gOMP under restricted isometry property of a
sensing matrix have received much attention in recent years. In this paper, we
show that if the restricted isometry constant $\delta_{NK+1}$ of the sensing
matrix $\A$ satisfies $\delta_{NK+1} &lt; 1/\sqrt {K/N+1} $, then under some
conditions on the signal-to-noise ratio (SNR), gOMP recovers at least one index
in the support of the $K$-sparse signal $\x$ from an observation vector
$\y=\A\x+\v$ (where $\v$ is a noise vector) in each iteration. Surprisingly,
this condition do not require $N\leq K$ which is needed in Wang, \textit{et al}
2012 and Liu, \textit{et al} 2012. Thus, $N$ can have more choices. When $N=1$,
this sufficient condition turns to be a sufficient condition for support
recovery with OMP. We show that it is weaker than that in Wang 2015 in terms of
both SNR and RIP. Moreover, in the noise free case, we obtain that
$\delta_{NK+1} &lt; 1/\sqrt {K/N+1} $ is a sufficient condition for recovering the
$K$-sparse signal $\x$ with gOMP in $K$ iterations which is better than the
best known one in terms of $\delta_{NK+1}$. In particular, this condition is
sharp when $N=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01508</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01508</id><created>2016-03-04</created><authors><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>Inferential Privacy Guarantees for Differentially Private Mechanisms</title><categories>cs.DS cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The correlations and network structure amongst individuals in datasets
today---whether explicitly articulated, or deduced from biological or
behavioral connections---pose new issues around privacy guarantees, because of
inferences that can be made about one individual from another's data. This
motivates quantifying privacy in networked contexts in terms of &quot;inferential
privacy&quot;---which measures the change in beliefs about an individual's data from
the result of a computation---as originally proposed by Dalenius in the 1970's.
Inferential privacy is implied by differential privacy when data are
independent, but can be much worse when data are correlated; indeed, simple
examples, as well as a general impossibility theorem of Dwork and Naor,
preclude the possibility of achieving non-trivial inferential privacy when the
adversary can have arbitrary auxiliary information. In this paper, we ask how
differential privacy guarantees translate to guarantees on inferential privacy
in networked contexts: specifically, under what limitations on the adversary's
information about correlations, modeled as a prior distribution over datasets,
can we deduce an inferential guarantee from a differential one?
  We prove two main results. The first result pertains to distributions that
satisfy a natural positive-affiliation condition, and gives an upper bound on
the inferential privacy guarantee for any differentially private mechanism.
This upper bound is matched by a simple mechanism that adds Laplace noise to
the sum of the data. The second result pertains to distributions that have weak
correlations, defined in terms of a suitable &quot;influence matrix&quot;. The result
provides an upper bound for inferential privacy in terms of the differential
privacy parameter and the spectral norm of this matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01511</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01511</id><created>2016-03-04</created><authors><author><keyname>Shao</keyname><forenames>Chengcheng</forenames></author><author><keyname>Ciampaglia</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Hoaxy: A Platform for Tracking Online Misinformation</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 6 figures, submitted to Third Workshop on Social News On the
  Web</comments><doi>10.1145/2872518.2890098</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive amounts of misinformation have been observed to spread in
uncontrolled fashion across social media. Examples include rumors, hoaxes, fake
news, and conspiracy theories. At the same time, several journalistic
organizations devote significant efforts to high-quality fact checking of
online claims. The resulting information cascades contain instances of both
accurate and inaccurate information, unfold over multiple time scales, and
often reach audiences of considerable size. All these factors pose challenges
for the study of the social dynamics of online news sharing. Here we introduce
Hoaxy, a platform for the collection, detection, and analysis of online
misinformation and its related fact-checking efforts. We discuss the design of
the platform and present a preliminary analysis of a sample of public tweets
containing both fake news and fact checking. We find that, in the aggregate,
the sharing of fact-checking content typically lags that of misinformation by
10--20 hours. Moreover, fake news are dominated by very active users, while
fact checking is a more grass-roots activity. With the increasing risks
connected to massive online misinformation, social news observatories have the
potential to help researchers, journalists, and the general public understand
the dynamics of real and fake news sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01512</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01512</id><created>2016-03-04</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author></authors><title>Rapidly Mixing Markov Chains: A Comparison of Techniques (A Survey)</title><categories>cs.DS</categories><comments>Unpublished and written in 2000; posted in original, unedited form to
  provide a permanent URL for citations; Disclaimer: This unpublished survey
  was written in 2000, and is being posted unedited in its original form, in
  response to requests for a permanent URL that can be cited. It is thus
  outdated and does not reflect the state of the art in 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey existing techniques to bound the mixing time of Markov chains. The
mixing time is related to a geometric parameter called conductance which is a
measure of edge-expansion. Bounds on conductance are typically obtained by a
technique called &quot;canonical paths&quot; where the idea is to find a set of paths,
one between every source-destination pair, such that no edge is heavily
congested. However, the canonical paths approach cannot always show rapid
mixing of a rapidly mixing chain. This drawback disappears if we allow the flow
between a pair of states to be spread along multiple paths. We prove that for a
large class of Markov chains canonical paths does capture rapid mixing.
Allowing multiple paths to route the flow still does help a great deal in
proofs, as illustrated by a result of Morris &amp; Sinclair (FOCS'99) on the rapid
mixing of a Markov chain for sampling 0/1 knapsack solutions.
  A different approach to prove rapid mixing is &quot;Coupling&quot;. Path Coupling is a
variant discovered by Bubley &amp; Dyer (FOCS'97) that often tremendously reduces
the complexity of designing good Couplings. We present several applications of
Path Coupling in proofs of rapid mixing. These invariably lead to much better
bounds on mixing time than known using conductance, and moreover Coupling based
proofs are typically simpler. This motivates the question of whether Coupling
can be made to work whenever the chain is rapidly mixing. This question was
answered in the negative by Kumar &amp; Ramesh (FOCS'99), who showed that no
Coupling strategy can prove the rapid mixing of the Jerrum-Sinclair chain for
sampling perfect and near-perfect matchings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01514</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01514</id><created>2016-03-04</created><authors><author><keyname>Garg</keyname><forenames>Nikhil</forenames></author><author><keyname>Henderson</keyname><forenames>James</forenames></author></authors><title>A Bayesian Model of Multilingual Unsupervised Semantic Role Induction</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian model of unsupervised semantic role induction in
multiple languages, and use it to explore the usefulness of parallel corpora
for this task. Our joint Bayesian model consists of individual models for each
language plus additional latent variables that capture alignments between roles
across languages. Because it is a generative Bayesian model, we can do
evaluations in a variety of scenarios just by varying the inference procedure,
without changing the model, thereby comparing the scenarios directly. We
compare using only monolingual data, using a parallel corpus, using a parallel
corpus with annotations in the other language, and using small amounts of
annotation in the target language. We find that the biggest impact of adding a
parallel corpus to training is actually the increase in mono-lingual data, with
the alignments to another language resulting in small improvements, even with
labeled data for the other language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01520</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01520</id><created>2016-03-04</created><authors><author><keyname>Bonilla</keyname><forenames>Daniel Rubio</forenames></author><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author><author><keyname>Kuper</keyname><forenames>Jan</forenames></author></authors><title>Optimized Polynomial Evaluation with Semantic Annotations</title><categories>cs.PL cs.CL</categories><acm-class>B.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss how semantic annotations can be used to introduce
mathematical algorithmic information of the underlying imperative code to
enable compilers to produce code transformations that will enable better
performance. By using this approaches not only good performance is achieved,
but also better programmability, maintainability and portability across
different hardware architectures. To exemplify this we will use polynomial
equations of different degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01523</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01523</id><created>2016-03-04</created><authors><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author><author><keyname>Gall</keyname><forenames>Manuel</forenames></author><author><keyname>Fdhila</keyname><forenames>Walid</forenames></author><author><keyname>Mangler</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Indiono</keyname><forenames>Conrad</forenames></author></authors><title>Collecting Examples for Instance-Spanning Constraints</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a meta analysis of various sources from literature,
research projects, and experience with the goal of collecting examples for
instance-spanning constraints to be implemented through Process-Aware
Information Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01524</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01524</id><created>2016-03-04</created><authors><author><keyname>Nehama</keyname><forenames>Ilan</forenames></author></authors><title>Analyzing Games with Ambiguous Player Types using the ${\rm MINthenMAX}$
  Decision Model</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many common interactive scenarios, participants lack information about
other participants, and specifically about the preferences of other
participants. In this work, we model an extreme case of incomplete information,
which we term games with type ambiguity, where a participant lacks even
information enabling him to form a belief on the preferences of others. Under
type ambiguity, one cannot analyze the scenario using the commonly used
Bayesian framework, and therefore he needs to model the participants using a
different decision model.
  In this work, we present the ${\rm MINthenMAX}$ decision model under
ambiguity. This model is a refinement of Wald's MiniMax principle, which we
show to be too coarse for games with type ambiguity. We characterize ${\rm
MINthenMAX}$ as the finest refinement of the MiniMax principle that satisfies
three properties we claim are necessary for games with type ambiguity. This
prior-less approach we present her also follows the common practice in computer
science of worst-case analysis.
  Finally, we define and analyze the corresponding equilibrium concept assuming
all players follow ${\rm MINthenMAX}$. We demonstrate this equilibrium by
applying it to two common economic scenarios: coordination games and bilateral
trade. We show that in both scenarios, an equilibrium in pure strategies always
exists and we analyze the equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01529</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01529</id><created>2016-03-04</created><authors><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author><author><keyname>Shoker</keyname><forenames>Ali</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author></authors><title>Delta State Replicated Data Types</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.2803</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CRDTs are distributed data types that make eventual consistency of a
distributed object possible and non ad-hoc. Specifically, state-based CRDTs
ensure convergence through disseminating the entire state, that may be large,
and merging it to other replicas; whereas operation-based CRDTs disseminate
operations (i.e., small states) assuming an exactly-once reliable dissemination
layer. We introduce Delta State Conflict-Free Replicated Data Types
($\delta$-CRDTs) that can achieve the best of both worlds: small messages with
an incremental nature, as in operation-based CRDTs, disseminated over
unreliable communication channels, as in traditional state-based CRDTs. This is
achieved by defining delta mutators to return a delta-state, typically with a
much smaller size than the full state, that to be joined with both local and
remote states. We introduce the $\delta$-CRDT framework, and we explain it
through establishing a correspondence to current state-based CRDTs. In
addition, we present an anti-entropy algorithm for eventual convergence, and
another one that ensures causal consistency. Finally, we introduce several
$\delta$-CRDT specifications of both well-known replicated datatypes and novel
datatypes, including a generic map composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01536</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01536</id><created>2016-03-04</created><authors><author><keyname>Idrees</keyname><forenames>Kamran</forenames></author><author><keyname>Fuchs</keyname><forenames>Tobias</forenames></author><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author></authors><title>Effective use of the PGAS Paradigm: Driving Transformations and
  Self-Adaptive Behavior in DASH-Applications</title><categories>cs.DC</categories><comments>10 pages, 8 figures, Program Transformation for Programmability in
  Heterogeneous Architectures (PROHA) Workshop, held in conjunction with the
  International Symposium on Code Generation and Optimization (CGO) 2016</comments><acm-class>D.1.2; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DASH is a library of distributed data structures and algorithms designed for
running the applications on modern HPC architectures, composed of hierarchical
network interconnections and stratified memory. DASH implements a PGAS
(partitioned global address space) model in the form of C++ templates, built on
top of DART -- a run-time system with an abstracted tier above existing
one-sided communication libraries.
  In order to facilitate the application development process for exploiting the
hierarchical organization of HPC machines, DART allows to reorder the placement
of the computational units. In this paper we present an automatic, hierarchical
units mapping technique (using a similar approach to the Hilbert curve
transformation) to reorder the placement of DART units on the Cray XC40 machine
Hazel Hen at HLRS. To evaluate the performance of new units mapping which takes
into the account the topology of allocated compute nodes, we perform latency
benchmark for a 3D stencil code. The technique of units mapping is generic and
can be be adopted in other DART communication substrates and on other hardware
platforms.
  Furthermore, high--level features of DASH are presented, enabling more
complex automatic transformations and optimizations in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01541</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01541</id><created>2016-03-04</created><authors><author><keyname>Naaijer</keyname><forenames>Martijn</forenames></author><author><keyname>Roorda</keyname><forenames>Dirk</forenames></author></authors><title>Parallel Texts in the Hebrew Bible, New Methods and Visualizations</title><categories>cs.CL</categories><comments>15 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this article we develop an algorithm to detect parallel texts in the
Masoretic Text of the Hebrew Bible. The results are presented online and
chapters in the Hebrew Bible containing parallel passages can be inspected
synoptically. Differences between parallel passages are highlighted. In a
similar way the MT of Isaiah is presented synoptically with 1QIsaa. We also
investigate how one can investigate the degree of similarity between parallel
passages with the help of a case study of 2 Kings 19-25 and its parallels in
Isaiah, Jeremiah and 2 Chronicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01542</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01542</id><created>2016-03-04</created><authors><author><keyname>Kumar</keyname><forenames>Vinod</forenames></author><author><keyname>Pandey</keyname><forenames>S. K.</forenames></author><author><keyname>Kumar</keyname><forenames>Rajendra</forenames></author></authors><title>Centralized group key management scheme for secure multicast
  communication without re-keying</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the secure group communication, data is transmitted in such a way that
only the group members are able to receive the messages. The main problem in
the solution using symmetric key is heavy re-keying cost. To reduce re-keying
cost tree based architecture is used. But it requires extra overhead to balance
the key- tree in order to achieve logarithmic re-keying cost. The main
challenging issue in dynamic and secure multimedia multicast communication is
to design a centralized group key management scheme with minimal computational,
communicational and storages complexities without breaching security issues.
Several authors have proposed different centralized group key management
schemes, wherein one of them proposes reducing communicational complexity but
increases computational and storage costs however another proposes decreasing
the computational and storage costs which eventually breaches forward and
backward secrecy. In this paper we propose a comparatively more efficient
centralized group key management scheme that not only minimize the
computational, communicational and storages complexities but also maintaining
the security at the optimal level. The message encryptions and decryptions
costs are also minimized. Further, we also provide an extended multicast
scheme, in which the several requests towards leaving or joining the group can
be done by large number of members simultaneously. In order to obtain better
performance of multicast encryption, the symmetric-key and asymmetric-key
cryptosystems may be combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01547</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01547</id><created>2016-03-04</created><authors><author><keyname>Kadlec</keyname><forenames>Rudolf</forenames></author><author><keyname>Schmid</keyname><forenames>Martin</forenames></author><author><keyname>Bajgar</keyname><forenames>Ondrej</forenames></author><author><keyname>Kleindienst</keyname><forenames>Jan</forenames></author></authors><title>Text Understanding with the Attention Sum Reader Network</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several large cloze-style context-question-answer datasets have been
introduced recently: the CNN and Daily Mail news data and the Children's Book
Test. Thanks to the size of these datasets, the associated text comprehension
task is well suited for deep-learning techniques that currently seem to
outperform all alternative approaches. We present a new, simple model that uses
attention to directly pick the answer from the context as opposed to computing
the answer using a blended representation of words in the document as is usual
in similar models. This makes the model particularly suitable for
question-answering problems where the answer is a single word from the
document. Our model outperforms models previously proposed for these tasks by a
large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01562</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01562</id><created>2016-03-04</created><authors><author><keyname>Le</keyname><forenames>Ellen B.</forenames></author><author><keyname>Myers</keyname><forenames>Aaron</forenames></author><author><keyname>Bui-Thanh</keyname><forenames>Tan</forenames></author></authors><title>A Randomized Misfit Approach for Data Reduction in Large-Scale Inverse
  Problems</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized misfit approach as a data reduction method for
large-scale inverse problems via a novel randomization of the objective
function. Derived using stochastic programming, the method may be understood as
an application of a Johnson-Lindenstrauss transform or random projection to
reduce the dimension of the data-misfit vector. Using large deviation bounds,
it is shown that the solution of this reduced inverse problem with a
significantly reduced data dimension is a discrepancy principle-satisfying
solution to the original problem. The purpose of this paper is to develop a
systematic framework for the application of random projections in the context
of ill-posed inverse problems with large data sets and give theoretical insight
into the efficacy. This randomized misfit approach permits the use of a large
class of distributions from the extensive literature on random projections. In
particular, we analyze sparse random projections which have additional
data-reduction properties. We also provide a different proof for a variant of
the Johnson-Lindenstrauss lemma. This proof provides intuition into the
$O(\varepsilon^{-2})$ factor. The main contribution of this paper is a
theoretical result that shows the method efficacy is attributable to the
combination of both Johnson-Lindenstrauss theory and Morozov's discrepancy
principle. This result provides justification for the suitability of the
proposed approach in solving inverse problems with big data. Numerical
verification of theoretical findings are presented for model problems of
estimating a distributed parameter in an elliptic partial differential
equation. Results with different random projections are presented to
demonstrate the viability and accuracy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01564</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01564</id><created>2016-03-04</created><authors><author><keyname>Gualtieri</keyname><forenames>Marcus</forenames></author><author><keyname>Pas</keyname><forenames>Andreas ten</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Platt</keyname><forenames>Robert</forenames></author></authors><title>High precision grasp pose detection in dense clutter</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper considers the problem of grasp pose detection in point clouds. We
follow a general algorithmic structure that first generates a large set of
6-DOF grasp candidates and then classifies each of them as a good or a bad
grasp. Our focus in this paper is on improving the second step by using depth
sensor scans from large online datasets to train a convolutional neural
network. We propose two new representations of grasp candidates, and we
quantify the effect of using prior knowledge of two forms: instance or category
knowledge of the object to be grasped, and pretraining the network on simulated
depth data obtained from idealized CAD models. Our analysis shows that a more
informative grasp candidate representation as well as pretraining and prior
knowledge significantly improve grasp detection. We evaluate our approach on a
Baxter Research Robot and demonstrate an average grasp success rate of 93% in
dense clutter. This is a 20% improvement compared to our prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01566</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01566</id><created>2016-03-04</created><authors><author><keyname>Comon</keyname><forenames>Pierre</forenames></author><author><keyname>Qi</keyname><forenames>Yang</forenames></author><author><keyname>Usevich</keyname><forenames>Konstantin</forenames></author></authors><title>X-rank and identifiability for a polynomial decomposition model</title><categories>cs.IT math.IT math.NA stat.ML</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a polynomial decomposition model that arises in
problems of system identification, signal processing and machine learning. We
show that this decomposition is a special case of the X-rank decomposition ---
a powerful novel concept in algebraic geometry that generalizes the tensor CP
decomposition. We prove new results on generic/maximal rank and on
identifiability of the polynomial decomposition model. In the paper, we try to
make results and basic tools accessible for a general audience (assuming no
knowledge of algebraic geometry or its prerequisites).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01570</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01570</id><created>2016-03-04</created><authors><author><keyname>Amornbunchornvej</keyname><forenames>Chainarong</forenames></author><author><keyname>Brugere</keyname><forenames>Ivan</forenames></author><author><keyname>Strandburg-Peshkin</keyname><forenames>Ariana</forenames></author><author><keyname>Farine</keyname><forenames>Damien</forenames></author><author><keyname>Crofoot</keyname><forenames>Margaret C.</forenames></author><author><keyname>Berger-Wolf</keyname><forenames>Tanya Y.</forenames></author></authors><title>FLICA: A Framework for Leader Identification in Coordinated Activity</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leadership is an important aspect of social organization that affects the
processes of group formation, coordination, and decision-making in human
societies, as well as in the social system of many other animal species. The
ability to identify leaders based on their behavior and the subsequent
reactions of others opens opportunities to explore how group decisions are
made. Understanding who exerts influence provides key insights into the
structure of social organizations. In this paper, we propose a simple yet
powerful leadership inference framework extracting group coordination periods
and determining leadership based on the activity of individuals within a group.
We are able to not only identify a leader or leaders but also classify the type
of leadership model that is consistent with observed patterns of group
decision-making. The framework performs well in differentiating a variety of
leadership models (e.g. dictatorship, linear hierarchy, or local influence). We
propose five simple features that can be used to categorize characteristics of
each leadership model, and thus make model classification possible. The
proposed approach automatically (1) identifies periods of coordinated group
activity, (2) determines the identities of leaders, and (3) classifies the
likely mechanism by which the group coordination occurred. We demonstrate our
framework on both simulated and real-world data: GPS tracks of a baboon troop
and video-tracking of fish schools, as well as stock market closing price data
of the NASDAQ index. The results of our leadership model are consistent with
ground-truthed biological data and the framework finds many known events in
financial data which are not otherwise reflected in the aggregate NASDAQ index.
Our approach is easily generalizable to any coordinated activity data from
interacting entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01573</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01573</id><created>2016-03-04</created><authors><author><keyname>Chv&#xe1;tal</keyname><forenames>Va&#x161;ek</forenames></author><author><keyname>Goldsmith</keyname><forenames>Mark</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author></authors><title>McCulloch-Pitts brains and pseudorandom functions</title><categories>math.DS cs.CR</categories><comments>Supersedes arXiv:1311.6531 [math.DS]</comments><msc-class>92B20, 65C10, 37B15, 62P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model
of the central nervous system. Motivated by EEG recordings of normal brain
activity, Chv\'atal and Goldsmith asked whether or not these dynamical systems
can be engineered to produce trajectories which are irregular, disorderly,
apparently unpredictable. We show that they cannot build weak pseudorandom
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01581</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01581</id><created>2016-03-04</created><authors><author><keyname>Geiger</keyname><forenames>Philipp</forenames></author><author><keyname>Carata</keyname><forenames>Lucian</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Causal models for debugging and control in cloud computing</title><categories>cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two challenges in the theory and practice of cloud computing are: (1) smart
control (allocation) of resources under exploration constraints, on
time-varying systems, and (2) understanding and debugging of the performance of
complex systems that involve e.g. virtualization. In this paper, we examine how
these challenges can be approached using causal models. For challenge (1) we
investigate how to infer and use causal models and selection diagrams to design
and integrate experiments in a principled way, as well as to cope with
partially varying systems. For challenge (2) we examine how to formalize
performance attribution and debugging questions by counterfactual
probabilities, and how to approximately answer them based on inferred
(non-deterministic) graphical causal models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01583</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01583</id><created>2016-03-04</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Randomized algorithms for finding a majority element</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ colored balls, we want to detect if more than $\lfloor n/2\rfloor$
of them have the same color, and if so find one ball with such majority color.
We are only allowed to choose two balls and compare their colors, and the goal
is to minimize the total number of such operations. A well-known exercise is to
show how to find such a ball with only $2n$ comparisons while using only a
logarithmic number of bits for bookkeeping. The resulting algorithm is called
the Boyer--Moore majority vote algorithm. It is known that any deterministic
method needs $\lceil 3n/2\rceil-2$ comparisons in the worst case, and this is
tight. However, it is not clear what is the required number of comparisons if
we allow randomization. We construct a randomized algorithm which always
correctly finds a ball of the majority color (or detects that there is none)
using, with high probability, only $7n/6+o(n)$ comparisons. We also prove that
the expected number of comparisons used by any such randomized method is at
least $1.038n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01592</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01592</id><created>2016-03-04</created><authors><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Cheng</keyname><forenames>Cheng</forenames></author><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author><author><keyname>Wang</keyname><forenames>Haichao</forenames></author></authors><title>Phase Retrieval of Real-Valued Signals in a Shift-Invariant Space</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase retrieval arises in various fields of science and engineering and it is
well studied in a finite-dimensional setting. In this paper, we consider an
infinite-dimensional phase retrieval problem to reconstruct real-valued signals
living in a shift-invariant space from its phaseless samples taken either on
the whole line or on a set with finite sampling rate. We find the equivalence
between nonseparability of signals in a linear space and its phase
retrievability with phaseless samples taken on the whole line. For a spline
signal of order $N$, we show that it can be well approximated, up to a sign,
from its noisy phaseless samples taken on a set with sampling rate $2N-1$. We
propose an algorithm to reconstruct nonseparable signals in a shift-invariant
space generated by a compactly supported continuous function. The proposed
algorithm is robust against bounded sampling noise and it could be implemented
in a distributed manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01595</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01595</id><created>2016-03-04</created><authors><author><keyname>Hamdan</keyname><forenames>Hussam</forenames></author><author><keyname>Bellot</keyname><forenames>Patrice</forenames></author><author><keyname>Bechet</keyname><forenames>Frederic</forenames></author></authors><title>Sentiment Analysis in Scholarly Book Reviews</title><categories>cs.CL cs.AI</categories><comments>10 pages</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  So far different studies have tackled the sentiment analysis in several
domains such as restaurant and movie reviews. But, this problem has not been
studied in scholarly book reviews which is different in terms of review style
and size. In this paper, we propose to combine different features in order to
be presented to a supervised classifiers which extract the opinion target
expressions and detect their polarities in scholarly book reviews. We construct
a labeled corpus for training and evaluating our methods in French book
reviews. We also evaluate them on English restaurant reviews in order to
measure their robustness across the domains and languages. The evaluation shows
that our methods are enough robust for English restaurant reviews and French
book reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01597</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01597</id><created>2016-03-04</created><authors><author><keyname>Kestemont</keyname><forenames>Mike</forenames></author><author><keyname>De Gussem</keyname><forenames>Jeroen</forenames></author></authors><title>Integrated Sequence Tagging for Medieval Latin Using Deep Representation
  Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider two sequence tagging tasks for medieval Latin:
part-of-speech tagging and lemmatization. These are both basic, yet
foundational preprocessing steps in applications such as text re-use detection.
Nevertheless, they are generally complicated by the considerable orthographic
variation which is typical of medieval Latin. In Digital Classics, these tasks
are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.
For example, a lexicon is used to generate all the potential lemma-tag pairs
for a token, and next, a context-aware PoS-tagger is used to select the most
appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,
error percolation is a major downside of such approaches. In this paper we
explore the possibility to elegantly solve these tasks using a single,
integrated approach. For this, we make use of a layered neural network
architecture from the field of deep representation learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01607</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01607</id><created>2016-03-04</created><authors><author><keyname>Campbell</keyname><forenames>Newton</forenames><suffix>Jr</suffix></author></authors><title>Computing Shortest Paths Using A*, Landmarks, and Polygon Inequalities
  (Abstract)</title><categories>cs.DS</categories><comments>Abstract for poster presented at the SIAM 2015 Workshop on Network
  Science in Snowbird, UT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new heuristic for the A* algorithm that references a data
structure much smaller than the one required by the ALT heuristic. This
heuristic's benefits are permitted by a new approach for computing lower bounds
using generalized polygon inequalities, leveraging distance information from
two landmarks as opposed to the common single landmark paradigm. In this paper,
we demonstrate that this heuristic stores a reduced amount of preprocessing
information in comparison to previous landmark algorithms while performing
faster search queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01633</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01633</id><created>2016-03-04</created><authors><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author><author><keyname>Boufounos</keyname><forenames>Petros T.</forenames></author></authors><title>Depth Superresolution using Motion Adaptive Regularization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial resolution of depth sensors is often significantly lower compared to
that of conventional optical cameras. Recent work has explored the idea of
improving the resolution of depth using higher resolution intensity as a side
information. In this paper, we demonstrate that further incorporating temporal
information in videos can significantly improve the results. In particular, we
propose a novel approach that improves depth resolution, exploiting the
space-time redundancy in the depth and intensity using motion-adaptive low-rank
regularization. Experiments confirm that the proposed approach substantially
improves the quality of the estimated high-resolution depth. Our approach can
be a first component in systems using vision techniques that rely on high
resolution depth information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01634</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01634</id><created>2016-03-04</created><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Xia</keyname><forenames>Pengfei</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>Low Complexity Hybrid Precoding and Channel Estimation Based on
  Hierarchical Multi-Beam Search for Millimeter-Wave MIMO Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures. This paper reports a Low Complexity Hybrid
  Precoding and Channel Estimation method for mmWave communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In millimeter-wave (mmWave) MIMO systems, while a hybrid digital/analog
precoding structure offers the potential to increase the achievable rate, it
also faces the challenge of the need of a low-complexity design. In specific,
the hybrid precoding may require matrix operations with a scale of antenna
size, which is generally large in mmWave communication. Moreover, the channel
estimation is also rather time consuming due to the large number of antennas at
both Tx/Rx sides. In this paper, a low-complexity hybrid precoding and channel
estimation approach is proposed. In the channel estimation phase, a
hierarchical multi-beam search scheme is proposed to fast acquire $N_{\rm{S}}$
(the number of streams) multipath components (MPCs)/clusters with the highest
powers. In the hybrid precoding phase, the analog and digital precodings are
decoupled. The analog precoding is designed to steer along the $N_{\rm{S}}$
acquired MPCs/clusters at both Tx/Rx sides, shaping an equivalent
$N_{\rm{S}}\times N_{\rm{S}}$ baseband channel, while the digital precoding
performs operations in the baseband with the reduced-scale channel. Performance
evaluations show that, compared with a state-of-the-art scheme, while achieving
a close or even better performance when the number of radio-frequency (RF)
chains or streams is small, both the computational complexity of the hybrid
precoding and the time complexity of the channel estimation are greatly
reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01635</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01635</id><created>2016-03-04</created><authors><author><keyname>Amy</keyname><forenames>Matthew</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Svore</keyname><forenames>Krysta</forenames></author></authors><title>Verified compilation of space-efficient reversible circuits</title><categories>quant-ph cs.ET</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of reversible circuits from high-level code is in important
problem in several application domains, including low-power electronics and
quantum computing. Existing tools compile and optimize reversible circuits for
various metrics, such as the overall circuit size or the total amount of space
required to implement a given function reversibly. However, little effort has
been spent on verifying the correctness of the results, an issue of particular
importance in quantum computing. There, compilation allows not only mapping to
hardware, but also the estimation of resources required to implement a given
quantum algorithm. This resource determination is crucial for identifying which
algorithms will outperform their classical counterparts. We present a
reversible circuit compiler called ReVer, which has been formally verified in
F* and compiles circuits that operate correctly with respect to the input
program. Our compiler compiles the Revs language to combinational reversible
circuits with as few ancillary bits as possible, and provably cleans temporary
values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01648</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01648</id><created>2016-03-04</created><authors><author><keyname>Stanovsky</keyname><forenames>Gabriel</forenames></author><author><keyname>Ficler</keyname><forenames>Jessica</forenames></author><author><keyname>Dagan</keyname><forenames>Ido</forenames></author><author><keyname>Goldberg</keyname><forenames>Yoav</forenames></author></authors><title>Getting More Out Of Syntax with PropS</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic NLP applications often rely on dependency trees to recognize major
elements of the proposition structure of sentences. Yet, while much semantic
structure is indeed expressed by syntax, many phenomena are not easily read out
of dependency trees, often leading to further ad-hoc heuristic post-processing
or to information loss. To directly address the needs of semantic applications,
we present PropS -- an output representation designed to explicitly and
uniformly express much of the proposition structure which is implied from
syntax, and an associated tool for extracting it from dependency trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01650</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01650</id><created>2016-03-04</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Learning Topology of the Power Distribution Grid with and without
  Missing Data</title><categories>math.OC cs.SY</categories><comments>8 pages, 7 figures, a version of this paper will appear in ECC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution grids refer to the part of the power grid that delivers
electricity from substations to the loads. Structurally a distribution grid is
operated in one of several radial/tree-like topologies that are derived from an
original loopy grid graph by opening switches on some lines. Due to limited
presence of real-time switch monitoring devices, the operating structure needs
to be estimated indirectly. This paper presents a new learning algorithm that
uses only nodal voltage measurements to determine the operational radial
structure. The algorithm is based on the key result stating that the correct
operating structure is the optimal solution of the minimum-weight spanning tree
problem over the original loopy graph where weights on all permissible
edges/lines (open or closed) is the variance of nodal voltage difference at the
edge ends. Compared to existing work, this spanning tree based approach has
significantly lower complexity as it does not require information on line
parameters. Further, a modified learning algorithm is developed for cases when
the input voltage measurements are limited to only a subset of the total grid
nodes. Performance of the algorithms (with and without missing data) is
demonstrated by experiments on test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01651</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01651</id><created>2016-03-04</created><authors><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>Degrees of Freedom of the MIMO 2x2 Interference Network with General
  Message Sets</title><categories>cs.IT math.IT</categories><comments>submitted to T-IT on Mar 4th, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the DoF region for the MIMO 2x2 interference network with a
general message set, consisting of nine messages, one for each pair of a subset
of transmitters at which that message is known and a subset of receivers where
that message is desired. An outer bound on the general nine-message network is
obtained and then it is shown to be tight, establishing the DoF region for the
most general antenna setting wherein all four nodes have an arbitrary number of
antennas each. The DoF-optimal scheme is applicable to the MIMO 2x2 network
with constant channel coefficients, and hence, a fortiori, to time/frequency
varying channel scenarios. In particular, a linear precoding scheme is proposed
that can achieve all the DoF tuples in the DoF region. In it, the precise roles
played by transmit zero-forcing, interference alignment, random beamforming,
symbol extensions and asymmetric complex signaling are delineated. For
instance, we identify a class of antenna settings in which ACS is required to
achieve the fractional-valued corner points. Evidently, the DoF regions of all
previously unknown cases of the 2x2 interference network with a subset of the
nine-messages are established as special cases of the general result of this
paper. In particular, the DoF region of the well-known four-message (and even
three-message) MIMO X channel is established. This problem had remained open
despite previous studies which had found inner and outer bounds that were not
tight in general. Hence, the DoF regions of all special cases obtained from the
general DoF region of the nine-message 2x2 interference network of this work
that include at least three of the four X channel messages are new, among many
others. Our work sheds light on how the same physical 2x2 network could be used
by a suitable choice of message sets to take most advantage of the channel
resource in a flexible and efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01659</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01659</id><created>2016-03-04</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Wu</keyname><forenames>Jingxian</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Zhang</keyname><forenames>Ping</forenames></author></authors><title>Queue-Aware Energy-Efficient Joint Remote Radio Head Activation and
  Beamforming in Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications, 14 pages, 8
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the stochastic optimization of cloud radio access
networks (C-RANs) by joint remote radio head (RRH) activation and beamforming
in the downlink. Unlike most previous works that only consider a static
optimization framework with full traffic buffers, we formulate a dynamic
optimization problem by explicitly considering the effects of random traffic
arrivals and time-varying channel fading. The stochastic formulation can
quantify the tradeoff between power consumption and queuing delay. Leveraging
on the Lyapunov optimization technique, the stochastic optimization problem can
be transformed into a per-slot penalized weighted sum rate maximization
problem, which is shown to be non-deterministic polynomial-time hard. Based on
the equivalence between the penalized weighted sum rate maximization problem
and the penalized weighted minimum mean square error (WMMSE) problem, the group
sparse beamforming optimization based WMMSE algorithm and the relaxed integer
programming based WMMSE algorithm are proposed to efficiently obtain the joint
RRH activation and beamforming policy. Both algorithms can converge to a
stationary solution with low-complexity and can be implemented in a parallel
manner, thus they are highly scalable to large-scale C-RANs. In addition, these
two proposed algorithms provide a flexible and efficient means to adjust the
power-delay tradeoff on demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01661</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01661</id><created>2016-03-04</created><authors><author><keyname>Saini</keyname><forenames>Vaibhav</forenames></author><author><keyname>Sajnani</keyname><forenames>Hitesh</forenames></author><author><keyname>Kim</keyname><forenames>Jaewoo</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina</forenames></author></authors><title>SourcererCC and SourcererCC-I: Tools to Detect Clones in Batch mode and
  During Software Development</title><categories>cs.SE</categories><comments>ICSE 2016 pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the availability of large source-code repositories, there has been a
large number of applications for large-scale clone detection. Unfortunately,
despite a decade of active research, there is a marked lack in clone detectors
that scale to big software systems or large repositories, specifically for
detecting near-miss (Type 3) clones where significant editing activities may
take place in the cloned code. This paper demonstrates: (i) SourcererCC, a
token-based clone detector that targets the first three clone types, and
exploits an index to achieve scalability to large inter-project repositories
using a standard workstation. It uses an optimized inverted-index to quickly
query the potential clones of a given code block. Filtering heuristics based on
token ordering are used to significantly reduce the size of the index, the
number of code-block comparisons needed to detect the clones, as well as the
number of required token-comparisons needed to judge a potential clone, and
(ii) SourcererCC-I, an Eclipse plug-in, that uses SourcererCC's core engine to
identify and navigate clones (both inter and intra project) in real-time during
software development. In our experiments, comparing SourcererCC with the
state-of-the-art tools, we found that it is the only clone detection tool to
successfully scale to 250 MLOC on a standard workstation with 12 GB RAM and
efficiently detect the first three types of clones (precision 86% and recall
86-100%). Link to the demo: https://youtu.be/l7F_9Qp-ks4
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01670</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01670</id><created>2016-03-04</created><updated>2016-03-08</updated><authors><author><keyname>Wei</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Changhu</forenames></author><author><keyname>Rui</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>Chang Wen</forenames></author></authors><title>Network Morphism</title><categories>cs.LG cs.CV cs.NE</categories><comments>Under review for ICML 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a systematic study on how to morph a well-trained
neural network to a new one so that its network function can be completely
preserved. We define this as \emph{network morphism} in this research. After
morphing a parent network, the child network is expected to inherit the
knowledge from its parent network and also has the potential to continue
growing into a more powerful one with much shortened training time. The first
requirement for this network morphism is its ability to handle diverse morphing
types of networks, including changes of depth, width, kernel size, and even
subnet. To meet this requirement, we first introduce the network morphism
equations, and then develop novel morphing algorithms for all these morphing
types for both classic and convolutional neural networks. The second
requirement for this network morphism is its ability to deal with non-linearity
in a network. We propose a family of parametric-activation functions to
facilitate the morphing of any continuous non-linear activation neurons.
Experimental results on benchmark datasets and typical neural networks
demonstrate the effectiveness of the proposed network morphism scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01675</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01675</id><created>2016-03-04</created><authors><author><keyname>Chatterjee</keyname><forenames>Avhishek</forenames></author><author><keyname>Seo</keyname><forenames>Daewon</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Capacity of Systems with Queue-Length Dependent Service Quality</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the information-theoretic limit of reliable information processing
by a server with queue-length dependent quality of service. We define the
capacity for such a system as the number of bits reliably processed per unit
time, and characterize it in terms of queuing system parameters. We also
characterize the distributions of the arrival and service processes that
maximize and minimize the capacity of such systems in a discrete-time setting.
For arrival processes with at most one arrival per time slot, we observed a
minimum around the memoryless distribution. We also studied the case of
multiple arrivals per time slot, and observed that burstiness in arrival has
adverse effects on the system. The problem is theoretically motivated by an
effort to incorporate the notion of reliability in queueing systems, and is
applicable in the contexts of crowdsourcing, multimedia communication, and
stream computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01682</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01682</id><created>2016-03-05</created><authors><author><keyname>Bera</keyname><forenames>Debajyoti</forenames></author><author><keyname>Pratap</keyname><forenames>Rameshwar</forenames></author></authors><title>Frequent-Itemset Mining using Locality-Sensitive Hashing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Apriori algorithm is a classical algorithm for the frequent itemset
mining problem. A significant bottleneck in Apriori is the number of I/O
operation involved, and the number of candidates it generates. We investigate
the role of LSH techniques to overcome these problems, without adding much
computational overhead. We propose randomized variations of Apriori that are
based on asymmetric LSH defined over Hamming distance and Jaccard similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01684</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01684</id><created>2016-03-05</created><authors><author><keyname>Zhang</keyname><forenames>Hanling</forenames></author><author><keyname>Xia</keyname><forenames>Chenxing</forenames></author></authors><title>Saliency Detection combining Multi-layer Integration algorithm with
  background prior and energy function</title><categories>cs.CV</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an improved mechanism for saliency detection.
Firstly,based on a neoteric background prior selecting four corners of an image
as background,we use color and spatial contrast with each superpixel to obtain
a salinecy map(CBP). Inspired by reverse-measurement methods to improve the
accuracy of measurement in Engineering,we employ the Objectness labels as
foreground prior based on part of information of CBP to construct a
map(OFP).Further,an original energy function is applied to optimize both of
them respectively and a single-layer saliency map(SLP)is formed by merging the
above twos.Finally,to deal with the scale problem,we obtain our multi-layer
map(MLP) by presenting an integration algorithm to take advantage of multiple
saliency maps. Quantitative and qualitative experiments on three datasets
demonstrate that our method performs favorably against the state-of-the-art
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01694</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01694</id><created>2016-03-05</created><authors><author><keyname>Mustafa</keyname><forenames>Hafiz Attaul</forenames></author><author><keyname>Shakir</keyname><forenames>Muhammad Zeeshan</forenames></author><author><keyname>Ekti</keyname><forenames>Ali Riza</forenames></author><author><keyname>Imran</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Tafazolli</keyname><forenames>Rahim</forenames></author></authors><title>Intracell Interference Characterization and Cluster Inference for D2D
  Communication</title><categories>cs.IT math.IT</categories><comments>11 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The homogeneous poisson point process (PPP) is widely used to model temporal,
spatial or both topologies of base stations (BSs) and mobile terminals (MTs).
However, negative spatial correlation in BSs, due to strategical deployments,
and positive spatial correlations in MTs, due to homophilic relations, cannot
be captured by homogeneous spatial PPP (SPPP). In this paper, we assume doubly
stochastic poisson process, a generalization of homogeneous PPP, with intensity
measure as another stochastic process. To this end, we assume Permanental Cox
Process (PCP) to capture positive spatial correlation in MTs. We consider
product density to derive closed-form approximation (CFA) of spatial summary
statistics. We propose Euler Characteristic (EC) based novel approach to
approximate intractable random intensity measure and subsequently derive
nearest neighbor distribution function. We further propose the threshold and
spatial extent of excursion set of chi-square random field as interference
control parameters to select different cluster sizes for device-to-device (D2D)
communication. The spatial extent of clusters is controlled by nearest neighbor
distribution function which is incorporated into Laplace functional of SPPP to
analyze the effect of D2D interfering clusters on average coverage probability
of cellular user. The CFA and empirical results are in good agreement and its
comparison with SPPP clearly shows spatial correlation between D2D nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01695</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01695</id><created>2016-03-05</created><authors><author><keyname>Chuang</keyname><forenames>Meng-Che</forenames></author><author><keyname>Hwang</keyname><forenames>Jenq-Neng</forenames></author><author><keyname>Ye</keyname><forenames>Jian-Hui</forenames></author><author><keyname>Huang</keyname><forenames>Shih-Chia</forenames></author><author><keyname>Williams</keyname><forenames>Kresimir</forenames></author></authors><title>Underwater Fish Tracking for Moving Cameras based on Deformable Multiple
  Kernels</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fishery surveys that call for the use of single or multiple underwater
cameras have been an emerging technology as a non-extractive mean to estimate
the abundance of fish stocks. Tracking live fish in an open aquatic environment
posts challenges that are different from general pedestrian or vehicle tracking
in surveillance applications. In many rough habitats fish are monitored by
cameras installed on moving platforms, where tracking is even more challenging
due to inapplicability of background models. In this paper, a novel tracking
algorithm based on the deformable multiple kernels (DMK) is proposed to address
these challenges. Inspired by the deformable part model (DPM) technique, a set
of kernels is defined to represent the holistic object and several parts that
are arranged in a deformable configuration. Color histogram, texture histogram
and the histogram of oriented gradients (HOG) are extracted and serve as object
features. Kernel motion is efficiently estimated by the mean-shift algorithm on
color and texture features to realize tracking. Furthermore, the HOG-feature
deformation costs are adopted as soft constraints on kernel positions to
maintain the part configuration. Experimental results on practical video set
from underwater moving cameras show the reliable performance of the proposed
method with much less computational cost comparing with state-of-the-art
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01696</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01696</id><created>2016-03-05</created><authors><author><keyname>Chuang</keyname><forenames>Meng-Che</forenames></author><author><keyname>Hwang</keyname><forenames>Jenq-Neng</forenames></author><author><keyname>Williams</keyname><forenames>Kresimir</forenames></author></authors><title>A Feature Learning and Object Recognition Framework for Underwater Fish
  Images</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Live fish recognition is one of the most crucial elements of fisheries survey
applications where vast amount of data are rapidly acquired. Different from
general scenarios, challenges to underwater image recognition are posted by
poor image quality, uncontrolled objects and environment, as well as difficulty
in acquiring representative samples. Also, most existing feature extraction
techniques are hindered from automation due to involving human supervision.
Toward this end, we propose an underwater fish recognition framework that
consists of a fully unsupervised feature learning technique and an
error-resilient classifier. Object parts are initialized based on saliency and
relaxation labeling to match object parts correctly. A non-rigid part model is
then learned based on fitness, separation and discrimination criteria. For the
classifier, an unsupervised clustering approach generates a binary class
hierarchy, where each node is a classifier. To exploit information from
ambiguous images, the notion of partial classification is introduced to assign
coarse labels by optimizing the &quot;benefit&quot; of indecision made by the classifier.
Experiments show that the proposed framework achieves high accuracy on both
public and self-collected underwater fish images with high uncertainty and
class imbalance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01698</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01698</id><created>2016-03-05</created><authors><author><keyname>Mustafa</keyname><forenames>Hafiz Attaul</forenames></author><author><keyname>Shakir</keyname><forenames>Muhammad Zeeshan</forenames></author><author><keyname>Imran</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Imran</keyname><forenames>Ali</forenames></author><author><keyname>Tafazolli</keyname><forenames>Rahim</forenames></author></authors><title>Coverage gain and Device-to-Device user Density: Stochastic Geometry
  Modeling and Analysis</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures</comments><journal-ref>IEEE Comml, Volume:19, Issue:10, pp. 1742-1745, 2015</journal-ref><doi>10.1109/LCOMM.2015.2459677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication has huge potential for capacity and
coverage enhancements for next generation cellular networks. The number of
potential nodes for D2D communication is an important parameter that directly
impacts the system capacity. In this paper, we derive analytic expression for
average coverage probability of cellular user and corresponding number of
potential D2D users. In this context, mature framework of stochastic geometry
and Poisson point process has been used. The retention probability has been
incorporated in Laplace functional to capture reduced path-loss and shortest
distance criterion based D2D pairing. The numerical results show a close match
between analytic expression and simulation setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01699</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01699</id><created>2016-03-05</created><authors><author><keyname>Lyu</keyname><forenames>Min</forenames></author><author><keyname>Su</keyname><forenames>Dong</forenames></author><author><keyname>Li</keyname><forenames>Ninghui</forenames></author></authors><title>Understanding the Sparse Vector Technique for Differential Privacy</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sparse Vector Technique (SVT) is a fundamental technique for satisfying
differential privacy and has the unique quality that one can output some query
answers without apparently paying any privacy cost. SVT has been used in both
the interactive setting, where one tries to answer a sequence of queries that
are not known ahead of the time, and in the non-interactive setting, where all
queries are known. Because of the potential savings on privacy budget, many
variants for SVT have been proposed and employed in privacy-preserving data
mining and publishing. However, most variants of SVT are actually not private.
In this paper, we analyze these errors and identify the misunderstandings that
likely contribute to them. We also propose a new version of SVT that provides
better utility, and introduce an effective technique to improve the performance
of SVT. These enhancements can be applied to improve utility in the interactive
setting. In the non-interactive setting (but not the interactive setting),
usage of SVT can be replaced by the Exponential Mechanism (EM); we have
conducted analytical and experimental comparisons to demonstrate that EM
outperforms SVT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01708</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01708</id><created>2016-03-05</created><authors><author><keyname>Huang</keyname><forenames>Huawei</forenames></author><author><keyname>Guo</keyname><forenames>Song</forenames></author><author><keyname>Liang</keyname><forenames>Weifa</forenames></author><author><keyname>Li</keyname><forenames>Keqiu</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author></authors><title>Technique Report: Near-Optimal Routing Protection for SDN Networks Using
  Distributed Markov Approximation</title><categories>cs.DC cs.NI</categories><comments>The technique report for &quot;Near-Optimal Routing Protection for SDN
  Networks Using Distributed Markov Approximation&quot;. 12 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) brings numbers of advantages along with
many challenges. One particular concern is on the control-plane resilience,
while the existing protection approaches proposed for SDN networks mainly focus
on data-plane. In order to achieve the carrier-grade recovery from link
failures, we adopt the dedicated protection scheme towards finding optimal
protection routing for control-plane traffic. To this end, we study a weighted
cost minimization problem, in which the traffic load balancing and flow table
rule placement are jointly considered when selecting protection paths for
controller-switch sessions. Because this problem is known as NP-hard, we
propose a Markov approximation based combinatorial optimization approach for
routing protection in SDN control-plane, which produces near-optimal solution
in a distributed fashion. We then extend our solution to an on-line case that
can handle the single-link failure one at a time. The induced performance
fluctuation is also analyzed with theoretical derivation. Extensive
experimental results show that our proposed algorithm has fast convergence and
high efficiency in resource utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01716</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01716</id><created>2016-03-05</created><authors><author><keyname>Antal</keyname><forenames>B&#xe1;lint</forenames></author></authors><title>Classifier ensemble creation via false labelling</title><categories>cs.LG</categories><acm-class>I.2, I.5.2</acm-class><journal-ref>Knowledge-based Systems 89: pp. 278-287. (2015)</journal-ref><doi>10.1016/j.knosys.2015.07.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach to classifier ensemble creation is presented.
While other ensemble creation techniques are based on careful selection of
existing classifiers or preprocessing of the data, the presented approach
automatically creates an optimal labelling for a number of classifiers, which
are then assigned to the original data instances and fed to classifiers. The
approach has been evaluated on high-dimensional biomedical datasets. The
results show that the approach outperformed individual approaches in all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01722</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01722</id><created>2016-03-05</created><authors><author><keyname>Pareti</keyname><forenames>Paolo</forenames></author><author><keyname>Klein</keyname><forenames>Ewan</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>A Linked Data Scalability Challenge: Concept Reuse Leads to Semantic
  Decay</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The increasing amount of available Linked Data resources is laying the
foundations for more advanced Semantic Web applications. One of their main
limitations, however, remains the general low level of data quality. In this
paper we focus on a measure of quality which is negatively affected by the
increase of the available resources. We propose a measure of semantic richness
of Linked Data concepts and we demonstrate our hypothesis that the more a
concept is reused, the less semantically rich it becomes. This is a significant
scalability issue, as one of the core aspects of Linked Data is the propagation
of semantic information on the Web by reusing common terms. We prove our
hypothesis with respect to our measure of semantic richness and we validate our
model empirically. Finally, we suggest possible future directions to address
this scalability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01729</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01729</id><created>2016-03-05</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Low-Rank Matrix Completion for Topological Interference Management by
  Riemannian Pursuit</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a flexible low-rank matrix completion (LRMC)
approach for topological interference management (TIM) in the partially
connected K-user interference channel. No channel state information (CSI) is
required at the transmitters except the network topology information. The
previous attempt on the TIM problem is mainly based on its equivalence to the
index coding problem, but so far only a few index coding problems have been
solved. In contrast, in this paper, we present an algorithmic approach to
investigate the achievable degrees-of-freedom (DoFs) by recasting the TIM
problem as an LRMC problem. Unfortunately, the resulting LRMC problem is known
to be NP-hard, and the main contribution of this paper is to propose a
Riemannian pursuit (RP) framework to detect the rank of the matrix to be
recovered by iteratively increasing the rank. This algorithm solves a sequence
of fixed-rank matrix completion problems. To address the convergence issues in
the existing fixed-rank optimization methods, the quotient manifold geometry of
the search space of fixed-rank matrices is exploited via Riemannian
optimization. By further exploiting the structure of the low-rank matrix
varieties, i.e., the closure of the set of fixed- rank matrices, we develop an
efficient rank increasing strategy to find good initial points in the procedure
of rank pursuit. Simulation results demonstrate that the proposed RP algorithm
achieves a faster convergence rate and higher achievable DoFs for the TIM
problem compared with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01733</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01733</id><created>2016-03-05</created><authors><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>New Algorithms for Heavy Hitters in Data Streams</title><categories>cs.DS</categories><comments>A preliminary version of this paper will appear as an invited paper
  in ICDT, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An old and fundamental problem in databases and data streams is that of
finding the heavy hitters, also known as the top-$k$, most popular items,
frequent items, elephants, or iceberg queries. There are several variants of
this problem, which quantify what it means for an item to be frequent,
including what are known as the $\ell_1$-heavy hitters and $\ell_2$-heavy
hitters. There are a number of algorithmic solutions for these problems,
starting with the work of Misra and Gries, as well as the CountMin and
CountSketch data structures, among others.
  In this survey paper, accompanying an ICDT invited talk, we cover several
recent results developed in this area, which improve upon the classical
solutions to these problems. In particular, with coauthors we develop new
algorithms for finding $\ell_1$-heavy hitters and $\ell_2$-heavy hitters, with
significantly less memory required than what was known, and which are optimal
in a number of parameter regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01739</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01739</id><created>2016-03-05</created><authors><author><keyname>Sudarshan</keyname><forenames>Viswanath P</forenames></author><author><keyname>Weiser</keyname><forenames>Tobias</forenames></author><author><keyname>Chintala</keyname><forenames>Phalgun</forenames></author><author><keyname>Mandal</keyname><forenames>Subhamoy</forenames></author><author><keyname>Dutta</keyname><forenames>Rahul</forenames></author></authors><title>Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for
  in Vitro Embryo Culture</title><categories>cs.CV physics.med-ph</categories><comments>IEEE BHI 2016</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Visual observation of Cumulus Oocyte Complexes provides only limited
information about its functional competence, whereas the molecular evaluations
methods are cumbersome or costly. Image analysis of mammalian oocytes can
provide attractive alternative to address this challenge. However, it is
complex, given the huge number of oocytes under inspection and the subjective
nature of the features inspected for identification. Supervised machine
learning methods like random forest with annotations from expert biologists can
make the analysis task standardized and reduces inter-subject variability. We
present a semi-automatic framework for predicting the class an oocyte belongs
to, based on multi-object parametric segmentation on the acquired microscopic
image followed by a feature based classification using random forests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01740</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01740</id><created>2016-03-05</created><authors><author><keyname>Fleszar</keyname><forenames>Krzysztof</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author></authors><title>New Algorithms for Maximum Disjoint Paths Based on Tree-Likeness</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classical $\mathsf{NP}$-hard problems of finding maximum-size
subsets from given sets of~$k$ terminal pairs that can be routed via
edge-disjoint paths (MaxEDP) or node-disjoint paths (MaxNDP) in a given graph.
The approximability of MaxEDP/NDP is currently not well understood; the best
known lower bound is $\Omega(\log^{1/2 - \varepsilon}{n})$, assuming
$\mathsf{P}\not=\mathsf{NP}$. This constitutes a significant gap to the best
known approximation upper bound of $\cal O(\sqrt{n})$, due to Chekuri et al.
(2006). In their seminal paper, Raghavan and Thompson (Combinatorica, 1987)
introduce the technique of randomized rounding for LPs; their technique gives
an $\cal O(1)$-approximation when edges (or nodes) may be used by $\cal
O\left(\frac{\log n}{\log\log n}\right)$ paths.
  In this paper, we provide the first non-trivial refinements of these
fundamental results by achieving bounds that are independent of the input size.
Our bounds are formulated in terms of the \emph{feedback vertex set number} $r$
of a graph which measures its vertex deletion distance to a forest. In
particular, we obtain the following.
  * For MaxEDP, we give an $\cal{O}(\sqrt{r}\cdot
\log^{1.5}{kr})$-approximation algorithm. As $r\leq n$, up to logarithmic
factors, our result can be considered a strengthening of the best known
approximation factor~$\cal O(\sqrt{n})$ for MaxEDP, due to Chekuri et al.
  * Further, we show how to route $\Omega(OPT)$ pairs with congestion $\cal
O\left(\frac{\log{kr}}{\log\log{kr}}\right)$, strengthening the bound obtained
by the classic approach of Raghavan and Thompson.
  * For MaxNDP, we give an algorithm that gives the optimal answer in
time~$(k+r)^{\cal O(r)}\cdot n$. If $r$ is at most triple exponential in $k$,
this improves the best known algorithm for MaxNDP with parameter $k$, by
Kawarabayashi and Wollan (STOC 2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01751</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01751</id><created>2016-03-05</created><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Stability analysis and stabilization of stochastic linear impulsive,
  switched and sampled-data systems</title><categories>math.OC cs.SY math.CA math.DS</categories><comments>10 pages, 2 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impulsive systems are a very flexible class of systems that can be used to
represent switched and sampled-data systems. We propose to extend the
previously obtained results on deterministic impulsive systems to the
stochastic setting. The concepts of mean-square stability and dwell-times are
utilized in order to formulate relevant stability conditions for such systems.
These conditions are formulated as convex clock-dependent linear matrix
inequality conditions that are applicable to robust analysis and control
design, and verifiable using discretization or sum of squares techniques.
Stability conditions under various dwell-time conditions are obtained and
non-conservatively turned into state-feedback stabilization conditions. The
results are finally applied to the analysis and control of stochastic
sampled-data systems. Several comparative examples demonstrate the accuracy and
the tractability of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01758</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01758</id><created>2016-03-05</created><authors><author><keyname>Bendkowski</keyname><forenames>Maciej</forenames></author></authors><title>Normal-order reduction grammars</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which, for given $n$, generates an unambiguous
regular tree grammar defining the set of combinatory logic terms, over the set
$\{S,K\}$ of primitive combinators, requiring exactly $n$ normal-order
reduction steps to normalize. As a consequence of the famous Curry and Feys's
standardization theorem, our reduction grammars form a complete syntactic
characterization of normalizing combinatory logic terms. We investigate the
size of generated grammars, giving a primitive recursive upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01765</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01765</id><created>2016-03-05</created><authors><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Tulloch</keyname><forenames>Andrew</forenames></author><author><keyname>Tygert</keyname><forenames>Mark</forenames></author></authors><title>Accurate principal component analysis via a few iterations of
  alternating least squares</title><categories>math.NA cs.NA stat.CO</categories><comments>9 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A few iterations of alternating least squares with a random starting point
provably suffice to produce nearly optimal spectral- and Frobenius-norm
accuracies of low-rank approximations to a matrix; iterating to convergence is
unnecessary. Thus, software implementing alternating least squares can be
retrofitted via appropriate setting of parameters to calculate nearly optimally
accurate low-rank approximations highly efficiently, with no need for
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01766</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01766</id><created>2016-03-05</created><authors><author><keyname>Goldblatt</keyname><forenames>Robert</forenames></author><author><keyname>Hodkinson</keyname><forenames>Ian</forenames></author></authors><title>Spatial logic of modal mu-calculus and tangled closure operators</title><categories>math.LO cs.LO</categories><msc-class>03B45, 54E35</msc-class><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been renewed interest in recent years in McKinsey and Tarski's
interpretation of modal logic in topological spaces and their proof that S4 is
the logic of any separable dense-in-itself metric space. Here we extend this
work to the modal mu-calculus and to a logic of tangled closure operators that
was developed by Fern\'andez-Duque after these two languages had been shown by
Dawar and Otto to have the same expressive power over finite transitive Kripke
models. We prove that this equivalence remains true over topological spaces.
  We establish the finite model property in Kripke semantics for various
tangled closure logics with and without the universal modality $\forall$. We
also extend the McKinsey--Tarski topological `dissection lemma'. These results
are used to construct a representation map (also called a d-p-morphism) from
any dense-in-itself metric space $X$ onto any finite connected locally
connected serial transitive Kripke frame.
  This yields completeness theorems over $X$ for a number of languages: (i) the
modal mu-calculus with the closure operator $\Diamond$; (ii) $\Diamond$ and the
tangled closure operators $\langle t \rangle$; (iii) $\Diamond,\forall$; (iv)
$\Diamond,\forall,\langle t \rangle$; (v) the derivative operator $\langle d
\rangle$; (vi) $\langle d \rangle$ and the associated tangled closure operators
$\langle dt \rangle$; (vii) $\langle d \rangle,\forall$; (viii) $\langle d
\rangle,\forall,\langle dt \rangle$. Soundness also holds, if: (a) for
languages with $\forall$, $X$ is connected; and (b) for languages with $\langle
d \rangle$, $X$ validates the well known axiom $\mathrm{G}_1$. For countable
languages without $\forall$, we prove strong completeness. We also show that in
the presence of $\forall$, strong completeness fails if $X$ is compact and
locally connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01768</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01768</id><created>2016-03-05</created><authors><author><keyname>Champandard</keyname><forenames>Alex J.</forenames></author></authors><title>Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Convolutional neural networks (CNNs) have proven highly effective at image
synthesis and style transfer. For most users, however, using them as tools can
be a challenging task due to their unpredictable behavior that goes against
common intuitions. This paper introduces a novel concept to augment such
generative architectures with semantic annotations, either by manually
authoring pixel labels or using existing solutions for semantic segmentation.
The result is a content-aware generative algorithm that offers meaningful
control over the outcome. Thus, we increase the quality of images generated by
avoiding common glitches, make the results look significantly more plausible,
and extend the functional range of these algorithms---whether for portraits or
landscapes, etc. Applications include semantic style transfer and turning
doodles with few colors into masterful paintings!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01770</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01770</id><created>2016-03-05</created><authors><author><keyname>Kaliakatsos-Papakostas</keyname><forenames>Maximos</forenames></author><author><keyname>Confalonieri</keyname><forenames>Roberto</forenames></author><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author><author><keyname>Zacharakis</keyname><forenames>Asterios</forenames></author><author><keyname>Cambouropoulos</keyname><forenames>Emilios</forenames></author></authors><title>An Argument-based Creative Assistant for Harmonic Blending</title><categories>cs.SD cs.AI</categories><comments>8 pp; submitted to 7th International Conference on Computational
  Creativity</comments><acm-class>H.5.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conceptual blending is a powerful tool for computational creativity where,
for example, the properties of two harmonic spaces may be combined in a
consistent manner to produce a novel harmonic space. However, deciding about
the importance of property features in the input spaces and evaluating the
results of conceptual blending is a nontrivial task. In the specific case of
musical harmony, defining the salient features of chord transitions and
evaluating invented harmonic spaces requires deep musicological background
knowledge. In this paper, we propose a creative tool that helps musicologists
to evaluate and to enhance harmonic innovation. This tool allows a music expert
to specify arguments over given transition properties. These arguments are then
considered by the system when defining combinations of features in an
idiom-blending process. A music expert can assess whether the new harmonic
idiom makes musicological sense and re-adjust the arguments (selection of
features) to explore alternative blends that can potentially produce better
harmonic spaces. We conclude with a discussion of future work that would
further automate the harmonisation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01772</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01772</id><created>2016-03-05</created><authors><author><keyname>Dourbal</keyname><forenames>Pavel</forenames></author><author><keyname>Pekker</keyname><forenames>Mikhail</forenames></author></authors><title>Fast calculation of correlations in recognition systems</title><categories>cs.CV</categories><comments>7 pages</comments><msc-class>62H30, 65F05, 65F10, 65F30, 65F50, 68T05, 68T10, 94A11, 94A12,
  94A13, 94A15</msc-class><acm-class>F.2.1; G.1.0; G.1.3; G.4; H.4.2; I.1.2; I.2.2; I.5.2; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computationally efficient classification system architecture is proposed. It
utilizes fast tensor-vector multiplication algorithm to apply linear operators
upon input signals . The approach is applicable to wide variety of recognition
system architectures ranging from single stage matched filter bank classifiers
to complex neural networks with unlimited number of hidden layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01774</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01774</id><created>2016-03-05</created><authors><author><keyname>Ghavimi</keyname><forenames>Behnam</forenames><affiliation>GESIS Leibniz Institute for the Social Sciences</affiliation><affiliation>University of Bonn</affiliation></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames><affiliation>GESIS Leibniz Institute for the Social Sciences</affiliation></author><author><keyname>Vahdati</keyname><forenames>Sahar</forenames><affiliation>University of Bonn</affiliation></author><author><keyname>Lange</keyname><forenames>Christoph</forenames><affiliation>University of Bonn</affiliation><affiliation>Fraunhofer IAIS</affiliation></author></authors><title>Identifying and Improving Dataset References in Social Sciences Full
  Texts</title><categories>cs.DL</categories><doi>10.5281/zenodo.44608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific full text papers are usually stored in separate places than their
underlying research datasets. Authors typically make references to datasets by
mentioning them for example by using their titles and the year of publication.
However, in most cases explicit links that would provide readers with direct
access to referenced datasets are missing. Manually detecting references to
datasets in papers is time consuming and requires an expert in the domain of
the paper. In order to make explicit all links to datasets in papers that have
been published already, we suggest and evaluate a semi-automatic approach for
finding references to datasets in social sciences papers. Our approach does not
need a corpus of papers (no cold start problem) and it performs well on a small
test corpus (gold standard). Our approach achieved an F-measure of 0.84 for
identifying references in full texts and an F-measure of 0.83 for finding
correct matches of detected references in the da|ra dataset registry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01776</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01776</id><created>2016-03-05</created><authors><author><keyname>Hayes</keyname><forenames>Ian J.</forenames></author></authors><title>Generalised rely-guarantee concurrency: An algebraic foundation</title><categories>cs.LO</categories><comments>23 pages, 3 figures, submitted to Formal Aspects of Computing</comments><acm-class>D.1.3; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rely-guarantee technique allows one to reason compositionally about
concurrent programs. To handle interference the technique makes use of rely and
guarantee conditions, both of which are binary relations on states. A rely
condition is an assumption that the environment performs only atomic steps
satisfying the rely relation and a guarantee is a commitment that every atomic
step the program makes satisfies the guarantee relation. In order to
investigate rely-guarantee reasoning more generally, in this paper we allow
interference to be represented by a process rather than a relation and hence
derive more general rely-guarantee laws. The paper makes use of a weak
conjunction operator between processes, which generalises a guarantee relation
to a guarantee process, and introduces a rely quotient operator, which
generalises a rely relation to a process. The paper focuses on the algebraic
properties of the general rely-guarantee theory. The Jones-style rely-guarantee
theory can be interpreted as a model of the general algebraic theory and hence
the general laws presented here hold for that theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01786</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01786</id><created>2016-03-05</created><authors><author><keyname>Khonji</keyname><forenames>Majid</forenames></author><author><keyname>Karapetyan</keyname><forenames>Areg</forenames></author><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Chau</keyname><forenames>Chi-Kin</forenames></author></authors><title>Complex-demand Scheduling Problem with Application in Smart Grid</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of scheduling complex-valued demands over a
discretized time horizon. Given a set of users, each user is associated with a
set of demands representing different user's preferences. A demand is
represented by a complex number, a time interval, and a utility value obtained
if the demand is satisfied. At each time slot, the magnitude of the total
selected demands should not exceed a given capacity. This naturally captures
the supply constraints in alternating current (AC) electric systems. In this
paper, we consider maximizing the aggregate user utility subject to power
supply limits over a time horizon. We present approximation algorithms
characterized by the maximum angle $\phi$ between any two complex-valued
demands. More precisely, a PTAS is presented for the case $\phi \in
[0,\tfrac{\pi}{2}]$, a bi-criteria FPTAS for $\phi \in [0,{\pi} \mbox{-}
\varepsilon]$ for any polynomially small $\varepsilon$, assuming the number of
time slots in the discretized time horizon is a constant. Furthermore, if the
number of time slots is polynomial, we present a reduction to the real-valued
unsplittable flow on a path problem with only a constant approximation ratio.
Finally, we present a practical greedy algorithm for the single time slot case
with an approximation ratio of $\tfrac{1}{2}\cos \frac{\phi}{2}$, while the
running time is ${O}(n\log n)$, which can be implemented efficiently in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01787</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01787</id><created>2016-03-05</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Skarin</keyname><forenames>Bruce</forenames></author></authors><title>Approaches to Modeling Insurgency</title><categories>cs.MA cs.CY cs.SI</categories><comments>A version of this paper appeared as a book chapter in Kott, A., &amp;
  Citrenbaum, G. (Eds.). Estimating Impact: A Handbook of Computational Methods
  and Models for Anticipating Economic, Social, Political and Security Effects
  in International Interventions. Springer, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper begins with an introduction to qualitative theories and models of
insurgency, quantitative measures of insurgency, influence diagrams, system
dynamics models of insurgency, agent based molding of insurgency,
human-in-the-loop wargaming of insurgency, and statistical models of
insurgency. The paper then presents a detailed case study of an agent-based
model that focuses on the Troubles in Northern Ireland starting in 1968. The
model is agent-based and uses a modeling tool called Simulation of Cultural
Identities for Prediction of Reactions (SCIPR). The objective in this modeling
effort was to predict trends in the degree of population's support to parties
in this conflict. The case studies describes in detail the agents, their
actions, model initialization and simulation process, and the results of the
simulation compared to actual historical results of elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01793</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01793</id><created>2016-03-06</created><authors><author><keyname>Poblet-Puig</keyname><forenames>J.</forenames></author><author><keyname>Shanin</keyname><forenames>A. V.</forenames></author></authors><title>Coupling of finite element method with boundary algebraic equations</title><categories>cs.NA cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a combined approach of CFIE--BAE has been proposed by authors for
solving external scattering problems in acoustics. CFIE stands for
combined-field integral equations, and BAE is the method of boundary
algebraical equation. The combined method is, essentially, a discrete analogue
of the boundary element method (BEM), having none of its disadvantages. Namely,
due to the discrete nature of BAE one should not compute quadratures of
oversingular integrals. Moreover, due to CFIE formulation, the method does not
possess spurious resonances.
  However, the CFIE--BAE method has an important drawback. Since the modelling
is performed in a regular discrete space, the shape of the obstacle should be
assembled of elementary &quot;bricks&quot;, so smooth scatterers (like spheres,
cylinders, etc) are approximated with a poor accuracy. This loss of accuracy
becomes the bottleneck of the method. Here this disadvantage is overcome. The
CFIE--BAE method developed for regular meshing of the outer space is coupled in
a standard way with a relatively small irregular mesh enabling one to describe
the shape of the obstacle accurately enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01799</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01799</id><created>2016-03-06</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author></authors><title>Noise Stability and Correlation with Half Spaces</title><categories>math.PR cs.CC</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benjamini, Kalai and Schramm showed that a monotone function $f : \{-1,1\}^n
\to \{-1,1\}$ is noise stable if and only if it is correlated with a half-space
(a set of the form $\{x: \langle x, a\rangle \le b\}$).
  We study noise stability in terms of correlation with half-spaces for general
(not necessarily monotone) functions. We show that a function $f: \{-1, 1\}^n
\to \{-1, 1\}$ is noise stable if and only if it becomes correlated with a
half-space when we modify $f$ by randomly restricting a constant fraction of
its coordinates.
  Looking at random restrictions is necessary: we construct noise stable
functions whose correlation with any half-space is $o(1)$. The examples further
satisfy that different restrictions are correlated with different half-spaces:
for any fixed half-space, the probability that a random restriction is
correlated with it goes to zero.
  We also provide quantitative versions of the above statements, and versions
that apply for the Gaussian measure on $\mathbb{R}^n$ instead of the discrete
cube. Our work is motivated by questions in learning theory and a recent
question of Khot and Moshkovitz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01801</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01801</id><created>2016-03-06</created><authors><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>Variational methods for Conditional Multimodal Learning: Generating
  Human Faces from Attributes</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior to this decade, the field of computer vision was primarily focused
around hand-crafted feature extraction methods used in conjunction with
discriminative models for specific tasks such as object recognition,
detection/localization, tracking etc. A generative image understanding was
neither within reach nor the prime concern of the period. In this paper, we
address the following problem: Given a description of a human face, can we
generate the image corresponding to it? We frame this problem as a conditional
modality learning problem and use variational methods for maximizing the
corresponding conditional log-likelihood. The resultant deep model, which we
refer to as conditional multimodal autoencoder (CMMA), forces the latent
representation obtained from the attributes alone to be 'close' to the joint
representation obtained from both face and attributes. We show that the faces
generated from attributes using the proposed model, are qualitatively and
quantitatively more representative of the attributes from which they were
generated, than those obtained by other deep generative models. We also propose
a secondary task, whereby the existing faces are modified by modifying the
corresponding attributes. We observe that the modifications in face introduced
by the proposed model are representative of the corresponding modifications in
attributes. Hence, our proposed method solves the above mentioned problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01807</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01807</id><created>2016-03-06</created><authors><author><keyname>AlBdaiwi</keyname><forenames>Bader F.</forenames></author></authors><title>On the Number of Cycles in a Graph</title><categories>cs.DM</categories><comments>Accepted for publication in Mathematica Slovaca</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a sizable literature on investigating the minimum and maximum
numbers of cycles in a class of graphs. However, the answer is known only for
special classes. This paper presents a result on the smallest number of cycles
in hamiltonian 3-connected cubic graphs. Further, it describes a proof
technique that could improve an upper bound of the largest number of cycles in
a hamiltonian graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01817</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01817</id><created>2016-03-06</created><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author><author><keyname>Dia</keyname><forenames>Mohamad</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Proof of Threshold Saturation for Spatially Coupled Sparse Superposition
  Codes</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>Submitted to the International Symposium on Information Theory (ISIT)
  2016, Barcelona, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new class of codes, called sparse superposition or sparse
regression codes, has been proposed for communication over the AWGN channel. It
has been proven that they achieve capacity using power allocation and various
forms of iterative decoding. Empirical evidence has also strongly suggested
that the codes achieve capacity when spatial coupling and approximate message
passing decoding are used, without need of power allocation. In this note we
prove that state evolution (which tracks message passing) indeed saturates the
potential threshold of the underlying code ensemble, which approaches in a
proper limit the optimal threshold. Our proof uses ideas developed in the
theory of low-density parity-check codes and compressive sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01819</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01819</id><created>2016-03-06</created><authors><author><keyname>Mosayebi</keyname><forenames>Reza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author></authors><title>Type Based Sign Modulation for Molecular Communication</title><categories>cs.IT cs.ET math.IT</categories><comments>Submitted to IWCIT2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the foremost challenges in modulation scheme for molecular
communication is positivity of the transmission signal (only a positive
concentration of molecules can be released in the environment). This
restriction makes handling of the InterSymbol Interference (ISI) a challenge
for molecular communications. In this paper, a novel modulation is proposed
which introduces use of negative signals to ameliorate the transmission link
performance. A precoder scheme based on the diffusion channel model is proposed
and shown to have a significant improvement compared to previous modulation
schemes such as CSK and MCSK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01820</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01820</id><created>2016-03-06</created><authors><author><keyname>Kimelfeld</keyname><forenames>Benny</forenames></author><author><keyname>Livshits</keyname><forenames>Ester</forenames></author><author><keyname>Peterfreund</keyname><forenames>Liat</forenames></author></authors><title>Unambiguous Prioritized Repairing of Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In its traditional definition, a repair of an inconsistent database is a
consistent database that differs from the inconsistent one in a &quot;minimal way&quot;.
Often, repairs are not equally legitimate, as it is desired to prefer one over
another; for example, one fact is regarded more reliable than another, or a
more recent fact should be preferred to an earlier one. Motivated by these
considerations, researchers have introduced and investigated the framework of
preferred repairs, in the context of denial constraints and subset repairs.
There, a priority relation between facts is lifted towards a priority relation
between consistent databases, and repairs are restricted to the ones that are
optimal in the lifted sense. Three notions of lifting (and optimal repairs)
have been proposed: Pareto, global, and completion.
  In this paper we investigate the complexity of deciding whether the priority
relation suffices to clean the database unambiguously, or in other words,
whether there is exactly one optimal repair. We show that the different lifting
semantics entail highly different complexities. Under Pareto optimality, the
problem is coNP-complete, in data complexity, for every set of functional
dependencies (FDs), except for the tractable case of (equivalence to) one FD
per relation. Under global optimality, one FD per relation is still tractable,
but we establish $\Pi^{p}_{2}$-completeness for a relation with two FDs. In
contrast, under completion optimality the problem is solvable in polynomial
time for every set of FDs. In fact, we present a polynomial-time algorithm for
arbitrary conflict hypergraphs. We further show that under a general assumption
of transitivity, this algorithm solves the problem even for global optimality.
The algorithm is extremely simple, but its proof of correctness is quite
intricate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01824</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01824</id><created>2016-03-06</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Smith</keyname><forenames>Daniel V.</forenames></author><author><keyname>Montgomery</keyname><forenames>Christopher</forenames></author><author><keyname>Terriberry</keyname><forenames>Timothy B.</forenames></author></authors><title>Low-Complexity Iterative Sinusoidal Parameter Estimation</title><categories>cs.SD</categories><comments>8 pages. arXiv admin note: substantial text overlap with
  arXiv:1602.05900</comments><journal-ref>Proceedings of International Conference on Signal Processing and
  Communication Systems (ICSPCS), pp. 276-283, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sinusoidal parameter estimation is a computationally-intensive task, which
can pose problems for real-time implementations. In this paper, we propose a
low-complexity iterative method for estimating sinusoidal parameters that is
based on the linearisation of the model around an initial frequency estimate.
We show that for N sinusoids in a frame of length L, the proposed method has a
complexity of O(LN), which is significantly less than the matching pursuits
method. Furthermore, the proposed method is shown to be more accurate than the
matching pursuits and time frequency reassignment methods in our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01833</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01833</id><created>2016-03-06</created><authors><author><keyname>Lancioni</keyname><forenames>Giuliano</forenames></author><author><keyname>Pettinari</keyname><forenames>Valeria</forenames></author><author><keyname>Garofalo</keyname><forenames>Laura</forenames></author><author><keyname>Campanelli</keyname><forenames>Marta</forenames></author><author><keyname>Pepe</keyname><forenames>Ivana</forenames></author><author><keyname>Olivieri</keyname><forenames>Simona</forenames></author><author><keyname>Cicola</keyname><forenames>Ilaria</forenames></author></authors><title>Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive
  Disambiguation: the eXtended Revised AraMorph (XRAM)</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extended, revised form of Tim Buckwalter's Arabic lexical and
morphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM),
is presented which addresses a number of weaknesses and inconsistencies of the
original model by allowing a wider coverage of real-world Classical and
contemporary (both formal and informal) Arabic texts. Building upon previous
research, XRAM enhancements include (i) flag-selectable usage markers, (ii)
probabilistic mildly context-sensitive POS tagging, filtering, disambiguation
and ranking of alternative morphological analyses, (iii) semi-automatic
increment of lexical coverage through extraction of lexical and morphological
information from existing lexical resources. Testing of XRAM through a
front-end Python module showed a remarkable success level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01840</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01840</id><created>2016-03-06</created><authors><author><keyname>Dalal</keyname><forenames>Gal</forenames></author><author><keyname>Gilboa</keyname><forenames>Elad</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Hierarchical Decision Making In Electricity Grid Management</title><categories>cs.AI cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power grid is a complex and vital system that necessitates careful
reliability management. Managing the grid is a difficult problem with multiple
time scales of decision making and stochastic behavior due to renewable energy
generations, variable demand and unplanned outages. Solving this problem in the
face of uncertainty requires a new methodology with tractable algorithms. In
this work, we introduce a new model for hierarchical decision making in complex
systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a
level of abstraction, for real-time power grid reliability. We devise an
algorithm that alternates between slow time-scale policy improvement, and fast
time-scale value function approximation. We compare our results to prevailing
heuristics, and show the strength of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01842</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01842</id><created>2016-03-06</created><authors><author><keyname>A-iyeh</keyname><forenames>Enoch</forenames></author><author><keyname>Peters</keyname><forenames>James F.</forenames></author></authors><title>Proximal groupoid patterns In digital images</title><categories>cs.CV</categories><comments>9 pages, 6 figures</comments><msc-class>54E40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this article is on the detection and classification of patterns
based on groupoids. The approach hinges on descriptive proximity of points in a
set based on the neighborliness property. This approach lends support to image
analysis and understanding and in studying nearness of image segments. A
practical application of the approach is in terms of the analysis of natural
images for pattern identification and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01845</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01845</id><created>2016-03-06</created><authors><author><keyname>Naoum-Sawaya</keyname><forenames>Joe</forenames></author><author><keyname>Crisostomi</keyname><forenames>Emanuele</forenames></author><author><keyname>Liu</keyname><forenames>Mingming</forenames></author><author><keyname>Gu</keyname><forenames>Yingqi</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author></authors><title>Smart Procurement of Naturally Generated Energy (SPONGE) for Plug-in
  Hybrid Electric Buses</title><categories>cs.SY</categories><comments>This paper is recently submitted to the IEEE Transactions on
  Automation Science and Engineering</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We discuss a recently introduced ECO-driving concept known as SPONGE in the
context of Plug-in Hybrid Electric Buses (PHEB)'s.Examples are given to
illustrate the benefits of this approach to ECO-driving. Finally, distributed
algorithms to realise SPONGE are discussed, paying attention to the privacy
implications of the underlying optimisation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01855</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01855</id><created>2016-03-06</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Online Learning to Rank with Feedback at the Top</title><categories>cs.LG</categories><comments>Appearing in AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an online learning to rank setting in which, at each round, an
oblivious adversary generates a list of $m$ documents, pertaining to a query,
and the learner produces scores to rank the documents. The adversary then
generates a relevance vector and the learner updates its ranker according to
the feedback received. We consider the setting where the feedback is restricted
to be the relevance levels of only the top $k$ documents in the ranked list for
$k \ll m$. However, the performance of learner is judged based on the
unrevealed full relevance vectors, using an appropriate learning to rank loss
function. We develop efficient algorithms for well known losses in the
pointwise, pairwise and listwise families. We also prove that no online
algorithm can have sublinear regret, with top-1 feedback, for any loss that is
calibrated with respect to NDCG. We apply our algorithms on benchmark datasets
demonstrating efficient online learning of a ranking function from highly
restricted feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01860</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01860</id><created>2016-03-06</created><authors><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author></authors><title>Generalization error bounds for learning to rank: Does the length of
  document lists matter?</title><categories>cs.LG</categories><comments>Appeared in ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the generalization ability of algorithms for learning to rank at
a query level, a problem also called subset ranking. Existing generalization
error bounds necessarily degrade as the size of the document list associated
with a query increases. We show that such a degradation is not intrinsic to the
problem. For several loss functions, including the cross-entropy loss used in
the well known ListNet method, there is \emph{no} degradation in generalization
ability as document lists become longer. We also provide novel generalization
error bounds under $\ell_1$ regularization and faster convergence rates if the
loss function is smooth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01863</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01863</id><created>2016-03-06</created><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Montgomery</keyname><forenames>Christopher</forenames></author></authors><title>Improved Noise Weighting in CELP Coding of Speech - Applying the Vorbis
  Psychoacoustic Model To Speex</title><categories>cs.SD</categories><comments>9 pages, Proceedings of the 120th Audio Engineering Society (AES)
  Convention, Paris, 2006</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One key aspect of the CELP algorithm is that it shapes the coding noise using
a simple, yet effective, weighting filter. In this paper, we improve the noise
shaping of CELP using a more modern psychoacoustic model. This has the
significant advantage of improving the quality of an existing codec without the
need to change the bit-stream. More specifically, we improve the Speex CELP
codec by using the psychoacoustic model used in the Vorbis audio codec. The
results show a significant increase in quality, especially at high bit-rates,
where the improvement is equivalent to a 20% reduction in bit-rate. The
technique itself is not specific to Speex and could be applied to other CELP
codecs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01864</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01864</id><created>2016-03-06</created><authors><author><keyname>Codevilla</keyname><forenames>Felipe</forenames></author><author><keyname>Gaya</keyname><forenames>Joel D. O.</forenames></author><author><keyname>Duarte</keyname><forenames>Amanda C.</forenames></author><author><keyname>Botelho</keyname><forenames>Silvia</forenames></author></authors><title>General Participative Media Single Image Restoration</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method to restore degraded images captured in general
participative media --- fog, turbid water, sand storm, etc. To obtain
generality, we, first, propose a novel interpretation of the participative
media image formation by considering the color variation of the media. Second,
we introduce that joining different image priors is an effective alternative
for image restoration. The proposed method contains a Composite Prior supported
by statistics collected on both haze-free and degraded participative
environment images. The key of the method is joining two complementary measures
--- local contrast and color. The results presented for a variety of underwater
and haze images demonstrate the power of the method. Moreover, we showed the
potential of our method using a special dataset for which a reference haze-free
image is available for comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01866</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01866</id><created>2016-03-06</created><authors><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author><author><keyname>Nguyen</keyname><forenames>Son H.</forenames></author><author><keyname>Kim</keyname><forenames>Hyun-Gyu</forenames></author><author><keyname>Hackl</keyname><forenames>Klaus</forenames></author></authors><title>An efficient adaptive polygonal finite element method for plastic
  collapse analysis of solids</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an adaptive polygonal finite element formulation for collapse
plastic analysis of solids. The article contributes into four crucial points:
1) Wachspress shape functions at vertex and bubble nodes handled at a
primal-mesh level; 2) plastic strain rates and dissipation performed over a
dual-mesh level; 3) a new adaptive primal-mesh strategy driven by the L^2
-norm-based indicator of strain rates; and 4) a spatial decomposition structure
obtained from a so-called polytree mesh scheme. We investigate both purely
cohesive and cohesive-frictional materials. We prove numerically that the
present method performs well for volumetric locking problem. In addition, the
optimization formulation of limit analysis is written by the form of
second-order cone programming (SOCP) in order to exploit the high efficiency of
interior-point solvers. The present method retains a low number of optimization
variables. This convenient approach allows us to design and solve the
large-scale optimization problems effectively. Numerical validations show the
excellent performance of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01869</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01869</id><created>2016-03-06</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Physical Layer Security for Massive MIMO Systems Impaired by Phase Noise</title><categories>cs.IT math.IT</categories><comments>Invited paper, submitted to IEEE SPAWC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the impact of phase noise on the secrecy
performance of downlink massive MIMO systems in the presence of a passive
multiple-antenna eavesdropper. Thereby, for the base station (BS) and the
legitimate users, the effect of multiplicative phase noise is taken into
account, whereas the eavesdropper is assumed to employ ideal hardware. We
derive a lower bound for the ergodic secrecy rate of a given user when matched
filter data precoding and artificial noise transmission are employed at the BS.
Based on the derived analytical expression, we investigate the impact of the
various system parameters on the secrecy rate. Our analytical and simulation
results reveal that distributively deployed local oscillators (LOs) can achieve
a better performance than one common LO for all BS antennas as long as a
sufficient amount of power is assigned for data transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01870</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01870</id><created>2016-03-06</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sougata</forenames></author><author><keyname>Theocharous</keyname><forenames>Georgios</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author></authors><title>Personalized Advertisement Recommendation: A Ranking Approach to Address
  the Ubiquitous Click Sparsity Problem</title><categories>cs.LG cs.IR</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of personalized advertisement recommendation (PAR),
which consist of a user visiting a system (website) and the system displaying
one of $K$ ads to the user. The system uses an internal ad recommendation
policy to map the user's profile (context) to one of the ads. The user either
clicks or ignores the ad and correspondingly, the system updates its
recommendation policy. PAR problem is usually tackled by scalable
\emph{contextual bandit} algorithms, where the policies are generally based on
classifiers. A practical problem in PAR is extreme click sparsity, due to very
few users actually clicking on ads. We systematically study the drawback of
using contextual bandit algorithms based on classifier-based policies, in face
of extreme click sparsity. We then suggest an alternate policy, based on
rankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,
which can significantly alleviate the problem of click sparsity. We conduct
extensive experiments on public datasets, as well as three industry proprietary
datasets, to illustrate the improvement in click-through-rate (CTR) obtained by
using the ranker-based policy over classifier-based policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01876</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01876</id><created>2016-03-06</created><authors><author><keyname>Dreher</keyname><forenames>Patrick</forenames></author><author><keyname>Byun</keyname><forenames>Chansup</forenames></author><author><keyname>Hill</keyname><forenames>Chris</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Kuszmaul</keyname><forenames>Bradley</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark
  for Big-Data Platforms</title><categories>cs.PF astro-ph.IM cs.DC physics.data-an</categories><comments>9 pages, 7 figures, to appear in IPDPS 2016 Graph Algorithms Building
  Blocks (GABB) workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of big data systems has created a need for benchmarks to measure and
compare the capabilities of these systems. Big data benchmarks present unique
scalability challenges. The supercomputing community has wrestled with these
challenges for decades and developed methodologies for creating rigorous
scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline
benchmark employs supercomputing benchmarking methodologies to create a
scalable benchmark that is reflective of many real-world big data processing
systems. The PageRank pipeline benchmark builds on existing prior scalable
benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with
multiple integrated kernels that can be run together or independently. Each
kernel is well defined mathematically and can be implemented in any programming
environment. The linear algebraic nature of PageRank makes it well suited to
being implemented using the GraphBLAS standard. The computations are simple
enough that performance predictions can be made based on simple computing
hardware models. The surrounding kernels provide the context for each kernel
that allows rigorous definition of both the input and the output for each
kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable
in both problem size and hardware, it can be used to measure and quantitatively
compare a wide range of present day and future systems. Serial implementations
in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been
implemented and their single threaded performance has been measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01882</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01882</id><created>2016-03-06</created><authors><author><keyname>Zinkov</keyname><forenames>Robert</forenames></author><author><keyname>Shan</keyname><forenames>Chung-chieh</forenames></author></authors><title>Composing inference algorithms as program transformations</title><categories>stat.ML cs.AI stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic inference procedures are usually coded painstakingly from
scratch, for each target model and each inference algorithm. We reduce this
coding effort by generating inference procedures from models automatically. We
make this code generation modular by decomposing inference algorithms into
reusable program transformations. These source-to-source transformations
perform exact inference as well as generate probabilistic programs that compute
expectations, densities, and MCMC samples. The resulting inference procedures
run in time comparable to that of handwritten procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01887</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01887</id><created>2016-03-06</created><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Rothblum</keyname><forenames>Guy N.</forenames></author></authors><title>Concentrated Differential Priacy</title><categories>cs.DS cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Concentrated Differential Privacy, a relaxation of Differential
Privacy enjoying better accuracy than both pure differential privacy and its
popular &quot;(epsilon,delta)&quot; relaxation without compromising on cumulative privacy
loss over multiple computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01895</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01895</id><created>2016-03-06</created><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Giakkoupis</keyname><forenames>George</forenames></author><author><keyname>Kermarrec</keyname><forenames>Anne-Marie</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author></authors><title>Bounds on the Voter Model in Dynamic Networks</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the voter model, each node of a graph has an opinion, and in every round
each node chooses independently a random neighbour and adopts its opinion. We
are interested in the consensus time, which is the first point in time where
all nodes have the same opinion. We consider dynamic graphs in which the edges
are rewired in every round (by an adversary) giving rise to the graph sequence
$G_1, G_2, \dots $, where we assume that $G_i$ has conductance at least
$\phi_i$. We assume that the degrees of nodes don't change over time as one can
show that the consensus time can become super-exponential otherwise. In the
case of a sequence of $d$-regular graphs, we obtain asymptotically tight
results. Even for some static graphs, such as the cycle, our results improve
the state of the art. Here we show that the expected number of rounds until all
nodes have the same opinion is bounded by $O(m/(d_{min} \cdot \phi))$, for any
graph with $m$ edges, conductance $\phi$, and degrees at least $d_{min}$. In
addition, we consider a biased dynamic voter model, where each opinion $i$ is
associated with a probability $P_i$, and when a node chooses a neighbour with
that opinion, it adopts opinion $i$ with probability $P_i$ (otherwise the node
keeps its current opinion). We show for any regular dynamic graph, that if
there is an $\epsilon&gt;0$ difference between the highest and second highest
opinion probabilities, and at least $\Omega(\log n)$ nodes have initially the
opinion with the highest probability, then all nodes adopt w.h.p. that opinion.
We obtain a bound on the convergences time, which becomes $O(\log n/\phi)$ for
static graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01901</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01901</id><created>2016-03-06</created><authors><author><keyname>Behmardi</keyname><forenames>Behrouz</forenames></author><author><keyname>Briggs</keyname><forenames>Forrest</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author><author><keyname>Raich</keyname><forenames>Raviv</forenames></author></authors><title>Confidence-Constrained Maximum Entropy Framework for Learning from
  Multi-Instance Data</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-instance data, in which each object (bag) contains a collection of
instances, are widespread in machine learning, computer vision, bioinformatics,
signal processing, and social sciences. We present a maximum entropy (ME)
framework for learning from multi-instance data. In this approach each bag is
represented as a distribution using the principle of ME. We introduce the
concept of confidence-constrained ME (CME) to simultaneously learn the
structure of distribution space and infer each distribution. The shared
structure underlying each density is used to learn from instances inside each
bag. The proposed CME is free of tuning parameters. We devise a fast
optimization algorithm capable of handling large scale multi-instance data. In
the experimental section, we evaluate the performance of the proposed approach
in terms of exact rank recovery in the space of distributions and compare it
with the regularized ME approach. Moreover, we compare the performance of CME
with Multi-Instance Learning (MIL) state-of-the-art algorithms and show a
comparable performance in terms of accuracy with reduced computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01911</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01911</id><created>2016-03-06</created><authors><author><keyname>Baffier</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Chiu</keyname><forenames>Man-Kwun</forenames></author><author><keyname>Diez</keyname><forenames>Yago</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Mitsou</keyname><forenames>Valia</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Roeloffzen</keyname><forenames>Marcel</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>Hanabi is NP-complete, Even for Cheaters who Look at Their Cards</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a cooperative card game called Hanabi from an algorithmic
combinatorial game theory viewpoint. The aim of the game is to play cards from
$1$ to $n$ in increasing order (this has to be done independently in $c$
different colors). Cards are drawn from a deck one by one. Drawn cards are
either immediately played, discarded or stored for future use (overall each
player can store up to $h$ cards).
  We introduce a simplified mathematical model of a single-player version of
the game, and show several complexity results: the game is intractable in a
general setting, but becomes tractable (and even linear) in some interesting
restricted cases (i.e., for small values of $h$ and $c$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01913</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01913</id><created>2016-03-06</created><authors><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Haffari</keyname><forenames>Gholamreza</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>A Latent Variable Recurrent Neural Network for Discourse Relation
  Language Models</title><categories>cs.CL</categories><comments>Accepted to NAACL 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel latent variable recurrent neural network
architecture for jointly modeling sequences of words and (possibly latent)
discourse relations that link adjacent sentences. A recurrent neural network
generates individual words, thus reaping the benefits of
discriminatively-trained vector representations. The discourse relations are
represented with a latent variable, which can be predicted or marginalized,
depending on the task. The resulting model outperforms state-of-the-art
alternatives for implicit discourse relation classification in the Penn
Discourse Treebank, and for dialog act classification in the Switchboard
corpus. By marginalizing over latent discourse relations, it also yields a
language model that improves on a strong recurrent neural network baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01921</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01921</id><created>2016-03-06</created><authors><author><keyname>Afshang</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author></authors><title>Optimal Geographic Caching in Finite Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cache-enabled device-to-device (D2D) networks turn memory of the devices at
the network edge, such as smart phones and tablets, into bandwidth by enabling
asynchronous content sharing directly between proximate devices. Limited
storage capacity of the mobile devices necessitates the determination of
optimal set of contents to be cached on each device. In order to study the
problem of optimal cache placement, we model the locations of devices in a
finite region (e.g., coffee shop, sports bar, library) as a uniform binomial
point process (BPP). For this setup, we first develop a generic framework to
analyze the coverage probability of the target receiver (target-Rx) when the
requested content is available at the $k^{th}$ closest device to it. Using this
coverage probability result, we evaluate optimal caching probability of the
popular content to maximize the total hit probability. Our analysis concretely
demonstrates that optimal caching probability strongly depends on the number of
simultaneously active devices in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01925</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01925</id><created>2016-03-06</created><authors><author><keyname>Guo</keyname><forenames>Longkun</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author></authors><title>${\cal NP}$-Completeness of the Negative $k$-Cycle Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a positive integer $k$ and a directed graph with real edge cost, the
negative $k$-cycle problem ($k$NCP) is to determine whether there exists a
negative cycle with at least $k$ edges. The $k$NCP problem generalizes two well
known problems: The negative cycle detection problem of determining whether
there exist negative cycles in a given graph, and the $k$-cycle problem of
determining whether there exist a cycle with at least $k$ edges.
  This paper proves the ${\cal NP}$-completeness of $k$NCP for any fixed
$k\geq3$ by giving a reduction from the 3 occurrence 3-Satisfiability
(\emph{3O3SAT}) problem. Compared to the fact that the negative cycle detection
problem is polynomial solvable and the $k$-cycle problem is fixed-parameter
tractable with respect to $k$, the hardness result for $k$NCP seems
interesting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01926</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01926</id><created>2016-03-06</created><authors><author><keyname>Kokshoorn</keyname><forenames>Matthew</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Millimetre Wave MIMO Channel Estimation using Overlapped Beam Patterns
  and Rate Adaptation</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the channel estimation problem in millimetre
wave (mmWave) wireless systems with large antenna arrays. By exploiting the
inherent sparse nature of the mmWave channel, we first propose a fast channel
estimation (FCE) algorithm based on a novel overlapped beam pattern design.
Compared to the existing non-overlapped beam pattern design, the overlap among
multiple pilot beam patterns can increase the amount of information carried by
each channel measurement and thus reduce the required channel estimation time.
We develop a maximum likelihood (ML) estimator to optimally extract the path
information from the channel measurements. Then, we propose a novel
rate-adaptive channel estimation (RACE) algorithm, which can dynamically adjust
the number of channel measurements based on the expected probability of
estimation error (PEE). The performance of both proposed algorithms is
analyzed. For the FCE algorithm, an approximate closed-form expression for the
PEE is derived. For the RACE algorithm, a lower bound for the minimum signal
energy-to-noise ratio required for a given number of channel measurements is
developed based on the Shannon-Hartley theorem. Simulation results show that
the FCE algorithm significantly reduces the number of channel estimation
measurements compared to the existing algorithms using non-overlapped beam
patterns. By adopting the RACE algorithm, we can achieve up to a 6dB gain in
signal energy-to-noise ratio for the same PEE compared to the existing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01929</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01929</id><created>2016-03-06</created><authors><author><keyname>Ye</keyname><forenames>Junting</forenames></author><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Temporal Opinion Spam Detection by Multivariate Indicative Signals</title><categories>cs.SI</categories><comments>10 pages, 29 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online consumer reviews reflect the testimonials of real people, unlike
advertisements. As such, they have critical impact on potential consumers, and
indirectly on businesses. According to a Harvard study (Luca 2011), +1 rise in
star-rating increases revenue by 5-9%. Problematically, such financial
incentives have created a market for spammers to fabricate reviews, to unjustly
promote or demote businesses, activities known as opinion spam (Jindal and Liu
2008). A vast majority of existing work on this problem have formulations based
on static review data, with respective techniques operating in an offline
fashion. Spam campaigns, however, are intended to make most impact during their
course. Abnormal events triggered by spammers' activities could be masked in
the load of future events, which static analysis would fail to identify. In
this work, we approach the opinion spam problem with a temporal formulation.
Specifically, we monitor a list of carefully selected indicative signals of
opinion spam over time and design efficient techniques to both detect and
characterize abnormal events in real-time. Experiments on datasets from two
different review sites show that our approach is fast, effective, and practical
to be deployed in real-world systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01931</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01931</id><created>2016-03-06</created><authors><author><keyname>Palmer</keyname><forenames>Andrew W.</forenames></author><author><keyname>Hill</keyname><forenames>Andrew J.</forenames></author><author><keyname>Scheding</keyname><forenames>Steven J.</forenames></author></authors><title>Stochastic Collection and Replenishment (SCAR): Objective Functions</title><categories>cs.RO cs.MA</categories><comments>Presented at IROS 2013</comments><doi>10.1109/IROS.2013.6696829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces two objective functions for computing the expected cost
in the Stochastic Collection and Replenishment (SCAR) scenario. In the SCAR
scenario, multiple user agents have a limited supply of a resource that they
either use or collect, depending on the scenario. To enable persistent
autonomy, dedicated replenishment agents travel to the user agents and
replenish or collect their supply of the resource, thus allowing them to
operate indefinitely in the field. Of the two objective functions, one uses a
Monte Carlo method, while the other uses a significantly faster analytical
method. Approximations to multiplication, division and inversion of Gaussian
distributed variables are used to facilitate propagation of probability
distributions in the analytical method when Gaussian distributed parameters are
used. The analytical objective function is shown to have greater than 99%
comparison accuracy when compared with the Monte Carlo objective function while
achieving speed gains of several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01932</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01932</id><created>2016-03-06</created><authors><author><keyname>Palmer</keyname><forenames>Andrew W.</forenames></author><author><keyname>Hill</keyname><forenames>Andrew J.</forenames></author><author><keyname>Scheding</keyname><forenames>Steven J.</forenames></author></authors><title>Stochastic Collection and Replenishment (SCAR) Optimisation for
  Persistent Autonomy</title><categories>cs.RO cs.MA</categories><comments>Presented at IROS 2014</comments><doi>10.1109/IROS.2014.6942968</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots have a finite supply of resources such as fuel, battery charge, and
storage space. The aim of the Stochastic Collection and Replenishment (SCAR)
scenario is to use dedicated agents to refuel, recharge, or otherwise replenish
robots in the field to facilitate persistent autonomy. This paper explores the
optimisation of the SCAR scenario with a single replenishment agent, using
several different objective functions. The problem is framed as a combinatorial
optimisation problem, and A* is used to find the optimal schedule. Through a
computational study, a ratio objective function is shown to have superior
performance compared with a total weighted tardiness objective function, with a
greater performance advantage present when using shorter schedule lengths. The
importance of incorporating uncertainty in the objective function used in the
optimisation process is also highlighted, in particular for scenarios in which
the replenishment agent is under- or fully-utilised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01942</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01942</id><created>2016-03-07</created><authors><author><keyname>Pan</keyname><forenames>Xiaqing</forenames></author><author><keyname>Chachada</keyname><forenames>Sachin</forenames></author><author><keyname>Kuo</keyname><forenames>C. -C. Jay</forenames></author></authors><title>A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust two-stage shape retrieval (TSR) method is proposed to address the 2D
shape retrieval problem. Most state-of-the-art shape retrieval methods are
based on local features matching and ranking. Their retrieval performance is
not robust since they may retrieve globally dissimilar shapes in high ranks. To
overcome this challenge, we decompose the decision process into two stages. In
the first irrelevant cluster filtering (ICF) stage, we consider both global and
local features and use them to predict the relevance of gallery shapes with
respect to the query. Irrelevant shapes are removed from the candidate shape
set. After that, a local-features-based matching and ranking (LMR) method
follows in the second stage. We apply the proposed TSR system to MPEG-7,
Kimia99 and Tari1000 three datasets and show that it outperforms all other
existing methods. The robust retrieval performance of the TSR system is
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01943</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01943</id><created>2016-03-07</created><authors><author><keyname>Zhao</keyname><forenames>Shancheng</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author></authors><title>Partially Block Markov Superposition Transmission of Gaussian Source
  with Nested Lattice Codes</title><categories>cs.IT math.IT</categories><comments>22 pages, 9 figures, Submitted to IEEE Transaction on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the transmission of Gaussian sources through additive
white Gaussian noise (AWGN) channels in bandwidth expansion regime, i.e., the
channel bandwidth is greater than the source bandwidth. To mitigate the error
propagation phenomenon of conventional digital transmission schemes, we propose
in this paper a new capacity-approaching joint source channel coding (JSCC)
scheme based on partially block Markov superposition transmission (BMST) of
nested lattice codes. In the proposed scheme, first, the Gaussian source
sequence is discretized by a lattice-based quantizer, resulting in a sequence
of lattice points. Second, these lattice points are encoded by a short
systematic group code. Third, the coded sequence is partitioned into blocks of
equal length and then transmitted in the BMST manner. Main characteristics of
the proposed JSCC scheme include: 1) Entropy coding is not used explicitly. 2)
Only parity-check sequence is superimposed, hence, termed partially BMST
(PBMST). This is different from the original BMST. To show the superior
performance of the proposed scheme, we present extensive simulation results
which show that the proposed scheme performs within 1.0 dB of the Shannon
limits. Hence, the proposed scheme provides an attractive candidate for
transmission of Gaussian sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01954</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01954</id><created>2016-03-07</created><authors><author><keyname>Wu</keyname><forenames>Nan</forenames></author><author><keyname>Liu</keyname><forenames>Zheyu</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Guo</keyname><forenames>Xiaojun</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Xie</keyname><forenames>Yuan</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>A Real-Time and Energy-Efficient Implementation of
  Difference-of-Gaussian with Flexible Thin-Film Transistors</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With many advantageous features, softness and better biocompatibility,
flexible electronic devices have developed rapidly and increasingly attracted
attention. Many currently applications with flexible devices are sensors and
drivers, while there is nearly no utilization aiming at complex computation
since flexible devices have lower electron mobility, simple structure and large
process variation. In this paper, we proposed an innovative method that enabled
flexible devices to implement real-time and energy-efficient
Difference-of-Gaussian, which illustrated feasibility and potentials for them
to achieve complicated real-time computation in future generation products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01965</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01965</id><created>2016-03-07</created><authors><author><keyname>Ingibergsson</keyname><forenames>Johann Thor Mogensen</forenames></author><author><keyname>Suvei</keyname><forenames>Stefan-Daniel</forenames></author><author><keyname>Hansen</keyname><forenames>Mikkel Kragh</forenames></author><author><keyname>Christiansen</keyname><forenames>Peter</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author></authors><title>Towards a DSL for Perception-Based Safety Systems</title><categories>cs.RO cs.PL</categories><comments>Presented at DSLRob 2015 (arXiv:1601.00877) This paper is a poster
  submission, an extension to the already accepted article (no abstract):
  1601.00877 Thus the introduction in this paper, is a compilation of the
  earlier article, which is referenced as [3]</comments><report-no>DSLRob/2015/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an extension to an early presented programming language, called
a domain specific language. This paper extends the proposed concept with new
sensors and behaviours to address real-life situations. The functionality was
tested in lab experiments, and an extension to the earlier concepts is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01973</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01973</id><created>2016-03-07</created><authors><author><keyname>An</keyname><forenames>Jisun</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>#greysanatomy vs. #yankees: Demographics and Hashtag Use on Twitter</title><categories>cs.SI</categories><comments>This is a preprint of an article appearing at ICWSM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demographics, in particular, gender, age, and race, are a key predictor of
human behavior. Despite the significant effect that demographics plays, most
scientific studies using online social media do not consider this factor,
mainly due to the lack of such information. In this work, we use
state-of-the-art face analysis software to infer gender, age, and race from
profile images of 350K Twitter users from New York. For the period from
November 1, 2014 to October 31, 2015, we study which hashtags are used by
different demographic groups. Though we find considerable overlap for the most
popular hashtags, there are also many group-specific hashtags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01976</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01976</id><created>2016-03-07</created><authors><author><keyname>Li</keyname><forenames>Guanbin</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Deep Contrast Learning for Salient Object Detection</title><categories>cs.CV</categories><comments>To appear in CVPR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection has recently witnessed substantial progress due to
powerful features extracted using deep convolutional neural networks (CNNs).
However, existing CNN-based methods operate at the patch level instead of the
pixel level. Resulting saliency maps are typically blurry, especially near the
boundary of salient objects. Furthermore, image patches are treated as
independent samples even when they are overlapping, giving rise to significant
redundancy in computation and storage. In this CVPR 2016 paper, we propose an
end-to-end deep contrast network to overcome the aforementioned limitations.
Our deep network consists of two complementary components, a pixel-level fully
convolutional stream and a segment-wise spatial pooling stream. The first
stream directly produces a saliency map with pixel-level accuracy from an input
image. The second stream extracts segment-wise features very efficiently, and
better models saliency discontinuities along object boundaries. Finally, a
fully connected CRF model can be optionally incorporated to improve spatial
coherence and contour localization in the fused result from these two streams.
Experimental results demonstrate that our deep model significantly improves the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01977</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01977</id><created>2016-03-07</created><authors><author><keyname>Chandoo</keyname><forenames>Maurice</forenames></author></authors><title>On the Implicit Graph Conjecture</title><categories>cs.CC cs.DM cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implicit graph conjecture states that every sufficiently small,
hereditary graph class has a labeling scheme with a polynomial-time computable
label decoder. We approach this conjecture by investigating classes of label
decoders defined in terms of complexity classes such as P and EXP. For
instance, GP denotes the class of graph classes that have a labeling scheme
with a polynomial-time computable label decoder. Until now it was not even
known whether GP is a strict subset of GR. We show that this is indeed the case
and reveal a strict hierarchy akin to classical complexity. We also show that
classes such as GP can be characterized in terms of graph parameters. This
could mean that certain algorithmic problems are feasible on every graph class
in GP. Lastly, we define a very restricted class of label decoders using
first-order logic that already contains many natural graph classes such as
forests and interval graphs. We give an alternative characterization of this
class in terms of directed acyclic graphs. By showing that some small,
hereditary graph class cannot be expressed with such label decoders a weaker
form of the implicit graph conjecture could be disproven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01979</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01979</id><created>2016-03-07</created><authors><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>An</keyname><forenames>Jisun</forenames></author></authors><title>Two Tales of the World: Comparison of Widely Used World News Datasets
  GDELT and EventRegistry</title><categories>cs.DL</categories><comments>To be appeared in ICWSM'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we compare GDELT and Event Registry, which monitor news
articles worldwide and provide big data to researchers regarding scale, news
sources, and news geography. We found significant differences in scale and news
sources, but surprisingly, we observed high similarity in news geography
between the two datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01983</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01983</id><created>2016-03-07</created><authors><author><keyname>Cenciarelli</keyname><forenames>Pietro</forenames></author><author><keyname>Gorla</keyname><forenames>Daniele</forenames></author><author><keyname>Salvo</keyname><forenames>Ivano</forenames></author></authors><title>Graph Theoretic Investigations on Inefficiencies in Network Models</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider network models where information items flow %are sent from a
source to a sink node. We start with a model where routing is constrained by
energy available on nodes in finite supply (like in Smartdust) and efficiency
is related to energy consumption. We characterize graph topologies ensuring
that every saturating flow under every energy-to-node assignment is maximum and
provide a polynomial-time algorithm for checking this property. We then
consider the standard flow networks with capacity on edges, where again
efficiency is related to maximality of saturating flows, and a traffic model
for selfish routing, where efficiency is related to latency at a Wardrop
equilibrium. Finally, we show that all these forms of inefficiency yield
different classes of graphs (apart from the acyclic case, where the last two
forms generate the same class). Interestingly, in all cases inefficient graphs
can be made efficient by removing edges; this resembles a well-known
phenomenon, called Braess's paradox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01987</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01987</id><created>2016-03-07</created><authors><author><keyname>Cozza</keyname><forenames>Vittoria</forenames></author><author><keyname>Petrocchi</keyname><forenames>Marinella</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author></authors><title>A matter of words: NLP for quality evaluation of Wikipedia medical
  articles</title><categories>cs.IR cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Automatic quality evaluation of Web information is a task with many fields of
applications and of great relevance, especially in critical domains like the
medical one. We move from the intuition that the quality of content of medical
Web documents is affected by features related with the specific domain. First,
the usage of a specific vocabulary (Domain Informativeness); then, the adoption
of specific codes (like those used in the infoboxes of Wikipedia articles) and
the type of document (e.g., historical and technical ones). In this paper, we
propose to leverage specific domain features to improve the results of the
evaluation of Wikipedia medical articles. In particular, we evaluate the
articles adopting an &quot;actionable&quot; model, whose features are related to the
content of the articles, so that the model can also directly suggest strategies
for improving a given article quality. We rely on Natural Language Processing
(NLP) and dictionaries-based techniques in order to extract the bio-medical
concepts in a text. We prove the effectiveness of our approach by classifying
the medical articles of the Wikipedia Medicine Portal, which have been
previously manually labeled by the Wiki Project team. The results of our
experiments confirm that, by considering domain-oriented features, it is
possible to obtain sensible improvements with respect to existing solutions,
mainly for those articles that other approaches have less correctly classified.
Other than being interesting by their own, the results call for further
research in the area of domain specific features suitable for Web data quality
assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01991</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01991</id><created>2016-03-07</created><updated>2016-03-08</updated><authors><author><keyname>Cha</keyname><forenames>Hyun-Su</forenames></author><author><keyname>Kim</keyname><forenames>Dong Ku</forenames></author></authors><title>PAR-Constrained Multiuser MIMO OFDM based on Convex Optimization and
  Concentration of Measure</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures, submitted to IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-user multiple-input multiple-output (MU-MIMO) orthogonal frequency
division multiplexing systems, a block diagonalization (BD) precoding scheme is
first designed to aim at minimizing the peak-to-average power ratio (PAR) for a
fixed cost, which is a sum of the received SNR loss of users. Then the BD
precoder is designed to provide the required PAR at the minimum cost, which is
our main goal. By the aid of the concentration of measure property, it is
almost exactly able to provide the required PAR on average, and also provide
the required performance specified as 0.1 percent PAR in an asymptotic way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.01999</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.01999</id><created>2016-03-07</created><authors><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author><author><keyname>Karandikar</keyname><forenames>Abhay</forenames></author><author><keyname>Naik</keyname><forenames>Gaurang</forenames></author><author><keyname>Khaturia</keyname><forenames>Meghna</forenames></author><author><keyname>Saha</keyname><forenames>Shubham</forenames></author><author><keyname>Arora</keyname><forenames>Mahak</forenames></author><author><keyname>Singh</keyname><forenames>Jaspreet</forenames></author></authors><title>Towards Enabling Broadband for a Billon Plus Population with TV White
  Spaces</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages, 6 figures, submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major impediments to providing broadband connectivity in
semi-urban and rural India is the lack of robust and affordable backhaul. Fiber
connectivity in terms of backhaul that is being planned (or provided) by the
Government of India would reach only till rural offices (named Gram Panchayat)
in the Indian rural areas. In this exposition, we articulate how TV white space
can address the challenge in providing broadband connectivity to a billion plus
population within India. The villages can form local Wi-Fi clusters. The
problem of connecting the Wi-Fi clusters to the optical fiber points can be
addressed using a TV white space based backhaul (middle-mile) network.
  The amount of TV white space present in India is very large when compared
with the developed world. Therefore, we discuss a backhaul architecture for
rural India, which utilizes TV white spaces. We also showcase results from our
TV white space testbed, which support the effectiveness of backhaul by using TV
white spaces. Our testbed provides a broadband access network to rural
population in thirteen villages. The testbed is deployed over an area of
$25$km$^2$, and extends seamless broadband connectivity from optical fiber
locations or Internet gateways to remote (difficult to connect) rural regions.
We also discuss standards and TV white space regulations, which are pertinent
to the backhaul architecture mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02000</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02000</id><created>2016-03-07</created><authors><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Identifying Randomly Activated Users via Sign-Compute-Resolve on Graphs</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at IEEE ICC'16 conference, Workshop on
  Massive Uncoordinated Access Protocols (MASSAP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we treat the problem of identification of a subset of active
users in a set of a large number of potentially active users. The users from
the subset are activated randomly, such that the access point (AP) does not
know the subset or its size a priori. The active users are contending to report
their activity to the AP over a multiple access channel. We devise a contention
algorithm that assumes a combination of physical-layer network coding and
K-out-of-N signature coding, allowing for multiple detection of up to K users
at the access point. In addition, we rely on the principles of coded slotted
ALOHA (CSA) and use of successive interference cancellation to enable
subsequent resolution of the collisions that originally featured more than K
users. The objective is to identify the subset of active users such that the
target performance, e.g., probability of active user resolution and/or
throughput is reached, which implies that the duration of the contention period
is also not known a priori. In contrast to standard CSA approaches, in the
proposed algorithm each user, active or not, has a predefined schedule of slots
in which it sends its signature. We analyze the performance of the proposed
algorithm both in the asymptotic and non-asymptotic settings. We also derive an
estimator that, based on the observation of collision multiplicities, estimates
how many users are active and thereby enables tuning of the length of the
contention period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02003</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02003</id><created>2016-03-07</created><authors><author><keyname>Upchurch</keyname><forenames>Paul</forenames></author><author><keyname>Snavely</keyname><forenames>Noah</forenames></author><author><keyname>Bala</keyname><forenames>Kavita</forenames></author></authors><title>From A to Z: Supervised Transfer of Style and Content Using Deep Neural
  Network Generators</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new neural network architecture for solving single-image
analogies - the generation of an entire set of stylistically similar images
from just a single input image. Solving this problem requires separating image
style from content. Our network is a modified variational autoencoder (VAE)
that supports supervised training of single-image analogies and in-network
evaluation of outputs with a structured similarity objective that captures
pixel covariances. On the challenging task of generating a 62-letter font from
a single example letter we produce images with 22.4% lower dissimilarity to the
ground truth than state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02008</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02008</id><created>2016-03-07</created><authors><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author></authors><title>Position paper: Towards an observer-oriented theory of shape comparison</title><categories>cs.CG cs.CV math.AT</categories><comments>Preprint of the position paper submitted to the Eurographics Workshop
  on 3D Object Retrieval (2016)</comments><msc-class>55N35 (Primary), 22F99, 47H09, 54H15, 57S10, 68U05, 65D18
  (Secondary)</msc-class><acm-class>I.4.7; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper we suggest a possible metric approach to shape
comparison that is based on a mathematical formalization of the concept of
observer, seen as a collection of suitable operators acting on a metric space
of functions. These functions represent the set of data that are accessible to
the observer, while the operators describe the way the observer elaborates the
data and enclose the invariance that he/she associates with them. We expose
this model and illustrate some theoretical reasons that justify its possible
use for shape comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02010</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02010</id><created>2016-03-07</created><authors><author><keyname>Balle</keyname><forenames>Borja</forenames></author><author><keyname>Gomrokchi</keyname><forenames>Maziar</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Differentially Private Policy Evaluation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first differentially private algorithms for reinforcement
learning, which apply to the task of evaluating a fixed policy. We establish
two approaches for achieving differential privacy, provide a theoretical
analysis of the privacy and utility of the two algorithms, and show promising
results on simple empirical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02011</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02011</id><created>2016-03-07</created><authors><author><keyname>Karthick</keyname><forenames>T.</forenames></author></authors><title>Independent Sets in Classes Related to Chair/Fork-free Graphs</title><categories>cs.DM math.CO</categories><journal-ref>LNCS 9602 (2016) pp. 224-232</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Weight Independent Set (MWIS) problem on graphs with vertex
weights asks for a set of pairwise nonadjacent vertices of maximum total
weight. MWIS is known to be $NP$-complete in general, even under various
restrictions. Let $S_{i,j,k}$ be the graph consisting of three induced paths of
lengths $i, j, k$ with a common initial vertex. The complexity of the MWIS
problem for $S_{1, 2, 2}$-free graphs, and for $S_{1, 1, 3}$-free graphs are
open. In this paper, we show that the MWIS problem can solved in polynomial
time for ($S_{1, 2, 2}$, $S_{1, 1, 3}$, co-chair)-free graphs, by analyzing the
structure of the subclasses of this class of graphs. This extends some known
results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02018</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02018</id><created>2016-03-07</created><authors><author><keyname>Zhang</keyname><forenames>Aixian</forenames></author><author><keyname>Li</keyname><forenames>Jin</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>Linear Codes over Galois Ring $GR(p^2,r)$ Related to Gauss sums</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes over finite rings become one of hot topics in coding theory
after Hommons et al.([4], 1994) discovered that several remarkable nonlinear
binary codes with some linear-like properties are the images of Gray map of
linear codes over $Z_4$. In this paper we consider two series of linear codes
$C(G)$ and $\widetilde{C}(G)$ over Galois ring $R=GR(p^2,r)$, where $G$ is a
subgroup of $R^{(s)^*}$ and $R^{(s)}=GR(p^2,rs)$. We present a general formula
on $N_\beta(a)$ in terms of Gauss sums on $R^{(s)}$ for each $a\in R$, where
$N_\beta(a)$ is the number of a-component of the codeword $c_\beta\in C(G)
(\beta\in R^{(s)})$ (Theorem 3.1). We have determined the complete Hamming
weight distribution of $C(G)$ and the minimum Hamming distance of
$\widetilde{C}(G)$ for some special G (Theorem 3.3 and 3.4). We show a general
formula on homogeneous weight of codewords in $C(G)$ and $\widetilde{C}(G)$
(Theorem 4.5) for the special $G$ given in Theorem 3.4. Finally we obtained
series of nonlinear codes over $\mathbb{F}_{q} \ (q=p^r)$ with two Hamming
distance by using Gray map (Corollary 4.6).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02023</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02023</id><created>2016-03-07</created><authors><author><keyname>Zhang</keyname><forenames>Renyuan</forenames></author><author><keyname>Cai</keyname><forenames>Kai</forenames></author></authors><title>Supervisor Localization of Timed Discrete-Event Systems under Partial
  Observation and Communication Delay</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study supervisor localization for timed discrete-event systems (TDES)
under partial partial observation in the Brandin-Wonham framework of timed
supervisory control. The essence of supervisor localization under partial
observation is the decomposition of partial-observation monolithic supervisor
synthesized by using relative observability into local control strategies for
individual controllable events. This study extends our previous work on
supervisor localization under partial observation for untimed DES, in that
timed relative observability is used to synthesize partial-observation
monolithic supervisor and monolithic timed control action typically includes
not only disabling action as in the untimed case, but also &quot;clock preempting&quot;
action. The latter action is executed by a class of &quot;forcible&quot; events;
accordingly, we localize partial-observation monolithic preemptive action with
respect to these events.
  We move on to study supervisor localization for TDES, considering specified
inneglectable communication delays. We model the transmissions of the
communication events by TDES channel models and treat the (bounded and
unbounded) communication delays as newly imposed specifications. Since the
plant with communication models may have multiple subsets of observable events,
we first adopt relative coobervability to synthesize partial-observation
decentralized supervisors, and then decompose them into local control
strategies by the newly proposed supervisor localization under partial
observation. We further show that the collective local controlled behavior is
equivalent to that of the decentralized supervisors, and thus the given delay
bounds are satisfied.
  We finally demonstrate the above results by a timed workcell example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02028</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02028</id><created>2016-03-07</created><authors><author><keyname>Martin</keyname><forenames>Hugo</forenames></author><author><keyname>Chevallier</keyname><forenames>Sylvain</forenames></author><author><keyname>Monacelli</keyname><forenames>Eric</forenames></author></authors><title>Adaptive Visualisation System for Construction Building Information
  Models Using Saliency</title><categories>cs.CV cs.AI</categories><comments>10 pages, 5 figures, to be submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building Information Modeling (BIM) is a recent construction process based on
a 3D model, containing every component related to the building achievement.
Architects, structure engineers, method engineers, and others participant to
the building process work on this model through the design-to-construction
cycle. The high complexity and the large amount of information included in
these models raise several issues, delaying its wide adoption in the industrial
world. One of the most important is the visualization: professionals have
difficulties to find out the relevant information for their job. Actual
solutions suffer from two limitations: the BIM models information are processed
manually and insignificant information are simply hidden, leading to
inconsistencies in the building model. This paper describes a system relying on
an ontological representation of the building information to label
automatically the building elements. Depending on the user's department, the
visualization is modified according to these labels by automatically adjusting
the colors and image properties based on a saliency model. The proposed
saliency model incorporates several adaptations to fit the specificities of
architectural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02031</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02031</id><created>2016-03-07</created><authors><author><keyname>Roman'kov</keyname><forenames>Vitalii</forenames></author></authors><title>How to make RSA and some other encryptions probabilistic</title><categories>cs.CR math.GR</categories><comments>7 pages</comments><msc-class>20F10, 20D60, 12E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new scheme of probabilistic subgroup-related encryption is introduced. Some
applications of this scheme based on the RSA, Diffie-Hellman and ElGamal
encryption algorithms are described. Security assumptions and main advantages
of this scheme are discussed. We outline that this scheme is potentially
semantically secure under reasonable cryptographic assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02034</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02034</id><created>2016-03-07</created><authors><author><keyname>Afortiori</keyname><forenames>Tristan</forenames></author></authors><title>Comments on &quot;Overdemodulation for High-Performance Receivers with
  Low-Resolution ADC&quot;</title><categories>cs.IT math.IT</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the above-titled paper by Stein, Theiler, and Nossek (see ibid., vol. 4,
no. 2, p. 169-172, April 2015), the design of analog demodulator for receivers
with low-resolution analog-to-digital converters (ADCs) was addressed. The
authors proposed to increase the number ($M$) of analog demodulation channels
in the radio front-end beyond that of the classical quadrature demodulation
(with $M=2$ orthogonal sinusoidal functions). The proposed design was called
\emph{overdemodulation}. The estimation performance was evaluated in terms of a
lower bound for the Fisher Information with respect to an unknown parameter
$\boldsymbol{\theta}$. It was demonstrated via simulations that the lower bound
could be improved as $M$ increases. In this comment, we point out that this
result trivially follows (not for the lower bound but for the actual Fisher
Information) from the chain rule for the Fisher Information. More specifically,
since each channel in the overdemodulation framework provides a 1-bit random
variable (RV) parametrized by $\boldsymbol{\theta}$, the Fisher Information
increases with the addition of each new channel unless the 1-bit RV from the
new channel conditioned on all the 1-bit RVs from the remaining channels is
independent of the parameter $\boldsymbol{\theta}$. Furthermore, we argue that
overdemodulation does not make sense as a radio front-end from an engineering
perspective. Additional channels in the overdemodulation framework are
deterministically related to the in-phase (I) and quadrature (Q) channels of
the classical quadrature demodulation. This, in turn, implies that the signals
over the auxiliary $M-2$ channels can be synthesized at the baseband without
any connection to the physical world outside and are therefore redundant.
Furthermore, the lower bound for Fisher Information, which was credited to an
earlier letter by the first author, is well-known in statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02038</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02038</id><created>2016-03-07</created><authors><author><keyname>Nogueira</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Martinez-Cantin</keyname><forenames>Ruben</forenames></author><author><keyname>Bernardino</keyname><forenames>Alexandre</forenames></author><author><keyname>Jamone</keyname><forenames>Lorenzo</forenames></author></authors><title>Unscented Bayesian Optimization for Safe Robot Grasping</title><categories>cs.RO cs.AI cs.LG cs.SY</categories><comments>conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the robot grasp optimization problem of unknown objects
considering uncertainty in the input space. Grasping unknown objects can be
achieved by using a trial and error exploration strategy. Bayesian optimization
is a sample efficient optimization algorithm that is especially suitable for
this setups as it actively reduces the number of trials for learning about the
function to optimize. In fact, this active object exploration is the same
strategy that infants do to learn optimal grasps. One problem that arises while
learning grasping policies is that some configurations of grasp parameters may
be very sensitive to error in the relative pose between the object and robot
end-effector. We call these configurations unsafe because small errors during
grasp execution may turn good grasps into bad grasps. Therefore, to reduce the
risk of grasp failure, grasps should be planned in safe areas. We propose a new
algorithm, Unscented Bayesian optimization that is able to perform sample
efficient optimization while taking into consideration input noise to find safe
optima. The contribution of Unscented Bayesian optimization is twofold as if
provides a new decision process that drives exploration to safe regions and a
new selection procedure that chooses the optimal in terms of its safety without
extra analysis or computational cost. Both contributions are rooted on the
strong theory behind the unscented transformation, a popular nonlinear
approximation method. We show its advantages with respect to the classical
Bayesian optimization both in synthetic problems and in realistic robot grasp
simulations. The results highlights that our method achieves optimal and robust
grasping policies after few trials while the selected grasps remain in safe
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02041</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02041</id><created>2016-03-07</created><authors><author><keyname>Borsa</keyname><forenames>Diana</forenames></author><author><keyname>Graepel</keyname><forenames>Thore</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author></authors><title>Learning Shared Representations in Multi-task Reinforcement Learning</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a paradigm in multi-task reinforcement learning (MT-RL) in
which an agent is placed in an environment and needs to learn to perform a
series of tasks, within this space. Since the environment does not change,
there is potentially a lot of common ground amongst tasks and learning to solve
them individually seems extremely wasteful. In this paper, we explicitly model
and learn this shared structure as it arises in the state-action value space.
We will show how one can jointly learn optimal value-functions by modifying the
popular Value-Iteration and Policy-Iteration procedures to accommodate this
shared representation assumption and leverage the power of multi-task
supervised learning. Finally, we demonstrate that the proposed model and
training procedures, are able to infer good value functions, even under low
samples regimes. In addition to data efficiency, we will show in our analysis,
that learning abstractions of the state space jointly across tasks leads to
more robust, transferable representations with the potential for better
generalization. this shared representation assumption and leverage the power of
multi-task supervised learning. Finally, we demonstrate that the proposed model
and training procedures, are able to infer good value functions, even under low
samples regimes. In addition to data efficiency, we will show in our analysis,
that learning abstractions of the state space jointly across tasks leads to
more robust, transferable representations with the potential for better
generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02044</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02044</id><created>2016-03-07</created><authors><author><keyname>Hernandez</keyname><forenames>Bernardo</forenames></author><author><keyname>Trodden</keyname><forenames>Paul</forenames></author></authors><title>Distributed Model Predictive Control Using a Chain of Tubes</title><categories>cs.SY math.OC</categories><comments>Already submitted to the UKACC16 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new distributed MPC algorithm for the regulation of dynamically coupled
subsystems is presented in this paper. The current control action is computed
via two robust controllers working in a nested fashion. The inner controller
builds a nominal reference trajectory from a decentralized perspective. The
outer controller uses this information to take into account the effects of the
coupling and generate a distributed control action. The tube-based approach to
robustness is employed. A supplementary constraint is included in the outer
optimization problem to provide recursive feasibility of the overall controller
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02056</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02056</id><created>2016-03-07</created><authors><author><keyname>Liu</keyname><forenames>Wenqiang</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Duan</keyname><forenames>Haimeng</forenames></author><author><keyname>Wei</keyname><forenames>Bifan</forenames></author></authors><title>TruthDiscover: A Demonstration of Resolving Object Conflicts on Massive
  Linked Data</title><categories>cs.DB</categories><comments>4 pages. arXiv admin note: substantial text overlap with
  arXiv:1509.00104</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable effort has been made to increase the scale of Linked Data.
However, because of the openness of the Semantic Web and the ease of extracting
Linked Data from semi-structured sources (e.g., Wikipedia) and unstructured
sources, many Linked Data sources often provide conflicting objects for a
certain predicate of a real-world entity. Existing methods cannot be trivially
extended to resolve conflicts in Linked Data because Linked Data has a
scale-free property. In this demonstration, we present a novel system called
TruthDiscover, to identify the truth in Linked Data with a scale-free property.
First, TruthDiscover leverages the topological properties of the Source Belief
Graph to estimate the priori beliefs of sources, which are utilized to smooth
the trustworthiness of sources. Second, the Hidden Markov Random Field is
utilized to model interdependencies among objects for estimating the trust
values of objects accurately. TruthDiscover can visualize the process of
resolving conflicts in Linked Data. Experiments results on four datasets show
that TruthDiscover exhibits satisfactory accuracy when confronted with data
having a scale-free property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02057</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02057</id><created>2016-02-18</created><authors><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author></authors><title>Bootstrap percolation on geometric inhomogeneous random graphs</title><categories>math.PR cs.SI math.CO</categories><comments>32 pages</comments><msc-class>05C80, 05C82, 60C05, 60K35, 91D25</msc-class><acm-class>C.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric inhomogeneous random graphs (GIRGs) are a model for scale-free
networks with underlying geometry. We study bootstrap percolation on these
graphs, which is a process modelling the spread of an infection of vertices
starting within a (small) local region. We show that the process exhibits a
phase transition in terms of the initial infection rate in this region. We
determine the speed of the process in the supercritical case, up to lower order
terms, and show that its evolution is fundamentally influenced by the
underlying geometry. For vertices with given position and expected degree, we
determine the infection time up to lower order terms. Finally, we show how this
knowledge can be used to contain the infection locally by removing relatively
few edges from the graph. This is the first time that the role of geometry on
bootstrap percolation is analysed mathematically for geometric scale-free
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02063</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02063</id><created>2016-03-07</created><authors><author><keyname>Brisaboa</keyname><forenames>Nieves R.</forenames></author><author><keyname>De Bernardo</keyname><forenames>Guillermo</forenames></author><author><keyname>Konow</keyname><forenames>Roberto</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Seco</keyname><forenames>Diego</forenames></author></authors><title>Aggregated 2D Range Queries on Clustered Points</title><categories>cs.DS</categories><comments>To be published in Information Systems (Elsevier). This research has
  received funding from the European Union's Horizon 2020 research and
  innovation programme under the Marie Sk{\l}odowska-Curie grant agreement No
  690941 (BIRDS project)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient processing of aggregated range queries on two-dimensional grids is
a common requirement in information retrieval and data mining systems, for
example in Geographic Information Systems and OLAP cubes. We introduce a
technique to represent grids supporting aggregated range queries that requires
little space when the data points in the grid are clustered, which is common in
practice. We show how this general technique can be used to support two
important types of aggregated queries, which are ranked range queries and
counting range queries. Our experimental evaluation shows that this technique
can speed up aggregated queries up to more than an order of magnitude, with a
small space overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02074</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02074</id><created>2016-03-07</created><authors><author><keyname>Sheriff</keyname><forenames>Mohammed Rayyan</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>Optimal dictionary for least squares representation</title><categories>cs.LG math.OC stat.ML</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary Learning problems are concerned with finding a collection of
vectors usually referred to as the dictionary, such that the representation of
random vectors with a given distribution using this dictionary is optimal. Most
of the recent research in dictionary learning is focused on developing
dictionaries which offer sparse representation, i.e., optimal representation in
the $\ell_0$ sense. We consider the problem of finding an optimal dictionary
with which representation of samples of a random vector on an average is
optimal. Optimality of representation is defined in the sense of attaining
minimal average $\ell_2$-norm of the coefficient vector used to represent the
random vector. With the help of recent results related to rank-one
decompositions of real symmetric positive semi-definite matrices, an explicit
solution for an $\ell_2$-optimal dictionary is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02078</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02078</id><created>2016-03-07</created><authors><author><keyname>Liu</keyname><forenames>Jiang</forenames></author><author><keyname>Gao</keyname><forenames>Chenqiang</forenames></author><author><keyname>Wang</keyname><forenames>Lan</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author></authors><title>A Learning-based Frame Pooling Model For Event Detection</title><categories>cs.CV</categories><comments>submitted to IEEE ICIP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting complex events in a large video collection crawled from video
websites is a challenging task. When applying directly good image-based feature
representation, e.g., HOG, SIFT, to videos, we have to face the problem of how
to pool multiple frame feature representations into one feature representation.
In this paper, we propose a novel learning-based frame pooling method. We
formulate the pooling weight learning as an optimization problem and thus our
method can automatically learn the best pooling weight configuration for each
specific event category. Experimental results conducted on TRECVID MED 2011
reveal that our method outperforms the commonly used average pooling and max
pooling strategies on both high-level and low-level 2D image features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02080</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02080</id><created>2016-03-07</created><authors><author><keyname>Abrahamsen</keyname><forenames>Mikkel</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Finding the Maximum Subset with Bounded Convex Curvature</title><categories>cs.CG</categories><comments>A short version of this paper will be presented at SoCG 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for solving an important geometric problem arising
in computer-aided manufacturing. When machining a pocket in a solid piece of
material such as steel using a rough tool in a milling machine, sharp convex
corners of the pocket cannot be done properly, but have to be left for finer
tools that are more expensive to use. We want to determine a tool path that
maximizes the use of the rough tool. Mathematically, this boils down to the
following problem. Given a simply-connected set of points $P$ in the plane such
that the boundary $\partial P$ is a curvilinear polygon consisting of $n$ line
segments and circular arcs of arbitrary radii, compute the maximum subset
$Q\subseteq P$ consisting of simply-connected sets where the boundary of each
set is a curve with bounded convex curvature. A closed curve has bounded convex
curvature if, when traversed in counterclockwise direction, it turns to the
left with curvature at most $1$. There is no bound on the curvature where it
turns to the right. The difference in the requirement to left- and
right-curvature is a natural consequence of different conditions when machining
convex and concave areas of the pocket. We devise an algorithm to compute the
unique maximum such set $Q$. The algorithm runs in $O(n\log n)$ time and uses
$O(n)$ space.
  For the correctness of our algorithm, we prove a new generalization of the
Pestov-Ionin Theorem. This is needed to show that the output $Q$ of our
algorithm is indeed maximum in the sense that if $Q'$ is any subset of $P$ with
a boundary of bounded convex curvature, then $Q'\subseteq Q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02094</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02094</id><created>2016-03-07</created><authors><author><keyname>Bondorf</keyname><forenames>Steffen</forenames></author><author><keyname>Nikolaus</keyname><forenames>Paul</forenames></author><author><keyname>Schmitt</keyname><forenames>Jens B.</forenames></author></authors><title>Delay Bounds in Feed-Forward Networks - A Fast and Accurate Network
  Calculus Solution</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guaranteeing accurate worst-case bounds on the end-to-end delay that data
flows experience in communication networks is required for a variety of
safety-critical systems, for instance in avionics. Deterministic Network
Calculus (DNC) is a widely used method to derive such bounds. The DNC theory
has been advanced in recent years to provide ever tighter delay bounds, though
this turned out a hard problem in the general feed-forward network case.
Currently, the only analysis to achieve tight delay bounds, i.e., best possible
ones, is based on an optimization formulation instead of the usual algebraic
DNC analysis. However, it has also been shown to be NP-hard and was accompanied
by a similar, yet relaxed optimization that trades tightness against
computational effort. In our article, we derive a novel, fast algebraic delay
analysis that nevertheless retains a high degree of accuracy. We show in
extensive numerical experiments that our solution enables the analysis of
large-scale networks by reducing the computation time by several orders of
magnitude in contrast to the optimization analysis. Moreover, in networks where
optimization is still feasible, our delay bounds stay within close range,
deviating on average by only 1.16% in our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02106</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02106</id><created>2016-03-02</created><authors><author><keyname>Xu</keyname><forenames>Tianhua</forenames></author></authors><title>Analytical Estimation of Carrier Phase Recovery Approaches in Long-Haul
  High-Speed Optical Communication Systems</title><categories>cs.IT math.IT physics.class-ph physics.optics</categories><comments>9 pages</comments><msc-class>94A12</msc-class><acm-class>C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analytical study on the carrier phase estimation (CPE) approaches,
involving a one-tap normalized least-mean-square (NLMS) algorithm, a block-wise
average (BWA) algorithm, and a Viterbi-Viterbi (VV) algorithm has been
investigated in the long-haul high-speed n-level phase shift keying (n-PSK)
coherent optical fiber communication systems. The close-form predictions for
the bit-error-rate (BER) performance have been derived and analyzed by
considering both the intrinsic laser phase noise and the equalization enhanced
phase noise (EEPN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02130</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02130</id><created>2016-03-07</created><authors><author><keyname>Jing</keyname><affiliation>Janet</affiliation></author><author><keyname>Liu</keyname></author><author><keyname>Backes</keyname><forenames>John D.</forenames></author><author><keyname>Cofer</keyname><forenames>Darren</forenames></author><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author></authors><title>From Design Contracts to Component Requirements Verification</title><categories>cs.SE</categories><comments>15 pages, 2 figures, conference submission</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the development and verification of complex airborne systems, a
variety of languages and development environments are used for different levels
of the system hierarchy. As a result, there may be manual steps to translate
requirements between these different environments. This paper presents a
tool-supported export technique that translates high-level requirements from
the software architecture modeling environment into property observers that can
be used for verification in the software component environment. This allows
efficient verification that the component designs comply with their high-level
requirements. It also provides an automated tool chain supporting formal
verification from system requirements down to low-level software requirements
that is consistent with certification guidance for avionics systems. The
effectiveness of the technique has been evaluated and demonstrated on a medical
infusion pump and an aircraft wheel braking system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02133</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02133</id><created>2016-03-07</created><authors><author><keyname>Cho</keyname><forenames>Kenta</forenames></author><author><keyname>Westerbaan</keyname><forenames>Abraham</forenames></author></authors><title>Von Neumann Algebras form a Model for the Quantum Lambda Calculus</title><categories>cs.LO math.OA quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model of Selinger and Valiron's quantum lambda calculus based on
von Neumann algebras, and show that the model is adequate with respect to the
operational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02139</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02139</id><created>2016-03-07</created><authors><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Learning a Discriminative Null Space for Person Re-identification</title><categories>cs.CV</categories><comments>accepted by CVPR2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing person re-identification (re-id) methods focus on learning the
optimal distance metrics across camera views. Typically a person's appearance
is represented using features of thousands of dimensions, whilst only hundreds
of training samples are available due to the difficulties in collecting matched
training images. With the number of training samples much smaller than the
feature dimension, the existing methods thus face the classic small sample size
(SSS) problem and have to resort to dimensionality reduction techniques and/or
matrix regularisation, which lead to loss of discriminative power. In this
work, we propose to overcome the SSS problem in re-id distance metric learning
by matching people in a discriminative null space of the training data. In this
null space, images of the same person are collapsed into a single point thus
minimising the within-class scatter to the extreme and maximising the relative
between-class separation simultaneously. Importantly, it has a fixed dimension,
a closed-form solution and is very efficient to compute. Extensive experiments
carried out on five person re-identification benchmarks including VIPeR,
PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats
the state-of-the-art alternatives, often by a big margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02148</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02148</id><created>2016-03-07</created><authors><author><keyname>Goncharov</keyname><forenames>Sergey</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Rauch</keyname><forenames>Christoph</forenames></author></authors><title>Complete Elgot Monads and Coalgebraic Resumptions</title><categories>cs.LO</categories><comments>full version, 39 p</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monads are extensively used nowadays to abstractly model a wide range of
computational effects such as nondeterminism, statefulness, and exceptions. It
turns out that equipping a monad with a (uniform) iteration operator satisfying
a set of natural axioms allows for modelling iterative computations just as
abstractly. The emerging monads are called complete Elgot monads. It has been
shown recently that extending complete Elgot monads with free effects (e.g.
operations of sending/receiving messages over channels) canonically leads to
generalized coalgebraic resumption monads, previously used as semantic domains
for non-wellfounded guarded processes. In this paper, we continue the study of
the relationship between abstract complete Elgot monads and those that capture
coalgebraic resumptions, by comparing the corresponding categories of
(Eilenberg-Moore) algebras. To this end we first provide a characterization of
the latter category; even more generally, we formulate this characterization in
terms of Uustalu's parametrized monads. This is further used for establishing a
characterization of complete Elgot monads as precisely those monads whose
algebras are coherently equipped with the structure of algebras of coalgebraic
resumption monads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02175</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02175</id><created>2016-03-07</created><authors><author><keyname>Yang</keyname><forenames>Chunfeng</forenames></author><author><keyname>Zhou</keyname><forenames>Yipeng</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>Who are Like-minded: Mining User Interest Similarity in Online Social
  Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we mine and learn to predict how similar a pair of users'
interests towards videos are, based on demographic (age, gender and location)
and social (friendship, interaction and group membership) information of these
users. We use the video access patterns of active users as ground truth (a form
of benchmark). We adopt tag-based user profiling to establish this ground
truth, and justify why it is used instead of video-based methods, or many
latent topic models such as LDA and Collaborative Filtering approaches. We then
show the effectiveness of the different demographic and social features, and
their combinations and derivatives, in predicting user interest similarity,
based on different machine-learning methods for combining multiple features. We
propose a hybrid tree-encoded linear model for combining the features, and show
that it out-performs other linear and treebased models. Our methods can be used
to predict user interest similarity when the ground-truth is not available,
e.g. for new users, or inactive users whose interests may have changed from old
access data, and is useful for video recommendation. Our study is based on a
rich dataset from Tencent, a popular service provider of social networks, video
services, and various other services in China.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02178</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02178</id><created>2016-03-07</created><authors><author><keyname>Hajibagheri</keyname><forenames>Nima</forenames></author></authors><title>Modeling Information Diffusion in Social Networks</title><categories>cs.SI</categories><comments>in Persian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major feature of social networks (e.g., massive online social networks)
is the dissemination of information, such as news, rumors and opinions.
Information can be propagated via natural connections in written, oral or
electronic forms. The physics of information diffusion has been changed with
the mainstream adoption of the Internet and Web. Until a few years ago, the
major barrier for someone who wanted a piece of information to spread through a
community was the cost of the technical infrastructure required to reach a
large number of people. Today, with widespread access to the Internet, this
bottleneck has largely been removed. Information diffusion has been one of the
focuses in social network research area, due to its importance in social
interactions and everyday life. More recently, during the last twenty to thirty
years, there has been interest and attention not just in observing information
and innovation flow, but also in influencing and creating them. Modeling
information diffusion in networks enables us to reason about its spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02182</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02182</id><created>2016-03-07</created><authors><author><keyname>Okandeji</keyname><forenames>Alexander A.</forenames></author><author><keyname>Khandaker</keyname><forenames>Muhammad R. A.</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author></authors><title>Wireless Information and Power Transfer in Full-Duplex Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of maximizing the sum-rate for simultaneous
wireless information and power transfer (SWIPT) in a full-duplex bi-directional
communication system subject to energy harvesting and transmit power
constraints at both nodes. We investigate the optimum design of the receive
power splitters and transmit powers for SWIPT in full-duplex mode. Exploiting
rate-split method, an iterative algorithm is derived to solve the non-convex
problem. The effectiveness of the proposed algorithm is justified through
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02185</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02185</id><created>2016-03-07</created><authors><author><keyname>Wang</keyname><forenames>Jialei</forenames></author><author><keyname>Kolar</keyname><forenames>Mladen</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Distributed Multi-Task Learning with Shared Representation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of distributed multi-task learning with shared
representation, where each machine aims to learn a separate, but related, task
in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix
has low rank. We consider a setting where each task is handled by a different
machine, with samples for the task available locally on the machine, and study
communication-efficient methods for exploiting the shared structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02187</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02187</id><created>2016-03-07</created><authors><author><keyname>Doychev</keyname><forenames>Goran</forenames></author><author><keyname>K&#xf6;pf</keyname><forenames>Boris</forenames></author></authors><title>Rigorous Analysis of Software Countermeasures against Cache Attacks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CPU caches reduce the latency of memory accesses on average, but not in the
worst case. Thus, they introduce variations into the execution time that can be
exploited by adversaries to recover secrets from programs, such as private
information about users or cryptographic keys. Establishing the security of
countermeasures against this threat often requires intricate reasoning about
the interactions of the program and the hardware platform and has so far only
been done for restricted cases. In this paper we devise novel techniques that
provide support for bit-level and arithmetic reasoning about pointers in the
presence of dynamic memory allocation. These techniques enable us to perform
the first rigorous analysis of widely deployed software countermeasures against
cache attacks on modular exponentiation, based on executable code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02188</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02188</id><created>2016-03-07</created><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Friedetzky</keyname><forenames>Tom</forenames></author><author><keyname>Kling</keyname><forenames>Peter</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author><author><keyname>Nagel</keyname><forenames>Lars</forenames></author><author><keyname>Wastell</keyname><forenames>Chris</forenames></author></authors><title>Self-stabilizing Balls &amp; Bins in Batches</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in distributed computing is the distribution of
requests to a set of uniform servers without a centralized controller.
Classically, such problems are modeled as static balls into bins processes,
where $m$ balls (tasks) are to be distributed to $n$ bins (servers). In a
seminal work, Azar et al. proposed the sequential strategy \greedy{d} for
$n=m$. When thrown, a ball queries the load of $d$ random bins and is allocated
to a least loaded of these. Azar et al. showed that $d=2$ yields an exponential
improvement compared to $d=1$. Berenbrink et al. extended this to $m\gg n$,
showing that the maximal load difference is independent of $m$ for $d=2$ (in
contrast to $d=1$).
  We propose a new variant of an \emph{infinite} balls into bins process. Each
round an expected number of $\lambda n$ new balls arrive and are distributed
(in parallel) to the bins. Each non-empty bin deletes one of its balls. This
setting models a set of servers processing incoming requests, where clients can
query a server's current load but receive no information about parallel
requests. We study the \greedy{d} distribution scheme in this setting and show
a strong self-stabilizing property: For \emph{any} arrival rate
$\lambda=\lambda(n)&lt;1$, the system load is time-invariant. Moreover, for
\emph{any} (even super-exponential) round $t$, the maximum system load is
(w.h.p.) $O(\frac{1}{1-\lambda}\cdot\log\frac{n}{1-\lambda})$ for $d=1$ and
$O(\log\frac{n}{1-\lambda})$ for $d=2$. In particular, \greedy{2} has an
exponentially smaller system load for high arrival rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02194</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02194</id><created>2016-03-07</created><authors><author><keyname>Barkan</keyname><forenames>Oren</forenames></author><author><keyname>Weill</keyname><forenames>Jonathan</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>Gaussian Process Regression for Out-of-Sample Extension</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manifold learning methods are useful for high dimensional data analysis. Many
of the existing methods produce a low dimensional representation that attempts
to describe the intrinsic geometric structure of the original data. Typically,
this process is computationally expensive and the produced embedding is limited
to the training data. In many real life scenarios, the ability to produce
embedding of unseen samples is essential. In this paper we propose a Bayesian
non-parametric approach for out-of-sample extension. The method is based on
Gaussian Process Regression and independent of the manifold learning algorithm.
Additionally, the method naturally provides a measure for the degree of
abnormality for a newly arrived data point that did not participate in the
training process. We derive the mathematical connection between the proposed
method and the Nystrom extension and show that the latter is a special case of
the former. We present extensive experimental results that demonstrate the
performance of the proposed method and compare it to other existing
out-of-sample extension methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02199</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02199</id><created>2016-03-07</created><authors><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Pastor</keyname><forenames>Peter</forenames></author><author><keyname>Krizhevsky</keyname><forenames>Alex</forenames></author><author><keyname>Quillen</keyname><forenames>Deirdre</forenames></author></authors><title>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning
  and Large-Scale Data Collection</title><categories>cs.LG cs.AI cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a learning-based approach to hand-eye coordination for robotic
grasping from monocular images. To learn hand-eye coordination for grasping, we
trained a large convolutional neural network to predict the probability that
task-space motion of the gripper will result in successful grasps, using only
monocular camera images and independently of camera calibration or the current
robot pose. This requires the network to observe the spatial relationship
between the gripper and objects in the scene, thus learning hand-eye
coordination. We then use this network to servo the gripper in real time to
achieve successful grasps. To train our network, we collected over 800,000
grasp attempts over the course of two months, using between 6 and 14 robotic
manipulators at any given time, with differences in camera placement and
hardware. Our experimental evaluation demonstrates that our method achieves
effective real-time control, can successfully grasp novel objects, and corrects
mistakes by continuous servoing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02200</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02200</id><created>2016-03-07</created><authors><author><keyname>Anirudh</keyname><forenames>Rushil</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author><author><keyname>Su</keyname><forenames>Jingyong</forenames></author><author><keyname>Srivastava</keyname><forenames>Anuj</forenames></author></authors><title>Elastic Functional Coding of Riemannian Trajectories</title><categories>cs.CV math.DG</categories><comments>Under major revision at IEEE T-PAMI, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual observations of dynamic phenomena, such as human actions, are often
represented as sequences of smoothly-varying features . In cases where the
feature spaces can be structured as Riemannian manifolds, the corresponding
representations become trajectories on manifolds. Analysis of these
trajectories is challenging due to non-linearity of underlying spaces and
high-dimensionality of trajectories. In vision problems, given the nature of
physical systems involved, these phenomena are better characterized on a
low-dimensional manifold compared to the space of Riemannian trajectories. For
instance, if one does not impose physical constraints of the human body, in
data involving human action analysis, the resulting representation space will
have highly redundant features. Learning an effective, low-dimensional
embedding for action representations will have a huge impact in the areas of
search and retrieval, visualization, learning, and recognition. The difficulty
lies in inherent non-linearity of the domain and temporal variability of
actions that can distort any traditional metric between trajectories. To
overcome these issues, we use the framework based on transported square-root
velocity fields (TSRVF); this framework has several desirable properties,
including a rate-invariant metric and vector space representations. We propose
to learn an embedding such that each action trajectory is mapped to a single
point in a low-dimensional Euclidean space, and the trajectories that differ
only in temporal rates map to the same point. We utilize the TSRVF
representation, and accompanying statistical summaries of Riemannian
trajectories, to extend existing coding methods such as PCA, KSVD and Label
Consistent KSVD to Riemannian trajectories or more generally to Riemannian
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02208</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02208</id><created>2016-03-07</created><authors><author><keyname>Shen</keyname><forenames>Wen</forenames></author><author><keyname>Lopes</keyname><forenames>Cristina V.</forenames></author><author><keyname>Crandall</keyname><forenames>Jacob W.</forenames></author></authors><title>An Online Mechanism for Ridesharing in Autonomous Mobility-on-Demand
  Systems</title><categories>cs.AI cs.GT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With proper management, Autonomous Mobility-on-Demand (AMoD) systems have
great potential to satisfy urban population's mobility demand by providing
safe, convenient, and affordable ridesharing services. Meanwhile, such systems
can substantially decrease private car ownership and use, and thus
significantly reduce traffic congestion, energy consumption and carbon
emissions. In order to schedule the assignments optimally, an AMoD system
requires detailed information about the demand from passengers. However,
chances are that passengers do not cooperate with the service providers because
of self-interestedness. Therefore, an online mechanism is desirable if it
incentivizes passengers to truthfully report their actual demand. For the
purpose of promoting ridesharing, we hereby introduce an integrated online
ridesharing mechanism (IORS). Numerical results show that the IORS mechanism
outperforms the offline, auction-based mechanism substantially. It has a very
close performance compared to the optimal solution, with less computation time
required and no future knowledge about the demand needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02209</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02209</id><created>2016-03-07</created><authors><author><keyname>Vasiliev</keyname><forenames>Alexander</forenames></author></authors><title>Quantum Hashing for Finite Abelian Groups</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generalization of the quantum hashing technique based on the
notion of the small-bias sets. These sets have proved useful in different areas
of computer science, and here their properties give an optimal construction for
succinct quantum presentation of elements of any finite abelian group, which
can be used in various computational and cryptographic scenarios. The known
quantum fingerprinting schemas turn out to be the special cases of the proposed
quantum hashing for the corresponding abelian group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02211</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02211</id><created>2016-03-07</created><authors><author><keyname>Kumar</keyname><forenames>Rajesh</forenames></author><author><keyname>Phoha</keyname><forenames>Vir V</forenames></author><author><keyname>Raina</keyname><forenames>Rahul</forenames></author></authors><title>Authenticating users through their arm movement patterns</title><categories>cs.CV cs.CR</categories><acm-class>K.6.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose four continuous authentication designs by using the
characteristics of arm movements while individuals walk. The first design uses
acceleration of arms captured by a smartwatch's accelerometer sensor, the
second design uses the rotation of arms captured by a smartwatch's gyroscope
sensor, third uses the fusion of both acceleration and rotation at the
feature-level and fourth uses the fusion at score-level. Each of these designs
is implemented by using four classifiers, namely, k nearest neighbors (k-NN)
with Euclidean distance, Logistic Regression, Multilayer Perceptrons, and
Random Forest resulting in a total of sixteen authentication mechanisms. These
authentication mechanisms are tested under three different environments, namely
an intra-session, inter-session on a dataset of 40 users and an inter-phase on
a dataset of 12 users. The sessions of data collection were separated by at
least ten minutes, whereas the phases of data collection were separated by at
least three months. Under the intra-session environment, all of the twelve
authentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0%
and dynamic false reject rate (DFRR) of 0%. For the inter-session environment,
feature level fusion-based design with classifier k-NN achieves the best error
rates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRR
increased from 5.68% and 4.23% to 15.03% and 14.62% respectively when feature
level fusion-based design with classifier k-NN was tested under the inter-phase
environment on a dataset of 12 users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02226</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02226</id><created>2016-03-07</created><authors><author><keyname>Zhou</keyname><forenames>Huan</forenames></author><author><keyname>Idrees</keyname><forenames>Kamran</forenames></author><author><keyname>Gracia</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Leveraging MPI-3 Shared-Memory Extensions for Efficient PGAS Runtime
  Systems</title><categories>cs.DC</categories><comments>12 papers, accepted for publication in EuroPar 2015</comments><doi>10.1007/978-3-662-48096-0_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relaxed semantics and rich functionality of one-sided communication
primitives of MPI-3 makes MPI an attractive candidate for the implementation of
PGAS models. However, the performance of such implementation suffers from the
fact, that current MPI RMA implementations typically have a large overhead when
source and target of a communication request share a common, local physical
memory. In this paper, we present an optimized PGAS-like runtime system which
uses the new MPI-3 shared-memory extensions to serve intra-node communication
requests and MPI-3 one-sided communication primitives to serve inter-node
communication requests. The performance of our runtime system is evaluated on a
Cray XC40 system through low-level communication benchmarks, a random-access
benchmark and a stencil kernel. The results of the experiments demonstrate that
the performance of our hybrid runtime system matches the performance of
low-level RMA libraries for intra-node transfers, and that of MPI-3 for
inter-node transfers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02238</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02238</id><created>2016-03-07</created><authors><author><keyname>Ambroszkiewicz</keyname><forenames>Stanislaw</forenames></author></authors><title>On higher order computations and synaptic meta-plasticity in the human
  brain: IT point of view (March, 2016)</title><categories>cs.NE</categories><comments>Keywords: computations in human brain, higher order functions and
  functionals, synaptic meta-plasticity, glia and atrocytes, non-von Neumann
  computer architecture, Backus' function-level programming language</comments><msc-class>92B20</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Glia modify neuronal connectivity by creating structural changes in the
neuronal connectome. Glia also influence the functional connectome by modifying
the flow of information through neural networks (Fields et al. 2015). There are
strong experimental evidences that glia are responsible for synaptic
meta-plasticity. Synaptic plasticity is the modification of the strength of
connections between neurons. Meta-plasticity, i.e. plasticity of synaptic
plasticity, may be viewed as mechanisms for dynamic reconfiguration of neuron
circuits. Since synapse creation corresponds to the mathematical notion of
function composition, the mechanisms may serve as a grounding for functionals,
i.e. higher order functions that take functions as their arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02250</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02250</id><created>2016-03-07</created><authors><author><keyname>Foster</keyname><forenames>Dean</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Karloff</keyname><forenames>Howard</forenames></author></authors><title>Online Sparse Linear Regression</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the online sparse linear regression problem, which is the problem
of sequentially making predictions observing only a limited number of features
in each round, to minimize regret with respect to the best sparse linear
regressor, where prediction accuracy is measured by square loss. We give an
inefficient algorithm that obtains regret bounded by $\tilde{O}(\sqrt{T})$
after $T$ prediction rounds. We complement this result by showing that no
algorithm running in polynomial time per iteration can achieve regret bounded
by $O(T^{1-\delta})$ for any constant $\delta &gt; 0$ unless $\text{NP} \subseteq
\text{BPP}$. This computational hardness result resolves an open problem
presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013).
This hardness result holds even if the algorithm is allowed to access more
features than the best sparse linear regressor up to a logarithmic factor in
the dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02252</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02252</id><created>2016-03-07</created><authors><author><keyname>Li</keyname><forenames>Wenbin</forenames></author><author><keyname>Cosker</keyname><forenames>Darren</forenames></author><author><keyname>Brown</keyname><forenames>Matthew</forenames></author></authors><title>Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences</title><categories>cs.CV</categories><comments>Preprint version of our paper accepted by Journal of Intelligent and
  Fuzzy Systems</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  It is hard to densely track a nonrigid object in long term, which is a
fundamental research issue in the computer vision community. This task often
relies on estimating pairwise correspondences between images over time where
the error is accumulated and leads to a drift issue. In this paper, we
introduce a novel optimization framework with an Anchor Patch constraint. It is
supposed to significantly reduce overall errors given long sequences containing
non-rigidly deformable objects. Our framework can be applied to any dense
tracking algorithm, e.g. optical flow. We demonstrate the success of our
approach by showing significant error reduction on 6 popular optical flow
algorithms applied to a range of real-world nonrigid benchmarks. We also
provide quantitative analysis of our approach given synthetic occlusions and
image noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1603.02253</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1603.02253</id><created>2016-03-07</created><authors><author><keyname>Li</keyname><forenames>Wenbin</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Lee</keyname><forenames>JeeHang</forenames></author><author><keyname>Ren</keyname><forenames>Gang</forenames></author><author><keyname>Cosker</keyname><forenames>Darren</forenames></author></authors><title>Blur Robust Optical Flow using Motion Channel</title><categories>cs.CV</categories><comments>Preprint of our paper accepted by Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is hard to estimate optical flow given a realworld video sequence with
camera shake and other motion blur. In this paper, we first investigate the
blur parameterization for video footage using near linear motion elements. we
then combine a commercial 3D pose sensor with an RGB camera, in order to film
video footage of interest together with the camera motion. We illustrates that
this additional camera motion/trajectory channel can be embedded into a hybrid
framework by interleaving an iterative blind deconvolution and warping based
optical flow scheme. Our method yields improved accuracy within three other
state-of-the-art baselines given our proposed ground truth blurry sequences;
and several other realworld sequences filmed by our imaging system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9807003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>adap-org/9807003</id><created>1998-07-16</created><authors><author><keyname>Astor</keyname><forenames>Jens C.</forenames><affiliation>Caltech</affiliation></author><author><keyname>Adami</keyname><forenames>Christoph</forenames><affiliation>Caltech</affiliation></author></authors><title>Development and Evolution of Neural Networks in an Artificial Chemistry</title><categories>adap-org cs.NE nlin.AO q-bio.PE</categories><comments>8 pages LaTeX, style file included, 8 embedded postscript figures. To
  be published in Proc. of 3rd German Workshop on Artificial Life (GWAL)</comments><report-no>KRL MAP-234</report-no><abstract>  We present a model of decentralized growth for Artificial Neural Networks
(ANNs) inspired by the development and the physiology of real nervous systems.
In this model, each individual artificial neuron is an autonomous unit whose
behavior is determined only by the genetic information it harbors and local
concentrations of substrates modeled by a simple artificial chemistry. Gene
expression is manifested as axon and dendrite growth, cell division and
differentiation, substrate production and cell stimulation. We demonstrate the
model's power with a hand-written genome that leads to the growth of a simple
network which performs classical conditioning. To evolve more complex
structures, we implemented a platform-independent, asynchronous, distributed
Genetic Algorithm (GA) that allows users to participate in evolutionary
experiments via the World Wide Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9903003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>adap-org/9903003</id><created>1999-03-05</created><authors><author><keyname>Ofria</keyname><forenames>Charles</forenames><affiliation>California Institute of Technology</affiliation></author><author><keyname>Adami</keyname><forenames>Christoph</forenames><affiliation>California Institute of Technology</affiliation></author></authors><title>Evolution of genetic organization in digital organisms</title><categories>adap-org cs.NE nlin.AO q-bio.PE</categories><comments>18 pages with 5 embedded figures. Proc. of DIMACS workshop on
  &quot;Evolution as Computation&quot;, Jan. 11-12, Princeton, NJ. L. Landweber and E.
  Winfree, eds. (Springer, 1999)</comments><abstract>  We examine the evolution of expression patterns and the organization of
genetic information in populations of self-replicating digital organisms.
Seeding the experiments with a linearly expressed ancestor, we witness the
development of complex, parallel secondary expression patterns. Using
principles from information theory, we demonstrate an evolutionary pressure
towards overlapping expressions causing variation (and hence further evolution)
to sharply drop. Finally, we compare the overlapping sections of dominant
genomes to those portions which are singly expressed and observe a significant
difference in the entropy of their encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9909006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>adap-org/9909006</id><created>1999-09-21</created><updated>2001-01-04</updated><authors><author><keyname>Fuks</keyname><forenames>Henryk</forenames></author><author><keyname>Lawniczak</keyname><forenames>Anna T.</forenames></author></authors><title>Performance of data networks with random links</title><categories>adap-org cs.NI nlin.AO</categories><comments>20 pages, 10 figures</comments><report-no>The Fields Institute report FI-PIA1999-012</report-no><journal-ref>Mathematics and Computers in Simulation 51 103-119 (1999)</journal-ref><abstract>  We investigate simplified models of computer data networks and examine how
the introduction of additional random links influences the performance of these
net works. In general, the impact of additional random links on the performance
of the network strongly depends on the routing algorithm used in the network.
Significant performance gains can be achieved if the routing is based on
&quot;geometrical distance&quot; or shortest path reduced table routing. With shortest
path full table routing degradation of performance is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:alg-geom/9608018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>alg-geom/9608018</id><created>1996-08-21</created><updated>2003-01-14</updated><authors><author><keyname>Johnsen</keyname><forenames>Trygve</forenames></author></authors><title>Rank Two Bundles on Algebraic Curves and Decoding of Goppa Codes</title><categories>alg-geom cs.IT math.AG math.IT</categories><comments>An error in (what is now called) Theorem 3.4 has been corrected</comments><msc-class>14D20, 94B</msc-class><abstract>  We study a connection between two topics: Decoding of Goppa codes arising
from an algebraic curve, and rank two extensions of certain line bundles on the
curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0005101</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0005101</id><created>2000-05-05</created><authors><author><keyname>Maris</keyname><forenames>M.</forenames></author><author><keyname>Maino</keyname><forenames>D.</forenames></author><author><keyname>Burigana</keyname><forenames>C.</forenames></author><author><keyname>Pasian</keyname><forenames>F.</forenames></author></authors><title>Data Streams from the Low Frequency Instrument On-Board the Planck
  Satellite: Statistical Analysis and Compression Efficiency</title><categories>astro-ph cs.OH physics.data-an physics.space-ph</categories><comments>May 3, 2000 release, 61 pages, 6 figures coded as eps, 9 tables (4
  included as eps), LaTeX 2.09 + assms4.sty, style file included, submitted for
  the pubblication on PASP May 3, 2000</comments><report-no>OAT Int. Rep. 71/00 - OAT Pub. Num. 2140</report-no><doi>10.1051/aas:2000289</doi><abstract>  The expected data rate produced by the Low Frequency Instrument (LFI) planned
to fly on the ESA Planck mission in 2007, is over a factor 8 larger than the
bandwidth allowed by the spacecraft transmission system to download the LFI
data. We discuss the application of lossless compression to Planck/LFI data
streams in order to reduce the overall data flow. We perform both theoretical
analysis and experimental tests using realistically simulated data streams in
order to fix the statistical properties of the signal and the maximal
compression rate allowed by several lossless compression algorithms. We studied
the influence of signal composition and of acquisition parameters on the
compression rate Cr and develop a semiempirical formalism to account for it.
The best performing compressor tested up to now is the arithmetic compression
of order 1, designed for optimizing the compression of white noise like
signals, which allows an overall compression rate &lt;Cr&gt; = 2.65 +/- 0.02. We find
that such result is not improved by other lossless compressors, being the
signal almost white noise dominated. Lossless compression algorithms alone will
not solve the bandwidth problem but needs to be combined with other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0008307</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0008307</id><created>2000-08-19</created><authors><author><keyname>Borne</keyname><forenames>Kirk D.</forenames></author></authors><title>Science User Scenarios for a Virtual Observatory Design Reference
  Mission: Science Requirements for Data Mining</title><categories>astro-ph cs.DB cs.DL cs.IR</categories><comments>4 pages. Paper to appear in the proceedings of the June 2000 &quot;Virtual
  Observatories of the Future&quot; conference at Caltech, edited by R. J. Brunner,
  S. G. Djorgovski, &amp; A. Szalay. (For figures and demos related to sample user
  scenarios, see
  http://adc.gsfc.nasa.gov/adc/adc_science/adc-science-scenario-papers.html .)</comments><abstract>  The knowledge discovery potential of the new large astronomical databases is
vast. When these are used in conjunction with the rich legacy data archives,
the opportunities for scientific discovery multiply rapidly. A Virtual
Observatory (VO) framework will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed. The
development of a VO should address the data mining needs of various
astronomical research constituencies. By way of example, two user scenarios are
presented which invoke applications and linkages of data across the catalog and
image domains in order to address specific astrophysics research problems.
These illustrate a subset of the desired capabilities and power of the VO, and
as such they represent potential components of a VO Design Reference Mission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0010583</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0010583</id><created>2000-10-27</created><updated>2000-10-30</updated><authors><author><keyname>Borne</keyname><forenames>Kirk D.</forenames><affiliation>Raytheon Information Technology and Scientific Services</affiliation><affiliation>NASA Goddard Space Flight Center</affiliation></author></authors><title>Data Mining in Astronomical Databases</title><categories>astro-ph cs.DB cs.DL cs.IR</categories><comments>3 pages. Uses eso.sty style file. Paper to appear in the proceedings
  of the August 2000 &quot;Mining the Sky&quot; conference at MPA/ESO/MPE, Garching,
  Germany. (For figures and demos related to sample user scenarios, see
  http://adc.gsfc.nasa.gov/adc/adc_science/adc-science-scenario-papers.html .)
  (Revised version v2 only corrected these comments.)</comments><doi>10.1007/10849171_88</doi><abstract>  A Virtual Observatory (VO) will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0107084</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0107084</id><created>2001-07-04</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames><affiliation>MIT Lincoln Laboratory</affiliation></author></authors><title>A Multi-Threaded Fast Convolver for Dynamically Parallel Image Filtering</title><categories>astro-ph cs.DC</categories><comments>25 pages including color figures. Submitted to the Journal of
  Parallel and Distributed Computing</comments><journal-ref>Journal of Parallel and Distributed Computing archive Volume 63
  Issue 3, March 2003 Pages 360 - 372</journal-ref><doi>10.1016/S0743-7315(02)00054-0</doi><abstract>  2D convolution is a staple of digital image processing. The advent of large
format imagers makes it possible to literally ``pave'' with silicon the focal
plane of an optical sensor, which results in very large images that can require
a significant amount computation to process. Filtering of large images via 2D
convolutions is often complicated by a variety of effects (e.g.,
non-uniformities found in wide field of view instruments). This paper describes
a fast (FFT based) method for convolving images, which is also well suited to
very large images. A parallel version of the method is implemented using a
multi-threaded approach, which allows more efficient load balancing and a
simpler software architecture. The method has been implemented within in a high
level interpreted language (IDL), while also exploiting open standards vector
libraries (VSIPL) and open standards parallel directives (OpenMP). The parallel
approach and software architecture are generally applicable to a variety of
algorithms and has the advantage of enabling users to obtain the convenience of
an easy operating environment while also delivering high performance using a
fully portable code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0112092</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0112092</id><created>2001-12-04</created><authors><author><keyname>Dorband</keyname><forenames>E. N.</forenames><affiliation>Rutgers University</affiliation></author><author><keyname>Hemsendorf</keyname><forenames>Marc</forenames><affiliation>Rutgers University</affiliation></author><author><keyname>Merritt</keyname><forenames>David</forenames><affiliation>Rutgers University</affiliation></author></authors><title>Systolic and Hyper-Systolic Algorithms for the Gravitational N-Body
  Problem, with an Application to Brownian Motion</title><categories>astro-ph cs.DC physics.comp-ph</categories><comments>33 pages, 10 postscript figures</comments><report-no>Rutgers Astrophysics Preprint Series No. 327</report-no><journal-ref>J.Comput.Phys. 185 (2003) 484-511</journal-ref><doi>10.1016/S0021-9991(02)00067-0</doi><abstract>  A systolic algorithm rhythmically computes and passes data through a network
of processors. We investigate the performance of systolic algorithms for
implementing the gravitational N-body problem on distributed-memory computers.
Systolic algorithms minimize memory requirements by distributing the particles
between processors. We show that the performance of systolic routines can be
greatly enhanced by the use of non-blocking communication, which allows
particle coordinates to be communicated at the same time that force
calculations are being carried out. Hyper-systolic algorithms reduce the
communication complexity at the expense of increased memory demands. As an
example of an application requiring large N, we use the systolic algorithm to
carry out direct-summation simulations using 10^6 particles of the Brownian
motion of the supermassive black hole at the center of the Milky Way galaxy. We
predict a 3D random velocity of 0.4 km/s for the black hole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0305447</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0305447</id><created>2003-05-23</created><updated>2005-04-29</updated><authors><author><keyname>Zhang</keyname><forenames>Lucy Liuxuan</forenames><affiliation>CITA, University of Toronto</affiliation></author><author><keyname>Pen</keyname><forenames>Ue-Li</forenames><affiliation>CITA, University of Toronto</affiliation></author></authors><title>Fast n-point correlation functions and three-point lensing application</title><categories>astro-ph cs.CC cs.DS</categories><comments>37 pages, 6 figures, LaTeX; added and modified figures, modified
  theoretical estimate of computing time; accepted by New Astronomy</comments><report-no>CITA-2003-51</report-no><journal-ref>New Astron. 10 (2005) 569-590</journal-ref><doi>10.1016/j.newast.2005.04.002</doi><abstract>  We present a new algorithm to rapidly compute the two-point (2PCF),
three-point (3PCF) and n-point (n-PCF) correlation functions in roughly O(N log
N) time for N particles, instead of O(N^n) as required by brute force
approaches. The algorithm enables an estimate of the full 3PCF for as many as
10^6 galaxies. This technique exploits node-to-node correlations of a recursive
bisectional binary tree. A balanced tree construction minimizes the depth of
the tree and the worst case error at each node. The algorithm presented in this
paper can be applied to problems with arbitrary geometry.
  We describe the detailed implementation to compute the two point function and
all eight components of the 3PCF for a two-component field, with attention to
shear fields generated by gravitational lensing. We also generalize the
algorithm to compute the n-point correlation function for a scalar field in k
dimensions where n and k are arbitrary positive integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0402591</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0402591</id><created>2004-02-25</created><authors><author><keyname>Bailer-Jones</keyname><forenames>C. A. L.</forenames><affiliation>Carnegie Mellon University, Pittsburgh, PA; Max-Planck-Institut fuer Astronomie, Heidelberg</affiliation></author></authors><title>Evolutionary design of photometric systems and its application to Gaia</title><categories>astro-ph cs.NE stat.ML</categories><comments>Accepted by Astronomy &amp; Astrophysics</comments><journal-ref>Astron.Astrophys. 419 (2004) 385-403</journal-ref><doi>10.1051/0004-6361:20035779</doi><abstract>  Designing a photometric system to best fulfil a set of scientific goals is a
complex task, demanding a compromise between conflicting requirements and
subject to various constraints. A specific example is the determination of
stellar astrophysical parameters (APs) - effective temperature, metallicity
etc. - across a wide range of stellar types. I present a novel approach to this
problem which makes minimal assumptions about the required filter system. By
considering a filter system as a set of free parameters it may be designed by
optimizing some figure-of-merit (FoM) with respect to these parameters. In the
example considered, the FoM is a measure of how well the filter system can
`separate' stars with different APs. This separation is vectorial in nature, in
the sense that the local directions of AP variance are preferably mutually
orthogonal to avoid AP degeneracy. The optimization is carried out with an
evolutionary algorithm, which uses principles of evolutionary biology to search
the parameter space. This model, HFD (Heuristic Filter Design), is applied to
the design of photometric systems for the Gaia space astrometry mission. The
optimized systems show a number of interesting features, not least the
persistence of broad, overlapping filters. These HFD systems perform as least
as well as other proposed systems for Gaia, although inadequacies remain in
all. The principles underlying HFD are quite generic and may be applied to
filter design for numerous other projects, such as the search for specific
types of objects or photometric redshift determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0502164</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0502164</id><created>2005-02-08</created><authors><author><keyname>Skokos</keyname><forenames>Ch.</forenames></author><author><keyname>Parsopoulos</keyname><forenames>K. E.</forenames></author><author><keyname>Patsis</keyname><forenames>P. A.</forenames></author><author><keyname>Vrahatis</keyname><forenames>M. N.</forenames></author></authors><title>Particle Swarm Optimization: An efficient method for tracing periodic
  orbits in 3D galactic potentials</title><categories>astro-ph cs.NA cs.NE math.NA nlin.CD</categories><comments>12 pages, 8 figures, accepted for publication in MNRAS</comments><journal-ref>Mon.Not.Roy.Astron.Soc. 359 (2005) 251-260</journal-ref><doi>10.1111/j.1365-2966.2005.08892.x</doi><abstract>  We propose the Particle Swarm Optimization (PSO) as an alternative method for
locating periodic orbits in a three--dimensional (3D) model of barred galaxies.
We develop an appropriate scheme that transforms the problem of finding
periodic orbits into the problem of detecting global minimizers of a function,
which is defined on the Poincar\'{e} Surface of Section (PSS) of the
Hamiltonian system. By combining the PSO method with deflection techniques, we
succeeded in tracing systematically several periodic orbits of the system. The
method succeeded in tracing the initial conditions of periodic orbits in cases
where Newton iterative techniques had difficulties. In particular, we found
families of 2D and 3D periodic orbits associated with the inner 8:1 to 12:1
resonances, between the radial 4:1 and corotation resonances of our 3D Ferrers
bar model. The main advantages of the proposed algorithm is its simplicity, its
ability to work using function values solely, as well as its ability to locate
many periodic orbits per run at a given Jacobian constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0504006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0504006</id><created>2005-03-31</created><authors><author><keyname>Djorgovski</keyname><forenames>S. G.</forenames></author><author><keyname>Williams</keyname><forenames>R.</forenames></author></authors><title>Virtual Observatory: From Concept to Implementation</title><categories>astro-ph cs.CE</categories><comments>Invited review, to appear in proc. &quot;From Clark Lake to the Long
  Wavelength Array: Bill Erickson's Radio Science&quot;, eds. N. Kassim, M. Perez,
  W. Junor &amp; P. Henning, ASPCS vol. 3xx, in press (2005). Latex file, 14 pages,
  4 eps figures, all included</comments><abstract>  We review the origins of the Virtual Observatory (VO) concept, and the
current status of the efforts in this field. VO is the response of the
astronomical community to the challenges posed by the modern massive and
complex data sets. It is a framework in which information technology is
harnessed to organize, maintain, and explore the rich information content of
the exponentially growing data sets, and to enable a qualitatively new science
to be done with them. VO will become a complete, open, distributed, web-based
framework for astronomy of the early 21st century. A number of significant
efforts worldwide are now striving to convert this vision into reality. The
technological and methodological challenges posed by the information-rich
astronomy are also common to many other fields. We see a fundamental change in
the way all science is done, driven by the information technology revolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0506110</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0506110</id><created>2005-06-06</created><authors><author><keyname>Cirkovic</keyname><forenames>Milan M.</forenames></author><author><keyname>Bradbury</keyname><forenames>Robert J.</forenames></author></authors><title>Galactic Gradients, Postbiological Evolution and the Apparent Failure of
  SETI</title><categories>astro-ph cs.AI physics.soc-ph</categories><comments>30 pages, 2 figures</comments><journal-ref>New Astron. 11 (2006) 628-639</journal-ref><doi>10.1016/j.newast.2006.04.003</doi><abstract>  Motivated by recent developments impacting our view of Fermi's paradox
(absence of extraterrestrials and their manifestations from our past light
cone), we suggest a reassessment of the problem itself, as well as of
strategies employed by SETI projects so far. The need for such reevaluation is
fueled not only by the failure of searches thus far, but also by great advances
recently made in astrophysics, astrobiology, computer science and future
studies, which have remained largely ignored in SETI practice. As an example of
the new approach, we consider the effects of the observed metallicity and
temperature gradients in the Milky Way on the spatial distribution of
hypothetical advanced extraterrestrial intelligent communities. While,
obviously, properties of such communities and their sociological and
technological preferences are entirely unknown, we assume that (1) they operate
in agreement with the known laws of physics, and (2) that at some point they
typically become motivated by a meta-principle embodying the central role of
information-processing; a prototype of the latter is the recently suggested
Intelligence Principle of Steven J. Dick. There are specific conclusions of
practical interest to be drawn from coupling of these reasonable assumptions
with the astrophysical and astrochemical structure of the Galaxy. In
particular, we suggest that the outer regions of the Galactic disk are most
likely locations for advanced SETI targets, and that intelligent communities
will tend to migrate outward through the Galaxy as their capacities of
information-processing increase, for both thermodynamical and astrochemical
reasons. This can also be regarded as a possible generalization of the Galactic
Habitable Zone, concept currently much investigated in astrobiology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0506308</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0506308</id><created>2005-06-14</created><updated>2006-05-23</updated><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Hobson</keyname><forenames>M. P.</forenames></author><author><keyname>Mortlock</keyname><forenames>D. J.</forenames></author><author><keyname>Lasenby</keyname><forenames>A. N.</forenames></author></authors><title>Fast directional continuous spherical wavelet transform algorithms</title><categories>astro-ph cs.IT math.IT</categories><comments>10 pages, 3 figures, replaced to match version accepted by IEEE
  Trans. Sig. Proc</comments><journal-ref>IEEE Trans.Signal Process. 55 (2007) 520-529</journal-ref><doi>10.1109/TSP.2006.887148</doi><abstract>  We describe the construction of a spherical wavelet analysis through the
inverse stereographic projection of the Euclidean planar wavelet framework,
introduced originally by Antoine and Vandergheynst and developed further by
Wiaux et al. Fast algorithms for performing the directional continuous wavelet
analysis on the unit sphere are presented. The fast directional algorithm,
based on the fast spherical convolution algorithm developed by Wandelt and
Gorski, provides a saving of O(sqrt(Npix)) over a direct quadrature
implementation for Npix pixels on the sphere, and allows one to perform a
directional spherical wavelet analysis of a 10^6 pixel map on a personal
computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0510041</identifier>
 <datestamp>2009-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0510041</id><created>2005-10-03</created><authors><author><keyname>Basden</keyname><forenames>A. G.</forenames></author><author><keyname>Assemat</keyname><forenames>F.</forenames></author><author><keyname>Butterley</keyname><forenames>T.</forenames></author><author><keyname>Geng</keyname><forenames>D.</forenames></author><author><keyname>Saunter</keyname><forenames>C. D.</forenames></author><author><keyname>Wilson</keyname><forenames>R. W.</forenames></author></authors><title>Acceleration of adaptive optics simulations using programmable logic</title><categories>astro-ph cs.DC</categories><comments>6 pages accepted by MNRAS</comments><journal-ref>Mon.Not.Roy.Astron.Soc.364:1413-1418,2005</journal-ref><doi>10.1111/j.1365-2966.2005.09670.x</doi><abstract>  Numerical Simulation is an essential part of the design and optimisation of
astronomical adaptive optics systems. Simulations of adaptive optics are
computationally expensive and the problem scales rapidly with telescope
aperture size, as the required spatial order of the correcting system
increases. Practical realistic simulations of AO systems for extremely large
telescopes are beyond the capabilities of all but the largest of modern
parallel supercomputers. Here we describe a more cost effective approach
through the use of hardware acceleration using field programmable gate arrays.
By transferring key parts of the simulation into programmable logic, large
increases in computational bandwidth can be expected. We show that the
calculation of wavefront sensor image centroids can be accelerated by a factor
of four by transferring the algorithm into hardware. Implementing more
demanding parts of the adaptive optics simulation in hardware will lead to much
greater performance improvements, of up to 1000 times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0510688</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0510688</id><created>2005-10-24</created><authors><author><keyname>Noble</keyname><forenames>M. S.</forenames></author><author><keyname>Houck</keyname><forenames>J. C.</forenames></author><author><keyname>Davis</keyname><forenames>J. E.</forenames></author><author><keyname>Young</keyname><forenames>A.</forenames></author><author><keyname>Nowak</keyname><forenames>M.</forenames></author></authors><title>Using the Parallel Virtual Machine for Everyday Analysis</title><categories>astro-ph cs.DC</categories><comments>4 pages; manuscript for oral presentation given at ADASS XV, Madrid</comments><abstract>  A review of the literature reveals that while parallel computing is sometimes
employed by astronomers for custom, large-scale calculations, no package
fosters the routine application of parallel methods to standard problems in
astronomical data analysis. This paper describes our attempt to close that gap
by wrapping the Parallel Virtual Machine (PVM) as a scriptable S-Lang module.
Using PVM within ISIS, the Interactive Spectral Interpretation System, we've
distributed a number of representive calculations over a network of 25+ CPUs to
achieve dramatic reductions in execution times. We discuss how the approach
applies to a wide class of modeling problems, outline our efforts to make it
more transparent for common use, and note its growing importance in the context
of the large, multi-wavelength datasets used in modern analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0605042</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0605042</id><created>2006-05-01</created><authors><author><keyname>Cuevas-Tello</keyname><forenames>Juan C.</forenames><affiliation>School of Computer Science, University of Birmingham, UK;</affiliation><affiliation>University of San Luis Potosi, Mexico</affiliation></author><author><keyname>Tino</keyname><forenames>Peter</forenames><affiliation>School of Computer Science, University of Birmingham, UK;</affiliation></author><author><keyname>Raychaudhury</keyname><forenames>Somak</forenames><affiliation>School of Physics &amp; Astronomy, University of Birmingham, UK;</affiliation></author></authors><title>How accurate are the time delay estimates in gravitational lensing?</title><categories>astro-ph cs.LG</categories><comments>14 pages, 12 figures; accepted for publication in Astronomy &amp;
  Astrophysics</comments><journal-ref>Astron.Astrophys. 454 (2006) 695-706</journal-ref><doi>10.1051/0004-6361:20054652</doi><abstract>  We present a novel approach to estimate the time delay between light curves
of multiple images in a gravitationally lensed system, based on Kernel methods
in the context of machine learning. We perform various experiments with
artificially generated irregularly-sampled data sets to study the effect of the
various levels of noise and the presence of gaps of various size in the
monitoring data. We compare the performance of our method with various other
popular methods of estimating the time delay and conclude, from experiments
with artificial data, that our method is least vulnerable to missing data and
irregular sampling, within reasonable bounds of Gaussian noise. Thereafter, we
use our method to determine the time delays between the two images of quasar
Q0957+561 from radio monitoring data at 4 cm and 6 cm, and conclude that if
only the observations at epochs common to both wavelengths are used, the time
delay gives consistent estimates, which can be combined to yield 408\pm 12
days. The full 6 cm dataset, which covers a longer monitoring period, yields a
value which is 10% larger, but this can be attributed to differences in
sampling and missing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0605514</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0605514</id><created>2006-05-20</created><updated>2006-11-06</updated><authors><author><keyname>Bratek</keyname><forenames>Lukasz</forenames></author><author><keyname>Kolonko</keyname><forenames>Marcin</forenames></author></authors><title>An algorithm for solving the pulsar equation</title><categories>astro-ph cs.NA</categories><comments>4 pages, 4 figures, accepted for publication in ApSS (a shortened
  version of the previous one)</comments><journal-ref>Astrophys.SpaceSci.309:231-234,2007</journal-ref><doi>10.1007/s10509-007-9406-y</doi><abstract>  We present an algorithm of finding numerical solutions of pulsar equation.
The problem of finding the solutions was reduced to finding expansion
coefficients of the source term of the equation in a base of orthogo- nal
functions defined on the unit interval by minimizing a multi-variable mismatch
function defined on the light cylinder. We applied the algorithm to Scharlemann
&amp; Wagoner boundary conditions by which a smooth solu- tion is reconstructed
that by construction passes success- fully the Gruzinov's test of the source
function exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0609159</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0609159</id><created>2006-09-06</created><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Hobson</keyname><forenames>M. P.</forenames></author><author><keyname>Lasenby</keyname><forenames>A. N.</forenames></author></authors><title>A directional continuous wavelet transform on the sphere</title><categories>astro-ph cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><abstract>  A new construction of a directional continuous wavelet analysis on the sphere
is derived herein. We adopt the harmonic scaling idea for the spherical
dilation operator recently proposed by Sanz et al. but extend the analysis to a
more general directional framework. Directional wavelets are a powerful
extension that allow one to also probe oriented structure in the analysed
function. Our spherical wavelet methodology has the advantage that all
functions and operators are defined directly on the sphere. The construction of
wavelets in our framework is demonstrated with an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0609794</identifier>
 <datestamp>2011-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0609794</id><created>2006-09-28</created><authors><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Eichhorn</keyname><forenames>Guenther</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin</forenames></author><author><keyname>Thompson</keyname><forenames>Donna</forenames></author><author><keyname>Bohlen</keyname><forenames>Elizabeth</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author></authors><title>The Future of Technical Libraries</title><categories>astro-ph cs.DL</categories><comments>To appear in Library and Information Systems in Astronomy V</comments><abstract>  Technical libraries are currently experiencing very rapid change. In the near
future their mission will change, their physical nature will change, and the
skills of their employees will change. While some will not be able to make
these changes, and will fail, others will lead us into a new era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0612688</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0612688</id><created>2006-12-22</created><updated>2008-07-28</updated><authors><author><keyname>McEwen</keyname><forenames>J. D.</forenames></author><author><keyname>Hobson</keyname><forenames>M. P.</forenames></author><author><keyname>Lasenby</keyname><forenames>A. N.</forenames></author></authors><title>Optimal filters on the sphere</title><categories>astro-ph cs.IT math.IT</categories><comments>10 pages, 5 figures, replaced to match version accepted by IEEE Sig.
  Proc</comments><journal-ref>IEEETrans.SignalProcess.56:3813-3823,2008</journal-ref><doi>10.1109/TSP.2008.923198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive optimal filters on the sphere in the context of detecting compact
objects embedded in a stochastic background process. The matched filter and the
scale adaptive filter are derived on the sphere in the most general setting,
allowing for directional template profiles and filters. The performance and
relative merits of the two optimal filters are discussed. The application of
optimal filter theory on the sphere to the detection of compact objects is
demonstrated on simulated mock data. A naive detection strategy is adopted,
with an initial aim of illustrating the application of the new optimal filters
derived on the sphere. Nevertheless, this simple object detection strategy is
demonstrated to perform well, even a low signal-to-noise ratio. Code written to
compute optimal filters on the sphere (S2FIL), to perform fast directional
filtering on the sphere (FastCSWT) and to construct the simulated mock data
(COMB) are all made publicly available from http://www.mrao.cam.ac.uk/~jdm57/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0703485</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/0703485</id><created>2007-03-19</created><authors><author><keyname>Hoekstra</keyname><forenames>A. G.</forenames></author><author><keyname>Zwart</keyname><forenames>S. F. Portegies</forenames></author><author><keyname>Bubak</keyname><forenames>M.</forenames></author><author><keyname>Sloot</keyname><forenames>P. M. A.</forenames></author></authors><title>Towards Distributed Petascale Computing</title><categories>astro-ph cs.DC</categories><comments>To appear in D. Bader (Ed.) Petascale, Computing: Algorithms and
  Applications, Chapman &amp; Hall / CRC Press, Taylor and Francis Group</comments><abstract>  In this chapter we will argue that studying such multi-scale multi-science
systems gives rise to inherently hybrid models containing many different
algorithms best serviced by different types of computing environments (ranging
from massively parallel computers, via large-scale special purpose machines to
clusters of PC's) whose total integrated computing capacity can easily reach
the PFlop/s scale. Such hybrid models, in combination with the by now
inherently distributed nature of the data on which the models `feed' suggest a
distributed computing model, where parts of the multi-scale multi-science model
are executed on the most suitable computing environment, and/or where the
computations are carried out close to the required data (i.e. bring the
computations to the data instead of the other way around). We presents an
estimate for the compute requirements to simulate the Galaxy as a typical
example of a multi-scale multi-physics application, requiring distributed
Petaflop/s computational power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/9912134</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>astro-ph/9912134</id><created>1999-12-07</created><authors><author><keyname>Kepner</keyname><forenames>Jeremy</forenames><affiliation>Princeton/MIT LL</affiliation></author><author><keyname>Gokhale</keyname><forenames>Maya</forenames><affiliation>Sarnoff/LANL</affiliation></author><author><keyname>Minnich</keyname><forenames>Ron</forenames><affiliation>Sarnoff/LANL</affiliation></author><author><keyname>Marks</keyname><forenames>Aaron</forenames><affiliation>Sarnoff</affiliation></author><author><keyname>DeGood</keyname><forenames>John</forenames><affiliation>Sarnoff</affiliation></author></authors><title>Interfacing Interpreted and Compiled Languages to Support Applications
  on a Massively Parallel Network of Workstations (MP-NOW)</title><categories>astro-ph cs.DC</categories><comments>To appear in Cluster Computing, 22 pages including 8 color figures</comments><journal-ref>Cluster Computing 07-2000, Volume 3, Issue 1, pp 35-44</journal-ref><doi>10.1023/A:1019011716367</doi><abstract>  Astronomers are increasingly using Massively Parallel Network of Workstations
(MP-NOW) to address their most challenging computing problems. Fully exploiting
these systems is made more difficult as more and more modeling and data
analysis software is written in interpreted languages (such as IDL, MATLAB, and
Mathematica) which do not lend themselves to parallel computing. We present a
specific example of a very simple, but generic solution to this problem. Our
example uses an interpreted language (IDL) to set up a calculation and then
interfaces with a computational kernel written in a compiled language (C). The
IDL code then calls the C code as an external library. We have added to the
computational kernel an additional layer, which manages multiple copies of the
kernel running on a MP-NOW and returns the results back to the interpreted
layer. Our implementation uses The Next generation Taskbag (TNT) library
developed at Sarnoff to provide an efficient means for implementing task
parallelism. A test problem (taken from Astronomy) has been implemented on the
Sarnoff Cyclone computer which consists of 160 heterogeneous nodes connected by
a ``fat'' tree 100 Mb/s switched Ethernet running the RedHat Linux and FreeBSD
operating systems. Our first results in this ongoing project have demonstrated
the feasibility of this approach and produced speedups of greater than 50 on 60
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:chao-dyn/9905036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>chao-dyn/9905036</id><created>1999-05-21</created><authors><author><keyname>Maurer</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>Restart Strategies and Internet Congestion</title><categories>chao-dyn adap-org cs.NI nlin.AO nlin.CD</categories><comments>15 pages, 8 figures</comments><abstract>  We recently presented a methodology for quantitatively reducing the risk and
cost of executing electronic transactions in a bursty network environment such
as the Internet. In the language of portfolio theory, time to complete a
transaction and its variance replace the expected return and risk associated
with a security, whereas restart times replace combinations of securities.
While such a strategy works well with single users, the question remains as to
its usefulness when used by many. By using mean field arguments and agent-based
simulations, we determine that a restart strategy remains advantageous even if
everybody uses it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:chao-dyn/9909031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>chao-dyn/9909031</id><created>1999-09-21</created><updated>1999-10-05</updated><authors><author><keyname>Segre</keyname><forenames>Gavriel</forenames></author></authors><title>Noncommutative Martin-Lof randomness : on the concept of a random
  sequence of qubits</title><categories>chao-dyn adap-org cs.CC math-ph math.MP nlin.AO nlin.CD quant-ph</categories><comments>revised version</comments><abstract>  Martin-Lof's definition of random sequences of cbits as those not belonging
to any set of constructive zero Lebesgue measure is reformulated in the
language of Algebraic Probability Theory.
  The adoption of the Pour-El Richards theory of computability structures on
Banach spaces allows us to give a natural noncommutative extension of
Martin-Lof's definition, characterizing the random elements of a chain Von
Neumann algebra.
  In the particular case of the minimally informative noncommutative alphabet
our definition reduces to the definition of a random sequence of qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404001</id><created>1994-04-03</created><authors><author><keyname>Schabes</keyname><forenames>Yves</forenames></author><author><keyname>Shieber</keyname><forenames>Stuart M.</forenames></author></authors><title>An Alternative Conception of Tree-Adjoining Derivation</title><categories>cmp-lg cs.CL</categories><comments>33 pages</comments><report-no>CRCT TR-08-92</report-no><journal-ref>Computational Linguistics 20(1):91-124</journal-ref><abstract>  The precise formulation of derivation for tree-adjoining grammars has
important ramifications for a wide variety of uses of the formalism, from
syntactic analysis to semantic interpretation and statistical language
modeling. We argue that the definition of tree-adjoining derivation must be
reformulated in order to manifest the proper linguistic dependencies in
derivations. The particular proposal is both precisely characterizable through
a definition of TAG derivations as equivalence classes of ordered derivation
trees, and computationally operational, by virtue of a compilation to linear
indexed grammars together with an efficient algorithm for recognition and
parsing according to the compiled grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404002</id><created>1994-04-03</created><authors><author><keyname>Shieber</keyname><forenames>Stuart M.</forenames></author></authors><title>Lessons from a Restricted Turing Test</title><categories>cmp-lg cs.CL</categories><comments>20 pages</comments><report-no>CRCT TR-19-92</report-no><abstract>  We report on the recent Loebner prize competition inspired by Turing's test
of intelligent behavior. The presentation covers the structure of the
competition and the outcome of its first instantiation in an actual event, and
an analysis of the purpose, design, and appropriateness of such a competition.
We argue that the competition has no clear purpose, that its design prevents
any useful outcome, and that such a competition is inappropriate given the
current level of technology. We then speculate as to suitable alternatives to
the Loebner prize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404003</id><created>1994-04-03</created><updated>1994-08-30</updated><authors><author><keyname>Shieber</keyname><forenames>Stuart M.</forenames><affiliation>Harvard University</affiliation></author></authors><title>Restricting the Weak-Generative Capacity of Synchronous Tree-Adjoining
  Grammars</title><categories>cmp-lg cs.CL</categories><comments>21 pages, uses lingmacros.sty, psfig.sty, fullname.sty; minor
  typographical changes only</comments><journal-ref>Computational Intelligence 10(4):371-385, November 1994</journal-ref><abstract>  The formalism of synchronous tree-adjoining grammars, a variant of standard
tree-adjoining grammars (TAG), was intended to allow the use of TAGs for
language transduction in addition to language specification. In previous work,
the definition of the transduction relation defined by a synchronous TAG was
given by appeal to an iterative rewriting process. The rewriting definition of
derivation is problematic in that it greatly extends the expressivity of the
formalism and makes the design of parsing algorithms difficult if not
impossible. We introduce a simple, natural definition of synchronous
tree-adjoining derivation, based on isomorphisms between standard
tree-adjoining derivations, that avoids the expressivity and implementability
problems of the original rewriting definition. The decrease in expressivity,
which would otherwise make the method unusable, is offset by the incorporation
of an alternative definition of standard tree-adjoining derivation, previously
proposed for completely separate reasons, thereby making it practical to
entertain using the natural definition of synchronous derivation. Nonetheless,
some remaining problematic cases call for yet more flexibility in the
definition; the isomorphism requirement may have to be relaxed. It remains for
future research to tune the exact requirements on the allowable mappings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404004</id><created>1994-04-06</created><authors><author><keyname>Covington</keyname><forenames>Michael A.</forenames><affiliation>University of Georgia</affiliation></author></authors><title>An Empirically Motivated Reinterpretation of Dependency Grammar</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX (PostScript version available by ftp from ai.uga.edu)</comments><report-no>AI-1994-01 (Artificial Intelligence Programs, U. of Georgia)</report-no><abstract>  Dependency grammar is usually interpreted as equivalent to a strict form of
X--bar theory that forbids the stacking of nodes of the same bar level (e.g.,
N' immediately dominating N' with the same head). But adequate accounts of
_one_--anaphora and of the semantics of multiple modifiers require such
stacking and accordingly argue against dependency grammar. Dependency grammar
can be salvaged by reinterpreting its claims about phrase structure, so that
modifiers map onto binary--branching X--bar trees rather than ``flat'' ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404005</id><created>1994-04-11</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Memoization in Constraint Logic Programming</title><categories>cmp-lg cs.CL</categories><comments>11 pages</comments><abstract>  This paper shows how to apply memoization (caching of subgoals and associated
answer substitutions) in a constraint logic programming setting. The research
is is motivated by the desire to apply constraint logic programming (CLP) to
problems in natural language processing that involve (constraint) interleaving
or coroutining, such as GB and HPSG parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404006</id><created>1994-04-15</created><authors><author><keyname>CHANDLER-BURNS</keyname><forenames>R. M.</forenames><affiliation>Medical College, Autonomous University of Nuevo Leon, Monterrey, Mexico</affiliation></author></authors><title>SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for
  medical purposes</title><categories>cmp-lg cs.CL</categories><comments>20 pages</comments><report-no>RMCB150494</report-no><abstract>  S92 research was begun in 1987 to analyze word frequencies in present-day
Spanish for making speech pathology evaluation tools. 500 2,000-word samples of
children, adolescents and adults' language were input between 1988-1991,
calculations done in 1992; statistical and Lewandowski analyses were carried
out in 1993.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404007</id><created>1994-04-19</created><authors><author><keyname>Bouma</keyname><forenames>Gosse</forenames><affiliation>Alfa-Informatica, Rijksuniversiteit Groningen</affiliation></author><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>Alfa-Informatica, Rijksuniversiteit Groningen</affiliation></author></authors><title>Constraint-Based Categorial Grammar</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX</comments><journal-ref>Proceedings ACL 94</journal-ref><abstract>  We propose a generalization of Categorial Grammar in which lexical categories
are defined by means of recursive constraints. In particular, the introduction
of relational constraints allows one to capture the effects of (recursive)
lexical rules in a computationally attractive manner. We illustrate the
linguistic merits of the new approach by showing how it accounts for the syntax
of Dutch cross-serial dependencies and the position and scope of adjuncts in
such constructions. Delayed evaluation is used to process grammars containing
recursive constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404008</id><created>1994-04-26</created><authors><author><keyname>Shieber</keyname><forenames>Stuart M.</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Schabes</keyname><forenames>Yves</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando C. N.</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author></authors><title>Principles and Implementation of Deductive Parsing</title><categories>cmp-lg cs.CL</categories><comments>69 pages, includes full Prolog code</comments><report-no>CRCT TR-11-94 (Computer Science Department, Harvard University)</report-no><abstract>  We present a system for generating parsers based directly on the metaphor of
parsing as deduction. Parsing algorithms can be represented directly as
deduction systems, and a single deduction engine can interpret such deduction
systems so as to implement the corresponding parser. The method generalizes
easily to parsers for augmented phrase structure formalisms, such as
definite-clause grammars and other logic grammar formalisms, and has been used
for rapid prototyping of parsing algorithms for a variety of formalisms
including variants of tree-adjoining grammars, categorial grammars, and
lexicalized context-free grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404009</id><created>1994-04-27</created><updated>1994-05-27</updated><authors><author><keyname>Dalrymple</keyname><forenames>Mary</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author><author><keyname>Lamping</keyname><forenames>John</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill NJ</affiliation></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author></authors><title>A Deductive Account of Quantification in LFG</title><categories>cmp-lg cs.CL</categories><comments>27 pages, extensively revised</comments><report-no>ISTL-NLTT-1993-06-01</report-no><abstract>  The relationship between Lexical-Functional Grammar (LFG) functional
structures (f-structures) for sentences and their semantic interpretations can
be expressed directly in a fragment of linear logic in a way that explains
correctly the constrained interactions between quantifier scope ambiguity and
bound anaphora. The use of a deductive framework to account for the
compositional properties of quantifying expressions in natural language
obviates the need for additional mechanisms, such as Cooper storage, to
represent the different scopes that a quantifier might take. Instead, the
semantic contribution of a quantifier is recorded as an ordinary logical
formula, one whose use in a proof will establish the scope of the quantifier.
The properties of linear logic ensure that each quantifier is scoped exactly
once. Our analysis of quantifier scope can be seen as a recasting of Pereira's
analysis (Pereira, 1991), which was expressed in higher-order intuitionistic
logic. But our use of LFG and linear logic provides a much more direct and
computationally more flexible interpretation mechanism for at least the same
range of phenomena. We have developed a preliminary Prolog implementation of
the linear deductions described in this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404010</id><created>1994-04-27</created><updated>1994-08-22</updated><authors><author><keyname>Dalrymple</keyname><forenames>Mary</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author><author><keyname>Lamping</keyname><forenames>John</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill NJ</affiliation></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames><affiliation>Xerox PARC, Palo Alto CA</affiliation></author></authors><title>Intensional Verbs Without Type-Raising or Lexical Ambiguity</title><categories>cmp-lg cs.CL</categories><comments>16 pages, revised and extended, to appear in the proceedings of the
  Conference on Information-Oriented Approaches to Logic, Language and
  Computation</comments><report-no>ISTL-NLTT-1994-02-01</report-no><abstract>  We present an analysis of the semantic interpretation of intensional verbs
such as seek that allows them to take direct objects of either individual or
quantifier type, producing both de dicto and de re readings in the quantifier
case, all without needing to stipulate type-raising or quantifying-in rules.
This simple account follows directly from our use of logical deduction in
linear logic to express the relationship between syntactic structures and
meanings. While our analysis resembles current categorial approaches in
important ways, it differs from them in allowing the greater type flexibility
of categorial semantics while maintaining a precise connection to syntax. As a
result, we are able to provide derivations for certain readings of sentences
with intensional verbs and complex direct objects that are not derivable in
current purely categorial accounts of the syntax-semantics interface. The
analysis forms a part of our ongoing work on semantic interpretation within the
framework of Lexical-Functional Grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9404011</id><created>1994-04-28</created><updated>1994-05-01</updated><authors><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>BCN RUG Groningen</affiliation></author><author><keyname>Bouma</keyname><forenames>Gosse</forenames><affiliation>BCN RUG Groningen</affiliation></author></authors><title>Adjuncts and the Processing of Lexical Rules</title><categories>cmp-lg cs.CL</categories><comments>8 pages (a4wide), to be published in Coling-94</comments><journal-ref>Proceedings of Coling 1994 Kyoto</journal-ref><abstract>  The standard HPSG analysis of Germanic verb clusters can not explain the
observed narrow-scope readings of adjuncts in such verb clusters. We present an
extension of the HPSG analysis that accounts for the systematic ambiguity of
the scope of adjuncts in verb cluster constructions, by treating adjuncts as
members of the subcat list. The extension uses powerful recursive lexical
rules, implemented as complex constraints. We show how `delayed evaluation'
techniques from constraint-logic programming can be used to process such
lexical rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405001</id><created>1994-05-02</created><authors><author><keyname>Dagan</keyname><forenames>Ido</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill, NJ 07974, USA</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill, NJ 07974, USA</affiliation></author><author><keyname>Lee</keyname><forenames>Lillian</forenames><affiliation>DAS, Harvard University, Cambridge MA 02138, USA</affiliation></author></authors><title>Similarity-Based Estimation of Word Cooccurrence Probabilities</title><categories>cmp-lg cs.CL</categories><comments>13 pages, to appear in proceedings of ACL-94</comments><abstract>  In many applications of natural language processing it is necessary to
determine the likelihood of a given word combination. For example, a speech
recognizer may need to determine which of the two word combinations ``eat a
peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine
the likelihood of a word combination according to its frequency in a training
corpus. However, the nature of language is such that many word combinations are
infrequent and do not occur in a given corpus. In this work we propose a method
for estimating the probability of such previously unseen word combinations
using available information on ``most similar'' words. We describe a
probabilistic word association model based on distributional word similarity,
and apply it to improving probability estimates for unseen word bigrams in a
variant of Katz's back-off model. The similarity-based method yields a 20%
perplexity improvement in the prediction of unseen bigrams and statistically
significant reductions in speech-recognition error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405002</id><created>1994-05-02</created><authors><author><keyname>Kehler</keyname><forenames>Andrew</forenames><affiliation>Harvard University</affiliation></author></authors><title>Temporal Relations: Reference or Discourse Coherence?</title><categories>cmp-lg cs.CL</categories><comments>To appear in the Proceedings of ACL-94, Student Session. 5 pages,
  LaTeX source, requires lingmacros. Comments are welcome</comments><journal-ref>ACL-94 (Student Session), Las Cruces, New Mexico</journal-ref><abstract>  The temporal relations that hold between events described by successive
utterances are often left implicit or underspecified. We address the role of
two phenomena with respect to the recovery of these relations: (1) the
referential properties of tense, and (2) the role of temporal constraints
imposed by coherence relations. We account for several facets of the
identification of temporal relations through an integration of these.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405003</id><created>1994-05-02</created><authors><author><keyname>Hirschberg</keyname><forenames>Julia</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author></authors><title>Some Bibliographical References on Intonation and Intonational Meaning</title><categories>cmp-lg cs.CL</categories><comments>14 pp of text and citations, bibtex added as separate file</comments><abstract>  A by-no-means-complete collection of references for those interested in
intonational meaning, with other miscellaneous references on intonation
included. Additional references are welcome, and should be sent to
julia@research.att.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405004</id><created>1994-05-03</created><authors><author><keyname>Koenig</keyname><forenames>Esther</forenames><affiliation>Institute for Computational Linguistics, Stuttgart University</affiliation></author></authors><title>Syntactic-Head-Driven Generation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, to appear in Proceedings of COLING 94</comments><abstract>  The previously proposed semantic-head-driven generation methods run into
problems if none of the daughter constituents in the syntacto-semantic rule
schemata of a grammar fits the definition of a semantic head given in Shieber
et al. 1990. This is the case for the semantic analysis rules of certain
constraint-based semantic representations, e.g. Underspecified Discourse
Representation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven
generation in general has its merits, we simply return to a syntactic
definition of `head' and demonstrate the feasibility of syntactic-head-driven
generation. In addition to its generality, a syntactic-head-driven algorithm
provides a basis for a logically well-defined treatment of the movement of
(syntactic) heads, for which only ad-hoc solutions existed, so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405005</id><created>1994-05-03</created><authors><author><keyname>Magerman</keyname><forenames>David M.</forenames></author><author><keyname>Marcus</keyname><forenames>Mitchell P.</forenames></author></authors><title>Pearl: A Probabilistic Chart Parser</title><categories>cmp-lg cs.CL</categories><comments>7 pages</comments><journal-ref>Proceedings, 2nd International Workshop for Parsing Technologies</journal-ref><abstract>  This paper describes a natural language parsing algorithm for unrestricted
text which uses a probability-based scoring function to select the &quot;best&quot; parse
of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser
with Earley-type top-down prediction which pursues the highest-scoring theory
in the chart, where the score of a theory represents the extent to which the
context of the sentence predicts that interpretation. This parser differs from
previous attempts at stochastic parsers in that it uses a richer form of
conditional probabilities based on context to predict likelihood. Pearl also
provides a framework for incorporating the results of previous work in
part-of-speech assignment, unknown word models, and other probabilistic models
of linguistic features into one parsing tool, interleaving these techniques
instead of using the traditional pipeline architecture. In preliminary tests,
Pearl has been successful at resolving part-of-speech and word (in speech
processing) ambiguity, determining categories for unknown words, and selecting
correct parses first using a very loosely fitting covering grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405006</id><created>1994-05-03</created><authors><author><keyname>Magerman</keyname><forenames>David M.</forenames></author><author><keyname>Weir</keyname><forenames>Carl</forenames></author></authors><title>Efficiency, Robustness, and Accuracy in Picky Chart Parsing</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>Proceedings, ACL 1992</journal-ref><abstract>  This paper describes Picky, a probabilistic agenda-based chart parsing
algorithm which uses a technique called {\em probabilistic prediction} to
predict which grammar rules are likely to lead to an acceptable parse of the
input. Using a suboptimal search method, Picky significantly reduces the number
of edges produced by CKY-like chart parsing algorithms, while maintaining the
robustness of pure bottom-up parsers and the accuracy of existing probabilistic
parsers. Experiments using Picky demonstrate how probabilistic modelling can
impact upon the efficiency, robustness and accuracy of a parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405007</id><created>1994-05-03</created><authors><author><keyname>Black</keyname><forenames>Ezra</forenames></author><author><keyname>Jelinek</keyname><forenames>Fred</forenames></author><author><keyname>Lafferty</keyname><forenames>John</forenames></author><author><keyname>Magerman</keyname><forenames>David M.</forenames></author><author><keyname>Mercer</keyname><forenames>Robert</forenames></author><author><keyname>Roukos</keyname><forenames>Salim</forenames></author></authors><title>Towards History-based Grammars: Using Richer Models for Probabilistic
  Parsing</title><categories>cmp-lg cs.CL</categories><comments>6 pages</comments><journal-ref>Proceedings, DARPA Speech and Natural Language Workshop, 1992</journal-ref><abstract>  We describe a generative probabilistic model of natural language, which we
call HBG, that takes advantage of detailed linguistic information to resolve
ambiguity. HBG incorporates lexical, syntactic, semantic, and structural
information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Treebank, in combination with
decision tree building to tease out the relevant aspects of a parse tree that
will determine the correct parse of a sentence. This stands in contrast to the
usual approach of further grammar tailoring via the usual linguistic
introspection in the hope of generating the correct parse. In head-to-head
tests against one of the best existing robust probabilistic parsing models,
which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing
the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405008</id><created>1994-05-03</created><updated>1994-05-05</updated><authors><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Shih</keyname><forenames>Chilin</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Gale</keyname><forenames>William</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Chang</keyname><forenames>Nancy</forenames><affiliation>Harvard University</affiliation></author></authors><title>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of ACL-94</comments><journal-ref>in Proceedings of ACL 94</journal-ref><abstract>  We present a stochastic finite-state model for segmenting Chinese text into
dictionary entries and productively derived words, and providing pronunciations
for these words; the method incorporates a class-based model in its treatment
of personal names. We also evaluate the system's performance, taking into
account the fact that people often do not agree on a single segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405009</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405009</id><created>1994-05-03</created><updated>1994-05-04</updated><authors><author><keyname>Magerman</keyname><forenames>David M.</forenames></author></authors><title>Natural Language Parsing as Statistical Pattern Recognition</title><categories>cmp-lg cs.CL</categories><comments>160+ pages, doctoral dissertation (latex with figures)</comments><abstract>  Traditional natural language parsers are based on rewrite rule systems
developed in an arduous, time-consuming manner by grammarians.  A majority
of the grammarian's efforts are devoted to the disambiguation process,
first hypothesizing rules which dictate constituent categories and
relationships among words in ambiguous sentences, and then seeking
exceptions and corrections to these rules.
  In this work, I propose an automatic method for acquiring a statistical
parser from a set of parsed sentences which takes advantage of some initial
linguistic input, but avoids the pitfalls of the iterative and seemingly
endless grammar development process.  Based on distributionally-derived and
linguistically-based features of language, this parser acquires a set of
statistical decision trees which assign a probability distribution on the
space of parse trees given the input sentence.  These decision trees take
advantage of significant amount of contextual information, potentially
including all of the lexical information in the sentence, to produce highly
accurate statistical models of the disambiguation process.  By basing the
disambiguation criteria selection on entropy reduction rather than human
intuition, this parser development method is able to consider more sentences
than a human grammarian can when making individual disambiguation rules.
  In experiments between a parser, acquired using this statistical
framework, and a grammarian's rule-based parser, developed over a ten-year
period, both using the same training material and test sentences, the
decision tree parser significantly outperformed the grammar-based parser on
the accuracy measure which the grammarian was trying to maximize, achieving
an accuracy of 78% compared to the grammar-based parser's 69%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405010</id><created>1994-05-03</created><authors><author><keyname>Kehler</keyname><forenames>Andrew</forenames><affiliation>Harvard University</affiliation></author></authors><title>Common Topics and Coherent Situations: Interpreting Ellipsis in the
  Context of Discourse Inference</title><categories>cmp-lg cs.CL</categories><comments>To be presented at ACL-94. 13 pages, LaTeX source, accompanying
  PostScript figures, requires psfig and lingmacros. Comments are welcome</comments><journal-ref>ACL-94, Las Cruces, New Mexico</journal-ref><abstract>  It is claimed that a variety of facts concerning ellipsis, event reference,
and interclausal coherence can be explained by two features of the linguistic
form in question: (1) whether the form leaves behind an empty constituent in
the syntax, and (2) whether the form is anaphoric in the semantics. It is
proposed that these features interact with one of two types of discourse
inference, namely {\it Common Topic} inference and {\it Coherent Situation}
inference. The differing ways in which these types of inference utilize
syntactic and semantic representations predicts phenomena for which it is
otherwise difficult to account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405011</id><created>1994-05-05</created><updated>1994-05-06</updated><authors><author><keyname>Chu-Carroll</keyname><forenames>Jennifer</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>University of Delaware</affiliation></author></authors><title>A Plan-Based Model for Response Generation in Collaborative
  Task-Oriented Dialogues</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in the Proceedings of AAAI-94. LaTeX source file,
  requires aaai.sty and epsf.tex. Figures included in separate files</comments><abstract>  This paper presents a plan-based architecture for response generation in
collaborative consultation dialogues, with emphasis on cases in which the
system (consultant) and user (executing agent) disagree. Our work contributes
to an overall system for collaborative problem-solving by providing a
plan-based framework that captures the {\em Propose-Evaluate-Modify} cycle of
collaboration, and by allowing the system to initiate subdialogues to negotiate
proposed additions to the shared plan and to provide support for its claims. In
addition, our system handles in a unified manner the negotiation of proposed
domain actions, proposed problem-solving actions, and beliefs proposed by
discourse actions. Furthermore, it captures cooperative responses within the
collaborative framework and accounts for why questions are sometimes never
answered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405012</id><created>1994-05-06</created><authors><author><keyname>Hong</keyname><forenames>Tao</forenames><affiliation>Center of Excellence for Document Analysis and Recognition, Department of Computer Science, State University of New York at Buffalo</affiliation></author></authors><title>Integration Of Visual Inter-word Constraints And Linguistic Knowledge In
  Degraded Text Recognition</title><categories>cmp-lg cs.CL</categories><comments>3 pages, PostScript File, to appear in the Proceedings of ACL-94,
  Student Session</comments><journal-ref>In Proceedings of ACL-94 (Student Session)</journal-ref><abstract>  Degraded text recognition is a difficult task. Given a noisy text image, a
word recognizer can be applied to generate several candidates for each word
image. High-level knowledge sources can then be used to select a decision from
the candidate set for each word image. In this paper, we propose that visual
inter-word constraints can be used to facilitate candidate selection. Visual
inter-word constraints provide a way to link word images inside the text page,
and to interpret them systematically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405013</id><created>1994-05-06</created><updated>1994-05-09</updated><authors><author><keyname>Edmonds</keyname><forenames>Philip G.</forenames></author></authors><title>Collaboration on reference to objects that are not mutually known</title><categories>cmp-lg cs.CL</categories><comments>6 pages, to appear in proceedings of COLING-94, LaTeX (now uses
  fullname.sty, fullname.bst)</comments><abstract>  In conversation, a person sometimes has to refer to an object that is not
previously known to the other participant. We present a plan-based model of how
agents collaborate on reference of this sort. In making a reference, an agent
uses the most salient attributes of the referent. In understanding a reference,
an agent determines his confidence in its adequacy as a means of identifying
the referent. To collaborate, the agents use judgment, suggestion, and
elaboration moves to refashion an inadequate referring expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405014</id><created>1994-05-09</created><authors><author><keyname>Litman</keyname><forenames>Diane J.</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill, NJ</affiliation></author></authors><title>Classifying Cue Phrases in Text and Speech Using Machine Learning</title><categories>cmp-lg cs.CL</categories><comments>8 pages, PostScript File, to appear in the Proceedings of AAAI-94</comments><abstract>  Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification rules from sets
of pre-classified cue phrases and their features. Machine learning is shown to
be an effective technique for not only automating the generation of
classification rules, but also for improving upon previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405015</id><created>1994-05-09</created><authors><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames><affiliation>Department of Computer Science, Columbia University, New York</affiliation></author><author><keyname>Litman</keyname><forenames>Diane J.</forenames><affiliation>AT&amp;T Bell Laboratories, Murray Hill, NJ</affiliation></author></authors><title>Intention-based Segmentation: Human Reliability and Correlation with
  Linguistic Cues</title><categories>cmp-lg cs.CL</categories><comments>8 pages, PostScript File, in Proceedings of ACL-93</comments><abstract>  Certain spans of utterances in a discourse, referred to here as segments, are
widely assumed to form coherent units. Further, the segmental structure of
discourse has been claimed to constrain and be constrained by many phenomena.
However, there is weak consensus on the nature of segments and the criteria for
recognizing or generating them. We present quantitative results of a two part
study using a corpus of spontaneous, narrative monologues. The first part
evaluates the statistical reliability of human segmentation of our corpus,
where speaker intention is the segmentation criterion. We then use the
subjects' segmentations to evaluate the correlation of discourse segmentation
with three linguistic cues (referential noun phrases, cue words, and pauses),
using information retrieval metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405016</id><created>1994-05-10</created><authors><author><keyname>Stolcke</keyname><forenames>Andreas</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author><author><keyname>Segal</keyname><forenames>Jonathan</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author></authors><title>Precise n-gram Probabilities from Stochastic Context-free Grammars</title><categories>cmp-lg cs.CL</categories><comments>12 pages, to appear in ACL-94</comments><report-no>ICSI TR-94-007</report-no><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We present an algorithm for computing n-gram probabilities from stochastic
context-free grammars, a procedure that can alleviate some of the standard
problems associated with n-grams (estimation from sparse data, lack of
linguistic structure, among others). The method operates via the computation of
substring expectations, which in turn is accomplished by solving systems of
linear equations derived from the grammar. We discuss efficient implementation
of the algorithm and report our practical experience with it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405017</id><created>1994-05-10</created><authors><author><keyname>Stolcke</keyname><forenames>Andreas</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author><author><keyname>Omohundro</keyname><forenames>Stephen M.</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author></authors><title>Best-first Model Merging for Hidden Markov Model Induction</title><categories>cmp-lg cs.CL</categories><comments>63 pages</comments><report-no>ICSI TR-94-003</report-no><abstract>  This report describes a new technique for inducing the structure of Hidden
Markov Models from data which is based on the general `model merging' strategy
(Omohundro 1992). The process begins with a maximum likelihood HMM that
directly encodes the training data. Successively more general models are
produced by merging HMM states. A Bayesian posterior probability criterion is
used to determine which states to merge and when to stop generalizing. The
procedure may be considered a heuristic search for the HMM structure with the
highest posterior probability. We discuss a variety of possible priors for
HMMs, as well as a number of approximations which improve the computational
efficiency of the algorithm. We studied three applications to evaluate the
procedure. The first compares the merging algorithm with the standard
Baum-Welch approach in inducing simple finite-state languages from small,
positive-only training samples. We found that the merging procedure is more
robust and accurate, particularly with a small amount of training data. The
second application uses labelled speech data from the TIMIT database to build
compact, multiple-pronunciation word models that can be used in speech
recognition. Finally, we describe how the algorithm was incorporated in an
operational speech understanding system, where it is combined with neural
network acoustic likelihood estimators to improve performance over
single-pronunciation word models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405018</id><created>1994-05-16</created><authors><author><keyname>Daelemans</keyname><forenames>Walter</forenames></author></authors><title>Memory-Based Lexical Acquisition and Processing</title><categories>cmp-lg cs.CL</categories><comments>18 pages</comments><journal-ref>Steffens (ed.) Machine Translation &amp; Lexion. Springer, 1995</journal-ref><abstract>  Current approaches to computational lexicology in language technology are
knowledge-based (competence-oriented) and try to abstract away from specific
formalisms, domains, and applications. This results in severe complexity,
acquisition and reusability bottlenecks. As an alternative, we propose a
particular performance-oriented approach to Natural Language Processing based
on automatic memory-based learning of linguistic (lexical) tasks. The
consequences of the approach for computational lexicology are discussed, and
the application of the approach on a number of lexical acquisition and
disambiguation tasks in phonology, morphology and syntax is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405019</id><created>1994-05-19</created><updated>1994-05-23</updated><authors><author><keyname>Murata</keyname><forenames>Masaki</forenames></author><author><keyname>Nagao</keyname><forenames>Makoto</forenames></author></authors><title>Determination of referential property and number of nouns in Japanese
  sentences for machine translation into English</title><categories>cmp-lg cs.CL</categories><comments>8 pages, TMI-93</comments><abstract>  When translating Japanese nouns into English, we face the problem of articles
and numbers which the Japanese language does not have, but which are necessary
for the English composition. To solve this difficult problem we classified the
referential property and the number of nouns into three types respectively.
This paper shows that the referential property and the number of nouns in a
sentence can be estimated fairly reliably by the words in the sentence. Many
rules for the estimation were written in forms similar to rewriting rules in
expert systems. We obtained the correct recognition scores of 85.5\% and 89.0\%
in the estimation of the referential property and the number respectively for
the sentences which were used for the construction of our rules. We tested
these rules for some other texts, and obtained the scores of 68.9\% and 85.6\%
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405020</id><created>1994-05-23</created><updated>1994-05-24</updated><authors><author><keyname>Rogers</keyname><forenames>James</forenames><affiliation>Univ. of Delaware</affiliation></author></authors><title>Capturing CFLs with Tree Adjoining Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 3 figures. To appear in proceedings of ACL'94</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We define a decidable class of TAGs that is strongly equivalent to CFGs and
is cubic-time parsable. This class serves to lexicalize CFGs in the same manner
as the LCFGs of Schabes and Waters but with considerably less restriction on
the form of the grammars. The class provides a normal form for TAGs that
generate local sets in much the same way that regular grammars provide a normal
form for CFGs that generate regular sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405021</id><created>1994-05-24</created><authors><author><keyname>Linden</keyname><forenames>Keith Vander</forenames><affiliation>ITRI, University of Brighton</affiliation></author></authors><title>Generating Precondition Expressions in Instructional Text</title><categories>cmp-lg cs.CL</categories><comments>8 pages, in postscript format (compressed and uuencoded), one of the
  figures has been removed due to excessive size</comments><journal-ref>proceedings of ACL-94</journal-ref><abstract>  This study employs a knowledge intensive corpus analysis to identify the
elements of the communicative context which can be used to determine the
appropriate lexical and grammatical form of instructional texts. \ig, an
instructional text generation system based on this analysis, is presented,
particularly with reference to its expression of precondition relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405022</id><created>1994-05-25</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>Swedish Institute of Computer Science</affiliation></author></authors><title>Grammar Specialization through Entropy Thresholds</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  Explanation-based generalization is used to extract a specialized grammar
from the original one using a training corpus of parse trees. This allows very
much faster parsing and gives a lower error rate, at the price of a small loss
in coverage. Previously, it has been necessary to specify the tree-cutting
criteria (or operationality criteria) manually; here they are derived
automatically from the training set and the desired coverage of the specialized
grammar. This is done by assigning an entropy value to each node in the parse
trees and cutting in the nodes with sufficiently high entropy values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405023</id><created>1994-05-25</created><authors><author><keyname>Lavie</keyname><forenames>Alon</forenames><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author></authors><title>An Integrated Heuristic Scheme for Partial Parse Evaluation</title><categories>cmp-lg cs.CL</categories><comments>3 pages, 1 table, LaTeX source, uses latex-acl.sty and named.sty To
  appear in proceedings of ACL-94 (student sessions)</comments><journal-ref>In Proceedings of ACL-94 (student session)</journal-ref><abstract>  GLR* is a recently developed robust version of the Generalized LR Parser,
that can parse almost ANY input sentence by ignoring unrecognizable parts of
the sentence. On a given input sentence, the parser returns a collection of
parses that correspond to maximal, or close to maximal, parsable subsets of the
original input. This paper describes recent work on developing an integrated
heuristic scheme for selecting the parse that is deemed ``best'' from such a
collection. We describe the heuristic measures used and their combination
scheme. Preliminary results from experiments conducted on parsing speech
recognized spontaneous speech are also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405024</id><created>1994-05-26</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames></author></authors><title>Abductive Equivalential Translation and its application to Natural
  Language Database Interfacing</title><categories>cmp-lg cs.CL</categories><comments>162 pages, Latex source, PhD thesis (U Stockholm, 1993). Uses
  style-file ustockholm_thesis.sty</comments><abstract>  The thesis describes a logical formalization of natural-language database
interfacing. We assume the existence of a ``natural language engine'' capable
of mediating between surface linguistic string and their representations as
``literal'' logical forms: the focus of interest will be the question of
relating ``literal'' logical forms to representations in terms of primitives
meaningful to the underlying database engine. We begin by describing the nature
of the problem, and show how a variety of interface functionalities can be
considered as instances of a type of formal inference task which we call
``Abductive Equivalential Translation'' (AET); functionalities which can be
reduced to this form include answering questions, responding to commands,
reasoning about the completeness of answers, answering meta-questions of type
``Do you know...'', and generating assertions and questions. In each case, a
``linguistic domain theory'' (LDT) $\Gamma$ and an input formula $F$ are given,
and the goal is to construct a formula with certain properties which is
equivalent to $F$, given $\Gamma$ and a set of permitted assumptions. If the
LDT is of a certain specified type, whose formulas are either conditional
equivalences or Horn-clauses, we show that the AET problem can be reduced to a
goal-directed inference method. We present an abstract description of this
method, and sketch its realization in Prolog. The relationship between AET and
several problems previously discussed in the literature is discussed. In
particular, we show how AET can provide a simple and elegant solution to the
so-called ``Doctor on Board'' problem, and in effect allows a
``relativization'' of the Closed World Assumption. The ideas in the thesis have
all been implemented concretely within the SRI CLARE project, using a real
projects and payments database. The LDT for the example database is described
in detail, and examples of the types of functionality that can be achieved
within the example domain are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405025</id><created>1994-05-26</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames><affiliation>University of Nijmegen</affiliation></author></authors><title>An Optimal Tabular Parsing Algorithm</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Unix compressed, uuencoded</comments><report-no>To appear in ACL-94</report-no><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  In this paper we relate a number of parsing algorithms which have been
developed in very different areas of parsing theory, and which include
deterministic algorithms, tabular algorithms, and a parallel algorithm. We show
that these algorithms are based on the same underlying ideas. By relating
existing ideas, we hope to provide an opportunity to improve some algorithms
based on features of others. A second purpose of this paper is to answer a
question which has come up in the area of tabular parsing, namely how to obtain
a parsing algorithm with the property that the table will contain as little
entries as possible, but without the possibility that two entries represent the
same subderivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405026</id><created>1994-05-26</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames><affiliation>University of Nijmegen</affiliation></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames><affiliation>University of Padova</affiliation></author></authors><title>An Extended Theory of Head-Driven Parsing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Unix compressed, uuencoded</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We show that more head-driven parsing algorithms can be formulated than those
occurring in the existing literature. These algorithms are inspired by a family
of left-to-right parsing algorithms from a recent publication. We further
introduce a more advanced notion of ``head-driven parsing'' which allows more
detailed specification of the processing order of non-head elements in the
right-hand side. We develop a parsing algorithm for this strategy, based on LR
parsing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405027</id><created>1994-05-27</created><authors><author><keyname>Gasser</keyname><forenames>Michael</forenames></author></authors><title>Acquiring Receptive Morphology: A Connectionist Model</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Postscript file; extract with Unix uudecode and uncompress</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  This paper describes a modular connectionist model of the acquisition of
receptive inflectional morphology. The model takes inputs in the form of phones
one at a time and outputs the associated roots and inflections. Simulations
using artificial language stimuli demonstrate the capacity of the model to
learn suffixation, prefixation, infixation, circumfixation, mutation, template,
and deletion rules. Separate network modules responsible for syllables enable
to the network to learn simple reduplication rules as well. The model also
embodies constraints against association-line crossing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405028</id><created>1994-05-28</created><authors><author><keyname>Nakagawa</keyname><forenames>Hiroshi</forenames></author><author><keyname>Nishizawa</keyname><forenames>Shin'ichiro</forenames></author></authors><title>Semantics of Complex Sentences in Japanese</title><categories>cmp-lg cs.CL</categories><comments>10pages, To appear at COLING-94</comments><abstract>  The important part of semantics of complex sentence is captured as relations
among semantic roles in subordinate and main clause respectively. However if
there can be relations between every pair of semantic roles, the amount of
computation to identify the relations that hold in the given sentence is
extremely large. In this paper, for semantics of Japanese complex sentence, we
introduce new pragmatic roles called `observer' and `motivated' respectively to
bridge semantic roles of subordinate and those of main clauses. By these new
roles constraints on the relations among semantic/pragmatic roles are known to
be almost local within subordinate or main clause. In other words, as for the
semantics of the whole complex sentence, the only role we should deal with is a
motivated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405029</id><created>1994-05-30</created><authors><author><keyname>McMahon</keyname><forenames>John</forenames><affiliation>Queen's University, Belfast</affiliation></author><author><keyname>Smith</keyname><forenames>F. J.</forenames><affiliation>Queen's University, Belfast</affiliation></author></authors><title>Structural Tags, Annealing and Automatic Word Classification</title><categories>cmp-lg cs.CL</categories><comments>14 Page Paper. PostScript File</comments><abstract>  This paper describes an automatic word classification system which uses a
locally optimal annealing algorithm and average class mutual information. A new
word-class representation, the structural tag is introduced and its advantages
for use in statistical language modelling are presented. A summary of some
results with the one million word LOB corpus is given; the algorithm is also
shown to discover the vowel-consonant distinction and displays an ability to
cluster words syntactically in a Latin corpus. Finally, a comparison is made
between the current classification system and several leading alternative
systems, which shows that the current system performs respectably well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405030</id><created>1994-05-30</created><authors><author><keyname>Grover</keyname><forenames>Claire</forenames><affiliation>HCRC Language Technology Group, University of Edinburgh</affiliation></author><author><keyname>Brew</keyname><forenames>Chris</forenames><affiliation>HCRC Language Technology Group, University of Edinburgh</affiliation></author><author><keyname>Manandhar</keyname><forenames>Suresh</forenames><affiliation>HCRC Language Technology Group, University of Edinburgh</affiliation></author><author><keyname>Moens</keyname><forenames>Marc</forenames><affiliation>HCRC Language Technology Group, University of Edinburgh</affiliation></author></authors><title>Priority Union and Generalization in Discourse Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Unix compressed and uuencoded Postscript file To appear in
  ACL-94</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We describe an implementation in Carpenter's typed feature formalism, ALE, of
a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine
their method for resolving parallelism-dependent anaphora and show that there
is a coherent feature-structural rendition of this type of grammar which uses
the operations of priority union and generalization. We describe an
augmentation of the ALE system to encompass these operations and we show that
an appropriate choice of definition for priority union gives the desired
multiple output for examples of VP-ellipsis which exhibit a strict/sloppy
ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405031</id><created>1994-05-30</created><authors><author><keyname>Manandhar</keyname><forenames>Suresh</forenames><affiliation>HCRC Language Technology Group, The University of Edinburgh, UK</affiliation></author></authors><title>An Attributive Logic of Set Descriptions and Set Operations</title><categories>cmp-lg cs.CL</categories><comments>8 pages, epsf.sty, leqno.sty, LaTeX, ACL'94</comments><abstract>  This paper provides a model theoretic semantics to feature terms augmented
with set descriptions. We provide constraints to specify HPSG style set
descriptions, fixed cardinality set descriptions, set-membership constraints,
restricted universal role quantifications, set union, intersection, subset and
disjointness. A sound, complete and terminating consistency checking procedure
is provided to determine the consistency of any given term in the logic. It is
shown that determining consistency of terms is a NP-complete problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405032</id><created>1994-05-30</created><authors><author><keyname>Gasser</keyname><forenames>Michael</forenames><affiliation>Indiana University</affiliation></author></authors><title>Modularity in a Connectionist Model of Morphology Acquisition</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded compressed Postscript file</comments><journal-ref>Proceedings of COLING 94</journal-ref><abstract>  This paper describes a modular connectionist model of the acquisition of
receptive inflectional morphology. The model takes inputs in the form of phones
one at a time and outputs the associated roots and inflections. In its simplest
version, the network consists of separate simple recurrent subnetworks for root
and inflection identification; both networks take the phone sequence as inputs.
It is shown that the performance of the two separate modular networks is
superior to a single network responsible for both root and inflection
identification. In a more elaborate version of the model, the network learns to
use separate hidden-layer modules to solve the separate tasks of root and
inflection identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405033</id><created>1994-05-31</created><authors><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>University of Cambridge, Computer Laboratory</affiliation></author></authors><title>Relating Complexity to Practical Performance in Parsing with
  Wide-Coverage Unification Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX source (one figure not included) To appear in ACL-94</comments><journal-ref>32nd Annual Meeting of the ACL, 287-294</journal-ref><abstract>  The paper demonstrates that exponential complexities with respect to grammar
size and input length have little impact on the performance of three
unification-based parsing algorithms, using a wide-coverage grammar. The
results imply that the study and optimisation of unification-based parsing must
rely on empirical data until complexity theory can more accurately predict the
practical behaviour of such parsers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405034</id><created>1994-05-31</created><authors><author><keyname>Chen</keyname><forenames>Kuang-hua</forenames><affiliation>National Taiwan University</affiliation></author><author><keyname>Chen</keyname><forenames>Hsin-Hsi</forenames><affiliation>National Taiwan University</affiliation></author></authors><title>Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and
  Its Automatic Evaluation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Postscript file, Unix compressed, uuencoded</comments><report-no>To appear in ACL-94</report-no><abstract>  To acquire noun phrases from running texts is useful for many applications,
such as word grouping,terminology indexing, etc. The reported literatures adopt
pure probabilistic approach, or pure rule-based noun phrases grammar to tackle
this problem. In this paper, we apply a probabilistic chunker to deciding the
implicit boundaries of constituents and utilize the linguistic knowledge to
extract the noun phrases by a finite state mechanism. The test texts are
SUSANNE Corpus and the results are evaluated by comparing the parse field of
SUSANNE Corpus automatically. The results of this preliminary experiment are
encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9405035</id><created>1994-05-31</created><authors><author><keyname>Wang</keyname><forenames>Ye-Yi</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Dual-Coding Theory and Connectionist Lexical Selection</title><categories>cmp-lg cs.CL</categories><comments>3 pages, ACL94 student session</comments><abstract>  We introduce the bilingual dual-coding theory as a model for bilingual mental
representation. Based on this model, lexical selection neural networks are
implemented for a connectionist transfer project in machine translation. This
lexical selection approach has two advantages. First, it is learnable. Little
human effort on knowledge engineering is required. Secondly, it is
psycholinguistically well-founded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406001</id><created>1994-06-01</created><authors><author><keyname>Asher</keyname><forenames>Nicholas</forenames><affiliation>IRIT, Universite Paul Sabatier, Toulouse</affiliation></author><author><keyname>Lascarides</keyname><forenames>Alex</forenames><affiliation>Department of Linguistics, Stanford University</affiliation></author></authors><title>Intentions and Information in Discourse</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of ACL-94</journal-ref><abstract>  This paper is about the flow of inference between communicative intentions,
discourse structure and the domain during discourse processing. We augment a
theory of discourse interpretation with a theory of distinct mental attitudes
and reasoning about them, in order to provide an account of how the attitudes
interact with reasoning about discourse structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406002</id><created>1994-06-01</created><authors><author><keyname>Nagao</keyname><forenames>Katashi</forenames><affiliation>Sony Computer Science Laboratory Inc.</affiliation></author><author><keyname>Takeuchi</keyname><forenames>Akikazu</forenames><affiliation>Sony Computer Science Laboratory Inc.</affiliation></author></authors><title>Speech Dialogue with Facial Displays: Multimodal Human-Computer
  Conversation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Postscript file, UNIX compressed, uuencoded</comments><report-no>To appear in Proceedings of ACL-94</report-no><abstract>  Human face-to-face conversation is an ideal model for human-computer
dialogue. One of the major features of face-to-face communication is its
multiplicity of communication channels that act on multiple modalities. To
realize a natural multimodal dialogue, it is necessary to study how humans
perceive information and determine the information to which humans are
sensitive. A face is an independent communication channel that conveys
emotional and conversational signals, encoded as facial expressions. We have
developed an experimental system that integrates speech dialogue and facial
animation, to investigate the effect of introducing communicative facial
expressions as a new modality in human-computer conversation. Our experiments
have shown that facial expressions are helpful, especially upon first contact
with the system. We have also discovered that featuring facial expressions at
an early stage improves subsequent interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406003</id><created>1994-06-01</created><authors><author><keyname>Pieraccini</keyname><forenames>Roberto</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Levin</keyname><forenames>Esther</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author></authors><title>A Learning Approach to Natural Language Understanding</title><categories>cmp-lg cs.CL</categories><comments>18 pages, Latex file + compressed figures</comments><journal-ref>&quot;New Advances and Trends in Speech Recognition and Coding&quot;, NATO
  ASI Series, Springer-Verlag, proceedings of the 1993 NATO ASI Summer School,
  Bubion, Spain, June-July 1993</journal-ref><abstract>  In this paper we propose a learning paradigm for the problem of understanding
spoken language. The basis of the work is in a formalization of the
understanding problem as a communication problem. This results in the
definition of a stochastic model of the production of speech or text starting
from the meaning of a sentence. The resulting understanding algorithm consists
in a Viterbi maximization procedure, analogous to that commonly used for
recognizing speech. The algorithm was implemented for building
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406004</id><created>1994-06-01</created><authors><author><keyname>Young</keyname><forenames>R. Michael</forenames><affiliation>Intelligent Systems Program, University of Pittsburgh</affiliation></author><author><keyname>Moore</keyname><forenames>Johanna D.</forenames><affiliation>Department of Computer Science and Learning Research and Development Center, University of Pittsburgh</affiliation></author><author><keyname>Pollack</keyname><forenames>Martha E.</forenames><affiliation>Department of Computer Science and Intelligent Systems Program, University of Pittsburgh</affiliation></author></authors><title>Towards a Principled Representation of Discourse Plans</title><categories>cmp-lg cs.CL</categories><comments>requires cogsci94.sty, psfig.sty</comments><report-no>ISP Technical Report# 94-2</report-no><journal-ref>To appear in Proceedings of the Sixteenth Annual Conference of the
  Cognitive Science Society, Atlanta, Ga, August, 1994</journal-ref><abstract>  We argue that discourse plans must capture the intended causal and
decompositional relations between communicative actions. We present a planning
algorithm, DPOCL, that builds plan structures that properly capture these
relations, and show how these structures are used to solve the problems that
plagued previous discourse planners, and allow a system to participate
effectively and flexibly in an ongoing dialogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406005</id><created>1994-06-01</created><authors><author><keyname>Bruce</keyname><forenames>Rebecca</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>New Mexico State University</affiliation></author></authors><title>Word-Sense Disambiguation Using Decomposable Models</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Unix compressed, uuencoded Postscript file</comments><report-no>To appear in ACL-94</report-no><abstract>  Most probabilistic classifiers used for word-sense disambiguation have either
been based on only one contextual feature or have used a model that is simply
assumed to characterize the interdependencies among multiple contextual
features. In this paper, a different approach to formulating a probabilistic
model is presented along with a case study of the performance of models
produced in this manner for the disambiguation of the noun &quot;interest&quot;. We
describe a method for formulating probabilistic models that use multiple
contextual features for word-sense disambiguation, without requiring untested
assumptions regarding the form of the model. Using this approach, the joint
distribution of all variables is described by only the most systematic variable
interactions, thereby limiting the number of parameters to be estimated,
supporting computational efficiency, and providing an understanding of the
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406006</id><created>1994-06-01</created><updated>1994-06-02</updated><authors><author><keyname>Heeman</keyname><forenames>Peter</forenames><affiliation>U of Rochester</affiliation></author><author><keyname>Allen</keyname><forenames>James</forenames><affiliation>U of Rochester</affiliation></author></authors><title>Detecting and Correcting Speech Repairs</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in acl-94, Added latex version</comments><abstract>  Interactive spoken dialog provides many new challenges for spoken language
systems. One of the most critical is the prevalence of speech repairs. This
paper presents an algorithm that detects and corrects speech repairs based on
finding the repair pattern. The repair pattern is built by finding word matches
and word replacements, and identifying fragments and editing terms. Rather than
using a set of prebuilt templates, we build the pattern on the fly. In a fair
test, our method, when combined with a statistical model to filter possible
repairs, was successful at detecting and correcting 80\% of the repairs,
without using prosodic information or a parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406007</id><created>1994-06-02</created><authors><author><keyname>Wu</keyname><forenames>Dekai</forenames><affiliation>Hong Kong University of Science &amp; Technology</affiliation></author></authors><title>Aligning a Parallel English-Chinese Corpus Statistically with Lexical
  Criteria</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded compressed PostScript</comments><report-no>HKUST-CS93-9 (revised)</report-no><journal-ref>In Proceedings of ACL-94.</journal-ref><abstract>  We describe our experience with automatic alignment of sentences in parallel
English-Chinese texts.  Our report concerns three related topics:
 (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus;
 (2) experiments addressing the applicability of Gale &amp; Church's length-based
statistical method to the task of alignment involving a non-Indo-European
language; and
 (3) an improved statistical method that also incorporates domain-specific
lexical cues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406008</id><created>1994-06-02</created><authors><author><keyname>Gungordu</keyname><forenames>Zelal</forenames><affiliation>Center for Cognitive Science, Univ. of Edinburgh</affiliation></author><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Dept of Computer Engineering, Bilkent University, Ankara</affiliation></author></authors><title>Parsing Turkish with the Lexical Functional Grammar Formalism</title><categories>cmp-lg cs.CL</categories><comments>7 pages, Postscript (compressed (gzip) and uuencoded)</comments><report-no>(BU-CEIS-9402 Bilkent University CS Dept Tech Report)</report-no><journal-ref>Proceedings of COLING'94</journal-ref><abstract>  This paper describes our work on parsing Turkish using the lexical-functional
grammar formalism. This work represents the first significant effort for
parsing Turkish. Our implementation is based on Tomita's parser developed at
Carnegie-Mellon University Center for Machine Translation. The grammar covers a
substantial subset of Turkish including simple and complex sentences, and deals
with a reasonable amount of word order freeness. The complex agglutinative
morphology of Turkish lexical structures is handled using a separate two-level
morphological analyzer. After a discussion of key relevant issues regarding
Turkish grammar, we discuss aspects of our system and present results from our
implementation. Our initial results suggest that our system can parse about
82\% of the sentences directly and almost all the remaining with very minor
pre-editing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406009</id><created>1994-06-02</created><authors><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>Universite Paris 7</affiliation></author></authors><title>Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on
  Derivations</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded compressed ps file</comments><journal-ref>Proc ACL 94</journal-ref><abstract>  This paper defines multiset-valued linear index grammar and unordered vector
grammar with dominance links. The former models certain uses of multiset-valued
feature structures in unification-based formalisms, while the latter is
motivated by word order variation and by ``quasi-trees'', a generalization of
trees. The two formalisms are weakly equivalent, and an important subset is at
most context-sensitive and polynomially parsable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406010</id><created>1994-06-02</created><authors><author><keyname>Brill</keyname><forenames>Eric</forenames><affiliation>MIT</affiliation></author></authors><title>Some Advances in Transformation-Based Part of Speech Tagging</title><categories>cmp-lg cs.CL</categories><comments>6 Pages. Code available</comments><journal-ref>Proceedings of AAAI94</journal-ref><abstract>  Most recent research in trainable part of speech taggers has explored
stochastic tagging. While these taggers obtain high accuracy, linguistic
information is captured indirectly, typically in tens of thousands of lexical
and contextual probabilities. In [Brill92], a trainable rule-based tagger was
described that obtained performance comparable to that of stochastic taggers,
but captured relevant linguistic information in a small number of simple
non-stochastic rules. In this paper, we describe a number of extensions to this
rule-based tagger. First, we describe a method for expressing lexical relations
in tagging that are not captured by stochastic taggers. Next, we show a
rule-based approach to tagging unknown words. Finally, we show how the tagger
can be extended into a k-best tagger, where multiple tags can be assigned to
words in some cases of uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406011</id><created>1994-06-03</created><authors><author><keyname>Ramshaw</keyname><forenames>Lance A.</forenames><affiliation>Univ. of Pennsylvania and Bowdoin College</affiliation></author><author><keyname>Marcus</keyname><forenames>Mitchell P.</forenames><affiliation>Univ. of Pennsylvania</affiliation></author></authors><title>Exploring the Statistical Derivation of Transformational Rule Sequences
  for Part-of-Speech Tagging</title><categories>cmp-lg cs.CL</categories><comments>10 pages, in proceedings of the ACL Balancing Act workshop</comments><journal-ref>ACL Balancing Act Workshop proceedings, July 94, pp. 86-95</journal-ref><abstract>  Eric Brill has recently proposed a simple and powerful corpus-based language
modeling approach that can be applied to various tasks including part-of-speech
tagging and building phrase structure trees. The method learns a series of
symbolic transformational rules, which can then be applied in sequence to a
test corpus to produce predictions. The learning process only requires counting
matches for a given set of rule templates, allowing the method to survey a very
large space of possible contextual factors. This paper analyses Brill's
approach as an interesting variation on existing decision tree methods, based
on experiments involving part-of-speech tagging for both English and ancient
Greek corpora. In particular, the analysis throws light on why the new
mechanism seems surprisingly resistant to overtraining. A fast, incremental
implementation and a mechanism for recording the dependencies that underlie the
resulting rule sequence are also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406012</id><created>1994-06-03</created><authors><author><keyname>Juola</keyname><forenames>Patrick</forenames><affiliation>University of Colorado</affiliation></author></authors><title>Self-Organizing Machine Translation: Example-Driven Induction of
  Transfer Functions</title><categories>cmp-lg cs.CL</categories><comments>PostScript, 30 pages, contact author for LaTeX version</comments><report-no>CU-CS-722-94</report-no><abstract>  With the advent of faster computers, the notion of doing machine translation
from a huge stored database of translation examples is no longer unreasonable.
This paper describes an attempt to merge the Example-Based Machine Translation
(EBMT) approach with psycholinguistic principles. A new formalism for context-
free grammars, called *marker-normal form*, is demonstrated and used to
describe language data in a way compatible with psycholinguistic theories. By
embedding this formalism in a standard multivariate optimization framework, a
system can be built that infers correct transfer functions for a set of
bilingual sentence pairs and then uses those functions to translate novel
sentences. The validity of this line of reasoning has been tested in the
development of a system called METLA-1. This system has been used to infer
English-&gt;French and English-&gt;Urdu transfer functions from small corpora. The
results of those experiments are examined, both in engineering terms as well as
in more linguistic terms. In general, the results of these experiments were
psycho- logically and linguistically well-grounded while still achieving a
respectable level of success when compared against a similar prototype using
Hidden Markov Models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406013</id><created>1994-06-05</created><authors><author><keyname>Kim</keyname><forenames>Albert</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Graded Unification: A Framework for Interactive Processing</title><categories>cmp-lg cs.CL</categories><comments>3 pages, To appear in ACL-94 Student Session, Postscript file;
  extract with Unix uudecode and uncompress</comments><abstract>  An extension to classical unification, called {\em graded unification} is
presented. It is capable of combining contradictory information. An interactive
processing paradigm and parser based on this new operator are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406014</id><created>1994-06-07</created><authors><author><keyname>Green</keyname><forenames>Nancy</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>University of Delaware and Visitor: IRCS, University of Pennsylvania</affiliation></author></authors><title>A Hybrid Reasoning Model for Indirect Answers</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proc. of ACL-94. 8 pages, uuencoded compressed
  Postscript file; extract with Unix uudecode and uncompress. Contact Author
  for latex version</comments><abstract>  This paper presents our implemented computational model for interpreting and
generating indirect answers to Yes-No questions. Its main features are 1) a
discourse-plan-based approach to implicature, 2) a reversible architecture for
generation and interpretation, 3) a hybrid reasoning model that employs both
plan inference and logical inference, and 4) use of stimulus conditions to
model a speaker's motivation for providing appropriate, unrequested
information. The model handles a wider range of types of indirect answers than
previous computational models and has several significant advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406015</id><created>1994-06-07</created><authors><author><keyname>Fung</keyname><forenames>Pascale</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Wu</keyname><forenames>Dekai</forenames><affiliation>Hong Kong University of Science &amp; Technology</affiliation></author></authors><title>Statistical Augmentation of a Chinese Machine-Readable Dictionary</title><categories>cmp-lg cs.CL</categories><comments>17 pages, uuencoded compressed PostScript</comments><report-no>cucs-015-94</report-no><journal-ref>In WVLC-94, Second Annual Workshop on Very Large</journal-ref><abstract>  We describe a method of using statistically-collected Chinese character
groups from a corpus to augment a Chinese dictionary. The method is
particularly useful for extracting domain-specific and regional words not
readily available in machine-readable dictionaries. Output was evaluated both
using human evaluators and against a previously available dictionary. We also
evaluated performance improvement in automatic Chinese tokenization. Results
show that our method outputs legitimate words, acronymic constructions, idioms,
names and titles, as well as technical compounds, many of which were lacking
from the original dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406016</id><created>1994-06-07</created><authors><author><keyname>Soderland</keyname><forenames>Stephen</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Lehnert</keyname><forenames>Wendy</forenames><affiliation>University of Massachusetts</affiliation></author></authors><title>Corpus-Driven Knowledge Acquisition for Discourse Analysis</title><categories>cmp-lg cs.CL</categories><comments>6 pages, AAAI-94</comments><abstract>  The availability of large on-line text corpora provides a natural and
promising bridge between the worlds of natural language processing (NLP) and
machine learning (ML). In recent years, the NLP community has been aggressively
investigating statistical techniques to drive part-of-speech taggers, but
application-specific text corpora can be used to drive knowledge acquisition at
much higher levels as well. In this paper we will show how ML techniques can be
used to support knowledge acquisition for information extraction systems. It is
often very difficult to specify an explicit domain model for many information
extraction applications, and it is always labor intensive to implement
hand-coded heuristics for each new domain. We have discovered that it is
nevertheless possible to use ML algorithms in order to capture knowledge that
is only implicitly present in a representative text corpus. Our work addresses
issues traditionally associated with discourse analysis and intersentential
inference generation, and demonstrates the utility of ML algorithms at this
higher level of language analysis. The benefits of our work address the
portability and scalability of information extraction (IE) technologies. When
hand-coded heuristics are used to manage discourse analysis in an information
extraction system, months of programming effort are easily needed to port a
successful IE system to a new domain. We will show how ML algorithms can reduce
this
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406017</id><created>1994-06-07</created><authors><author><keyname>Reynar</keyname><forenames>Jeffrey C.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>An Automatic Method of Finding Topic Boundaries</title><categories>cmp-lg cs.CL</categories><comments>3 pages, in student session of ACL 1994</comments><abstract>  This article outlines a new method of locating discourse boundaries based on
lexical cohesion and a graphical technique called dotplotting. The application
of dotplotting to discourse segmentation can be performed either manually, by
examining a graph, or automatically, using an optimization algorithm. The
results of two experiments involving automatically locating boundaries between
a series of concatenated documents are presented. Areas of application and
future directions for this work are also outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406018</id><created>1994-06-08</created><updated>1994-06-15</updated><authors><author><keyname>Krieger</keyname><forenames>Hans-Ulrich</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Ulrich</forenames></author></authors><title>TDL--- A Type Description Language for Constraint-Based Grammars</title><categories>cmp-lg cs.CL</categories><comments>Will Appear in Proc. COLING-94</comments><abstract>  This paper presents \tdl, a typed feature-based representation language and
inference system. Type definitions in \tdl\ consist of type and feature
constraints over the boolean connectives. \tdl\ supports open- and closed-world
reasoning over types and allows for partitions and incompatible types. Working
with partially as well as with fully expanded types is possible. Efficient
reasoning in \tdl\ is accomplished through specialized modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406019</id><created>1994-06-10</created><updated>1994-06-17</updated><authors><author><keyname>Backofen</keyname><forenames>Rolf</forenames></author><author><keyname>Smolka</keyname><forenames>Gert</forenames></author></authors><title>A Complete and Recursive Feature Theory</title><categories>cmp-lg cs.CL</categories><comments>Short version appeared in the 1992 Annual Meeting of the Association
  for Computational Linguistics</comments><report-no>RR-92-30</report-no><abstract>  Various feature descriptions are being employed in logic programming
languages and constrained-based grammar formalisms. The common notational
primitive of these descriptions are functional attributes called features. The
descriptions considered in this paper are the possibly quantified first-order
formulae obtained from a signature of binary and unary predicates called
features and sorts, respectively. We establish a first-order theory FT by means
of three axiom schemes, show its completeness, and construct three elementarily
equivalent models. One of the models consists of so-called feature graphs, a
data structure common in computational linguistics. The other two models
consist of so-called feature trees, a record-like data structure generalizing
the trees corresponding to first-order terms. Our completeness proof exhibits a
terminating simplification system deciding validity and satisfiability of
possibly quantified feature descriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406020</id><created>1994-06-10</created><authors><author><keyname>Young</keyname><forenames>R. Michael</forenames><affiliation>Intelligent Systems Program, University of Pittsburgh</affiliation></author><author><keyname>Moore</keyname><forenames>Johanna D.</forenames><affiliation>Department of Computer Science and Learning Research and Development Center, University of Pittsburgh</affiliation></author></authors><title>DPOCL: A Principled Approach to Discourse Planning</title><categories>cmp-lg cs.CL</categories><journal-ref>proceedings of the Seventh International Workshop on Natural
  Langauge Generation, Kennebunkport, ME, June, 1994</journal-ref><abstract>  Research in discourse processing has identified two representational
requirements for discourse planning systems. First, discourse plans must
adequately represent the intentional structure of the utterances they produce
in order to enable a computational discourse agent to respond effectively to
communicative failures \cite{MooreParisCL}. Second, discourse plans must
represent the informational structure of utterances. In addition to these
representational requirements, we argue that discourse planners should be
formally characterizable in terms of soundness and completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406021</id><created>1994-06-13</created><authors><author><keyname>Binsted</keyname><forenames>Kim</forenames><affiliation>Department of Artificial Intelligence, University of Edinburgh</affiliation></author><author><keyname>Ritchie</keyname><forenames>Graeme</forenames><affiliation>Department of Artificial Intelligence, University of Edinburgh</affiliation></author></authors><title>A symbolic description of punning riddles and its computer
  implementation</title><categories>cmp-lg cs.CL</categories><comments>40 pages, also available by email (kimb@aisb.ed.ac.uk) and www
  (http://www.dai.ed.ac.uk/students/kimb). Longer version of an AAAI-94 paper.
  Figs and bib separate</comments><report-no>688 (Dept of AI). Submitted to &quot;Humor&quot;</report-no><abstract>  Riddles based on simple puns can be classified according to the patterns of
word, syllable or phrase similarity they depend upon. We have devised a formal
model of the semantic and syntactic regularities underlying some of the simpler
types of punning riddle. We have also implemented this preliminary theory in a
computer program which can generate riddles from a lexicon containing general
data about words and phrases; that is, the lexicon content is not customised to
produce jokes. Informal evaluation of the program's results by a set of human
judges suggest that the riddles produced by this program are of comparable
quality to those in general circulation among school children.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406022</id><created>1994-06-13</created><authors><author><keyname>Binsted</keyname><forenames>Kim</forenames><affiliation>Department of Artificial Intelligence, University of Edinburgh</affiliation></author><author><keyname>Ritchie</keyname><forenames>Graeme</forenames><affiliation>Department of Artificial Intelligence, University of Edinburgh</affiliation></author></authors><title>An implemented model of punning riddles</title><categories>cmp-lg cs.CL</categories><comments>6 pages, using aaai.sty and aaai.bst (in cmp-lg style list). Figs and
  bib in separate file. Also available by email (kimb@aisb.ed.ac.uk) and www
  (http://www.dai.ed.ac.uk/students/kimb)</comments><journal-ref>In proceedings of AAAI-94</journal-ref><abstract>  In this paper, we discuss a model of simple question-answer punning,
implemented in a program, JAPE, which generates riddles from humour-independent
lexical entries. The model uses two main types of structure: schemata, which
determine the relationships between key words in a joke, and templates, which
produce the surface form of the joke. JAPE succeeds in generating pieces of
text that are recognizably jokes, but some of them are not very good jokes. We
mention some potential improvements and extensions, including post-production
heuristics for ordering the jokes according to quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406023</id><created>1994-06-14</created><authors><author><keyname>Le&#xf3;n</keyname><forenames>Fernando S&#xe1;nchez</forenames><affiliation>Laboratorio de Ling&#xfc;&#xed;stica Inform&#xe1;tica, Facultad de Filosof&#xed;a y Letras, Universidad Aut&#xf3;noma de Madrid</affiliation></author></authors><title>A Spanish Tagset for the CRATER Project</title><categories>cmp-lg cs.CL</categories><comments>20 pages, LaTeX format</comments><abstract>  This working paper describes the Spanish tagset to be used in the context of
CRATER, a CEC funded project aiming at the creation of a multilingual (English,
French, Spanish) aligned corpus using the International Telecommunications
Union corpus. In this respect, each version of the corpus will be (or is
currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to
perform the tagging of the Spanish version. This tagset has been devised as the
ideal one for Spanish, and has been posted to several lists in order to get
feedback to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406024</id><created>1994-06-16</created><authors><author><keyname>Wermter</keyname><forenames>Stefan</forenames><affiliation>Dept. of Computer Science, University of Hamburg, FRG</affiliation></author><author><keyname>Weber</keyname><forenames>Volker</forenames><affiliation>Dept. of Computer Science, University of Hamburg, FRG</affiliation></author></authors><title>Learning Fault-tolerant Speech Parsing with SCREEN</title><categories>cmp-lg cs.CL</categories><comments>6 pages, postscript, compressed, uuencoded to appear in Proceedings
  of AAAI 94</comments><abstract>  This paper describes a new approach and a system SCREEN for fault-tolerant
speech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for
Natural language. Speech parsing describes the syntactic and semantic analysis
of spontaneous spoken language. The general approach is based on incremental
immediate flat analysis, learning of syntactic and semantic speech parsing,
parallel integration of current hypotheses, and the consideration of various
forms of speech related errors. The goal for this approach is to explore the
parallel interactions between various knowledge sources for learning
incremental fault-tolerant speech parsing. This approach is examined in a
system SCREEN using various hybrid connectionist techniques. Hybrid
connectionist techniques are examined because of their promising properties of
inherent fault tolerance, learning, gradedness and parallel constraint
integration. The input for SCREEN is hypotheses about recognized words of a
spoken utterance potentially analyzed by a speech system, the output is
hypotheses about the flat syntactic and semantic analysis of the utterance. In
this paper we focus on the general approach, the overall architecture, and
examples for learning flat syntactic speech parsing. Different from most other
speech language architectures SCREEN emphasizes an interactive rather than an
autonomous position, learning rather than encoding, flat analysis rather than
in-depth analysis, and fault-tolerant processing of phonetic, syntactic and
semantic knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406025</id><created>1994-06-16</created><authors><author><keyname>Koiti</keyname><forenames>HASIDA</forenames><affiliation>Electrotechnical Laboratory</affiliation></author></authors><title>Emergent Parsing and Generation with Generalized Chart</title><categories>cmp-lg cs.CL</categories><comments>11 pages. To appear in COLING-94. LaTeX source. psfig-scale.tex,
  a4wide.sty, and fullname.sty needed</comments><abstract>  A new, flexible inference method for Horn logic program is proposed, which is
a drastic generalization of chart parsing, partial instantiations of clauses in
a program roughly corresponding to arcs in a chart. Chart-like parsing and
semantic-head-driven generation emerge from this method. With a parsimonious
instantiation scheme for ambiguity packing, the parsing complexity reduces to
that of standard chart-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406026</id><created>1994-06-16</created><authors><author><keyname>Israel</keyname><forenames>David</forenames><affiliation>Artificial Intelligence Center, SRI</affiliation></author></authors><title>The Very Idea of Dynamic Semantics</title><categories>cmp-lg cs.CL</categories><comments>22 pages. Vanilla LaTex</comments><journal-ref>Proc. Ninth Amsterdam Colloquium, 1993</journal-ref><abstract>  &quot;Natural languages are programming languages for minds.&quot; Can we or should we
take this slogan seriously? If so, how? Can answers be found by looking at the
various &quot;dynamic&quot; treatments of natural language developed over the last decade
or so, mostly in response to problems associated with donkey anaphora? In
Dynamic Logic of Programs, the meaning of a program is a binary relation on the
set of states of some abstract machine. This relation is meant to model aspects
of the effects of the execution of the program, in particular its input-output
behavior. What, if anything, are the dynamic aspects of various proposed
dynamic semantics for natural languages supposed to model? Is there anything
dynamic to be modeled? If not, what is all the full about? We shall try to
answer some, at least, of these questions and provide materials for answers to
others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406027</id><created>1994-06-17</created><authors><author><keyname>Ueberla</keyname><forenames>Joerg P.</forenames></author></authors><title>Analyzing and Improving Statistical Language Models for Speech
  Recognition</title><categories>cmp-lg cs.CL</categories><comments>140 pages, postscript, approx 500KB, if problems with delivery, mail
  to ueberla@cs.sfu.ca</comments><report-no>SFU-CMPT 94-05-02</report-no><abstract>  In many current speech recognizers, a statistical language model is used to
indicate how likely it is that a certain word will be spoken next, given the
words recognized so far. How can statistical language models be improved so
that more complex speech recognition tasks can be tackled? Since the knowledge
of the weaknesses of any theory often makes improving the theory easier, the
central idea of this thesis is to analyze the weaknesses of existing
statistical language models in order to subsequently improve them. To that end,
we formally define a weakness of a statistical language model in terms of the
logarithm of the total probability, LTP, a term closely related to the standard
perplexity measure used to evaluate statistical language models. We apply our
definition of a weakness to a frequently used statistical language model,
called a bi-pos model. This results, for example, in a new modeling of unknown
words which improves the performance of the model by 14% to 21%. Moreover, one
of the identified weaknesses has prompted the development of our generalized
N-pos language model, which is also outlined in this thesis. It can incorporate
linguistic knowledge even if it extends over many words and this is not
feasible in a traditional N-pos model. This leads to a discussion of
whatknowledge should be added to statistical language models in general and we
give criteria for selecting potentially useful knowledge. These results show
the usefulness of both our definition of a weakness and of performing an
analysis of weaknesses of statistical language models in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406028</id><created>1994-06-20</created><updated>1994-06-20</updated><authors><author><keyname>Niv</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Resolution of Syntactic Ambiguity: the Case of New Subjects</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX source</comments><journal-ref>COGSCI-93</journal-ref><abstract>  I review evidence for the claim that syntactic ambiguities are resolved on
the basis of the meaning of the competing analyses, not their structure. I
identify a collection of ambiguities that do not yet have a meaning-based
account and propose one which is based on the interaction of discourse and
grammatical function. I provide evidence for my proposal by examining
statistical properties of the Penn Treebank of syntactically annotated text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406029</id><created>1994-06-20</created><authors><author><keyname>Niv</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Computational Model of Syntactic Processing: Ambiguity Resolution from
  Interpretation</title><categories>cmp-lg cs.CL</categories><comments>128 pages, LaTeX source compressed and uuencoded, figures separate
  macros: rotate.sty, lingmacros.sty, psfig.tex. Dissertation, Computer and
  Information Science Dept., October 1993</comments><report-no>IRCS-93-27</report-no><journal-ref>Dissertation, Computer and Information Science Dept. 1993</journal-ref><abstract>  Syntactic ambiguity abounds in natural language, yet humans have no
difficulty coping with it. In fact, the process of ambiguity resolution is
almost always unconscious. But it is not infallible, however, as example 1
demonstrates.
  1. The horse raced past the barn fell.
  This sentence is perfectly grammatical, as is evident when it appears in the
following context:
  2. Two horses were being shown off to a prospective buyer. One was raced past
a meadow. and the other was raced past a barn. ...
  Grammatical yet unprocessable sentences such as 1 are called `garden-path
sentences.' Their existence provides an opportunity to investigate the human
sentence processing mechanism by studying how and when it fails. The aim of
this thesis is to construct a computational model of language understanding
which can predict processing difficulty. The data to be modeled are known
examples of garden path and non-garden path sentences, and other results from
psycholinguistics.
  It is widely believed that there are two distinct loci of computation in
sentence processing: syntactic parsing and semantic interpretation. One
longstanding controversy is which of these two modules bears responsibility for
the immediate resolution of ambiguity. My claim is that it is the latter, and
that the syntactic processing module is a very simple device which blindly and
faithfully constructs all possible analyses for the sentence up to the current
point of processing. The interpretive module serves as a filter, occasionally
discarding certain of these analyses which it deems less appropriate for the
ongoing discourse than their competitors.
  This document is divided into three parts. The first is introductory, and
reviews a selection of proposals from the sentence processing literature. The
second part explores a body of data which has been adduced in support of a
theory of structural preferences --- one that is inconsistent with the present
claim. I show how the current proposal can be specified to account for the
available data, and moreover to predict where structural preference theories
will go wrong. The third part is a theoretical investigation of how well the
proposed architecture can be realized using current conceptions of linguistic
competence. In it, I present a parsing algorithm and a meaning-based ambiguity
resolution method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406030</id><created>1994-06-20</created><updated>1994-06-20</updated><authors><author><keyname>Niv</keyname><forenames>Michael</forenames><affiliation>Technion</affiliation></author></authors><title>The complexity of normal form rewrite sequences for Associativity</title><categories>cmp-lg cs.CL</categories><comments>5 pages</comments><report-no>Computer Science Department, LCL 94-6</report-no><abstract>  The complexity of a particular term-rewrite system is considered: the rule of
associativity (x*y)*z --&gt; x*(y*z). Algorithms and exact calculations are given
for the longest and shortest sequences of applications of --&gt; that result in
normal form (NF). The shortest NF sequence for a term x is always n-drm(x),
where n is the number of occurrences of * in x and drm(x) is the depth of the
rightmost leaf of x. The longest NF sequence for any term is of length
n(n-1)/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406031</id><created>1994-06-20</created><authors><author><keyname>Niv</keyname><forenames>Michael</forenames><affiliation>Technion</affiliation></author></authors><title>A Psycholinguistically Motivated Parser for CCG</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, requires psfig.tex, 2 figures in separate file</comments><journal-ref>ACL-94</journal-ref><abstract>  Considering the speed in which humans resolve syntactic ambiguity, and the
overwhelming evidence that syntactic ambiguity is resolved through selection of
the analysis whose interpretation is the most `sensible', one comes to the
conclusion that interpretation, hence parsing take place incrementally, just
about every word. Considerations of parsimony in the theory of the syntactic
processor lead one to explore the simplest of parsers: one which represents
only analyses as defined by the grammar and no other information.
  Toward this aim of a simple, incremental parser I explore the proposal that
the competence grammar is a Combinatory Categorial Grammar (CCG). I address the
problem of the proliferating analyses that stem from CCG's associativity of
derivation. My solution involves maintaining only the maximally incremental
analysis and, when necessary, computing the maximally right-branching analysis.
I use results from the study of rewrite systems to show that this computation
is efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406032</id><created>1994-06-21</created><authors><author><keyname>Goerz</keyname><forenames>Guenther</forenames><affiliation>University of Erlangen-Nuernberg, IMMD VIII</affiliation></author><author><keyname>Kesseler</keyname><forenames>Marcus</forenames><affiliation>University of Erlangen-Nuernberg, IMMD VIII</affiliation></author></authors><title>Anytime Algorithms for Speech Parsing?</title><categories>cmp-lg cs.CL</categories><comments>5 pages, 2 figures</comments><journal-ref>COLING-94</journal-ref><abstract>  This paper discusses to which extent the concept of ``anytime algorithms''
can be applied to parsing algorithms with feature unification. We first try to
give a more precise definition of what an anytime algorithm is. We arque that
parsing algorithms have to be classified as contract algorithms as opposed to
(truly) interruptible algorithms. With the restriction that the transaction
being active at the time an interrupt is issued has to be completed before the
interrupt can be executed, it is possible to provide a parser with limited
anytime behavior, which is in fact being realized in our research prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406033</id><created>1994-06-21</created><updated>1994-06-23</updated><authors><author><keyname>Wu</keyname><forenames>Zhibiao</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Palmer</keyname><forenames>Martha</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Verb Semantics and Lexical Selection</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Figures and bib files are in part 2</comments><journal-ref>Proceedings of ACL 94</journal-ref><abstract>  This paper will focus on the semantic representation of verbs in computer
systems and its impact on lexical selection problems in machine translation
(MT). Two groups of English and Chinese verbs are examined to show that lexical
selection must be based on interpretation of the sentence as well as selection
restrictions placed on the verb arguments. A novel representation scheme is
suggested, and is compared to representations with selection restrictions used
in transfer-based MT. We see our approach as closely aligned with
knowledge-based MT approaches (KBMT), and as a separate component that could be
incorporated into existing systems. Examples and experimental results will show
that, using this scheme, inexact matches can achieve correct lexical selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406034</id><created>1994-06-22</created><authors><author><keyname>Yarowsky</keyname><forenames>David</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Decision Lists for Lexical Ambiguity Resolution: Application to Accent
  Restoration in Spanish and French</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex-acl, to appear in ACL-94</comments><abstract>  This paper presents a statistical decision procedure for lexical ambiguity
resolution. The algorithm exploits both local syntactic patterns and more
distant collocational evidence, generating an efficient, effective, and highly
perspicuous recipe for resolving a given ambiguity. By identifying and
utilizing only the single best disambiguating evidence in a target context, the
algorithm avoids the problematic complex modeling of statistical dependencies.
Although directly applicable to a wide class of ambiguities, the algorithm is
described and evaluated in a realistic case study, the problem of restoring
missing accents in Spanish and French text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406035</id><created>1994-06-23</created><updated>1994-06-30</updated><authors><author><keyname>Uszkoreit</keyname><forenames>Hans</forenames></author><author><keyname>Backofen</keyname><forenames>Rolf</forenames></author><author><keyname>Busemann</keyname><forenames>Stephan</forenames></author><author><keyname>Diagne</keyname><forenames>Abdel Kader</forenames></author><author><keyname>Hinkelman</keyname><forenames>Elizabeth A.</forenames></author><author><keyname>Kasper</keyname><forenames>Walter</forenames></author><author><keyname>Kiefer</keyname><forenames>Bernd</forenames></author><author><keyname>Krieger</keyname><forenames>Hans-Ulrich</forenames></author><author><keyname>Netter</keyname><forenames>Klaus</forenames></author><author><keyname>Neumann</keyname><forenames>Guenter</forenames></author><author><keyname>Oepen</keyname><forenames>Stephan</forenames></author><author><keyname>Spackman</keyname><forenames>Stephen P.</forenames></author></authors><title>DISCO---An HPSG-based NLP System and its Application for Appointment
  Scheduling (Project Note)</title><categories>cmp-lg cs.CL</categories><abstract>  The natural language system DISCO is described. It combines o a powerful and
flexible grammar development system; o linguistic competence for German
including morphology, syntax and semantics; o new methods for linguistic
performance modelling on the basis of high-level competence grammars; o new
methods for modelling multi-agent dialogue competence; o an interesting sample
application for appointment scheduling and calendar management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406036</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406036</id><created>1994-06-23</created><authors><author><keyname>Riley</keyname><forenames>Michael</forenames><affiliation>Linguistics Research Department, AT&amp;T Bell Laboratories, Murray Hill, NJ</affiliation></author><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>Linguistics Research Department, AT&amp;T Bell Laboratories, Murray Hill, NJ</affiliation></author></authors><title>Text Analysis Tools in Spoken Language Processing</title><categories>cmp-lg cs.CL</categories><comments>Slides for ACL-94 Tutorial</comments><abstract>  This submission contains the postscript of the final version of the slides
used in our ACL-94 tutorial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406037</id><created>1994-06-23</created><authors><author><keyname>Hearst</keyname><forenames>Marti A.</forenames><affiliation>UC Berkeley and Xerox PARC</affiliation></author></authors><title>Multi-Paragraph Segmentation of Expository Text</title><categories>cmp-lg cs.CL</categories><comments>To Appear in ACL '94 Proceedings; 8 pages POSTSCRIPT format</comments><abstract>  This paper describes TextTiling, an algorithm for partitioning expository
texts into coherent multi-paragraph discourse units which reflect the subtopic
structure of the texts. The algorithm uses domain-independent lexical frequency
and distribution information to recognize the interactions of multiple
simultaneous themes. Two fully-implemented versions of the algorithm are
described and shown to produce segmentation that corresponds well to human
judgments of the major subtopic boundaries of thirteen lengthy texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406038</id><created>1994-06-27</created><authors><author><keyname>Novick</keyname><forenames>David G.</forenames><affiliation>Oregon Graduate Institute</affiliation></author><author><keyname>Sutton</keyname><forenames>Stephen</forenames><affiliation>Oregon Graduate Institute</affiliation></author></authors><title>An Empirical Model of Acknowledgment for Spoken-Language Systems</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded compressed tar file</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We refine and extend prior views of the description, purposes, and
contexts-of-use of acknowledgment acts through empirical examination of the use
of acknowledgments in task-based conversation. We distinguish three broad
classes of acknowledgments (other--&gt;ackn, self--&gt;other--&gt;ackn, and self+ackn)
and present a catalogue of 13 patterns within these classes that account for
the specific uses of acknowledgment in the corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406039</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406039</id><created>1994-06-27</created><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Computational Linguistics, University of Helsinki</affiliation></author></authors><title>Three studies of grammar-based surface-syntactic parsing of unrestricted
  English text. A summary and orientation</title><categories>cmp-lg cs.CL</categories><comments>PhD dissertation. 36pp, gzipped and uuencoded .ps file</comments><report-no>Publications 24, Department of General Linguistics, University of
  Helsinki, 1994</report-no><abstract>  The dissertation addresses the design of parsing grammars for automatic
surface-syntactic analysis of unconstrained English text. It consists of a
summary and three articles. {\it Morphological disambiguation} documents a
grammar for morphological (or part-of-speech) disambiguation of English, done
within the Constraint Grammar framework proposed by Fred Karlsson. The
disambiguator seeks to discard those of the alternative morphological analyses
proposed by the lexical analyser that are contextually illegitimate. The 1,100
constraints express some 23 general, essentially syntactic statements as
restrictions on the linear order of morphological tags. The error rate of the
morphological disambiguator is about ten times smaller than that of another
state-of-the-art probabilistic disambiguator, given that both are allowed to
leave some of the hardest ambiguities unresolved. This accuracy suggests the
viability of the grammar-based approach to natural language parsing, thus also
contributing to the more general debate concerning the viability of
probabilistic vs.\ linguistic techniques. {\it Experiments with heuristics}
addresses the question of how to resolve those ambiguities that survive the
morphological disambiguator. Two approaches are presented and empirically
evaluated: (i) heuristic disambiguation constraints and (ii) techniques for
learning from the fully disambiguated part of the corpus and then applying this
information to resolving remaining ambiguities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406040</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9406040</id><created>1994-06-28</created><authors><author><keyname>Osborne</keyname><forenames>Miles</forenames><affiliation>Dept. of Computer Science, University of York, York YO1 5DD, England</affiliation></author><author><keyname>Bridge</keyname><forenames>Derek</forenames><affiliation>Dept. of Computer Science, University of York, York, YO1 5DD, England</affiliation></author></authors><title>Learning unification-based grammars using the Spoken English Corpus</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><journal-ref>ICGI-94 Colloquium</journal-ref><abstract>  This paper describes a grammar learning system that combines model-based and
data-driven learning within a single framework. Our results from learning
grammars using the Spoken English Corpus (SEC) suggest that combined
model-based and data-driven learning can produce a more plausible grammar than
is the case when using either learning style isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407001</id><created>1994-07-04</created><authors><author><keyname>Trost</keyname><forenames>Harald</forenames><affiliation>Austrian Research Institute for Artificial Intelligence</affiliation></author><author><keyname>Matiasek</keyname><forenames>Johannes</forenames><affiliation>Austrian Research Institute for Artificial Intelligence</affiliation></author></authors><title>Morphology with a Null-Interface</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, uses fullname.sty, to appear in COLING'95</comments><report-no>OeFAI-TR-94-02</report-no><journal-ref>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994, pp. 141-147</journal-ref><abstract>  We present an integrated architecture for word-level and sentence-level
processing in a unification-based paradigm. The core of the system is a CLP
implementation of a unification engine for feature structures supporting
relational values. In this framework an HPSG-style grammar is implemented.
Word-level processing uses X2MorF, a morphological component based on an
extended version of two-level morphology. This component is tightly integrated
with the grammar as a relation. The advantage of this approach is that
morphology and syntax are kept logically autonomous while at the same time
minimizing interface problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407002</id><created>1994-07-04</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames><affiliation>IGM-LADL, Paris, FRANCE</affiliation></author></authors><title>Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm</title><categories>cmp-lg cs.CL</categories><comments>13 pages,Postscript uuencoded,to appear in Proceedings of COMPLEX '94</comments><abstract>  Local grammars can be represented in a very convenient way by automata. This
paper describes and illustrates an efficient algorithm for the application of
local grammars put in this form to lemmatized texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407003</id><created>1994-07-04</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames><affiliation>IGM-Ladl, Paris, France</affiliation></author></authors><title>Compact Representations by Finite-State Transducers</title><categories>cmp-lg cs.CL</categories><comments>5 pages,latex+5figures,to appear in Proceedings of ACL 94</comments><abstract>  Finite-state transducers give efficient representations of many Natural
Language phenomena. They allow to account for complex lexicon restrictions
encountered, without involving the use of a large set of complex rules
difficult to analyze. We here show that these representations can be made very
compact, indicate how to perform the corresponding minimization, and point out
interesting linguistic side-effects of this operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407004</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407004</id><created>1994-07-05</created><authors><author><keyname>Matsumoto</keyname><forenames>Mitsutaka</forenames><affiliation>Kyoto University</affiliation></author></authors><title>Japanese word sense disambiguation based on examples of synonyms</title><categories>cmp-lg cs.CL</categories><comments>41 pages, PostScript file,Compressed and encoded with &quot;uufiles&quot;</comments><abstract>  (This is not the abstract): The language is Japanese. If your printer does
not have fonts for Japases characters, the characters in figures will not be
printed out correctly. Dissertation for Bachelor's degree at Kyoto
University(Nagao lab.),March 1994.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407005</id><created>1994-07-06</created><authors><author><keyname>Chen</keyname><forenames>Hsin-Hsi</forenames><affiliation>University of Taiwan</affiliation></author><author><keyname>Lee</keyname><forenames>Yue-Shi</forenames><affiliation>University of Taiwan</affiliation></author></authors><title>A Corrective Training Algorithm for Adaptive Learning in Bag Generation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded compressed PostScript file; extract with Unix
  uudecode and uncompress</comments><journal-ref>proceedings of NeMLaP-94</journal-ref><abstract>  The sampling problem in training corpus is one of the major sources of errors
in corpus-based applications. This paper proposes a corrective training
algorithm to best-fit the run-time context domain in the application of bag
generation. It shows which objects to be adjusted and how to adjust their
probabilities. The resulting techniques are greatly simplified and the
experimental results demonstrate the promising effects of the training
algorithm from generic domain to specific domain. In general, these techniques
can be easily extended to various language models and corpus-based
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407006</id><created>1994-07-05</created><authors><author><keyname>Dowding</keyname><forenames>John</forenames></author><author><keyname>Moore</keyname><forenames>Robert</forenames></author><author><keyname>Andry</keyname><forenames>Francois</forenames></author><author><keyname>Moran</keyname><forenames>Douglas</forenames></author></authors><title>Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser</title><categories>cmp-lg cs.CL</categories><comments>8 pages, postscript</comments><journal-ref>32nd ACL, Las Cruces, New Mexico, June 1994, pp. 110-116</journal-ref><abstract>  We describe an efficient bottom-up parser that interleaves syntactic and
semantic structure building. Two techniques are presented for reducing search
by reducing local ambiguity: Limited left-context constraints are used to
reduce local syntactic ambiguity, and deferred sortal-constraint application is
used to reduce local semantic ambiguity. We experimentally evaluate these
techniques, and show dramatic reductions in both number of chart-edges and
total parsing time. The robust processing capabilities of the parser are
demonstrated in its use in improving the accuracy of a speech recognizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407007</id><created>1994-07-05</created><authors><author><keyname>Dowding</keyname><forenames>John</forenames></author><author><keyname>Gawron</keyname><forenames>Jean Mark</forenames></author><author><keyname>Appelt</keyname><forenames>Doug</forenames></author><author><keyname>Bear</keyname><forenames>John</forenames></author><author><keyname>Cherny</keyname><forenames>Lynn</forenames></author><author><keyname>Moore</keyname><forenames>Robert</forenames></author><author><keyname>Moran</keyname><forenames>Douglas</forenames></author></authors><title>GEMINI: A Natural Language System for Spoken-Language Understanding</title><categories>cmp-lg cs.CL</categories><comments>8 pages, postscript</comments><journal-ref>appeared in 31st ACL, Columbus, Ohio, June 1993, pp. 54-61</journal-ref><abstract>  Gemini is a natural language understanding system developed for spoken
language applications. The paper describes the architecture of Gemini, paying
particular attention to resolving the tension between robustness and
overgeneration. Gemini features a broad-coverage unification-based grammar of
English, fully interleaved syntactic and semantic processing in an all-paths,
bottom-up parser, and an utterance-level parser to find interpretations of
sentences that might not be analyzable as complete sentences. Gemini also
includes novel components for recognizing and correcting grammatical
disfluencies, and for doing parse preferences. This paper presents a
component-by-component view of Gemini, providing detailed relevant measurements
of size, efficiency, and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407008</id><created>1994-07-06</created><authors><author><keyname>Takeda</keyname><forenames>Koichi</forenames><affiliation>IBM Research, Tokyo Research Lab.</affiliation></author></authors><title>Tricolor DAGs for Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Kanji text in the original paper has been romanized</comments><report-no>TRL-SPI-4030</report-no><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  Machine translation (MT) has recently been formulated in terms of
constraint-based knowledge representation and unification theories, but it is
becoming more and more evident that it is not possible to design a practical MT
system without an adequate method of handling mismatches between semantic
representations in the source and target languages. In this paper, we introduce
the idea of ``information-based'' MT, which is considerably more flexible than
interlingual MT or the conventional transfer-based MT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407009</id><created>1994-07-12</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International</affiliation></author><author><keyname>Price</keyname><forenames>Patti</forenames><affiliation>SRI International</affiliation></author><author><keyname>Lyberg</keyname><forenames>Bertil</forenames><affiliation>Telia Research AB</affiliation></author></authors><title>Estimating Performance of Pipelined Spoken Language Translation Systems</title><categories>cmp-lg cs.CL</categories><comments>10 pages, Latex source. To appear in Proc. ICSLP '94</comments><abstract>  Most spoken language translation systems developed to date rely on a
pipelined architecture, in which the main stages are speech recognition,
linguistic analysis, transfer, generation and speech synthesis. When making
projections of error rates for systems of this kind, it is natural to assume
that the error rates for the individual components are independent, making the
system accuracy the product of the component accuracies.
  The paper reports experiments carried out using the SRI-SICS-Telia Research
Spoken Language Translator and a 1000-utterance sample of unseen data. The
results suggest that the naive performance model leads to serious overestimates
of system error rates, since there are in fact strong dependencies between the
components. Predicting the system error rate on the independence assumption by
simple multiplication resulted in a 16\% proportional overestimate for all
utterances, and a 19\% overestimate when only utterances of length 1-10 words
were considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407010</id><created>1994-07-12</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International</affiliation></author><author><keyname>Digalakis</keyname><forenames>Vassilios</forenames><affiliation>SRI International</affiliation></author><author><keyname>Price</keyname><forenames>Patti</forenames><affiliation>SRI International</affiliation></author></authors><title>Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists</title><categories>cmp-lg cs.CL</categories><comments>13 pages, Latex source. To appear in Proc. HLT '94</comments><abstract>  A simple and general method is described that can combine different knowledge
sources to reorder N-best lists of hypotheses produced by a speech recognizer.
The method is automatically trainable, acquiring information from both positive
and negative examples. Experiments are described in which it was tested on a
1000-utterance sample of unseen ATIS data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407011</id><created>1994-07-14</created><authors><author><keyname>Traum</keyname><forenames>David R.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Allen</keyname><forenames>James F.</forenames><affiliation>University of Rochester</affiliation></author></authors><title>Discourse Obligations in Dialogue Processing</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>In Proceedings of ACL-94</journal-ref><abstract>  We show that in modeling social interaction, particularly dialogue, the
attitude of obligation can be a useful adjunct to the popularly considered
attitudes of belief, goal, and intention and their mutual and shared
counterparts. In particular, we show how discourse obligations can be used to
account in a natural manner for the connection between a question and its
answer in dialogue and how obligations can be used along with other parts of
the discourse context to extend the coverage of a dialogue system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407012</id><created>1994-07-15</created><authors><author><keyname>Huebener</keyname><forenames>Kai</forenames><affiliation>University of Hamburg, Germany</affiliation></author><author><keyname>Carson-Berndsen</keyname><forenames>Julie</forenames><affiliation>University of Bielefeld, Germany</affiliation></author></authors><title>Phoneme Recognition Using Acoustic Events</title><categories>cmp-lg cs.CL</categories><comments>4 pages, to appear at ICSLP'94, PostScript version (compressed and
  uuencoded)</comments><report-no>Verbmobil-Report 15</report-no><abstract>  This paper presents a new approach to phoneme recognition using nonsequential
sub--phoneme units. These units are called acoustic events and are
phonologically meaningful as well as recognizable from speech signals. Acoustic
events form a phonologically incomplete representation as compared to
distinctive features. This problem may partly be overcome by incorporating
phonological constraints. Currently, 24 binary events describing manner and
place of articulation, vowel quality and voicing are used to recognize all
German phonemes. Phoneme recognition in this paradigm consists of two steps:
After the acoustic events have been determined from the speech signal, a
phonological parser is used to generate syllable and phoneme hypotheses from
the event lattice. Results obtained on a speaker--dependent corpus are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407013</id><created>1994-07-15</created><authors><author><keyname>de Marcken</keyname><forenames>Carl</forenames><affiliation>MIT Artificial Intelligence Laboratory</affiliation></author></authors><title>The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic
  Representations</title><categories>cmp-lg cs.CL</categories><comments>12 pages, postscript, to appear in ICGI '94</comments><abstract>  We present an algorithm that acquires words (pairings of phonological forms
and semantic representations) from larger utterances of unsegmented phoneme
sequences and semantic representations. The algorithm maintains from utterance
to utterance only a single coherent dictionary, and learns in the presence of
homonymy, synonymy, and noise. Test results over a corpus of utterances
generated from the Childes database of mother-child interactions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407014</id><created>1994-07-17</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Technion, Israel Institute of Technology</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Technion, Israel Institute of Technology</affiliation></author></authors><title>Abstract Machine for Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>38 pages, uuencoded compressed postscript</comments><report-no>TR #LCL 94-8, Laboratory for Computational Linguistics, Technion</report-no><abstract>  This paper describes a first step towards the definition of an abstract
machine for linguistic formalisms that are based on typed feature structures,
such as HPSG. The core design of the abstract machine is given in detail,
including the compilation process from a high-level specification language to
the abstract machine language and the implementation of the abstract
instructions. We thus apply methods that were proved useful in computer science
to the study of natural languages: a grammar specified using the formalism is
endowed with an operational semantics. Currently, our machine supports the
unification of simple feature structures, unification of sequences of such
structures, cyclic structures and disjunction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407015</id><created>1994-07-18</created><authors><author><keyname>Prevost</keyname><forenames>Scott</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Steedman</keyname><forenames>Mark</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Specifying Intonation from Context for Speech Synthesis</title><categories>cmp-lg cs.CL</categories><comments>18 pages</comments><report-no>MS-CIS-94-37/LINC LAB 273</report-no><abstract>  This paper presents a theory and a computational implementation for
generating prosodically appropriate synthetic speech in response to database
queries. Proper distinctions of contrast and emphasis are expressed in an
intonation contour that is synthesized by rule under the control of a grammar,
a discourse model, and a knowledge base. The theory is based on Combinatory
Categorial Grammar, a formalism which easily integrates the notions of
syntactic constituency, semantics, prosodic phrasing and information structure.
Results from our current implementation demonstrate the system's ability to
generate a variety of intonational possibilities for a given sentence depending
on the discourse context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407016</id><created>1994-07-19</created><updated>1994-07-20</updated><authors><author><keyname>Walker</keyname><forenames>Marilyn</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>Universite Paris 7 / CoGenTex, Inc</affiliation></author></authors><title>The Role of Cognitive Modeling in Achieving Communicative Intentions</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded compressed ps file</comments><abstract>  A discourse planner for (task-oriented) dialogue must be able to make choices
about whether relevant, but optional information (for example, the &quot;satellites&quot;
in an RST-based planner) should be communicated. We claim that effective text
planners must explicitly model aspects of the Hearer's cognitive state, such as
what the hearer is attending to and what inferences the hearer can draw, in
order to make these choices. We argue that a mere representation of the
Hearer's knowledge is inadequate. We support this claim by (1) an analysis of
naturally occurring dialogue, and (2) by simulating the generation of
discourses in a situation in which we can vary the cognitive parameters of the
hearer. Our results show that modeling cognitive state can lead to more
effective discourses (measured with respect to a simple task).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407017</id><created>1994-07-20</created><authors><author><keyname>Hoffman</keyname><forenames>Beryl</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Generating Context-Appropriate Word Orders in Turkish</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded compressed postscript file, appears in the
  Proceedings of the 7th International Generation Workshop</comments><abstract>  Turkish has considerably freer word order than English. The interpretations
of different word orders in Turkish rely on information that describes how a
sentence relates to its discourse context. To capture the syntactic features of
a free word order language, I present an adaptation of Combinatory Categorial
Grammars called {}-CCGs (set-CCGs). In {}-CCGs, a verb's subcategorization
requirements are relaxed so that it requires a set of arguments without
specifying their linear order. I integrate a level of information structure,
representing pragmatic functions such as topic and focus, with {}-CCGs to allow
certain pragmatic distinctions in meaning to influence the word order of a
sentence in a compositional way. Finally, I discuss how this strategy is used
within an implemented generation system which produces Turkish sentences with
context-appropriate word orders in a simple database query task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407018</id><created>1994-07-21</created><updated>1994-07-22</updated><authors><author><keyname>R&#xf6;sner</keyname><forenames>Dietmar</forenames><affiliation>FAW Ulm, Ulm, Germany</affiliation></author><author><keyname>Stede</keyname><forenames>Manfred</forenames><affiliation>University of Toronto and FAW Ulm</affiliation></author></authors><title>Generating Multilingual Documents from a Knowledge Base: The TECHDOC
  Project</title><categories>cmp-lg cs.CL</categories><comments>5 pages, TEX + PS figure</comments><abstract>  TECHDOC is an implemented system demonstrating the feasibility of generating
multilingual technical documents on the basis of a language-independent
knowledge base. Its application domain is user and maintenance instructions,
which are produced from underlying plan structures representing the activities,
the participating objects with their properties, relations, and so on. This
paper gives a brief outline of the system architecture and discusses some
recent developments in the project: the addition of actual event simulation in
the KB, steps towards a document authoring tool, and a multimodal user
interface. (slightly corrected version of a paper to appear in: COLING 94,
Proceedings)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407019</id><created>1994-07-22</created><authors><author><keyname>Wiebe</keyname><forenames>Janyce M.</forenames><affiliation>New Mexico State University</affiliation></author></authors><title>Tracking Point of View in Narrative</title><categories>cmp-lg cs.CL</categories><comments>55 pages, uuencoded compressed ps, appears in Computational
  Linguistics 20:2, pp. 233-287 (electronic version does not reflect all
  copy-editing changes)</comments><journal-ref>Computational Lingustics 20:2, 233-287</journal-ref><abstract>  Third-person fictional narrative text is composed not only of passages that
objectively narrate events, but also of passages that present characters'
thoughts, perceptions, and inner states. Such passages take a character's
``psychological point of view''. A language understander must determine the
current psychological point of view in order to distinguish the beliefs of the
characters from the facts of the story, to correctly attribute beliefs and
other attitudes to their sources, and to understand the discourse relations
among sentences. Tracking the psychological point of view is not a trivial
problem, because many sentences are not explicitly marked for point of view,
and whether the point of view of a sentence is objective or that of a character
(and if the latter, which character it is) often depends on the context in
which the sentence appears. Tracking the psychological point of view is the
problem addressed in this work. The approach is to seek, by extensive
examinations of naturally-occurring narrative, regularities in the ways that
authors manipulate point of view, and to develop an algorithm that tracks point
of view on the basis of the regularities found. This paper presents this
algorithm, gives demonstrations of an implemented system, and describes the
results of some preliminary empirical studies, which lend support to the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407020</id><created>1994-07-24</created><updated>1994-07-24</updated><authors><author><keyname>Lewis</keyname><forenames>David D.</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author><author><keyname>Gale</keyname><forenames>William A.</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author></authors><title>A Sequential Algorithm for Training Text Classifiers</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded, compressed PostScript; Proc. SIGIR-94 LaTex
  available from lewis@research.att.com</comments><abstract>  The ability to cheaply train text classifiers is critical to their use in
information retrieval, content analysis, natural language processing, and other
tasks involving data which is partly or fully textual. An algorithm for
sequential sampling during machine learning of statistical classifiers was
developed and tested on a newswire text categorization task. This method, which
we call uncertainty sampling, reduced by as much as 500-fold the amount of
training data that would have to be manually classified to achieve a given
level of effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407021</id><created>1994-07-25</created><authors><author><keyname>Fung</keyname><forenames>Pascale</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Church</keyname><forenames>Kenneth</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author></authors><title>K-vec: A New Approach for Aligning Parallel Texts</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded, compressed PostScript; Proc. COLING-94</comments><abstract>  Various methods have been proposed for aligning texts in two or more
languages such as the Canadian Parliamentary Debates(Hansards). Some of these
methods generate a bilingual lexicon as a by-product. We present an alternative
alignment strategy which we call K-vec, that starts by estimating the lexicon.
For example, it discovers that the English word &quot;fisheries&quot; is similar to the
French &quot;pe^ches&quot; by noting that the distribution of &quot;fisheries&quot; in the English
text is similar to the distribution of &quot;pe^ches&quot; in the French. K-vec does not
depend on sentence boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407022</id><created>1994-07-26</created><authors><author><keyname>van der Eijk</keyname><forenames>Pim</forenames><affiliation>Digital Equipment Corporation</affiliation></author></authors><title>Comparative Discourse Analysis of Parallel Texts</title><categories>cmp-lg cs.CL</categories><comments>11 pages uuencoded compressed Postscript</comments><abstract>  A quantitative representation of discourse structure can be computed by
measuring lexical cohesion relations among adjacent blocks of text. These
representations have been proposed to deal with sub-topic text segmentation. In
a parallel corpus, similar representations can be derived for versions of a
text in various languages. These can be used for parallel segmentation and as
an alternative measure of text-translation similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407023</id><created>1994-07-26</created><authors><author><keyname>Kiraz</keyname><forenames>George</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Multi-Tape Two-Level Morphology: A Case Study in Semitic Non-linear
  Morphology</title><categories>cmp-lg cs.CL</categories><comments>to appear in COLING-94, uuencoded, compressed .ps file, 7 pages</comments><abstract>  This paper presents an implemented multi-tape two-level model capable of
describing Semitic non-linear morphology. The computational framework behind
the current work is motivated by Kay (1987); the formalism presented here is an
extension to the formalism reported by Pulman and Hepple (1993). The objectives
of the current work are: to stay as close as possible, in spirit, to standard
two-level morphology, to stay close to the linguistic description of Semitic
stems, and to present a model which can be used with ease by the Semitist. The
paper illustrates that if finite-state transducers (FSTs) in a standard
two-level morphology model are replaced with multi-tape auxiliary versions
(AFSTs), one can account for Semitic root-and-pattern morphology using high
level notation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407024</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407024</id><created>1994-07-27</created><authors><author><keyname>Lin</keyname><forenames>Dekang</forenames><affiliation>Department of Computer Science, University of Manitoba</affiliation></author></authors><title>PRINCIPAR---An Efficient, Broad-coverage, Principle-based Parser</title><categories>cmp-lg cs.CL</categories><comments>To appear in COLING-94</comments><abstract>  We present an efficient, broad-coverage, principle-based parser for English.
The parser has been implemented in C++ and runs on SUN Sparcstations with
X-windows. It contains a lexicon with over 90,000 entries, constructed
automatically by applying a set of extraction and conversion rules to entries
from machine readable dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407025</id><created>1994-07-28</created><authors><author><keyname>Rose'</keyname><forenames>Carolyn Penstein</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Waibel</keyname><forenames>Alex</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Recovering From Parser Failures: A Hybrid Statistical/Symbolic Approach</title><categories>cmp-lg cs.CL</categories><abstract>  We describe an implementation of a hybrid statistical/symbolic approach to
repairing parser failures in a speech-to-speech translation system. We describe
a module which takes as input a fragmented parse and returns a repaired meaning
representation. It negotiates with the speaker about what the complete meaning
of the utterance is by generating hypotheses about how to fit the fragments of
the partial parse together into a coherent meaning representation. By drawing
upon both statistical and symbolic information, it constrains its repair
hypotheses to those which are both likely and meaningful. Because it updates
its statistical model during use, it improves its performance over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407026</id><created>1994-07-29</created><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author><author><keyname>Kuruoz</keyname><forenames>Ilker</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author></authors><title>Tagging and Morphological Disambiguation of Turkish Text</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of 4th ACL-ANLP Conf. uuencoded gzip'ed
  postscript file, 6 pages</comments><report-no>Bilkent University CS Dept. Tech Report NO: BU-CEIS-9416</report-no><abstract>  Automatic text tagging is an important component in higher level analysis of
text corpora, and its output can be used in many natural language processing
applications. In languages like Turkish or Finnish, with agglutinative
morphology, morphological disambiguation is a very crucial process in tagging,
as the structures of many lexical forms are morphologically ambiguous. This
paper describes a POS tagger for Turkish text based on a full-scale two-level
specification of Turkish morphology that is based on a lexicon of about 24,000
root words. This is augmented with a multi-word and idiomatic construct
recognizer, and most importantly morphological disambiguator based on local
neighborhood constraints, heuristics and limited amount of statistical
information. The tagger also has functionality for statistics compilation and
fine tuning of the morphological analyzer, such as logging erroneous
morphological parses, commonly used roots, etc. Preliminary results indicate
that the tagger can tag about 98-99\% of the texts accurately with very minimal
user intervention. Furthermore for sentences morphologically disambiguated with
the tagger, an LFG parser developed for Turkish, generates, on the average,
50\% less ambiguous parses and parses almost 2.5 times faster. The tagging
functionality is not specific to Turkish, and can be applied to any language
with a proper morphological analysis interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407027</id><created>1994-07-29</created><authors><author><keyname>Gerdemann</keyname><forenames>Dale</forenames><affiliation>University of Tuebingen, Germany</affiliation></author></authors><title>Parsing as Tree Traversal</title><categories>cmp-lg cs.CL</categories><comments>COLING 94 paper, Postscript, compressed and uuencoded</comments><abstract>  This paper presents a unified approach to parsing, in which top-down,
bottom-up and left-corner parsers are related to preorder, postorder and
inorder tree traversals. It is shown that the simplest bottom-up and
left-corner parsers are left recursive and must be converted using an extended
Greibach normal form. With further partial execution, the bottom-up and
left-corner parsers collapse together as in the BUP parser of Matsumoto.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407028</id><created>1994-07-29</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Chander</keyname><forenames>Ishwar</forenames><affiliation>USC/Information Sciences Institute</affiliation></author></authors><title>Automated Postediting of Documents</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Compressed and uuencoded postscript. To appear: AAAI-94</comments><abstract>  Large amounts of low- to medium-quality English texts are now being produced
by machine translation (MT) systems, optical character readers (OCR), and
non-native speakers of English. Most of this text must be postedited by hand
before it sees the light of day. Improving text quality is tedious work, but
its automation has not received much research attention. Anyone who has
postedited a technical report or thesis written by a non-native speaker of
English knows the potential of an automated postediting system. For the case of
MT-generated text, we argue for the construction of postediting modules that
are portable across MT systems, as an alternative to hardcoding improvements
inside any one system. As an example, we have built a complete self-contained
postediting module for the task of article selection (a, an, the) for English
noun phrases. This is a notoriously difficult problem for Japanese-English MT.
Our system contains over 200,000 rules derived automatically from online text
resources. We report on learning algorithms, accuracy, and comparisons with
human performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407029</id><created>1994-07-29</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Luk</keyname><forenames>Steve K.</forenames><affiliation>USC/Information Sciences Institute</affiliation></author></authors><title>Building a Large-Scale Knowledge Base for Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Compressed and uuencoded postscript. To appear: AAAI-94</comments><abstract>  Knowledge-based machine translation (KBMT) systems have achieved excellent
results in constrained domains, but have not yet scaled up to newspaper text.
The reason is that knowledge resources (lexicons, grammar rules, world models)
must be painstakingly handcrafted from scratch. One of the hypotheses being
tested in the PANGLOSS machine translation project is whether or not these
resources can be semi-automatically acquired on a very large scale. This paper
focuses on the construction of a large ontology (or knowledge base, or world
model) for supporting KBMT. It contains representations for some 70,000
commonly encountered objects, processes, qualities, and relations. The ontology
was constructed by merging various online dictionaries, semantic networks, and
bilingual resources, through semi-automatic methods. Some of these methods
(e.g., conceptual matching of semantic taxonomies) are broadly applicable to
problems of importing/exporting knowledge from one KB to another. Other methods
(e.g., bilingual matching) allow a knowledge engineer to build up an index to a
KB in a second language, such as Spanish or Japanese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9407030</id><created>1994-07-30</created><authors><author><keyname>Trujillo</keyname><forenames>Arturo</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Computing FIRST and FOLLOW Functions for Feature-Theoretic Grammars</title><categories>cmp-lg cs.CL</categories><comments>6 pages, COLING 94, compressed, uuencoded PostScript, 93KB</comments><abstract>  This paper describes an algorithm for the computation of FIRST and FOLLOW
sets for use with feature-theoretic grammars in which the value of the sets
consists of pairs of feature-theoretic categories. The algorithm preserves as
much information from the grammars as possible, using negative restriction to
define equivalence classes. Addition of a simple data structure leads to an
order of magnitude improvement in execution time over a naive implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408001</id><created>1994-08-01</created><updated>1994-08-02</updated><authors><author><keyname>Gerdemann</keyname><forenames>Dale</forenames><affiliation>University of Tuebingen, Germany</affiliation></author><author><keyname>King</keyname><forenames>Paul John</forenames><affiliation>University of Tuebingen, Germany</affiliation></author></authors><title>The Correct and Efficient Implementation of Appropriateness
  Specifications for Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>5 pages, postscript, compressed and uuencoded (uudecodes to
  gerd_king.ps.Z)</comments><journal-ref>COLING 94 paper</journal-ref><abstract>  In this paper, we argue that type inferencing incorrectly implements
appropriateness specifications for typed feature structures, promote a
combination of type resolution and unfilling as a correct and efficient
alternative, and consider the expressive limits of this alternative approach.
Throughout, we use feature cooccurence restrictions as illustration and
linguistic motivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408002</id><created>1994-08-01</created><authors><author><keyname>Kiraz</keyname><forenames>George A.</forenames></author></authors><title>Computational Analyses of Arabic Morphology</title><categories>cmp-lg cs.CL</categories><comments>to appear in Narayanan A., Ditters E. (eds). The Linguistic
  Computation of Arabic. uuencoded, compressed .ps file, 27 pages</comments><abstract>  This paper demonstrates how a (multi-tape) two-level formalism can be used to
write two-level grammars for Arabic non-linear morphology using a high level,
but computationally tractable, notation. Three illustrative grammars are
provided based on CV-, moraic- and affixational analyses. These are
complemented by a proposal for handling the hitherto computationally untreated
problem of the broken plural. It will be shown that the best grammars for
describing Arabic non-linear morphology are moraic in the case of templatic
stems, and affixational in the case of a-templatic stems. The paper will
demonstrate how the broken plural can be derived under two-level theory via the
`implicit' derivation of the singular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408003</id><created>1994-08-02</created><authors><author><keyname>King</keyname><forenames>Paul John</forenames><affiliation>University of Tuebingen, Germany</affiliation></author></authors><title>Typed Feature Structures as Descriptions</title><categories>cmp-lg cs.CL</categories><comments>COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica)</comments><abstract>  A description is an entity that can be interpreted as true or false of an
object, and using feature structures as descriptions accrues several
computational benefits. In this paper, I create an explicit interpretation of a
typed feature structure used as a description, define the notion of a
satisfiable feature structure, and create a simple and effective algorithm to
decide if a feature structure is satisfiable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408004</id><created>1994-08-02</created><authors><author><keyname>Fordham</keyname><forenames>Andrew</forenames><affiliation>Dept. of Sociology, University of Surrey, UK</affiliation></author><author><keyname>Crocker</keyname><forenames>Matthew</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh, UK</affiliation></author></authors><title>Parsing with Principles and Probabilities</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><abstract>  This paper is an attempt to bring together two approaches to language
analysis. The possible use of probabilistic information in principle-based
grammars and parsers is considered, including discussion on some theoretical
and computational problems that arise. Finally a partial implementation of
these ideas is presented, along with some preliminary results from testing on a
small set of sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408005</id><created>1994-08-02</created><authors><author><keyname>Christ</keyname><forenames>Oliver</forenames><affiliation>IMS Stuttgart, Germany</affiliation></author></authors><title>A Modular and Flexible Architecture for an Integrated Corpus Query System</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded gzip'ped PostScript; presented at COMPLEX'94</comments><abstract>  The paper describes the architecture of an integrated and extensible corpus
query system developed at the University of Stuttgart and gives examples of
some of the modules realized within this architecture. The modules form the
core of a corpus workbench. Within the proposed architecture, information
required for the evaluation of queries may be derived from different knowledge
sources (the corpus text, databases, on-line thesauri) and by different means:
either through direct lookup in a database or by calling external tools which
may infer the necessary information at the time of query evaluation. The
information available and the method of information access can be stated
declaratively and individually for each corpus, leading to a flexible,
extensible and modular corpus workbench.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408006</id><created>1994-08-03</created><authors><author><keyname>Ballim</keyname><forenames>Afzal</forenames><affiliation>ISSCO, Geneva</affiliation></author><author><keyname>Russell</keyname><forenames>Graham</forenames><affiliation>ISSCO, Geneva</affiliation></author></authors><title>LHIP: Extended DCGs for Configurable Robust Parsing</title><categories>cmp-lg cs.CL</categories><comments>10 pages, in Proc. Coling94</comments><journal-ref>Proc. Coling 1994, vol.1 pp.501-507</journal-ref><abstract>  We present LHIP, a system for incremental grammar development using an
extended DCG formalism. The system uses a robust island-based parsing method
controlled by user-defined performance thresholds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408007</id><created>1994-08-13</created><authors><author><keyname>Siegel</keyname><forenames>Eric V.</forenames><affiliation>Columbia University</affiliation></author><author><keyname>McKeown</keyname><forenames>Kathleen R.</forenames><affiliation>Columbia University</affiliation></author></authors><title>Emergent Linguistic Rules from Inducing Decision Trees: Disambiguating
  Discourse Clue Words</title><categories>cmp-lg cs.CL</categories><journal-ref>AAAI94 proceedings</journal-ref><abstract>  We apply decision tree induction to the problem of discourse clue word sense
disambiguation with a genetic algorithm. The automatic partitioning of the
training set which is intrinsic to decision tree induction gives rise to
linguistically viable rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408008</id><created>1994-08-15</created><authors><author><keyname>Rowe</keyname><forenames>Neil C.</forenames><affiliation>Code CS/Rp, Department of Computer Science, Naval Postgraduate School, Monterey</affiliation></author></authors><title>Statistical versus symbolic parsing for captioned-information retrieval</title><categories>cmp-lg cs.CL</categories><comments>Workshop on the Balancing Act, ACL-94, Las Cruces NM, July 1994</comments><abstract>  We discuss implementation issues of MARIE-1, a mostly symbolic parser fully
implemented, and MARIE-2, a more statistical parser partially implemented. They
address a corpus of 100,000 picture captions. We argue that the mixed approach
of MARIE-2 should be better for this corpus because its algorithms (not data)
are simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408009</id><created>1994-08-16</created><authors><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory</affiliation></author><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Computational Linguistics, University of Helsinki</affiliation></author></authors><title>Tagging accurately -- Don't guess if you know</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Proc. ANLP94, uuencoded and gzipped postscript</comments><abstract>  We discuss combining knowledge-based (or rule-based) and statistical
part-of-speech taggers. We use two mature taggers, ENGCG and Xerox Tagger, to
independently tag the same text and combine the results to produce a fully
disambiguated text. In a 27000 word test sample taken from a previously unseen
corpus we achieve 98.5% accuracy. This paper presents the data in detail. We
describe the problems we encountered in the course of combining the two taggers
and discuss the problem of evaluating taggers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408010</id><created>1994-08-19</created><authors><author><keyname>Ueberla</keyname><forenames>Joerg P.</forenames><affiliation>Simon Fraser University, Vancouver, Canada</affiliation></author></authors><title>On Using Selectional Restriction in Language Models for Speech
  Recognition</title><categories>cmp-lg cs.CL</categories><comments>feedback is welcome to ueberla@cs.sfu.ca</comments><report-no>CMPT TR 94-03</report-no><abstract>  In this paper, we investigate the use of selectional restriction -- the
constraints a predicate imposes on its arguments -- in a language model for
speech recognition. We use an un-tagged corpus, followed by a public domain
tagger and a very simple finite state machine to obtain verb-object pairs from
unrestricted English text. We then measure the impact the knowledge of the verb
has on the prediction of the direct object in terms of the perplexity of a
cluster-based language model. The results show that even though a clustered
bigram is more useful than a verb-object model, the combination of the two
leads to an improvement over the clustered bigram model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408011</id><created>1994-08-22</created><authors><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames><affiliation>Hebrew University</affiliation></author><author><keyname>Lee</keyname><forenames>Lillian</forenames><affiliation>Harvard University</affiliation></author></authors><title>Distributional Clustering of English Words</title><categories>cmp-lg cs.CL</categories><comments>8 pages, appeared in the proceedings of ACL-93, Columbus, Ohio</comments><abstract>  We describe and experimentally evaluate a method for automatically clustering
words according to their distribution in particular syntactic contexts.
Deterministic annealing is used to find lowest distortion sets of clusters. As
the annealing parameter increases, existing clusters become unstable and
subdivide, yielding a hierarchical ``soft'' clustering of the data. Clusters
are used as the basis for class models of word coocurrence, and the models
evaluated with respect to held-out test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408012</id><created>1994-08-24</created><authors><author><keyname>Chen</keyname><forenames>Hsin-Hsi</forenames><affiliation>National Taiwan University</affiliation></author><author><keyname>Lee</keyname><forenames>Yue-Shi</forenames><affiliation>National Taiwan University</affiliation></author></authors><title>Approximate N-Gram Markov Model for Natural Language Generation</title><categories>cmp-lg cs.CL</categories><comments>to appear in proceedings of QUALICO-94, 6 pages, uuencoded compressed
  Postscript file; extract with Unix uudecode and uncompress</comments><abstract>  This paper proposes an Approximate n-gram Markov Model for bag generation.
Directed word association pairs with distances are used to approximate
(n-1)-gram and n-gram training tables. This model has parameters of word
association model, and merits of both word association model and Markov Model.
The training knowledge for bag generation can be also applied to lexical
selection in machine translation design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408013</id><created>1994-08-23</created><authors><author><keyname>Alshawi</keyname><forenames>Hiyan</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author></authors><title>Training and Scaling Preference Functions for Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>To appear in Computational Linguistics (probably volume 20, December
  94). LaTeX, 21 pages</comments><abstract>  We present an automatic method for weighting the contributions of preference
functions used in disambiguation. Initial scaling factors are derived as the
solution to a least-squares minimization problem, and improvements are then
made by hill-climbing. The method is applied to disambiguating sentences in the
ATIS (Air Travel Information System) corpus, and the performance of the
resulting scaling factors is compared with hand-tuned factors. We then focus on
one class of preference function, those based on semantic lexical collocations.
Experimental results are presented showing that such functions vary
considerably in selecting correct analyses. In particular we define a function
that performs significantly better than ones based on mutual information and
likelihood ratios of lexical associations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408014</id><created>1994-08-23</created><authors><author><keyname>Alshawi</keyname><forenames>Hiyan</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author></authors><title>Qualitative and Quantitative Models of Speech Translation</title><categories>cmp-lg cs.CL</categories><comments>Appeared in proceedings of the ACL workshop &quot;The Balancing Act,
  Combining Symbolic and Statistical Approaches to Language&quot;, Las Cruces NM,
  July 1994. LaTeX, 24 pages</comments><abstract>  This paper compares a qualitative reasoning model of translation with a
quantitative statistical model. We consider these models within the context of
two hypothetical speech translation systems, starting with a logic-based design
and pointing out which of its characteristics are best preserved or eliminated
in moving to the second, quantitative design. The quantitative language and
translation models are based on relations between lexical heads of phrases.
Statistical parameters for structural dependency, lexical transfer, and linear
order are used to select a set of implicit relations between words in a source
utterance, a corresponding set of relations between target language words, and
the most likely translation of the original utterance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408015</id><created>1994-08-24</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames><affiliation>Mitsubishi Electric Research Laboratories, Cambridge, Mass.</affiliation></author></authors><title>Experimentally Evaluating Communicative Strategies: The Effect of the
  Task</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex with psfig, lingmacros.sty, available at
  ftp://atlantic.merl.com/pub/walker/aaai94.ps.Z</comments><journal-ref>Proceedings of AAAI 94, Seattle</journal-ref><abstract>  Effective problem solving among multiple agents requires a better
understanding of the role of communication in collaboration. In this paper we
show that there are communicative strategies that greatly improve the
performance of resource-bounded agents, but that these strategies are highly
sensitive to the task requirements, situation parameters and agents' resource
limitations. We base our argument on two sources of evidence: (1) an analysis
of a corpus of 55 problem solving dialogues, and (2) experimental simulations
of collaborative problem solving dialogues in an experimental world,
Design-World, where we parameterize task requirements, agents' resources and
communicative strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408016</id><created>1994-08-31</created><authors><author><keyname>Meurers</keyname><forenames>Walt Detmar</forenames><affiliation>SFB 340/B4, University of Tuebingen, Germany</affiliation></author></authors><title>On Implementing an HPSG theory -- Aspects of the logical architecture,
  the formalization, and the implementation of head-driven phrase structure
  grammars</title><categories>cmp-lg cs.CL</categories><comments>paper (56 pages), appendix (28 pages), format: LaTeX, tared,
  compressed, and uuencoded (using uufiles) used: chicago.sty, tree-dvips.sty,
  tree-dvips.pro, psfig.tex, 2up.sty (all available from cmp-lg server)</comments><report-no>SFB report Nr.58</report-no><abstract>  The paper presents some aspects involved in the formalization and
implementation of HPSG theories. As basis, the logical setups of Carpenter
(1992) and King (1989, 1994) are briefly compared regarding their usefulness as
basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG
theories in the HPSGII architecture and in various computational systems (ALE,
Troll, CUF, and TFS) are discussed. Beside a formal characterization of the
possibilities, the paper investigates the specific choices for constraints with
certain linguistic motivations, i.e. the lexicon, structure licencing, and
grammatical principles. An ALE implementation of a theory for German proposed
by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is
included in the appendix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408017</id><created>1994-08-31</created><authors><author><keyname>Mahesh</keyname><forenames>Kavi</forenames><affiliation>Georgia Institute of Technology</affiliation></author></authors><title>Reaping the Benefits of Interactive Syntax and Semantics</title><categories>cmp-lg cs.CL</categories><comments>3 pages, uses latex-acl.sty macro</comments><journal-ref>appeared in ACL-94 Proceedings</journal-ref><abstract>  Semantic feedback is an important source of information that a parser could
use to deal with local ambiguities in syntax. However, it is difficult to
devise a systematic communication mechanism for interactive syntax and
semantics. In this article, I propose a variant of left-corner parsing to
define the points at which syntax and semantics should interact, an account of
grammatical relations and thematic roles to define the content of the
communication, and a conflict resolution strategy based on independent
preferences from syntax and semantics. The resulting interactive model has been
implemented in a program called COMPERE and shown to account for a wide variety
of psycholinguistic data on structural and lexical ambiguities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408018</id><created>1994-08-31</created><authors><author><keyname>Mahesh</keyname><forenames>Kavi</forenames><affiliation>Georgia Institute of Technology</affiliation></author><author><keyname>Eiselt</keyname><forenames>Kurt P.</forenames><affiliation>Georgia Institute of Technology</affiliation></author></authors><title>Uniform Representations for Syntax-Semantics Arbitration</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses cogsci94.sty macro</comments><journal-ref>appeared in Cogsci94 Proceedings</journal-ref><abstract>  Psychological investigations have led to considerable insight into the
working of the human language comprehension system. In this article, we look at
a set of principles derived from psychological findings to argue for a
particular organization of linguistic knowledge along with a particular
processing strategy and present a computational model of sentence processing
based on those principles. Many studies have shown that human sentence
comprehension is an incremental and interactive process in which semantic and
other higher-level information interacts with syntactic information to make
informed commitments as early as possible at a local ambiguity. Early
commitments may be made by using top-down guidance from knowledge of different
types, each of which must be applicable independently of others. Further
evidence from studies of error recovery and delayed decisions points toward an
arbitration mechanism for combining syntactic and semantic information in
resolving ambiguities. In order to account for all of the above, we propose
that all types of linguistic knowledge must be represented in a common form but
must be separable so that they can be applied independently of each other and
integrated at processing time by the arbitrator. We present such a uniform
representation and a computational model called COMPERE based on the
representation and the processing strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408019</id><created>1994-08-31</created><authors><author><keyname>Mahesh</keyname><forenames>Kavi</forenames><affiliation>Georgia Institute of Technology</affiliation></author></authors><title>Building a Parser That can Afford to Interact with Semantics</title><categories>cmp-lg cs.CL</categories><comments>2 pages, uses latex-acl.sty macro</comments><journal-ref>appeared in AAAI-94 Proceedings</journal-ref><abstract>  Natural language understanding programs get bogged down by the multiplicity
of possible syntactic structures while processing real world texts that human
understanders do not have much difficulty with. In this work, I analyze the
relationships between parsing strategies, the degree of local ambiguity
encountered by them, and semantic feedback to syntax, and propose a parsing
algorithm called {\em Head-Signaled Left Corner Parsing} (HSLC) that minimizes
local ambiguities while supporting interactive syntactic and semantic analysis.
Such a parser has been implemented in a sentence understanding program called
COMPERE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408020</id><created>1994-08-31</created><authors><author><keyname>Eiselt</keyname><forenames>Kurt P.</forenames><affiliation>College of Computing, Georgia Tech</affiliation></author><author><keyname>Mahesh</keyname><forenames>Kavi</forenames><affiliation>College of Computing, Georgia Tech</affiliation></author><author><keyname>Holbrook</keyname><forenames>Jennifer K.</forenames><affiliation>Dept. of Psychology, Albion College</affiliation></author></authors><title>Having Your Cake and Eating It Too: Autonomy and Interaction in a Model
  of Sentence Processing</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses aaai.sty macro</comments><journal-ref>appeared in AAAI-93 Proceedings</journal-ref><abstract>  Is the human language understander a collection of modular processes
operating with relative autonomy, or is it a single integrated process? This
ongoing debate has polarized the language processing community, with two
fundamentally different types of model posited, and with each camp concluding
that the other is wrong. One camp puts forth a model with separate processors
and distinct knowledge sources to explain one body of data, and the other
proposes a model with a single processor and a homogeneous, monolithic
knowledge source to explain the other body of data. In this paper we argue that
a hybrid approach which combines a unified processor with separate knowledge
sources provides an explanation of both bodies of data, and we demonstrate the
feasibility of this approach with the computational model called COMPERE. We
believe that this approach brings the language processing community
significantly closer to offering human-like language processing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9408021</id><created>1994-08-31</created><authors><author><keyname>Holbrook</keyname><forenames>Jennifer K.</forenames><affiliation>Dept. of Psychology, Albion College</affiliation></author><author><keyname>Eiselt</keyname><forenames>Kurt P.</forenames><affiliation>College of Computing, Georgia Tech</affiliation></author><author><keyname>Mahesh</keyname><forenames>Kavi</forenames><affiliation>College of Computing, Georgia Tech</affiliation></author></authors><title>A Unified Process Model of Syntactic and Semantic Error Recovery in
  Sentence Understanding</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses cogsci94.sty macro</comments><journal-ref>appeared in Cogsci-92 Conference Proceedings</journal-ref><abstract>  The development of models of human sentence processing has traditionally
followed one of two paths. Either the model posited a sequence of processing
modules, each with its own task-specific knowledge (e.g., syntax and
semantics), or it posited a single processor utilizing different types of
knowledge inextricably integrated into a monolithic knowledge base. Our
previous work in modeling the sentence processor resulted in a model in which
different processing modules used separate knowledge sources but operated in
parallel to arrive at the interpretation of a sentence. One highlight of this
model is that it offered an explanation of how the sentence processor might
recover from an error in choosing the meaning of an ambiguous word. Recent
experimental work by Laurie Stowe strongly suggests that the human sentence
processor deals with syntactic error recovery using a mechanism very much like
that proposed by our model of semantic error recovery. Another way to interpret
Stowe's finding is this: the human sentence processor consists of a single
unified processing module utilizing multiple independent knowledge sources in
parallel. A sentence processor built upon this architecture should at times
exhibit behavior associated with modular approaches, and at other times act
like an integrated system. In this paper we explore some of these ideas via a
prototype computational model of sentence processing called COMPERE, and
propose a set of psychological experiments for testing our theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409001</id><created>1994-09-05</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Chander</keyname><forenames>Ishwar</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Haines</keyname><forenames>Matthew</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Hatzivassiloglou</keyname><forenames>Vasileios</forenames><affiliation>Columbia Univ.</affiliation></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Iida</keyname><forenames>Masayo</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Luk</keyname><forenames>Steve K.</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Okumura</keyname><forenames>Akitoshi</forenames><affiliation>NEC</affiliation></author><author><keyname>Whitney</keyname><forenames>Richard</forenames><affiliation>USC/ISI</affiliation></author><author><keyname>Yamada</keyname><forenames>Kenji</forenames><affiliation>USC/ISI</affiliation></author></authors><title>Integrating Knowledge Bases and Statistics in MT</title><categories>cmp-lg cs.CL</categories><comments>8 pages, compressed, uuencoded postscript</comments><journal-ref>Proc Association for Machine Translation in the Americas (AMTA-94)</journal-ref><abstract>  We summarize recent machine translation (MT) research at the Information
Sciences Institute of USC, and we describe its application to the development
of a Japanese-English newspaper MT system. Our work aims at scaling up
grammar-based, knowledge-based MT techniques. This scale-up involves the use of
statistical methods, both in acquiring effective knowledge resources and in
making reasonable linguistic choices in the face of knowledge gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409002</id><created>1994-09-06</created><updated>1996-09-10</updated><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author></authors><title>Conceptual Association for Compound Noun Analysis</title><categories>cmp-lg cs.CL</categories><comments>3 pages, postscript only, replaced because original postscript
  version incompatible with some printers</comments><journal-ref>Proceedings of the Student Session, 32nd Annual Meeting of the
  Association for Computational Linguistics, Las Cruces, NM., 1994 pp337-339</journal-ref><abstract>  This paper describes research toward the automatic interpretation of compound
nouns using corpus statistics. An initial study aimed at syntactic
disambiguation is presented. The approach presented bases associations upon
thesaurus categories. Association data is gathered from unambiguous cases
extracted from a corpus and is then applied to the analysis of ambiguous
compound nouns. While the work presented is still in progress, a first attempt
to syntactically analyse a test set of 244 examples shows 75% correctness.
Future work is aimed at improving this accuracy and extending the technique to
assign semantic role information, thus producing a complete interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409003</id><created>1994-09-06</created><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author><author><keyname>Dras</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author></authors><title>A Probabilistic Model of Compound Nouns</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uuencoded compressed postscript, please ignore any undefined
  command error at end</comments><journal-ref>7th Australian Joint Conference on AI, 1994</journal-ref><abstract>  Compound nouns such as example noun compound are becoming more common in
natural language and pose a number of difficult problems for NLP systems,
notably increasing the complexity of parsing. In this paper we develop a
probabilistic model for syntactically analysing such compounds. The model
predicts compound noun structures based on knowledge of affinities between
nouns, which can be acquired from a corpus. Problems inherent in this
corpus-based approach are addressed: data sparseness is overcome by the use of
semantically motivated word classes and sense ambiguity is explicitly handled
in the model. An implementation based on this model is described in Lauer
(1994) and correctly parses 77% of the test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409004</id><created>1994-09-07</created><authors><author><keyname>Ribas</keyname><forenames>Francesc</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author></authors><title>An Experiment on Learning Appropriate Selectional Restrictions from a
  Parsed Corpus</title><categories>cmp-lg cs.CL</categories><comments>11 pages</comments><journal-ref>COLING-94 Proceedings, 769-774</journal-ref><abstract>  We present a methodology to extract Selectional Restrictions at a variable
level of abstraction from phrasally analyzed corpora. The method relays in the
use of a wide-coverage noun taxonomy and a statistical measure of the
co-occurrence of linguistic items. Some experimental results about the
performance of the method are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409005</id><created>1994-09-07</created><authors><author><keyname>Ersan</keyname><forenames>Ebru</forenames><affiliation>Brown University</affiliation></author><author><keyname>Akman</keyname><forenames>Varol</forenames><affiliation>Bilkent University, Ankara</affiliation></author></authors><title>Focusing for Pronoun Resolution in English Discourse: An Implementation</title><categories>cmp-lg cs.CL</categories><comments>iii + 49 pages, compressed, uuencoded Postscript file; revised
  version of the first author's Bilkent M.S. thesis, written under the
  supervision of the second author; notify Akman via e-mail
  (akman@cs.bilkent.edu.tr) or fax (+90-312-266-4126) if you are unable to
  obtain hardcopy, he'll work out something</comments><report-no>BU-CEIS-94-29</report-no><abstract>  Anaphora resolution is one of the most active research areas in natural
language processing. This study examines focusing as a tool for the resolution
of pronouns which are a kind of anaphora. Focusing is a discourse phenomenon
like anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and
devised several algorithms to resolve definite anaphora including pronouns. She
presented her theory in a computational framework but did not generally
implement the algorithms. Her algorithms related to focusing and pronoun
resolution are implemented in this thesis. This implementation provides a
better comprehension of the theory both from a conceptual and a computational
point of view. The resulting program is tested on different discourse segments,
and evaluation and analysis of the experiments are presented together with the
statistical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409006</id><created>1994-09-07</created><authors><author><keyname>Ersan</keyname><forenames>Murat</forenames><affiliation>Brown University</affiliation></author><author><keyname>Akman</keyname><forenames>Varol</forenames><affiliation>Bilkent University, Ankara</affiliation></author></authors><title>Situated Modeling of Epistemic Puzzles</title><categories>cmp-lg cs.CL</categories><comments>iii + 49 pages, compressed, uuencoded Postscript file; revised
  version of the first author's Bilkent M.S. thesis, written under the
  supervision of the second author; notify Akman via e-mail
  (akman@cs.bilkent.edu.tr) or fax (+90-312-266-4126) if you are unable to
  obtain hardcopy, he'll work out something</comments><report-no>BU-CEIS-94-30</report-no><abstract>  Situation theory is a mathematical theory of meaning introduced by Jon
Barwise and John Perry. It has evoked great theoretical and practical interest
and motivated the framework of a few `computational' systems. PROSIT is the
pioneering work in this direction. Unfortunately, there is a lack of real-life
applications on these systems and this study is a preliminary attempt to remedy
this deficiency. Here, we examine how much PROSIT reflects situation-theoretic
concepts and solve a group of epistemic puzzles using the constructs provided
by this programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409007</id><created>1994-09-08</created><authors><author><keyname>Steinberger</keyname><forenames>Ralf</forenames><affiliation>UMIST, Manchester, Uk</affiliation></author></authors><title>Treating `Free Word Order' in Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded compressed postscript file, Coling 94</comments><abstract>  In `free word order' languages, every sentence is embedded in its specific
context. Among others, the order of constituents is determined by the
categories `theme', `rheme' and `contrastive focus'. This paper shows how to
recognise and to translate these categories automatically on a sentential
basis, so that sentence embedding can be achieved without having to refer to
the context. Modifier classes, which are traditionally neglected in linguistic
description, are fully covered by the proposed method. (Coling 94, Kyoto, Vol.
I, pages 69-75)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409008</id><created>1994-09-09</created><authors><author><keyname>Menzel</keyname><forenames>Wolfgang</forenames><affiliation>University of Hamburg, Germany</affiliation></author></authors><title>Parsing of Spoken Language under Time Constraints</title><categories>cmp-lg cs.CL</categories><comments>19 pages, LaTeX</comments><abstract>  Spoken language applications in natural dialogue settings place serious
requirements on the choice of processing architecture. Especially under adverse
phonetic and acoustic conditions parsing procedures have to be developed which
do not only analyse the incoming speech in a time-synchroneous and incremental
manner, but which are able to schedule their resources according to the varying
conditions of the recognition process. Depending on the actual degree of local
ambiguity the parser has to select among the available constraints in order to
narrow down the search space with as little effort as possible.
  A parsing approach based on constraint satisfaction techniques is discussed.
It provides important characteristics of the desired real-time behaviour and
attempts to mimic some of the attention focussing capabilities of the human
speech comprehension mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409009</id><created>1994-09-09</created><authors><author><keyname>Nourani</keyname><forenames>Cyrus F.</forenames></author></authors><title>Linguistics Computation, Automatic Model Generation, and Intensions</title><categories>cmp-lg cs.CL</categories><comments>The paper is plain text.</comments><report-no>METAAI-93-01</report-no><abstract>  Techniques are presented for defining models of computational linguistics
theories. The methods of generalized diagrams that were developed by this
author for modeling artificial intelligence planning and reasoning are shown to
be applicable to models of computation of linguistics theories. It is shown
that for extensional and intensional interpretations, models can be generated
automatically which assign meaning to computations of linguistics theories for
natural languages.
  Keywords: Computational Linguistics, Reasoning Models, G-diagrams For Models,
Dynamic Model Implementation, Linguistics and Logics For Artificial
Intelligence
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409010</id><created>1994-09-13</created><authors><author><keyname>Stolcke</keyname><forenames>Andreas</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author><author><keyname>Omohundro</keyname><forenames>Stephen M.</forenames><affiliation>ICSI, Berkeley, CA</affiliation></author></authors><title>Inducing Probabilistic Grammars by Bayesian Model Merging</title><categories>cmp-lg cs.CL</categories><comments>To appear in Grammatical Inference and Applications, Second
  International Colloquium on Grammatical Inference; Springer Verlag, 1994. 13
  pages</comments><abstract>  We describe a framework for inducing probabilistic grammars from corpora of
positive samples. First, samples are {\em incorporated} by adding ad-hoc rules
to a working grammar; subsequently, elements of the model (such as states or
nonterminals) are {\em merged} to achieve generalization and a more compact
representation. The choice of what to merge and when to stop is governed by the
Bayesian posterior probability of the grammar given the data, which formalizes
a trade-off between a close fit to the data and a default preference for
simpler models (`Occam's Razor'). The general scheme is illustrated using three
types of probabilistic grammars: Hidden Markov models, class-based $n$-grams,
and stochastic context-free grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409011</id><created>1994-09-22</created><authors><author><keyname>Fung</keyname><forenames>Pascale</forenames><affiliation>Columbia University</affiliation></author><author><keyname>McKeown</keyname><forenames>Kathleen</forenames><affiliation>Columbia University</affiliation></author></authors><title>Aligning Noisy Parallel Corpora Across Language Groups : Word Pair
  Feature Matching by Dynamic Time Warping</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded, compressed PostScript</comments><journal-ref>Proc. AMTA-94</journal-ref><abstract>  We propose a new algorithm called DK-vec for aligning pairs of
Asian/Indo-European noisy parallel texts without sentence boundaries. DK-vec
improves on previous alignment algorithms in that it handles better the
non-linear nature of noisy corpora. The algorithm uses frequency, position and
recency information as features for pattern matching. Dynamic Time Warping is
used as the matching technique between word pairs. This algorithm produces a
small bilingual lexicon which provides anchor points for alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9409012</id><created>1994-09-28</created><authors><author><keyname>Dymetman</keyname><forenames>Marc</forenames></author><author><keyname>Brousseau</keyname><forenames>Julie</forenames></author><author><keyname>Foster</keyname><forenames>George</forenames></author><author><keyname>Isabelle</keyname><forenames>Pierre</forenames></author><author><keyname>Normandin</keyname><forenames>Yves</forenames></author><author><keyname>Plamondon</keyname><forenames>Pierre</forenames></author></authors><title>Towards an Automatic Dictation System for Translators: the TransTalk
  Project</title><categories>cmp-lg cs.CL</categories><comments>Published in proceedings of the International Conference on Spoken
  Language Processing (ICSLP) 94. 4 pages, uuencoded compressed latex source
  with 4 postscript figures</comments><abstract>  Professional translators often dictate their translations orally and have
them typed afterwards. The TransTalk project aims at automating the second part
of this process. Its originality as a dictation system lies in the fact that
both the acoustic signal produced by the translator and the source text under
translation are made available to the system. Probable translations of the
source text can be predicted and these predictions used to help the speech
recognition system in its lexical choices. We present the results of the first
prototype, which show a marked improvement in the performance of the speech
recognition task when translation predictions are taken into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410001</id><created>1994-10-04</created><authors><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge, UK</affiliation></author></authors><title>Improving Language Models by Clustering Training Sentences</title><categories>cmp-lg cs.CL</categories><comments>Expanded version of a paper to appear in ANLP-94, Stuttgart. Latex, 7
  pages. Needs latex-acl.sty</comments><report-no>SRI-CRC-045</report-no><abstract>  Many of the kinds of language model used in speech understanding suffer from
imperfect modeling of intra-sentential contextual influences. I argue that this
problem can be addressed by clustering the sentences in a training corpus
automatically into subcorpora on the criterion of entropy reduction, and
calculating separate language model parameters for each cluster. This kind of
clustering offers a way to represent important contextual effects and can
therefore significantly improve the performance of a model. It also offers a
reasonably automatic means to gather evidence on whether a more complex,
context-sensitive model using the same general kind of linguistic information
is likely to reward the effort that would be required to develop it: if
clustering improves the performance of a model, this proves the existence of
further context dependencies, not exploited by the unclustered model. As
evidence for these claims, I present results showing that clustering improves
some models but not others for the ATIS domain. These results are consistent
with other findings for such models, suggesting that the existence or otherwise
of an improvement brought about by clustering is indeed a good pointer to
whether it is worth developing further the unclustered model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410002</id><created>1994-10-04</created><authors><author><keyname>Steinberger</keyname><forenames>Ralf</forenames><affiliation>UMIST, Manchester, Uk</affiliation></author></authors><title>Lexikoneintraege fuer deutsche Adverbien (Dictionary Entries for German
  Adverbs)</title><categories>cmp-lg cs.CL</categories><comments>In German; 10 pages, uuencoded gz-compressed tar file of LaTeX file;
  In: H. Trost (ed), Informatik Xpress 6. Tagungsband der 2. Konferenz
  &quot;Verarbeitung natuerlicher Sprache&quot; KONVENS '94, Wien (Austria), September
  1994, pages: 320-329</comments><abstract>  Modifiers in general, and adverbs in particular, are neglected categories in
linguistics, and consequently, their treatment in Natural Language Processing
poses problems. In this article, we present the dictionary information for
German adverbs which is necessary to deal with word order, degree modifier
scope and other problems in NLP. We also give evidence for the claim that a
classification according to position classes differs from any semantic
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410003</id><created>1994-10-05</created><authors><author><keyname>Frank</keyname><forenames>A.</forenames><affiliation>Institute for Computational Linguistics, University of Stuttgart</affiliation></author><author><keyname>Reyle</keyname><forenames>U.</forenames><affiliation>Institute for Computational Linguistics, University of Stuttgart</affiliation></author></authors><title>Principle Based Semantics for HPSG</title><categories>cmp-lg cs.CL</categories><abstract>  The paper presents a constraint based semantic formalism for HPSG. The
advantages of the formlism are shown with respect to a grammar for a fragment
of German that deals with (i) quantifier scope ambiguities triggered by
scrambling and/or movement and (ii) ambiguities that arise from the
collective/distributive distinction of plural NPs. The syntax-semantics
interface directly implements syntactic conditions on quantifier scoping and
distributivity. The construction of semantic representations is guided by
general principles governing the interaction between syntax and semantics. Each
of these principles acts as a constraint to narrow down the set of possible
interpretations of a sentence. Meanings of ambiguous sentences are represented
by single partial representations (so-called U(nderspecified) D(iscourse)
R(epresentation) S(tructure)s) to which further constraints can be added
monotonically to gain more information about the content of a sentence. There
is no need to build up a large number of alternative representations of the
sentence which are then filtered by subsequent discourse and world knowledge.
The advantage of UDRSs is not only that they allow for monotonic incremental
interpretation but also that they are equipped with truth conditions and a
proof theory that allows for inferences to be drawn directly on structures
where quantifier scope is not resolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410004</id><created>1994-10-06</created><updated>1994-10-07</updated><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames></author></authors><title>Spelling Correction in Agglutinative Languages</title><categories>cmp-lg cs.CL</categories><comments>uuencoded postscript file, poster version to appear in ANLP
  proceedings. (Abstract now fixed)</comments><abstract>  This paper presents an approach to spelling correction in agglutinative
languages that is based on two-level morphology and a dynamic programming based
search algorithm. Spelling correction in agglutinative languages is
significantly different than in languages like English. The concept of a word
in such languages is much wider that the entries found in a dictionary, owing
to {}~productive word formation by derivational and inflectional affixations.
After an overview of certain issues and relevant mathematical preliminaries, we
formally present the problem and our solution. We then present results from our
experiments with spelling correction in Turkish, a Ural--Altaic agglutinative
language. Our results indicate that we can find the intended correct word in
95\% of the cases and offer it as the first candidate in 74\% of the cases,
when the edit distance is 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410005</id><created>1994-10-10</created><authors><author><keyname>Brennan</keyname><forenames>Susan E.</forenames></author><author><keyname>Friedman</keyname><forenames>Marilyn Walker</forenames></author><author><keyname>Pollard</keyname><forenames>Carl J.</forenames></author></authors><title>A Centering Approach to Pronouns</title><categories>cmp-lg cs.CL</categories><comments>plain latex but includes psfig.tex, 8 pages with one psfig, published
  in 25th Annual Meeting of the Association of Computational Linguistics, 1987</comments><journal-ref>Association of Computational Linguistics 1987, p. 155-162</journal-ref><abstract>  In this paper we present a formalization of the centering approach to
modeling attentional structure in discourse and use it as the basis for an
algorithm to track discourse context and bind pronouns. As described in Grosz,
Joshi and Weinstein (1986), the process of centering attention on entities in
the discourse gives rise to the intersentential transitional states of
continuing, retaining and shifting. We propose an extension to these states
which handles some additional cases of multiple ambiguous pronouns. The
algorithm has been implemented in an HPSG natural language system which serves
as the interface to a database query application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410006</id><created>1994-10-10</created><updated>1994-10-11</updated><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author></authors><title>Evaluating Discourse Processing Algorithms</title><categories>cmp-lg cs.CL</categories><comments>plain latex but includes psfig.tex, 11 pages with one psfig,
  published in 27th Annual Meeting of the ACL, 1989</comments><journal-ref>Association of Computational Linguistics, 1989, p. 251-262</journal-ref><abstract>  In order to take steps towards establishing a methodology for evaluating
Natural Language systems, we conducted a case study. We attempt to evaluate two
different approaches to anaphoric processing in discourse by comparing the
accuracy and coverage of two published algorithms for finding the co-specifiers
of pronouns in naturally occurring texts and dialogues. We present the
quantitative results of hand-simulating these algorithms, but this analysis
naturally gives rise to both a qualitative evaluation and recommendations for
performing such evaluations in general. We illustrate the general difficulties
encountered with quantitative evaluation. These are problems with: (a) allowing
for underlying assumptions, (b) determining how to handle underspecifications,
and (c) evaluating the contribution of false positives and error chaining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410007</id><created>1994-10-18</created><authors><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>Paris 7</affiliation></author><author><keyname>Joshi</keyname><forenames>Aravind</forenames><affiliation>U. Penn.</affiliation></author></authors><title>A Formal Look at Dependency Grammars and Phrase-Structure Grammars, with
  Special Consideration of Word-Order Phenomena</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed ps file, 20 pages</comments><abstract>  The central role of the lexicon in Meaning-Text Theory (MTT) and other
dependency-based linguistic theories cannot be replicated in linguistic
theories based on context-free grammars (CFGs). We describe Tree Adjoining
Grammar (TAG) as a system that arises naturally in the process of lexicalizing
CFGs. A TAG grammar can therefore be compared directly to an Meaning-Text Model
(MTM). We illustrate this point by discussing the computational complexity of
certain non-projective constructions, and suggest a way of incorporating
locality of word-order definitions into the Surface-Syntactic Component of MTT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410008</id><created>1994-10-20</created><authors><author><keyname>Karlgren</keyname><forenames>Jussi</forenames><affiliation>SICS</affiliation></author><author><keyname>Cutting</keyname><forenames>Douglass</forenames><affiliation>Apple</affiliation></author></authors><title>Recognizing Text Genres with Simple Metrics Using Discriminant Analysis</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, In proceedings of COLING 94</comments><abstract>  A simple method for categorizing texts into predetermined text genre
categories using the statistical standard technique of discriminant analysis is
demonstrated with application to the Brown corpus. Discriminant analysis makes
it possible use a large number of parameters that may be specific for a certain
corpus or information stream, and combine them into a small number of
functions, with the parameters weighted on basis of how useful they are for
discriminating text genres. An application to information retrieval is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410009</id><created>1994-10-20</created><authors><author><keyname>Heylen</keyname><forenames>Dirk</forenames></author><author><keyname>Maxwell</keyname><forenames>Kerry G.</forenames></author><author><keyname>Verhagen</keyname><forenames>Marc</forenames></author></authors><title>Lexical Functions and Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses named.sty, twocolumn.sty, a4.sty</comments><abstract>  This paper discusses the lexicographical concept of lexical functions and
their potential exploitation in the development of a machine translation
lexicon designed to handle collocations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410010</id><created>1994-10-20</created><authors><author><keyname>Doran</keyname><forenames>Christy</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Zaidel</keyname><forenames>Martin</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>XTAG system - A Wide Coverage Grammar for English</title><categories>cmp-lg cs.CL</categories><comments>ps file. 7 pages</comments><journal-ref>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994, pp. 922-928</journal-ref><abstract>  This paper presents the XTAG system, a grammar development tool based on the
Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic
grammar for English. The various components of the system are discussed and
preliminary evaluation results from the parsing of various corpora are given.
Results from the comparison of XTAG against the IBM statistical parser and the
Alvey Natural Language Tool parser are also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410011</id><created>1994-10-21</created><authors><author><keyname>Karlgren</keyname><forenames>Hans</forenames><affiliation>Scandface</affiliation></author><author><keyname>Karlgren</keyname><forenames>Jussi</forenames><affiliation>SICS</affiliation></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Magnus</forenames><affiliation>SICS</affiliation></author><author><keyname>Pettersson</keyname><forenames>Paul</forenames><affiliation>SICS</affiliation></author><author><keyname>Wahrol&#xe9;n</keyname><forenames>Bengt</forenames><affiliation>Scandface</affiliation></author></authors><title>Dilemma - An Instant Lexicographer</title><categories>cmp-lg cs.CL</categories><comments>3 pages, LaTeX, in proceedings of COLING 94</comments><abstract>  Dilemma is intended to enhance quality and increase productivity of expert
human translators by presenting to the writer relevant lexical information
mechanically extracted from comparable existing translations, thus replacing -
or compensating for the absence of - a lexicographer and stand-by terminologist
rather than the translator. Using statistics and crude surface analysis and a
minimum of prior information, Dilemma identifies instances and suggests their
counterparts in parallel source and target texts, on all levels down to
individual words. Dilemma forms part of a tool kit for translation where focus
is on text structure and over-all consistency in large text volumes rather than
on framing sentences, on interaction between many actors in a large project
rather than on retrieval of machine-stored data and on decision making rather
than on application of given rules. In particular, the system has been tuned to
the needs of the ongoing translation of European Community legislation into the
languages of candidate member countries. The system has been demonstrated to
and used by professional translators with promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410012</id><created>1994-10-21</created><updated>1994-10-24</updated><authors><author><keyname>Elworthy</keyname><forenames>David</forenames><affiliation>Sharp Laboratories of Europe Ltd</affiliation></author></authors><title>Does Baum-Welch Re-estimation Help Taggers?</title><categories>cmp-lg cs.CL</categories><comments>Uses aclap.sty. Appeared in ANLP 94</comments><abstract>  In part of speech tagging by Hidden Markov Model, a statistical model is used
to assign grammatical categories to words in a text. Early work in the field
relied on a corpus which had been tagged by a human annotator to train the
model. More recently, Cutting {\it et al.} (1992) suggest that training can be
achieved with a minimal lexicon and a limited amount of {\em a priori}
information about probabilities, by using Baum-Welch re-estimation to
automatically refine the model. In this paper, I report two experiments
designed to determine how much manual training information is needed. The first
experiment suggests that initial biasing of either lexical or transition
probabilities is essential to achieve a good accuracy. The second experiment
reveals that there are three distinct patterns of Baum-Welch re-estimation. In
two of the patterns, the re-estimation ultimately reduces the accuracy of the
tagging rather than improving it. The pattern which is applicable can be
predicted from the quality of the initial model and the similarity between the
tagged training corpus (if any) and the corpus to be tagged. Heuristics for
deciding how to use re-estimation in an effective manner are given. The
conclusions are broadly in agreement with those of Merialdo (1994), but give
greater detail about the contributions of different parts of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410013</id><created>1994-10-21</created><authors><author><keyname>Elworthy</keyname><forenames>David</forenames><affiliation>Sharp Laboratories of Europe</affiliation></author></authors><title>Automatic Error Detection in Part of Speech Tagging</title><categories>cmp-lg cs.CL</categories><comments>Postscript. Appeared in NeMLaP 1994</comments><abstract>  A technique for detecting errors made by Hidden Markov Model taggers is
described, based on comparing observable values of the tagging process with a
threshold. The resulting approach allows the accuracy of the tagger to be
improved by accepting a lower efficiency, defined as the proportion of words
which are tagged. Empirical observations are presented which demonstrate the
validity of the technique and suggest how to choose an appropriate threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410014</id><created>1994-10-21</created><authors><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Martin</keyname><forenames>Patrick</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Freely Available Syntactic Lexicon for English</title><categories>cmp-lg cs.CL</categories><comments>Latex file with .eps figure. 8 pages</comments><journal-ref>Proceedings of the International Workshop on Sharable Natural
  Language Resources, Nara, Japan, August 1994</journal-ref><abstract>  This paper presents a syntactic lexicon for English that was originally
derived from the Oxford Advanced Learner's Dictionary and the Oxford Dictionary
of Current Idiomatic English, and then modified and augmented by hand. There
are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows
based tool is available for maintaining the lexicon and performing searches. C
and Lisp hooks are also available so that the lexicon can be easily utilized by
parsers and other programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410015</id><created>1994-10-21</created><authors><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Doran</keyname><forenames>Christy</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Becker</keyname><forenames>Tilman</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Lexicalization and Grammar Development</title><categories>cmp-lg cs.CL</categories><comments>ps file. English w/ German abstract. 10 pages</comments><journal-ref>Proceedings of KONVENS 94, Vienna, Austria, September 1994</journal-ref><abstract>  In this paper we present a fully lexicalized grammar formalism as a
particularly attractive framework for the specification of natural language
grammars. We discuss in detail Feature-based, Lexicalized Tree Adjoining
Grammars (FB-LTAGs), a representative of the class of lexicalized grammars. We
illustrate the advantages of lexicalized grammars in various contexts of
natural language processing, ranging from wide-coverage grammar development to
parsing and machine translation. We also present a method for compact and
efficient representation of lexicalized trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410016</id><created>1994-10-21</created><updated>1994-10-24</updated><authors><author><keyname>Rentier</keyname><forenames>Gerrit</forenames><affiliation>ITK, Tilburg University</affiliation></author></authors><title>Dutch Cross Serial Dependencies in HPSG</title><categories>cmp-lg cs.CL</categories><comments>5 pages uuencoded compressed PostScript</comments><abstract>  We present an analysis of Dutch cross serial dependencies in Head-driven
Phrase Structure Grammar. Arguably, our analysis differs from other analyses in
that we do not refer to `additional' mechanisms (e.g., sequence union, head
wrapping): just standard structure sharing, an immediate dominance schema and a
linear precedence rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410017</id><created>1994-10-24</created><authors><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>Computational Linguisitcs Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguisitcs Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Schacht</keyname><forenames>Susanne</forenames><affiliation>Computational Linguisitcs Research Group, Freiburg University, Germany</affiliation></author></authors><title>Concurrent Lexicalized Dependency Parsing: The ParseTalk Model</title><categories>cmp-lg cs.CL</categories><comments>90kB, 7pages Postscript</comments><report-no>CLIF Report 9/94</report-no><journal-ref>Proc.15th Intl Conference on Computational Linguistics, Kyoto,
  Japan, August 1994, pp.379-385</journal-ref><abstract>  A grammar model for concurrent, object-oriented natural language parsing is
introduced. Complete lexical distribution of grammatical knowledge is achieved
building upon the head-oriented notions of valency and dependency, while
inheritance mechanisms are used to capture lexical generalizations. The
underlying concurrent computation model relies upon the actor paradigm. We
consider message passing protocols for establishing dependency relations and
ambiguity handling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410018</id><created>1994-10-24</created><authors><author><keyname>Schmid</keyname><forenames>Helmut</forenames><affiliation>University of Stuttgart</affiliation></author></authors><title>Part-of-Speech Tagging with Neural Networks</title><categories>cmp-lg cs.CL</categories><comments>5 pages</comments><journal-ref>Coling-94, 172-176</journal-ref><abstract>  Text corpora which are tagged with part-of-speech information are useful in
many areas of linguistic research. In this paper, a new part-of-speech tagging
method based on neural networks (Net- Tagger) is presented and its performance
is compared to that of a HMM-tagger and a trigram-based tagger. It is shown
that the Net- Tagger performs as well as the trigram-based tagger and better
than the HMM-tagger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410019</id><created>1994-10-24</created><authors><author><keyname>Schacht</keyname><forenames>Susanne</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Broeker</keyname><forenames>Norbert</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Germany</affiliation></author></authors><title>Concurrent Lexicalized Dependency Parsing: A Behavioral View on
  ParseTalk Events</title><categories>cmp-lg cs.CL</categories><comments>68kB, 5pages Postscript</comments><report-no>CLIF Report 9/94</report-no><journal-ref>Proc.15th Intl Conference on Computational Linguistics, Kyoto,
  Japan, August 1994, pp.498-493</journal-ref><abstract>  The behavioral specification of an object-oriented grammar model is
considered. The model is based on full lexicalization, head-orientation via
valency constraints and dependency relations, inheritance as a means for
non-redundant lexicon specification, and concurrency of computation. The
computation model relies upon the actor paradigm, with concurrency entering
through asynchronous message passing between actors. In particular, we here
elaborate on principles of how the global behavior of a lexically distributed
grammar and its corresponding parser can be specified in terms of event type
networks and event networks, resp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410020</id><created>1994-10-24</created><authors><author><keyname>TANAKA</keyname><forenames>Kumiko</forenames><affiliation>Univ. Tokyo</affiliation></author><author><keyname>UMEMURA</keyname><forenames>Kyoji</forenames><affiliation>NTT Basic Research Laboratories</affiliation></author></authors><title>Construction of a Bilingual Dictionary Intermediated by a Third Language</title><categories>cmp-lg cs.CL</categories><comments>7 pages ps file compressed and uuencoded</comments><abstract>  When using a third language to construct a bilingual dictionary, it is
necessary to discriminate equivalencies from inappropriate words derived as a
result of ambiguity in the third language. We propose a method to treat this by
utilizing the structures of dictionaries to measure the nearness of the
meanings of words. The resulting dictionary is a word-to-word bilingual
dictionary of nouns and can be used to refine the entries and equivalencies in
published bilingual dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410021</id><created>1994-10-24</created><authors><author><keyname>Wakao</keyname><forenames>Takahiro</forenames></author></authors><title>Reference Resolution Using Semantic Patterns in Japanese Newspaper
  Articles</title><categories>cmp-lg cs.CL</categories><comments>5 pages, Latex, In Proceedings of COLING 94, uses coling.sty</comments><abstract>  Reference resolution is one of the important tasks in natural language
processing. In this paper, the author first determines the referents and their
locations of &quot;dousha&quot;, literally meaning &quot;the same company&quot;, which appear in
Japanese newspaper articles. Secondly, three heuristic methods, two of which
use semantic information in text such as company names and their patterns, are
proposed and tested on how accurately they identify the correct referents. The
proposed methods based on semantic patterns show high accuracy for reference
resolution of &quot;dousha&quot; (more than 90\%). This suggests that semantic
pattern-matching methods are effective for reference resolution in newspaper
articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410022</id><created>1994-10-24</created><updated>1994-10-25</updated><authors><author><keyname>Bird</keyname><forenames>Steven</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Automated tone transcription</title><categories>cmp-lg cs.CL</categories><comments>12 pages, 4 postscript figures, uses examples.sty, newapa.sty,
  latex-acl.sty, ipamacs.sty</comments><journal-ref>Proceedings of the First Meeting of the ACL Special</journal-ref><abstract>  In this paper I report on an investigation into the problem of assigning
tones to pitch contours. The proposed model is intended to serve as a tool for
phonologists working on instrumentally obtained pitch data from tone languages.
Motivation and exemplification for the model is provided by data taken from my
fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and
others, I provide a parametrised F_0 prediction function P which generates F_0
values from a tone sequence, and I explore the asymptotic behaviour of
downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0)
values amounts to finding a tone sequence T such that P(T) {}~= X. This is a
combinatorial optimisation problem, for which two non-deterministic search
techniques are provided: a genetic algorithm and a simulated annealing
algorithm. Finally, two implementations---one for each technique---are
described and then compared using both artificial and real data for sequences
of up to 20 tones. These programs can be adapted to other tone languages by
adjusting the F_0 prediction function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410023</id><created>1994-10-24</created><authors><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Palmer</keyname><forenames>Martha</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Park</keyname><forenames>Hyun S.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Joshi</keyname><forenames>Aravind K.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Korean to English Translation Using Synchronous TAGs</title><categories>cmp-lg cs.CL</categories><comments>ps file. 8 pages</comments><journal-ref>Proceedings of AMTA 94</journal-ref><abstract>  It is often argued that accurate machine translation requires reference to
contextual knowledge for the correct treatment of linguistic phenomena such as
dropped arguments and accurate lexical selection. One of the historical
arguments in favor of the interlingua approach has been that, since it revolves
around a deep semantic representation, it is better able to handle the types of
linguistic phenomena that are seen as requiring a knowledge-based approach. In
this paper we present an alternative approach, exemplified by a prototype
system for machine translation of English and Korean which is implemented in
Synchronous TAGs. This approach is essentially transfer based, and uses
semantic feature unification for accurate lexical selection of polysemous
verbs. The same semantic features, when combined with a discourse model which
stores previously mentioned entities, can also be used for the recovery of
topicalized arguments. In this paper we concentrate on the translation of
Korean to English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410024</id><created>1994-10-24</created><authors><author><keyname>Karp</keyname><forenames>Daniel</forenames><affiliation>Stanford U.</affiliation></author><author><keyname>Schabes</keyname><forenames>Yves</forenames><affiliation>U. Penn</affiliation></author><author><keyname>Zaidel</keyname><forenames>Martin</forenames><affiliation>U. Penn</affiliation></author><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>U. Penn</affiliation></author></authors><title>A Freely Available Wide Coverage Morphological Analyzer for English</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed ps file. 5 pages. Contact info has been upated
  from Coling '92 version</comments><journal-ref>Proceedings of Coling 92</journal-ref><abstract>  This paper presents a morphological lexicon for English that handles more
than 317000 inflected forms derived from over 90000 stems. The lexicon is
available in two formats. The first can be used by an implementation of a
two-level processor for morphological analysis. The second, derived from the
first one for efficiency reasons, consists of a disk-based database using a
UNIX hash table facility. We also built an X Window tool to facilitate the
maintenance and browsing of the lexicon. The package is ready to be integrated
into an natural language application such as a parser through hooks written in
Lisp and C.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410025</id><created>1994-10-25</created><authors><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames><affiliation>Rank Xerox Research Centre</affiliation></author><author><keyname>J&#xe4;rvinen</keyname><forenames>Timo</forenames><affiliation>University of Helsinki</affiliation></author></authors><title>Syntactic Analysis Of Natural Language Using Linguistic Rules And
  Corpus-based Patterns</title><categories>cmp-lg cs.CL</categories><comments>in Proc Coling-94, Vol I, pp. 629-634, Kyoto. Postscript</comments><abstract>  We are concerned with the syntactic annotation of unrestricted text. We
combine a rule-based analysis with subsequent exploitation of empirical data.
The rule-based surface syntactic analyser leaves some amount of ambiguity in
the output that is resolved using empirical patterns. We have implemented a
system for generating and applying corpus-based patterns. Some patterns
describe the main constituents in the sentence and some the local context of
the each syntactic function. There are several (partly) reduntant patterns, and
the ``pattern'' parser selects analysis of the sentence that matches the
strictest possible pattern(s). The system is applied to an experimental corpus.
We present the results and discuss possible refinements of the method from a
linguistic point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410026</id><created>1994-10-25</created><authors><author><keyname>Brill</keyname><forenames>Eric</forenames><affiliation>Johns Hopkins</affiliation></author><author><keyname>Resnik</keyname><forenames>Philip</forenames><affiliation>Sun Microsystems</affiliation></author></authors><title>A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, compressed uuencoded postscript</comments><journal-ref>COLING 1994</journal-ref><abstract>  In this paper, we describe a new corpus-based approach to prepositional
phrase attachment disambiguation, and present results comparing performance of
this algorithm with other corpus-based approaches to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410027</id><created>1994-10-25</created><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames><affiliation>University of Stuttgart</affiliation></author></authors><title>Probabilistic Tagging with Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>Coling-94, 85 KB, 5 pages</comments><journal-ref>COLING-94, vol.1, pp.161-165, Kyoto, Japan. August 5-9, 1994.</journal-ref><abstract>  The described tagger is based on a hidden Markov model and uses tags composed
of features such as part-of-speech, gender, etc. The contextual probability of
a tag (state transition probability) is deduced from the contextual
probabilities of its feature-value-pairs. This approach is advantageous when
the available training corpus is small and the tag set large, which can be the
case with morphologically rich languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410028</id><created>1994-10-25</created><authors><author><keyname>Wir&#xe9;n</keyname><forenames>Mats</forenames><affiliation>Universitat des Saarlandes</affiliation></author></authors><title>Minimal Change and Bounded Incremental Parsing</title><categories>cmp-lg cs.CL</categories><comments>7 pages, compressed and uuencoded</comments><abstract>  Ideally, the time that an incremental algorithm uses to process a change
should be a function of the size of the change rather than, say, the size of
the entire current input. Based on a formalization of ``the set of things
changed'' by an incremental modification, this paper investigates how and to
what extent it is possible to give such a guarantee for a chart-based parsing
framework and discusses the general utility of a minimality notion in
incremental processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410029</id><created>1994-10-26</created><authors><author><keyname>Joshi</keyname><forenames>Aravind K.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing</title><categories>cmp-lg cs.CL</categories><comments>ps file. 8 pages</comments><journal-ref>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994</journal-ref><abstract>  In a lexicalized grammar formalism such as Lexicalized Tree-Adjoining Grammar
(LTAG), each lexical item is associated with at least one elementary structure
(supertag) that localizes syntactic and semantic dependencies. Thus a parser
for a lexicalized grammar must search a large set of supertags to choose the
right ones to combine for the parse of the sentence. We present techniques for
disambiguating supertags using local information such as lexical preference and
local lexical dependencies. The similarity between LTAG and Dependency grammars
is exploited in the dependency model of supertag disambiguation. The
performance results for various models of supertag disambiguation such as
unigram, trigram and dependency-based models are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410030</id><created>1994-10-26</created><authors><author><keyname>Hockey</keyname><forenames>B. A.</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Feature-Based TAG in place of multi-component adjunction: Computational
  Implications</title><categories>cmp-lg cs.CL</categories><comments>ps file. 9 pages</comments><journal-ref>Natural Language Processing Pacific Rim Symposium (NLPRS 93)</journal-ref><abstract>  Using feature-based Tree Adjoining Grammar (TAG), this paper presents
linguistically motivated analyses of constructions claimed to require
multi-component adjunction. These feature-based TAG analyses permit parsing of
these constructions using an existing unification-based Earley-style TAG
parser, thus obviating the need for a multi-component TAG parser without
sacrificing linguistic coverage for English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410031</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410031</id><created>1994-10-27</created><authors><author><keyname>Genthial</keyname><forenames>Damien</forenames><affiliation>LGI-Imag Campus, BP 53, Grenoble</affiliation></author><author><keyname>Courtin</keyname><forenames>Jacques</forenames><affiliation>LGI-Imag Campus, BP 53, Grenoble</affiliation></author><author><keyname>Trilan</keyname><forenames>Jacques Menezo Equipe</forenames><affiliation>LGI-Imag Campus, BP 53, Grenoble</affiliation></author></authors><title>Towards a More User-friendly Correction</title><categories>cmp-lg cs.CL</categories><comments>Postscript file, compressed and uuencoded, 6 pages, published at
  CoLing'94, Kyoto, Japan, August 94</comments><abstract>  We first present our view of detection and correction of syntactic errors. We
then introduce a new correction method, based on heuristic criteria used to
decide which correction should be preferred. Weighting of these criteria leads
to a flexible and parametrable system, which can adapt itself to the user. A
partitioning of the trees based on linguistic criteria: agreement rules, rather
than computational criteria is then necessary. We end by proposing extensions
to lexical correction and to some syntactic errors. Our aim is an adaptable and
user-friendly system capable of automatic correction for some applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410032</id><created>1994-10-28</created><authors><author><keyname>Huang</keyname><forenames>Xiaorong</forenames><affiliation>Fachbereich Informatik, Universit&#xe4;t des Saarlandes, Saarbr&#xfc;cken, Germany</affiliation></author></authors><title>Planning Argumentative Texts</title><categories>cmp-lg cs.CL</categories><comments>Coling94, email: huang@cs.uni-sb.de</comments><abstract>  This paper presents \proverb\, a text planner for argumentative texts.
\proverb\'s main feature is that it combines global hierarchical planning and
unplanned organization of text with respect to local derivation relations in a
complementary way. The former splits the task of presenting a particular proof
into subtasks of presenting subproofs. The latter simulates how the next
intermediate conclusion to be presented is chosen under the guidance of the
local focus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410033</id><created>1994-10-30</created><authors><author><keyname>Harbusch</keyname><forenames>Karin</forenames><affiliation>DFKI</affiliation></author><author><keyname>Kikui</keyname><forenames>Gen-ichiro</forenames><affiliation>ATR</affiliation></author><author><keyname>Kilger</keyname><forenames>Anne</forenames><affiliation>DFKI</affiliation></author></authors><title>Default Handling in Incremental Generation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded LaTeX file</comments><report-no>VM-Report 22</report-no><abstract>  Natural language generation must work with insufficient input.
Underspecifications can be caused by shortcomings of the component providing
the input or by the preliminary state of incrementally given input. The paper
aims to escape from such dead-end situations by making assumptions. We discuss
global aspects of default handling. Two problem classes for defaults in the
incremental syntactic generator VM-GEN are presented to substantiate our
discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9410034</id><created>1994-10-31</created><authors><author><keyname>Peto</keyname><forenames>Linda Bauman</forenames><affiliation>University of Toronto</affiliation></author></authors><title>A Comparison of Two Smoothing Methods for Word Bigram Models</title><categories>cmp-lg cs.CL</categories><comments>M.Sc. thesis, 57 pages, compressed and uucencoded postscript file</comments><report-no>CSRI-304</report-no><abstract>  A COMPARISON OF TWO SMOOTHING METHODS FOR WORD BIGRAM MODELS
  Linda Bauman Peto
  Department of Computer Science
  University of Toronto Abstract Word bigram models estimated from text corpora
require smoothing methods to estimate the probabilities of unseen bigrams. The
deleted estimation method uses the formula:
  Pr(i|j) = lambda f_i + (1-lambda)f_i|j, where f_i and f_i|j are the relative
frequency of i and the conditional relative frequency of i given j,
respectively, and lambda is an optimized parameter. MacKay (1994) proposes a
Bayesian approach using Dirichlet priors, which yields a different formula:
  Pr(i|j) = (alpha/F_j + alpha) m_i + (1 - alpha/F_j + alpha) f_i|j where F_j
is the count of j and alpha and m_i are optimized parameters. This thesis
describes an experiment in which the two methods were trained on a
two-million-word corpus taken from the Canadian _Hansard_ and compared on the
basis of the experimental perplexity that they assigned to a shared test
corpus. The methods proved to be about equally accurate, with MacKay's method
using fewer resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411001</id><created>1994-11-01</created><authors><author><keyname>Losee</keyname><forenames>Robert M.</forenames><affiliation>School of Information and Library Science, U. of North Carolina, Chapel Hill</affiliation></author><author><keyname>Haas</keyname><forenames>Stephanie W.</forenames><affiliation>School of Information and Library Science, U. of North Carolina, Chapel Hill</affiliation></author></authors><title>Sublanguage Terms: Dictionaries, Usage, and Automatic Classification</title><categories>cmp-lg cs.CL</categories><comments>LaTeX with bibliography file attached</comments><abstract>  The use of terms from natural and social scientific titles and abstracts is
studied from the perspective of sublanguages and their specialized
dictionaries. Different notions of sublanguage distinctiveness are explored.
Objective methods for separating hard and soft sciences are suggested based on
measures of sublanguage use, dictionary characteristics, and sublanguage
distinctiveness. Abstracts were automatically classified with a high degree of
accuracy by using a formula that considers the degree of uniqueness of terms in
each sublanguage. This may prove useful for text filtering or information
retrieval systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411002</id><created>1994-11-01</created><authors><author><keyname>Alshawi</keyname><forenames>Hiyan</forenames></author><author><keyname>Carter</keyname><forenames>David</forenames></author><author><keyname>Crouch</keyname><forenames>Richard</forenames></author><author><keyname>Pulman</keyname><forenames>Steve</forenames></author><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Smith</keyname><forenames>Arnold</forenames></author></authors><title>CLARE: A Contextual Reasoning and Cooperative Response Framework for the
  Core Language Engine</title><categories>cmp-lg cs.CL</categories><comments>250 pages, uuencoded compressed tar-ed LaTeX. Written 1992</comments><report-no>CRC-028</report-no><abstract>  This report describes the research, design and implementation work carried
out in building the CLARE system at SRI International, Cambridge, England.
CLARE was designed as a natural language processing system with facilities for
reasoning and understanding in context and for generating cooperative
responses. The project involved both further development of SRI's Core Language
Engine (Alshawi, 1992, MIT Press) natural language processor and the design and
implementation of new components for reasoning and response generation. The
CLARE system has advanced the state of the art in a wide variety of areas, both
through the use of novel techniques developed on the project, and by extending
the coverage or scale of known techniques. The language components are
application-independent and provide interfaces for the development of new types
of application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411003</id><created>1994-11-02</created><updated>1994-11-05</updated><authors><author><keyname>Mahootian</keyname><forenames>Shahrzad</forenames></author><author><keyname>Santorini</keyname><forenames>Beatrice</forenames></author></authors><title>Adnominal adjectives, code-switching and lexicalized TAG</title><categories>cmp-lg cs.CL</categories><comments>e-mails - usmahoot@uxa.ecn.bgu.edu, b-santorini@nwu.edu</comments><abstract>  In codeswitching contexts, the language of a syntactic head determines the
distribution of its complements. Mahootian 1993 derives this generalization by
representing heads as the anchors of elementary trees in a lexicalized TAG.
However, not all codeswitching sequences are amenable to a head-complement
analysis. For instance, adnominal adjectives can occupy positions not available
to them in their own language, and the TAG derivation of such sequences must
use unanchored auxiliary trees. palabras heavy-duty `heavy-duty words'
(Spanish-English; Poplack 1980:584) taste lousy sana `very lousy taste'
(English-Swahili; Myers-Scotton 1993:29, (10)) Given the null hypothesis that
codeswitching and monolingual sequences are derived in an identical manner,
sequences like those above provide evidence that pure lexicalized TAGs are
inadequate for the description of natural language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411004</id><created>1994-11-03</created><authors><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Determining Determiner Sequencing: A Syntactic Analysis for English</title><categories>cmp-lg cs.CL</categories><comments>ps file. 4 pages. Proceedings of TAG+3, 1994</comments><abstract>  Previous work on English determiners has primarily concentrated on their
semantics or scoping properties rather than their complex ordering behavior.
The little work that has been done on determiner ordering generally splits
determiners into three subcategories. However, this small number of categories
does not capture the finer distinctions necessary to correctly order
determiners. This paper presents a syntactic account of determiner sequencing
based on eight independently identified semantic features. Complex determiners,
such as genitives, partitives, and determiner modifying adverbials, are also
presented. This work has been implemented as part of XTAG, a wide-coverage
grammar for English based in the Feature-Based, Lexicalized Tree Adjoining
Grammar (FB-LTAG) formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411005</id><created>1994-11-03</created><authors><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Palmer</keyname><forenames>Martha</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Constraining Lexical Selection Across Languages Using TAGs</title><categories>cmp-lg cs.CL</categories><comments>ps file. 4 pages, Proceedings of TAG+3, 1994</comments><abstract>  Lexical selection in Machine Translation consists of several related
components. Two that have received a lot of attention are lexical mapping from
an underlying concept or lexical item, and choosing the correct
subcategorization frame based on argument structure. Because most MT
applications are small or relatively domain specific, a third component of
lexical selection is generally overlooked - distinguishing between lexical
items that are closely related conceptually. While some MT systems have
proposed using a 'world knowledge' module to decide which word is more
appropriate based on various pragmatic or stylistic constraints, we are
interested in seeing how much we can accomplish using a combination of syntax
and lexical semantics. By using separate ontologies for each language
implemented in FB-LTAGs, we are able to elegantly model the more specific and
language dependent syntactic and semantic distinctions necessary to further
filter the choice of the lexical item.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411006</id><created>1994-11-03</created><authors><author><keyname>Doran</keyname><forenames>Christy</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Egedi</keyname><forenames>Dania</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Hockey</keyname><forenames>Beth Ann</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Status of the XTAG System</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed ps file. 4 pages</comments><journal-ref>Proceedings of TAG+3, 1994</journal-ref><abstract>  XTAG is an ongoing project to develop a wide-coverage grammar for English,
based on the Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG)
formalism. The XTAG system integrates a morphological analyzer, an N-best
part-of-speech tagger, an Early-style parser and an X-window interface, along
with a wide-coverage grammar for English developed using the system. This
system serves as a linguist's workbench for developing FB-LTAG specifications.
This paper presents a description of and recent improvements to the various
components of the XTAG system. It also presents the recent performance of the
wide-coverage grammar on various corpora and compares it against the
performance of other wide-coverage and domain-specific grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411007</id><created>1994-11-03</created><authors><author><keyname>Kroch</keyname><forenames>Anthony</forenames><affiliation>U. Penn.</affiliation></author><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>Paris 7</affiliation></author></authors><title>The Linguistic Relevance of Quasi-Trees</title><categories>cmp-lg cs.CL</categories><comments>4 pages, uuencoded compressed ps file</comments><report-no>Report TALANA-RT-94-01, TALANA, Universit{\'e} Paris 7, 1994</report-no><journal-ref>In {\em 3e Colloque International sur les Grammaires d'Arbres
  Adjoints (TAG+3)}</journal-ref><abstract>  We discuss two constructions (long scrambling and ECM verbs) which challenge
most syntactic theories (including traditional TAG approaches) since they seem
to require exceptional mechanisms and postulates. We argue that these
constructions should in fact be analyzed in a similar manner, namely as
involving a verb which selects for a ``defective'' complement. These
complements are defective in that they lack certain Case-assigning abilities
(represented as functional heads). The constructions differ in how many such
abilities are lacking. Following the previous analysis of scrambling of Rambow
(1994), we propose a TAG analysis based on quasi-trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411008</id><created>1994-11-03</created><authors><author><keyname>Becker</keyname><forenames>Tilman</forenames><affiliation>U. Penn.</affiliation></author><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>Paris 7</affiliation></author></authors><title>Parsing Free Word-Order Languages in Polynomial Time</title><categories>cmp-lg cs.CL</categories><comments>4 pages, uuencoded compressed ps file</comments><report-no>TALANA-RT-94-01, TALANA, Universite' Paris 7, 1994</report-no><journal-ref>In {\em 3e Colloque International sur les Grammaires d'Arbres
  Adjoints (TAG+3)}</journal-ref><abstract>  We present a parsing algorithm with polynomial time complexity for a large
subset of V-TAG languages. V-TAG, a variant of multi-component TAG, can handle
free-word order phenomena which are beyond the class LCFRS (which includes
regular TAG). Our algorithm is based on a CYK-style parser for TAGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411009</id><created>1994-11-03</created><authors><author><keyname>Doran</keyname><forenames>Christine</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Bootstrapping A Wide-Coverage CCG from FB-LTAG</title><categories>cmp-lg cs.CL</categories><comments>ps file. 4 pages, Proceedings of TAG+3, 1994</comments><abstract>  A number of researchers have noted the similarities between LTAGs and CCGs.
Observing this resemblance, we felt that we could make use of the wide-coverage
grammar developed in the XTAG project to build a wide-coverage CCG. To our
knowledge there have been no attempts to construct a large-scale CCG parser
with the lexicon to support it. In this paper, we describe such a system, built
by adapting various XTAG components to CCG. We find that, despite the
similarities between the formalisms, certain parts of the grammatical workload
are distributed differently. In addition, the flexibility of CCG derivations
allows the translated grammar to handle a number of ``non-constituent''
constructions which the XTAG grammar cannot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411010</id><created>1994-11-04</created><authors><author><keyname>Boitet</keyname><forenames>Christian</forenames><affiliation>GETA, IMAG</affiliation></author><author><keyname>Seligman</keyname><forenames>Mark</forenames><affiliation>ATR Interpreting Telecommunications Research Labs</affiliation></author></authors><title>The &quot;Whiteboard&quot; Architecture: a way to integrate heterogeneous
  components of NLP systems</title><categories>cmp-lg cs.CL</categories><comments>Postscript, 6 pages</comments><journal-ref>COLING-94</journal-ref><abstract>  We present a new software architecture for NLP systems made of heterogeneous
components, and demonstrate an architectural prototype we have built at ATR in
the context of Speech Translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411011</id><created>1994-11-04</created><authors><author><keyname>Gomez</keyname><forenames>Fernando</forenames><affiliation>UCF</affiliation></author><author><keyname>Hull</keyname><forenames>Richard</forenames><affiliation>UCF</affiliation></author><author><keyname>Segami</keyname><forenames>Carlos</forenames><affiliation>Barry University</affiliation></author></authors><title>Acquiring Knowledge from Encyclopedic Texts</title><categories>cmp-lg cs.CL</categories><comments>7 pages, 7 Postscript figures, uses aclap.sty</comments><journal-ref>Proceedings of the Fourth ACL Conference on Applied Natural
  Language Processing, Stuttgart, Germany, October 13-15, 1994</journal-ref><abstract>  A computational model for the acquisition of knowledge from encyclopedic
texts is described. The model has been implemented in a program, called SNOWY,
that reads unedited texts from {\em The World Book Encyclopedia}, and acquires
new concepts and conceptual relations about topics dealing with the dietary
habits of animals, their classifications and habitats. The program is also able
to answer an ample set of questions about the knowledge that it has acquired.
This paper describes the essential components of this model, namely semantic
interpretation, inferences and representation, and ends with an evaluation of
the performance of the program, a sample of the questions that it is able to
answer, and its relation to other programs of similar nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411012</id><created>1994-11-04</created><authors><author><keyname>Frank</keyname><forenames>Robert</forenames><affiliation>University of Delaware</affiliation></author></authors><title>From Regular to Context Free to Mildly Context Sensitive Tree Rewriting
  Systems: The Path of Child Language Acquisition</title><categories>cmp-lg cs.CL</categories><comments>4 pages</comments><report-no>TALANA-RT-94-01, TALANA, Universit\'{e} Paris 7, 1994</report-no><journal-ref>Appeared in {\em 3e Colloque International sur les grammaires
  d'Arbres Adjoints (TAG+3).}</journal-ref><abstract>  Current syntactic theory limits the range of grammatical variation so
severely that the logical problem of grammar learning is trivial. Yet, children
exhibit characteristic stages in syntactic development at least through their
sixth year. Rather than positing maturational delays, I suggest that
acquisition difficulties are the result of limitations in manipulating
grammatical representations. I argue that the genesis of complex sentences
reflects increasing generative capacity in the systems generating structural
descriptions: conjoined clauses demand only a regular tree rewriting system;
sentential embedding uses a context-free tree substitution grammar;
modification requires TAG, a mildly context-sensitive system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411013</id><created>1994-11-05</created><authors><author><keyname>Kim</keyname><forenames>Geunbae Lee Jong-Hyeok Lee Kyunghee</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory Pohang University of Science &amp; Technology, Korea</affiliation></author></authors><title>Phoneme-level speech and natural language intergration for agglutinative
  languages</title><categories>cmp-lg cs.CL</categories><comments>12 pages, Latex Postscript, compressed, uuencoded, will be presented
  in TWLT-8</comments><abstract>  A new tightly coupled speech and natural language integration model is
presented for a TDNN-based large vocabulary continuous speech recognition
system. Unlike the popular n-best techniques developed for integrating mainly
HMM-based speech and natural language systems in word level, which is obviously
inadequate for the morphologically complex agglutinative languages, our model
constructs a spoken language system based on the phoneme-level integration. The
TDNN-CYK spoken language architecture is designed and implemented using the
TDNN-based diphone recognition module integrated with the table-driven
phonological/morphological co-analysis. Our integration model provides a
seamless integration of speech and natural language for connectionist speech
recognition systems especially for morphologically complex languages such as
Korean. Our experiment results show that the speaker-dependent continuous
Eojeol (word) recognition can be integrated with the morphological analysis
with over 80\% morphological analysis success rate directly from the speech
input for the middle-level vocabularies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411014</id><created>1994-11-07</created><updated>1994-11-18</updated><authors><author><keyname>Pentheroudakis</keyname><forenames>Joseph</forenames></author><author><keyname>Vanderwende</keyname><forenames>Lucy</forenames></author><author><keyname>Corporation</keyname><forenames>Microsoft</forenames></author></authors><title>Automatically Identifying Morphological Relations in = Machine-Readable
  Dictionaries</title><categories>cmp-lg cs.CL</categories><comments>PostScript, 19 pages, 250kb; from Proceedings of the 9th Annual
  Conference of the UW Centre for the New OED and Text Research, 1993</comments><report-no>MSR-TR-93-06</report-no><abstract>  We describe an automated method for identifying classes of morphologically
related words in an on-line dictionary, and for linking individual senses in
the derived form to one or more senses in the base form by means of
morphological relation attributes. We also present an algorithm for computing a
score reflecting the system=92s certainty in these derivational links; this
computation relies on the content of semantic relations associated with each
sense, which are extracted automatically by parsing each sense definition and
subjecting the parse structure to automated semantic analysis. By processing
the entire set of headwords in the dictionary in this fashion we create a large
set of directed derivational graphs, which can then be accessed by other
components in our broad-coverage NLP system. Spurious or unlikely derivations
are not discarded, but are rather added to the dictionary and assigned a
negative score; this allows the system to handle non-standard uses of these
forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411015</id><created>1994-11-08</created><updated>1994-11-08</updated><authors><author><keyname>Maxwell</keyname><forenames>Michael</forenames><affiliation>Summer Institute of Linguistics</affiliation></author></authors><title>Parsing Using Linearly Ordered Phonological Rules</title><categories>cmp-lg cs.CL</categories><comments>105kb, 12 pages, published in Computational Phonology: First Meeting
  of the ACL Special Interest Group in Computational Phonology, 1994, pages
  59-70</comments><abstract>  A generate and test algorithm is described which parses a surface form into
one or more lexical entries using linearly ordered phonological rules. This
algorithm avoids the exponential expansion of search space which a naive
parsing algorithm would face by encoding into the form being parsed the
ambiguities which arise during parsing. The algorithm has been implemented and
tested on real language data, and its speed compares favorably with that of a
KIMMO-type parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411016</id><created>1994-11-09</created><authors><author><keyname>Abracos</keyname><forenames>Jose</forenames></author><author><keyname>Lopes</keyname><forenames>Jose Gabriel</forenames></author></authors><title>Extending DRT with a Focusing Mechanism for Pronominal Anaphora and
  Ellipsis Resolution</title><categories>cmp-lg cs.CL</categories><abstract>  Cormack (1992) proposed a framework for pronominal anaphora resolution. Her
proposal integrates focusing theory (Sidner et al.) and DRT (Kamp and Reyle).
We analyzed this methodology and adjusted it to the processing of Portuguese
texts. The scope of the framework was widened to cover sentences containing
restrictive relative clauses and subject ellipsis. Tests were conceived and
applied to probe the adequacy of proposed modifications when dealing with
processing of current texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411017</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411017</id><created>1994-11-10</created><authors><author><keyname>Grishman</keyname><forenames>Ralph</forenames><affiliation>Computer Science Department, New York University</affiliation></author><author><keyname>Macleod</keyname><forenames>Catherine</forenames><affiliation>Computer Science Department, New York University</affiliation></author><author><keyname>Meyers</keyname><forenames>Adam</forenames><affiliation>Computer Science Department, New York University</affiliation></author></authors><title>Comlex Syntax: Building a Computational Lexicon</title><categories>cmp-lg cs.CL</categories><abstract>  We describe the design of Comlex Syntax, a computational lexicon providing
detailed syntactic information for approximately 38,000 English headwords. We
consider the types of errors which arise in creating such a lexicon, and how
such errors can be measured and controlled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411018</id><created>1994-11-11</created><authors><author><keyname>Ro</keyname><forenames>Atle</forenames><affiliation>Department of Phonetics and Linguistics, University of Bergen</affiliation></author></authors><title>Interlanguage Signs and Lexical Transfer Errors</title><categories>cmp-lg cs.CL</categories><comments>Paper presented at COLING-94. 4 pages, Postscript</comments><abstract>  A theory of interlanguage (IL) lexicons is outlined, with emphasis on IL
lexical entries, based on the HPSG notion of lexical sign. This theory accounts
for idiosyncratic or lexical transfer of syntactic subcategorisation and idioms
from the first language to the IL. It also accounts for developmental stages in
IL lexical grammar, and grammatical variation in the use of the same lexical
item. The theory offers a tool for robust parsing of lexical transfer errors
and diagnosis of such errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411019</id><created>1994-11-11</created><updated>1994-11-14</updated><authors><author><keyname>Ramsay</keyname><forenames>Allan</forenames><affiliation>Department of Computer Science, Univ. College Dublin</affiliation></author></authors><title>Focus on ``only&quot; and ``Not&quot;</title><categories>cmp-lg cs.CL</categories><comments>LaTeX bug removed from previous version</comments><journal-ref>COLING-94, 881-885</journal-ref><abstract>  Krifka [1993] has suggested that focus should be seen as a means of providing
material for a range of semantic and pragmatic functions to work on, rather
than as a specific semantic or pragmatic function itself. The current paper
describes an implementation of this general idea, and applies it to the
interpretation of {\em only} and {\em not}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411020</id><created>1994-11-14</created><authors><author><keyname>Rentier</keyname><forenames>Gerrit</forenames><affiliation>Tilburg University</affiliation></author></authors><title>Extraction in Dutch with Lexical Rules</title><categories>cmp-lg cs.CL</categories><comments>Extension of KONVENS94 publication, 10 pages, PostScript</comments><report-no>ITK Research Report-no 53</report-no><abstract>  Unbounded dependencies are often modelled by ``traces'' (and ``gap
threading'') in unification-based grammars. Pollard and Sag, however, suggest
an analysis of extraction based on lexical rules, which excludes the notion of
traces (P&amp;S 1994, Chapter 9). In parsing, it suggests a trade of indeterminism
for lexical ambiguity. This paper provides a short introduction to this
approach to extraction with lexical rules, and illustrates the linguistic power
of the approach by applying it to particularly idiosyncratic Dutch extraction
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411021</id><created>1994-11-16</created><authors><author><keyname>Tojo</keyname><forenames>Satoshi</forenames><affiliation>Mitsubishi Research Institute, Inc</affiliation></author></authors><title>Free-ordered CUG on Chemical Abstract Machine</title><categories>cmp-lg cs.CL</categories><comments>COLING 94, five pages, one PS file of figure `t.eps'</comments><abstract>  We propose a paradigm for concurrent natural language generation. In order to
represent grammar rules distributively, we adopt categorial unification grammar
(CUG) where each category owns its functional type. We augment typed lambda
calculus with several new combinators, to make the order of lambda-conversions
free for partial / local processing. The concurrent calculus is modeled with
Chemical Abstract Machine. We show an example of a Japanese causative auxiliary
verb that requires a drastic rearrangement of case domination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411022</id><created>1994-11-16</created><updated>1994-11-21</updated><authors><author><keyname>Palmer</keyname><forenames>David D.</forenames></author><author><keyname>Hearst</keyname><forenames>Marti A.</forenames></author></authors><title>Adaptive Sentence Boundary Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>This is a Latex version of the previously submitted ps file
  (formatted as a uuencoded gz-compressed .tar file created by csh script). The
  software from the work described in this paper is available by contacting
  dpalmer@cs.berkeley.edu</comments><journal-ref>Proceedings of ANLP 94</journal-ref><abstract>  Labeling of sentence boundaries is a necessary prerequisite for many natural
language processing tasks, including part-of-speech tagging and sentence
alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate
them most systems use brittle, special-purpose regular expression grammars and
exception rules. As an alternative, we have developed an efficient, trainable
algorithm that uses a lexicon with part-of-speech probabilities and a
feed-forward neural network. After training for less than one minute, the
method correctly labels over 98.5\% of sentence boundaries in a corpus of over
27,000 sentence-boundary marks. We show the method to be efficient and easily
adaptable to different text genres, including single-case texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411023</id><created>1994-11-17</created><authors><author><keyname>Ono</keyname><forenames>Kenji</forenames></author><author><keyname>Sumita</keyname><forenames>Kazuo</forenames></author><author><keyname>Research</keyname><forenames>Seiji Miike</forenames></author><author><keyname>Center</keyname><forenames>Development</forenames></author><author><keyname>1</keyname><forenames>Toshiba Corporation Komukai-Toshiba-cho</forenames></author><author><keyname>Saiwai-ku</keyname></author><author><keyname>Kawasaki</keyname></author><author><keyname>210</keyname></author><author><keyname>Japan</keyname></author></authors><title>Abstract Generation based on Rhetorical Structure Extraction</title><categories>cmp-lg cs.CL</categories><comments>5 pages including 2 eps Figure, using epsbox.sty, art10.sty</comments><report-no>COLING-94, pp.344 - 348</report-no><abstract>  We have developed an automatic abstract generation system for Japanese
expository writings based on rhetorical structure extraction. The system first
extracts the rhetorical structure, the compound of the rhetorical relations
between sentences, and then cuts out less important parts in the extracted
structure to generate an abstract of the desired length.
  Evaluation of the generated abstract showed that it contains at maximum 74\%
of the most important sentences of the original text. The system is now
utilized as a text browser for a prototypical interactive document retrieval
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411024</id><created>1994-11-17</created><authors><author><keyname>Langer</keyname><forenames>Hagen</forenames><affiliation>University of Osnabrueck, Germany</affiliation></author></authors><title>Reverse Queries in DATR</title><categories>cmp-lg cs.CL</categories><comments>PostScript, 7 pages</comments><report-no>Proceedings of COLING-94, Vol. 2, pp. 1089-1095</report-no><abstract>  DATR is a declarative representation language for lexical information and as
such, in principle, neutral with respect to particular processing strategies.
Previous DATR compiler/interpreter systems support only one access strategy
that closely resembles the set of inference rules of the procedural semantics
of DATR (Evans &amp; Gazdar 1989a). In this paper we present an alternative access
strategy (reverse query strategy) for a non-trivial subset of DATR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411025</id><created>1994-11-17</created><authors><author><keyname>Erbach</keyname><forenames>Gregor</forenames><affiliation>University of the Saarland, Computational Linguistics Dept.</affiliation></author></authors><title>Multi-Dimensional Inheritance</title><categories>cmp-lg cs.CL</categories><comments>9 pages, styles: a4,figfont,eepic,epsf</comments><report-no>CLAUS Report 40</report-no><journal-ref>Proceedings of KONVENS 94 (ed. H. Trost), Vienna, pp. 102-111</journal-ref><abstract>  In this paper, we present an alternative approach to multiple inheritance for
typed feature structures. In our approach, a feature structure can be
associated with several types coming from different hierarchies (dimensions).
In case of multiple inheritance, a type has supertypes from different
hierarchies. We contrast this approach with approaches based on a single type
hierarchy where a feature structure has only one unique most general type, and
multiple inheritance involves computation of greatest lower bounds in the
hierarchy. The proposed approach supports current linguistic analyses in
constraint-based formalisms like HPSG, inheritance in the lexicon, and
knowledge representation for NLP systems. Finally, we show that
multi-dimensional inheritance hierarchies can be compiled into a Prolog term
representation, which allows to compute the conjunction of two types
efficiently by Prolog term unification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411026</id><created>1994-11-18</created><updated>1994-11-23</updated><authors><author><keyname>Gaschler</keyname><forenames>Jean</forenames><affiliation>GETA, IMAG</affiliation></author><author><keyname>Lafourcade</keyname><forenames>Mathieu</forenames><affiliation>GETA, IMAG</affiliation></author></authors><title>Manipulating Human-oriented Dictionaries with very simple tools</title><categories>cmp-lg cs.CL</categories><comments>Postscript, 4 pages</comments><report-no>COLING-94</report-no><abstract>  This paper presents a methodology for building and manipulating
human-oriented dictionaries. This methodology has been applied in the
construction of a French-English-Malay dictionary which has been obtained by
&quot;crossing&quot; semi-automatically two bilingual dictionaries. We use only Microsoft
Word, a specialized language for writing transcriptors and a small but powerful
dictionary tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411027</id><created>1994-11-20</created><updated>1995-10-07</updated><authors><author><keyname>Sornlertlamvanich</keyname><forenames>Virach</forenames></author><author><keyname>Pantachat</keyname><forenames>Wantanee</forenames></author><author><keyname>Meknavin</keyname><forenames>Surapant</forenames></author></authors><title>Classifier Assignment by Corpus-based Approach</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded gzip compressed Postscript file; COLING-94, Vol.1,
  pp.556-561</comments><abstract>  This paper presents an algorithm for selecting an appropriate classifier word
for a noun. In Thai language, it frequently happens that there is fluctuation
in the choice of classifier for a given concrete noun, both from the point of
view of the whole spe ech community and individual speakers. Basically, there
is no exect rule for classifier selection. As far as we can do in the
rule-based approach is to give a default rule to pick up a corresponding
classifier of each noun. Registration of classifier for each noun is limited to
the type of unit classifier because other types are open due to the meaning of
representation. We propose a corpus-based method (Biber, 1993; Nagao, 1993;
Smadja, 1993) which generates Noun Classifier Associations (NCA) to overcome
the problems in classifier assignment and semantic construction of noun phrase.
The NCA is created statistically from a large corpus and recomposed under
concept hierarchy constraints and frequency of occurrences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411028</id><created>1994-11-23</created><authors><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author></authors><title>The Speech-Language Interface in the Spoken Language Translator</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX. Published: Proceedings of TWLT-8, December 1994</comments><report-no>CRC-051</report-no><abstract>  The Spoken Language Translator is a prototype for practically useful systems
capable of translating continuous spoken language within restricted domains.
The prototype system translates air travel (ATIS) queries from spoken English
to spoken Swedish and to French. It is constructed, with as few modifications
as possible, from existing pieces of speech and language processing software.
The speech recognizer and language understander are connected by a fairly
conventional pipelined N-best interface. This paper focuses on the ways in
which the language processor makes intelligent use of the sentence hypotheses
delivered by the recognizer. These ways include (1) producing modified
hypotheses to reflect the possible presence of repairs in the uttered word
sequence; (2) fast parsing with a version of the grammar automatically
specialized to the more frequent constructions in the training corpus; and (3)
allowing syntactic and semantic factors to interact with acoustic ones in the
choice of a meaning structure for translation, so that the acoustically
preferred hypothesis is not always selected even if it is within linguistic
coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411029</id><created>1994-11-28</created><authors><author><keyname>Stolcke</keyname><forenames>Andreas</forenames><affiliation>SRI International, Menlo Park, CA 94025</affiliation></author></authors><title>An Efficient Probabilistic Context-Free Parsing Algorithm that Computes
  Prefix Probabilities</title><categories>cmp-lg cs.CL</categories><comments>45 pages. Slightly shortened version to appear in Computational
  Linguistics 21</comments><report-no>ICSI TR-93-065 (Revised 11/94)</report-no><abstract>  We describe an extension of Earley's parser for stochastic context-free
grammars that computes the following quantities given a stochastic context-free
grammar and an input string: a) probabilities of successive prefixes being
generated by the grammar; b) probabilities of substrings being generated by the
nonterminals, including the entire string being generated by the grammar; c)
most likely (Viterbi) parse of the string; d) posterior expected number of
applications of each grammar production, as required for reestimating rule
probabilities. (a) and (b) are computed incrementally in a single left-to-right
pass over the input. Our algorithm compares favorably to standard bottom-up
parsing methods for SCFGs in that it works efficiently on sparse grammars by
making use of Earley's top-down control structure. It can process any
context-free rule format without conversion to some normal form, and combines
computations for (a) through (d) in a single algorithm. Finally, the algorithm
has simple extensions for processing partially bracketed inputs, and for
finding partial parses and their likelihoods on ungrammatical inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411030</id><created>1994-11-29</created><updated>1994-11-30</updated><authors><author><keyname>Joshi</keyname><forenames>Aravind K</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Complexity of Scrambling: A New Twist to the Competence - Performance
  Distinction</title><categories>cmp-lg cs.CL</categories><comments>ps, 4 pages, Proceedings of TAG+3, 1994</comments><abstract>  In this paper we discuss the following issue: How do we decide whether a
certain property of language is a competence property or a performance
property? Our claim is that the answer to this question is not given a-priori.
The answer depends on the formal devices (formal grammars and machines)
available to us for describing language. We discuss this issue in the context
of the complexity of processing of center embedding (of relative clauses in
English) and scrambling (in German, for example) from arbitrary depths of
embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411031</id><created>1994-11-29</created><updated>1994-11-30</updated><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>CoGenTex, Ithaca, USA</affiliation></author><author><keyname>Mellish</keyname><forenames>Chris</forenames><affiliation>University of Edinburgh, UK</affiliation></author><author><keyname>Levine</keyname><forenames>John</forenames><affiliation>University of Edinburgh, UK</affiliation></author></authors><title>Automatic Generation of Technical Documentation</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed tar file, with LaTeX source and ps figures. Will
  appear in APPLIED ARTIFICIAL INTELLIGENCE journal, volume 9 (1995)</comments><abstract>  Natural-language generation (NLG) techniques can be used to automatically
produce technical documentation from a domain knowledge base and linguistic and
contextual models. We discuss this application of NLG technology from both a
technical and a usefulness (costs and benefits) perspective. This discussion is
based largely on our experiences with the IDAS documentation-generation
project, and the reactions various interested people from industry have had to
IDAS. We hope that this summary of our experiences with IDAS and the lessons we
have learned from it will be beneficial for other researchers who wish to build
technical-documentation generation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9411032</id><created>1994-11-30</created><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>CoGenTex, Ithaca, USA</affiliation></author></authors><title>Has a Consensus NL Generation Architecture Appeared, and is it
  Psycholinguistically Plausible?</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed tar file, containing LaTeX source and two style
  files. This paper appeared in the 1994 International NLG workshop</comments><abstract>  I survey some recent applications-oriented NL generation systems, and claim
that despite very different theoretical backgrounds, these systems have a
remarkably similar architecture in terms of the modules they divide the
generation process into, the computations these modules perform, and the way
the modules interact with each other. I also compare this `consensus
architecture' among applied NLG systems with psycholinguistic knowledge about
how humans speak, and argue that at least some aspects of the consensus
architecture seem to be in agreement with what is known about human language
production, despite the fact that psycholinguistic plausibility was not in
general a goal of the developers of the surveyed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412001</id><created>1994-12-01</created><authors><author><keyname>Lai</keyname><forenames>Bong Yeung Tom</forenames><affiliation>City University of Hong Kong &amp; Tsinghua University, Beijing</affiliation></author><author><keyname>Huang</keyname><forenames>Changning</forenames><affiliation>Tsinghua University, Beijing</affiliation></author></authors><title>Dependency Grammar and the Parsing of Chinese Sentences</title><categories>cmp-lg cs.CL</categories><comments>PostScript. 8 pages. Contains bitmap figures and will therefore print
  slowly. Will appear in Proceedings of 1994 Joint Conference of 8th ACLIC and
  2nd PacFoCoL, Kyoto, Aug. 10-12, 1994</comments><abstract>  Dependency Grammar has been used by linguists as the basis of the syntactic
components of their grammar formalisms. It has also been used in natural
language parsing. In China, attempts have been made to use this grammar
formalism to parse Chinese sentences using corpus-based techniques. This paper
reviews the properties of Dependency Grammar as embodied in four axioms for the
well-formedness conditions for dependency structures. It is shown that allowing
multiple governors as done by some followers of this formalism is unnecessary.
The practice of augmenting Dependency Grammar with functional labels is also
discussed in the light of building functional structures when the sentence is
parsed. This will also facilitate semantic interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412002</id><created>1994-12-05</created><authors><author><keyname>Collier</keyname><forenames>Robin</forenames><affiliation>Department of Computer Science, University of Sheffield, England</affiliation></author></authors><title>N-Gram Cluster Identification During Empirical Knowledge Representation
  Generation</title><categories>cmp-lg cs.CL</categories><abstract>  This paper presents an overview of current research concerning knowledge
extraction from technical texts. In particular, the use of empirical techniques
during the identification and generation of a semantic representation is
considered. A key step is the discovery of useful n-grams and correlations
between clusters of these n-grams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412003</id><created>1994-12-06</created><authors><author><keyname>Ueberla</keyname><forenames>Joerg P.</forenames><affiliation>Forum Technology - DRA Malvern</affiliation></author></authors><title>An Extended Clustering Algorithm for Statistical Language Models</title><categories>cmp-lg cs.CL</categories><comments>27 pages, latex, comments welcome</comments><report-no>DRA/CIS(CSE1)/RN94/13</report-no><abstract>  Statistical language models frequently suffer from a lack of training data.
This problem can be alleviated by clustering, because it reduces the number of
free parameters that need to be trained. However, clustered models have the
following drawback: if there is ``enough'' data to train an unclustered model,
then the clustered variant may perform worse. On currently used language
modeling corpora, e.g. the Wall Street Journal corpus, how do the performances
of a clustered and an unclustered model compare? While trying to address this
question, we develop the following two ideas. First, to get a clustering
algorithm with potentially high performance, an existing algorithm is extended
to deal with higher order N-grams. Second, to make it possible to cluster large
amounts of training data more efficiently, a heuristic to speed up the
algorithm is presented. The resulting clustering algorithm can be used to
cluster trigrams on the Wall Street Journal corpus and the language models it
produces can compete with existing back-off models. Especially when there is
only little training data available, the clustered models clearly outperform
the back-off models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412004</id><created>1994-12-10</created><authors><author><keyname>Light</keyname><forenames>Marc</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Schubert</keyname><forenames>Lenhart</forenames><affiliation>University of Rochester</affiliation></author></authors><title>Knowledge Representation for Lexical Semantics: Is Standard First Order
  Logic Enough?</title><categories>cmp-lg cs.CL</categories><comments>Presented at the &quot;Future of the Dictionary&quot; workshop, Grenoble,
  France (October, 1994), 12 pages PostScript</comments><abstract>  Natural language understanding applications such as interactive planning and
face-to-face translation require extensive inferencing. Many of these
inferences are based on the meaning of particular open class words. Providing a
representation that can support such lexically-based inferences is a primary
concern of lexical semantics. The representation language of first order logic
has well-understood semantics and a multitude of inferencing systems have been
implemented for it. Thus it is a prime candidate to serve as a lexical
semantics representation. However, we argue that FOL, although a good starting
point, needs to be extended before it can efficiently and concisely support all
the lexically-based inferences needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412005</id><created>1994-12-15</created><authors><author><keyname>Cartwright</keyname><forenames>Timothy Andrew</forenames></author><author><keyname>Brent</keyname><forenames>Michael R.</forenames></author></authors><title>Segmenting speech without a lexicon: The roles of phonotactics and
  speech source</title><categories>cmp-lg cs.CL</categories><comments>Uses wsuipa font package and latex-acl.sty</comments><abstract>  Infants face the difficult problem of segmenting continuous speech into words
without the benefit of a fully developed lexicon. Several sources of
information in speech might help infants solve this problem, including prosody,
semantic correlations and phonotactics. Research to date has focused on
determining to which of these sources infants might be sensitive, but little
work has been done to determine the potential usefulness of each source. The
computer simulations reported here are a first attempt to measure the
usefulness of distributional and phonotactic information in segmenting phoneme
sequences. The algorithms hypothesize different segmentations of the input into
words and select the best hypothesis according to the Minimum Description
Length principle. Our results indicate that while there is some useful
information in both phoneme distributions and phonotactic rules, the
combination of both sources is most useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412006</id><created>1994-12-19</created><authors><author><keyname>Briscoe</keyname><affiliation>University of Cambridge</affiliation></author><author><keyname>Ted</keyname><affiliation>University of Cambridge</affiliation></author><author><keyname>Waegner</keyname><affiliation>University of Cambridge</affiliation></author><author><keyname>Nick</keyname><affiliation>University of Cambridge</affiliation></author></authors><title>Robust stochastic parsing using the inside-outside algorithm</title><categories>cmp-lg cs.CL</categories><comments>Revised and updated version of paper from AAAI Workshop on
  Probabilistically-based Natural Language Processing Techniques, 1992, 16
  pages, uuencoded, compressed postscript</comments><abstract>  The paper describes a parser of sequences of (English) part-of-speech labels
which utilises a probabilistic grammar trained using the inside-outside
algorithm. The initial (meta)grammar is defined by a linguist and further rules
compatible with metagrammatical constraints are automatically generated. During
training, rules with very low probability are rejected yielding a wide-coverage
parser capable of ranking alternative analyses. A series of corpus-based
experiments describe the parser's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412007</id><created>1994-12-23</created><authors><author><keyname>Walther</keyname><forenames>Markus</forenames><affiliation>University of Duesseldorf</affiliation></author><author><keyname>Kroeger</keyname><forenames>Bernd J.</forenames><affiliation>University of Cologne, Germany</affiliation></author></authors><title>Coupling Phonology and Phonetics in a Constraint-Based Gestural Model</title><categories>cmp-lg cs.CL</categories><comments>English version of the German original: Walther, Markus and Kroeger,
  Bernd J. (1994): Phonologie-Phonetikkopplung in einem constraint- basierten
  gesturalen Modell. In: Harald Trost (ed.), Proceedings KONVENS '94, Vienna.
  10 pages, gzip'ed, uuencoded postscript</comments><abstract>  An implemented approach which couples a constraint-based phonology component
with an articulatory speech synthesizer is proposed. Articulatory gestures
ensure a tight connection between both components, as they comprise both
physical-phonetic and phonological aspects. The phonological modelling of e.g.
syllabification and phonological processes such as German final devoicing is
expressed in the constraint logic programming language CUF. Extending CUF by
arithmetic constraints allows the simultaneous description of both phonology
and phonetics. Thus declarative lexicalist theories of grammar such as HPSG may
be enriched up to the level of detailed phonetic realisation. Initial acoustic
demonstrations show that our approach is in principle capable of synthesizing
full utterances in a linguistically motivated fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9412008</id><created>1994-12-25</created><authors><author><keyname>Yosiyuki</keyname><forenames>Kobayasi</forenames></author><author><keyname>Takenobu</keyname><forenames>Takunaga</forenames></author><author><keyname>Hozumi</keyname><forenames>Tanaka</forenames></author></authors><title>Analysis of Japanese Compound Nouns using Collocational Information</title><categories>cmp-lg cs.CL</categories><comments>COLING'94 papar, Technical Paper at Tokyo Institute of Technology,
  6pages and LaTeX</comments><abstract>  Analyzing compound nouns is one of the crucial issues for natural language
processing systems, in particular for those systems that aim at a wide coverage
of domains. In this paper, we propose a method to analyze structures of
Japanese compound nouns by using both word collocations statistics and a
thesaurus. An experiment is conducted with 160,000 word collocations to analyze
compound nouns of with an average length of 4.9 characters. The accuracy of
this method is about 80%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9501001</id><created>1995-01-09</created><authors><author><keyname>Evans</keyname><forenames>Roger</forenames><affiliation>University of Brighton</affiliation></author><author><keyname>Gazdar</keyname><forenames>Gerald</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Weir</keyname><forenames>David</forenames><affiliation>University of Sussex</affiliation></author></authors><title>Using default inheritance to describe LTAG</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of TAG+3, 1994</journal-ref><abstract>We present the results of an investigation into how the set of elementary trees
of a Lexicalized Tree Adjoining Grammar can be represented in the lexical
knowledge representation language DATR (Evans &amp; Gazdar 1989a,b). The LTAG under
consideration is based on the one described in Abeille et al. (1990). Our
approach is similar to that of Vijay-Shanker &amp; Schabes (1992) in that we
formulate an inheritance hierarchy that efficiently encodes the elementary
trees. However, rather than creating a new representation formalism for this
task, we employ techniques of established utility in other lexically-oriented
frameworks. In particular, we show how DATR's default mechanism can be used to
eliminate the need for a non-immediate dominance relation in the descriptions
of the surface LTAG entries. This allows us to embed the tree structures in the
feature theory in a manner reminiscent of HPSG subcategorisation frames, and
hence express lexical rules as relations over feature structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9501002</id><created>1995-01-17</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames></author><author><keyname>Szummer</keyname><forenames>Marcin</forenames></author><author><keyname>Jarecki</keyname><forenames>Stanislaw</forenames></author><author><keyname>Johnson</keyname><forenames>David E.</forenames></author><author><keyname>Morgenstern</keyname><forenames>Leora</forenames></author></authors><title>NL Understanding with a Grammar of Constructions</title><categories>cmp-lg cs.CL</categories><comments>appeared in Proc. Coling'94, Kyoto, Japan, 1994; 5 postscript pages;
  email to wlodz@watson.ibm.com</comments><abstract>We present an approach to natural language understanding based on a computable
grammar of constructions. A &quot;construction&quot; consists of a set of features of
form and a description of meaning in a context. A grammar is a set of
constructions. This kind of grammar is the key element of Mincal, an
implemented natural language, speech-enabled interface to an on-line calendar
system. The system consists of a NL grammar, a parser, an on-line calendar, a
domain knowledge base (about dates, times and meetings), an application
knowledge base (about the calendar), a speech recognizer, a speech generator,
and the interfaces between those modules. We claim that this architecture
should work in general for spoken interfaces in small domains. In this paper we
present two novel aspects of the architecture: (a) the use of constructions,
integrating descriptions of form, meaning and context into one whole; and (b)
the separation of domain knowledge from application knowledge. We describe the
data structures for encoding constructions, the structure of the knowledge
bases, and the interactions of the key modules of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9501003</id><created>1995-01-18</created><authors><author><keyname>Quantz</keyname><forenames>J. Joachim</forenames><affiliation>Technische Universit&quot;at Berlin</affiliation></author></authors><title>An HPSG Parser Based on Description Logics</title><categories>cmp-lg cs.CL</categories><abstract>In this paper I present a parser based on Description Logics (DL) for a German
HPSG -style fragment. The specified parser relies mainly on the inferential
capabilities of the underlying DL system. Given a preferential default
extension for DL disambiguation is achieved by choosing the parse containing a
qualitatively minimal number of exceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9501004</id><created>1995-01-30</created><authors><author><keyname>Agirre</keyname><forenames>E.</forenames><affiliation>Informatika Fakultatea, Basque Country University</affiliation></author><author><keyname>Arregi</keyname><forenames>X.</forenames><affiliation>Informatika Fakultatea, Basque Country University</affiliation></author><author><keyname>Artola</keyname><forenames>X.</forenames><affiliation>Informatika Fakultatea, Basque Country University</affiliation></author><author><keyname>de Ilarraza</keyname><forenames>A. Diaz</forenames><affiliation>Informatika Fakultatea, Basque Country University</affiliation></author><author><keyname>Sarasola</keyname><forenames>K.</forenames><affiliation>Informatika Fakultatea, Basque Country University</affiliation></author></authors><title>Lexical Knowledge Representation in an Intelligent Dictionary Help
  System</title><categories>cmp-lg cs.CL</categories><comments>8 pages, postscript file, originally written in Mac Word</comments><journal-ref>Proceedings of COLING 94, Vol. 1, 544-550.</journal-ref><abstract>The frame-based knowledge representation model adopted in IDHS (Intelligent
Dictionary Help System) is described in this paper. It is used to represent the
lexical knowledge acquired automatically from a conventional dictionary.
Moreover, the enrichment processes that have been performed on the Dictionary
Knowledge Base and the dynamic exploitation of this knowledge - both based on
the exploitation of the properties of lexical semantic relations - are also
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9501005</id><created>1995-01-31</created><authors><author><keyname>Andry</keyname><forenames>Francois</forenames></author><author><keyname>Gawron</keyname><forenames>Mark</forenames></author><author><keyname>Dowding</keyname><forenames>John</forenames></author><author><keyname>Moore</keyname><forenames>Robert</forenames></author></authors><title>A Tool for Collecting Domain Dependent Sortal Constraints From Corpora</title><categories>cmp-lg cs.CL</categories><comments>COLING 94's paper - Latex document - 6 pages</comments><abstract>  In this paper, we describe a tool designed to generate semi-automatically the
sortal constraints specific to a domain to be used in a natural language (NL)
understanding system. This tool is evaluated using the SRI Gemini NL
understanding system in the ATIS domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502001</id><created>1995-02-02</created><authors><author><keyname>Serasset</keyname><forenames>Gilles</forenames><affiliation>GETA-IMAG, Universite de Grenoble 1 &amp; CNRS</affiliation></author></authors><title>Interlingual Lexical Organisation for Multilingual Lexical Databases in
  NADIA</title><categories>cmp-lg cs.CL</categories><comments>5 pages, Macintosh Postscript, published in COLING-94, pp. 278-282</comments><abstract>  We propose a lexical organisation for multilingual lexical databases (MLDB).
This organisation is based on acceptions (word-senses). We detail this lexical
organisation and show a mock-up built to experiment with it. We also present
our current work in defining and prototyping a specialised system for the
management of acception-based MLDB. Keywords: multilingual lexical database,
acception, linguistic structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502002</id><created>1995-02-03</created><authors><author><keyname>Osborne</keyname><forenames>Miles</forenames><affiliation>Dept. of Computer Science, University of York, York, England</affiliation></author></authors><title>Learning Unification-Based Natural Language Grammars</title><categories>cmp-lg cs.CL</categories><comments>DPhil thesis, self-unpacking latex file, 114 pages with 33 pages of
  appendices.</comments><abstract>When parsing unrestricted language, wide-covering grammars often undergenerate.
Undergeneration can be tackled either by sentence correction, or by grammar
correction. This thesis concentrates upon automatic grammar correction (or
machine learning of grammar) as a solution to the problem of undergeneration.
Broadly speaking, grammar correction approaches can be classified as being
either {\it data-driven}, or {\it model-based}. Data-driven learners use
data-intensive methods to acquire grammar. They typically use grammar
formalisms unsuited to the needs of practical text processing and cannot
guarantee that the resulting grammar is adequate for subsequent semantic
interpretation. That is, data-driven learners acquire grammars that generate
strings that humans would judge to be grammatically ill-formed (they {\it
overgenerate}) and fail to assign linguistically plausible parses. Model-based
learners are knowledge-intensive and are reliant for success upon the
completeness of a {\it model of grammaticality}. But in practice, the model
will be incomplete. Given that in this thesis we deal with undergeneration by
learning, we hypothesise that the combined use of data-driven and model-based
learning would allow data-driven learning to compensate for model-based
learning's incompleteness, whilst model-based learning would compensate for
data-driven learning's unsoundness. We describe a system that we have used to
test the hypothesis empirically. The system combines data-driven and
model-based learning to acquire unification-based grammars that are more
suitable for practical text parsing. Using the Spoken English Corpus as data,
and by quantitatively measuring undergeneration, overgeneration and parse
plausibility, we show that this hypothesis is correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502003</id><created>1995-02-05</created><authors><author><keyname>Erbach</keyname><forenames>Gregor</forenames><affiliation>Universitaet des Saarlandes, Comptational Linguistics Dept.</affiliation></author></authors><title>ProFIT: Prolog with Features, Inheritance and Templates</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, eaclap.sty</comments><report-no>CLAUS Report 42</report-no><abstract>ProFIT is an extension of Standard Prolog with Features, Inheritance and
Templates. ProFIT allows the programmer or grammar developer to declare an
inheritance hierarchy, features and templates. Sorted feature terms can be used
in ProFIT programs together with Prolog terms to provide a clearer description
language for linguistic structures. ProFIT compiles all sorted feature terms
into a Prolog term representation, so that the built-in Prolog term unification
can be used for the unification of sorted feature structures, and no special
unification algorithm is needed. ProFIT programs are compiled into Prolog
programs, so that no meta-interpreter is needed for their execution. ProFIT
thus provides a direct step from grammars developed with sorted feature terms
to Prolog programs usable for practical NLP systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502004</id><created>1995-02-05</created><authors><author><keyname>Erbach</keyname><forenames>Gregor</forenames><affiliation>University of the Saarland, Comptational Linguistics Dept.</affiliation></author></authors><title>Bottom-Up Earley Deduction</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, eaclap.sty</comments><report-no>CLAUS Report 39</report-no><journal-ref>Proceedings of COLING 94, pages 796-802</journal-ref><abstract>We propose a bottom-up variant of Earley deduction. Bottom-up deduction is
preferable to top-down deduction because it allows incremental processing (even
for head-driven grammars), it is data-driven, no subsumption check is needed,
and preference values attached to lexical items can be used to guide best-first
search. We discuss the scanning step for bottom-up Earley deduction and
indexing schemes that help avoid useless deduction steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502005</id><created>1995-02-07</created><authors><author><keyname>Minnen</keyname><forenames>Guido</forenames><affiliation>SFB340, University of Tuebingen</affiliation></author><author><keyname>Gerdemann</keyname><forenames>Dale</forenames><affiliation>SFB340, University of Tuebingen</affiliation></author><author><keyname>Goetz</keyname><forenames>Thilo</forenames><affiliation>SFB340, University of Tuebingen</affiliation></author></authors><title>Off-line Optimization for Earley-style HPSG Processing</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX (avm.sty, eaclap.sty and tree-dvips)</comments><journal-ref>Proceedings EACL 95</journal-ref><abstract>A novel approach to HPSG based natural language processing is described that
uses an off-line compiler to automatically prime a declarative grammar for
generation or parsing, and inputs the primed grammar to an advanced
Earley-style processor. This way we provide an elegant solution to the problems
with empty heads and efficient bidirectional processing which is illustrated
for the special case of HPSG generation. Extensive testing with a large HPSG
grammar revealed some important constraints on the form of the grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502006</id><created>1995-02-08</created><authors><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge, UK</affiliation></author></authors><title>Rapid Development of Morphological Descriptions for Full Language
  Processing Systems</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX (2.09 preferred); eaclap.sty; Procs of Euro ACL-95</comments><report-no>CRC-047; see http://www.cam.sri.com/</report-no><abstract>I describe a compiler and development environment for feature-augmented
two-level morphology rules integrated into a full NLP system. The compiler is
optimized for a class of languages including many or most European ones, and
for rapid development and debugging of descriptions of new languages. The key
design decision is to compose morphophonological and morphosyntactic
information, but not the lexicon, when compiling the description. This results
in typical compilation times of about a minute, and has allowed a reasonably
full, feature-based description of French inflectional morphology to be
developed in about a month by a linguist new to the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502007</id><created>1995-02-09</created><authors><author><keyname>Vagelatos</keyname><forenames>A.</forenames><affiliation>Computer Technology Institute &amp; Computer Eng. Dept. University of Patras</affiliation></author><author><keyname>Triantopoulou</keyname><forenames>T.</forenames><affiliation>Computer Technology Institute &amp; Computer Eng. Dept. University of Patras</affiliation></author><author><keyname>Tsalidis</keyname><forenames>C.</forenames><affiliation>Computer Technology Institute &amp; Computer Eng. Dept. University of Patras</affiliation></author><author><keyname>Christodoulakis</keyname><forenames>D.</forenames><affiliation>Computer Technology Institute &amp; Computer Eng. Dept. University of Patras</affiliation></author></authors><title>Utilization of a Lexicon for Spelling Correction in Modern Greek</title><categories>cmp-lg cs.CL</categories><comments>5 pages, PS File, gzip compressed, uuencoded, to be presented at
  SAC95, ACM Computing Week, Nashville, USA.</comments><abstract>In this paper we present an interactive spelling correction system for Modern
Greek. The entire system is based on a morphological lexicon. Emphasis is given
to the development of the lexicon, especially as far as storage economy, speed
efficiency and dictionary coverage is concerned. Extensive research was
conducted from both the computer engineering and linguisting fields, in order
to describe inflectional morphology as economically as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502008</id><created>1995-02-10</created><authors><author><keyname>Alexandersson</keyname><forenames>Jan</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author><author><keyname>Maier</keyname><forenames>Elisabeth</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author><author><keyname>Reithinger</keyname><forenames>Norbert</forenames><affiliation>DFKI GmbH, Saarbruecken, Germany</affiliation></author></authors><title>A Robust and Efficient Three-Layered Dialogue Component for a
  Speech-to-Speech Translation System</title><categories>cmp-lg cs.CL</categories><comments>Postscript file, compressed and uuencoded, 15 pages, to appear in
  Proceedings of EACL-95, Dublin.</comments><report-no>VERBMOBIL Report No. 50, December 1994</report-no><abstract>We present the dialogue component of the speech-to-speech translation system
VERBMOBIL. In contrast to conventional dialogue systems it mediates the
dialogue while processing maximally 50% of the dialogue in depth. Special
requirements like robustness and efficiency lead to a 3-layered hybrid
architecture for the dialogue module, using statistics, an automaton and a
planner. A dialogue memory is constructed incrementally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502009</id><created>1995-02-09</created><authors><author><keyname>Ribas</keyname><forenames>Francesc</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author></authors><title>On Learning More Appropriate Selectional Restrictions</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX (eaclap.sty)</comments><journal-ref>Proceedings EACL-95, Ireland</journal-ref><abstract>We present some variations affecting the association measure and thresholding
on a technique for learning Selectional Restrictions from on-line corpora. It
uses a wide-coverage noun taxonomy and a statistical measure to generalize the
appropriate semantic classes. Evaluation measures for the Selectional
Restrictions learning task are discussed. Finally, an experimental evaluation
of these variations is reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502010</id><created>1995-02-13</created><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Computational Linguistics, University of Helsinki, Finland</affiliation></author></authors><title>NPtool, a detector of English noun phrases</title><categories>cmp-lg cs.CL</categories><comments>uuencoded and gzipped .ps, 10 pages.</comments><abstract>NPtool is a fast and accurate system for extracting noun phrases from English
texts for the purposes of e.g. information retrieval, translation unit
discovery, and corpus studies. After a general introduction, the system
architecture is presented in outline. Then follows an examination of a recently
written Constraint Syntax. An evaluation report concludes the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502011</id><created>1995-02-13</created><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Multilingual Language Technology, University of Helsinki, Finland</affiliation></author><author><keyname>Jarvinen</keyname><forenames>Timo</forenames><affiliation>Research Unit for Multilingual Language Technology, University of Helsinki, Finland</affiliation></author></authors><title>Specifying a shallow grammatical representation for parsing purposes</title><categories>cmp-lg cs.CL</categories><comments>EACL95, uuencoded and gzipped .ps</comments><abstract>Is it possible to specify a grammatical representation (descriptors and their
application guidelines) to such a degree that it can be consistently applied by
different grammarians e.g. for producing a benchmark corpus for parser
evaluation? Arguments for and against have been given, but very little
empirical evidence. In this article we report on a double-blind experiment with
a surface-oriented morphosyntactic grammatical representation used in a
large-scale English parser. We argue that a consistently applicable
representation for morphology and also shallow syntax can be specified. A
grammatical representation with a near-100% coverage of running text can be
specified with a reasonable effort, especially if the representation is based
on structural distinctions (i.e. it is structurally resolvable).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502012</id><created>1995-02-13</created><updated>1995-02-14</updated><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Multilingual Language Technology, University of Helsinki, Finland</affiliation></author></authors><title>A syntax-based part-of-speech analyser</title><categories>cmp-lg cs.CL</categories><comments>EACL95, uuencoded and gzipped .ps. (Bibliographic mistake corrected.)</comments><abstract>There are two main methodologies for constructing the knowledge base of a
natural language analyser: the linguistic and the data-driven. Recent
state-of-the-art part-of-speech taggers are based on the data-driven approach.
Because of the known feasibility of the linguistic rule-based approach at
related levels of description, the success of the data-driven approach in
part-of-speech analysis may appear surprising. In this paper, a case is made
for the syntactic nature of part-of-speech tagging. A new tagger of English
that uses only linguistic distributional rules is outlined and empirically
evaluated. Tested against a benchmark corpus of 38,000 words of previously
unseen text, this syntax-based system reaches an accuracy of above 99%.
Compared to the 95-97% accuracy of its best competitors, this result suggests
the feasibility of the linguistic approach also in part-of-speech analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502013</id><created>1995-02-13</created><authors><author><keyname>Voutilainen</keyname><forenames>Atro</forenames><affiliation>Research Unit for Computational Linguistics, University of Helsinki, Finland</affiliation></author><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames><affiliation>Research Unit for Computational Linguistics, University of Helsinki, Finland</affiliation></author></authors><title>Ambiguity resolution in a reductionistic parser</title><categories>cmp-lg cs.CL</categories><comments>EACL93, .ps</comments><abstract>  We are concerned with dependency-oriented morphosyntactic parsing of running
text. While a parsing grammar should avoid introducing structurally
unresolvable distinctions in order to optimise on the accuracy of the parser,
it also is beneficial for the grammarian to have as expressive a structural
representation available as possible. In a reductionistic parsing system this
policy may result in considerable ambiguity in the input; however, even massive
ambiguity can be tackled efficiently with an accurate parsing description and
effective parsing technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502014</id><created>1995-02-13</created><authors><author><keyname>Crouch</keyname><forenames>Richard</forenames><affiliation>SRI International, Cambridge, UK</affiliation></author></authors><title>Ellipsis and Quantification: a substitutional approach</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX uses eaclap.sty; Procs of Euro ACL-95</comments><report-no>CRC-054; see http://www.cam.sri.com/</report-no><abstract>The paper describes a substitutional approach to ellipsis resolution giving
comparable results to Dalrymple, Shieber and Pereira (1991), but without the
need for order-sensitive interleaving of quantifier scoping and ellipsis
resolution. It is argued that the order-independence results from viewing
semantic interpretation as building a description of a semantic composition,
instead of the more common view of interpretation as actually performing the
composition
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502015</id><created>1995-02-13</created><authors><author><keyname>Kehler</keyname><forenames>Andrew</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Dalrymple</keyname><forenames>Mary</forenames><affiliation>Xerox PARC</affiliation></author><author><keyname>Lamping</keyname><forenames>John</forenames><affiliation>Xerox PARC</affiliation></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames><affiliation>Xerox PARC</affiliation></author></authors><title>The Semantics of Resource Sharing in Lexical-Functional Grammar</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in EACL-95. Requires eaclap.sty, tree-dvips.sty,
  tree-dvips.pro, lingmacros.sty, dgmacros.tex, lfgmacros.tex. Comments
  welcome.</comments><journal-ref>Proceedings of EACL-95</journal-ref><abstract>We argue that the resource sharing that is commonly manifest in semantic
accounts of coordination is instead appropriately handled in terms of
structure-sharing in LFG f-structures. We provide an extension to the previous
account of LFG semantics (Dalrymple et al., 1993b) according to which
dependencies between f-structures are viewed as resources; as a result a
one-to-one correspondence between uses of f-structures and meanings is
maintained. The resulting system is sufficiently restricted in cases where
other approaches overgenerate; the very property of resource-sensitivity for
which resource sharing appears to be problematic actually provides explanatory
advantages over systems that more freely replicate resources during derivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502016</id><created>1995-02-14</created><updated>1995-02-17</updated><authors><author><keyname>Morrill</keyname><forenames>Glyn</forenames></author></authors><title>Higher-order Linear Logic Programming of Categorial Deduction</title><categories>cmp-lg cs.CL</categories><comments>8 pages LaTeX, uses eaclap.sty, to appear EACL95</comments><abstract>We show how categorial deduction can be implemented in higher-order (linear)
logic programming, thereby realising parsing as deduction for the associative
and non-associative Lambek calculi. This provides a method of solution to the
parsing problem of Lambek categorial grammar applicable to a variety of its
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502017</id><created>1995-02-14</created><updated>1995-02-16</updated><authors><author><keyname>Manandhar</keyname><forenames>Suresh</forenames></author></authors><title>Deterministic Consistency Checking of LP Constraints</title><categories>cmp-lg cs.CL</categories><comments>EACL'95, 8 pages, LaTeX, eepic.sty, epsf.sty, eaclap.sty, figures -
  tar-ed, gzip-ed, uuencode-d</comments><abstract>We provide a constraint based computational model of linear precedence as
employed in the HPSG grammar formalism. An extended feature logic which adds a
wide range of constraints involving precedence is described. A sound, complete
and terminating deterministic constraint solving procedure is given.
Deterministic computational model is achieved by weakening the logic such that
it is sufficient for linguistic applications involving word-order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502018</id><created>1995-02-15</created><updated>1995-02-20</updated><authors><author><keyname>Hitzeman</keyname><forenames>Janet</forenames></author><author><keyname>Moens</keyname><forenames>Marc</forenames></author><author><keyname>Grover</keyname><forenames>Claire</forenames></author></authors><title>Algorithms for Analysing the Temporal Structure of Discourse</title><categories>cmp-lg cs.CL</categories><comments>EACL '95, 8 pages, 1 eps picture, tar-ed, compressed, uuencoded, uses
  eaclap.sty, a4wide.sty, epsf.tex</comments><abstract>We describe a method for analysing the temporal structure of a discourse which
takes into account the effects of tense, aspect, temporal adverbials and
rhetorical structure and which minimises unnecessary ambiguity in the temporal
structure. It is part of a discourse grammar implemented in Carpenter's ALE
formalism. The method for building up the temporal structure of the discourse
combines constraints and preferences: we use constraints to reduce the number
of possible structures, exploiting the HPSG type hierarchy and unification for
this purpose; and we apply preferences to choose between the remaining options
using a temporal centering mechanism. We end by recommending that an
underspecified representation of the structure using these techniques be used
to avoid generating the temporal/rhetorical structure until higher-level
information can be used to disambiguate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502019</id><created>1995-02-15</created><authors><author><keyname>Hoffman</keyname><forenames>Beryl</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Integrating &quot;Free&quot; Word Order Syntax and Information Structure</title><categories>cmp-lg cs.CL</categories><comments>8 pages PostScript in EACL 95</comments><abstract>  This paper describes a combinatory categorial formalism called Multiset-CCG
that can capture the syntax and interpretation of ``free'' word order in
languages such as Turkish. The formalism compositionally derives the
predicate-argument structure and the information structure (e.g. topic, focus)
of a sentence in parallel, and uniformly handles word order variation among the
arguments and adjuncts within a clause, as well as in complex clauses and
across clause boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502020</id><created>1995-02-14</created><authors><author><keyname>Morawietz</keyname><forenames>Frank</forenames><affiliation>Master's Thesis, University of Tuebingen, Germany</affiliation></author></authors><title>Formalization and Parsing of Typed Unification-Based ID/LP Grammars</title><categories>cmp-lg cs.CL</categories><comments>paper (81 pages), appendix (17 pages, Prolog code), format: .ps
  compressed and uuencoded</comments><abstract>This paper defines unification based ID/LP grammars based on typed feature
structures as nonterminals and proposes a variant of Earley's algorithm to
decide whether a given input sentence is a member of the language generated by
a particular typed unification ID/LP grammar. A solution to the problem of the
nonlocal flow of information in unification ID/LP grammars as discussed in
Seiffert (1991) is incorporated into the algorithm. At the same time, it tries
to connect this technical work with linguistics by presenting an example of the
problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa
1994, Richter and Sailer 1995) and with computational linguistics by drawing
connections from this approach to systems implementing HPSG, especially the
TROLL system, Gerdemann et al. (forthcoming).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502021</id><created>1995-02-17</created><authors><author><keyname>Keller</keyname><forenames>Bill</forenames><affiliation>School of Cognitive &amp; Computing Sciences, University of Sussex, UK</affiliation></author><author><keyname>Weir</keyname><forenames>David</forenames><affiliation>School of Cognitive &amp; Computing Sciences, University of Sussex, UK</affiliation></author></authors><title>A Tractable Extension of Linear Indexed Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages LaTeX, uses eaclap.sty, to appear in EACL-95</comments><abstract>  It has been shown that Linear Indexed Grammars can be processed in polynomial
time by exploiting constraints which make possible the extensive use of
structure-sharing. This paper describes a formalism that is more powerful than
Linear Indexed Grammar, but which can also be processed in polynomial time
using similar techniques. The formalism, which we refer to as Partially Linear
PATR manipulates feature structures rather than stacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502022</id><created>1995-02-17</created><authors><author><keyname>Brew</keyname><forenames>Chris</forenames><affiliation>Human Communication Research Centre, Edinburgh University</affiliation></author></authors><title>Stochastic HPSG</title><categories>cmp-lg cs.CL</categories><comments>7 pages, eaclap, Proceedings of EACL-95</comments><abstract>In this paper we provide a probabilistic interpretation for typed feature
structures very similar to those used by Pollard and Sag. We begin with a
version of the interpretation which lacks a treatment of re-entrant feature
structures, then provide an extended interpretation which allows them. We
sketch algorithms allowing the numerical parameters of our probabilistic
interpretations of HPSG to be estimated from corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502023</id><created>1995-02-18</created><authors><author><keyname>Nelken</keyname><forenames>Rani</forenames><affiliation>Tel Aviv University, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>The Technion, Israel</affiliation></author><author><keyname>.</keyname></author></authors><title>Splitting the Reference Time: Temporal Anaphora and Quantification in
  DRT</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, uses eaclap.sty, to appear in Proceedings of EACL-95</comments><report-no>#LCL 94-10</report-no><abstract>This paper presents an analysis of temporal anaphora in sentences which contain
quantification over events, within the framework of Discourse Representation
Theory. The analysis in (Partee 1984) of quantified sentences, introduced by a
temporal connective, gives the wrong truth-conditions when the temporal
connective in the subordinate clause is &quot;before&quot; or &quot;after&quot;. This problem has
been previously analyzed in (de Swart 1991) as an instance of the proportion
problem, and given a solution from a Generalized Quantifier approach. By using
a careful distinction between the different notions of reference time, based on
(Kamp and Reyle 1993), we propose a solution to this problem, within the
framework of DRT. We show some applications of this solution to additional
temporal anaphora phenomena in quantified sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502024</id><created>1995-02-20</created><updated>1995-02-21</updated><authors><author><keyname>Lee</keyname><forenames>Kong Joo</forenames></author><author><keyname>Kweon</keyname><forenames>Cheol Jung</forenames></author><author><keyname>Seo</keyname><forenames>Jungyun</forenames></author><author><keyname>Kim</keyname><forenames>Gil Chang</forenames></author></authors><title>A Robust Parser Based on Syntactic Information</title><categories>cmp-lg cs.CL</categories><comments>6 pages LaTeX, uses eaclap.sty, to appear in EACL-95.</comments><abstract>In this paper, we propose a robust parser which can parse extragrammatical
sentences. This parser can recover them using only syntactic information. It
can be easily modified and extended because it utilize only syntactic
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502025</id><created>1995-02-21</created><authors><author><keyname>Frank</keyname><forenames>Anette</forenames><affiliation>Institute for Computational Linguistics University of Stuttgart</affiliation></author><author><keyname>Reyle</keyname><forenames>Uwe</forenames><affiliation>Institute for Computational Linguistics University of Stuttgart</affiliation></author></authors><title>Principle Based Semantics for HPSG</title><categories>cmp-lg cs.CL</categories><comments>EACL-95</comments><abstract>  The paper presents a constraint based semantic formalism for HPSG. The
syntax-semantics interface directly implements syntactic conditions on
quantifier scoping and distributivity. The construction of semantic
representations is guided by general principles governing the interaction
between syntax and semantics. Each of these principles acts as a constraint to
narrow down the set of possible interpretations of a sentence. Meanings of
ambiguous sentences are represented by single partial representations
(so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which
further constraints can be added monotonically to gain more information about
the content of a sentence. There is no need to build up a large number of
alternative representations of the sentence which are then filtered by
subsequent discourse and world knowledge. The advantage of UDRSs is not only
that they allow for monotonic incremental interpretation but also that they are
equipped with truth conditions and a proof theory that allows for inferences to
be drawn directly on structures where quantifier scope is not resolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502026</id><created>1995-02-21</created><updated>1995-02-22</updated><authors><author><keyname>Reyle</keyname><forenames>Uwe</forenames><affiliation>Institute for Computational Linguistics, University of Stuttgart</affiliation></author></authors><title>On Reasoning with Ambiguities</title><categories>cmp-lg cs.CL</categories><comments>EACL 1995</comments><abstract>  The paper adresses the problem of reasoning with ambiguities. Semantic
representations are presented that leave scope relations between quantifiers
and/or other operators unspecified. Truth conditions are provided for these
representations and different consequence relations are judged on the basis of
intuitive correctness. Finally inference patterns are presented that operate
directly on these underspecified structures, i.e. do not rely on any
translation into the set of their disambiguations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502027</id><created>1995-02-21</created><authors><author><keyname>Keller</keyname><forenames>Frank</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>Towards an Account of Extraposition in HPSG</title><categories>cmp-lg cs.CL</categories><comments>6 pages Postscript; use uudecode and gunzip to decode</comments><journal-ref>Proceedings of the EACL-95, Dublin</journal-ref><abstract>This paper investigates the syntax of extraposition in the HPSG framework. We
present English and German data (partly taken from corpora), and provide an
analysis using lexical rules and a nonlocal dependency. The condition for
binding this dependency is formulated relative to the antecedent of the
extraposed phrase, which entails that no fixed site for extraposition exists.
Our analysis accounts for the interaction of extraposition with fronting and
coordination, and predicts constraints on multiple extraposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502028</id><created>1995-02-22</created><authors><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>Southern Methodist University</affiliation></author><author><keyname>Chen</keyname><forenames>Weidong</forenames><affiliation>Southern Methodist University</affiliation></author></authors><title>Lexical Acquisition via Constraint Solving</title><categories>cmp-lg cs.CL</categories><comments>6 pages, AAAI 1995 Spring Symposium Series</comments><abstract>This paper describes a method to automatically acquire the syntactic and
semantic classifications of unknown words. Our method reduces the search space
of the lexical acquisition problem by utilizing both the left and the right
context of the unknown word. Link Grammar provides a convenient framework in
which to implement our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502029</id><created>1995-02-23</created><authors><author><keyname>Chen</keyname><forenames>Kuang-hua</forenames><affiliation>Department of Computer Science and Information Engineering, National Taiwan University</affiliation></author></authors><title>Topic Identification in Discourse</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uuencoded and compressed postscript file (EACL 95)</comments><abstract>This paper proposes a corpus-based language model for topic identification. We
analyze the association of noun-noun and noun-verb pairs in LOB Corpus. The
word association norms are based on three factors: 1) word importance, 2) pair
co-occurrence, and 3) distance. They are trained on the paragraph and sentence
levels for noun-noun and noun-verb pairs, respectively. Under the topic
coherence postulation, the nouns that have the strongest connectivities with
the other nouns and verbs in the discourse form the preferred topic set. The
collocational semantics then is used to identify the topics from paragraphs and
to discuss the topic shift phenomenon among paragraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502030</id><created>1995-02-23</created><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Dept. of comp. sci, POSTECH, Korea.</affiliation></author><author><keyname>Jung</keyname><forenames>Hanmin</forenames><affiliation>Dept. of comp. sci, POSTECH, Korea.</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Dept. of comp. sci, POSTECH, Korea.</affiliation></author></authors><title>Bi-directional memory-based dialog translation: The KEMDT approach</title><categories>cmp-lg cs.CL</categories><comments>latex postscript with psfig, 7 pages, to be presented at pacific
  association for computational lingusitics conference (pacling95)</comments><abstract>A bi-directional Korean/English dialog translation system is designed and
implemented using the memory-based translation technique. The system KEMDT
(Korean/English Memory-based Dialog Translation system) can perform Korean to
English, and English to Korean translation using unified memory network and
extended marker passing algorithm. We resolve the word order variation and
frequent word omission problems in Korean by classifying the concept sequence
element in four different types and extending the marker-
passing-based-translation algorithm. Unlike the previous memory-based
translation systems, the KEMDT system develops the bilingual memory network and
the unified bi-directional marker passing translation algorithm. For efficient
language specific processing, we separate the morphological processors from the
memory-based translator. The KEMDT technology provides a hierarchical memory
network and an efficient marker-based control for the recent example-based MT
paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502031</id><created>1995-02-23</created><authors><author><keyname>Bowden</keyname><forenames>Tanya</forenames><affiliation>Computer Lab, University of Cambridge</affiliation></author></authors><title>Cooperative Error Handling and Shallow Processing</title><categories>cmp-lg cs.CL</categories><comments>EACL 95 (student session)</comments><abstract>This paper is concerned with the detection and correction of sub-sentential
English text errors. Previous spelling programs, unless restricted to a very
small set of words, have operated as post-processors. And to date, grammar
checkers and other programs which deal with ill-formed input usually step
directly from spelling considerations to a full-scale parse, assuming a
complete sentence. Work described below is aimed at evaluating the
effectiveness of shallow (sub-sentential) processing and the feasibility of
cooperative error checking, through building and testing appropriately an
error-processing system. A system under construction is outlined which
incorporates morphological checks (using new two-level error rules) over a
directed letter graph, tag positional trigrams and partial parsing. Intended
testing is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502032</id><created>1995-02-23</created><authors><author><keyname>Estival</keyname><forenames>Dominique</forenames><affiliation>ISSCO, Universite de Geneve</affiliation></author><author><keyname>Gayral</keyname><forenames>Francoise</forenames><affiliation>LIPN, Universite Paris-Nord</affiliation></author></authors><title>An NLP Approach to a Specific Type of Texts: Car Accident Reports</title><categories>cmp-lg cs.CL</categories><comments>20 pages</comments><abstract>The work reported here is the result of a study done within a larger project on
the ``Semantics of Natural Languages'' viewed from the field of Artificial
Intelligence and Computational Linguistics. In this project, we have chosen a
corpus of insurance claim reports. These texts deal with a relatively
circumscribed domain, that of road traffic, thereby limiting the
extra-linguistic knowledge necessary to understand them. Moreover, these texts
present a number of very specific characteristics, insofar as they are written
in a quasi-institutional setting which imposes many constraints on their
production. We first determine what these constraints are in order to then show
how they provide the writer with the means to create as succint a text as
possible, and in a symmetric way, how they provide the reader with the means to
interpret the text and to distinguish between its factual and argumentative
aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502033</id><created>1995-02-24</created><authors><author><keyname>Azzam</keyname><forenames>Saliha</forenames><affiliation>University of La Sorbonne</affiliation></author></authors><title>An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation
  Process</title><categories>cmp-lg cs.CL</categories><comments>EACL 95 (student session)(re-revised, minor changes: correction of
  affiliation)</comments><abstract>This paper concerns both anaphora resolution and prepositional phrase (PP)
attachment that are the most frequent ambiguities in natural language
processing. Several methods have been proposed to deal with each phenomenon
separately, however none of proposed systems has considered the way of dealing
both phenomena. We tackle this issue, proposing an algorithm to co-ordinate the
treatment of these two problems efficiently, i.e., the aim is also to exploit
at each step all the results that each component can provide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502034</id><created>1995-02-24</created><authors><author><keyname>Huckle</keyname><forenames>Christopher C.</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Grouping Words Using Statistical Context</title><categories>cmp-lg cs.CL</categories><comments>This file should be converted using uudecode followed by uncompress.
  It is in PostScript format, and 3 pages in length</comments><abstract>  This paper (cmp-lg/yymmnnn) has been accepted for publication in the student
session of EACL-95. It outlines ongoing work using statistical and unsupervised
neural network methods for clustering words in untagged corpora. Such
approaches are of interest when attempting to understand the development of
human intuitive categorization of language as well as for trying to improve
computational methods in natural language understanding. Some preliminary
results using a simple statistical approach are described, along with work
using an unsupervised neural network to distinguish between the sense classes
into which words fall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502035</id><created>1995-02-24</created><authors><author><keyname>Sturt</keyname><forenames>Patrick</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>Incorporating &quot;Unconscious Reanalysis&quot; into an Incremental, Monotonic
  Parser</title><categories>cmp-lg cs.CL</categories><comments>standard Latex209 6 pages</comments><abstract>  This paper describes an implementation based on a recent model in the
psycholinguistic literature. We define a parsing operation which allows the
reanalysis of dependencies within an incremental and monotonic processing
architecture, and discuss search strategies for its application in a
head-initial language (English) and a head-final language (Japanese).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502036</id><created>1995-02-27</created><authors><author><keyname>Groenink</keyname><forenames>Annius V.</forenames><affiliation>CWI, Amsterdam</affiliation></author></authors><title>Literal Movement Grammars</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Postscript. To be presented at EACL95, March 1995, Dublin</comments><abstract>Literal movement grammars (LMGs) provide a general account of extraposition
phenomena through an attribute mechanism allowing top-down displacement of
syntactical information. LMGs provide a simple and efficient treatment of
complex linguistic phenomena such as cross-serial dependencies in German and
Dutch---separating the treatment of natural language into a parsing phase
closely resembling traditional context-free treatment, and a disambiguation
phase which can be carried out using matching, as opposed to full unification
employed in most current grammar formalisms of linguistical relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502037</id><created>1995-02-27</created><authors><author><keyname>Tugwell</keyname><forenames>David</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>A State-Transition Grammar for Data-Oriented Parsing</title><categories>cmp-lg cs.CL</categories><comments>Latex 2e, 6 pages, EACL 95 student session</comments><abstract>This paper presents a grammar formalism designed for use in data-oriented
approaches to language processing. The formalism is best described as a
right-linear indexed grammar extended in linguistically interesting ways. The
paper goes on to investigate how a corpus pre-parsed with this formalism may be
processed to provide a probabilistic language model for use in the parsing of
fresh texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502038</id><created>1995-02-27</created><authors><author><keyname>Feldweg</keyname><forenames>Helmut</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Implementation and evaluation of a German HMM for POS disambiguation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses eaclap.sty, EACL SIGDAT workshop 1995</comments><journal-ref>Proceedings of the ACL SIGDAT Workshop, Dublin 1995</journal-ref><abstract>A German language model for the Xerox HMM tagger is presented. This model's
performance is compared with two other German taggers with partial parameter
re-estimation and full adaption of parameters from pre-tagged corpora. The
ambiguity types resolved by this model are analysed and compared to ambiguity
types of English and French. Finally, the model's error types are described. I
argue that although the overall performance of these models for German is
comparable to results for English and French, a more exact analysis
demonstrates important differences in the types of disambiguation involved for
German.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502039</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9502039</id><created>1995-02-28</created><updated>1995-03-10</updated><authors><author><keyname>Giguet</keyname><forenames>Emmanuel</forenames></author></authors><title>Multilingual Sentence Categorization according to Language</title><categories>cmp-lg cs.CL</categories><comments>4 pages --- LaTeX</comments><journal-ref>Eacl 95 SIGDAT Workshop : From text to tags</journal-ref><abstract>  In this paper, we describe an approach to sentence categorization which has
the originality to be based on natural properties of languages with no training
set dependency. The implementation is fast, small, robust and textual errors
tolerant. Tested for french, english, spanish and german discrimination, the
system gives very interesting results, achieving in one test 99.4% correct
assignments on real sentences.
 The resolution power is based on grammatical words (not the most common words)
and alphabet. Having the grammatical words and the alphabet of each language at
its disposal, the system computes for each of them its likelihood to be
selected. The name of the language having the optimum likelihood will tag the
sentence --- but non resolved ambiguities will be maintained. We will discuss
the reasons which lead us to use these linguistic facts and present several
directions to improve the system's classification performance.
  Categorization sentences with linguistic properties shows that difficult
problems have sometimes simple solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503001</id><created>1995-03-01</created><authors><author><keyname>Guvenir</keyname><forenames>H. Altay</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Bilkent University, Ankara, Turkey</affiliation></author></authors><title>Using a Corpus for Teaching Turkish Morphology</title><categories>cmp-lg cs.CL</categories><comments>uuencoded gzip'ed postscript file. Appeared in Proceedings of TWLT-7,
  University of Twente, The Netherlands, June 1994. Software described is
  available at ftp://ftp.cs.bilkent.edu.tr/pub/Turklang/corpus-search/</comments><report-no>Bilkent University CS Dept Tech Report BU-CEIS-9423</report-no><abstract>This paper reports on the preliminary phase of our ongoing research towards
developing an intelligent tutoring environment for Turkish grammar. One of the
components of this environment is a corpus search tool which, among other
aspects of the language, will be used to present the learner sample sentences
along with their morphological analyses. Following a brief introduction to the
Turkish language and its morphology, the paper describes the morphological
analysis and ambiguity resolution used to construct the corpus used in the
search tool. Finally, implementation issues and details involving the user
interface of the tool are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503002</id><created>1995-03-01</created><authors><author><keyname>Kessler</keyname><forenames>Brett</forenames><affiliation>Stanford University</affiliation></author></authors><title>Computational dialectology in Irish Gaelic</title><categories>cmp-lg cs.CL</categories><abstract>Dialect groupings can be discovered objectively and automatically by cluster
analysis of phonetic transcriptions such as those found in a linguistic atlas.
The first step in the analysis, the computation of linguistic distance between
each pair of sites, can be computed as Levenshtein distance between phonetic
strings. This correlates closely with the much more laborious technique of
determining and counting isoglosses, and is more accurate than the more
familiar metric of computing Hamming distance based on whether vocabulary
entries match. In the actual clustering step, traditional agglomerative
clustering works better than the top-down technique of partitioning around
medoids. When agglomerative clustering of phonetic string comparison distances
is applied to Gaelic, reasonable dialect boundaries are obtained, corresponding
to national and (within Ireland) provincial boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503003</id><created>1995-03-02</created><authors><author><keyname>Chanod</keyname><forenames>Jean-Pierre</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory</affiliation></author><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory</affiliation></author></authors><title>Tagging French -- comparing a statistical and a constraint-based method</title><categories>cmp-lg cs.CL</categories><comments>in Proceedings of EACL-95, uuencoded gzipped postscript</comments><report-no>see also technical report MLTT-016 at
  http://www.xerox.fr/grenoble/mltt/reports/mltt-016.ps</report-no><journal-ref>Seventh Conference of the European Chapter of the ACL (EACL95).
  149-156. ACL, Dublin, 1995.</journal-ref><abstract>In this paper we compare two competing approaches to part-of-speech tagging,
statistical and constraint-based disambiguation, using French as our test
language. We imposed a time limit on our experiment: the amount of time spent
on the design of our constraint system was about the same as the time we used
to train and test the easy-to-implement statistical model. We describe the two
systems and compare the results. The accuracy of the statistical method is
reasonably good, comparable to taggers for English. But the constraint-based
tagger seems to be superior even with the limited time we allowed ourselves for
rule development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503004</id><created>1995-03-02</created><updated>1995-04-05</updated><authors><author><keyname>Chanod</keyname><forenames>Jean-Pierre</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory</affiliation></author><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory</affiliation></author></authors><title>Creating a tagset, lexicon and guesser for a French tagger</title><categories>cmp-lg cs.CL</categories><comments>aclap.sty</comments><journal-ref>ACL SIGDAT workshop: From Texts To Tags: Issues In Multilingual
  Language Analysis. 58-64. University College Dublin, Ireland, 1995.</journal-ref><abstract>  We earlier described two taggers for French, a statistical one and a
constraint-based one. The two taggers have the same tokeniser and morphological
analyser. In this paper, we describe aspects of this work concerned with the
definition of the tagset, the building of the lexicon, derived from an existing
two-level morphological analyser, and the definition of a lexical transducer
for guessing unknown words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503005</id><created>1995-03-03</created><authors><author><keyname>Blackburn</keyname><forenames>Patrick</forenames><affiliation>University of Saarbruecken</affiliation></author><author><keyname>Gardent</keyname><forenames>Claire</forenames><affiliation>University of Saarbruecken</affiliation></author></authors><title>A specification language for Lexical Functional Grammars</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX uses eaclap.sty; Procs of Euro ACL-95</comments><report-no>CLAUS Report Nr. 51</report-no><abstract>This paper defines a language L for specifying LFG grammars. This enables
constraints on LFG's composite ontology (c-structures synchronised with
f-structures) to be stated directly; no appeal to the LFG construction
algorithm is needed. We use L to specify schemata annotated rules and the LFG
uniqueness, completeness and coherence principles. Broader issues raised by
this work are noted and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503006</id><created>1995-03-03</created><authors><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Lingusitic Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Lingusitic Research Group, Freiburg University, Germany</affiliation></author></authors><title>ParseTalk about Sentence- and Text-Level Anaphora</title><categories>cmp-lg cs.CL</categories><comments>in Proceedings of EACL-95, uuencoded and gzipped postscript (see also
  technical Report at
  http://www.coling.uni-freiburg.de:80/forschung/papers/eacl95.ps)</comments><abstract>We provide a unified account of sentence-level and text-level anaphora within
the framework of a dependency-based grammar model. Criteria for anaphora
resolution within sentence boundaries rephrase major concepts from GB's binding
theory, while those for text-level anaphora incorporate an adapted version of a
Grosz-Sidner-style focus model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503007</id><created>1995-03-07</created><authors><author><keyname>Sablayrolles</keyname><forenames>Pierre</forenames><affiliation>IRIT, University Paul Sabatier, Toulouse, FRANCE</affiliation></author></authors><title>The Semantics of Motion</title><categories>cmp-lg cs.CL</categories><comments>3 pages, uses eaclap.sty</comments><journal-ref>Proceedings of the EACL95 Dublin</journal-ref><abstract>In this paper we present a semantic study of motion complexes (ie. of a motion
verb followed by a spatial preposition). We focus on the spatial and the
temporal intrinsic semantic properties of the motion verbs, on the one hand,
and of the spatial prepositions, on the other hand. Then, we address the
problem of combining these basic semantics in order to formally and
automatically derive the spatiotemporal semantics of a motion complex from the
spatiotemporal properties of its components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503008</id><created>1995-03-08</created><authors><author><keyname>Dalrymple</keyname><forenames>Mary</forenames><affiliation>Xerox PARC</affiliation></author><author><keyname>Shieber</keyname><forenames>Stuart M.</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando C. N.</forenames><affiliation>AT&amp;T Bell Labs</affiliation></author></authors><title>Ellipsis and Higher-Order Unification</title><categories>cmp-lg cs.CL</categories><comments>54 pages</comments><report-no>CSLI-19-91 and Xerox SSL-91-105</report-no><journal-ref>Linguistics and Philosophy 14(4):399-452</journal-ref><abstract>We present a new method for characterizing the interpretive possibilities
generated by elliptical constructions in natural language. Unlike previous
analyses, which postulate ambiguity of interpretation or derivation in the full
clause source of the ellipsis, our analysis requires no such hidden ambiguity.
Further, the analysis follows relatively directly from an abstract statement of
the ellipsis interpretation problem. It predicts correctly a wide range of
interactions between ellipsis and other semantic phenomena such as quantifier
scope and bound anaphora. Finally, although the analysis itself is stated
nonprocedurally, it admits of a direct computational method for generating
interpretations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503009</id><created>1995-03-08</created><authors><author><keyname>Schuetze</keyname><forenames>Hinrich</forenames><affiliation>CSLI, Stanford University</affiliation></author></authors><title>Distributional Part-of-Speech Tagging</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>EACL 95</journal-ref><abstract>  This paper presents an algorithm for tagging words whose part-of-speech
properties are unknown. Unlike previous work, the algorithm categorizes word
tokens in context instead of word types. The algorithm is evaluated on the
Brown Corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503010</id><created>1995-03-09</created><authors><author><keyname>Grefenstette</keyname><forenames>Gregory</forenames><affiliation>Rank Xerox Research Centre</affiliation></author><author><keyname>Teufel</keyname><forenames>Simone</forenames><affiliation>Universitat Stuttgart, Institut fur maschinelle Sprachverarbeitung</affiliation></author></authors><title>Corpus-based Method for Automatic Identification of Support Verbs for
  Nominalizations</title><categories>cmp-lg cs.CL</categories><comments>EACL'95</comments><abstract>Nominalization is a highly productive phenomena in most languages. The process
of nominalization ejects a verb from its syntactic role into a nominal
position. The original verb is often replaced by a semantically emptied support
verb (e.g., &quot;make a proposal&quot;). The choice of a support verb for a given
nominalization is unpredictable, causing a problem for language learners as
well as for natural language processing systems. We present here a method of
discovering support verbs from an untagged corpus via low-level syntactic
processing and comparison of arguments attached to verbal forms and potential
nominalized forms. The result of the process is a list of potential support
verbs for the nominalized form of a given predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503011</id><created>1995-03-09</created><authors><author><keyname>McMahon</keyname><forenames>John</forenames><affiliation>Queen's University, Belfast</affiliation></author><author><keyname>Smith</keyname><forenames>F. J.</forenames><affiliation>Queen's University, Belfast</affiliation></author></authors><title>Improving Statistical Language Model Performance with Automatically
  Generated Word Hierarchies</title><categories>cmp-lg cs.CL</categories><comments>17 Page Paper. Self-extracting PostScript File</comments><abstract> An automatic word classification system has been designed which processes word
unigram and bigram frequency statistics extracted from a corpus of natural
language utterances. The system implements a binary top-down form of word
clustering which employs an average class mutual information metric. Resulting
classifications are hierarchical, allowing variable class granularity. Words
are represented as structural tags --- unique $n$-bit numbers the most
significant bit-patterns of which incorporate class information. Access to a
structural tag immediately provides access to all classification levels for the
corresponding word. The classification system has successfully revealed some of
the structure of English, from the phonemic to the semantic level. The system
has been compared --- directly and indirectly --- with other recent word
classification systems. Class based interpolated language models have been
constructed to exploit the extra information supplied by the classifications
and some experiments have shown that the new models improve model performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503012</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503012</id><created>1995-03-09</created><authors><author><keyname>Niyogi</keyname><forenames>Partha</forenames><affiliation>MIT</affiliation></author><author><keyname>Berwick</keyname><forenames>Robert C.</forenames><affiliation>MIT</affiliation></author></authors><title>A Note on Zipf's Law, Natural Languages, and Noncoding DNA regions</title><categories>cmp-lg cs.CL q-bio</categories><comments>compressed uuencoded postscript file: 14 pages</comments><report-no>MIT CBCL Memo No. 118</report-no><abstract>  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the
basis of Zipf rank frequency data that noncoding DNA sequence regions are more
like natural languages than coding regions. We argue on the contrary that an
empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to
natural languages. Although DNA is a presumably an ``organized system of
signs'' in Mandelbrot's (1961) sense, an observation of statistical features of
the sort presented in the Mantegna et al. paper does not shed light on the
similarity between DNA's ``grammar'' and natural language grammars, just as the
observation of exact Zipf-like behavior cannot distinguish between the
underlying processes of tossing an $M$ sided die or a finite-state branching
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503013</id><created>1995-03-13</created><updated>1995-03-14</updated><authors><author><keyname>Milward</keyname><forenames>David</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author><author><keyname>Cooper</keyname><forenames>Robin</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>Incremental Interpretation: Applications, Theory, and Relationship to
  Dynamic Semantics</title><categories>cmp-lg cs.CL</categories><comments>Procs. of COLING 94, LaTeX (2.09 preferred), 8 pages</comments><abstract>Why should computers interpret language incrementally? In recent years
psycholinguistic evidence for incremental interpretation has become more and
more compelling, suggesting that humans perform semantic interpretation before
constituent boundaries, possibly word by word. However, possible computational
applications have received less attention. In this paper we consider various
potential applications, in particular graphical interaction and dialogue. We
then review the theoretical and computational tools available for mapping from
fragments of sentences to fully scoped semantic representations. Finally, we
tease apart the relationship between dynamic semantics and incremental
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503014</id><created>1995-03-14</created><authors><author><keyname>Milward</keyname><forenames>David</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>Non-Constituent Coordination: Theory and Practice</title><categories>cmp-lg cs.CL</categories><comments>Procs. of COLING 94, LaTeX (2.09 preferred), 7 pages</comments><abstract>Despite the large amount of theoretical work done on non-constituent
coordination during the last two decades, many computational systems still
treat coordination using adapted parsing strategies, in a similar fashion to
the SYSCONJ system developed for ATNs. This paper reviews the theoretical
literature, and shows why many of the theoretical accounts actually have worse
coverage than accounts based on processing. Finally, it shows how processing
accounts can be described formally and declaratively in terms of Dynamic
Grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503015</id><created>1995-03-14</created><authors><author><keyname>Milward</keyname><forenames>David</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>Incremental Interpretation of Categorial Grammar</title><categories>cmp-lg cs.CL</categories><comments>Procs. of EACL 95, LaTeX (2.09 preferred), eaclap.sty, 9 pages</comments><abstract>The paper describes a parser for Categorial Grammar which provides fully word
by word incremental interpretation. The parser does not require fragments of
sentences to form constituents, and thereby avoids problems of spurious
ambiguity. The paper includes a brief discussion of the relationship between
basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar
and the Lambek Calculus. It also includes a discussion of some of the issues
which arise when parsing lexicalised grammars, and the possibilities for using
statistical techniques for tuning to particular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503016</id><created>1995-03-14</created><updated>1995-03-16</updated><authors><author><keyname>Androutsopoulos</keyname><forenames>I.</forenames><affiliation>Dept.of Artificial Intelligence, Univ.of Edinburgh</affiliation></author><author><keyname>Ritchie</keyname><forenames>G. D.</forenames><affiliation>Dept.of Artificial Intelligence, Univ.of Edinburgh</affiliation></author><author><keyname>Thanisch</keyname><forenames>P.</forenames><affiliation>Dept.of Computer Science, Univ.of Edinburgh</affiliation></author></authors><title>Natural Language Interfaces to Databases - An Introduction</title><categories>cmp-lg cs.CL</categories><comments>50 pages, uuencoded compressed tar file, containing LaTeX code and
  .eps figures. Uses a4wide.sty. (No changes in the text. Fixed problem with
  epsf macro. Use the epsf.sty included in the tar file, not the epsf.sty of
  the cmp-lg server.)</comments><report-no>DAI RP-709</report-no><journal-ref>Natural Language Engineering 1:1, 29-81</journal-ref><abstract>This paper is an introduction to natural language interfaces to databases
(NLIDBs). A brief overview of the history of NLIDBs is first given. Some
advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to
formal query languages, form-based interfaces, and graphical interfaces. An
introduction to some of the linguistic problems NLIDBs have to confront
follows, for the benefit of readers less familiar with computational
linguistics. The discussion then moves on to NLIDB architectures, portability
issues, restricted natural language input systems (including menu-based
NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of
NLIDB research are then presented, namely database updates, meta-knowledge
questions, temporal questions, and multi-modal NLIDBs. The paper ends with
reflections on the current state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503017</id><created>1995-03-16</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Redundancy in Collaborative Dialogue</title><categories>cmp-lg cs.CL</categories><comments>8 pages, lingmacros, reformatted version of Coling92 paper</comments><journal-ref>Fourteenth International Conference on Computational Linguistics,
  Nantes, 1992</journal-ref><abstract>In dialogues in which both agents are autonomous, each agent deliberates
whether to accept or reject the contributions of the current speaker. A speaker
cannot simply assume that a proposal or an assertion will be accepted. However,
an examination of a corpus of naturally-occurring problem-solving dialogues
shows that agents often do not explicitly indicate acceptance or rejection.
Rather the speaker must infer whether the hearer understands and accepts the
current contribution based on indirect evidence provided by the hearer's next
dialogue contribution. In this paper, I propose a model of the role of
informationally redundant utterances in providing evidence to support
inferences about mutual understanding and acceptance. The model (1) requires a
theory of mutual belief that supports mutual beliefs of various strengths; (2)
explains the function of a class of informationally redundant utterances that
cannot be explained by other accounts; and (3) contributes to a theory of
dialogue by showing how mutual beliefs can be inferred in the absence of the
master-slave assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503018</id><created>1995-03-16</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author></authors><title>Discourse and Deliberation: Testing a Collaborative Strategy</title><categories>cmp-lg cs.CL</categories><comments>8 pages, psfig.sty, lingmacros.sty, reformatted version of Coling94
  paper</comments><journal-ref>Fifteenth International Conference on Computational Linguistics,
  Kyoto</journal-ref><abstract>A discourse strategy is a strategy for communicating with another agent.
Designing effective dialogue systems requires designing agents that can choose
among discourse strategies. We claim that the design of effective strategies
must take cognitive factors into account, propose a new method for testing the
hypothesized factors, and present experimental results on an effective strategy
for supporting deliberation. The proposed method of computational dialogue
simulation provides a new empirical basis for computational linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503019</id><created>1995-03-20</created><authors><author><keyname>Palmer</keyname><forenames>David D.</forenames><affiliation>University of California at Berkeley</affiliation></author></authors><title>SATZ - An Adaptive Sentence Segmentation System</title><categories>cmp-lg cs.CL</categories><comments>30 pages, uuencoded compressed PostScript file (about 170k)</comments><report-no>UC Berkeley Technical Report UCB/CSD-94-846</report-no><abstract>  This paper provides a detailed description of the sentence segmentation
system first introduced in cmp-lg/9411022. It provides results of systematic
experiments involving sentence boundary determination, including context size,
lexicon size, and single-case texts. Also included are the results of
successfully adapting the system to German and French.
The source code for the system is available as a compressed tar file at
ftp://cs-tr.CS.Berkeley.EDU/pub/cstr/satz.tar.Z .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503020</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503020</id><created>1995-03-20</created><authors><author><keyname>Aduriz</keyname><forenames>I.</forenames></author><author><keyname>Alegria</keyname><forenames>I.</forenames></author><author><keyname>Arriola</keyname><forenames>J. M.</forenames></author><author><keyname>Artola</keyname><forenames>X.</forenames></author><author><keyname>A.</keyname><forenames>Diaz de Illarraza</forenames></author><author><keyname>Ezeiza</keyname><forenames>N.</forenames></author><author><keyname>Gojenola</keyname><forenames>K.</forenames></author><author><keyname>Maritxalar</keyname><forenames>M.</forenames></author></authors><title>Different Issues in the Design of a Lemmatizer/Tagger for Basque</title><categories>cmp-lg cs.CL</categories><comments>in Proceedings of SIGDAT-95 (EACL-Dublin), 6 pages, uuencoded and
  gz-compressed postscript</comments><abstract>  This paper presents relevant issues that have been considered in the design
of a general purpose lemmatizer/tagger for Basque (EUSLEM). The
lemmatizer/tagger is conceived as a basic tool necessary for other linguistic
applications. It uses the lexical data base and the morphological analyzer
previously developed and implemented. Due to the characteristics of the
language, the tagset here proposed in structured in for levels, so that each
level is a refinement of the previous one in the sense that it adds more
detailed information. We will focus on the problems found in designing this
tagset and on the strategies for morphological disambiguation that will be
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503021</id><created>1995-03-21</created><authors><author><keyname>Torenvliet</keyname><forenames>Leen</forenames><affiliation>University of Amsterdam</affiliation></author><author><keyname>Trautwein</keyname><forenames>Marten</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>A Note on the Complexity of Restricted Attribute-Value Grammars</title><categories>cmp-lg cs.CL</categories><comments>18 pages, also available by (1) anonymous ftp at
  ftp://ftp.fwi.uva.nl/pub/theory/illc/researchReports/CT-95-02.ps.gz ; (2) WWW
  from http://www.fwi.uva.nl/~mtrautwe/</comments><report-no>CT-95-02</report-no><abstract>  The recognition problem for attribute-value grammars (AVGs) was shown to be
undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no
practical use. In this paper we study a very restricted form of AVG, for which
the recognition problem is decidable (though still NP-complete), the R-AVG. We
show that the R-AVG formalism captures all of the context free languages and
more, and introduce a variation on the so-called `off-line parsability
constraint', the `honest parsability constraint', which lets different types of
R-AVG coincide precisely with well-known time complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503022</id><created>1995-03-21</created><authors><author><keyname>Trautwein</keyname><forenames>Marten</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Assessing Complexity Results in Feature Theories</title><categories>cmp-lg cs.CL</categories><comments>16 pages, includes 3 Postscript figures, uses epsf.sty, also
  available by (1) anonymous ftp at
  ftp://ftp.fwi.uva.nl/pub/theory/illc/researchReports/LP-95-01.ps.gz (2) WWW
  from http://www.fwi.uva.nl/~mtrautwe/ This version differs slightly from the
  original technical Report. Some non-standard style-files are removed from
  this version.</comments><report-no>LP-95-01</report-no><abstract>  In this paper, we assess the complexity results of formalisms that describe
the feature theories used in computational linguistics. We show that from these
complexity results no immediate conclusions can be drawn about the complexity
of the recognition problem of unification grammars using these feature
theories. On the one hand, the complexity of feature theories does not provide
an upper bound for the complexity of such unification grammars.
  On the other hand, the complexity of feature theories need not provide a
lower bound. Therefore, we argue for formalisms that describe actual
unification grammars instead of feature theories. Thus the complexity results
of these formalisms judge upon the hardness of unification grammars in
computational linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503023</id><created>1995-03-22</created><authors><author><keyname>Lyon</keyname><forenames>Caroline</forenames><affiliation>University of Hertfordshire</affiliation></author><author><keyname>Dickerson</keyname><forenames>Bob</forenames><affiliation>University of Hertfordshire</affiliation></author></authors><title>A fast partial parse of natural language sentences using a connectionist
  method</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 3 Postscript figures, uses eaclap.sty it needs LaTeX2e and
  uses begin{filecontents*} to write out the 3 PostScript figures.</comments><abstract>  The pattern matching capabilities of neural networks can be used to locate
syntactic constituents of natural language. This paper describes a fully
automated hybrid system, using neural nets operating within a grammatic
framework. It addresses the representation of language for connectionist
processing, and describes methods of constraining the problem size. The
function of the network is briefly explained, and results are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503024</id><created>1995-03-24</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames></author></authors><title>From compositional to systematic semantics</title><categories>cmp-lg cs.CL</categories><comments>11 pp. Latex.;</comments><journal-ref>Linguistics and Philosophy(17):329-342, 1994</journal-ref><abstract>  We prove a theorem stating that any semantics can be encoded as a
compositional semantics, which means that, essentially, the standard definition
of compositionality is formally vacuous. We then show that when compositional
semantics is required to be &quot;systematic&quot; (that is, the meaning function cannot
be arbitrary, but must belong to some class), it is possible to distinguish
between compositional and non-compositional semantics. As a result, we believe
that the paper clarifies the concept of compositionality and opens a
possibility of making systematic formal comparisons of different systems of
grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9503025</id><created>1995-04-01</created><authors><author><keyname>Niwa</keyname><forenames>Yoshiki</forenames><affiliation>ARL, Hitachi, Ltd.</affiliation></author><author><keyname>Nitta</keyname><forenames>Yoshihiko</forenames><affiliation>ARL, Hitachi, Ltd.</affiliation></author></authors><title>Co-occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries</title><categories>cmp-lg cs.CL</categories><comments>6 pages, appeared in the Proc. of COLING94 (pp. 304-309).</comments><report-no>ARL Research Report No. 94-003</report-no><journal-ref>COLING94, 304-309.</journal-ref><abstract>  A comparison was made of vectors derived by using ordinary co-occurrence
statistics from large text corpora and of vectors derived by measuring the
inter-word distances in dictionary definitions. The precision of word sense
disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal
(20M total words) was higher than that by using distance vectors from the
Collins English Dictionary (60K head words + 1.6M definition words). However,
other experimental results suggest that distance vectors contain some different
semantic information from co-occurrence vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504001</id><created>1995-04-03</created><updated>1995-04-05</updated><authors><author><keyname>Wolinski</keyname><forenames>Francis</forenames><affiliation>Informatique CDC</affiliation></author><author><keyname>Vichot</keyname><forenames>Frantz</forenames><affiliation>Informatique CDC</affiliation></author><author><keyname>Dillet</keyname><forenames>Bruno</forenames><affiliation>Informatique CDC</affiliation></author></authors><title>Automatic processing proper names in texts</title><categories>cmp-lg cs.CL</categories><comments>10 Postscript pages (compress, uuencode)</comments><abstract>  This paper shows first the problems raised by proper names in natural
language processing. Second, it introduces the knowledge representation
structure we use based on conceptual graphs. Then it explains the techniques
which are used to process known and unknown proper names. At last, it gives the
performance of the system and the further works we intend to deal with.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504002</id><created>1995-04-03</created><updated>1995-04-04</updated><authors><author><keyname>Elworthy</keyname><forenames>David</forenames></author></authors><title>Tagset Design and Inflected Languages</title><categories>cmp-lg cs.CL</categories><comments>EACL-94 SIGDAT paper. Includes large tables and figures, which may
  upset some version of LaTeX. Revised version to correct a minor typo!</comments><abstract>  An experiment designed to explore the relationship between tagging accuracy
and the nature of the tagset is described, using corpora in English, French and
Swedish. In particular, the question of internal versus external criteria for
tagset design is considered, with the general conclusion that external
(linguistic) criteria should be followed. Some problems associated with tagging
unknown words in inflected languages are briefly considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504003</id><created>1995-04-04</created><authors><author><keyname>Heeman</keyname><forenames>Peter A.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Hirst</keyname><forenames>Graeme</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Collaborating on Referring Expressions</title><categories>cmp-lg cs.CL</categories><comments>32 pages, 2 figures, to appear in Computation Linguistics 21-3</comments><report-no>TR 435</report-no><abstract>  This paper presents a computational model of how conversational participants
collaborate in order to make a referring action successful. The model is based
on the view of language as goal-directed behavior. We propose that the content
of a referring expression can be accounted for by the planning paradigm. Not
only does this approach allow the processes of building referring expressions
and identifying their referents to be captured by plan construction and plan
inference, it also allows us to account for how participants clarify a
referring expression by using meta-actions that reason about and manipulate the
plan derivation that corresponds to the referring expression. To account for
how clarification goals arise and how inferred clarification plans affect the
agent, we propose that the agents are in a certain state of mind, and that this
state includes an intention to achieve the goal of referring and a plan that
the agents are currently considering. It is this mental state that sanctions
the adoption of goals and the acceptance of inferred plans, and so acts as a
link between understanding and generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504004</id><created>1995-04-04</created><authors><author><keyname>Meurers</keyname><forenames>Walt Detmar</forenames><affiliation>SFB 340, Univ. T&quot;ubingen</affiliation></author><author><keyname>Minnen</keyname><forenames>Guido</forenames><affiliation>SFB 340, Univ. T&quot;ubingen</affiliation></author></authors><title>A Computational Treatment of HPSG Lexical Rules as Covariation in
  Lexical Entries</title><categories>cmp-lg cs.CL</categories><comments>15 pages, uuencoded and compressed postscript to appear in
  Proceedings of the 5th Int. Workshop on Natural Language Understanding and
  Logic Programming. Lisbon, Portugal. 1995</comments><abstract>  We describe a compiler which translates a set of HPSG lexical rules and their
interaction into definite relations used to constrain lexical entries. The
compiler ensures automatic transfer of properties unchanged by a lexical rule.
Thus an operational semantics for the full lexical rule mechanism as used in
HPSG linguistics is provided. Program transformation techniques are used to
advance the resulting encoding. The final output constitutes a computational
counterpart of the linguistic generalizations captured by lexical rules and
allows ``on the fly'' application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504005</id><created>1995-04-05</created><authors><author><keyname>Blache</keyname><forenames>Philippe</forenames><affiliation>2LC-CNRS</affiliation></author><author><keyname>Hathout</keyname><forenames>Nabil</forenames><affiliation>INaLF-CNRS</affiliation></author></authors><title>Constraint Logic Programming for Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>15 pages, uuencoded and compressed postscript to appear in
  Proceedings of the 5th Int. Workshop on Natural Language Understanding and
  Logic Programming. Lisbon, Portugal. 1995</comments><abstract>  This paper proposes an evaluation of the adequacy of the constraint logic
programming paradigm for natural language processing. Theoretical aspects of
this question have been discussed in several works. We adopt here a pragmatic
point of view and our argumentation relies on concrete solutions. Using actual
contraints (in the CLP sense) is neither easy nor direct. However, CLP can
improve parsing techniques in several aspects such as concision, control,
efficiency or direct representation of linguistic formalism. This discussion is
illustrated by several examples and the presentation of an HPSG parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504006</id><created>1995-04-05</created><authors><author><keyname>Whittaker</keyname><forenames>Steve</forenames><affiliation>Hewlett Packard Laboratories</affiliation></author><author><keyname>Stenton</keyname><forenames>Phil</forenames><affiliation>Hewlett Packard Laboratories</affiliation></author></authors><title>Cues and control in Expert-Client Dialogues</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex</comments><journal-ref>Proceedings of the 26th Annual Meeting of the Association of
  Computational Linguistics, 1988</journal-ref><abstract>  We conducted an empirical analysis into the relation between control and
discourse structure. We applied control criteria to four dialogues and
identified 3 levels of discourse structure. We investigated the mechanism for
changing control between these structures and found that utterance type and not
cue words predicted shifts of control. Participants used certain types of
signals when discourse goals were proceeding successfully but resorted to
interruptions when they were not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504007</id><created>1995-04-05</created><authors><author><keyname>Walker</keyname><forenames>Marilyn</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Whittaker</keyname><forenames>Steve</forenames><affiliation>Hewlett Packard Laboratories</affiliation></author></authors><title>Mixed Initiative in Dialogue: An Investigation into Discourse
  Segmentation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex</comments><journal-ref>Proceedings of the 28th Annual Meeting of the Association of
  Computational Linguistics, 1990</journal-ref><abstract>  Conversation between two people is usually of mixed-initiative, with control
over the conversation being transferred from one person to another. We apply a
set of rules for the transfer of control to 4 sets of dialogues consisting of a
total of 1862 turns. The application of the control rules lets us derive
domain-independent discourse structures. The derived structures indicate that
initiative plays a role in the structuring of discourse. In order to explore
the relationship of control and initiative to discourse processes like
centering, we analyze the distribution of four different classes of anaphora
for two data sets. This distribution indicates that some control segments are
hierarchically related to others. The analysis suggests that discourse
participants often mutually agree to a change of topic. We also compared
initiative in Task Oriented and Advice Giving dialogues and found that both
allocation of control and the manner in which control is transferred is
radically different for the two dialogue types. These differences can be
explained in terms of collaborative planning principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504008</id><created>1995-04-07</created><updated>1995-04-24</updated><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory Pohang University of Science &amp; Technology, Hoja-Dong, Pohang, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory Pohang University of Science &amp; Technology, Hoja-Dong, Pohang, Korea</affiliation></author></authors><title>SKOPE: A connectionist/symbolic architecture of spoken Korean processing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex, use aaai.sty &amp; aaai.bst, bibfile: nlpsp.bib, to be
  presented at IJCAI95 workshops on new approaches to learning for natural
  language processing</comments><abstract>  Spoken language processing requires speech and natural language integration.
Moreover, spoken Korean calls for unique processing methodology due to its
linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic
spoken Korean processing engine, which emphasizes that: 1) connectionist and
symbolic techniques must be selectively applied according to their relative
strength and weakness, and 2) the linguistic characteristics of Korean must be
fully considered for phoneme recognition, speech and language integration, and
morphological/syntactic processing. The design and implementation of SKOPE
demonstrates how connectionist/symbolic hybrid architectures can be constructed
for spoken agglutinative language processing. Also SKOPE presents many novel
ideas for speech and language processing. The phoneme recognition,
morphological analysis, and syntactic analysis experiments show that SKOPE is a
viable approach for the spoken Korean processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504009</id><created>1995-04-13</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Computer Science, Technion, Israel Institute of Techniology, Haifa 32000, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Computer Science, Technion, Israel Institute of Techniology, Haifa 32000, Israel</affiliation></author></authors><title>Abstract Machine for Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>Self-contained LaTeX, 15 pages, to appear in NLULP95</comments><journal-ref>Proc. 5th Int. Workshop on Natural Language Understanding and
  Logic Programming, Lisbon, May 1995</journal-ref><abstract>  This paper describes an abstract machine for linguistic formalisms that are
based on typed feature structures, such as HPSG. The core design of the
abstract machine is given in detail, including the compilation process from a
high-level language to the abstract machine language and the implementation of
the abstract instructions. The machine's engine supports the unification of
typed, possibly cyclic, feature structures. A separate module deals with
control structures and instructions to accommodate parsing for phrase structure
grammars. We treat the linguistic formalism as a high-level declarative
programming language, applying methods that were proved useful in computer
science to the study of natural languages: a grammar specified using the
formalism is endowed with an operational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504010</id><created>1995-04-13</created><authors><author><keyname>Collet</keyname><forenames>P.</forenames><affiliation>CNRS</affiliation></author><author><keyname>Galves</keyname><forenames>A.</forenames><affiliation>USP</affiliation></author><author><keyname>Lopes</keyname><forenames>A.</forenames><affiliation>UFRGS</affiliation></author></authors><title>MAXIMUM LIKELIHOOD AND MINIMUM ENTROPY IDENTIFICATION OF GRAMMARS</title><categories>cmp-lg cs.CL</categories><comments>12 pages, ps, gzip, uuencoded</comments><abstract>  Using the Thermodynamic Formalism, we introduce a Gibbsian model for the
identification of regular grammars based only on positive evidence. This model
mimics the natural language acquisition procedure driven by prosody which is
here represented by the thermodynamical potential. The statistical question we
face is how to estimate the incidenc e matrix of a subshift of finite type from
a sample produced by a Gibbs state whose potential is known. The model
acquaints for both the robustness of t he language acquisition procedure and
language changes. The probabilistic appr oach we use avoids invoking ad-hoc
restrictions as Berwick's Subset Principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504011</id><created>1995-04-15</created><authors><author><keyname>Rambow</keyname><forenames>Owen</forenames></author><author><keyname>Joshi</keyname><forenames>Aravind K.</forenames></author></authors><title>A Processing Model for Free Word Order Languages</title><categories>cmp-lg cs.CL</categories><comments>23 pages, uuencoded compressed ps file. In {\em Perspectives on
  Sentence Processing}, C. Clifton, Jr., L. Frazier and K. Rayner, editors.
  Lawrence Erlbaum Associates, 1994</comments><abstract>  Like many verb-final languages, Germn displays considerable word-order
freedom: there is no syntactic constraint on the ordering of the nominal
arguments of a verb, as long as the verb remains in final position. This effect
is referred to as ``scrambling'', and is interpreted in transformational
frameworks as leftward movement of the arguments. Furthermore, arguments from
an embedded clause may move out of their clause; this effect is referred to as
``long-distance scrambling''. While scrambling has recently received
considerable attention in the syntactic literature, the status of long-distance
scrambling has only rarely been addressed. The reason for this is the
problematic status of the data: not only is long-distance scrambling highly
dependent on pragmatic context, it also is strongly subject to degradation due
to processing constraints. As in the case of center-embedding, it is not
immediately clear whether to assume that observed unacceptability of highly
complex sentences is due to grammatical restrictions, or whether we should
assume that the competence grammar does not place any restrictions on
scrambling (and that, therefore, all such sentences are in fact grammatical),
and the unacceptability of some (or most) of the grammatically possible word
orders is due to processing limitations. In this paper, we will argue for the
second view by presenting a processing model for German.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504012</id><created>1995-04-17</created><authors><author><keyname>Dalrymple</keyname><forenames>Mary</forenames></author><author><keyname>Lamping</keyname><forenames>John</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames></author></authors><title>Linear Logic for Meaning Assembly</title><categories>cmp-lg cs.CL</categories><comments>19 pages, uses lingmacros.sty, fullname.sty, tree-dvips.sty,
  latexsym.sty, requires the new version of Latex</comments><journal-ref>Proceedings of CLNLP, Edinburgh, April 1995</journal-ref><abstract>  Semantic theories of natural language associate meanings with utterances by
providing meanings for lexical items and rules for determining the meaning of
larger units given the meanings of their parts. Meanings are often assumed to
combine via function application, which works well when constituent structure
trees are used to guide semantic composition. However, we believe that the
functional structure of Lexical-Functional Grammar is best used to provide the
syntactic information necessary for constraining derivations of meaning in a
cross-linguistically uniform format. It has been difficult, however, to
reconcile this approach with the combination of meanings by function
application. In contrast to compositional approaches, we present a deductive
approach to assembling meanings, based on reasoning with constraints, which
meshes well with the unordered nature of information in the functional
structure. Our use of linear logic as a `glue' for assembling meanings allows
for a coherent treatment of the LFG requirements of completeness and coherence
as well as of modification and quantification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504013</id><created>1995-04-23</created><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>CoGenTex, Ithaca, USA</affiliation></author></authors><title>NLG vs. Templates</title><categories>cmp-lg cs.CL</categories><comments>Uuencoded compressed tar file, containing LaTeX source and a style
  file. This paper will appear in the 1995 European NL Generation Workshop</comments><abstract>  One of the most important questions in applied NLG is what benefits (or
`value-added', in business-speak) NLG technology offers over template-based
approaches. Despite the importance of this question to the applied NLG
community, however, it has not been discussed much in the research NLG
community, which I think is a pity. In this paper, I try to summarize the
issues involved and recap current thinking on this topic. My goal is not to
answer this question (I don't think we know enough to be able to do so), but
rather to increase the visibility of this issue in the research community, in
the hope of getting some input and ideas on this very important question. I
conclude with a list of specific research areas I would like to see more work
in, because I think they would increase the `value-added' of NLG over
templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504014</id><created>1995-04-24</created><authors><author><keyname>Koenig</keyname><forenames>Esther</forenames><affiliation>University of Stuttgart</affiliation></author></authors><title>LexGram - a practical categorial grammar formalism -</title><categories>cmp-lg cs.CL</categories><comments>16 pages</comments><journal-ref>Proc.CLNLP95, Edinburgh</journal-ref><abstract>  We present the LexGram system, an amalgam of (Lambek) categorial grammar and
Head Driven Phrase Structure Grammar (HPSG), and show that the grammar
formalism it implements is a well-structured and useful tool for actual grammar
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504015</id><created>1995-04-24</created><authors><author><keyname>Baayen</keyname><forenames>Harald</forenames><affiliation>Max Planck Institute for Psycholinguistics</affiliation></author><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author></authors><title>Estimating Lexical Priors for Low-Frequency Syncretic Forms</title><categories>cmp-lg cs.CL</categories><comments>Submitted to Computational Linguistics</comments><abstract>  Given a previously unseen form that is morphologically n-ways ambiguous, what
is the best estimator for the lexical prior probabilities for the various
functions of the form? We argue that the best estimator is provided by
computing the relative frequencies of the various functions among the hapax
legomena --- the forms that occur exactly once in a corpus. This result has
important implications for the development of stochastic morphological taggers,
especially when some initial hand-tagging of a corpus is required: For
predicting lexical priors for very low-frequency morphologically ambiguous
types (most of which would not occur in any given corpus) one should
concentrate on tagging a good representative sample of the hapax legomena,
rather than extensively tagging words of all frequency ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504016</id><created>1995-04-25</created><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Memoization of Top Down Parsing</title><categories>cmp-lg cs.CL</categories><comments>uuencoded, compressed postscript file</comments><abstract>  This paper discusses the relationship between memoized top-down recognizers
and chart parsers. It presents a version of memoization suitable for
continuation-passing style programs. When applied to a simple formalization of
a top-down recognizer it yields a terminating parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504017</id><created>1995-04-25</created><authors><author><keyname>Marcu</keyname><forenames>Daniel</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Hirst</keyname><forenames>Graeme</forenames><affiliation>University of Toronto</affiliation></author></authors><title>A Uniform Treatment of Pragmatic Inferences in Simple and Complex
  Utterances and Sequences of Utterances</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX Source. To appear in the Proceedings of ACL-95.
  Requires aclap.sty file</comments><abstract>  Drawing appropriate defeasible inferences has been proven to be one of the
most pervasive puzzles of natural language processing and a recurrent problem
in pragmatics. This paper provides a theoretical framework, called ``stratified
logic'', that can accommodate defeasible pragmatic inferences. The framework
yields an algorithm that computes the conversational, conventional, scalar,
clausal, and normal state implicatures; and the presuppositions that are
associated with utterances. The algorithm applies equally to simple and complex
utterances and sequences of utterances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504018</id><created>1995-04-25</created><authors><author><keyname>Marcu</keyname><forenames>Daniel</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Hirst</keyname><forenames>Graeme</forenames><affiliation>University of Toronto</affiliation></author></authors><title>An Implemented Formalism for Computing Linguistic Presuppositions and
  Existential Commitments</title><categories>cmp-lg cs.CL</categories><comments>10 pages, LaTeX Source. Requires iwcs.sty file. Ignore LaTeX warning
  messages (Proceedings of the International Workshop on Computational
  Semantics, Tilburg, The Netherlands, pages 141--150, December 1994.)</comments><abstract>  We rely on the strength of linguistic and philosophical perspectives in
constructing a framework that offers a unified explanation for presuppositions
and existential commitment. We use a rich ontology and a set of methodological
principles that embed the essence of Meinong's philosophy and Grice's
conversational principles into a stratified logic, under an unrestricted
interpretation of the quantifiers. The result is a logical formalism that
yields a tractable computational method that uniformly calculates all the
presuppositions of a given utterance, including the existential ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504019</id><created>1995-04-25</created><updated>1995-04-26</updated><authors><author><keyname>Marcu</keyname><forenames>Daniel</forenames><affiliation>University of Toronto</affiliation></author></authors><title>A Formalism and an Algorithm for Computing Pragmatic Inferences and
  Detecting Infelicities</title><categories>cmp-lg cs.CL</categories><comments>132 pages, LaTeX Source. Master Thesis, October 1994. Requires
  epsf.sty, ut-thesis.sty, named.sty, headerfooter.sty, my-macros.sty files</comments><report-no>Technical Report CSRI-309, Computer Systems Research Institute,
  University of Toronto, October 1994</report-no><abstract>  Since Austin introduced the term ``infelicity'', the linguistic literature
has been flooded with its use, but no formal or computational explanation has
been given for it. This thesis provides one for those infelicities that occur
when a pragmatic inference is cancelled.
  Our contribution assumes the existence of a finer grained taxonomy with
respect to pragmatic inferences. It is shown that if one wants to account for
the natural language expressiveness, one should distinguish between pragmatic
inferences that are felicitous to defeat and pragmatic inferences that are
infelicitously defeasible. Thus, it is shown that one should consider at least
three types of information: indefeasible, felicitously defeasible, and
infelicitously defeasible. The cancellation of the last of these determines the
pragmatic infelicities.
  A new formalism has been devised to accommodate the three levels of
information, called ``stratified logic''. Within it, we are able to express
formally notions such as ``utterance U presupposes P'' or ``utterance U is
infelicitous''. Special attention is paid to the implications that our work has
in solving some well-known existential philosophical puzzles. The formalism
yields an algorithm for computing interpretations for utterances, for
determining their associated presuppositions, and for signalling infelicitous
utterances that has been implemented in Common Lisp. The algorithm applies
equally to simple and complex utterances and sequences of utterances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504020</id><created>1995-04-26</created><authors><author><keyname>Dale</keyname><forenames>Robert</forenames><affiliation>Microsoft, Sydney</affiliation></author><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>CoGenTeX, Ithaca</affiliation></author></authors><title>Computational Interpretations of the Gricean Maxims in the Generation of
  Referring Expressions</title><categories>cmp-lg cs.CL</categories><comments>29 pages, compressed PS file</comments><abstract>  We examine the problem of generating definite noun phrases that are
appropriate referring expressions; i.e, noun phrases that (1) successfully
identify the intended referent to the hearer whilst (2) not conveying to her
any false conversational implicatures (Grice, 1975). We review several possible
computational interpretations of the conversational implicature maxims, with
different computational costs, and argue that the simplest may be the best,
because it seems to be closest to what human speakers do. We describe our
recommended algorithm in detail, along with a specification of the resources a
host system must provide in order to make use of the algorithm, and an
implementation used in the natural language generation component of the IDAS
system.
 This paper will appear in the the April--June 1995 issue of Cognitive Science,
and is made available on cmp-lg with the permission of Ablex, the publishers of
that journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504021</id><created>1995-04-26</created><authors><author><keyname>Ellison</keyname><forenames>T. Mark</forenames><affiliation>now INESC, Lisbon</affiliation></author></authors><title>Phonological Derivation in Optimality Theory</title><categories>cmp-lg cs.CL</categories><comments>7 pages</comments><journal-ref>Coling 94:1007-1013 (Vol II)</journal-ref><abstract>  Optimality Theory is a constraint-based theory of phonology which allows
constraints to be violated. Consequently, implementing the theory presents
problems for declarative constraint-based processing frameworks. On the basis
of two regularity assumptions, that candidate sets are regular and that
constraints can be modelled by transducers, this paper presents and proves
correct algorithms for computing the action of constraints, and hence deriving
surface forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504022</id><created>1995-04-26</created><authors><author><keyname>Ellison</keyname><forenames>T. Mark</forenames><affiliation>currently INESC, Lisbon</affiliation></author></authors><title>Constraints, Exceptions and Representations</title><categories>cmp-lg cs.CL</categories><comments>9 pages postscript</comments><journal-ref>(Proc of ACL SIGPHON First Meeting)</journal-ref><abstract>  This paper shows that default-based phonologies have the potential to capture
morphophonological generalisations which cannot be captured by non-defaul
theories. In achieving this result, I offer a characterisation of
Underspecification Theory and Optimality Theory in terms of their methods for
ordering defaults. The result means that machine learning techniques for
building non-defualt analyses may not provide a suitable basis for
morphophonological analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504023</id><created>1995-04-26</created><updated>1995-05-28</updated><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory, Pohang University of Science &amp; Technology</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory, Pohang University of Science &amp; Technology</affiliation></author><author><keyname>Shin</keyname><forenames>Sanghyun</forenames><affiliation>Department of Computer Science &amp; Engineering and Postech Information Research Laboratory, Pohang University of Science &amp; Technology</affiliation></author></authors><title>TAKTAG: Two-phase learning method for hybrid statistical/rule-based
  part-of-speech disambiguation</title><categories>cmp-lg cs.CL</categories><comments>10pages, latex, named.sty &amp; named.bst, use psfig figures, submitted</comments><abstract>  Both statistical and rule-based approaches to part-of-speech (POS)
disambiguation have their own advantages and limitations. Especially for
Korean, the narrow windows provided by hidden markov model (HMM) cannot cover
the necessary lexical and long-distance dependencies for POS disambiguation. On
the other hand, the rule-based approaches are not accurate and flexible to new
tag-sets and languages. In this regard, the statistical/rule-based hybrid
method that can take advantages of both approaches is called for the robust and
flexible POS disambiguation. We present one of such method, that is, a
two-phase learning architecture for the hybrid statistical/rule-based POS
disambiguation, especially for Korean. In this method, the statistical learning
of morphological tagging is error-corrected by the rule-based learning of Brill
[1992] style tagger. We also design the hierarchical and flexible Korean
tag-set to cope with the multiple tagging applications, each of which requires
different tag-set. Our experiments show that the two-phase learning method can
overcome the undesirable features of solely HMM-based or solely rule-based
tagging, especially for morphologically complex Korean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504024</id><created>1995-04-27</created><updated>1995-04-28</updated><authors><author><keyname>Bowden</keyname><forenames>Tanya</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Kiraz</keyname><forenames>George Anton</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>A Morphographemic Model for Error Correction in Nonconcatenative Strings</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX</comments><abstract>  This paper introduces a spelling correction system which integrates
seamlessly with morphological analysis using a multi-tape formalism. Handling
of various Semitic error problems is illustrated, with reference to Arabic and
Syriac examples. The model handles errors vocalisation, diacritics, phonetic
syncopation and morphographemic idiosyncrasies, in addition to Damerau errors.
A complementary correction strategy for morphologically sound but
morphosyntactically ill-formed words is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504025</id><created>1995-04-27</created><authors><author><keyname>Rose'</keyname><forenames>Carolyn Penstein</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Levin</keyname><forenames>Lori S.</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Van Ess-Dykema</keyname><forenames>Carol</forenames><affiliation>Department Of Defense</affiliation></author></authors><title>Discourse Processing of Dialogues with Multiple Threads</title><categories>cmp-lg cs.CL</categories><comments>8 pages, compressed, uuencoded postscript. If you have trouble
  printing the postscript, please send mail to cprose@lcl.cmu.edu</comments><journal-ref>Proceedings of the 33rd Annual Meeting of the Association for
  Computational Linguistics, MIT, 1995</journal-ref><abstract>  In this paper we will present our ongoing work on a plan-based discourse
processor developed in the context of the Enthusiast Spanish to English
translation system as part of the JANUS multi-lingual speech-to-speech
translation system. We will demonstrate that theories of discourse which
postulate a strict tree structure of discourse on either the intentional or
attentional level are not totally adequate for handling spontaneous dialogues.
We will present our extension to this approach along with its implementation in
our plan-based discourse processor. We will demonstrate that the implementation
of our approach outperforms an implementation based on the strict tree
structure approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504026</id><created>1995-04-28</created><authors><author><keyname>van Noord</keyname><forenames>Gertjan</forenames><affiliation>Alfa-informatica and BCN, Groningen</affiliation></author></authors><title>The intersection of Finite State Automata and Definite Clause Grammars</title><categories>cmp-lg cs.CL</categories><comments>7 pages. Requires pictexwd package. To appear in ACL 95</comments><journal-ref>Proceedings of the 33rd ACL. Boston 1995.</journal-ref><abstract>  Bernard Lang defines parsing as the calculation of the intersection of a FSA
(the input) and a CFG. Viewing the input for parsing as a FSA rather than as a
string combines well with some approaches in speech understanding systems, in
which parsing takes a word lattice as input (rather than a word string).
Furthermore, certain techniques for robust parsing can be modelled as finite
state transducers. In this paper we investigate how we can generalize this
approach for unification grammars. In particular we will concentrate on how we
might the calculation of the intersection of a FSA and a DCG. It is shown that
existing parsing algorithms can be easily extended for FSA inputs. However, we
also show that the termination properties change drastically: we show that it
is undecidable whether the intersection of a FSA and a DCG is empty (even if
the DCG is off-line parsable). Furthermore we discuss approaches to cope with
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504027</id><created>1995-04-28</created><updated>1995-05-01</updated><authors><author><keyname>Poznanski</keyname><forenames>Victor</forenames></author><author><keyname>Beaven</keyname><forenames>John L.</forenames></author><author><keyname>Whitelock</keyname><forenames>Pete</forenames></author></authors><title>An Efficient Generation Algorithm for Lexicalist MT</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of ACL-95</comments><abstract>  The lexicalist approach to Machine Translation offers significant advantages
in the development of linguistic descriptions. However, the Shake-and-Bake
generation algorithm of (Whitelock, COLING-92) is NP-complete. We present a
polynomial time algorithm for lexicalist MT generation provided that sufficient
information can be transferred to ensure more determinism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504028</id><created>1995-04-28</created><updated>1995-05-15</updated><authors><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author><author><keyname>D&#xf6;rre</keyname><forenames>Jochen</forenames><affiliation>Universit&#xe4;t Stuttgart</affiliation></author></authors><title>Memoization of Coroutined Constraints</title><categories>cmp-lg cs.CL</categories><comments>To appear in The Proceedings of ACL '95, uses aclap.sty</comments><abstract>  Some linguistic constraints cannot be effectively resolved during parsing at
the location in which they are most naturally introduced. This paper shows how
constraints can be propagated in a memoizing parser (such as a chart parser) in
much the same way that variable bindings are, providing a general treatment of
constraint coroutining in memoization. Prolog code for a simple application of
our technique to Bouma and van Noord's (1994) categorial grammar analysis of
Dutch is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504029</id><created>1995-04-28</created><updated>1995-04-28</updated><authors><author><keyname>Dalrymple</keyname><forenames>Mary</forenames><affiliation>Xerox PARC</affiliation></author><author><keyname>Lamping</keyname><forenames>John</forenames><affiliation>Xerox PARC</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>AT&amp;T Bell Laboratories</affiliation></author><author><keyname>Saraswat</keyname><forenames>Vijay</forenames><affiliation>Xerox PARC</affiliation></author></authors><title>Quantifiers, Anaphora, and Intensionality</title><categories>cmp-lg cs.CL</categories><comments>41 pages, uses lingmacros.sty, fullname.sty, lfgmacros.tex,
  tree-dvips.sty, tree-dvips.pro and attached macros. Extends and revises
  cmp-lg/9404009 and cmp-lg/9404010</comments><abstract>  The relationship between Lexical-Functional Grammar (LFG) {\em functional
structures} (f-structures) for sentences and their semantic interpretations can
be expressed directly in a fragment of linear logic in a way that correctly
explains the constrained interactions between quantifier scope ambiguity, bound
anaphora and intensionality. This deductive approach to semantic interpretaion
obviates the need for additional mechanisms, such as Cooper storage, to
represent the possible scopes of a quantified NP, and explains the interactions
between quantified NPs, anaphora and intensional verbs such as `seek'. A single
specification in linear logic of the argument requirements of intensional verbs
is sufficient to derive the correct reading predictions for intensional-verb
clauses both with nonquantified and with quantified direct objects. In
particular, both de dicto and de re readings are derived for quantified
objects. The effects of type-raising or quantifying-in rules in other
frameworks here just follow as linear-logic theorems.
 While our approach resembles current categorial approaches in important ways,
it differs from them in allowing the greater type flexibility of categorial
semantics while maintaining a precise connection to syntax. As a result, we are
able to provide derivations for certain readings of sentences with intensional
verbs and complex direct objects that are not derivable in current purely
categorial accounts of the syntax-semantics interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504030</id><created>1995-04-28</created><authors><author><keyname>Magerman</keyname><forenames>David M.</forenames></author></authors><title>Statistical Decision-Tree Models for Parsing</title><categories>cmp-lg cs.CL</categories><comments>uses aclap.sty, psfig.tex (v1.9), postscript figures</comments><journal-ref>Proceedings of the 33rd Annual Meeting of the ACL</journal-ref><abstract>  Syntactic natural language parsers have shown themselves to be inadequate for
processing highly-ambiguous large-vocabulary text, as is evidenced by their
poor performance on domains like the Wall Street Journal, and by the movement
away from parsing-based approaches to text-processing in general. In this
paper, I describe SPATTER, a statistical parser based on decision-tree learning
techniques which constructs a complete parse for every sentence and achieves
accuracy rates far better than any published result. This work is based on the
following premises: (1) grammars are too complex and detailed to develop
manually for most interesting domains; (2) parsing models must rely heavily on
lexical and contextual information to analyze sentences accurately; and (3)
existing {$n$}-gram modeling techniques are inadequate for parsing models. In
experiments comparing SPATTER with IBM's computer manuals parser, SPATTER
significantly outperforms the grammar-based parser. Evaluating SPATTER against
the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures,
SPATTER achieves 86\% precision, 86\% recall, and 1.3 crossing brackets per
sentence for sentences of 40 words or less, and 91\% precision, 90\% recall,
and 0.5 crossing brackets for sentences between 10 and 20 words in length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504031</id><created>1995-04-28</created><updated>1995-07-21</updated><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Department of Computer Engineering and Information Science Bilkent University, Ankara Turkey</affiliation></author></authors><title>Error-tolerant Finite State Recognition with Applications to
  Morphological Analysis and Spelling Correction</title><categories>cmp-lg cs.CL</categories><comments>Replaces 9504031. gzipped, uuencoded postscript file. To appear in
  Computational Linguistics Volume 22 No:1, 1996, Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/clpaper9512.ps.z</comments><abstract>  Error-tolerant recognition enables the recognition of strings that deviate
mildly from any string in the regular set recognized by the underlying finite
state recognizer. Such recognition has applications in error-tolerant
morphological processing, spelling correction, and approximate string matching
in information retrieval. After a description of the concepts and algorithms
involved, we give examples from two applications: In the context of
morphological analysis, error-tolerant recognition allows misspelled input word
forms to be corrected, and morphologically analyzed concurrently. We present an
application of this to error-tolerant analysis of agglutinative morphology of
Turkish words. The algorithm can be applied to morphological analysis of any
language whose morphology is fully captured by a single (and possibly very
large) finite state transducer, regardless of the word formation processes and
morphographemic phenomena involved. In the context of spelling correction,
error-tolerant recognition can be used to enumerate correct candidate forms
from a given misspelled string within a certain edit distance. Again, it can be
applied to any language with a word list comprising all inflected forms, or
whose morphology is fully described by a finite state transducer. We present
experimental results for spelling correction for a number of languages. These
results indicate that such recognition works very efficiently for candidate
generation in spelling correction for many European languages such as English,
Dutch, French, German, Italian (and others) with very large word lists of root
and inflected forms (some containing well over 200,000 forms), generating all
candidate solutions within 10 to 45 milliseconds (with edit distance 1) on a
SparcStation 10/41. For spelling correction in Turkish, error-tolerant
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504032</id><created>1995-04-29</created><updated>1995-05-08</updated><authors><author><keyname>Karttunen</keyname><forenames>Lauri</forenames></author><author><keyname>Centre</keyname><forenames>Rank Xerox Research</forenames></author></authors><title>The Replace Operator</title><categories>cmp-lg cs.CL</categories><comments>To appear in the Proceedings of ACL-95. 8 pages, PostScript</comments><abstract>  This paper introduces to the calculus of regular expressions a replace
operator, -&gt;, and defines a set of replacement expressions that concisely
encode several alternate variations of the operation.
  The basic case is unconditional obligatory replacement:
  UPPER -&gt; LOWER
  Conditional versions of replacement, such as,
  UPPER -&gt; LOWER || LEFT _ RIGHT constrain the operation by left and right
contexts. UPPER, LOWER, LEFT, and RIGHT may be regular expressions of any
complexity.
  Replace expressions denote regular relations. The replace operator is defined
in terms of other regular expression operators using techniques introduced by
Ronald M. Kaplan and Martin Kay in &quot;Regular Models of Phonological Rule
Systems&quot; (Computational Linguistics 20:3 331-378. 1994).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504033</id><created>1995-04-28</created><updated>1996-09-10</updated><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author></authors><title>Corpus Statistics Meet the Noun Compound: Some Empirical Results</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 5 figures, uses modified version of aclap.sty, replaced
  because old version failed to TeX properly</comments><journal-ref>Proceedings of the 33rd Annual Meeting of the Association for
  Computational Linguistics, Boston, MA., 1995 pp47-54</journal-ref><abstract>  A variety of statistical methods for noun compound analysis are implemented
and compared. The results support two main conclusions. First, the use of
conceptual association not only enables a broad coverage, but also improves the
accuracy. Second, an analysis model based on dependency grammar is
substantially more accurate than one based on deepest constituents, even though
the latter is more prevalent in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9504034</id><created>1995-04-30</created><authors><author><keyname>Chen</keyname><forenames>Stanley F.</forenames><affiliation>Harvard University</affiliation></author></authors><title>Bayesian Grammar Induction for Language Modeling</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, uses aclap.sty</comments><journal-ref>Proc. 33rd Annual Meeting of the ACL, p. 228-235, Cambridge, MA
  1995</journal-ref><abstract>  We describe a corpus-based induction algorithm for probabilistic context-free
grammars. The algorithm employs a greedy heuristic search within a Bayesian
framework, and a post-pass using the Inside-Outside algorithm. We compare the
performance of our algorithm to n-gram models and the Inside-Outside algorithm
in three language modeling tasks. In two of the tasks, the training data is
generated by a probabilistic context-free grammar and in both tasks our
algorithm outperforms the other techniques. The third task involves
naturally-occurring data, and in this task our algorithm does not perform as
well as n-gram models but vastly outperforms the Inside-Outside algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505001</id><created>1995-05-01</created><authors><author><keyname>Chu-Carroll</keyname><forenames>Jennifer</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>Carberry</keyname><forenames>Sandra</forenames><affiliation>University of Delaware</affiliation></author></authors><title>Response Generation in Collaborative Negotiation</title><categories>cmp-lg cs.CL</categories><comments>9 pages, 1 Postscript figure, requires aclap.sty and epsf.sty. To
  appear in ACL-95</comments><abstract>  In collaborative planning activities, since the agents are autonomous and
heterogeneous, it is inevitable that conflicts arise in their beliefs during
the planning process. In cases where such conflicts are relevant to the task at
hand, the agents should engage in collaborative negotiation as an attempt to
square away the discrepancies in their beliefs. This paper presents a
computational strategy for detecting conflicts regarding proposed beliefs and
for engaging in collaborative negotiation to resolve the conflicts that warrant
resolution. Our model is capable of selecting the most effective aspect to
address in its pursuit of conflict resolution in cases where multiple conflicts
arise, and of selecting appropriate evidence to justify the need for such
modification. Furthermore, by capturing the negotiation process in a recursive
Propose-Evaluate-Modify cycle of actions, our model can successfully handle
embedded negotiation subdialogues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505002</id><created>1995-05-01</created><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames><affiliation>Princeton University</affiliation></author><author><keyname>Thomas</keyname><forenames>Robert G.</forenames><affiliation>Princeton University</affiliation></author></authors><title>New Techniques for Context Modeling</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in Proc. ACL 1995</comments><abstract>  We introduce three new techniques for statistical language models: extension
modeling, nonmonotonic contexts, and the divergence heuristic. Together these
techniques result in language models that have few states, even fewer
parameters, and low message entropies. For example, our techniques achieve a
message entropy of 1.97 bits/char on the Brown corpus using only 89,325
parameters. In contrast, the character 4-gram model requires more than 250
times as many parameters in order to achieve a message entropy of only 2.47
bits/char. The fact that our model performs significantly better while using
vastly fewer parameters indicates that it is a better probability model of
natural language text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505003</id><created>1995-05-02</created><authors><author><keyname>Goetz</keyname><forenames>Thilo</forenames><affiliation>SFB 340, Univ. Tuebingen</affiliation></author><author><keyname>Meurers</keyname><forenames>Walt Detmar</forenames><affiliation>SFB 340, Univ. Tuebingen</affiliation></author></authors><title>Compiling HPSG type constraints into definite clause programs</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded compressed postscript</comments><journal-ref>Proceedings of ACL-95</journal-ref><abstract>  We present a new approach to HPSG processing: compiling HPSG grammars
expressed as type constraints into definite clause programs. This provides a
clear and computationally useful correspondence between linguistic theories and
their implementation. The compiler performs off-line constraint inheritance and
code optimization. As a result, we are able to efficiently process with HPSG
grammars without having to hand-translate them into definite clause or phrase
structure based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505004</id><created>1995-05-02</created><authors><author><keyname>Keller</keyname><forenames>Bill</forenames><affiliation>University of Sussex</affiliation></author></authors><title>DATR Theories and DATR Models</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LATEX, uses aclap.sty, Procs. ACL95</comments><abstract>  Evans and Gazdar introduced DATR as a simple, non-monotonic language for
representing natural language lexicons. Although a number of implementations of
DATR exist, the full language has until now lacked an explicit, declarative
semantics. This paper rectifies the situation by providing a mathematical
semantics for DATR. We present a view of DATR as a language for defining
certain kinds of partial functions by cases. The formal model provides a
transparent treatment of DATR's notion of global context. It is shown that
DATR's default mechanism can be accounted for by interpreting value descriptors
as families of values indexed by paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505005</id><created>1995-05-02</created><updated>1995-05-04</updated><authors><author><keyname>Losee</keyname><forenames>Robert M.</forenames><affiliation>U. of North Carolina, Chapel Hill</affiliation></author></authors><title>Learning Syntactic Rules and Tags with Genetic Algorithms for
  Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules</title><categories>cmp-lg cs.CL</categories><comments>latex document, postscript figures not included. Accepted for
  publication in Information Processing and Management</comments><abstract>  The grammars of natural languages may be learned by using genetic algorithms
that reproduce and mutate grammatical rules and part-of-speech tags, improving
the quality of later generations of grammatical components. Syntactic rules are
randomly generated and then evolve; those rules resulting in improved parsing
and occasionally improved retrieval and filtering performance are allowed to
further propagate. The LUST system learns the characteristics of the language
or sublanguage used in document abstracts by learning from the document
rankings obtained from the parsed abstracts. Unlike the application of
traditional linguistic rules to retrieval and filtering applications, LUST
develops grammatical structures and tags without the prior imposition of some
common grammatical assumptions (e.g., part-of-speech assumptions), producing
grammars that are empirically based and are optimized for this particular
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505006</id><created>1995-05-02</created><authors><author><keyname>Dahl</keyname><forenames>Veronica</forenames></author><author><keyname>Tarau</keyname><forenames>Paul</forenames></author><author><keyname>Moreno</keyname><forenames>Lidia</forenames></author><author><keyname>Palomar</keyname><forenames>Manolo</forenames></author></authors><title>Treating Coordination with Datalog Grammars</title><categories>cmp-lg cs.CL</categories><abstract>  In previous work we studied a new type of DCGs, Datalog grammars, which are
inspired on database theory. Their efficiency was shown to be better than that
of their DCG counterparts under (terminating) OLDT-resolution. In this article
we motivate a variant of Datalog grammars which allows us a meta-grammatical
treatment of coordination. This treatment improves in some respects over
previous work on coordination in logic grammars, although more research is
needed for testing it in other respects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505007</id><created>1995-05-03</created><authors><author><keyname>Pericliev</keyname><forenames>Vladimir</forenames><affiliation>Institute of Mathematics, Sofia, Bulgaria</affiliation></author><author><keyname>Grigorov</keyname><forenames>Alexander</forenames><affiliation>Institute of Mathematics, Sofia, Bulgaria</affiliation></author></authors><title>Parsing a Flexible Word Order Language</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, in proceedings of COLING-94</comments><abstract>  A logic formalism is presented which increases the expressive power of the
ID/LP format of GPSG by enlarging the inventory of ordering relations and
extending the domain of their application to non-siblings. This allows a
concise, modular and declarative statement of intricate word order
regularities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505008</id><created>1995-05-03</created><authors><author><keyname>Shaw</keyname><forenames>James</forenames><affiliation>Columbia University</affiliation></author></authors><title>Conciseness through Aggregation in Text Generation</title><categories>cmp-lg cs.CL</categories><comments>3 pages, LaTex, uses aclap.sty, student sessions of ACL95</comments><journal-ref>student sessions ACL 95</journal-ref><abstract>  Aggregating different pieces of similar information is necessary to generate
concise and easy to understand reports in technical domains. This paper
presents a general algorithm that combines similar messages in order to
generate one or more coherent sentences for them. The process is not as trivial
as might be expected. Problems encountered are briefly described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505009</id><created>1995-05-03</created><authors><author><keyname>Kasper</keyname><forenames>Robert</forenames><affiliation>Ohio State University</affiliation></author><author><keyname>Kiefer</keyname><forenames>Bernd</forenames><affiliation>DFKI Saarbruecken</affiliation></author><author><keyname>Netter</keyname><forenames>Klaus</forenames><affiliation>DFKI Saarbruecken</affiliation></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames><affiliation>University of Delaware</affiliation></author></authors><title>Compilation of HPSG to TAG</title><categories>cmp-lg cs.CL</categories><comments>8 pages, aclap.sty, appears in ACL-95, Note: LaTeX'ing this file
  requires a modified LaTeX with an increased &quot;semantic nest size.&quot;</comments><abstract>  We present an implemented compilation algorithm that translates HPSG into
lexicalized feature-based TAG, relating concepts of the two theories. While
HPSG has a more elaborated principle-based theory of possible phrase
structures, TAG provides the means to represent lexicalized structures more
explicitly. Our objectives are met by giving clear definitions that determine
the projection of structures from the lexicon, and identify maximal
projections, auxiliary trees and foot nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505010</id><created>1995-05-03</created><authors><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Universit&#xe4;t des Saarlandes, Computational Linguistics</affiliation></author></authors><title>Tagset Reduction Without Information Loss</title><categories>cmp-lg cs.CL</categories><comments>3 pages, LaTeX, to appear in proceedings of ACL-95, student session</comments><abstract>  A technique for reducing a tagset used for n-gram part-of-speech
disambiguation is introduced and evaluated in an experiment. The technique
ensures that all information that is provided by the original tagset can be
restored from the reduced one. This is crucial, since we are interested in the
linguistically motivated tags for part-of-speech disambiguation. The reduced
tagset needs fewer parameters for its statistical model and allows more
accurate parameter estimation. Additionally, there is a slight but not
significant improvement of tagging accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505011</id><created>1995-05-04</created><authors><author><keyname>Agarwal</keyname><forenames>Rajeev</forenames><affiliation>Mississippi State University</affiliation></author></authors><title>Evaluation of Semantic Clusters</title><categories>cmp-lg cs.CL</categories><comments>3 pages; Self-contained LaTeX; Uses aclap.sty; To appear in ACL-95
  student session</comments><abstract>  Semantic clusters of a domain form an important feature that can be useful
for performing syntactic and semantic disambiguation. Several attempts have
been made to extract the semantic clusters of a domain by probabilistic or
taxonomic techniques. However, not much progress has been made in evaluating
the obtained semantic clusters. This paper focuses on an evaluation mechanism
that can be used to evaluate semantic clusters produced by a system against
those provided by human experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505012</id><created>1995-05-04</created><updated>1995-05-04</updated><authors><author><keyname>Jacquemin</keyname><forenames>Christian</forenames><affiliation>Institut de Recherches en Informatique de Nantes</affiliation></author></authors><title>A Symbolic and Surgical Acquisition of Terms through Variation</title><categories>cmp-lg cs.CL</categories><comments>8 pages compressed uuencoded latex, uses aaai.sty, 1 figure .eps To
  appear in Proceedings Workshop &quot;New Approaches to Learning for NLP&quot; at
  IJCAI'95</comments><abstract>  Terminological acquisition is an important issue in learning for NLP due to
the constant terminological renewal through technological changes. Terms play a
key role in several NLP-activities such as machine translation, automatic
indexing or text understanding. In opposition to classical once-and-for-all
approaches, we propose an incremental process for terminological enrichment
which operates on existing reference lists and large corpora. Candidate terms
are acquired by extracting variants of reference terms through {\em FASTR}, a
unification-based partial parser. As acquisition is performed within specific
morpho-syntactic contexts (coordinations, insertions or permutations of
compounds), rich conceptual links are learned together with candidate terms. A
clustering of terms related through coordination yields classes of conceptually
close terms while graphs resulting from insertions denote generic/specific
relations. A graceful degradation of the volume of acquisition on partial
initial lists confirms the robustness of the method to incomplete data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505013</id><created>1995-05-05</created><authors><author><keyname>Reithinger</keyname><forenames>Norbert</forenames><affiliation>DFKI GmbH, Saarbr&#xfc;cken, Germany</affiliation></author><author><keyname>Maier</keyname><forenames>Elisabeth</forenames><affiliation>DFKI GmbH, Saarbr&#xfc;cken, Germany</affiliation></author></authors><title>Utilizing Statistical Dialogue Act Processing in Verbmobil</title><categories>cmp-lg cs.CL</categories><comments>6 pages; compressed and uuencoded postscript file; to appear in
  ACL-95</comments><abstract>  In this paper, we present a statistical approach for dialogue act processing
in the dialogue component of the speech-to-speech translation system \vm.
Statistics in dialogue processing is used to predict follow-up dialogue acts.
As an application example we show how it supports repair when unexpected
dialogue states occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505014</id><created>1995-05-05</created><authors><author><keyname>Gervas</keyname><forenames>Pablo</forenames><affiliation>Department of Computing, Imperial College, London</affiliation></author></authors><title>Compositionality for Presuppositions over Tableaux</title><categories>cmp-lg cs.CL</categories><abstract>  Tableaux originate as a decision method for a logical language. They can also
be extended to obtain a structure that spells out all the information in a set
of sentences in terms of truth value assignments to atomic formulas that appear
in them. This approach is pursued here. Over such a structure, compositional
rules are provided for obtaining the presuppositions of a logical statement
from its atomic subformulas and their presuppositions. The rules are based on
classical logic semantics and they are shown to model the behaviour of
presuppositions observed in natural language sentences built with {\em if
\ldots then}, {\em and} and {\em or}. The advantages of this method over
existing frameworks for presuppositions are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505015</id><created>1995-05-05</created><authors><author><keyname>Futrelle</keyname><forenames>Robert P.</forenames><affiliation>Northeastern U.</affiliation></author><author><keyname>Nikolakis</keyname><forenames>Nikos</forenames><affiliation>Northeastern U.</affiliation></author></authors><title>Efficient Analysis of Complex Diagrams using Constraint-Based Parsing</title><categories>cmp-lg cs.CL</categories><comments>9 pages, Postscript, no fonts, compressed, uuencoded. Composed in
  MSWord 5.1a for the Mac. To appear in ICDAR '95. Other versions at
  ftp://ftp.ccs.neu.edu/pub/people/futrelle</comments><abstract>  This paper describes substantial advances in the analysis (parsing) of
diagrams using constraint grammars. The addition of set types to the grammar
and spatial indexing of the data make it possible to efficiently parse real
diagrams of substantial complexity. The system is probably the first to
demonstrate efficient diagram parsing using grammars that easily be retargeted
to other domains. The work assumes that the diagrams are available as a flat
collection of graphics primitives: lines, polygons, circles, Bezier curves and
text. This is appropriate for future electronic documents or for vectorized
diagrams converted from scanned images. The classes of diagrams that we have
analyzed include x,y data graphs and genetic diagrams drawn from the biological
literature, as well as finite state automata diagrams (states and arcs). As an
example, parsing a four-part data graph composed of 133 primitives required 35
sec using Macintosh Common Lisp on a Macintosh Quadra 700.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505016</id><created>1995-05-06</created><authors><author><keyname>Fung</keyname><forenames>Pascale</forenames><affiliation>Computer Science Department, Columbia Univ</affiliation></author></authors><title>A Pattern Matching method for finding Noun and Proper Noun Translations
  from Noisy Parallel Corpora</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded compressed postscript file. To appear in the
  Proceedings of the 33rd ACL</comments><abstract>  We present a pattern matching method for compiling a bilingual lexicon of
nouns and proper nouns from unaligned, noisy parallel texts of
Asian/Indo-European language pairs. Tagging information of one language is
used. Word frequency and position information for high and low frequency words
are represented in two different vector forms for pattern matching. New anchor
point finding and noise elimination techniques are introduced. We obtained a
73.1\% precision. We also show how the results can be used in the compilation
of domain-specific noun phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505017</id><created>1995-05-08</created><authors><author><keyname>Hanrieder</keyname><forenames>Gerhard</forenames><affiliation>Bavarian Research Center for Knowledge Based Systems, Erlangen, Germany</affiliation></author><author><keyname>Goerz</keyname><forenames>Guenther</forenames><affiliation>Bavarian Research Center for Knowledge Based Systems, Erlangen, Germany</affiliation></author></authors><title>Robust Parsing of Spoken Dialogue Using Contextual Knowledge and
  Recognition Probabilities</title><categories>cmp-lg cs.CL</categories><comments>4 pages, LaTex source, 3 PostScript figures, uses epsf.sty and
  ETRW.sty, to appear in Proceedings of ESCA Workshop on Spoken Dialogue
  Systems, Denmark, May 30-June 2</comments><abstract>  In this paper we describe the linguistic processor of a spoken dialogue
system. The parser receives a word graph from the recognition module as its
input. Its task is to find the best path through the graph. If no complete
solution can be found, a robust mechanism for selecting multiple partial
results is applied. We show how the information content rate of the results can
be improved if the selection is based on an integrated quality score combining
word recognition scores and context-dependent semantic predictions. Results of
parsing word graphs with and without predictions are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505018</id><created>1995-05-08</created><authors><author><keyname>de Marcken</keyname><forenames>Carl</forenames><affiliation>MIT AI Laboratory</affiliation></author></authors><title>Acquiring a Lexicon from Unsegmented Speech</title><categories>cmp-lg cs.CL</categories><comments>Postscript, 3-pages, to appear in ACL '95 Student Session</comments><abstract>  We present work-in-progress on the machine acquisition of a lexicon from
sentences that are each an unsegmented phone sequence paired with a primitive
representation of meaning. A simple exploratory algorithm is described, along
with the direction of current work and a discussion of the relevance of the
problem for child language acquisition and computer speech recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505019</id><created>1995-05-08</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames><affiliation>IBM Research, T. J. Watson Research Center</affiliation></author></authors><title>Measuring semantic complexity</title><categories>cmp-lg cs.CL</categories><comments>11 pp. Latex.. To appear in Proc. BISFAI'95, The Fourth Bar-Ilan
  Symposium on Foundations of Artificial Intelligence, June 20-22, 1995,
  Ramat-Gan and Jerusalem, Israel. Correspondence to wlodz@watson.ibm.com</comments><abstract>  We define {\em semantic complexity} using a new concept of {\em meaning
automata}. We measure the semantic complexity of understanding of prepositional
phrases, of an &quot;in depth understanding system&quot;, and of a natural language
interface to an on-line calendar. We argue that it is possible to measure some
semantic complexities of natural language processing systems before building
them, and that systems that exhibit relatively complex behavior can be built
from semantically simple components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505020</id><created>1995-05-09</created><authors><author><keyname>Soderland</keyname><forenames>Stephen</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Fisher</keyname><forenames>David</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Aseltine</keyname><forenames>Jonathan</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Lehnert</keyname><forenames>Wendy</forenames><affiliation>University of Massachusetts</affiliation></author></authors><title>CRYSTAL: Inducing a Conceptual Dictionary</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Postscript, IJCAI-95
  http://ciir.cs.umass.edu/info/psfiles/tepubs/tepubs.html</comments><abstract>  One of the central knowledge sources of an information extraction system is a
dictionary of linguistic patterns that can be used to identify the conceptual
content of a text. This paper describes CRYSTAL, a system which automatically
induces a dictionary of &quot;concept-node definitions&quot; sufficient to identify
relevant information from a training corpus. Each of these concept-node
definitions is generalized as far as possible without producing errors, so that
a minimum number of dictionary entries cover the positive training instances.
Because it tests the accuracy of each proposed definition, CRYSTAL can often
surpass human intuitions in creating reliable extraction rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505021</id><created>1995-05-09</created><authors><author><keyname>Popowich</keyname><forenames>Fred</forenames><affiliation>Simon Fraser University</affiliation></author></authors><title>Improving the Efficiency of a Generation Algorithm for Shake and Bake
  Machine Translation Using Head-Driven Phrase Structure Grammar</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uuencoded and compressed Postscript. To appear in
  Proceedings of the Fifth International Workshop on Natural Language and Logic
  Programming (NLULP5)</comments><abstract>  A Shake and Bake machine translation algorithm for Head-Driven Phrase
Structure Grammar is introduced based on the algorithm proposed by Whitelock
for unification categorial grammar. The translation process is then analysed to
determine where the potential sources of inefficiency reside, and some
proposals are introduced which greatly improve the efficiency of the generation
algorithm. Preliminary empirical results from tests involving a small grammar
are presented, and suggestions for greater improvement to the algorithm are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505022</id><created>1995-05-09</created><authors><author><keyname>Dale</keyname><forenames>Robert</forenames><affiliation>Microsoft, Sydney</affiliation></author></authors><title>Generating One-Anaphoric Expressions: Where Does the Decision Lie?</title><categories>cmp-lg cs.CL</categories><comments>10 pages, compressed PS file; This paper appears in the Working
  Papers of Pacling-II, Brisbane, Australia, April 19--22 1995.</comments><abstract>  Most natural language generation systems embody mechanisms for choosing
whether to subsequently refer to an already-introduced entity by means of a
pronoun or a definite noun phrase. Relatively few systems, however, consider
referring to entites by means of one-anaphoric expressions such as
\lingform{the small green one}. This paper looks at what is involved in
generating referring expressions of this type. Consideration of how to fit this
capability into a standard algorithm for referring expression generation leads
us to suggest a revision of some of the assumptions that underlie existing
approaches. We demonstrate the usefulness of our approach to one-anaphora
generation in the context of a simple database interface application, and make
some observations about the impact of this approach on referring expression
generation more generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505023</id><created>1995-05-10</created><authors><author><keyname>Srinivas</keyname><forenames>B.</forenames><affiliation>Department of Computer and Information Science, University of Pennsylvania</affiliation></author><author><keyname>Joshi</keyname><forenames>Aravind</forenames><affiliation>Department of Computer and Information Science, University of Pennsylvania</affiliation></author></authors><title>Some Novel Applications of Explanation-Based Learning to Parsing
  Lexicalized Tree-Adjoining Grammars</title><categories>cmp-lg cs.CL</categories><comments>uuencoded postscript file</comments><journal-ref>ACL 1995</journal-ref><abstract>  In this paper we present some novel applications of Explanation-Based
Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars. The
novel aspects are (a) immediate generalization of parses in the training set,
(b) generalization over recursive structures and (c) representation of
generalized parses as Finite State Transducers. A highly impoverished parser
called a ``stapler'' has also been introduced. We present experimental results
using EBL for different corpora and architectures to show the effectiveness of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505024</id><created>1995-05-10</created><authors><author><keyname>Jones</keyname><forenames>Bernard</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh, UK</affiliation></author></authors><title>Exploring the role of Punctuation in Parsing Natural Text</title><categories>cmp-lg cs.CL</categories><comments>5 pages, LaTeX (2.09 preferred), COLING-94 paper</comments><abstract>  Few, if any, current NLP systems make any significant use of punctuation.
Intuitively, a treatment of punctuation seems necessary to the analysis and
production of text. Whilst this has been suggested in the fields of discourse
structure, it is still unclear whether punctuation can help in the syntactic
field. This investigation attempts to answer this question by parsing some
corpus-based material with two similar grammars --- one including rules for
punctuation, the other ignoring it. The punctuated grammar significantly
out-performs the unpunctuated one, and so the conclusion is that punctuation
can play a useful role in syntactic processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505025</id><created>1995-05-10</created><authors><author><keyname>Litman</keyname><forenames>Diane J.</forenames><affiliation>AT\&amp;T Bell Laboratories</affiliation></author><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames><affiliation>Bellcore</affiliation></author></authors><title>Combining Multiple Knowledge Sources for Discourse Segmentation</title><categories>cmp-lg cs.CL</categories><comments>8 pages. Self-contained latex source. To appear in Proceedings of the
  33rd ACL, 1995. (This replacement version revised so that no lines exceed 80
  characters.)</comments><abstract>  We predict discourse segment boundaries from linguistic features of
utterances, using a corpus of spoken narratives as data. We present two methods
for developing segmentation algorithms from training data: hand tuning and
machine learning. When multiple types of features are used, results approach
human performance on an independent test set (both methods), and using
cross-validation (machine learning).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505026</id><created>1995-05-11</created><authors><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Universit&quot;at des Saarlandes, Computational Linguistics, Saarbr&quot;ucken, Germany</affiliation></author><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>Universit&quot;at des Saarlandes, Computational Linguistics, Saarbr&quot;ucken, Germany</affiliation></author></authors><title>Tagging the Teleman Corpus</title><categories>cmp-lg cs.CL</categories><comments>14 pages, LaTeX, to appear in Proceedings of the 10th Nordic
  Conference of Computational Linguistics, Helsinki, Finland, 1995</comments><abstract>  Experiments were carried out comparing the Swedish Teleman and the English
Susanne corpora using an HMM-based and a novel reductionistic statistical
part-of-speech tagger. They indicate that tagging the Teleman corpus is the
more difficult task, and that the performance of the two different taggers is
comparable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505027</id><created>1995-05-11</created><authors><author><keyname>Park</keyname><forenames>Jong C.</forenames><affiliation>Department of Computer and Information Science, University of Pennsylvania</affiliation></author></authors><title>Quantifier Scope and Constituency</title><categories>cmp-lg cs.CL</categories><comments>8 pages, compressed and uuencoded postscript file, ACL-95</comments><abstract>  Traditional approaches to quantifier scope typically need stipulation to
exclude readings that are unavailable to human understanders. This paper shows
that quantifier scope phenomena can be precisely characterized by a semantic
representation constrained by surface constituency, if the distinction between
referential and quantificational NPs is properly observed. A CCG implementation
is described and compared to other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505028</id><created>1995-05-12</created><authors><author><keyname>Rambow</keyname><forenames>Owen</forenames></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames></author><author><keyname>Weir</keyname><forenames>David</forenames></author></authors><title>D-Tree Grammars</title><categories>cmp-lg cs.CL</categories><comments>Latex source, needs aclap.sty, 8 pages, to appear in ACL-95</comments><abstract>  DTG are designed to share some of the advantages of TAG while overcoming some
of its limitations. DTG involve two composition operations called subsertion
and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG,
there is complete uniformity in the way that the two DTG operations relate
lexical items: subsertion always corresponds to complementation and
sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a
uniform analysis for em wh-movement in English and Kashmiri, despite the fact
that the em wh element in Kashmiri appears in sentence-second position, and not
sentence-initial position as in English.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505029</id><created>1995-05-13</created><authors><author><keyname>Park</keyname><forenames>Hyun S.</forenames></author></authors><title>Mapping Scrambled Korean Sentences into English Using Synchronous TAGs</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed ps file. 3 pages. To appear ACL95</comments><abstract>  Synchronous Tree Adjoining Grammars can be used for Machine Translation.
However, translating a free order language such as Korean to English is
complicated. I present a mechanism to translate scrambled Korean sentences into
English by combining the concepts of Multi-Component TAGs (MC-TAGs) and
Synchronous TAGs (STAGs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505030</id><created>1995-05-15</created><authors><author><keyname>Evans</keyname><forenames>Roger</forenames><affiliation>University of Brighton</affiliation></author><author><keyname>Gazdar</keyname><forenames>Gerald</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Weir</keyname><forenames>David</forenames><affiliation>University of Sussex</affiliation></author></authors><title>Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic
  Inheritance Hierarchy</title><categories>cmp-lg cs.CL</categories><comments>Latex source, needs aclap.sty, 8 pages</comments><journal-ref>Proceedings of ACL95</journal-ref><abstract>  This paper shows how DATR, a widely used formal language for lexical
knowledge representation, can be used to define an LTAG lexicon as an
inheritance hierarchy with internal lexical rules. A bottom-up featural
encoding is used for LTAG trees and this allows lexical rules to be implemented
as covariation constraints within feature structures. Such an approach
eliminates the considerable redundancy otherwise associated with an LTAG
lexicon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505031</id><created>1995-05-15</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames><affiliation>IBM Research, T. J. Watson Research Center</affiliation></author></authors><title>The Compactness of Construction Grammars</title><categories>cmp-lg cs.CL</categories><comments>10 pp. Latex. Correspondence to wlodz@watson.ibm.com</comments><report-no>IBM Research Report RC 20003 (88493)</report-no><abstract>  We present an argument for {\em construction grammars} based on the minimum
description length (MDL) principle (a formal version of the Ockham Razor). The
argument consists in using linguistic and computational evidence in setting up
a formal model, and then applying the MDL principle to prove its superiority
with respect to alternative models. We show that construction-based
representations are at least an order of magnitude more compact that the
corresponding lexicalized representations of the same linguistic data.
 The result is significant for our understanding of the relationship between
syntax and semantics, and consequently for choosing NLP architectures. For
instance, whether the processing should proceed in a pipeline from syntax to
semantics to pragmatics, and whether all linguistic information should be
combined in a set of constraints. From a broader perspective, this paper does
not only argue for a certain model of processing, but also provides a
methodology for determining advantages of different approaches to NLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505032</id><created>1995-05-15</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames><affiliation>IBM Research, T. J. Watson Research Center</affiliation></author></authors><title>Context and ontology in understanding of dialogs</title><categories>cmp-lg cs.CL</categories><comments>7 pp. Latex (documentstyle[ijcai89,named]). Proc. IJCAI'95 Workshop
  on Context in NLP. Montreal, Aug.1995. Correspondence to wlodz@watson.ibm.com</comments><abstract>  We present a model of NLP in which ontology and context are directly included
in a grammar. The model is based on the concept of {\em construction},
consisting of a set of features of form, a set of semantic and pragmatic
conditions describing its application context, and a description of its
meaning. In this model ontology is embedded into the grammar; e.g. the
hierarchy of {\it np} constructions is based on the corresponding ontology.
Ontology is also used in defining contextual parameters; e.g. $\left[
current\_question \ time(\_) \right] $.
 A parser based on this model allowed us to build a set of dialog understanding
systems that include an on-line calendar, a banking machine, and an insurance
quote system. The proposed approach is an alternative to the standard
&quot;pipeline&quot; design of morphology-syntax-semantics-pragmatics; the account of
meaning conforms to our intuitions about compositionality, but there is no
homomorphism from syntax to semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505033</id><created>1995-05-16</created><updated>1995-05-17</updated><authors><author><keyname>Stromback</keyname><forenames>Lena</forenames><affiliation>Department of Computer and Information Science Linkoping University, Sweden</affiliation></author></authors><title>User-Defined Nonmonotonicity in Unification-Based Formalisms</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses aclap.sty and acl.bst</comments><journal-ref>ACL-95</journal-ref><abstract>  A common feature of recent unification-based grammar formalisms is that they
give the user the ability to define his own structures. However, this
possibility is mostly limited and does not include nonmonotonic operations. In
this paper we show how nonmonotonic operations can also be user-defined by
applying default logic (Reiter 1980) and generalizing previous results on
nonmonotonic sorts (Young &amp; Rounds 1993).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505034</id><created>1995-05-16</created><authors><author><keyname>Poesio</keyname><forenames>Massimo</forenames><affiliation>University of Edinburgh, Centre for Cognitive Science</affiliation></author></authors><title>Semantic Ambiguity and Perceived Ambiguity</title><categories>cmp-lg cs.CL</categories><comments>Latex, 47 pages. Uses tree-dvips.sty, lingmacros.sty, fullname.sty</comments><journal-ref>K. van Deemter and S. Peters (eds.), Semantic ambiguity and
  Underspecification, CSLI, 1995</journal-ref><abstract>  I explore some of the issues that arise when trying to establish a connection
between the underspecification hypothesis pursued in the NLP literature and
work on ambiguity in semantics and in the psychological literature. A theory of
underspecification is developed `from the first principles', i.e., starting
from a definition of what it means for a sentence to be semantically ambiguous
and from what we know about the way humans deal with ambiguity. An
underspecified language is specified as the translation language of a grammar
covering sentences that display three classes of semantic ambiguity: lexical
ambiguity, scopal ambiguity, and referential ambiguity. The expressions of this
language denote sets of senses. A formalization of defeasible reasoning with
underspecified representations is presented, based on Default Logic. Some
issues to be confronted by such a formalization are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505035</id><created>1995-05-19</created><authors><author><keyname>Le&#xf3;n</keyname><forenames>Fernando S&#xe1;nchez</forenames><affiliation>Laboratorio de Ling&#xfc;&#xed;stica Inform&#xe1;tica, Facultad de Filosof&#xed;a y Letras, Universidad Aut&#xf3;noma de Madrid</affiliation></author><author><keyname>Serrano</keyname><forenames>Amalio F. Nieto</forenames><affiliation>Departamento de Ingenier&#xed;a de Sistemas Telem&#xe1;ticos, Escuela Superior de Ingenieros de Telecomunicaciones, Universidad Polit&#xe9;cnica de Madrid</affiliation></author></authors><title>Development of a Spanish Version of the Xerox Tagger</title><categories>cmp-lg cs.CL</categories><comments>13 pages</comments><report-no>CRATER/WP6/FR1</report-no><abstract>  This paper describes work performed withing the CRATER ({\em C}orpus {\em
R}esources {\em A}nd {\em T}erminology {\em E}xt{\em R}action, MLAP-93/20)
project, funded by the Commission of the European Communities. In particular,
it addresses the issue of adapting the Xerox Tagger to Spanish in order to tag
the Spanish version of the ITU (International Telecommunications Union) corpus.
The model implemented by this tagger is briefly presented along with some
modifications performed on it in order to use some parameters not
probabilistically estimated. Initial decisions, like the tagset, the lexicon
and the training corpus are also discussed. Finally, results are presented and
the benefits of the {\em mixed model} justified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505036</id><created>1995-05-19</created><authors><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames><affiliation>Bellcore</affiliation></author></authors><title>Integrating Gricean and Attentional Constraints</title><categories>cmp-lg cs.CL</categories><comments>7 pages. Self-contained latex source. Uses ijcai95.sty and named.bst.
  To appear in IJCAI 1995</comments><abstract>  This paper concerns how to generate and understand discourse anaphoric noun
phrases. I present the results of an analysis of all discourse anaphoric noun
phrases (N=1,233) in a corpus of ten narrative monologues, where the choice
between a definite pronoun or phrasal NP conforms largely to Gricean
constraints on informativeness. I discuss Dale and Reiter's [To appear] recent
model and show how it can be augmented for understanding as well as generating
the range of data presented here. I argue that integrating centering [Grosz et
al., 1983] [Kameyama, 1985] with this model can be applied uniformly to
discourse anaphoric pronouns and phrasal NPs. I conclude with a hypothesis for
addressing the interaction between local and global discourse processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505037</id><created>1995-05-22</created><authors><author><keyname>Rapp</keyname><forenames>Reinhard</forenames><affiliation>ISSCO, University of Geneva</affiliation></author></authors><title>Identifying Word Translations in Non-Parallel Texts</title><categories>cmp-lg cs.CL</categories><comments>3 pages, requires aclap.sty and epic.sty</comments><journal-ref>Proceedings of ACL-95</journal-ref><abstract>  Common algorithms for sentence and word-alignment allow the automatic
identification of word translations from parallel texts. This study suggests
that the identification of word translations should also be possible with
non-parallel and even unrelated texts. The method proposed is based on the
assumption that there is a correlation between the patterns of word
co-occurrences in texts of different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505038</id><created>1995-05-23</created><authors><author><keyname>Nagao</keyname><forenames>Katashi</forenames><affiliation>Sony Computer Science Laboratory Inc.</affiliation></author><author><keyname>Rekimoto</keyname><forenames>Jun</forenames><affiliation>Sony Computer Science Laboratory Inc.</affiliation></author></authors><title>Ubiquitous Talker: Spoken Language Interaction with Real World Objects</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX file with PostScript files, to appear in Proc.
  IJCAI-95, also available from http://www.csl.sony.co.jp/person/nagao.html</comments><abstract>  Augmented reality is a research area that tries to embody an electronic
information space within the real world, through computational devices. A
crucial issue within this area, is the recognition of real world objects or
situations.
  In natural language processing, it is much easier to determine
interpretations of utterances, even if they are ill-formed, when the context or
situation is fixed. We therefore introduce robust, natural language processing
into a system of augmented reality with situation awareness. Based on this
idea, we have developed a portable system, called the Ubiquitous Talker. This
consists of an LCD display that reflects the scene at which a user is looking
as if it is a transparent glass, a CCD camera for recognizing real world
objects with color-bar ID codes, a microphone for recognizing a human voice and
a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its
user with some information related to a recognized object, by using the display
and voice. It also accepts requests or questions as voice inputs. The user
feels as if he/she is talking with the object itself through the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505039</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505039</id><created>1995-05-23</created><updated>1995-08-07</updated><authors><author><keyname>Taylor</keyname><forenames>Jasper</forenames><affiliation>Human Communication Research Centre University of Edinburgh</affiliation></author></authors><title>Generating efficient belief models for task-oriented dialogues</title><categories>cmp-lg cs.CL</categories><comments>17 pages, 1 postscript figure. Paper presented at CLNLP workshop,
  Edinburgh, April 3-5, 1995</comments><abstract>  We have shown that belief modelling for dialogue can be simplified if the
assumption is made that the participants are cooperating, i.e., they are not
committed to any goals requiring deception. In such domains, there is no need
to maintain individual representations of deeply nested beliefs; instead, three
specific types of belief can be used to summarize all the states of nested
belief that can exist about a domain entity.
  Here, we set out to design a ``compiler'' for belief models. This system will
accept as input a description of agents' interactions with a task domain
expressed in a fully-expressive belief logic with non-monotonic and temporal
extensions. It generates an operational belief model for use in that domain,
sufficient for the requirements of cooperative dialogue, including the
negotiation of complex domain plans. The compiled model incorporates the belief
simplification mentioned above, and also uses a simplified temporal logic of
belief based on the restricted circumstances under which beliefs can change.
  We shall review the motivation for creating such a system, and introduce a
general procedure for taking a logical specification for a domain and procesing
it into an operational model. We shall then discuss the specific changes that
are made during this procedure for limiting the level of abstraction at which
the concepts of belief nesting, default reasoning and time are expressed.
Finally we shall go through a worked example relating to the Map Task, a simple
cooperative problem-solving exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505040</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505040</id><created>1995-05-23</created><authors><author><keyname>Ramshaw</keyname><forenames>Lance A.</forenames><affiliation>Bowdoin College</affiliation></author><author><keyname>Marcus</keyname><forenames>Mitchell P.</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Text Chunking using Transformation-Based Learning</title><categories>cmp-lg cs.CL</categories><comments>13 pages, LaTeX2e, 1 included figure</comments><journal-ref>ACL Third Workshop on Very Large Corpora, June 1995, pp. 82-94</journal-ref><abstract>  Eric Brill introduced transformation-based learning and showed that it can do
part-of-speech tagging with fairly high accuracy. The same method can be
applied at a higher level of textual interpretation for locating chunks in the
tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is
convenient to view chunking as a tagging problem by encoding the chunk
structure in new tags attached to each word. In automatic tests using
Treebank-derived data, this technique achieved recall and precision rates of
roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that
partition the sentence. Some interesting adaptations to the
transformation-based learning approach are also suggested by this application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505041</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505041</id><created>1995-05-23</created><authors><author><keyname>Rogers</keyname><forenames>James</forenames><affiliation>Institute for Research in Cognitive Science, University of Pennsylvania</affiliation></author></authors><title>On Descriptive Complexity, Language Complexity, and GB</title><categories>cmp-lg cs.CL</categories><comments>To appear in Specifying Syntactic Structures, papers from the Logic,
  Structures, and Syntax workshop, Amsterdam, Sept. 1994. LaTeX source with
  nine included postscript figures</comments><abstract>  We introduce $L^2_{K,P}$, a monadic second-order language for reasoning about
trees which characterizes the strongly Context-Free Languages in the sense that
a set of finite trees is definable in $L^2_{K,P}$ iff it is (modulo a
projection) a Local Set---the set of derivation trees generated by a CFG. This
provides a flexible approach to establishing language-theoretic complexity
results for formalisms that are based on systems of well-formedness constraints
on trees. We demonstrate this technique by sketching two such results for
Government and Binding Theory. First, we show that {\em free-indexation\/}, the
mechanism assumed to mediate a variety of agreement and binding relationships
in GB, is not definable in $L^2_{K,P}$ and therefore not enforcible by CFGs.
Second, we show how, in spite of this limitation, a reasonably complete GB
account of English can be defined in $L^2_{K,P}$. Consequently, the language
licensed by that account is strongly context-free. We illustrate some of the
issues involved in establishing this result by looking at the definition, in
$L^2_{K,P}$, of chains. The limitations of this definition provide some insight
into the types of natural linguistic principles that correspond to higher
levels of language complexity. We close with some speculation on the possible
significance of these results for generative linguistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505042</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505042</id><created>1995-05-24</created><authors><author><keyname>Nasukawa</keyname><forenames>Tetsuya</forenames><affiliation>IBM Research, Tokyo Research Laboratory</affiliation></author></authors><title>Robust Parsing Based on Discourse Information: Completing partial parses
  of ill-formed sentences on the basis of discourse information</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of ACL-95, 8 pages, 4 Postscript figures,
  uses aclap.sty and epsbox.sty</comments><report-no>TRL-4372</report-no><abstract>  In a consistent text, many words and phrases are repeatedly used in more than
one sentence. When an identical phrase (a set of consecutive words) is repeated
in different sentences, the constituent words of those sentences tend to be
associated in identical modification patterns with identical parts of speech
and identical modifiee-modifier relationships. Thus, when a syntactic parser
cannot parse a sentence as a unified structure, parts of speech and
modifiee-modifier relationships among morphologically identical words in
complete parses of other sentences within the same text provide useful
information for obtaining partial parses of the sentence. In this paper, we
describe a method for completing partial parses by maintaining consistency
among morphologically identical words within the same text as regards their
part of speech and their modifiee-modifier relationship. The experimental
results obtained by using this method with technical documents offer good
prospects for improving the accuracy of sentence analysis in a broad-coverage
natural language processing system such as a machine translation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505043</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505043</id><created>1995-05-24</created><authors><author><keyname>McCarthy</keyname><forenames>Joseph F.</forenames><affiliation>University of Massachusetts</affiliation></author><author><keyname>Lehnert</keyname><forenames>Wendy G.</forenames><affiliation>University of Massachusetts</affiliation></author></authors><title>Using Decision Trees for Coreference Resolution</title><categories>cmp-lg cs.CL</categories><comments>6 pages; LaTeX source; 1 uuencoded compressed EPS file (separate);
  uses ijcai95.sty, named.bst, epsf.tex; to appear in Proc. IJCAI '95</comments><abstract>  This paper describes RESOLVE, a system that uses decision trees to learn how
to classify coreferent phrases in the domain of business joint ventures. An
experiment is presented in which the performance of RESOLVE is compared to the
performance of a manually engineered set of rules for the same task. The
results show that decision trees achieve higher performance than the rules in
two of three evaluation metrics developed for the coreference task. In addition
to achieving better performance than the rules, RESOLVE provides a framework
that facilitates the exploration of the types of knowledge that are useful for
solving the coreference problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505044</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505044</id><created>1995-05-25</created><authors><author><keyname>Melamed</keyname><forenames>I. Dan</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best
  Translation Lexicons</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of the Third Workshop on Very Large Corpora,
  15 pages, uuencoded compressed PostScript</comments><abstract>  This paper shows how to induce an N-best translation lexicon from a bilingual
text corpus using statistical properties of the corpus together with four
external knowledge sources. The knowledge sources are cast as filters, so that
any subset of them can be cascaded in a uniform framework. A new objective
evaluation measure is used to compare the quality of lexicons induced with
different filter cascades. The best filter cascades improve lexicon quality by
up to 137% over the plain vanilla statistical method, and approach human
performance. Drastically reducing the size of the training corpus has a much
smaller impact on lexicon quality when these knowledge sources are used. This
makes it practical to train on small hand-built corpora for language pairs
where large bilingual corpora are unavailable. Moreover, three of the four
filters prove useful even when used with large training corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505045</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9505045</id><created>1995-05-26</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge, UK</affiliation></author><author><keyname>Bouillon</keyname><forenames>Pierrette</forenames><affiliation>ISSCO, Geneva, Switzerland</affiliation></author></authors><title>Hybrid Transfer in an English-French Spoken Language Translator</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX (2.09 preferred); eaclap.sty; Procs of IA '95
  (Montpellier, France)</comments><report-no>CRC-056; see http://www.cam.sri.com/</report-no><abstract>  The paper argues the importance of high-quality translation for spoken
language translation systems. It describes an architecture suitable for rapid
development of high-quality limited-domain translation systems, which has been
implemented within an advanced prototype English to French spoken language
translator. The focus of the paper is the hybrid transfer model which combines
unification-based rules and a set of trainable statistical preferences;
roughly, rules encode domain-independent grammatical information and
preferences encode domain-dependent distributional information. The preferences
are trained from sets of examples produced by the system, which have been
annotated by human judges as correct or incorrect. An experiment is described
in which the model was tested on a 2000 utterance sample of previously unseen
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506001</id><created>1995-06-01</created><authors><author><keyname>Grote</keyname><forenames>Brigitte</forenames><affiliation>FAW Ulm</affiliation></author><author><keyname>Lenke</keyname><forenames>Nils</forenames><affiliation>Universitaet Duisburg</affiliation></author><author><keyname>Stede</keyname><forenames>Manfred</forenames><affiliation>FAW Ulm and University of Toronto</affiliation></author></authors><title>Ma(r)king concessions in English and German</title><categories>cmp-lg cs.CL</categories><comments>23 pages, uuencoded compressed postscript</comments><journal-ref>Proc. of the 6th European Workshop on Natural Language Generation,
  NL-Leiden, May 1995</journal-ref><abstract>  In order to generate cohesive discourse, many of the relations holding
between text segments need to be signalled to the reader by means of cue words,
or {\em discourse markers}. Programs usually do this in a simplistic way, e.g.,
by using one marker per relation. In reality, however, language offers a very
wide range of markers from which informed choices should be made. In order to
account for the variety and to identify the parameters governing the choices,
detailled linguistic analyses are necessary. We worked with one area of
discourse relations, the Concession family, identified its underlying
pragmatics and semantics, and undertook extensive corpus studies to examine the
range of markers used in both English and German. On the basis of an initial
classification of these markers, we propose a generation model for producing
bilingual text that can incorporate marker choice into its overall decision
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506002</id><created>1995-06-02</created><updated>1995-11-20</updated><authors><author><keyname>Mueller</keyname><forenames>Martin</forenames><affiliation>German Research Center for Artificial Intelligence</affiliation></author><author><keyname>Niehren</keyname><forenames>Joachim</forenames><affiliation>German Research Center for Artificial Intelligence</affiliation></author></authors><title>Weak subsumption Constraints for Type Diagnosis: An Incremental
  Algorithm</title><categories>cmp-lg cs.CL</categories><comments>Presented at CLNLP'95. An improved version is available under the
  name &quot;A Type is a Type is a Type&quot; from the Authors</comments><abstract>  We introduce constraints necessary for type checking a higher-order
concurrent constraint language, and solve them with an incremental algorithm.
Our constraint system extends rational unification by constraints x$\subseteq$
y saying that ``$x$ has at least the structure of $y$'', modelled by a weak
instance relation between trees. This notion of instance has been carefully
chosen to be weaker than the usual one which renders semi-unification
undecidable. Semi-unification has more than once served to link unification
problems arising from type inference and those considered in computational
linguistics. Just as polymorphic recursion corresponds to subsumption through
the semi-unification problem, our type constraint problem corresponds to weak
subsumption of feature graphs in linguistics. The decidability problem for
\WhatsIt for feature graphs has been settled by
D\&quot;orre~\cite{Doerre:WeakSubsumption:94}. \nocite{RuppRosnerJohnson:94} In
contrast to D\&quot;orre's, our algorithm is fully incremental and does not refer to
finite state automata. Our algorithm also is a lot more flexible. It allows a
number of extensions (records, sorts, disjunctive types, type declarations, and
others) which make it suitable for type inference of a full-fledged programming
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506003</id><created>1995-06-02</created><authors><author><keyname>Hammond</keyname><forenames>Michael</forenames><affiliation>University of Arizona</affiliation></author></authors><title>Syllable parsing in English and French</title><categories>cmp-lg cs.CL</categories><comments>postscript (mac), unix-compressed, mac(!)-uuencoded</comments><abstract>  In this paper I argue that Optimality Theory provides for an explanatory
model of syllabic parsing in English and French. The argument is based on
psycholinguistic facts that have been mysterious up to now. This argument is
further buttressed by the computational implementation developed here. This
model is important for several reasons. First, it provides a demonstration of
how OT can be used in a performance domain. Second, it suggests a new
relationship between phonological theory and psycholinguistics. (Code in Perl
is included and a WWW-interface is running at
http://mayo.douglass.arizona.edu.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506004</id><created>1995-06-06</created><authors><author><keyname>Kulick</keyname><forenames>Seth</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Using Higher-Order Logic Programming for Semantic Interpretation of
  Coordinate Constructs</title><categories>cmp-lg cs.CL</categories><comments>7 pages, ACL-95, uses aclap.sty</comments><abstract>  Many theories of semantic interpretation use lambda-term manipulation to
compositionally compute the meaning of a sentence. These theories are usually
implemented in a language such as Prolog that can simulate lambda-term
operations with first-order unification. However, for some interesting cases,
such as a Combinatory Categorial Grammar account of coordination constructs,
this can only be done by obscuring the underlying linguistic theory with the
``tricks'' needed for implementation. This paper shows how the use of abstract
syntax permitted by higher-order logic programming allows an elegant
implementation of the semantics of Combinatory Categorial Grammar, including
its handling of coordination constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506005</id><created>1995-06-08</created><updated>1995-09-07</updated><authors><author><keyname>Teufel</keyname><forenames>Simone</forenames><affiliation>IMS-CL, University of Stuttgart</affiliation></author></authors><title>A Support Tool for Tagset Mapping</title><categories>cmp-lg cs.CL</categories><comments>EACL-Sigdat 95, contains 4 ps figures (minor graphic changes)</comments><abstract>  Many different tagsets are used in existing corpora; these tagsets vary
according to the objectives of specific projects (which may be as far apart as
robust parsing vs. spelling correction). In many situations, however, one would
like to have uniform access to the linguistic information encoded in corpus
annotations without having to know the classification schemes in detail. This
paper describes a tool which maps unstructured morphosyntactic tags to a
constraint-based, typed, configurable specification language, a ``standard
tagset''. The mapping relies on a manually written set of mapping rules, which
is automatically checked for consistency. In certain cases, unsharp mappings
are unavoidable, and noise, i.e. groups of word forms {\sl not} conforming to
the specification, will appear in the output of the mapping. The system
automatically detects such noise and informs the user about it. The tool has
been tested with rules for the UPenn tagset \cite{up} and the SUSANNE tagset
\cite{garside}, in the framework of the EAGLES\footnote{LRE project EAGLES, cf.
\cite{eagles}.} validation phase for standardised tagsets for European
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506006</id><created>1995-06-08</created><authors><author><keyname>Hughes</keyname><forenames>John</forenames><affiliation>University of Leeds, UK</affiliation></author><author><keyname>Souter</keyname><forenames>Clive</forenames><affiliation>University of Leeds, UK</affiliation></author><author><keyname>Atwell</keyname><forenames>Eric</forenames><affiliation>University of Leeds, UK</affiliation></author></authors><title>Automatic Extraction of Tagset Mappings from Parallel-Annotated Corpora</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX, uses EACL95 style file: aclap.sty</comments><abstract>  This paper describes some of the recent work of project AMALGAM (automatic
mapping among lexico-grammatical annotation models). We are investigating ways
to map between the leading corpus annotation schemes in order to improve their
resuability. Collation of all the included corpora into a single large
annotated corpus will provide a more detailed language model to be developed
for tasks such as speech and handwriting recognition. In particular, we focus
here on a method of extracting mappings from corpora that have been annotated
according to more than one annotation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506007</id><created>1995-06-08</created><authors><author><keyname>Bayer</keyname><forenames>Sam</forenames><affiliation>Brown University</affiliation></author><author><keyname>Johnson</keyname><forenames>Mark</forenames><affiliation>Brown University</affiliation></author></authors><title>Features and Agreement</title><categories>cmp-lg cs.CL</categories><abstract>  This paper compares the consistency-based account of agreement phenomena in
`unification-based' grammars with an implication-based account based on a
simple feature extension to Lambek Categorial Grammar (LCG). We show that the
LCG treatment accounts for constructions that have been recognized as
problematic for `unification-based' treatments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506008</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506008</id><created>1995-06-09</created><authors><author><keyname>Editors</keyname></author><author><keyname>:</keyname></author><author><keyname>Stone</keyname><forenames>Matthew</forenames></author><author><keyname>Levison</keyname><forenames>Libby</forenames></author></authors><title>CLiFF Notes: Research in the Language, Information and Computation
  Laboratory of the University of Pennsylvania</title><categories>cmp-lg cs.CL</categories><comments>Annual Research Survey. 112 pages. uuencoded compressed postscript.
  Available as http://www.cis.upenn.edu/~cliff-group/94/cliffnotes.html</comments><report-no>Technical Report CIS 95-07</report-no><abstract>  Short abstracts by computational linguistics researchers at the University of
Pennsylvania describing ongoing individual and joint projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506009</id><created>1995-06-09</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Chander</keyname><forenames>Ishwar</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Haines</keyname><forenames>Matthew</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Hatzivassiloglou</keyname><forenames>Vasileios</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Iida</keyname><forenames>Masayo</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Luk</keyname><forenames>Steve K.</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Whitney</keyname><forenames>Richard</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Yamada</keyname><forenames>Kenji</forenames><affiliation>USC/Information Sciences Institute</affiliation></author></authors><title>Filling Knowledge Gaps in a Broad-Coverage Machine Translation System</title><categories>cmp-lg cs.CL</categories><comments>7 pages, Compressed and uuencoded postscript. To appear: IJCAI-95</comments><abstract>  Knowledge-based machine translation (KBMT) techniques yield high quality in
domains with detailed semantic models, limited vocabulary, and controlled input
grammar. Scaling up along these dimensions means acquiring large knowledge
resources. It also means behaving reasonably when definitive knowledge is not
yet available. This paper describes how we can fill various KBMT knowledge
gaps, often using robust statistical techniques. We describe quantitative and
qualitative results from JAPANGLOSS, a broad-coverage Japanese-English MT
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506010</id><created>1995-06-09</created><authors><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/Information Sciences Institute</affiliation></author><author><keyname>Hatzivassiloglou</keyname><forenames>Vasileios</forenames><affiliation>Columbia University</affiliation></author></authors><title>Two-level, Many-Paths Generation</title><categories>cmp-lg cs.CL</categories><comments>9 pages, Compressed and uuencoded postscript. To appear: ACL-95</comments><abstract>  Large-scale natural language generation requires the integration of vast
amounts of knowledge: lexical, grammatical, and conceptual. A robust generator
must be able to operate well even when pieces of knowledge are missing. It must
also be robust against incomplete or inaccurate inputs. To attack these
problems, we have built a hybrid generator, in which gaps in symbolic knowledge
are filled by statistical methods. We describe algorithms and show experimental
results. We also discuss how the hybrid generation model can be used to
simplify current generators and enhance their portability, even when perfect
knowledge is in principle obtainable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506011</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506011</id><created>1995-06-09</created><authors><author><keyname>Hatzivassiloglou</keyname><forenames>Vasileios</forenames><affiliation>Columbia University</affiliation></author><author><keyname>Knight</keyname><forenames>Kevin</forenames><affiliation>USC/Information Sciences Institute</affiliation></author></authors><title>Unification-Based Glossing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Compressed and uuencoded postscript. To appear: IJCAI-95</comments><abstract>  We present an approach to syntax-based machine translation that combines
unification-style interpretation with statistical processing. This approach
enables us to translate any Japanese newspaper article into English, with
quality far better than a word-for-word translation. Novel ideas include the
use of feature structures to encode word lattices and the use of unification to
compose and manipulate lattices. Unification also allows us to specify abstract
features that delay target-language synthesis until enough source-language
information is assembled. Our statistical component enables us to search
efficiently among competing translations and locate those with high English
fluency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506012</id><created>1995-06-10</created><authors><author><keyname>White</keyname><forenames>Michael</forenames><affiliation>CoGenTex, Inc.</affiliation></author></authors><title>Presenting Punctuation</title><categories>cmp-lg cs.CL</categories><comments>compressed uuencoded PostScript, 19 pages; Word 6.0 doc available
  upon request from mike@cogentex.com</comments><journal-ref>In Proceedings of the Fifth European Workshop on Natural Language
  Generation, Leiden, The Netherlands, pp. 107--125.</journal-ref><abstract>  Until recently, punctuation has received very little attention in the
linguistics and computational linguistics literature. Since the publication of
Nunberg's (1990) monograph on the topic, however, punctuation has seen its
stock begin to rise: spurred in part by Nunberg's ground-breaking work, a
number of valuable inquiries have been subsequently undertaken, including Hovy
and Arens (1991), Dale (1991), Pascual (1993), Jones (1994), and Briscoe
(1994). Continuing this line of research, I investigate in this paper how
Nunberg's approach to presenting punctuation (and other formatting devices)
might be incorporated into NLG systems. Insofar as the present paper focuses on
the proper syntactic treatment of punctuation, it differs from these other
subsequent works in that it is the first to examine this issue from the
generation perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506013</id><created>1995-06-13</created><authors><author><keyname>Estival</keyname><forenames>Dominique</forenames><affiliation>ISSCO, Universite de Geneve</affiliation></author><author><keyname>Gayral</keyname><forenames>Francoise</forenames><affiliation>LIPN, Universite Paris-Nord</affiliation></author></authors><title>A Study of the Context(s) in a Specific Type of Texts: Car Accident
  Reports</title><categories>cmp-lg cs.CL</categories><comments>9 pages, in Proceedings of the Workshop on &quot;Context in Natural
  Language Processing&quot;, IJCAI'95, Montreal. requires `ijcai89.sty', `named.sty'
  and `named.bst'</comments><abstract>  This paper addresses the issue of defining context, and more specifically the
different contexts needed for understanding a particular type of texts. The
corpus chosen is homogeneous and allows us to determine characteristic
properties of the texts from which certain inferences can be drawn by the
reader. These characteristic properties come from the real world domain
(K-context), the type of events the texts describe (F-context) and the genre of
the texts (E-context). Together, these three contexts provide elements for the
resolution of anaphoric expressions and for several types of disambiguation. We
show in particular that the argumentation aspect of these texts is an essential
part of the context and explains some of the inferences that can be drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506014</id><created>1995-06-13</created><authors><author><keyname>Della Pietra</keyname><forenames>S.</forenames><affiliation>IBM</affiliation></author><author><keyname>Della Pietra</keyname><forenames>V.</forenames><affiliation>IBM</affiliation></author><author><keyname>Lafferty</keyname><forenames>J.</forenames><affiliation>CMU</affiliation></author></authors><title>Inducing Features of Random Fields</title><categories>cmp-lg cs.CL</categories><comments>34 pages, compressed postscript</comments><report-no>CMU-CS-95-144</report-no><abstract>  We present a technique for constructing random fields from a set of training
samples. The learning paradigm builds increasingly complex fields by allowing
potential functions, or features, that are supported by increasingly large
subgraphs. Each feature has a weight that is trained by minimizing the
Kullback-Leibler divergence between the model and the empirical distribution of
the training data. A greedy algorithm determines how features are incrementally
added to the field and an iterative scaling algorithm is used to estimate the
optimal values of the weights.
 The statistical modeling techniques introduced in this paper differ from those
common to much of the natural language processing literature since there is no
probabilistic finite state or push-down automaton on which the model is built.
Our approach also differs from the techniques common to the computer vision
literature in that the underlying random fields are non-Markovian and have a
large number of parameters that must be estimated. Relations to other learning
approaches including decision trees and Boltzmann machines are given. As a
demonstration of the method, we describe its application to the problem of
automatic word classification in natural language processing.
 Key words: random field, Kullback-Leibler divergence, iterative scaling,
divergence geometry, maximum entropy, EM algorithm, statistical learning,
clustering, word morphology, natural language processing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506015</id><created>1995-06-14</created><updated>1995-06-20</updated><authors><author><keyname>Vanderwende</keyname><forenames>Lucy</forenames></author></authors><title>Ambiguity in the Acquisition of Lexical Information</title><categories>cmp-lg cs.CL</categories><abstract>  This paper describes an approach to the automatic identification of lexical
information in on-line dictionaries. This approach uses bootstrapping
techniques, specifically so that ambiguity in the dictionary text can be
treated properly. This approach consists of processing an on-line dictionary
multiple times, each time refining the lexical information previously acquired
and identifying new lexical information. The strength of this approach is that
lexical information can be acquired from definitions which are syntactically
ambiguous, given that information acquired during the first pass can be used to
improve the syntactic analysis of definitions in subsequent passes. In the
context of a lexical knowledge base, the types of lexical information that need
to be represented cannot be viewed as a fixed set, but rather as a set that
will change given the resources of the lexical knowledge base and the
requirements of analysis systems which access it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506016</id><created>1995-06-14</created><authors><author><keyname>Kameyama</keyname><forenames>Megumi</forenames><affiliation>AI Center and CSLI, SRI International</affiliation></author></authors><title>Indefeasible Semantics and Defeasible Pragmatics</title><categories>cmp-lg cs.CL</categories><comments>29 pages, self-contained LaTeX source. To appear in Kanazawa, M., C.
  Pinon, and H. de Swart, eds., Quantifiers, Deduction, and Context. Stanford,
  CA: CSLI</comments><report-no>This is a revised and shorter version of CWI Report CS-R9441 and SRI
  Technical Note 544, 1994 (these reports contain an additional section on
  prioritized circumscription -- available from web page
  http://www.ai.sri.com/~megumi/ )</report-no><abstract>  An account of utterance interpretation in discourse needs to face the issue
of how the discourse context controls the space of interacting preferences.
Assuming a discourse processing architecture that distinguishes the grammar and
pragmatics subsystems in terms of monotonic and nonmonotonic inferences, I will
discuss how independently motivated default preferences interact in the
interpretation of intersentential pronominal anaphora. In the framework of a
general discourse processing model that integrates both the grammar and
pragmatics subsystems, I will propose a fine structure of the preferential
interpretation in pragmatics in terms of defeasible rule interactions. The
pronoun interpretation preferences that serve as the empirical ground draw from
the survey data specifically obtained for the present purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506017</id><created>1995-06-18</created><authors><author><keyname>Cahn</keyname><forenames>Janet</forenames><affiliation>Massachusetts Institute of Technology</affiliation></author></authors><title>The Effect of Pitch Accenting on Pronoun Referent Resolution</title><categories>cmp-lg cs.CL</categories><comments>3 pages, uses aclap.sty. In Proceedings of the ACL, 1995</comments><journal-ref>Proceedings of the ACL, 1995.</journal-ref><abstract>  By strictest interpretation, theories of both centering and intonational
meaning fail to predict the existence of pitch accented pronominals. Yet they
occur felicitously in spoken discourse. To explain this, I emphasize the dual
functions served by pitch accents, as markers of both propositional
(semantic/pragmatic) and attentional salience. This distinction underlies my
proposals about the attentional consequences of pitch accents when applied to
pronominals, in particular, that while most pitch accents may weaken or
reinforce a cospecifier's status as the center of attention, a contrastively
stressed pronominal may force a shift, even when contraindicated by textual
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506018</id><created>1995-06-21</created><authors><author><keyname>Vaillant</keyname><forenames>Pascal</forenames><affiliation>Thomson-CSF LCR</affiliation></author><author><keyname>Checler</keyname><forenames>Michael</forenames><affiliation>ESIEA / Thomson-CSF LCR</affiliation></author></authors><title>Intelligent Voice Prosthesis: Converting Icons into Natural Language
  Sentences</title><categories>cmp-lg cs.CL</categories><comments>Montpellier'95. 11 pages, 6 Encapsulated Postscript figures, uses
  a4.sty and epsf.sty</comments><abstract>  The Intelligent Voice Prosthesis is a communication tool which reconstructs
the meaning of an ill-structured sequence of icons or symbols, and expresses
this meaning into sentences of a Natural Language (French). It has been
developed for the use of people who cannot express themselves orally in natural
language, and further, who are not able to comply to grammatical rules such as
those of natural language. We describe how available corpora of iconic
communication by children with Cerebral Palsy has led us to implement a simple
and relevant semantic description of the symbol lexicon. We then show how a
unification-based, bottom-up semantic analysis allows the system to uncover the
meaning of the user's utterances by computing proper dependencies between the
symbols. The result of the analysis is then passed to a lexicalization module
which chooses the right words of natural language to use, and builds a
linguistic semantic network. This semantic network is then generated into
French sentences via hierarchization into trees, using a lexicalized Tree
Adjoining Grammar. Finally we describe the modular, customizable interface
which has been developed for this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506019</id><created>1995-06-21</created><authors><author><keyname>Magerman</keyname><forenames>David M.</forenames></author></authors><title>Review of Charniak's &quot;Statistical Language Learning&quot;</title><categories>cmp-lg cs.CL</categories><journal-ref>Computational Linguistics 21:1, 103-111</journal-ref><abstract>  This article is an in-depth review of Eugene Charniak's book, &quot;Statistical
Language Learning&quot;. The review evaluates the appropriateness of the book as
an introductory text for statistical language learning for a variety of
audiences. It also includes an extensive bibliography of articles and papers
which might be used as a supplement to this book for learning or teaching
statistical language modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506020</id><created>1995-06-22</created><authors><author><keyname>Staab</keyname><forenames>Steffen</forenames><affiliation>Universitaet Erlangen-Nuernberg, IMMD 8</affiliation></author></authors><title>GLR-Parsing of Word Lattices Using a Beam Search Method</title><categories>cmp-lg cs.CL</categories><comments>4 pages, 61K postscript, compressed, uuencoded, Eurospeech 9/95,
  Madrid</comments><abstract>  This paper presents an approach that allows the efficient integration of
speech recognition and language understanding using Tomita's generalized
LR-parsing algorithm. For this purpose the GLRP-algorithm is revised so that an
agenda mechanism can be used to control the flow of computation of the parsing
process. This new approach is used to integrate speech recognition and speech
understanding incrementally with a beam search method. These considerations
have been implemented and tested on ten word lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506021</id><created>1995-06-22</created><authors><author><keyname>Collins</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Brooks</keyname><forenames>James</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>Prepositional Phrase Attachment through a Backed-Off Model</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of the Third Workshop on Very Large Corpora,
  12 pages, LaTeX</comments><abstract>  Recent work has considered corpus-based or statistical approaches to the
problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb
phrases of the form {v np1 p np2} are resolved through a model which considers
values of the four head words (v, n1, p and n2). This paper shows that the
problem is analogous to n-gram language models in speech recognition, and that
one of the most common methods for language modeling, the backed-off estimate,
is applicable. Results on Wall Street Journal data of 84.5% accuracy are
obtained using this method. A surprising result is the importance of low-count
events - ignoring events which occur less than 5 times in training data reduces
performance to 81.6%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506022</id><created>1995-06-23</created><updated>1995-06-24</updated><authors><author><keyname>Ansari</keyname><forenames>Daniel</forenames><affiliation>University of Toronto</affiliation></author></authors><title>Deriving Procedural and Warning Instructions from Device and Environment
  Models</title><categories>cmp-lg cs.CL</categories><comments>63 pages, uses authdate.sty</comments><report-no>CSRI-329</report-no><abstract>  This study is centred on the generation of instructions for household
appliances. We show how knowledge about a device, together with knowledge about
the environment, can be used for reasoning about instructions. The information
communicated by the instructions can be planned from a version of the knowledge
of the artifact and environment. We present the latter, which we call the {\it
planning knowledge}, in the form of axioms in the {\it situation calculus}.
This planning knowledge formally characterizes the behaviour of the artifact,
and it is used to produce a basic plan of actions that the device and user take
to accomplish a given goal. We explain how both procedural and warning
instructions can be generated from this basic plan.
 In order to partially justify that instruction generation can be automated
from a formal device design specification, we assume that the planning
knowledge is {\it derivable\/} from the device and world knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506023</id><created>1995-06-26</created><authors><author><keyname>Pericliev</keyname><forenames>Vladimir</forenames><affiliation>Institute of Mathematics, bl.8, 1113 Sofia, Bulgaria</affiliation></author></authors><title>Empirical Discovery in Linguistics</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX, in Working Notes of AAAI Spring Symposium Series,
  Symposium: Systematic Methods of Scientific Discovery, March 27-29, Stanford
  University</comments><abstract>  A discovery system for detecting correspondences in data is described, based
on the familiar induction methods of J. S. Mill. Given a set of observations,
the system induces the ``causally'' related facts in these observations. Its
application to empirical linguistic discovery is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506024</id><created>1995-06-28</created><updated>1995-06-29</updated><authors><author><keyname>Thielen</keyname><forenames>Christine</forenames><affiliation>SfS, University of T&quot;ubingen</affiliation></author></authors><title>An Approach to Proper Name Tagging for German</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, 2 uuencoded tar-compressed eps-figures added,
  EACL-SIGDAT 95</comments><abstract>  This paper presents an incremental method for the tagging of proper names in
German newspaper texts. The tagging is performed by the analysis of the
syntactic and textual contexts of proper names together with a morphological
analysis. The proper names selected by this process supply new contexts which
can be used for finding new proper names, and so on. This procedure was applied
to a small German corpus (50,000 words) and correctly disambiguated 65% of the
capitalized words, which should improve when it is applied to a very large
corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506025</id><created>1995-06-29</created><authors><author><keyname>Bozsahin</keyname><forenames>Cem</forenames><affiliation>Middle East Technical University</affiliation></author><author><keyname>Gocmen</keyname><forenames>Elvan</forenames><affiliation>Middle East Technical University</affiliation></author></authors><title>A Categorial Framework for Composition in Multiple Linguistic Domains</title><categories>cmp-lg cs.CL</categories><comments>7 pages LaTeX, CSNLP-95 (Dublin), uses {a4,avm,lingmacros}.sty</comments><journal-ref>Proceedings of the Fourth Int Conf on Cognitive Science of NLP,</journal-ref><abstract>  This paper describes a computational framework for a grammar architecture in
which different linguistic domains such as morphology, syntax, and semantics
are treated not as separate components but compositional domains. Word and
phrase formation are modeled as uniform processes contributing to the
derivation of the semantic form. The morpheme, as well as the lexeme, has
lexical representation in the form of semantic content, tactical constraints,
and phonological realization. The motivation for this work is to handle
morphology-syntax interaction (e.g., valency change in causatives,
subcategorization imposed by case-marking affixes) in an incremental way. The
model is based on Combinatory Categorial Grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9506026</id><created>1995-07-01</created><authors><author><keyname>White</keyname><forenames>Michael</forenames><affiliation>CoGenTex, Inc.</affiliation></author></authors><title>A Computational Approach to Aspectual Composition</title><categories>cmp-lg cs.CL</categories><comments>15 pages</comments><journal-ref>In Workshop Notes of the 5th International Workshop TSM 95 (Time,
  Space and Movement: Meaning and Knowledge in the Sensible World), Chateau de
  Bonas, Gascony, France, 23-27 June 1995.</journal-ref><abstract>  In this paper, I argue, contrary to the prevailing opinion in the linguistics
and philosophy literature, that a sortal approach to aspectual composition can
indeed be explanatory. In support of this view, I develop a synthesis of
competing proposals by Hinrichs, Krifka and Jackendoff which takes Jackendoff's
cross-cutting sortal distinctions as its point of departure. To show that the
account is well-suited for computational purposes, I also sketch an implemented
calculus of eventualities which yields many of the desired inferences. Further
details on both the model-theoretic semantics and the implementation can be
found in (White, 1994).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507001</id><created>1995-07-04</created><authors><author><keyname>Damas</keyname><forenames>Luis</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author></authors><title>Constraint Categorial Grammars</title><categories>cmp-lg cs.CL</categories><comments>12 pages, 5 Postscript figures, uses llncs.sty and epsfig.sty. To
  appear in Proceedings of EPIA'95, 7th Portuguese Conference on Artificial
  Intelligence, Funchal- Madeira Island, Portugal 3-6 October , 1995</comments><abstract>  Although unification can be used to implement a weak form of
$\beta$-reduction, several linguistic phenomena are better handled by using
some form of $\lambda$-calculus. In this paper we present a higher order
feature description calculus based on a typed $\lambda$-calculus. We show how
the techniques used in \CLG for resolving complex feature constraints can be
efficiently extended. \CCLG is a simple formalism, based on categorial
grammars, designed to test the practical feasibility of such a calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507002</id><created>1995-07-06</created><authors><author><keyname>Go&#xf1;i</keyname><forenames>Jos&#xe9; M.</forenames><affiliation>E.T.S.I. Telecomunicaci&#xf3;n, Universidad Polit&#xe9;cnica de Madrid, Madrid, Spain</affiliation></author><author><keyname>Gonz&#xe1;lez</keyname><forenames>Jos&#xe9; C.</forenames><affiliation>E.T.S.I. Telecomunicaci&#xf3;n, Universidad Polit&#xe9;cnica de Madrid, Madrid, Spain</affiliation></author></authors><title>A framework for lexical representation</title><categories>cmp-lg cs.CL</categories><comments>9 pages</comments><report-no>UPM/DIT/GSI 18/95</report-no><journal-ref>AI95: 15th International Conference. Language Engineering 95
  (Montpellier, France), pp. 243-252.</journal-ref><abstract>  In this paper we present a unification-based lexical platform designed for
highly inflected languages (like Roman ones). A formalism is proposed for
encoding a lemma-based lexical source, well suited for linguistic
generalizations. From this source, we automatically generate an allomorph
indexed dictionary, adequate for efficient processing. A set of software tools
have been implemented around this formalism: access libraries, morphological
processors, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507003</id><created>1995-07-13</created><updated>1995-07-14</updated><authors><author><keyname>Menzel</keyname><forenames>Wolfgang</forenames><affiliation>University of Hamburg</affiliation></author></authors><title>Robust Processing of Natural Language</title><categories>cmp-lg cs.CL</categories><comments>16 pages, LaTeX, uses pstricks.sty, pstricks.tex, pstricks.pro,
  pst-node.sty, pst-node.tex, pst-node.pro. To appear in: Proc. KI-95, 19th
  German Conference on Artificial Intelligence, Bielefeld (Germany), Lecture
  Notes in Computer Science, Springer 1995</comments><abstract>  Previous approaches to robustness in natural language processing usually
treat deviant input by relaxing grammatical constraints whenever a successful
analysis cannot be provided by ``normal'' means. This schema implies, that
error detection always comes prior to error handling, a behaviour which hardly
can compete with its human model, where many erroneous situations are treated
without even noticing them.
 The paper analyses the necessary preconditions for achieving a higher degree
of robustness in natural language processing and suggests a quite different
approach based on a procedure for structural disambiguation. It not only offers
the possibility to cope with robustness issues in a more natural way but
eventually might be suited to accommodate quite different aspects of robust
behaviour within a single framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507004</id><created>1995-07-18</created><authors><author><keyname>Moreno</keyname><forenames>Antonio</forenames><affiliation>Universidad Aut&#xf3;noma de Madrid, Madrid, Spain</affiliation></author><author><keyname>Go&#xf1;i</keyname><forenames>Jos&#xe9; M.</forenames><affiliation>Universidad Polit&#xe9;cnica de Madrid, Madrid, Spain</affiliation></author></authors><title>GRAMPAL: A Morphological Processor for Spanish implemented in Prolog</title><categories>cmp-lg cs.CL</categories><comments>11 pages</comments><journal-ref>GULP-PRODE95: Joint Conference on Declarative Programming, Marina
  di Vietri, Salerno (Italy). September, 1995</journal-ref><abstract>  A model for the full treatment of Spanish inflection for verbs, nouns and
adjectives is presented. This model is based on feature unification and it
relies upon a lexicon of allomorphs both for stems and morphemes. Word forms
are built by the concatenation of allomorphs by means of special contextual
features. We make use of standard Definite Clause Grammars (DCG) included in
most Prolog implementations, instead of the typical finite-state approach. This
allows us to take advantage of the declarativity and bidirectionality of Logic
Programming for NLP.
 The most salient feature of this approach is simplicity: A really
straightforward rule and lexical components. We have developed a very simple
model for complex phenomena.
 Declarativity, bidirectionality, consistency and completeness of the model are
discussed: all and only correct word forms are analysed or generated, even
alternative ones and gaps in paradigms are preserved. A Prolog implementation
has been developed for both analysis and generation of Spanish word forms. It
consists of only six DCG rules, because our {\em lexicalist\/} approach --i.e.
most information is in the dictionary. Although it is quite efficient, the
current implementation could be improved for analysis by using the non logical
features of Prolog, especially in word segmentation and dictionary access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507005</id><created>1995-07-19</created><authors><author><keyname>Lerner</keyname><forenames>Jan</forenames><affiliation>University of the Saarland, Dept. of Computational Linguistics</affiliation></author><author><keyname>Pinkal</keyname><forenames>Manfred</forenames><affiliation>University of the Saarland, Dept. of Computational Linguistics</affiliation></author></authors><title>Comparative Ellipsis and Variable Binding</title><categories>cmp-lg cs.CL</categories><comments>Postscript, 15 pages</comments><report-no>CLAUS 64</report-no><journal-ref>to appear in SALT V Proceedings, Cornell University,</journal-ref><abstract>  In this paper, we discuss the question whether phrasal comparatives should be
given a direct interpretation, or require an analysis as elliptic
constructions, and answer it with Yes and No. The most adequate analysis of
wide reading attributive (WRA) comparatives seems to be as cases of ellipsis,
while a direct (but asymmetric) analysis fits the data for narrow scope
attributive comparatives. The question whether it is a syntactic or a semantic
process which provides the missing linguistic material in the complement of WRA
comparatives is also given a complex answer: Linguistic context is accessed by
combining a reconstruction operation and a mechanism of anaphoric reference.
The analysis makes only few and straightforward syntactic assumptions. In part,
this is made possible because the use of Generalized Functional Application as
a semantic operation allows us to model semantic composition in a flexible way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507006</id><created>1995-07-20</created><authors><author><keyname>Gasser</keyname><forenames>Michael</forenames><affiliation>Indiana University</affiliation></author></authors><title>Transfer in a Connectionist Model of the Acquisition of Morphology</title><categories>cmp-lg cs.CL</categories><comments>21 pages, uuencoded compressed Postscript</comments><report-no>IU Cognitive Science TR 147</report-no><abstract>  The morphological systems of natural languages are replete with examples of
the same devices used for multiple purposes: (1) the same type of morphological
process (for example, suffixation for both noun case and verb tense) and (2)
identical morphemes (for example, the same suffix for English noun plural and
possessive). These sorts of similarity would be expected to convey advantages
on language learners in the form of transfer from one morphological category to
another. Connectionist models of morphology acquisition have been faulted for
their supposed inability to represent phonological similarity across
morphological categories and hence to facilitate transfer. This paper describes
a connectionist model of the acquisition of morphology which is shown to
exhibit transfer of this type. The model treats the morphology acquisition
problem as one of learning to map forms onto meanings and vice versa. As the
network learns these mappings, it makes phonological generalizations which are
embedded in connection weights. Since these weights are shared by different
morphological categories, transfer is enabled. In a set of experiments with
artificial stimuli, networks were trained first on one morphological task
(e.g., tense) and then on a second (e.g., number). It is shown that in the
context of suffixation, prefixation, and template rules, the second task is
facilitated when the second category either makes use of the same forms or the
same general process type (e.g., prefixation) as the first.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507007</id><created>1995-07-21</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>University of the Saarland</affiliation></author></authors><title>An Efficient Algorithm for Surface Generation</title><categories>cmp-lg cs.CL</categories><comments>Uuencoded compressed PostScript format</comments><report-no>CLAUS 44 Technical Report</report-no><journal-ref>IJCAI 95</journal-ref><abstract>  A method is given that &quot;inverts&quot; a logic grammar and displays it from the
point of view of the logical form, rather than from that of the word string.
LR-compiling techniques are used to allow a recursive-descent generation
algorithm to perform &quot;functor merging&quot; much in the same way as an LR parser
performs prefix merging.
 This is an improvement on the semantic-head-driven generator that results in a
much smaller search space. The amount of semantic lookahead can be varied, and
appropriate tradeoff points between table size and resulting nondeterminism can
be found automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507008</id><created>1995-07-21</created><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Department of Computer Engineering and Information Science, Bilkent University, Ankara Turkey</affiliation></author><author><keyname>Yilmaz</keyname><forenames>Okan</forenames><affiliation>Department of Computer Engineering and Information Science, Bilkent University, Ankara Turkey</affiliation></author></authors><title>A Constraint-based Case Frame Lexicon Architecture</title><categories>cmp-lg cs.CL</categories><comments>gzipped, uuencoded postscipt file, 11 pages. To be presented at the
  ESSLLI Workshop -- The Computational Lexicon. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1995/BU-CEIS-9511.ps.z</comments><abstract>  In Turkish, (and possibly in many other languages) verbs often convey several
meanings (some totally unrelated) when they are used with subjects, objects,
oblique objects, adverbial adjuncts, with certain lexical, morphological, and
semantic features, and co-occurrence restrictions. In addition to the usual
sense variations due to selectional restrictions on verbal arguments, in most
cases, the meaning conveyed by a case frame is idiomatic and not compositional,
with subtle constraints. In this paper, we present an approach to building a
constraint-based case frame lexicon for use in natural language processing in
Turkish, whose prototype we have implemented under the TFS system developed at
Univ. of Stuttgart.
 A number of observations that we have made on Turkish have indicated that we
need something beyond the traditional transitive and intransitive distinction,
and utilize a framework where verb valence is considered as the obligatory
co-existence of an arbitrary subset of possible arguments along with the
obligatory exclusion of certain others, relative to a verb sense. Additional
morphological lexical and semantic constraints on the syntactic constituents
organized as a 5-tier constraint hierarchy, are utilized to map a given
syntactic structure case-fraame to a specific verb sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507009</id><created>1995-07-21</created><authors><author><keyname>Fuchs</keyname><forenames>Norbert E.</forenames><affiliation>Department of Computer Science, University of Zurich</affiliation></author><author><keyname>Schwitter</keyname><forenames>Rolf</forenames><affiliation>Department of Computer Science, University of Zurich</affiliation></author></authors><title>Specifying Logic Programs in Controlled Natural Language</title><categories>cmp-lg cs.CL</categories><comments>16 pages, compressed, uuencoded Postscript, published in Proceedings
  CLNLP 95, COMPULOGNET/ELSNET/EAGLES Workshop on Computational Logic for
  Natural Language Processing, Edinburgh, April 3-5, 1995</comments><report-no>IFI 95.17</report-no><journal-ref>Proceedings CLNLP 95, COMPULOGNET/ELSNET/EAGLES</journal-ref><abstract>  Writing specifications for computer programs is not easy since one has to
take into account the disparate conceptual worlds of the application domain and
of software development. To bridge this conceptual gap we propose controlled
natural language as a declarative and application-specific specification
language. Controlled natural language is a subset of natural language that can
be accurately and efficiently processed by a computer, but is expressive enough
to allow natural usage by non-specialists. Specifications in controlled natural
language are automatically translated into Prolog clauses, hence become formal
and executable. The translation uses a definite clause grammar (DCG) enhanced
by feature structures. Inter-text references of the specification, e.g.
anaphora, are resolved with the help of discourse representation theory (DRT).
The generated Prolog clauses are added to a knowledge base. We have implemented
a prototypical specification system that successfully processes the
specification of a simple automated teller machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507010</id><created>1995-07-24</created><updated>1995-07-24</updated><authors><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author><author><keyname>Nakamura</keyname><forenames>Atsuyoshi</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author></authors><title>On-line Learning of Binary Lexical Relations Using Two-dimensional
  Weighted Majority Algorithms</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uuencoded compressed postscript</comments><journal-ref>Proc. of The 12th Int. Conf. on Machine Learning, 1995</journal-ref><abstract>  We consider the problem of learning a certain type of lexical semantic
knowledge that can be expressed as a binary relation between words, such as the
so-called sub-categorization of verbs (a verb-noun relation) and the compound
noun phrase relation (a noun-noun relation). Specifically, we view this problem
as an on-line learning problem in the sense of Littlestone's learning model in
which the learner's goal is to minimize the total number of prediction
mistakes. In the computational learning theory literature, Goldman, Rivest and
Schapire and subsequently Goldman and Warmuth have considered the on-line
learning problem for binary relations R : X * Y -&gt; {0, 1} in which one of the
domain sets X can be partitioned into a relatively small number of types,
namely clusters consisting of behaviorally indistinguishable members of X. In
this paper, we extend this model and suppose that both of the sets X, Y can be
partitioned into a small number of types, and propose a host of prediction
algorithms which are two-dimensional extensions of Goldman and Warmuth's
weighted majority type algorithm proposed for the original model. We apply
these algorithms to the learning problem for the `compound noun phrase'
relation, in which a noun is related to another just in case they can form a
noun phrase together. Our experimental results show that all of our algorithms
out-perform Goldman and Warmuth's algorithm. We also theoretically analyze the
performance of one of our algorithms, in the form of an upper bound on the
worst case number of prediction mistakes it makes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507011</id><created>1995-07-24</created><updated>1996-03-13</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>C&amp;C Res. Labs.,NEC</affiliation></author><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>C&amp;C Res. Labs.,NEC</affiliation></author></authors><title>Generalizing Case Frames Using a Thesaurus and the MDL Principle</title><categories>cmp-lg cs.CL</categories><comments>11 pages, uuencoded compressed postscript, a revised version</comments><journal-ref>Proc. of Recent Advances in Natural Language Processing, 239-248,
  1995.</journal-ref><abstract>  We address the problem of automatically acquiring case-frame patterns from
large corpus data. In particular, we view this problem as the problem of
estimating a (conditional) distribution over a partition of words, and propose
a new generalization method based on the MDL (Minimum Description Length)
principle. In order to assist with the efficiency, our method makes use of an
existing thesaurus and restricts its attention on those partitions that are
present as `cuts' in the thesaurus tree, thus reducing the generalization
problem to that of estimating the `tree cut models' of the thesaurus. We then
give an efficient algorithm which provably obtains the optimal tree cut model
for the given frequency data, in the sense of MDL. We have used the case-frame
patterns obtained using our method to resolve pp-attachment ambiguity.Our
experimental results indicate that our method improves upon or is at least as
effective as existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507012</id><created>1995-07-24</created><authors><author><keyname>Burheim</keyname><forenames>Tore</forenames><affiliation>University of Bergen</affiliation></author></authors><title>A Grammar Formalism and Cross-Serial Dependencies</title><categories>cmp-lg cs.CL</categories><comments>19 pages uuencodet gnu-compressed PostScript format. A previous
  version of this paper is printed in the proceedings from the joint
  ELSNET/COMPULOG-NET/EAGLES workshop Computational Logic for Natural Language
  Processing (CLNLP95) in Edinburgh in April 1995</comments><abstract>  First we define a unification grammar formalism called the Tree Homomorphic
Feature Structure Grammar. It is based on Lexical Functional Grammar (LFG), but
has a strong restriction on the syntax of the equations. We then show that this
grammar formalism defines a full abstract family of languages, and that it is
capable of describing cross-serial dependencies of the type found in Swiss
German.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507013</id><created>1995-07-24</created><authors><author><keyname>Burheim</keyname><forenames>Tore</forenames><affiliation>University of Bergen</affiliation></author></authors><title>Indexed Languages and Unification Grammars</title><categories>cmp-lg cs.CL</categories><comments>16 pages uuencodet gnu-compressed PostScript format. Also in
  Proceedings of the 10th Nordic Conference of Computational Linguistics,
  NODALIDA-95, Helsinki, 1995</comments><abstract>  Indexed languages are interesting in computational linguistics because they
are the least class of languages in the Chomsky hierarchy that has not been
shown not to be adequate to describe the string set of natural language
sentences. We here define a class of unification grammars that exactly describe
the class of indexed languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9507014</id><created>1995-07-25</created><authors><author><keyname>Reyle</keyname><forenames>Uwe</forenames><affiliation>Institute for Computational Linguistics, University of Stuttgart</affiliation></author></authors><title>Co-Indexing Labelled DRSs to Represent and Reason with Ambiguities</title><categories>cmp-lg cs.CL</categories><comments>gzipped ps-file. To appear in: Stanley Peters, Kees van Deemter
  (1995): Semantic Ambiguity and Underspecification, CSLI Publications,
  Stanford</comments><abstract>  The paper addresses the problem of representing ambiguities in a way that
allows for monotonic disambiguation and for direct deductive computation. The
paper focuses on an extension of the formalism of underspecified DRSs to
ambiguities introduced by plural NPs. It deals with the collective/distributive
distinction, and also with generic and cumulative readings. In addition it
provides a systematic account for an underspecified treatment of plural pronoun
resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508001</id><created>1995-08-02</created><authors><author><keyname>Bos</keyname><forenames>Johan</forenames><affiliation>University of the Saarland, Germany</affiliation></author><author><keyname>Buitelaar</keyname><forenames>Paul</forenames><affiliation>Brandeis University, USA</affiliation></author><author><keyname>Mineur</keyname><forenames>Anne-Marie</forenames><affiliation>University of the Saarland, Germany</affiliation></author></authors><title>Bridging as Coercive Accommodation</title><categories>cmp-lg cs.CL</categories><comments>LaTeX file, 16 pages, uses named.sty. Paper presented at CLNLP
  workshop, Edinburgh, April 3-5, 1995</comments><report-no>CLAUS 52 Technical Report</report-no><abstract>  In this paper we discuss the notion of &quot;bridging&quot; in Discourse Representation
Theory as a tool to account for discourse referents that have only been
established implicitly, through the lexical semantics of other referents. In
doing so, we use ideas from Generative Lexicon theory, to introduce antecedents
for anaphoric expressions that cannot be &quot;linked&quot; to a proper antecedent, but
that do not need to be &quot;accommodated&quot; because they have some connection to the
network of discourse referents that is already established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508002</id><created>1995-08-02</created><updated>1995-09-10</updated><authors><author><keyname>Mineur</keyname><forenames>Anne-Marie</forenames><affiliation>University of the Saarland, Germany</affiliation></author><author><keyname>Buitelaar</keyname><forenames>Paul</forenames><affiliation>Brandeis University, USA</affiliation></author></authors><title>A Compositional Treatment of Polysemous Arguments in Categorial Grammar</title><categories>cmp-lg cs.CL</categories><comments>LaTeX file, 19 pages, uses pubsmacs, pubsbib, pubsarticle, leqno</comments><report-no>CLAUS 49 Technical Report</report-no><abstract>  We discuss an extension of the standard logical rules (functional application
and abstraction) in Categorial Grammar (CG), in order to deal with some
specific cases of polysemy. We borrow from Generative Lexicon theory which
proposes the mechanism of {\em coercion}, next to a rich nominal lexical
semantic structure called {\em qualia structure}.
 In a previous paper we introduced coercion into the framework of {\em
sign-based} Categorial Grammar and investigated its impact on traditional
Fregean compositionality. In this paper we will elaborate on this idea, mostly
working towards the introduction of a new semantic dimension. Where in current
versions of sign-based Categorial Grammar only two representations are derived:
a prosodic one (form) and a logical one (modelling), here we introduce also a
more detaled representation of the lexical semantics. This extra knowledge will
serve to account for linguistic phenomena like {\em metonymy\/}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508003</id><created>1995-08-02</created><authors><author><keyname>Grinberg</keyname><forenames>Dennis</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Lafferty</keyname><forenames>John</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Sleator</keyname><forenames>Daniel</forenames><affiliation>Carnegie Mellon</affiliation></author></authors><title>A Robust Parsing Algorithm For Link Grammars</title><categories>cmp-lg cs.CL</categories><comments>17 pages, compressed postscript</comments><report-no>CMU-CS-TR-95-125</report-no><abstract>  In this paper we present a robust parsing algorithm based on the link grammar
formalism for parsing natural languages. Our algorithm is a natural extension
of the original dynamic programming recognition algorithm which recursively
counts the number of linkages between two words in the input sentence. The
modified algorithm uses the notion of a null link in order to allow a
connection between any pair of adjacent words, regardless of their dictionary
definitions. The algorithm proceeds by making three dynamic programming passes.
In the first pass, the input is parsed using the original algorithm which
enforces the constraints on links to ensure grammaticality. In the second pass,
the total cost of each substring of words is computed, where cost is determined
by the number of null links necessary to parse the substring. The final pass
counts the total number of parses with minimal cost. All of the original
pruning techniques have natural counterparts in the robust algorithm. When used
together with memoization, these techniques enable the algorithm to run
efficiently with cubic worst-case complexity. We have implemented these ideas
and tested them by parsing the Switchboard corpus of conversational English.
This corpus is comprised of approximately three million words of text,
corresponding to more than 150 hours of transcribed speech collected from
telephone conversations restricted to 70 different topics. Although only a
small fraction of the sentences in this corpus are &quot;grammatical&quot; by standard
criteria, the robust link grammar parser is able to extract relevant structure
for a large portion of the sentences. We present the results of our experiments
using this system, including the analyses of selected and random sentences from
the corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508004</id><created>1995-08-02</created><authors><author><keyname>Sleator</keyname><forenames>Daniel D. K.</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Temperley</keyname><forenames>Davy</forenames><affiliation>Columbia University</affiliation></author></authors><title>Parsing English with a Link Grammar</title><categories>cmp-lg cs.CL</categories><comments>91 pages, compressed postscript</comments><report-no>CMU-CS-TR-91-126</report-no><abstract>  We develop a formal grammatical system called a link grammar, show how
English grammar can be encoded in such a system, and give algorithms for
efficiently parsing with a link grammar. Although the expressive power of link
grammars is equivalent to that of context free grammars, encoding natural
language grammars appears to be much easier with the new system. We have
written a program for general link parsing and written a link grammar for the
English language. The performance of this preliminary system -- both in the
breadth of English phenomena that it captures and in the computational
resources used -- indicates that the approach may have practical uses as well
as linguistic significance. Our program is written in C and may be obtained
through the internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508005</id><created>1995-08-10</created><authors><author><keyname>Cranias</keyname><forenames>Lambros</forenames><affiliation>Institute for Language and Speech Processing, Greece</affiliation></author><author><keyname>Papageorgiou</keyname><forenames>Harris</forenames><affiliation>Institute for Language and Speech Processing, Greece</affiliation></author><author><keyname>Piperidis</keyname><forenames>Stelios</forenames><affiliation>Institute for Language and Speech Processing, Greece</affiliation></author></authors><title>A Matching Technique in Example-Based Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>5 pages,LaTeX uses aclap.sty</comments><abstract>  This paper addresses an important problem in Example-Based Machine
Translation (EBMT), namely how to measure similarity between a sentence
fragment and a set of stored examples. A new method is proposed that measures
similarity according to both surface structure and content. A second
contribution is the use of clustering to make retrieval of the best matching
example from the database more efficient. Results on a large number of test
cases from the CELEX database are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508006</id><created>1995-08-12</created><authors><author><keyname>Trujillo</keyname><forenames>Arturo</forenames><affiliation>SCMS, The Robert Gordon University, Aberdeen</affiliation></author></authors><title>Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT</title><categories>cmp-lg cs.CL</categories><comments>21 pages; tarred, compressed, uuencoded Latex. In `Proceedings of the
  Sixth International Conference on Theoretical and Methodological Issues in
  Machine Translation', Leuven, Belgium, July 1995, pp. 48--66</comments><abstract>  The paper presents a prototype lexicalist Machine Translation system (based
on the so-called `Shake-and-Bake' approach of Whitelock (1992) consisting of an
analysis component, a dynamic bilingual lexicon, and a generation component,
and shows how it is applied to a range of MT problems. Multi-Lexeme
translations are handled through bi-lexical rules which map bilingual lexical
signs into new bilingual lexical signs. It is argued that much translation can
be handled by equating translationally equivalent lists of lexical signs,
either directly in the bilingual lexicon, or by deriving them through
bi-lexical rules. Lexical semantic information organized as Qualia structures
(Pustejovsky 1991) is used as a mechanism for restricting the domain of the
rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508007</id><created>1995-08-13</created><authors><author><keyname>Port</keyname><forenames>Robert</forenames><affiliation>Indiana University</affiliation></author><author><keyname>Cummins</keyname><forenames>Fred</forenames><affiliation>Indiana University</affiliation></author><author><keyname>Gasser</keyname><forenames>Michael</forenames><affiliation>Indiana University</affiliation></author></authors><title>A Dynamic Approach to Rhythm in Language: Toward a Temporal Phonology</title><categories>cmp-lg cs.CL</categories><comments>31 pages; compressed, uuencoded Postscript</comments><report-no>IU Cognitive Science TR 150</report-no><abstract>  It is proposed that the theory of dynamical systems offers appropriate tools
to model many phonological aspects of both speech production and perception. A
dynamic account of speech rhythm is shown to be useful for description of both
Japanese mora timing and English timing in a phrase repetition task. This
orientation contrasts fundamentally with the more familiar symbolic approach to
phonology, in which time is modeled only with sequentially arrayed symbols. It
is proposed that an adaptive oscillator offers a useful model for perceptual
entrainment (or `locking in') to the temporal patterns of speech production.
This helps to explain why speech is often perceived to be more regular than
experimental measurements seem to justify. Because dynamic models deal with
real time, they also help us understand how languages can differ in their
temporal detail---contributing to foreign accents, for example. The fact that
languages differ greatly in their temporal detail suggests that these effects
are not mere motor universals, but that dynamical models are intrinsic
components of the phonological characterization of language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508008</id><created>1995-08-15</created><authors><author><keyname>Doerre</keyname><forenames>Jochen</forenames></author><author><keyname>Manandhar</keyname><forenames>Suresh</forenames></author></authors><title>On Constraint-Based Lambek Calculi</title><categories>cmp-lg cs.CL</categories><comments>uuencoded gzipped ps file, 97k, 18 pages</comments><report-no>Tech. Report HCRC/TR-69, University of Edinburgh</report-no><abstract>  We explore the consequences of layering a Lambek proof system over an
arbitrary (constraint) logic. A simple model-theoretic semantics for our hybrid
language is provided for which a particularly simple combination of Lambek's
and the proof system of the base logic is complete. Furthermore the proof
system for the underlying base logic can be assumed to be a black box. The
essential reasoning needed to be performed by the black box is that of {\em
entailment checking}. Assuming feature logic as the base logic entailment
checking amounts to a {\em subsumption} test which is a well-known quasi-linear
time decidable problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508009</id><created>1995-08-15</created><authors><author><keyname>Luz-Filho</keyname><forenames>Saturnino F.</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author><author><keyname>Sturt</keyname><forenames>Patrick</forenames><affiliation>Centre for Cognitive Science, University of Edinburgh</affiliation></author></authors><title>A Labelled Analytic Theorem Proving Environment for Categorial Grammar</title><categories>cmp-lg cs.CL</categories><comments>11 pages, LaTeX2e, uses examples.sty and a4wide.sty</comments><journal-ref>To appear in the Proceedings of IWPT-95</journal-ref><abstract>  We present a system for the investigation of computational properties of
categorial grammar parsing based on a labelled analytic tableaux theorem
prover. This proof method allows us to take a modular approach, in which the
basic grammar can be kept constant, while a range of categorial calculi can be
captured by assigning different properties to the labelling algebra. The
theorem proving strategy is particularly well suited to the treatment of
categorial grammar, because it allows us to distribute the computational cost
between the algorithm which deals with the grammatical types and the algebraic
checker which constrains the derivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508010</id><created>1995-08-28</created><authors><author><keyname>Srinivas</keyname><forenames>B.</forenames></author><author><keyname>Doran</keyname><forenames>Christine</forenames></author><author><keyname>Kulick</keyname><forenames>Seth</forenames></author></authors><title>Heuristics and Parse Ranking</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed ps file. A4 format. 10 pages</comments><journal-ref>International Workshop on Parsing Technologies (IWPT 95)</journal-ref><abstract>  There are currently two philosophies for building grammars and parsers --
Statistically induced grammars and Wide-coverage grammars. One way to combine
the strengths of both approaches is to have a wide-coverage grammar with a
heuristic component which is domain independent but whose contribution is tuned
to particular domains. In this paper, we discuss a three-stage approach to
disambiguation in the context of a lexicalized grammar, using a variety of
domain independent heuristic techniques. We present a training algorithm which
uses hand-bracketed treebank parses to set the weights of these heuristics. We
compare the performance of our grammar against the performance of the IBM
statistical grammar, using both untrained and trained weights for the
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508011</id><created>1995-08-29</created><authors><author><keyname>Lochbaum</keyname><forenames>Karen E.</forenames><affiliation>U S WEST Advanced Technologies</affiliation></author></authors><title>The Use of Knowledge Preconditions in Language Processing</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, uses ijcai95.sty, postscript figures</comments><journal-ref>Proceedings of IJCAI-95</journal-ref><abstract>  If an agent does not possess the knowledge needed to perform an action, it
may privately plan to obtain the required information on its own, or it may
involve another agent in the planning process by engaging it in a dialogue. In
this paper, we show how the requirements of knowledge preconditions can be used
to account for information-seeking subdialogues in discourse. We first present
an axiomatization of knowledge preconditions for the SharedPlan model of
collaborative activity (Grosz &amp; Kraus, 1993), and then provide an analysis of
information-seeking subdialogues within a general framework for discourse
processing. In this framework, SharedPlans and relationships among them are
used to model the intentional component of Grosz and Sidner's (1986) theory of
discourse structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9508012</id><created>1995-08-30</created><authors><author><keyname>Ristad</keyname><forenames>Eric Sven</forenames><affiliation>Princeton University</affiliation></author></authors><title>A Natural Law of Succession</title><categories>cmp-lg cs.CL</categories><comments>23 pages</comments><report-no>pu-495-95</report-no><abstract>  Consider the problem of multinomial estimation. You are given an alphabet of
k distinct symbols and are told that the i-th symbol occurred exactly n_i times
in the past. On the basis of this information alone, you must now estimate the
conditional probability that the next symbol will be i. In this report, we
present a new solution to this fundamental problem in statistics and
demonstrate that our solution outperforms standard approaches, both in theory
and in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9509001</id><created>1995-09-07</created><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author></authors><title>How much is enough?: Data requirements for statistical NLP</title><categories>cmp-lg cs.CL</categories><comments>9 pages</comments><journal-ref>2nd Conference of the Pacific Association for Computational
  Linguistics, Brisbane, Australia</journal-ref><abstract>  In this paper I explore a number of issues in the analysis of data
requirements for statistical NLP systems. A preliminary framework for viewing
such systems is proposed and a sample of existing works are compared within
this framework. The first steps toward a theory of data requirements are made
by establishing some results relevant to bounding the expected error rate of a
class of simplified statistical language learners as a function of the volume
of training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9509002</id><created>1995-09-07</created><authors><author><keyname>Lauer</keyname><forenames>Mark</forenames><affiliation>Microsoft Institute, Sydney</affiliation></author></authors><title>Conserving Fuel in Statistical Language Learning: Predicting Data
  Requirements</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><journal-ref>Eighth Australian Joint Conference on Artificial Intelligence,
  Canberra, 1995.</journal-ref><abstract>  In this paper I address the practical concern of predicting how much training
data is sufficient for a statistical language learning system. First, I briefly
review earlier results and show how these can be combined to bound the expected
accuracy of a mode-based learner as a function of the volume of training data.
I then develop a more accurate estimate of the expected accuracy function under
the assumption that inputs are uniformly distributed. Since this estimate is
expensive to compute, I also give a close but cheaply computable approximation
to it. Finally, I report on a series of simulations exploring the effects of
inputs that are not uniformly distributed. Although these results are based on
simplistic assumptions, they are a tentative step toward a useful theory of
data requirements for SLL systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9509003</id><created>1995-09-09</created><authors><author><keyname>Lafferty</keyname><forenames>John D.</forenames><affiliation>Carnegie Mellon</affiliation></author><author><keyname>Suhm</keyname><forenames>Bernhard</forenames><affiliation>Carnegie Mellon</affiliation></author></authors><title>Cluster Expansions and Iterative Scaling for Maximum Entropy Language
  Models</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded and compressed postscript</comments><abstract>  The maximum entropy method has recently been successfully introduced to a
variety of natural language applications. In each of these applications,
however, the power of the maximum entropy method is achieved at the cost of a
considerable increase in computational requirements. In this paper we present a
technique, closely related to the classical cluster expansion from statistical
mechanics, for reducing the computational demands necessary to calculate
conditional maximum entropy language models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9509004</id><created>1995-09-13</created><authors><author><keyname>Losee</keyname><forenames>Robert M.</forenames><affiliation>SILS, U. of North Carolina, Chapel Hill</affiliation></author></authors><title>The Development and Migration of Concepts from Donor to Borrower
  Disciplines: Sublanguage Term Use in Hard &amp; Soft Sciences</title><categories>cmp-lg cs.CL</categories><comments>uuencoded compressed postscript file</comments><abstract>  Academic disciplines, often divided into hard and soft sciences, may be
understood as &quot;donor disciplines&quot; if they produce more concepts than they
borrow from other disciplines, or &quot;borrower disciplines&quot; if they import more
than they originate. Terms used to describe these concepts can be used to
distinguish between hard and soft, donor and borrower, as well as individual
discipline-specific sublanguages. Using term frequencies, the birth, growth,
death, and migration of concepts and their associated terms are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9509005</id><created>1995-09-28</created><authors><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Germany</affiliation></author><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Research Group, Freiburg University, Germany</affiliation></author></authors><title>ParseTalk about Textual Ellipsis</title><categories>cmp-lg cs.CL</categories><comments>11 pages, uuencoded compressed PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/ranlp95.ps)</comments><journal-ref>RANLP 95: Proc. of the Intl. Conf. on Recent Advances in Natural
  Language Processing. Tzigov Chark, Bulgaria, Sep. 14-16 1995, pp.62-72.</journal-ref><abstract>  A hybrid methodology for the resolution of text-level ellipsis is presented
in this paper. It incorporates conceptual proximity criteria applied to
ontologically well-engineered domain knowledge bases and an approach to
centering based on functional topic/comment patterns. We state text grammatical
predicates for ellipsis and then turn to the procedural aspects of their
evaluation within the framework of an actor-based implementation of a lexically
distributed parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510001</id><created>1995-10-02</created><authors><author><keyname>Padro</keyname><forenames>Lluis</forenames><affiliation>Dept LSI, Universitat Politecnica de Catalunya</affiliation></author></authors><title>POS Tagging Using Relaxation Labelling</title><categories>cmp-lg cs.CL</categories><comments>compressed &amp; uuencoded postscript file. Paper length: 39 pages</comments><abstract>  Relaxation labelling is an optimization technique used in many fields to
solve constraint satisfaction problems. The algorithm finds a combination of
values for a set of variables such that satisfies -to the maximum possible
degree- a set of given constraints. This paper describes some experiments
performed applying it to POS tagging, and the results obtained. It also ponders
the possibility of applying it to word sense disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510002</id><created>1995-09-27</created><authors><author><keyname>Hasan</keyname><forenames>Md Maruf</forenames><affiliation>Dept. of Information System &amp; Computer Science National University of Singapore</affiliation></author></authors><title>Using Chinese Text Processing Technique for the Processing of Sanskrit
  Based Indian Languages: Maximum Resource Utilization and Maximum
  Compatibility</title><categories>cmp-lg cs.CL</categories><comments>It may take longer time to print this file in a postscript printer. A
  Microsoft Word version of this paper can be provided on request. Interested
  people can obtain our Bengali fonts. Please send mail to the author</comments><abstract>  Chinese text processing systems are using Double Byte Coding , while almost
all existing Sanskrit Based Indian Languages have been using Single Byte coding
for text processing. Through observation, Chinese Information Processing
Technique has already achieved great technical development both in east and
west. In contrast,Indian Languages are being processed by computer, more or
less, for word processing purpose. This paper mainly emphasizes the method of
processing Indian languages from a Computational Linguistic point of view. An
overall design method is illustrated in this paper.This method concentrated on
maximum resource utilization and compatibility: the ultimate goal is to have a
Multiplatform Multilingual System. Keywords Text Procrssing, Multilingual Text
Processing, Chinese Language Processing, Indian Language Processing, Character
Coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510003</id><created>1995-10-04</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames><affiliation>Euskal Herriko Unibertsitatea</affiliation></author><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author></authors><title>A Proposal for Word Sense Disambiguation using Conceptual Distance</title><categories>cmp-lg cs.CL</categories><comments>Postscript version. 7 pages</comments><journal-ref>1st Intl. Conf. on recent Advances in NLP. Bulgaria. 1995.</journal-ref><abstract>  This paper presents a method for the resolution of lexical ambiguity and its
automatic evaluation over the Brown Corpus. The method relies on the use of the
wide-coverage noun taxonomy of WordNet and the notion of conceptual distance
among concepts, captured by a Conceptual Density formula developed for this
purpose. This fully automatic method requires no hand coding of lexical
entries, hand tagging of text nor any kind of training process. The results of
the experiment have been automatically evaluated against SemCor, the
sense-tagged version of the Brown Corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510004</id><created>1995-10-04</created><authors><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author><author><keyname>Agirre</keyname><forenames>Eneko</forenames><affiliation>Euskal Herriko Unibertsitatea</affiliation></author></authors><title>Disambiguating bilingual nominal entries against WordNet</title><categories>cmp-lg cs.CL</categories><comments>Postscrip version. 12 pages</comments><journal-ref>Workshop On The Computational Lexicon - ESSLLI 95.</journal-ref><abstract>  This paper explores the acquisition of conceptual knowledge from bilingual
dictionaries (French/English, Spanish/English and English/Spanish) using a
pre-existing broad coverage Lexical Knowledge Base (LKB) WordNet. Bilingual
nominal entries are disambiguated agains WordNet, therefore linking the
bilingual dictionaries to WordNet yielding a multilingual LKB (MLKB). The
resulting MLKB has the same structure as WordNet, but some nodes are attached
additionally to disambiguated vocabulary of other languages.
  Two different, complementary approaches are explored. In one of
the approaches each entry of the dictionary is taken in turn, exploiting the
information in the entry itself. The inferential capability for disambiguating
the translation is given by Semantic Density over WordNet. In the other
approach, the bilingual dictionary was merged with WordNet, exploiting mainly
synonymy relations. Each of the approaches was used in a different dictionary.
  Both approaches attain high levels of precision on their own, showing that
disambiguating bilingual nominal entries, and therefore linking bilingual
dictionaries to WordNet is a feasible task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510005</id><created>1995-10-09</created><authors><author><keyname>Briscoe</keyname><forenames>Ted</forenames><affiliation>Cambridge University</affiliation></author><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>Cambridge University</affiliation></author></authors><title>Developing and Evaluating a Probabilistic LR Parser of Part-of-Speech
  and Punctuation Labels</title><categories>cmp-lg cs.CL</categories><comments>11 pages, standard LaTeX</comments><journal-ref>4th International Workshop on Parsing Technologies (IWPT-95),
  48-58</journal-ref><abstract>  We describe an approach to robust domain-independent syntactic parsing of
unrestricted naturally-occurring (English) input. The technique involves
parsing sequences of part-of-speech and punctuation labels using a
unification-based grammar coupled with a probabilistic LR parser. We describe
the coverage of several corpora using this grammar and report the results of a
parsing experiment using probabilities derived from bracketed training data. We
report the first substantial experiments to assess the contribution of
punctuation to deriving an accurate syntactic analysis, by parsing identical
texts both with and without naturally-occurring punctuation marks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510006</id><created>1995-10-15</created><authors><author><keyname>Stys</keyname><forenames>Malgorzata E.</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Zemke</keyname><forenames>Stefan S.</forenames><affiliation>Linkoping University</affiliation></author></authors><title>Incorporating Discourse Aspects in English -- Polish MT: Towards Robust
  Implementation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded and compressed postscript file (presented at
  Recent Advances in NLP 95)</comments><abstract>  The main aim of translation is an accurate transfer of meaning so that the
result is not only grammatically and lexically correct but also communicatively
adequate. This paper stresses the need for discourse analysis the aim of which
is to preserve the communicative meaning in English--Polish machine
translation. Unlike English, which is a positional language with word order
grammatically determined, Polish displays a strong tendency to order
constituents according to their degree of salience, so that the most
informationally salient elements are placed towards the end of the clause
regardless of their grammatical function. The Centering Theory developed for
tracking down given information units in English and the Theory of Functional
Sentence Perspective predicting informativeness of subsequent constituents
provide theoretical background for this work. The notion of {\em center} is
extended to accommodate not only for pronominalisation and exact reiteration
but also for definiteness and other center pointing constructs. Center
information is additionally graded and applicable to all primary constituents
in a given utterance. This information is used to order the post-transfer
constituents correctly, relying on statistical regularities and some syntactic
clues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510007</id><created>1995-10-25</created><updated>1995-10-26</updated><authors><author><keyname>Dras</keyname><forenames>Mark</forenames><affiliation>Natural Language Unit, Microsoft Institute, Australia</affiliation></author></authors><title>Automatic Identification of Support Verbs: A Step Towards a Definition
  of Semantic Weight</title><categories>cmp-lg cs.CL</categories><comments>8 pages, standard LaTeX (replaced to fix LaTeX style used)</comments><abstract>  Current definitions of notions of lexical density and semantic weight are
based on the division of words into closed and open classes, and on intuition.
This paper develops a computationally tractable definition of semantic weight,
concentrating on what it means for a word to be semantically light; the
definition involves looking at the frequency of a word in particular syntactic
constructions which are indicative of lightness. Verbs such as &quot;make&quot; and
&quot;take&quot;, when they function as support verbs, are often considered to be
semantically light. To test our definition, we carried out an experiment based
on that of Grefenstette and Teufel (1995), where we automatically identify
light instances of these words in a corpus; this was done by incorporating our
frequency-related definition of semantic weight into a statistical approach
similar to that of Grefenstette and Teufel. The results show that this is a
plausible definition of semantic lightness for verbs, which can possibly be
extended to defining semantic lightness for other classes of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9510008</id><created>1995-10-31</created><updated>1995-11-01</updated><authors><author><keyname>Ikehara</keyname><forenames>Satoru</forenames><affiliation>NTT</affiliation></author><author><keyname>Shirai</keyname><forenames>Satoshi</forenames><affiliation>NTT</affiliation></author><author><keyname>Yokoo</keyname><forenames>Akio</forenames><affiliation>NTT</affiliation></author><author><keyname>Nakaiwa</keyname><forenames>Hiromi</forenames><affiliation>NTT</affiliation></author></authors><title>Toward an MT System without Pre-Editing --- Effects of New Methods in
  ALT-J/E ---</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX, optional Japanese commented out, uses twocolumn.sty,
  a4wide.sty, lsalike.sty, gb4e.sty</comments><journal-ref>Proceedings of MT Summit III, 1991, 101-106.</journal-ref><abstract>  Recently, several types of Japanese-to-English machine translation systems
have been developed, but all of them require an initial process of rewriting
the original text into easily translatable Japanese. Therefore these systems
are unsuitable for translating information that needs to be speedily
disseminated. To overcome this limitation, a Multi-Level Translation Method
based on the Constructive Process Theory has been proposed. This paper
describes the benefits of using this method in the Japanese-to-English machine
translation system ALT-J/E.
 In comparison with conventional compositional methods, the Multi-Level
Translation Method emphasizes the importance of the meaning contained in
expression structures as a whole. It is shown to be capable of translating
typical written Japanese based on the meaning of the text in its context, with
comparative ease. We are now hopeful of carrying out useful machine translation
with no manual pre-editing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511001</id><created>1995-11-03</created><authors><author><keyname>Bond</keyname><forenames>Francis</forenames><affiliation>NTT</affiliation></author><author><keyname>Ogura</keyname><forenames>Kentaro</forenames><affiliation>NTT</affiliation></author><author><keyname>Ikehara</keyname><forenames>Satoru</forenames><affiliation>NTT</affiliation></author></authors><title>Countability and Number in Japanese-to-English Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX, uses twocolumn.sty, 11pt, lsalike.sty, gb4e.sty</comments><journal-ref>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING'94), pp 32--38.</journal-ref><abstract>  This paper presents a heuristic method that uses information in the Japanese
text along with knowledge of English countability and number stored in transfer
dictionaries to determine the countability and number of English noun phrases.
Incorporating this method into the machine translation system ALT-J/E, helped
to raise the percentage of noun phrases generated with correct use of articles
and number from 65% to 73%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511002</id><created>1995-11-13</created><authors><author><keyname>Brew</keyname><forenames>Chris</forenames><affiliation>Language Technology Group, HCRC, Edinburgh</affiliation></author></authors><title>Letting the Cat out of the Bag: Generation for Shake-and-Bake MT</title><categories>cmp-lg cs.CL</categories><comments>9 pages, published in proceedings of COLING-92, gzipped postscript</comments><abstract>  Describes an algorithm for the generation phase of a Shake-and-Bake Machine
Translation system. Since the problem is NP-complete, it is unlikely that the
algorithm will be efficient in all cases, but for the cases tested it offers an
improvement over Whitelock's previously published algorithm. The work was
carried out while the author was employed at Sharp Laboratories of Europe Ltd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511003</id><created>1995-11-15</created><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author></authors><title>The Effect of Resource Limits and Task Complexity on Collaborative
  Planning in Dialogue</title><categories>cmp-lg cs.CL</categories><comments>64 pages, uses psfig, lingmacros, named</comments><journal-ref>Artificial Intelligence Journal 85(1-2), pp. 181-243, 1996</journal-ref><abstract>  This paper shows how agents' choice in communicative action can be designed
to mitigate the effect of their resource limits in the context of particular
features of a collaborative planning task. I first motivate a number of
hypotheses about effective language behavior based on a statistical analysis of
a corpus of natural collaborative planning dialogues. These hypotheses are then
tested in a dialogue testbed whose design is motivated by the corpus analysis.
Experiments in the testbed examine the interaction between (1) agents' resource
limits in attentional capacity and inferential capacity; (2) agents' choice in
communication; and (3) features of communicative tasks that affect task
difficulty such as inferential complexity, degree of belief coordination
required, and tolerance for errors. The results show that good algorithms for
communication must be defined relative to the agents' resource limits and the
features of the task. Algorithms that are inefficient for inferentially simple,
low coordination or fault-tolerant tasks are effective when tasks require
coordination or complex inferences, or are fault-intolerant. The results
provide an explanation for the occurrence of utterances in human dialogues
that, prima facie, appear inefficient, and provide the basis for the design of
effective algorithms for communicative choice for resource limited agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511004</id><created>1995-11-22</created><authors><author><keyname>Cahn</keyname><forenames>Janet</forenames><affiliation>Massachusetts Institute of Technology</affiliation></author></authors><title>An investigation into the correlation of cue phrases, unfilled pauses
  and the structuring of spoken discourse</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uufile'd source includes 3 .sty files - darpasls.sty,
  times.sty, psfonts.sty</comments><journal-ref>Proceedings of the IRCS Workshop on Prosody in Natural Speech,
  University of Pennsylvania. (1992) 19-30</journal-ref><abstract>  Expectations about the correlation of cue phrases, the duration of unfilled
pauses and the structuring of spoken discourse are framed in light of Grosz and
Sidner's theory of discourse and are tested for a directions-giving dialogue.
The results suggest that cue phrase and discourse structuring tasks may align,
and show a correlation for pause length and some of the modifications that
speakers can make to discourse structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511005</id><created>1995-11-29</created><authors><author><keyname>Lee</keyname><forenames>WonIl</forenames><affiliation>POSTECH, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>POSTECH, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>POSTECH, Korea</affiliation></author></authors><title>Chart-driven Connectionist Categorial Parsing of Spoken Korean</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Postscript file, Proceedings of ICCPOL'95</comments><abstract>  While most of the speech and natural language systems which were developed
for English and other Indo-European languages neglect the morphological
processing and integrate speech and natural language at the word level, for the
agglutinative languages such as Korean and Japanese, the morphological
processing plays a major role in the language processing since these languages
have very complex morphological phenomena and relatively simple syntactic
functionality. Obviously degenerated morphological processing limits the usable
vocabulary size for the system and word-level dictionary results in exponential
explosion in the number of dictionary entries. For the agglutinative languages,
we need sub-word level integration which leaves rooms for general morphological
processing. In this paper, we developed a phoneme-level integration model of
speech and linguistic processings through general morphological analysis for
agglutinative languages and a efficient parsing scheme for that integration.
Korean is modeled lexically based on the categorial grammar formalism with
unordered argument and suppressed category extensions, and chart-driven
connectionist parsing method is introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511006</id><created>1995-11-29</created><authors><author><keyname>Resnik</keyname><forenames>Philip</forenames></author></authors><title>Disambiguating Noun Groupings with Respect to WordNet Senses</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 16 pages, uses breakcites.sty, authdate.sty</comments><journal-ref>Proceedings of the 3rd Workshop on Very Large Corpora, MIT, 30
  June 1995</journal-ref><abstract>  Word groupings useful for language processing tasks are increasingly
available, as thesauri appear on-line, and as distributional word clustering
techniques improve. However, for many tasks, one is interested in relationships
among word {\em senses}, not words. This paper presents a method for automatic
sense disambiguation of nouns appearing within sets of related nouns --- the
kind of data one finds in on-line thesauri, or as the output of distributional
clustering algorithms. Disambiguation is performed with respect to WordNet
senses, which are fairly fine-grained; however, the method also permits the
assignment of higher-level WordNet categories rather than sense labels. The
method is illustrated primarily by example, though results of a more rigorous
evaluation are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9511007</id><created>1995-11-29</created><authors><author><keyname>Resnik</keyname><forenames>Philip</forenames></author></authors><title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy</title><categories>cmp-lg cs.CL</categories><comments>6 pages, 2 postscript figures, uses ijcai95.sty</comments><journal-ref>Proceedings of the 14th International Joint Conference on
  Artificial Intelligence</journal-ref><abstract>  This paper presents a new measure of semantic similarity in an IS-A taxonomy,
based on the notion of information content. Experimental evaluation suggests
that the measure performs encouragingly well (a correlation of r = 0.79 with a
benchmark set of human similarity judgments, with an upper bound of r = 0.90
for human subjects performing the same task), and significantly better than the
traditional edge counting approach (r = 0.66).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9512001</id><created>1995-12-09</created><authors><author><keyname>Kiraz</keyname><forenames>George A.</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Analysis of the Arabic Broken Plural and Diminutive</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded compressed .ps file, ICEMCO-96: 5th Inter. Conf.
  and Exhibition on Multi-Lingual Computing, Cambridge</comments><abstract>  This paper demonstrates how the challenging problem of the Arabic broken
plural and diminutive can be handled under a multi-tape two-level model, an
extension to two-level morphology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9512002</id><created>1995-12-12</created><authors><author><keyname>de Marcken</keyname><forenames>Carl</forenames><affiliation>MIT Artificial Intelligence Laboratory</affiliation></author></authors><title>The Unsupervised Acquisition of a Lexicon from Continuous Speech</title><categories>cmp-lg cs.CL</categories><comments>27 page technical report</comments><report-no>MIT AI Memo No. 1558/CBCL Memo No. 129</report-no><abstract>  We present an unsupervised learning algorithm that acquires a
natural-language lexicon from raw speech. The algorithm is based on the optimal
encoding of symbol sequences in an MDL framework, and uses a hierarchical
representation of language that overcomes many of the problems that have
stymied previous grammar-induction procedures. The forward mapping from symbol
sequences to the speech stream is modeled using features based on articulatory
gestures. We present results on the acquisition of lexicons and language models
from raw speech, text, and phonetic transcripts, and demonstrate that our
algorithm compares very favorably to other reported results with respect to
segmentation performance and statistical efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9512003</id><created>1995-12-18</created><updated>1996-08-14</updated><authors><author><keyname>Walker</keyname><forenames>Marilyn A.</forenames></author></authors><title>Limited Attention and Discourse Structure</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uses twoside,cl,lingmacros</comments><abstract>  This squib examines the role of limited attention in a theory of discourse
structure and proposes a model of attentional state that relates current
hierarchical theories of discourse structure to empirical evidence about human
discourse processing capabilities. First, I present examples that are not
predicted by Grosz and Sidner's stack model of attentional state. Then I
consider an alternative model of attentional state, the cache model, which
accounts for the examples, and which makes particular processing predictions.
Finally I suggest a number of ways that future research could distinguish the
predictions of the cache model and the stack model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9512004</id><created>1995-12-21</created><authors><author><keyname>Jones</keyname><forenames>Karen Sparck</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Natural language processing: she needs something old and something new
  (maybe something borrowed and something blue, too)</title><categories>cmp-lg cs.CL</categories><comments>Presidential Address, 1994, Association for Computational Linguistics</comments><abstract>  Given the present state of work in natural language processing, this address
argues first, that advance in both science and applications requires a revival
of concern about what language is about, broadly speaking the world; and
second, that an attack on the summarising task, which is made ever more
important by the growth of electronic text resources and requires an
understanding of the role of large-scale discourse structure in marking
important text content, is a good way forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9512005</id><created>1995-12-22</created><authors><author><keyname>Gerdemann</keyname><forenames>Dale</forenames></author></authors><title>Term Encoding of Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>12 pages Latex2e, ecltree, epic, eepic, tree-dvips</comments><journal-ref>Proceedings of the Fourth International Workshop on Parsing
  Technologies, pp. 89-98, 1995</journal-ref><abstract>  This paper presents an approach to Prolog-style term encoding of typed
feature structures. The type feature structures to be encoded are constrained
by appropriateness conditions as in Carpenter's ALE system. But unlike ALE, we
impose a further independently motivated closed-world assumption. This
assumption allows us to apply term encoding in cases that were problematic for
previous approaches. In particular, previous approaches have ruled out multiple
inheritance and further specification of feature-value declarations on
subtypes. In the present approach, these spececial cases can be handled as
well, though with some increase in complexity. For grammars without multiple
inheritance and specification of feature values, the encoding presented here
reduces to that of previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601001</id><created>1996-01-04</created><authors><author><keyname>Barg</keyname><forenames>Petra</forenames><affiliation>University of Duesseldorf</affiliation></author></authors><title>Automatic Inference of DATR Theories</title><categories>cmp-lg cs.CL</categories><comments>Latex 10 pages, 1 Postscript figure. To appear in H.-H. Bock, W.
  Polasek (eds.) Data Analysis and Information Systems: Statistical and
  conceptual approaches (Proceedings of the 19th Annual Conference of the
  Gesellschaft fuer Klassifikation e.V., University of Basel), Springer Verlag,
  pp. 506-515</comments><abstract>  This paper presents an approach for the automatic acquisition of linguistic
knowledge from unstructured data. The acquired knowledge is represented in the
lexical knowledge representation language DATR. A set of transformation rules
that establish inheritance relationships and a default-inference algorithm make
up the basis components of the system. Since the overall approach is not
restricted to a special domain, the heuristic inference strategy uses criteria
to evaluate the quality of a DATR theory, where different domains may require
different criteria. The system is applied to the linguistic learning task of
German noun inflection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601002</id><created>1996-01-09</created><authors><author><keyname>Gonzalo</keyname><forenames>Julio</forenames></author><author><keyname>Solias</keyname><forenames>Teresa</forenames></author></authors><title>Generic rules and non-constituent coordination</title><categories>cmp-lg cs.CL</categories><comments>latex2e, 12 pages, uses tree-dvips.sty. Appeared in IWPT'95</comments><journal-ref>IV International Workshop on Parsing Technologies (IWPT 95)</journal-ref><abstract>  We present a metagrammatical formalism, {\em generic rules}, to give a
default interpretation to grammar rules. Our formalism introduces a process of
{\em dynamic binding} interfacing the level of pure grammatical knowledge
representation and the parsing level. We present an approach to non-constituent
coordination within categorial grammars, and reformulate it as a generic rule.
This reformulation is context-free parsable and reduces drastically the search
space associated to the parsing task for such phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601003</identifier>
 <datestamp>2008-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601003</id><created>1996-01-18</created><authors><author><keyname>Crouch</keyname><forenames>Richard</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames><affiliation>Sheffield University</affiliation></author><author><keyname>Netter</keyname><forenames>Klaus</forenames><affiliation>DFKI, Saarbruecken</affiliation></author></authors><title>Report of the Study Group on Assessment and Evaluation</title><categories>cmp-lg cs.CL</categories><comments>83 pages</comments><abstract>  This is an interim report discussing possible guidelines for the assessment
and evaluation of projects developing speech and language systems. It was
prepared at the request of the European Commission DG XIII by an ad hoc study
group, and is now being made available in the form in which it was submitted to
the Commission. However, the report is not an official European Commission
document, and does not reflect European Commission policy, official or
otherwise.
  After a discussion of terminology, the report focusses on combining
user-centred and technology-centred assessment, and on how meaningful
comparisons can be made of a variety of systems performing different tasks for
different domains. The report outlines the kind of infra-structure that might
be required to support comparative assessment and evaluation of heterogenous
projects, and also the results of a questionnaire concerning different
approaches to evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601004</id><created>1996-01-23</created><authors><author><keyname>Kozima</keyname><forenames>Hideki</forenames><affiliation>University of Electro-Communications, Japan</affiliation></author><author><keyname>Furugori</keyname><forenames>Teiji</forenames><affiliation>University of Electro-Communications, Japan</affiliation></author></authors><title>Similarity between Words Computed by Spreading Activation on an English
  Dictionary</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uufiles (paper.tex, eacl93.sty, named.bst)</comments><journal-ref>Proceedings of EACL-93 (Utrecht), pp.232-239, 1993.</journal-ref><abstract>  This paper proposes a method for measuring semantic similarity between words
as a new tool for text analysis. The similarity is measured on a semantic
network constructed systematically from a subset of the English dictionary,
LDOCE (Longman Dictionary of Contemporary English). Spreading activation on the
network can directly compute the similarity between any two words in the
Longman Defining Vocabulary, and indirectly the similarity of all the other
words in LDOCE. The similarity represents the strength of lexical cohesion or
semantic relation, and also provides valuable information about similarity and
coherence of texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601005</id><created>1996-01-23</created><authors><author><keyname>Kozima</keyname><forenames>Hideki</forenames><affiliation>University of Electro-Communications, Japan</affiliation></author></authors><title>Text Segmentation Based on Similarity between Words</title><categories>cmp-lg cs.CL</categories><comments>3 pages, uufiles (paper.tex, acl.sty, bezier.sty)</comments><journal-ref>Proceedings of ACL-93 (Ohio), pp.286-288, 1993.</journal-ref><abstract>  This paper proposes a new indicator of text structure, called the lexical
cohesion profile (LCP), which locates segment boundaries in a text. A text
segment is a coherent scene; the words in a segment are linked together via
lexical cohesion relations. LCP records mutual similarity of words in a
sequence of text. The similarity of words, which represents their cohesiveness,
is computed using a semantic network. Comparison with the text segments marked
by a number of subjects shows that LCP closely correlates with the human
judgments. LCP may provide valuable information for resolving anaphora and
ellipsis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601006</id><created>1996-01-23</created><authors><author><keyname>Bond</keyname><forenames>Francis</forenames><affiliation>NTT</affiliation></author><author><keyname>Ogura</keyname><forenames>Kentaro</forenames><affiliation>NTT</affiliation></author><author><keyname>Ikehara</keyname><forenames>Satoru</forenames><affiliation>NTT</affiliation></author></authors><title>Possessive Pronouns as Determiners in Japanese-to-English Machine
  Translation</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX, uses twocolumn.sty, lsalike.sty</comments><abstract>  Possessive pronouns are used as determiners in English when no equivalent
would be used in a Japanese sentence with the same meaning. This paper proposes
a heuristic method of generating such possessive pronouns even when there is no
equivalent in the Japanese. The method uses information about the use of
possessive pronouns in English treated as a lexical property of nouns, in
addition to contextual information about noun phrase referentiality and the
subject and main verb of the sentence that the noun phrase appears in. The
proposed method has been implemented in NTT Communication Science Laboratories'
Japanese-to-English machine translation system ALT-J/E. In a test set of 6,200
sentences, the proposed method increased the number of noun phrases with
appropriate possessive pronouns generated, by 263 to 609, at the cost of
generating 83 noun phrases with inappropriate possessive pronouns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601007</id><created>1996-01-23</created><updated>1996-06-25</updated><authors><author><keyname>Kozima</keyname><forenames>Hideki</forenames><affiliation>Communications Research Laboratory, Japan</affiliation></author><author><keyname>Ito</keyname><forenames>Akira</forenames><affiliation>Communications Research Laboratory, Japan</affiliation></author></authors><title>Context-Sensitive Measurement of Word Distance by Adaptive Scaling of a
  Semantic Space</title><categories>cmp-lg cs.CL</categories><comments>8 pages, single LaTeX file</comments><abstract>  The paper proposes a computationally feasible method for measuring
context-sensitive semantic distance between words. The distance is computed by
adaptive scaling of a semantic space. In the semantic space, each word in the
vocabulary V is represented by a multi-dimensional vector which is obtained
from an English dictionary through a principal component analysis. Given a word
set C which specifies a context for measuring word distance, each dimension of
the semantic space is scaled up or down according to the distribution of C in
the semantic space. In the space thus transformed, distance between words in V
becomes dependent on the context C. An evaluation through a word prediction
task shows that the proposed measurement successfully extracts the context of a
text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601008</id><created>1996-01-23</created><authors><author><keyname>Bond</keyname><forenames>Francis</forenames><affiliation>NTT</affiliation></author><author><keyname>Ogura</keyname><forenames>Kentaro</forenames><affiliation>NTT</affiliation></author><author><keyname>Kawaoka</keyname><forenames>Tsukasa</forenames><affiliation>Doshisha University</affiliation></author></authors><title>Noun Phrase Reference in Japanese-to-English Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LaTeX, uses 11pt, a4wide, lsalike.sty</comments><abstract>  This paper shows the necessity of distinguishing different referential uses
of noun phrases in machine translation. We argue that differentiating between
the generic, referential and ascriptive uses of noun phrases is the minimum
necessary to generate articles and number correctly when translating from
Japanese to English. Heuristics for determining these differences are proposed
for a Japanese-to-English machine translation system. Finally the results of
using the proposed heuristics are shown to have raised the percentage of noun
phrases generated with correct use of articles and number in the
Japanese-to-English machine translation system ALT-J/E from 65% to 77%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601009</id><created>1996-01-23</created><updated>1996-01-30</updated><authors><author><keyname>Cunningham</keyname><forenames>Hamish</forenames><affiliation>Institute for Language Speech and Hearing</affiliation></author><author><keyname>Gaizauskas</keyname><forenames>Robert J.</forenames><affiliation>Institute for Language Speech and Hearing</affiliation></author><author><keyname>Wilks</keyname><forenames>Yorick</forenames><affiliation>Institute for Language Speech and Hearing</affiliation></author></authors><title>A General Architecture for Language Engineering (GATE) - a new approach
  to Language Engineering R&amp;D</title><categories>cmp-lg cs.CL</categories><comments>52 page technical report, LaTeX 2e source</comments><report-no>CS - 95 - 21</report-no><abstract>  This report argues for the provision of a common software infrastructure for
NLP systems. Current trends in Language Engineering research are reviewed as
motivation for this infrastructure, and relevant recent work discussed. A
freely-available system called GATE is described which builds on this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601010</id><created>1996-01-31</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author></authors><title>Parsing with Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>PostScript, 15 pages; Proc. 4th Intl. Workshop on Parsing
  Technologies, Prague, September 1995</comments><abstract>  In this paper we provide for parsing with respect to grammars expressed in a
general TFS-based formalism, a restriction of ALE. Our motivation being the
design of an abstract (WAM-like) machine for the formalism, we consider parsing
as a computational process and use it as an operational semantics to guide the
design of the control structures for the abstract machine.
 We emphasize the notion of abstract typed feature structures (AFSs) that
encode the essential information of TFSs and define unification over AFSs
rather than over TFSs. We then introduce an explicit construct of multi-rooted
feature structures (MRSs) that naturally extend TFSs and use them to represent
phrasal signs as well as grammar rules. We also employ abstractions of MRSs and
give the mathematical foundations needed for manipulating them. We then present
a simple bottom-up chart parser as a model for computation: grammars written in
the TFS-based formalism are executed by the parser. Finally, we show that the
parser is correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9601011</id><created>1996-01-31</created><authors><author><keyname>Wintner</keyname><forenames>Shuly</forenames><affiliation>Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author><author><keyname>Francez</keyname><forenames>Nissim</forenames><affiliation>Computer Science, Technion, Israel Institute of Technology, Haifa, Israel</affiliation></author></authors><title>Parsing with Typed Feature Structures</title><categories>cmp-lg cs.CL</categories><comments>PostScript, 29 pages</comments><report-no>Laboratory for Computational Linguistics TR #LCL 95-1</report-no><abstract>  In this paper we provide for parsing with respect to grammars expressed in a
general TFS-based formalism, a restriction of ALE. Our motivation being the
design of an abstract (WAM-like) machine for the formalism, we consider parsing
as a computational process and use it as an operational semantics to guide the
design of the control structures for the abstract machine.
 We emphasize the notion of abstract typed feature structures (AFSs) that
encode the essential information of TFSs and define unification over AFSs
rather than over TFSs. We then introduce an explicit construct of multi-rooted
feature structures (MRSs) that naturally extend TFSs and use them to represent
phrasal signs as well as grammar rules. We also employ abstractions of MRSs and
give the mathematical foundations needed for manipulating them. We formally
define grammars and the languages they generate, and then describe a model for
computation that corresponds to bottom-up chart parsing: grammars written in
the TFS-based formalism are executed by the parser. We show that the
computation is correct with respect to the independent definition. Finally, we
discuss the class of grammars for which computations terminate and prove that
termination can be guaranteed for off-line parsable grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9602001</id><created>1996-02-08</created><authors><author><keyname>Losee</keyname><forenames>Robert M.</forenames><affiliation>University of North Carolina Chapel Hill, NC, U.S.A.</affiliation></author></authors><title>How Part-of-Speech Tags Affect Text Retrieval and Filtering Performance</title><categories>cmp-lg cs.CL</categories><comments>uuencoded and compressed postscript</comments><abstract>  Natural language processing (NLP) applied to information retrieval (IR) and
filtering problems may assign part-of-speech tags to terms and, more generally,
modify queries and documents. Analytic models can predict the performance of a
text filtering system as it incorporates changes suggested by NLP, allowing us
to make precise statements about the average effect of NLP operations on IR.
Here we provide a model of retrieval and tagging that allows us to both compute
the performance change due to syntactic parsing and to allow us to understand
what factors affect performance and how. In addition to a prediction of
performance with tags, upper and lower bounds for retrieval performance are
derived, giving the best and worst effects of including part-of-speech tags.
Empirical grounds for selecting sets of tags are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9602002</id><created>1996-02-15</created><authors><author><keyname>TIN</keyname><forenames>Erkan</forenames><affiliation>Bilkent University</affiliation></author><author><keyname>AKMAN</keyname><forenames>Varol</forenames><affiliation>Bilkent University</affiliation></author></authors><title>Situations and Computation: An Overview of Recent Research</title><categories>cmp-lg cs.CL</categories><comments>30 pages, also published in the University of Tuebingen Technical
  Report Series</comments><report-no>SfS-Report-04-95</report-no><abstract>  Serious thinking about the computational aspects of situation theory is just
starting. There have been some recent proposals in this direction (viz. PROSIT
and ASTL), with varying degrees of divergence from the ontology of the theory.
We believe that a programming environment incorporating bona fide
situation-theoretic constructs is needed and describe our very recent BABY-SIT
implementation. A detailed critical account of PROSIT and ASTL is also offered
in order to compare our system with these pioneering and influential
frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9602003</id><created>1996-02-18</created><authors><author><keyname>Losee</keyname><forenames>Robert M.</forenames><affiliation>University of North Carolina Chapel Hill</affiliation></author></authors><title>Text Windows and Phrases Differing by Discipline, Location in Document,
  and Syntactic Structure</title><categories>cmp-lg cs.CL</categories><comments>uuencoded and compressed postscript</comments><abstract>  Knowledge of window style, content, location and grammatical structure may be
used to classify documents as originating within a particular discipline or may
be used to place a document on a theory versus practice spectrum. This
distinction is also studied here using the type-token ratio to differentiate
between sublanguages. The statistical significance of windows is computed,
based on the the presence of terms in titles, abstracts, citations, and section
headers, as well as binary independent (BI) and inverse document frequency
(IDF) weightings. The characteristics of windows are studied by examining their
within window density (WWD) and the S concentration (SC), the concentration of
terms from various document fields (e.g. title, abstract) in the fulltext. The
rate of window occurrences from the beginning to the end of document fulltext
differs between academic fields. Different syntactic structures in sublanguages
are examined, and their use is considered for discriminating between specific
academic disciplines and, more generally, between theory versus practice or
knowledge versus applications oriented documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9602004</id><created>1996-02-27</created><authors><author><keyname>Carletta</keyname><forenames>Jean</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Assessing agreement on classification tasks: the kappa statistic</title><categories>cmp-lg cs.CL</categories><comments>9 pages</comments><journal-ref>Computational Lingustics 22:2 (1996 forthcoming)</journal-ref><abstract>  Currently, computational linguists and cognitive scientists working in the
area of discourse and dialogue argue that their subjective judgments are
reliable using several different statistics, none of which are easily
interpretable or comparable to each other. Meanwhile, researchers in content
analysis have already experienced the same difficulties and come up with a
solution in the kappa statistic. We discuss what is wrong with reliability
measures as they are currently used for discourse and dialogue work in
computational linguistics and cognitive science, and argue that we would be
better off as a field adopting techniques from content analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603001</id><created>1996-03-07</created><authors><author><keyname>Pereira</keyname><forenames>Fernando C. N.</forenames><affiliation>AT&amp;T Research</affiliation></author><author><keyname>Riley</keyname><forenames>Michael D.</forenames><affiliation>AT&amp;T Research</affiliation></author></authors><title>Speech Recognition by Composition of Weighted Finite Automata</title><categories>cmp-lg cs.CL</categories><comments>24 pages, uses psfig.sty</comments><abstract>  We present a general framework based on weighted finite automata and weighted
finite-state transducers for describing and implementing speech recognizers.
The framework allows us to represent uniformly the information sources and data
structures used in recognition, including context-dependent units,
pronunciation dictionaries, language models and lattices. Furthermore, general
but efficient algorithms can used for combining information sources in actual
recognizers and for optimizing their application. In particular, a single
composition algorithm is used both to combine in advance information sources
such as language models and dictionaries, and to combine acoustic observations
and information sources dynamically during recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603002</id><created>1996-03-08</created><authors><author><keyname>Pereira</keyname><forenames>Fernando C. N.</forenames><affiliation>AT&amp;T Research</affiliation></author><author><keyname>Wright</keyname><forenames>Rebecca N.</forenames><affiliation>AT&amp;T Research</affiliation></author></authors><title>Finite-State Approximation of Phrase-Structure Grammars</title><categories>cmp-lg cs.CL</categories><comments>24 pages, uses psfig.sty; revised and extended version of the 1991
  ACL meeting paper with the same title</comments><abstract>  Phrase-structure grammars are effective models for important syntactic and
semantic aspects of natural languages, but can be computationally too demanding
for use as language models in real-time speech recognition. Therefore,
finite-state models are used instead, even though they lack expressive power.
To reconcile those two alternatives, we designed an algorithm to compute
finite-state approximations of context-free grammars and
context-free-equivalent augmented phrase-structure grammars. The approximation
is exact for certain context-free grammars generating regular languages,
including all left-linear and right-linear context-free grammars. The algorithm
has been used to build finite-state language models for limited-domain speech
recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603003</id><created>1996-03-13</created><authors><author><keyname>Fuchs</keyname><forenames>Norbert E.</forenames><affiliation>Department of Computer Science, University of Zurich</affiliation></author><author><keyname>Schwitter</keyname><forenames>Rolf</forenames><affiliation>Department of Computer Science, University of Zurich</affiliation></author></authors><title>Attempto Controlled English (ACE)</title><categories>cmp-lg cs.CL</categories><comments>13 pages, compressed, uuencoded Postscript, to be presented at CLAW
  96, The First International Workshop on Controlled Language Applications,
  Katholieke Universiteit Leuven, 26-27 March 1996</comments><abstract>  Attempto Controlled English (ACE) allows domain specialists to interactively
formulate requirements specifications in domain concepts. ACE can be accurately
and efficiently processed by a computer, but is expressive enough to allow
natural usage. The Attempto system translates specification texts in ACE into
discourse representation structures and optionally into Prolog. Translated
specification texts are incrementally added to a knowledge base. This knowledge
base can be queried in ACE for verification, and it can be executed for
simulation, prototyping and validation of the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603004</id><created>1996-03-12</created><authors><author><keyname>Schwitter</keyname><forenames>Rolf</forenames></author><author><keyname>Fuchs</keyname><forenames>Norbert E.</forenames></author></authors><title>Attempto - From Specifications in Controlled Natural Language towards
  Executable Specifications</title><categories>cmp-lg cs.CL</categories><comments>15 pages, compressed, uuencoded Postscript, to be presented at EMISA
  Workshop 'Naturlichsprachlicher Entwurf von Informationssystemen -
  Grundlagen, Methoden, Werkzeuge, Anwendungen', May 28-30, 1996, Ev. Akademie
  Tutzing</comments><abstract>  Deriving formal specifications from informal requirements is difficult since
one has to take into account the disparate conceptual worlds of the application
domain and of software development. To bridge the conceptual gap we propose
controlled natural language as a textual view on formal specifications in
logic. The specification language Attempto Controlled English (ACE) is a subset
of natural language that can be accurately and efficiently processed by a
computer, but is expressive enough to allow natural usage. The Attempto system
translates specifications in ACE into discourse representation structures and
into Prolog. The resulting knowledge base can be queried in ACE for
verification, and it can be executed for simulation, prototyping and validation
of the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603005</id><created>1996-03-18</created><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Department of Computer Science and Engineering Pohang University of Science and Technology</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Department of Computer Science and Engineering Pohang University of Science and Technology</affiliation></author></authors><title>Integrated speech and morphological processing in a connectionist
  continuous speech understanding for Korean</title><categories>cmp-lg cs.CL</categories><comments>latex source with a4 style, 15 pages, to be published in computer
  processing of oriental language journal</comments><abstract>  A new tightly coupled speech and natural language integration model is
presented for a TDNN-based continuous possibly large vocabulary speech
recognition system for Korean. Unlike popular n-best techniques developed for
integrating mainly HMM-based speech recognition and natural language processing
in a {\em word level}, which is obviously inadequate for morphologically
complex agglutinative languages, our model constructs a spoken language system
based on a {\em morpheme-level} speech and language integration. With this
integration scheme, the spoken Korean processing engine (SKOPE) is designed and
implemented using a TDNN-based diphone recognition module integrated with a
Viterbi-based lexical decoding and symbolic phonological/morphological
co-analysis. Our experiment results show that the speaker-dependent continuous
{\em eojeol} (Korean word) recognition and integrated morphological analysis
can be achieved with over 80.6% success rate directly from speech inputs for
the middle-level vocabularies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9603006</id><created>1996-03-18</created><authors><author><keyname>Breidt</keyname><forenames>Elisabeth</forenames><affiliation>Seminar f&#xfc;r Sprachwissenschaft, Universit&#xe4;t T&#xfc;bingen, Germany</affiliation></author></authors><title>Extraction of V-N-Collocations from Text Corpora: A Feasibility Study
  for German</title><categories>cmp-lg cs.CL</categories><comments>12 pages, revised version of paper presented at 1st ACL-Workshop on
  Very Large Corpora, Columbus, Ohio, June 1993</comments><abstract>  The usefulness of a statistical approach suggested by Church et al. (1991) is
evaluated for the extraction of verb-noun (V-N) collocations from German text
corpora. Some problematic issues of that method arising from properties of the
German language are discussed and various modifications of the method are
considered that might improve extraction results for German. The precision and
recall of all variant methods is evaluated for V-N collocations containing
support verbs, and the consequences for further work on the extraction of
collocations from German corpora are discussed.
 With a sufficiently large corpus (&gt;= 6 mio. word-tokens), the average error
rate of wrong extractions can be reduced to 2.2% (97.8% precision) with the
most restrictive method, however with a loss in data of almost 50% compared to
a less restrictive method with still 87.6% precision. Depending on the goal to
be achieved, emphasis can be put on a high recall for lexicographic purposes or
on high precision for automatic lexical acquisition, in each case unfortunately
leading to a decrease of the corresponding other variable. Low recall can still
be acceptable if very large corpora (i.e. 50 - 100 million words) are available
or if corpora for special domains are used in addition to the data found in
machine readable (collocation) dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604001</id><created>1996-04-11</created><updated>1996-04-12</updated><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Department of Computer Engineering, Bilkent University, Ankara Turkey</affiliation></author><author><keyname>Tur</keyname><forenames>Gokhan</forenames><affiliation>Department of Computer Engineering, Bilkent University, Ankara Turkey</affiliation></author></authors><title>Combining Hand-crafted Rules and Unsupervised Learning in
  Constraint-based Morphological Disambiguation</title><categories>cmp-lg cs.CL</categories><comments>gzipped and uuencoded postscript, 13 pages. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/emnlp.ps.z</comments><abstract>  This paper presents a constraint-based morphological disambiguation approach
that is applicable languages with complex morphology--specifically
agglutinative languages with productive inflectional and derivational
morphological phenomena. In certain respects, our approach has been motivated
by Brill's recent work, but with the observation that his transformational
approach is not directly applicable to languages like Turkish. Our system
combines corpus independent hand-crafted constraint rules, constraint rules
that are learned via unsupervised learning from a training corpus, and
additional statistical information from the corpus to be morphologically
disambiguated. The hand-crafted rules are linguistically motivated and tuned to
improve precision without sacrificing recall. The unsupervised learning process
produces two sets of rules: (i) choose rules which choose morphological parses
of a lexical item satisfying constraint effectively discarding other parses,
and (ii) delete rules, which delete parses satisfying a constraint. Our
approach also uses a novel approach to unknown word processing by employing a
secondary morphological processor which recovers any relevant inflectional and
derivational information from a lexical item whose root is unknown. With this
approach, well below 1 percent of the tokens remains as unknown in the texts we
have experimented with. Our results indicate that by combining these
hand-crafted,statistical and learned information sources, we can attain a
recall of 96 to 97 percent with a corresponding precision of 93 to 94 percent,
and ambiguity of 1.02 to 1.03 parses per token.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604002</id><created>1996-04-11</created><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Department of Computer Engineering, Bilkent University, Ankara, Turkey</affiliation></author><author><keyname>Yilmaz</keyname><forenames>Okan</forenames><affiliation>Department of Computer Engineering, Bilkent University, Ankara, Turkey</affiliation></author></authors><title>A Constraint-based Case Frame Lexicon</title><categories>cmp-lg cs.CL</categories><comments>gzipped, uuencoded postscript, 6 pages. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/coling96-ccl.ps.z ; To Appear in
  Proceedings of COLING 96, Copenhaged, Denmark, August 1996</comments><abstract>  We present a constraint-based case frame lexicon architecture for
bi-directional mapping between a syntactic case frame and a semantic frame. The
lexicon uses a semantic sense as the basic unit and employs a multi-tiered
constraint structure for the resolution of syntactic information into the
appropriate senses and/or idiomatic usage. Valency changing transformations
such as morphologically marked passivized or causativized forms are handled via
lexical rules that manipulate case frames templates. The system has been
implemented in a typed-feature system and applied to Turkish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604003</id><created>1996-04-11</created><updated>1996-04-17</updated><authors><author><keyname>Oflazer</keyname><forenames>Kemal</forenames><affiliation>Department of Computer Engineering, Bilkent University, Ankara, Turkey</affiliation></author></authors><title>Error-tolerant Tree Matching</title><categories>cmp-lg cs.CL</categories><comments>gzipped and uuencoded postscript, 5 pages. Minor fix in one of the
  figures. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/coling96-ettm.ps.z</comments><abstract>  This paper presents an efficient algorithm for retrieving from a database of
trees, all trees that match a given query tree approximately, that is, within a
certain error tolerance. It has natural language processing applications in
searching for matches in example-based translation systems, and retrieval from
lexical databases containing entries of complex feature structures. The
algorithm has been implemented on SparcStations, and for large randomly
generated synthetic tree databases (some having tens of thousands of trees) it
can associatively search for trees with a small error, in a matter of tenths of
a second to few seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604004</id><created>1996-04-12</created><authors><author><keyname>Carroll</keyname><forenames>John</forenames><affiliation>University of Sussex</affiliation></author><author><keyname>Briscoe</keyname><forenames>Ted</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Apportioning Development Effort in a Probabilistic LR Parsing System
  through Evaluation</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 1 Postscript figure. To Appear in Proceedings of the
  Conference on Empirical Methods in Natural Language Processing, University of
  Pennsylvania, May 1996</comments><journal-ref>Conference on Empirical Methods in Natural Language Processing
  (EMNLP-96), 92-100</journal-ref><abstract>  We describe an implemented system for robust domain-independent syntactic
parsing of English, using a unification-based grammar of part-of-speech and
punctuation labels coupled with a probabilistic LR parser. We present
evaluations of the system's performance along several different dimensions;
these enable us to assess the contribution that each individual part is making
to the success of the system as a whole, and thus prioritise the effort to be
devoted to its further enhancement. Currently, the system is able to parse
around 80% of sentences in a substantial corpus of general text containing a
number of distinct genres. On a random sample of 250 such sentences the system
has a mean crossing bracket rate of 0.71 and recall and precision of 83% and
84% respectively when evaluated against manually-disambiguated analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604005</id><created>1996-04-17</created><authors><author><keyname>Brants</keyname><forenames>Thorsten</forenames><affiliation>Universit&quot;at des Saarlandes, Computational Linguistics, Saarbr&quot;ucken, Germany</affiliation></author></authors><title>Better Language Models with Model Merging</title><categories>cmp-lg cs.CL</categories><comments>LaTeX, 9 pages. In Proceedings of EMNLP-96, Philadelphia, PA</comments><abstract>  This paper investigates model merging, a technique for deriving Markov models
from text or speech corpora. Models are derived by starting with a large and
specific model and by successively combining states to build smaller and more
general models. We present methods to reduce the time complexity of the
algorithm and report on experiments on deriving language models for a speech
recognition task. The experiments show the advantage of model merging over the
standard bigram approach. The merged model assigns a lower perplexity to the
test set and uses considerably fewer states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604006</id><created>1996-04-18</created><authors><author><keyname>Dale</keyname><forenames>Robert</forenames><affiliation>Microsoft Institute, Sydney, Australia</affiliation></author><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>University of Aberdeen, UK</affiliation></author></authors><title>The Role of the Gricean Maxims in the Generation of Referring
  Expressions</title><categories>cmp-lg cs.CL</categories><comments>LaTeX file, needs aaai.sty (available from the cmp-lg macro library).
  This paper was presented at the 1996 AAAI Spring Symposium on Computational
  Models of Conversational Implicature</comments><abstract>  Grice's maxims of conversation [Grice 1975] are framed as directives to be
followed by a speaker of the language. This paper argues that, when considered
from the point of view of natural language generation, such a characterisation
is rather misleading, and that the desired behaviour falls out quite naturally
if we view language generation as a goal-oriented process. We argue this
position with particular regard to the generation of referring expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604007</id><created>1996-04-18</created><updated>1996-04-19</updated><authors><author><keyname>Freeman</keyname><forenames>Robert John</forenames><affiliation>Hong Kong University of Science and Technology</affiliation></author></authors><title>Collocational Grammar</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uuencoded gzipped Postscript</comments><abstract>  A perspective of statistical language models which emphasizes their
collocational aspect is advocated. It is suggested that strings be generalized
in terms of classes of relationships instead of classes of objects. The single
most important characteristic of such a model is a mechanism for comparing
patterns. When patterns are fully generalized a natural definition of syntactic
class emerges as a subset of relational class. These collocational syntactic
classes should be an unambiguous partition of traditional syntactic classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604008</id><created>1996-04-21</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames><affiliation>Harvard University</affiliation></author></authors><title>Efficient Algorithms for Parsing the DOP Model</title><categories>cmp-lg cs.CL</categories><comments>10 pages</comments><journal-ref>Proceedings of the Conference on Empirical Methods in Natural
  Language Processing, May 1996</journal-ref><abstract>  Excellent results have been reported for Data-Oriented Parsing (DOP) of
natural language texts (Bod, 1993). Unfortunately, existing algorithms are both
computationally intensive and difficult to implement. Previous algorithms are
expensive due to two factors: the exponential number of rules that must be
generated and the use of a Monte Carlo parsing algorithm. In this paper we
solve the first problem by a novel reduction of the DOP model to a small,
equivalent probabilistic context-free grammar. We solve the second problem by a
novel deterministic parsing strategy that maximizes the expected number of
correct constituents, rather than the probability of a correct parse tree.
Using the optimizations, experiments yield a 97% crossing brackets rate and 88%
zero crossing brackets rate. This differs significantly from the results
reported by Bod, and is comparable to results from a duplication of Pereira and
Schabes's (1992) experiment on the same data. We show that Bod's results are at
least partially due to an extremely fortuitous choice of test data, and
partially due to using cleaner data than other researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604009</id><created>1996-04-23</created><authors><author><keyname>Boullier</keyname><forenames>Pierre</forenames><affiliation>INRIA-Rocquencourt, Le Chesnay Cedex, France</affiliation></author></authors><title>Another Facet of LIG Parsing</title><categories>cmp-lg cs.CL</categories><comments>LaTex, 8 pages. To appear in Proceedings of ACL'96, Univ. of
  California, Santa Cruz, June 1996</comments><abstract>  In this paper we present a new parsing algorithm for linear indexed grammars
(LIGs) in the same spirit as the one described in (Vijay-Shanker and Weir,
1993) for tree adjoining grammars. For a LIG $L$ and an input string $x$ of
length $n$, we build a non ambiguous context-free grammar whose sentences are
all (and exclusively) valid derivation sequences in $L$ which lead to $x$. We
show that this grammar can be built in ${\cal O}(n^6)$ time and that individual
parses can be extracted in linear time with the size of the extracted parse
tree. Though this ${\cal O}(n^6)$ upper bound does not improve over previous
results, the average case behaves much better. Moreover, practical parsing
times can be decreased by some statically performed computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604010</id><created>1996-04-23</created><authors><author><keyname>Meurers</keyname><forenames>Walt Detmar</forenames><affiliation>SFB 340, Univ. Tuebingen</affiliation></author><author><keyname>Minnen</keyname><forenames>Guido</forenames><affiliation>SFB 340, Univ. Tuebingen</affiliation></author></authors><title>Off-line Constraint Propagation for Efficient HPSG Processing</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded gzipped Postscript</comments><journal-ref>Proceedings HPSG/TALN Conference, Marseille, France, May 20-22</journal-ref><abstract>  We investigate the use of a technique developed in the constraint programming
community called constraint propagation to automatically make a HPSG theory
more specific at those places where linguistically motivated underspecification
would lead to inefficient processing. We discuss two concrete HPSG examples
showing how off-line constraint propagation helps improve processing
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604011</id><created>1996-04-24</created><authors><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, Korea</affiliation></author><author><keyname>Yoo</keyname><forenames>JinHee</forenames><affiliation>Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, Korea</affiliation></author></authors><title>Multi-level post-processing for Korean character recognition using
  morphological analysis and linguistic evaluation</title><categories>cmp-lg cs.CL</categories><comments>latex with a4, epsfig style, 21 pages, 11 postscript figures,
  accepted in pattern recognition journal</comments><abstract>  Most of the post-processing methods for character recognition rely on
contextual information of character and word-fragment levels. However, due to
linguistic characteristics of Korean, such low-level information alone is not
sufficient for high-quality character-recognition applications, and we need
much higher-level contextual information to improve the recognition results.
This paper presents a domain independent post-processing technique that
utilizes multi-level morphological, syntactic, and semantic information as well
as character-level information. The proposed post-processing system performs
three-level processing: candidate character-set selection, candidate eojeol
(Korean word) generation through morphological analysis, and final single
eojeol-sequence selection by linguistic evaluation. All the required linguistic
information and probabilities are automatically acquired from a statistical
corpus analysis. Experimental results demonstrate the effectiveness of our
method, yielding error correction rate of 80.46%, and improved recognition rate
of 95.53% from before-post-processing rate 71.2% for single best-solution
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604012</id><created>1996-04-24</created><authors><author><keyname>Kiraz</keyname><forenames>George Anton</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>SemHe: A Generalised Two-Level System</title><categories>cmp-lg cs.CL</categories><comments>uuencoded Z-compressed .tar file created by csh script uufiles. 8
  pages. To appear in Proceedings of ACL'96, Univ. of California, Santa Cruz,
  June 1996</comments><abstract>  This paper presents a generalised two-level implementation which can handle
linear and non-linear morphological operations. An algorithm for the
interpretation of multi-tape two-level rules is described. In addition, a
number of issues which arise when developing non-linear grammars are discussed
with examples from Syriac.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604013</id><created>1996-04-24</created><authors><author><keyname>Butt</keyname><forenames>Miriaam</forenames><affiliation>University of Stuttgart</affiliation></author><author><keyname>Fortman</keyname><forenames>Christian</forenames><affiliation>University of Stuttgart</affiliation></author><author><keyname>Rohrer</keyname><forenames>Christian</forenames><affiliation>University of Stuttgart</affiliation></author></authors><title>Syntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs</title><categories>cmp-lg cs.CL</categories><comments>To Appear in Proceedings of COLING'96 Copenhagen, Denmark, August
  1996</comments><abstract>  This paper focuses on two disparate aspects of German syntax from the
perspective of parallel grammar development. As part of a cooperative project,
we present an innovative approach to auxiliaries and multiple genitive NPs in
German. The LFG-based implementation presented here avoids unnessary structural
complexity in the representation of auxiliaries by challenging the traditional
analysis of auxiliaries as raising verbs. The approach developed for multiple
genitive NPs provides a more abstract, language independent representation of
genitives associated with nominalized verbs. Taken together, the two approaches
represent a step towards providing uniformly applicable treatments for
differing languages, thus lightening the burden for machine translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604014</id><created>1996-04-25</created><authors><author><keyname>G&#xf6;tz</keyname><forenames>Thilo</forenames><affiliation>Univ. of T&#xfc;bingen, Germany</affiliation></author><author><keyname>Meurers</keyname><forenames>Walt Detmar</forenames><affiliation>Univ. of T&#xfc;bingen, Germany</affiliation></author></authors><title>The importance of being lazy -- using lazy evaluation to process queries
  to HPSG grammars</title><categories>cmp-lg cs.CL</categories><comments>uuencoded, gzipped postscript. In proceedings of TALN and HPSG 96,
  Marseille, France</comments><abstract>  Linguistic theories formulated in the architecture of {\sc hpsg} can be very
precise and explicit since {\sc hpsg} provides a formally well-defined setup.
However, when querying a faithful implementation of such an explicit theory,
the large data structures specified can make it hard to see the relevant
aspects of the reply given by the system. Furthermore, the system spends much
time applying constraints which can never fail just to be able to enumerate
specific answers. In this paper we want to describe lazy evaluation as the
result of an off-line compilation technique. This method of evaluation can be
used to answer queries to an {\sc hpsg} system so that only the relevant
aspects are checked and output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604015</id><created>1996-04-25</created><authors><author><keyname>Kiraz</keyname><forenames>George Anton</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Computing Prosodic Morphology</title><categories>cmp-lg cs.CL</categories><comments>uuencoded Z-compressed .tar file created by csh script uufiles. 6
  pages. To appear in coling-96</comments><abstract>  This paper establishes a framework under which various aspects of prosodic
morphology, such as templatic morphology and infixation, can be handled under
two-level theory using an implemented multi-tape two-level model. The paper
provides a new computational analysis of root-and-pattern morphology based on
prosody.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604016</id><created>1996-04-26</created><authors><author><keyname>Bouaud</keyname><forenames>Jacques</forenames><affiliation>DIAM: SIM/AP-HP &amp; Dept de Biomath&#xe9;matiques, Universit&#xe9; Paris 6</affiliation></author><author><keyname>Bachimont</keyname><forenames>Bruno</forenames><affiliation>DIAM: SIM/AP-HP &amp; Dept de Biomath&#xe9;matiques, Universit&#xe9; Paris 6</affiliation></author><author><keyname>Zweigenbaum</keyname><forenames>Pierre</forenames><affiliation>DIAM: SIM/AP-HP &amp; Dept de Biomath&#xe9;matiques, Universit&#xe9; Paris 6</affiliation></author></authors><title>Processing Metonymy: a Domain-Model Heuristic Graph Traversal Approach</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX, one encapsulated PostScript figure, uses colap.sty
  (included) and epsf.sty (available from the cmp-lg macro library). To appear
  in Coling-96</comments><report-no>DIAM RI-95-159</report-no><abstract>  We address here the treatment of metonymic expressions from a knowledge
representation perspective, that is, in the context of a text understanding
system which aims to build a conceptual representation from texts according to
a domain model expressed in a knowledge representation formalism.
  We focus in this paper on the part of the semantic analyser which deals with
semantic composition. We explain how we use the domain model to handle metonymy
dynamically, and more generally, to underlie semantic composition, using the
knowledge descriptions attached to each concept of our ontology as a kind of
concept-level, multiple-role qualia structure.
  We rely for this on a heuristic path search algorithm that exploits the
graphic aspects of the conceptual graphs formalism. The methods described have
been implemented and applied on French texts in the medical domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604017</id><created>1996-04-26</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author></authors><title>Fast Parsing using Pruning and Grammar Specialization</title><categories>cmp-lg cs.CL</categories><comments>8 pages LaTeX, ACL-96, needs aclap.sty; see also
  http://www.cam.sri.com/</comments><report-no>CRC-060</report-no><abstract>  We show how a general grammar may be automatically adapted for fast parsing
of utterances from a specific domain by means of constituent pruning and
grammar specialization based on explanation-based learning. These methods
together give an order of magnitude increase in speed, and the coverage loss
entailed by grammar specialization is reduced to approximately half that
reported in previous work. Experiments described here suggest that the loss of
coverage has been reduced to the point where it no longer causes significant
performance degradation in the context of a real application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604018</id><created>1996-04-28</created><authors><author><keyname>Bruce</keyname><forenames>Rebecca</forenames><affiliation>Southern Methodist University</affiliation></author><author><keyname>Wiebe</keyname><forenames>Janyce</forenames><affiliation>New Mexico State University</affiliation></author><author><keyname>Pedersen</keyname><forenames>Ted</forenames><affiliation>Southern Methodist University</affiliation></author></authors><title>The Measure of a Model</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uuencoded compressed postscript file</comments><journal-ref>In Proceedings of the Empirical Methods in Natural Language
  Processing Conference, May 1996, Philadelphia, PA</journal-ref><abstract>  This paper describes measures for evaluating the three determinants of how
well a probabilistic classifier performs on a given test set. These
determinants are the appropriateness, for the test set, of the results of (1)
feature selection, (2) formulation of the parametric form of the model, and (3)
parameter estimation. These are part of any model formulation procedure, even
if not broken out as separate steps, so the tradeoffs explored in this paper
are relevant to a wide variety of methods. The measures are demonstrated in a
large experiment, in which they are used to analyze the results of roughly 300
classifiers that perform word-sense disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604019</id><created>1996-04-29</created><authors><author><keyname>Minnen</keyname><forenames>Guido</forenames><affiliation>SFB 340, Univ. of Tuebingen</affiliation></author></authors><title>Magic for Filter Optimization in Dynamic Bottom-up Processing</title><categories>cmp-lg cs.CL</categories><comments>8 pages LaTeX (uses aclap.sty)</comments><journal-ref>Proceedings of ACL 96, Santa Cruz, USA, June 23-28</journal-ref><abstract>  Off-line compilation of logic grammars using Magic allows an incorporation of
filtering into the logic underlying the grammar. The explicit definite clause
characterization of filtering resulting from Magic compilation allows processor
independent and logically clean optimizations of dynamic bottom-up processing
with respect to goal-directedness. Two filter optimizations based on the
program transformation technique of Unfolding are discussed which are of
practical and theoretical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604020</id><created>1996-04-29</created><authors><author><keyname>Hoffman</keyname><forenames>Beryl</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Translating into Free Word Order Languages</title><categories>cmp-lg cs.CL</categories><comments>COLING 1996, latex, 6 pages, colap = aclap = eaclap.sty</comments><abstract>  In this paper, I discuss machine translation of English text into Turkish, a
relatively ``free'' word order language. I present algorithms that determine
the topic and the focus of each target sentence (using salience (Centering
Theory), old vs. new information, and contrastiveness in the discourse model)
in order to generate the contextually appropriate word orders in the target
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604021</id><created>1996-04-29</created><updated>1996-05-09</updated><authors><author><keyname>Dymetman</keyname><forenames>Marc</forenames><affiliation>Rank Xerox Research Centre, Grenoble</affiliation></author><author><keyname>Copperman</keyname><forenames>Max</forenames><affiliation>Rank Xerox Research Centre, Grenoble</affiliation></author></authors><title>Extended Dependency Structures and their Formal Interpretation</title><categories>cmp-lg cs.CL</categories><comments>uuencoded gz-compressed .tar file created by csh script uufiles. 17
  pages. To appear in Proceedings of Coling-96. (Change from original
  submission: increased portability)</comments><abstract>  We describe two ``semantically-oriented'' dependency-structure formalisms,
U-forms and S-forms. U-forms have been previously used in machine translation
as interlingual representations, but without being provided with a formal
interpretation. S-forms, which we introduce in this paper, are a scoped version
of U-forms, and we define a compositional semantics mechanism for them. Two
types of semantic composition are basic: complement incorporation and modifier
incorporation. Binding of variables is done at the time of incorporation,
permitting much flexibility in composition order and a simple account of the
semantic effects of permuting several incorporations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604022</id><created>1996-04-30</created><authors><author><keyname>Mikheev</keyname><forenames>Andrei</forenames><affiliation>HCRC, Edinburgh University</affiliation></author></authors><title>Unsupervised Learning of Word-Category Guessing Rules</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX (aclap.sty for ACL-96); Proceedings of ACL-96 Santa
  Cruz, USA; also see cmp-lg/9604025</comments><abstract>  Words unknown to the lexicon present a substantial problem to part-of-speech
tagging. In this paper we present a technique for fully unsupervised
statistical acquisition of rules which guess possible parts-of-speech for
unknown words. Three complementary sets of word-guessing rules are induced from
the lexicon and a raw corpus: prefix morphological rules, suffix morphological
rules and ending-guessing rules. The learning was performed on the Brown Corpus
data and rule-sets, with a highly competitive performance, were produced and
compared with the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604023</id><created>1996-04-30</created><authors><author><keyname>Rogers</keyname><forenames>James</forenames><affiliation>Institute for Research in Cognitive Science, University of Pennsylvania</affiliation></author></authors><title>A Model-Theoretic Framework for Theories of Syntax</title><categories>cmp-lg cs.CL</categories><comments>To appear in Proceedings of the 34th Annual Meeting of the
  Association for Computational Linguistics. Uses aclap.sty. 7 Pages</comments><abstract>  A natural next step in the evolution of constraint-based grammar formalisms
from rewriting formalisms is to abstract fully away from the details of the
grammar mechanism---to express syntactic theories purely in terms of the
properties of the class of structures they license. By focusing on the
structural properties of languages rather than on mechanisms for generating or
checking structures that exhibit those properties, this model-theoretic
approach can offer simpler and significantly clearer expression of theories and
can potentially provide a uniform formalization, allowing disparate theories to
be compared on the basis of those properties. We discuss $\LKP$, a monadic
second-order logical framework for such an approach to syntax that has the
distinctive virtue of being superficially expressive---supporting direct
statement of most linguistically significant syntactic properties---but having
well-defined strong generative capacity---languages are definable in $\LKP$ iff
they are strongly context-free. We draw examples from the realms of GPSG and
GB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604024</id><created>1996-04-30</created><authors><author><keyname>Trujillo</keyname><forenames>Arturo</forenames><affiliation>The Robert Gordon University, Aberdeen</affiliation></author><author><keyname>Berry</keyname><forenames>Simon</forenames><affiliation>The Robert Gordon University, Aberdeen</affiliation></author></authors><title>Connectivity in Bag Generation</title><categories>cmp-lg cs.CL</categories><comments>Latex, 6 pages, needs colap.sty. To appear in COLING-96</comments><abstract>  This paper presents a pruning technique which can be used to reduce the
number of paths searched in rule-based bag generators of the type proposed by
\cite{poznanskietal95} and \cite{popowich95}. Pruning the search space in these
generators is important given the computational cost of bag generation. The
technique relies on a connectivity constraint between the semantic indices
associated with each lexical sign in a bag. Testing the algorithm on a range of
sentences shows reductions in the generation time and the number of edges
constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604025</id><created>1996-04-30</created><authors><author><keyname>Mikheev</keyname><forenames>Andrei</forenames><affiliation>HCRC, Edinburgh University</affiliation></author></authors><title>Learning Part-of-Speech Guessing Rules from Lexicon: Extension to
  Non-Concatenative Operations</title><categories>cmp-lg cs.CL</categories><comments>6 pages, LaTeX (colap.sty for COLING-96); to appear in Proceedings of
  COLING-96</comments><abstract>  One of the problems in part-of-speech tagging of real-word texts is that of
unknown to the lexicon words. In Mikheev (ACL-96 cmp-lg/9604022), a technique
for fully unsupervised statistical acquisition of rules which guess possible
parts-of-speech for unknown words was proposed. One of the over-simplification
assumed by this learning technique was the acquisition of morphological rules
which obey only simple concatenative regularities of the main word with an
affix. In this paper we extend this technique to the non-concatenative cases of
suffixation and assess the gain in the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9604026</id><created>1996-04-30</created><authors><author><keyname>Mikheev</keyname><forenames>Andrei</forenames><affiliation>HCRC, Edinburgh University</affiliation></author><author><keyname>Finch</keyname><forenames>Steven</forenames><affiliation>Thomson Technical Labs, Rockville Maryland</affiliation></author></authors><title>Towards a Workbench for Acquisition of Domain Knowledge from Natural
  Language</title><categories>cmp-lg cs.CL</categories><comments>8 pages, compressed postscript; Proceedings of EACL-95 Dublin,
  Ireland</comments><abstract>  In this paper we describe an architecture and functionality of main
components of a workbench for an acquisition of domain knowledge from large
text corpora. The workbench supports an incremental process of corpus analysis
starting from a rough automatic extraction and organization of lexico-semantic
regularities and ending with a computer supported analysis of extracted data
and a semi-automatic refinement of obtained hypotheses. For doing this the
workbench employs methods from computational linguistics, information retrieval
and knowledge engineering. Although the workbench is currently under
implementation some of its components are already implemented and their
performance is illustrated with samples from engineering for a medical domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605001</id><created>1996-05-02</created><authors><author><keyname>Grimley-Evans</keyname><forenames>Edmund</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Kiraz</keyname><forenames>George Anton</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Pulman</keyname><forenames>Stephen G.</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Compiling a Partition-Based Two-Level Formalism</title><categories>cmp-lg cs.CL</categories><comments>Uuencoded gz-compressed .tar file created by csh script uufiles,
  needs colap.sty and psfig (available from the cmp-lg macro library). 6 pages,
  to appear in COLING-96</comments><abstract>  This paper describes an algorithm for the compilation of a two (or more)
level orthographic or phonological rule notation into finite state transducers.
The notation is an alternative to the standard one deriving from Koskenniemi's
work: it is believed to have some practical descriptive advantages, and is
quite widely used, but has a different interpretation. Efficient interpreters
exist for the notation, but until now it has not been clear how to compile to
equivalent automata in a transparent way. The present paper shows how to do
this, using some of the conceptual tools provided by Kaplan and Kay's regular
relations calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605002</id><created>1996-05-02</created><authors><author><keyname>Reiter</keyname><forenames>Ehud</forenames><affiliation>University of Aberdeen, UK</affiliation></author></authors><title>Building Natural-Language Generation Systems</title><categories>cmp-lg cs.CL</categories><comments>Standard LaTeX. A 3-page paper of no research interest, but
  occasionally useful in helping to explain applied NLG to people with little
  knowledge of the field. Presented at the AIPE workshop in Glasgow</comments><abstract>  This is a very short paper that briefly discusses some of the tasks that NLG
systems perform. It is of no research interest, but I have occasionally found
it useful as a way of introducing NLG to potential project collaborators who
know nothing about the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605003</id><created>1996-05-02</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Stefan</forenames><affiliation>Lehrstuhl Computerlinguistik, Humboldt-University Berlin</affiliation></author></authors><title>Yet Another Paper about Partial Verb Phrase Fronting in German</title><categories>cmp-lg cs.CL</categories><comments>COLING 1996, gz-compressed .tar file</comments><abstract>  I describe a very simple HPSG analysis for partial verb phrase fronting. I
will argue that the presented account is more adequate than others made during
the past years because it allows the description of constituents in fronted
positions with their modifier remaining in the non-fronted part of the
sentence. A problem with ill-formed signs that are admitted by all HPSG
accounts for partial verb phrase fronting known so far will be explained and a
solution will be suggested that uses the difference between combinatoric
relations of signs and their representation in word order domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605004</id><created>1996-05-02</created><authors><author><keyname>Gardent</keyname><forenames>Claire</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author></authors><title>Higher-Order Coloured Unification and Natural Language Semantics</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LateX file, uses aclap.sty, To appear in Proceedings of
  ACL96</comments><report-no>CLAUS-76</report-no><abstract>  In this paper, we show that Higher-Order Coloured Unification - a form of
unification developed for automated theorem proving - provides a general theory
for modeling the interface between the interpretation process and other sources
of linguistic, non semantic information. In particular, it provides the general
theory for the Primary Occurrence Restriction which (Dalrymple, Shieber and
Pereira, 1991)'s analysis called for.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605005</id><created>1996-05-02</created><authors><author><keyname>Gardent</keyname><forenames>Claire</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames><affiliation>Universitaet des Saarlandes, Saarbruecken, Germany</affiliation></author></authors><title>Focus and Higher-Order Unification</title><categories>cmp-lg cs.CL</categories><comments>6 pages, Latex file, uses colap.sty, to appear in Proceedings of
  COLING 96</comments><report-no>CLAUS-75</report-no><abstract>  Pulman has shown that Higher--Order Unification (HOU) can be used to model
the interpretation of focus. In this paper, we extend the unification--based
approach to cases which are often seen as a test--bed for focus theory:
utterances with multiple focus operators and second occurrence expressions. We
then show that the resulting analysis favourably compares with two prominent
theories of focus (namely, Rooth's Alternative Semantics and Krifka's
Structured Meanings theory) in that it correctly generates interpretations
which these alternative theories cannot yield. Finally, we discuss the formal
properties of the approach and argue that even though HOU need not terminate,
for the class of unification--problems dealt with in this paper, HOU avoids
this shortcoming and is in fact computationally tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605006</id><created>1996-05-03</created><authors><author><keyname>Blache</keyname><forenames>Philippe</forenames><affiliation>2LC-CNRS, France</affiliation></author><author><keyname>Paquelin</keyname><forenames>Jean-Louis</forenames><affiliation>2LC-CNRS, France</affiliation></author></authors><title>Active Constraints for a Direct Interpretation of HPSG</title><categories>cmp-lg cs.CL</categories><comments>Uuencoded gz-compressed .tar, in Proceedings of TALN-HPSG'96</comments><abstract>  In this paper, we characterize the properties of a direct interpretation of
HPSG and present the advantages of this approach. High-level programming
languages constitute in this perspective an efficient solution: we show how a
multi-paradigm approach, containing in particular constraint logic programming,
offers mechanims close to that of the theory and preserves its fundamental
properties. We take the example of LIFE and describe the implementation of the
main HPSG mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605007</id><created>1996-05-04</created><authors><author><keyname>Azzam</keyname><forenames>Saliha</forenames><affiliation>Sheffield University</affiliation></author></authors><title>Resolving Anaphors in Embedded Sentences</title><categories>cmp-lg cs.CL</categories><comments>7 pages, to appear in Proceedings of ACL 1996. LaTeX source, 1 EPS
  figure, needs aclap.sty</comments><abstract>  We propose an algorithm to resolve anaphors, tackling mainly the problem of
intrasentential antecedents. We base our methodology on the fact that such
antecedents are likely to occur in embedded sentences. Sidner's focusing
mechanism is used as the basic algorithm in a more complete approach. The
proposed algorithm has been tested and implemented as a part of a conceptual
analyser, mainly to process pronouns. Details of an evaluation are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605008</id><created>1996-05-05</created><authors><author><keyname>Hakkani</keyname><forenames>Dilek Zeynep</forenames></author><author><keyname>Oflazer</keyname><forenames>Kemal</forenames></author><author><keyname>Cicekli</keyname><forenames>Ilyas</forenames></author></authors><title>Tactical Generation in a Free Constituent Order Language</title><categories>cmp-lg cs.CL</categories><comments>gzipped, uuencoded postscript file</comments><journal-ref>Proceedings of 1996 International Workshop on Natural Language
  Generation</journal-ref><abstract>  This paper describes tactical generation in Turkish, a free constituent order
language, in which the order of the constituents may change according to the
information structure of the sentences to be generated. In the absence of any
information regarding the information structure of a sentence (i.e., topic,
focus, background, etc.), the constituents of the sentence obey a default
order, but the order is almost freely changeable, depending on the constraints
of the text flow or discourse. We have used a recursively structured finite
state machine for handling the changes in constituent order, implemented as a
right-linear grammar backbone. Our implementation environment is the GenKit
system, developed at Carnegie Mellon University--Center for Machine
Translation. Morphological realization has been implemented using an external
morphological analysis/generation component which performs concrete morpheme
selection and handles morphographemic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605009</id><created>1996-05-05</created><updated>1996-07-11</updated><authors><author><keyname>Karov</keyname><forenames>Yael</forenames><affiliation>The Weizmann Institute of Science</affiliation></author><author><keyname>Edelman</keyname><forenames>Shimon</forenames><affiliation>The Weizmann Institute of Science</affiliation></author></authors><title>Learning similarity-based word sense disambiguation from sparse data</title><categories>cmp-lg cs.CL</categories><comments>To appear in the Fourth Workshop on Very Large Corpora, 1996,
  Copenhagen. 18 pages. (revised, format change only)</comments><report-no>Weizmann CS-TR 96-05, March 1996</report-no><abstract>  We describe a method for automatic word sense disambiguation using a text
corpus and a machine-readable dictionary (MRD). The method is based on word
similarity and context similarity measures. Words are considered similar if
they appear in similar contexts; contexts are similar if they contain similar
words. The circularity of this definition is resolved by an iterative,
converging process, in which the system learns from the corpus a set of typical
usages for each of the senses of the polysemous word listed in the MRD. A new
instance of a polysemous word is assigned the sense associated with the typical
usage most similar to its context. Experiments show that this method performs
well, and can learn even from very sparse training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605010</id><created>1996-05-06</created><authors><author><keyname>Busemann</keyname><forenames>Stephan</forenames><affiliation>DFKI GmbH</affiliation></author></authors><title>Best-First Surface Realization</title><categories>cmp-lg cs.CL</categories><comments>10 pages, LaTeX source, one EPS figure</comments><abstract>  Current work in surface realization concentrates on the use of general,
abstract algorithms that interpret large, reversible grammars. Only little
attention has been paid so far to the many small and simple applications that
require coverage of a small sublanguage at different degrees of sophistication.
The system TG/2 described in this paper can be smoothly integrated with deep
generation processes, it integrates canned text, templates, and context-free
rules into a single formalism, it allows for both textual and tabular output,
and it can be parameterized according to linguistic preferences. These features
are based on suitably restricted production system techniques and on a generic
backtracking regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605011</id><created>1996-05-06</created><updated>1996-05-07</updated><authors><author><keyname>Cremers</keyname><forenames>Crit</forenames></author><author><keyname>Hijzelendoorn</keyname><forenames>Maarten</forenames></author></authors><title>Counting Coordination Categorially</title><categories>cmp-lg cs.CL</categories><comments>7 pages, PostScript</comments><abstract>  This paper presents a way of reducing the complexity of parsing free
coordination. It lives on the Coordinative Count Invariant, a property of
derivable sequences in occurrence-sensitive categorial grammar. This invariant
can be exploited to cut down deterministically the search space for coordinated
sentences to minimal fractions. The invariant is based on inequalities, which
is shown to be the best one can get in the presence of coordination without
proper parsing. It is implemented in a categorial parser for Dutch. Some
results of applying the invariant to the parsing of coordination in this parser
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605012</id><created>1996-05-06</created><authors><author><keyname>Collins</keyname><forenames>Michael</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A New Statistical Parser Based on Bigram Lexical Dependencies</title><categories>cmp-lg cs.CL</categories><comments>8 pages, to appear in Proceedings of ACL 96. Uuencoded gz-compressed
  postscript file created by csh script uufiles</comments><abstract>  This paper describes a new statistical parser which is based on probabilities
of dependencies between head-words in the parse tree. Standard bigram
probability estimation techniques are extended to calculate probabilities of
dependencies between pairs of words. Tests using Wall Street Journal data show
that the method performs at least as well as SPATTER (Magerman 95, Jelinek et
al 94), which has the best published results for a statistical parser on this
task. The simplicity of the approach means the model trains on 40,000 sentences
in under 15 minutes. With a beam search strategy parsing speed can be improved
to over 200 sentences a minute with negligible loss in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605013</id><created>1996-05-12</created><updated>1996-09-26</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author></authors><title>Learning Dependencies between Case Frame Slots</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LaTex source, 2 eps figures, uses colap.sty and epsf.sty ;
  an extended abstract of this paper appeared in Proc. of COLING'96, page 10-15</comments><abstract>  We address the problem of automatically acquiring case frame patterns
(selectional patterns) from large corpus data. In particular, we propose a
method of learning dependencies between case frame slots. We view the problem
of learning case frame patterns as that of learning multi-dimensional discrete
joint distributions, where random variables represent case slots. We then
formalize the dependencies between case slots as the probabilistic dependencies
between these random variables. Since the number of parameters in a
multi-dimensional joint distribution is exponential, it is infeasible to
accurately estimate them in practice. To overcome this difficulty, we settle
with approximating the target joint distribution by the product of low order
component distributions, based on corpus data. In particular we propose to
employ an efficient learning algorithm based on the MDL principle to realize
this task. Our experimental results indicate that for certain classes of verbs,
the accuracy achieved in a disambiguation experiment is improved by using the
acquired knowledge of dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605014</id><created>1996-05-12</created><updated>1996-09-25</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author></authors><title>Clustering Words with the MDL Principle</title><categories>cmp-lg cs.CL</categories><comments>11 pages, LaTex source, 4 eps figures, uses colap.sty and epsf.sty;
  an extended abstract of this paper appeared in Proc. of COLING'96, page 4-9</comments><abstract>  We address the problem of automatically constructing a thesaurus
(hierarchically clustering words) based on corpus data. We view the problem of
clustering words as that of estimating a joint distribution over the Cartesian
product of a partition of a set of nouns and a partition of a set of verbs, and
propose an estimation algorithm using simulated annealing with an energy
function based on the Minimum Description Length (MDL) Principle. We
empirically compared the performance of our method based on the MDL Principle
against that of one based on the Maximum Likelihood Estimator, and found that
the former outperforms the latter. We also evaluated the method by conducting
pp-attachment disambiguation experiments using an automatically constructed
thesaurus. Our experimental results indicate that we can improve accuracy in
disambiguation by using such a thesaurus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605015</id><created>1996-05-10</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Carter</keyname><forenames>David</forenames><affiliation>SRI International, Cambridge</affiliation></author><author><keyname>Bouillon</keyname><forenames>Pierrette</forenames><affiliation>ISSCO, Geneva</affiliation></author></authors><title>Adapting the Core Language Engine to French and Spanish</title><categories>cmp-lg cs.CL</categories><comments>9 pages, aclap.sty; to appear in NLP+IA 96; see also
  http://www.cam.sri.com/</comments><report-no>CRC-061</report-no><abstract>  We describe how substantial domain-independent language-processing systems
for French and Spanish were quickly developed by manually adapting an existing
English-language system, the SRI Core Language Engine. We explain the
adaptation process in detail, and argue that it provides a fairly general
recipe for converting a grammar-based system for English into a corresponding
one for a Romance language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605016</id><created>1996-05-10</created><authors><author><keyname>Doerre</keyname><forenames>Jochen</forenames><affiliation>Univ. Stuttgart</affiliation></author></authors><title>Parsing for Semidirectional Lambek Grammar is NP-Complete</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX source (uses aclap.sty, tree-dvips.{sty,pro})</comments><journal-ref>Proceedings ACL '96 (Santa Cruz)</journal-ref><abstract>  We study the computational complexity of the parsing problem of a variant of
Lambek Categorial Grammar that we call {\em semidirectional}. In
semidirectional Lambek calculus $\SDL$ there is an additional non-directional
abstraction rule allowing the formula abstracted over to appear anywhere in the
premise sequent's left-hand side, thus permitting non-peripheral extraction.
$\SDL$ grammars are able to generate each context-free language and more than
that. We show that the parsing problem for semidirectional Lambek Grammar is
NP-complete by a reduction of the 3-Partition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605017</id><created>1996-05-10</created><authors><author><keyname>Popowich</keyname><forenames>Fred</forenames><affiliation>Simon Fraser University, Canada</affiliation></author></authors><title>A Chart Generator for Shake and Bake Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>12 pages, uuencoded, unix compressed postscript, to appear in the
  Proceedings of AI'96, the 11th Canadian Conference on Artificial Intelligence</comments><report-no>CMPT TR 95-08</report-no><abstract>  A generation algorithm based on an active chart parsing algorithm is
introduced which can be used in conjunction with a Shake and Bake machine
translation system. A concise Prolog implementation of the algorithm is
provided, and some performance comparisons with a shift-reduce based algorithm
are given which show the chart generator is much more efficient for generating
all possible sentences from an input specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605018</id><created>1996-05-13</created><authors><author><keyname>Nederhof</keyname><forenames>Mark-Jan</forenames><affiliation>University of Groningen</affiliation></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames><affiliation>University of Padua</affiliation></author></authors><title>Efficient Tabular LR Parsing</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses aclap.sty</comments><journal-ref>Proceedings ACL '96 (Santa Cruz)</journal-ref><abstract>  We give a new treatment of tabular LR parsing, which is an alternative to
Tomita's generalized LR algorithm. The advantage is twofold. Firstly, our
treatment is conceptually more attractive because it uses simpler concepts,
such as grammar transformations and standard tabulation techniques also know as
chart parsing. Secondly, the static and dynamic complexity of parsing, both in
space and time, is significantly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605019</id><created>1996-05-13</created><authors><author><keyname>Evans</keyname><forenames>David A.</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Zhai</keyname><forenames>Chengxiang</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Noun-Phrase Analysis in Unrestricted Text for Information Retrieval</title><categories>cmp-lg cs.CL</categories><comments>8 pages, gzipped, uuencoded Postscript file, to appear in ACL'96</comments><journal-ref>Proceedings of the 34th Annual Meeting of Association for
  Computational Linguistics, Santa Cruz, California, June 24-28, 1996. 17-24.</journal-ref><abstract>  Information retrieval is an important application area of natural-language
processing where one encounters the genuine challenge of processing large
quantities of unrestricted natural-language text. This paper reports on the
application of a few simple, yet robust and efficient noun-phrase analysis
techniques to create better indexing phrases for information retrieval. In
particular, we describe a hybrid approach to the extraction of meaningful
(continuous or discontinuous) subcompounds from complex noun phrases using both
corpus statistics and linguistic heuristics. Results of experiments show that
indexing based on such extracted subcompounds improves both recall and
precision in an information retrieval system. The noun-phrase analysis
techniques are also potentially useful for book indexing and automatic
thesaurus extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605020</id><created>1996-05-13</created><authors><author><keyname>Nakisa</keyname><forenames>Ramin Charles</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Hahn</keyname><forenames>Ulrike</forenames><affiliation>Oxford University</affiliation></author></authors><title>Where Defaults Don't Help: the Case of the German Plural System</title><categories>cmp-lg cs.CL</categories><comments>In proceedings of the 18th annual meeting of the Cognitive Science
  Society. 6 pages, 4 postscript figures. cmp-lg has trouble handling PS fonts
  under NFSS, so the postscript version is 6 pages in postscript Times Roman
  and the LaTeX source produces 7 pages of Computer Modern. Just add times to
  the \usepackage command if your system can handle it</comments><abstract>  The German plural system has become a focal point for conflicting theories of
language, both linguistic and cognitive. We present simulation results with
three simple classifiers - an ordinary nearest neighbour algorithm, Nosofsky's
`Generalized Context Model' (GCM) and a standard, three-layer backprop network
- predicting the plural class from a phonological representation of the
singular in German. Though these are absolutely `minimal' models, in terms of
architecture and input information, they nevertheless do remarkably well. The
nearest neighbour predicts the correct plural class with an accuracy of 72% for
a set of 24,640 nouns from the CELEX database. With a subset of 8,598
(non-compound) nouns, the nearest neighbour, the GCM and the network score
71.0%, 75.0% and 83.5%, respectively, on novel items. Furthermore, they
outperform a hybrid, `pattern-associator + default rule', model, as proposed by
Marcus et al. (1995), on this data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605021</id><created>1996-05-14</created><authors><author><keyname>Strube</keyname><forenames>Michael</forenames></author><author><keyname>Hahn</keyname><forenames>Udo</forenames></author></authors><title>Functional Centering</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uuencoded compressed PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/acl96.ps.gz)</comments><journal-ref>Proceedings of ACL '96 (Santa Cruz)</journal-ref><abstract>  Based on empirical evidence from a free word order language (German) we
propose a fundamental revision of the principles guiding the ordering of
discourse entities in the forward-looking centers within the centering model.
We claim that grammatical role criteria should be replaced by indicators of the
functional information structure of the utterances, i.e., the distinction
between context-bound and unbound discourse elements. This claim is backed up
by an empirical evaluation of functional centering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605022</id><created>1996-05-14</created><authors><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author></authors><title>Processing Complex Sentences in the Centering Framework</title><categories>cmp-lg cs.CL</categories><comments>3 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/acl96-student.ps.gz)</comments><journal-ref>Proceedings of ACL '96 (Santa Cruz), Student Session</journal-ref><abstract>  We extend the centering model for the resolution of intra-sentential anaphora
and specify how to handle complex sentences. An empirical evaluation indicates
that the functional information structure guides the search for an antecedent
within the sentence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605023</id><created>1996-05-14</created><authors><author><keyname>Dymetman</keyname><forenames>Marc</forenames><affiliation>Rank Xerox Research Centre, Grenoble</affiliation></author></authors><title>A Simple Transformation for Offline-Parsable Grammars and its
  Termination Properties</title><categories>cmp-lg cs.CL</categories><comments>Latex. 5 pages. Appeared in Coling-94 Proceedings</comments><abstract>  We present, in easily reproducible terms, a simple transformation for
offline-parsable grammars which results in a provably terminating parsing
program directly top-down interpretable in Prolog. The transformation consists
in two steps: (1) removal of empty-productions, followed by: (2) left-recursion
elimination. It is related both to left-corner parsing (where the grammar is
compiled, rather than interpreted through a parsing program, and with the
advantage of guaranteed termination in the presence of empty productions) and
to the Generalized Greibach Normal Form for DCGs (with the advantage of
implementation simplicity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605024</id><created>1996-05-14</created><authors><author><keyname>Jordan</keyname><forenames>Pamela W.</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>Using Terminological Knowledge Representation Languages to Manage
  Linguistic Resources</title><categories>cmp-lg cs.CL</categories><comments>3 pages, ACL96 Student Session, uses aclap.sty</comments><journal-ref>Proceedings of ACL 96, Santa Cruz, USA, June 23-28</journal-ref><abstract>  I examine how terminological languages can be used to manage linguistic data
during NL research and development. In particular, I consider the lexical
semantics task of characterizing semantic verb classes and show how the
language can be extended to flag inconsistencies in verb class definitions,
identify the need for new verb classes, and identify appropriate linguistic
hypotheses for a new verb's behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605025</id><created>1996-05-15</created><authors><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author><author><keyname>Markert</keyname><forenames>Katja</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author></authors><title>A Conceptual Reasoning Approach to Textual Ellipsis</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/ecai96.ps.gz)</comments><journal-ref>ECAI '96: Proc. of 12th European Conference on Artificial
  Intelligence. Budapest, Aug 12-16 1996, pp.572-576</journal-ref><abstract>  We present a hybrid text understanding methodology for the resolution of
textual ellipsis. It integrates conceptual criteria (based on the
well-formedness and conceptual strength of role chains in a terminological
knowledge base) and functional constraints reflecting the utterances'
information structure (based on the distinction between context-bound and
unbound discourse elements). The methodological framework for text ellipsis
resolution is the centering model that has been adapted to these constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605026</id><created>1996-05-15</created><authors><author><keyname>Neuhaus</keyname><forenames>Peter</forenames></author><author><keyname>Hahn</keyname><forenames>Udo</forenames></author></authors><title>Trading off Completeness for Efficiency --- The \textsc{ParseTalk}
  Performance Grammar Approach to Real-World Text Parsing</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded gzipped PS file</comments><journal-ref>Proceedings of FLAIRS '96 (Key West)</journal-ref><abstract>  We argue for a performance-based design of natural language grammars and
their associated parsers in order to meet the constraints posed by real-world
natural language understanding. This approach incorporates declarative and
procedural knowledge about language and language use within an object-oriented
specification framework. We discuss several message passing protocols for
real-world text parsing and provide reasons for sacrificing completeness of the
parse in favor of efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605027</id><created>1996-05-15</created><authors><author><keyname>Neuhaus</keyname><forenames>Peter</forenames></author><author><keyname>Hahn</keyname><forenames>Udo</forenames></author></authors><title>Restricted Parallelism in Object-Oriented Lexical Parsing</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded gzipped PS file</comments><journal-ref>Proceedings of COLING '96 (Copenhagen)</journal-ref><abstract>  We present an approach to parallel natural language parsing which is based on
a concurrent, object-oriented model of computation. A depth-first, yet
incomplete parsing algorithm for a dependency grammar is specified and several
restrictions on the degree of its parallelization are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605028</id><created>1996-05-15</created><authors><author><keyname>Boros</keyname><forenames>M.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author><author><keyname>Eckert</keyname><forenames>W.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author><author><keyname>Gallwitz</keyname><forenames>F.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author><author><keyname>Goerz</keyname><forenames>G.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author><author><keyname>Hanrieder</keyname><forenames>G.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author><author><keyname>Niemann</keyname><forenames>H.</forenames><affiliation>Bavarian Research Center for Knowledge-Based Systems &amp; Department of Pattern Recognition, University of Erlangen-Nuremberg, Germany</affiliation></author></authors><title>Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept
  Accuracy</title><categories>cmp-lg cs.CL</categories><comments>4 pages PS, Latex2e source importing 2 eps figures, uses icslp.cls,
  caption.sty, psfig.sty; to appear in the Proceedings of the Fourth
  International Conference on Spoken Language Processing (ICSLP 96)</comments><abstract>  In this paper we describe an approach to automatic evaluation of both the
speech recognition and understanding capabilities of a spoken dialogue system
for train time table information. We use word accuracy for recognition and
concept accuracy for understanding performance judgement. Both measures are
calculated by comparing these modules' output with a correct reference answer.
We report evaluation results for a spontaneous speech corpus with about 10000
utterances. We observed a nearly linear relationship between word accuracy and
concept accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605029</id><created>1996-05-15</created><authors><author><keyname>Abe</keyname><forenames>Naoki</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>Theory NEC Lab., RWCP</affiliation></author></authors><title>Learning Word Association Norms Using Tree Cut Pair Models</title><categories>cmp-lg cs.CL</categories><comments>10 pages, LaTex source, 6 eps figures, uses ml94.sty and epsf.sty; to
  appear in the 13th Int. Conf. on Machine Learning (ICML'96)</comments><abstract>  We consider the problem of learning co-occurrence information between two
word categories, or more in general between two discrete random variables
taking values in a hierarchically classified domain. In particular, we consider
the problem of learning the `association norm' defined by A(x,y)=p(x,
y)/(p(x)*p(y)), where p(x, y) is the joint distribution for x and y and p(x)
and p(y) are marginal distributions induced by p(x, y). We formulate this
problem as a sub-task of learning the conditional distribution p(x|y), by
exploiting the identity p(x|y) = A(x,y)*p(x). We propose a two-step estimation
method based on the MDL principle, which works as follows: It first estimates
p(x) as p1 using MDL, and then estimates p(x|y) for a fixed y by applying MDL
on the hypothesis class of {A * p1 | A \in B} for some given class B of
representations for association norm. The estimation of A is therefore obtained
as a side-effect of a near optimal estimation of p(x|y). We then apply this
general framework to the problem of acquiring case-frame patterns. We assume
that both p(x) and A(x, y) for given y are representable by a model based on a
classification that exists within an existing thesaurus tree as a `cut,' and
hence p(x|y) is represented as the product of a pair of `tree cut models.' We
then devise an efficient algorithm that implements our general strategy. We
tested our method by using it to actually acquire case-frame patterns and
conducted disambiguation experiments using the acquired knowledge. The
experimental results show that our method improves upon existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605030</id><created>1996-05-16</created><authors><author><keyname>Hahn</keyname><forenames>Udo</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author><author><keyname>Strube</keyname><forenames>Michael</forenames><affiliation>Computational Linguistics Lab, Freiburg University, Germany</affiliation></author></authors><title>Incremental Centering and Center Ambiguity</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/cogsci96-center.ps.gz)</comments><journal-ref>CogSci '96: Proc. of 18th Annual Conference of the Cognitive
  Science Society. La Jolla, Ca., Jul 12-15 1996.</journal-ref><abstract>  In this paper, we present a model of anaphor resolution within the framework
of the centering model. The consideration of an incremental processing mode
introduces the need to manage structural ambiguity at the center level. Hence,
the centering framework is further refined to account for local and global
parsing ambiguities which propagate up to the level of center representations,
yielding moderately adapted data structures for the centering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605031</id><created>1996-05-24</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Efficient Algorithms for Parsing the DOP Model? A Reply to Joshua Goodman</title><categories>cmp-lg cs.CL</categories><comments>5 pages, Postscript file</comments><abstract>  This note is a reply to Joshua Goodman's paper &quot;Efficient Algorithms for
Parsing the DOP Model&quot; (Goodman, 1996; cmp-lg/9604008). In his paper, Goodman
makes a number of claims about (my work on) the Data-Oriented Parsing model
(Bod, 1992-1996). This note shows that some of these claims must be mistaken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605032</id><created>1996-05-27</created><authors><author><keyname>Rambow</keyname><forenames>Owen</forenames><affiliation>CoGenTex, Inc</affiliation></author><author><keyname>Satta</keyname><forenames>Giorgio</forenames><affiliation>Padova</affiliation></author></authors><title>Synchronous Models of Language</title><categories>cmp-lg cs.CL</categories><comments>8 pages uuencoded gzipped ps file</comments><abstract>  In synchronous rewriting, the productions of two rewriting systems are paired
and applied synchronously in the derivation of a pair of strings. We present a
new synchronous rewriting system and argue that it can handle certain phenomena
that are not covered by existing synchronous systems. We also prove some
interesting formal/computational properties of our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605033</id><created>1996-05-29</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>Swedish Institute of Computer Science</affiliation></author></authors><title>Notes on LR Parser Design</title><categories>cmp-lg cs.CL</categories><comments>5 pages, uuncoded, gzipped PostScript</comments><journal-ref>Coling 94</journal-ref><abstract>  The design of an LR parser based on interleaving the atomic symbol processing
of a context-free backbone grammar with the full constraints of the underlying
unification grammar is described. The parser employs a set of reduced
constraints derived from the unification grammar in the LR parsing step. Gap
threading is simulated to reduce the applicability of empty productions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605034</id><created>1996-05-29</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>University of the Saarland</affiliation></author></authors><title>Handling Sparse Data by Successive Abstraction</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uuencoded, gzipped PostScript</comments><report-no>CLAUS 69</report-no><journal-ref>Coling 96</journal-ref><abstract>  A general, practical method for handling sparse data that avoids held-out
data and iterative reestimation is derived from first principles. It has been
tested on a part-of-speech tagging task and outperformed (deleted)
interpolation with context-independent weights, even when the latter used a
globally optimal parameter setting determined a posteriori.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605035</id><created>1996-05-29</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>University of the Saarland</affiliation></author></authors><title>Example-Based Optimization of Surface-Generation Tables</title><categories>cmp-lg cs.CL</categories><comments>22 pages, uuencoded, gzipped PostScript</comments><report-no>CLAUS 56</report-no><journal-ref>R. Mitkov and N. Nicolov (eds.) &quot;Recent Advances in Natural
  Language Processing&quot;, vol. 136 of &quot;Current Issues in Linguistic Theory&quot;, John
  Benjamins, Amsterdam, 1996.</journal-ref><abstract>  A method is given that &quot;inverts&quot; a logic grammar and displays it from the
point of view of the logical form, rather than from that of the word string.
LR-compiling techniques are used to allow a recursive-descent generation
algorithm to perform &quot;functor merging&quot; much in the same way as an LR parser
performs prefix merging. This is an improvement on the semantic-head-driven
generator that results in a much smaller search space. The amount of semantic
lookahead can be varied, and appropriate tradeoff points between table size and
resulting nondeterminism can be found automatically. This can be done by
removing all spurious nondeterminism for input sufficiently close to the
examples of a training corpus, and large portions of it for other input, while
preserving completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605036</id><created>1996-05-30</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames><affiliation>Harvard University</affiliation></author></authors><title>Parsing Algorithms and Metrics</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</journal-ref><abstract>  Many different metrics exist for evaluating parsing results, including
Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several
others. However, most parsing algorithms, including the Viterbi algorithm,
attempt to optimize the same metric, namely the probability of getting the
correct labelled tree. By choosing a parsing algorithm appropriate for the
evaluation metric, better performance can be achieved. We present two new
algorithms: the ``Labelled Recall Algorithm,'' which maximizes the expected
Labelled Recall Rate, and the ``Bracketed Recall Algorithm,'' which maximizes
the Bracketed Recall Rate. Experimental results are given, showing that the two
new algorithms have improved performance over the Viterbi algorithm on many
criteria, especially the ones that they optimize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605037</id><created>1996-05-31</created><updated>1996-06-03</updated><authors><author><keyname>Golding</keyname><forenames>Andrew R.</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author><author><keyname>Schabes</keyname><forenames>Yves</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author></authors><title>Combining Trigram-based and Feature-based Methods for Context-Sensitive
  Spelling Correction</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><report-no>TR96-03a</report-no><abstract>  This paper addresses the problem of correcting spelling errors that result in
valid, though unintended words (such as ``peace'' and ``piece'', or ``quiet''
and ``quite'') and also the problem of correcting particular word usage errors
(such as ``amount'' and ``number'', or ``among'' and ``between''). Such
corrections require contextual information and are not handled by conventional
spelling programs such as Unix `spell'. First, we introduce a method called
Trigrams that uses part-of-speech trigrams to encode the context. This method
uses a small number of parameters compared to previous methods based on word
trigrams. However, it is effectively unable to distinguish among words that
have the same part of speech. For this case, an alternative feature-based
method called Bayes performs better; but Bayes is less effective than Trigrams
when the distinction among words depends on syntactic constraints. A hybrid
method called Tribayes is then introduced that combines the best of the
previous two methods. The improvement in performance of Tribayes over its
components is verified experimentally. Tribayes is also compared with the
grammar checker in Microsoft Word, and is found to have substantially higher
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9605038</id><created>1996-06-01</created><authors><author><keyname>Eisner</keyname><forenames>Jason</forenames><affiliation>Univ. of Pennsylvania</affiliation></author></authors><title>Efficient Normal-Form Parsing for Combinatory Categorial Grammar</title><categories>cmp-lg cs.CL</categories><comments>8 pages, LaTeX packaged with three .sty files, also uses cgloss4e.sty</comments><journal-ref>Proceedings of ACL '96 (34th Meeting of the Association for
  Computational Linguistics), Santa Cruz</journal-ref><abstract>  Under categorial grammars that have powerful rules like composition, a simple
n-word sentence can have exponentially many parses. Generating all parses is
inefficient and obscures whatever true semantic ambiguities are in the input.
This paper addresses the problem for a fairly general form of Combinatory
Categorial Grammar, by means of an efficient, correct, and easy to implement
normal-form parsing technique. The parser is proved to find exactly one parse
in each semantic equivalence class of allowable parses; that is, spurious
ambiguity (as carefully defined) is shown to be both safely and completely
eliminated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606001</id><created>1996-06-03</created><authors><author><keyname>Golding</keyname><forenames>Andrew R.</forenames><affiliation>Mitsubishi Electric Research Laboratories</affiliation></author></authors><title>A Bayesian hybrid method for context-sensitive spelling correction</title><categories>cmp-lg cs.CL</categories><comments>15 pages</comments><report-no>TR95-13</report-no><abstract>  Two classes of methods have been shown to be useful for resolving lexical
ambiguity. The first relies on the presence of particular words within some
distance of the ambiguous target word; the second uses the pattern of words and
part-of-speech tags around the target word. These methods have complementary
coverage: the former captures the lexical ``atmosphere'' (discourse topic,
tense, etc.), while the latter captures local syntax. Yarowsky has exploited
this complementarity by combining the two methods using decision lists. The
idea is to pool the evidence provided by the component methods, and to then
solve a target problem by applying the single strongest piece of evidence,
whatever type it happens to be. This paper takes Yarowsky's work as a starting
point, applying decision lists to the problem of context-sensitive spelling
correction. Decision lists are found, by and large, to outperform either
component method. However, it is found that further improvements can be
obtained by taking into account not just the single strongest piece of
evidence, but ALL the available evidence. A new hybrid method, based on
Bayesian classifiers, is presented for doing this, and its performance
improvements are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606002</id><created>1996-06-04</created><authors><author><keyname>Ueberla</keyname><forenames>J. P.</forenames></author><author><keyname>Gransden</keyname><forenames>I. R.</forenames></author></authors><title>Clustered Language Models with Context-Equivalent States</title><categories>cmp-lg cs.CL</categories><comments>3 pages, latex</comments><abstract>  In this paper, a hierarchical context definition is added to an existing
clustering algorithm in order to increase its robustness. The resulting
algorithm, which clusters contexts and events separately, is used to experiment
with different ways of defining the context a language model takes into
account. The contexts range from standard bigram and trigram contexts to part
of speech five-grams. Although none of the models can compete directly with a
backoff trigram, they give up to 9\% improvement in perplexity when
interpolated with a trigram. Moreover, the modified version of the algorithm
leads to a performance increase over the original version of up to 12\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606003</id><created>1996-06-04</created><authors><author><keyname>Light</keyname><forenames>Marc</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Morphological Cues for Lexical Semantics</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</journal-ref><abstract>  Most natural language processing tasks require lexical semantic information.
Automated acquisition of this information would thus increase the robustness
and portability of NLP systems. This paper describes an acquisition method
which makes use of fixed correspondences between derivational affixes and
lexical semantic information. One advantage of this method, and of other
methods that rely only on surface characteristics of language, is that the
necessary input is currently available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606004</id><created>1996-06-04</created><authors><author><keyname>Light</keyname><forenames>Marc</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Classification in Feature-based Default Inheritance Hierarchies</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of KONVENS-94</journal-ref><abstract>  Increasingly, inheritance hierarchies are being used to reduce redundancy in
natural language processing lexicons. Systems that utilize inheritance
hierarchies need to be able to insert words under the optimal set of classes in
these hierarchies. In this paper, we formalize this problem for feature-based
default inheritance hierarchies. Since the problem turns out to be NP-complete,
we present an approximation algorithm for it. We show that this algorithm is
efficient and that it performs well with respect to a number of standard
problems for default inheritance. A prototype implementation has been tested on
lexical hierarchies and it has produced encouraging results. The work presented
here is also relevant to other types of default hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606005</id><created>1996-06-04</created><authors><author><keyname>Ludwig</keyname><forenames>Bernd</forenames><affiliation>University of Erlangen-Nuremberg, Germany Department of Artifical Intelligence</affiliation></author></authors><title>Part-of-Speech-Tagging using morphological information</title><categories>cmp-lg cs.CL</categories><comments>Implementing a morphological part-of-speech-tagger with an
  application to the question of authenticity of the Euripidean Rhesus</comments><abstract>  This paper presents the results of an experiment to decide the question of
authenticity of the supposedly spurious Rhesus - a attic tragedy sometimes
credited to Euripides. The experiment involves use of statistics in order to
test whether significant deviations in the distribution of word categories
between Rhesus and the other works of Euripides can or cannot be found. To
count frequencies of word categories in the corpus, a part-of-speech-tagger for
Greek has been implemented. Some special techniques for reducing the problem of
sparse data are used resulting in an accuracy of ca. 96.6%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606006</id><created>1996-06-06</created><authors><author><keyname>Sarkar</keyname><forenames>Anoop</forenames><affiliation>Dept of Computer and Information Science, University of Pennsylvania</affiliation></author><author><keyname>Joshi</keyname><forenames>Aravind</forenames><affiliation>Dept of Computer and Information Science, University of Pennsylvania</affiliation></author></authors><title>Coordination in Tree Adjoining Grammars: Formalization and
  Implementation</title><categories>cmp-lg cs.CL</categories><comments>6 pages, 16 Postscript figures, uses colap.sty. To appear in the
  proceedings of COLING 1996</comments><abstract>  In this paper we show that an account for coordination can be constructed
using the derivation structures in a lexicalized Tree Adjoining Grammar (LTAG).
We present a notion of derivation in LTAGs that preserves the notion of fixed
constituency in the LTAG lexicon while providing the flexibility needed for
coordination phenomena. We also discuss the construction of a practical parser
for LTAGs that can handle coordination including cases of non-constituent
coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606007</id><created>1996-06-07</created><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames><affiliation>Euskal Herriko Unibertsitatea</affiliation></author><author><keyname>Rigau</keyname><forenames>German</forenames><affiliation>Universitat Politecnica de Catalunya</affiliation></author></authors><title>Word Sense Disambiguation using Conceptual Density</title><categories>cmp-lg cs.CL</categories><comments>Postscript version. 8 pages. To appear in the proceedings of COLING
  1996</comments><abstract>  This paper presents a method for the resolution of lexical ambiguity of nouns
and its automatic evaluation over the Brown Corpus. The method relies on the
use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual
distance among concepts, captured by a Conceptual Density formula developed for
this purpose. This fully automatic method requires no hand coding of lexical
entries, hand tagging of text nor any kind of training process. The results of
the experiments have been automatically evaluated against SemCor, the
sense-tagged version of the Brown Corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606008</id><created>1996-06-07</created><authors><author><keyname>Mela</keyname><forenames>Augusta</forenames><affiliation>Universite Paris-Nord</affiliation></author><author><keyname>Fouquere</keyname><forenames>Christophe</forenames><affiliation>Universite Paris-Nord</affiliation></author></authors><title>Coordination as a Direct Process</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><report-no>LIPN-URA1507</report-no><abstract>  We propose a treatment of coordination based on the concepts of functor,
argument and subcategorization. Its formalization comprises two parts which are
conceptually independent. On one hand, we have extended the feature structure
unification to disjunctive and set values in order to check the compatibility
and the satisfiability of subcategorization requirements by structured
complements. On the other hand, we have considered the conjunction {\em et
(and)} as the head of the coordinate structure, so that coordinate structures
stem simply from the subcategorization specifications of {\em et} and the
general schemata of a head saturation. Both parts have been encoded within HPSG
using the same resource that is the subcategorization and its principle which
we have just extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606009</id><created>1996-06-08</created><authors><author><keyname>Griffith</keyname><forenames>John</forenames><affiliation>University of Tuebingen</affiliation></author></authors><title>Modularizing Contexted Constraints</title><categories>cmp-lg cs.CL</categories><comments>6 pages LaTeX (uses colap.sty)D; To appear in the proceedings of
  COLING 1996</comments><abstract>  This paper describes a method for compiling a constraint-based grammar into a
potentially more efficient form for processing. This method takes dependent
disjunctions within a constraint formula and factors them into non-interacting
groups whenever possible by determining their independence. When a group of
dependent disjunctions is split into smaller groups, an exponential amount of
redundant information is reduced. At runtime, this means that an exponential
amount of processing can be saved as well. Since the performance of an
algorithm for processing constraints with dependent disjunctions is highly
determined by its input, the transformation presented in this paper should
prove beneficial for all such algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606010</id><created>1996-06-10</created><updated>1996-07-03</updated><authors><author><keyname>Prevost</keyname><forenames>Scott</forenames><affiliation>MIT Media Laboratory</affiliation></author></authors><title>An Information Structural Approach to Spoken Language Generation</title><categories>cmp-lg cs.CL</categories><comments>8 pages</comments><abstract>  This paper presents an architecture for the generation of spoken monologues
with contextually appropriate intonation. A two-tiered information structure
representation is used in the high-level content planning and sentence planning
stages of generation to produce efficient, coherent speech that makes certain
discourse relationships, such as explicit contrasts, appropriately salient. The
system is able to produce appropriate intonational patterns that cannot be
generated by other systems which rely solely on word class and given/new
distinctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606011</id><created>1996-06-10</created><authors><author><keyname>Chen</keyname><forenames>Stanley F.</forenames><affiliation>Harvard University</affiliation></author><author><keyname>Goodman</keyname><forenames>Joshua T.</forenames><affiliation>Harvard University</affiliation></author></authors><title>An Empirical Study of Smoothing Techniques for Language Modeling</title><categories>cmp-lg cs.CL</categories><comments>9 pages, LaTeX, uses aclap.sty</comments><journal-ref>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL '96)</journal-ref><abstract>  We present an extensive empirical comparison of several smoothing techniques
in the domain of language modeling, including those described by Jelinek and
Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the
first time how factors such as training data size, corpus (e.g., Brown versus
Wall Street Journal), and n-gram order (bigram versus trigram) affect the
relative performance of these methods, which we measure through the
cross-entropy of test data. In addition, we introduce two novel smoothing
techniques, one a variation of Jelinek-Mercer smoothing and one a very simple
linear interpolation technique, both of which outperform existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606012</id><created>1996-06-11</created><authors><author><keyname>Lua</keyname><forenames>K T</forenames><affiliation>Department of Information Systems and Computer Science National University of Singapore, Singapore</affiliation></author></authors><title>An Efficient Inductive Unsupervised Semantic Tagger</title><categories>cmp-lg cs.CL</categories><comments>uuencoded postscript file. email: cmp-lg/9606012</comments><abstract>  We report our development of a simple but fast and efficient inductive
unsupervised semantic tagger for Chinese words. A POS hand-tagged corpus of
348,000 words is used. The corpus is being tagged in two steps. First, possible
semantic tags are selected from a semantic dictionary(Tong Yi Ci Ci Lin), the
POS and the conditional probability of semantic from POS, i.e., P(S|P). The
final semantic tag is then assigned by considering the semantic tags before and
after the current word and the semantic-word conditional probability P(S|W)
derived from the first step. Semantic bigram probabilities P(S|S) are used in
the second step. Final manual checking shows that this simple but efficient
algorithm has a hit rate of 91%. The tagger tags 142 words per second, using a
120 MHz Pentium running FOXPRO. It runs about 2.3 times faster than a Viterbi
tagger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606013</id><created>1996-06-11</created><updated>1996-06-21</updated><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames><affiliation>University of the Saarland</affiliation></author></authors><title>Relating Turing's Formula and Zipf's Law</title><categories>cmp-lg cs.CL</categories><comments>9 pages, uuencoded, gzipped PostScript; some typos removed</comments><report-no>CLAUS 78</report-no><journal-ref>WVLC-4</journal-ref><abstract>  An asymptote is derived from Turing's local reestimation formula for
population frequencies, and a local reestimation formula is derived from Zipf's
law for the asymptotic behavior of population frequencies. The two are shown to
be qualitatively different asymptotically, but nevertheless to be instances of
a common class of reestimation-formula-asymptote pairs, in which they
constitute the upper and lower bounds of the convergence region of the
cumulative of the frequency function, as rank tends to infinity. The results
demonstrate that Turing's formula is qualitatively different from the various
extensions to Zipf's law, and suggest that it smooths the frequency estimates
towards a geometric distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606014</id><created>1996-06-11</created><authors><author><keyname>Chen</keyname><forenames>Stanley F.</forenames><affiliation>Harvard University</affiliation></author></authors><title>Building Probabilistic Models for Natural Language</title><categories>cmp-lg cs.CL</categories><comments>162 pages, LaTeX, doctoral dissertation</comments><report-no>CRCT TR-02-96</report-no><abstract>  In this thesis, we investigate three problems involving the probabilistic
modeling of language: smoothing n-gram models, statistical grammar induction,
and bilingual sentence alignment. These three problems employ models at three
different levels of language; they involve word-based, constituent-based, and
sentence-based models, respectively. We describe techniques for improving the
modeling of language at each of these levels, and surpass the performance of
existing algorithms for each problem. We approach the three problems using
three different frameworks. We relate each of these frameworks to the Bayesian
paradigm, and show why each framework used was appropriate for the given
problem. Finally, we show how our research addresses two central issues in
probabilistic modeling: the sparse data problem and the problem of inducing
hidden structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606015</identifier>
 <datestamp>2009-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606015</id><created>1996-06-11</created><authors><author><keyname>He</keyname><forenames>Song</forenames><affiliation>Bell Laboratories, Lucent Technologies, Murray Hill, NJ</affiliation></author></authors><title>Stabilizing the Richardson Algorithm by Controlling Chaos</title><categories>cmp-lg chao-dyn comp-gas cond-mat cs.CL nlin.CD nlin.CG</categories><comments>Send email to song@bell-lab.com or song@physics.att.com for uuencoded
  tarred gzipped postscript files for the five figures</comments><doi>10.1063/1.168602</doi><abstract>  By viewing the operations of the Richardson purification algorithm as a
discrete time dynamical process, we propose a method to overcome the
instability of the algorithm by controlling chaos. We present theoretical
analysis and numerical results on the behavior and performance of the
stabilized algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606016</id><created>1996-06-13</created><updated>1996-09-25</updated><authors><author><keyname>Li</keyname><forenames>Hang</forenames><affiliation>C&amp;C Res. Labs., NEC Corporation</affiliation></author></authors><title>A Probabilistic Disambiguation Method Based on Psycholinguistic
  Principles</title><categories>cmp-lg cs.CL</categories><comments>16 pages, LaTex source, 7 eps figures, uses a4.sty and epsf.sty; in
  Proc. of the 4th Workshop on Very Large Corpora (WVLC-4), page 141-154, 1996</comments><abstract>  We address the problem of structural disambiguation in syntactic parsing. In
psycholinguistics, a number of principles of disambiguation have been proposed,
notably the Lexical Preference Rule (LPR), the Right Association Principle
(RAP), and the Attach Low and Parallel Principle (ALPP) (an extension of RAP).
We argue that in order to improve disambiguation results it is necessary to
implement these principles on the basis of a probabilistic methodology. We
define a `three-word probability' for implementing LPR, and a `length
probability' for implementing RAP and ALPP. Furthermore, we adopt the
`back-off' method to combine these two types of probabilities. Our experimental
results indicate our method to be effective, attaining an accuracy of 89.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606017</id><created>1996-06-14</created><authors><author><keyname>Scheler</keyname><forenames>Gabriele</forenames><affiliation>Institut fur Informatik, TU Munchen</affiliation></author></authors><title>With raised eyebrows or the eyebrows raised ? A Neural Network Approach
  to Grammar Checking for Definiteness</title><categories>cmp-lg cs.CL</categories><comments>14 pages</comments><report-no>FKI-215-96</report-no><abstract>  In this paper, we use a feature model of the semantics of plural determiners
to present an approach to grammar checking for definiteness. Using neural
network techniques, a semantics -- morphological category mapping was learned.
We then applied a textual encoding technique to the 125 occurences of the
relevant category in a 10 000 word narrative text and learned a surface --
semantics mapping. By applying the learned generation function to the newly
generated representations, we achieved a correct category assignment in many
cases (87 %). These results are considerably better than a direct surface
categorization approach (54 %), with a baseline (always guessing the dominant
category) of 60 %. It is discussed, how these results could be used in
multilingual NLP applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606018</id><created>1996-06-14</created><authors><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>Bell Laboratories</affiliation></author><author><keyname>Riley</keyname><forenames>Michael</forenames><affiliation>AT&amp;T Research</affiliation></author></authors><title>Compilation of Weighted Finite-State Transducers from Decision Trees</title><categories>cmp-lg cs.CL</categories><journal-ref>34th Annual Meeting of the ACL</journal-ref><abstract>  We report on a method for compiling decision trees into weighted finite-state
transducers. The key assumptions are that the tree predictions specify how to
rewrite symbols from an input string, and the decision at each tree node is
stateable in terms of regular expressions on the input string. Each leaf node
can then be treated as a separate rule where the left and right contexts are
constructable from the decisions made traversing the tree from the root to the
leaf. These rules are compiled into transducers using the weighted rewrite-rule
rule-compilation algorithm described in (Mohri and Sproat, 1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606019</id><created>1996-06-17</created><authors><author><keyname>Sima'an</keyname><forenames>Khalil</forenames><affiliation>Utrecht University</affiliation></author></authors><title>Computational Complexity of Probabilistic Disambiguation by means of
  Tree-Grammars</title><categories>cmp-lg cs.CL</categories><comments>6 pages. To appear in COLING 1996</comments><abstract>  This paper studies the computational complexity of disambiguation under
probabilistic tree-grammars and context-free grammars. It presents a proof that
the following problems are NP-hard: computing the Most Probable Parse (MPP)
from a sentence or from a word-graph, and computing the Most Probable Sentence
(MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph
also holds for Stochastic Context-Free Grammars. Consequently, the existence of
deterministic polynomial-time algorithms for solving these disambiguation
problems is a highly improbable event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606020</id><created>1996-06-17</created><authors><author><keyname>Tesar</keyname><forenames>Bruce</forenames><affiliation>Rutgers University</affiliation></author></authors><title>Computing Optimal Descriptions for Optimality Theory Grammars with
  Context-Free Position Structures</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses aclap.sty. To appear in ACL 1996</comments><abstract>  This paper describes an algorithm for computing optimal structural
descriptions for Optimality Theory grammars with context-free position
structures. This algorithm extends Tesar's dynamic programming approach [Tesar
1994][Tesar 1995] to computing optimal structural descriptions from regular to
context-free structures. The generalization to context-free structures creates
several complications, all of which are overcome without compromising the core
dynamic programming approach. The resulting algorithm has a time complexity
cubic in the length of the input, and is applicable to grammars with universal
constraints that exhibit context-free locality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606021</id><created>1996-06-17</created><authors><author><keyname>Luo</keyname><forenames>Xiaoqiang</forenames><affiliation>Center for Language and Speech Processing, The Johns Hopkins University</affiliation></author><author><keyname>Roukos</keyname><forenames>Salim</forenames><affiliation>IBM T. J. Watson Research Center</affiliation></author></authors><title>An Iterative Algorithm to Build Chinese Language Models</title><categories>cmp-lg cs.CL</categories><abstract>  We present an iterative procedure to build a Chinese language model (LM). We
segment Chinese text into words based on a word-based Chinese language model.
However, the construction of a Chinese LM itself requires word boundaries. To
get out of the chicken-and-egg problem, we propose an iterative procedure that
alternates two operations: segmenting text into words and building an LM.
Starting with an initial segmented corpus and an LM based upon it, we use a
Viterbi-liek algorithm to segment another set of data. Then, we build an LM
based on the second set and use the resulting LM to segment again the first
corpus. The alternating procedure provides a self-organized way for the
segmenter to detect automatically unseen words and correct segmentation errors.
Our preliminary experiment shows that the alternating procedure not only
improves the accuracy of our segmentation, but discovers unseen words
surprisingly well. The resulting word-based LM has a perplexity of 188 for a
general Chinese corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606022</id><created>1996-06-17</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Two Questions about Data-Oriented Parsing</title><categories>cmp-lg cs.CL</categories><comments>16 pages, Postscript; to appear in Proceedings Fourth Workshop on
  Very Large Corpora (WVLC-4)</comments><abstract>  In this paper I present ongoing work on the data-oriented parsing (DOP)
model. In previous work, DOP was tested on a cleaned-up set of analyzed
part-of-speech strings from the Penn Treebank, achieving excellent test
results. This left, however, two important questions unanswered: (1) how does
DOP perform if tested on unedited data, and (2) how can DOP be used for parsing
word strings that contain unknown words? This paper addresses these questions.
We show that parse results on unedited data are worse than on cleaned-up data,
although very competitive if compared to other models. As to the parsing of
word strings, we show that the hardness of the problem does not so much depend
on unknown words, but on previously unseen lexical categories of known words.
We give a novel method for parsing these words by estimating the probabilities
of unknown subtrees. The method is of general interest since it shows that good
performance can be obtained without the use of a part-of-speech tagger. To the
best of our knowledge, our method outperforms other statistical parsers tested
on Penn Treebank word strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606023</id><created>1996-06-18</created><authors><author><keyname>Allen</keyname><forenames>James F.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Miller</keyname><forenames>Bradford W.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Ringger</keyname><forenames>Eric K.</forenames><affiliation>University of Rochester</affiliation></author><author><keyname>Sikorski</keyname><forenames>Teresa</forenames><affiliation>University of Rochester</affiliation></author></authors><title>A Robust System for Natural Spoken Dialogue</title><categories>cmp-lg cs.CL</categories><comments>uuencoded, gzipped PostScript. Includes extra Appendix</comments><journal-ref>Proceedings of the 34th Annual Meeting of the ACL</journal-ref><abstract>  This paper describes a system that leads us to believe in the feasibility of
constructing natural spoken dialogue systems in task-oriented domains. It
specifically addresses the issue of robust interpretation of speech in the
presence of recognition errors. Robustness is achieved by a combination of
statistical error post-correction, syntactically- and semantically-driven
robust parsing, and extensive use of the dialogue context. We present an
evaluation of the system using time-to-completion and the quality of the final
solution that suggests that most native speakers of English can use the system
successfully with virtually no training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606024</id><created>1996-06-18</created><authors><author><keyname>Bod</keyname><forenames>Rens</forenames><affiliation>University of Amsterdam</affiliation></author><author><keyname>Bonnema</keyname><forenames>Remko</forenames><affiliation>University of Amsterdam</affiliation></author><author><keyname>Scha</keyname><forenames>Remko</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>A Data-Oriented Approach to Semantic Interpretation</title><categories>cmp-lg cs.CL</categories><comments>10 pages, Postscript; to appear in Proceedings Workshop on
  Corpus-Oriented Semantic Analysis, ECAI-96, Budapest</comments><abstract>  In Data-Oriented Parsing (DOP), an annotated language corpus is used as a
stochastic grammar. The most probable analysis of a new input sentence is
constructed by combining sub-analyses from the corpus in the most probable way.
This approach has been succesfully used for syntactic analysis, using corpora
with syntactic annotations such as the Penn Treebank. If a corpus with
semantically annotated sentences is used, the same approach can also generate
the most probable semantic interpretation of an input sentence. The present
paper explains this semantic interpretation method, and summarizes the results
of a preliminary experiment. Semantic annotations were added to the syntactic
annotations of most of the sentences of the ATIS corpus. A data-oriented
semantic interpretation algorithm was succesfully tested on this semantically
enriched corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606025</id><created>1996-06-19</created><authors><author><keyname>Hartley</keyname><forenames>Anthony</forenames><affiliation>Language Center, University of Brighton</affiliation></author><author><keyname>Paris</keyname><forenames>Cecile</forenames><affiliation>Information Technology Research Institute</affiliation></author></authors><title>Two Sources of Control over the Generation of Software Instructions</title><categories>cmp-lg cs.CL</categories><comments>8 pages, Latex file -- uses aclap.sty</comments><report-no>ITRI96-02</report-no><abstract>  This paper presents an analysis conducted on a corpus of software
instructions in French in order to establish whether task structure elements
(the procedural representation of the users' tasks) are alone sufficient to
control the grammatical resources of a text generator. We show that the
construct of genre provides a useful additional source of control enabling us
to resolve undetermined cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606026</id><created>1996-06-19</created><authors><author><keyname>Mohri</keyname><forenames>Mehryar</forenames><affiliation>AT&amp;T Research</affiliation></author><author><keyname>Sproat</keyname><forenames>Richard</forenames><affiliation>Bell Laboratories</affiliation></author></authors><title>An Efficient Compiler for Weighted Rewrite Rules</title><categories>cmp-lg cs.CL</categories><journal-ref>34th Annual Meeting of the ACL</journal-ref><abstract>  Context-dependent rewrite rules are used in many areas of natural language
and speech processing. Work in computational phonology has demonstrated that,
given certain conditions, such rewrite rules can be represented as finite-state
transducers (FSTs). We describe a new algorithm for compiling rewrite rules
into FSTs. We show the algorithm to be simpler and more efficient than existing
algorithms. Further, many of our applications demand the ability to compile
weighted rules into weighted FSTs, transducers generalized by providing
transitions with weights. We have extended the algorithm to allow for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606027</id><created>1996-06-20</created><authors><author><keyname>de Marcken</keyname><forenames>Carl</forenames><affiliation>MIT Artificial Intelligence Lab.</affiliation></author></authors><title>Linguistic Structure as Composition and Perturbation</title><categories>cmp-lg cs.CL</categories><comments>7 pages</comments><abstract>  This paper discusses the problem of learning language from unprocessed text
and speech signals, concentrating on the problem of learning a lexicon. In
particular, it argues for a representation of language in which linguistic
parameters like words are built by perturbing a composition of existing
parameters. The power of this representation is demonstrated by several
examples in text segmentation and compression, acquisition of a lexicon from
raw speech, and the acquisition of mappings between text and artificial
representations of meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606028</id><created>1996-06-20</created><authors><author><keyname>Tomuro</keyname><forenames>Noriko</forenames><affiliation>DePaul University</affiliation></author></authors><title>Maximizing Top-down Constraints for Unification-based Systems</title><categories>cmp-lg cs.CL</categories><comments>1 postscript figure, uses psfig.sty and aclap.sty</comments><abstract>  A left-corner parsing algorithm with top-down filtering has been reported to
show very efficient performance for unification-based systems. However, due to
the nontermination of parsing with left-recursive grammars, top-down
constraints must be weakened. In this paper, a general method of maximizing
top-down constraints is proposed. The method provides a procedure to
dynamically compute *restrictor*, a minimum set of features involved in an
infinite loop for every propagation path; thus top-down constraints are
maximally propagated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606029</id><created>1996-06-23</created><authors><author><keyname>Karttunen</keyname><forenames>Lauri</forenames></author></authors><title>Directed Replacement</title><categories>cmp-lg cs.CL</categories><comments>To appear in the Proceedings of ACL-96</comments><abstract>  This paper introduces to the finite-state calculus a family of directed
replace operators. In contrast to the simple replace expression, UPPER -&gt;
LOWER, defined in Karttunen (ACL-95), the new directed version, UPPER @-&gt;
LOWER, yields an unambiguous transducer if the lower language consists of a
single string. It transduces the input string from left to right, making only
the longest possible replacement at each point.
  A new type of replacement expression, UPPER @-&gt; PREFIX ... SUFFIX, yields a
transducer that inserts text around strings that are instances of UPPER. The
symbol ... denotes the matching part of the input which itself remains
unchanged. PREFIX and SUFFIX are regular expressions describing the insertions.
  Expressions of the type UPPER @-&gt; PREFIX ... SUFFIX may be used to compose a
deterministic parser for a ``local grammar'' in the sense of Gross (1989).
Other useful applications of directed replacement include tokenization and
filtering of text streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606030</id><created>1996-06-24</created><authors><author><keyname>Engelson</keyname><forenames>Sean P.</forenames></author><author><keyname>Dagan</keyname><forenames>Ido</forenames></author></authors><title>Minimizing Manual Annotation Cost In Supervised Training From Corpora</title><categories>cmp-lg cs.CL</categories><comments>8 pages, uses epsf.sty and aclap.sty, 6 postscript figures</comments><abstract>  Corpus-based methods for natural language processing often use supervised
training, requiring expensive manual annotation of training corpora. This paper
investigates methods for reducing annotation cost by {\it sample selection}. In
this approach, during training the learning program examines many unlabeled
examples and selects for labeling (annotation) only those that are most
informative at each stage. This avoids redundantly annotating examples that
contribute little new information. This paper extends our previous work on {\it
committee-based sample selection} for probabilistic classifiers. We describe a
family of methods for committee-based sample selection, and report experimental
results for the task of stochastic part-of-speech tagging. We find that all
variants achieve a significant reduction in annotation cost, though their
computational efficiency differs. In particular, the simplest method, which has
no parameters to tune, gives excellent results. We also show that sample
selection yields a significant reduction in the size of the model used by the
tagger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606031</id><created>1996-06-25</created><authors><author><keyname>G&#xf6;rz</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Kesseler</keyname><forenames>Marcus</forenames></author><author><keyname>Spilker</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Weber</keyname><forenames>Hans</forenames></author></authors><title>Research on Architectures for Integrated Speech/Language Systems in
  Verbmobil</title><categories>cmp-lg cs.CL</categories><comments>6 pages, 2 Postscript figures</comments><journal-ref>accepted for COLING 96</journal-ref><abstract>  The German joint research project Verbmobil (VM) aims at the development of a
speech to speech translation system. This paper reports on research done in our
group which belongs to Verbmobil's subproject on system architectures (TP15).
Our specific research areas are the construction of parsers for spontaneous
speech, investigations in the parallelization of parsing and to contribute to
the development of a flexible communication architecture with distributed
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9606032</id><created>1996-06-29</created><authors><author><keyname>Ng</keyname><forenames>Hwee Tou</forenames><affiliation>Defence Science Organisation</affiliation></author><author><keyname>Lee</keyname><forenames>Hian Beng</forenames><affiliation>Defence Science Organisation</affiliation></author></authors><title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An
  Exemplar-Based Approach</title><categories>cmp-lg cs.CL</categories><comments>In Proceedings of ACL96, 8 pages</comments><journal-ref>ACL-96</journal-ref><abstract>  In this paper, we present a new approach for word sense disambiguation (WSD)
using an exemplar-based learning algorithm. This approach integrates a diverse
set of knowledge sources to disambiguate word sense, including part of speech
of neighboring words, morphological form, the unordered set of surrounding
words, local collocations, and verb-object syntactic relation. We tested our
WSD program, named {\sc Lexas}, on both a common data set used in previous
work, as well as on a large sense-tagged corpus that we separately constructed.
{\sc Lexas} achieves a higher accuracy on the common data set, and performs
better than the most frequent heuristic on the highly ambiguous words in the
large corpus tagged with the refined senses of {\sc WordNet}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607001</id><created>1996-07-01</created><authors><author><keyname>Bustamante</keyname><forenames>Flora Ram&#xed;rez</forenames><affiliation>Universidad Carlos III de Madrid</affiliation></author><author><keyname>Le&#xf3;n</keyname><forenames>Fernando S&#xe1;nchez</forenames><affiliation>Universidad Aut&#xf3;noma de Madrid</affiliation></author></authors><title>GramCheck: A Grammar and Style Checker</title><categories>cmp-lg cs.CL</categories><comments>7 pages, LaTeX format, uses colap.sty Published: To appear in
  Proceedings of COLING-96</comments><abstract>  This paper presents a grammar and style checker demonstrator for Spanish and
Greek native writers developed within the project GramCheck. Besides a brief
grammar error typology for Spanish, a linguistically motivated approach to
detection and diagnosis is presented, based on the generalized use of PROLOG
extensions to highly typed unification-based grammars. The demonstrator,
currently including full coverage for agreement errors and certain
head-argument relation issues, also provides correction by means of an
analysis-transfer-synthesis cycle. Finally, future extensions to the current
system are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607002</id><created>1996-07-01</created><authors><author><keyname>Samuelsson</keyname><forenames>Christer</forenames></author><author><keyname>Tapanainen</keyname><forenames>Pasi</forenames></author><author><keyname>Voutilainen</keyname><forenames>Atro</forenames></author></authors><title>Inducing Constraint Grammars</title><categories>cmp-lg cs.CL</categories><comments>10 pages, uuencoded, gzipped PostScript</comments><journal-ref>ICGI-3</journal-ref><abstract>  Constraint Grammar rules are induced from corpora. A simple scheme based on
local information, i.e., on lexical biases and next-neighbour contexts,
extended through the use of barriers, reached 87.3 percent precision (1.12
tags/word) at 98.2 percent recall. The results compare favourably with other
methods that are used for similar tasks although they are by no means as good
as the results achieved using the original hand-written rules developed over
several years time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607003</id><created>1996-07-02</created><authors><author><keyname>Bayer</keyname><forenames>Thomas</forenames><affiliation>Daimler Benz Research and Technology</affiliation></author><author><keyname>Renz</keyname><forenames>Ingrid</forenames><affiliation>Daimler Benz Research and Technology</affiliation></author><author><keyname>Stein</keyname><forenames>Michael</forenames><affiliation>Daimler Benz Research and Technology</affiliation></author><author><keyname>Kressel</keyname><forenames>Ulrich</forenames><affiliation>Daimler Benz Research and Technology</affiliation></author></authors><title>Domain and Language Independent Feature Extraction for Statistical Text
  Categorization</title><categories>cmp-lg cs.CL</categories><comments>12 pages, TeX file, 9 Postscript figures, uses epsf.sty</comments><journal-ref>proceedings of workshop on language engineering for document
  analysis and recognition - ed. by L. Evett and T. Rose, part of the AISB 1996
  Workshop Series, April 96, Sussex University, England, 21-32 (ISBN 0 905
  488628)</journal-ref><abstract>  A generic system for text categorization is presented which uses a
representative text corpus to adapt the processing steps: feature extraction,
dimension reduction, and classification. Feature extraction automatically
learns features from the corpus by reducing actual word forms using statistical
information of the corpus and general linguistic knowledge. The dimension of
feature vector is then reduced by linear transformation keeping the essential
information. The classification principle is a minimum least square approach
based on polynomials. The described system can be readily adapted to new
domains or new languages. In application, the system is reliable, fast, and
processes completely automatically. It is shown that the text categorizer works
successfully both on text generated by document image analysis - DIA and on
ground truth data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607004</id><created>1996-07-02</created><authors><author><keyname>Batliner</keyname><forenames>Anton</forenames></author><author><keyname>Feldhaus</keyname><forenames>Anke</forenames></author><author><keyname>Geissler</keyname><forenames>Stefan</forenames></author><author><keyname>Kiessling</keyname><forenames>Andreas</forenames></author><author><keyname>Kiss</keyname><forenames>Tibor</forenames></author><author><keyname>Kompe</keyname><forenames>Ralf</forenames></author><author><keyname>Noeth</keyname><forenames>Elmar</forenames></author></authors><title>Integrating Syntactic and Prosodic Information for the Efficient
  Detection of Empty Categories</title><categories>cmp-lg cs.CL</categories><comments>To appear in the Proceedings of Coling 1996, Copenhagen. 6 pages</comments><abstract>  We describe a number of experiments that demonstrate the usefulness of
prosodic information for a processing module which parses spoken utterances
with a feature-based grammar employing empty categories. We show that by
requiring certain prosodic properties from those positions in the input where
the presence of an empty category has to be hypothesized, a derivation can be
accomplished more efficiently. The approach has been implemented in the machine
translation project VERBMOBIL and results in a significant reduction of the
work-load for the parser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607005</id><created>1996-07-03</created><authors><author><keyname>Alshawi</keyname><forenames>Hiyan</forenames><affiliation>AT&amp;T Research</affiliation></author></authors><title>Head Automata and Bilingual Tiling: Translation with Minimal
  Representations</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of the 34th Annual Meeting of the Association for
  Computational Linguistics, 167-176, Santa Cruz, California, 1996.</journal-ref><abstract>  We present a language model consisting of a collection of costed
bidirectional finite state automata associated with the head words of phrases.
The model is suitable for incremental application of lexical associations in a
dynamic programming search for optimal dependency tree derivations. We also
present a model and algorithm for machine translation involving optimal
``tiling'' of a dependency tree with entries of a costed bilingual lexicon.
Experimental results are reported comparing methods for assigning cost
functions to these models. We conclude with a discussion of the adequacy of
annotated linguistic strings as representations for machine translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607006</id><created>1996-07-03</created><authors><author><keyname>Alshawi</keyname><forenames>Hiyan</forenames><affiliation>AT&amp;T Research</affiliation></author></authors><title>Head Automata for Speech Translation</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings of ICSLP 96, the Fourth International Conference on
  Spoken Language Processing, Philadelphia, Pennsylvania, 1996.</journal-ref><abstract>  This paper presents statistical language and translation models based on
collections of small finite state machines we call ``head automata''. The
models are intended to capture the lexical sensitivity of N-gram models and
direct statistical translation models, while at the same time taking account of
the hierarchical phrasal structure of language. Two types of head automata are
defined: relational head automata suitable for translation by transfer of
dependency trees, and head transducers suitable for direct recursive lexical
translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607007</id><created>1996-07-05</created><updated>1996-08-12</updated><authors><author><keyname>Kempe</keyname><forenames>Andre</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory, France</affiliation></author><author><keyname>Karttunen</keyname><forenames>Lauri</forenames><affiliation>Rank Xerox Research Centre, Grenoble Laboratory, France</affiliation></author></authors><title>Parallel Replacement in Finite State Calculus</title><categories>cmp-lg cs.CL</categories><comments>6 pages, dvi (+ 1x eps) tar gzip uuencode</comments><journal-ref>COLING-96, Copenhagen DK. August 5, 1996.</journal-ref><abstract>  This paper extends the calculus of regular expressions with new types of
replacement expressions that enhance the expressiveness of the simple replace
operator defined in Karttunen (1995). Parallel replacement allows multiple
replacements to apply simultaneously to the same input without interfering with
each other. We also allow a replacement to be constrained by any number of
alternative contexts. With these enhancements, the general replacement
expressions are more versatile than two-level rules for the description of
complex morphological alternations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607008</id><created>1996-07-08</created><authors><author><keyname>Viegas</keyname><forenames>Evelyne</forenames></author><author><keyname>Onyshkevych</keyname><forenames>Boyan</forenames></author><author><keyname>Raskin</keyname><forenames>Victor</forenames></author><author><keyname>Nirenburg</keyname><forenames>Sergei</forenames></author></authors><title>From Submit to Submitted via Submission: On Lexical Rules in Large-Scale
  Lexicon Acquisition</title><categories>cmp-lg cs.CL</categories><comments>Compressed and encoded postscript file</comments><abstract>  This paper deals with the discovery, representation, and use of lexical rules
(LRs) during large-scale semi-automatic computational lexicon acquisition. The
analysis is based on a set of LRs implemented and tested on the basis of
Spanish and English business- and finance-related corpora. We show that, though
the use of LRs is justified, they do not come cost-free. Semi-automatic output
checking is required, even with blocking and preemtion procedures built in.
Nevertheless, large-scope LRs are justified because they facilitate the
unavoidable process of large-scale semi-automatic lexical acquisition. We also
argue that the place of LRs in the computational process is a complex issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607009</id><created>1996-07-09</created><authors><author><keyname>Dorna</keyname><forenames>Michael</forenames><affiliation>IMS, Stuttgart, Germany</affiliation></author><author><keyname>Emele</keyname><forenames>Martin C.</forenames><affiliation>IMS, Stuttgart, Germany</affiliation></author></authors><title>Semantic-based Transfer</title><categories>cmp-lg cs.CL</categories><comments>6 pages (to appear in Proceedings of COLING '96)</comments><abstract>  This article presents a new semantic-based transfer approach developed and
applied within the Verbmobil Machine Translation project. We give an overview
of the declarative transfer formalism together with its procedural realization.
Our approach is discussed and compared with several other approaches from the
MT literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607010</id><created>1996-07-09</created><authors><author><keyname>Dorna</keyname><forenames>Michael</forenames><affiliation>IMS, Stuttgart, Germany</affiliation></author><author><keyname>Emele</keyname><forenames>Martin C.</forenames><affiliation>IMS, Stuttgart, Germany</affiliation></author></authors><title>Efficient Implementation of a Semantic-based Transfer Approach</title><categories>cmp-lg cs.CL</categories><comments>5 pages (to appear in Proceedings of ECAI '96), uuencoded gzipped PS
  file</comments><abstract>  This article gives an overview of a new semantic-based transfer approach
developed and applied within the Verbmobil Machine Translation project. We
present the declarative transfer formalism and discuss its implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607011</id><created>1996-07-11</created><authors><author><keyname>Takeda</keyname><forenames>Koichi</forenames><affiliation>ToKyo Research Laboratory, IBM, Japan</affiliation></author></authors><title>Pattern-Based Context-Free Grammars for Machine Translation</title><categories>cmp-lg cs.CL</categories><comments>8 pages, latex + aclap.sty</comments><journal-ref>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</journal-ref><abstract>  This paper proposes the use of ``pattern-based'' context-free grammars as a
basis for building machine translation (MT) systems, which are now being
adopted as personal tools by a broad range of users in the cyberspace society.
We discuss major requirements for such tools, including easy customization for
diverse domains, the efficiency of the translation algorithm, and scalability
(incremental improvement in translation quality through user interaction), and
describe how our approach meets these requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607012</id><created>1996-07-11</created><authors><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>U. Tilburg, U. Antwerp</affiliation></author><author><keyname>Zavrel</keyname><forenames>Jakub</forenames><affiliation>U. Tilburg</affiliation></author><author><keyname>Berck</keyname><forenames>Peter</forenames><affiliation>U. Antwerp</affiliation></author><author><keyname>Gillis</keyname><forenames>Steven</forenames><affiliation>U. Antwerp</affiliation></author></authors><title>MBT: A Memory-Based Part of Speech Tagger-Generator</title><categories>cmp-lg cs.CL</categories><comments>14 pages, 2 Postscript figures</comments><journal-ref>Proceedings WVLC, Copenhagen</journal-ref><abstract>  We introduce a memory-based approach to part of speech tagging. Memory-based
learning is a form of supervised learning based on similarity-based reasoning.
The part of speech tag of a word in a particular context is extrapolated from
the most similar cases held in memory. Supervised learning approaches are
useful when a tagged corpus is available as an example of the desired output of
the tagger. Based on such a corpus, the tagger-generator automatically builds a
tagger which is able to tag new text the same way, diminishing development time
for the construction of a tagger considerably. Memory-based tagging shares this
advantage with other statistical or machine learning approaches. Additional
advantages specific to a memory-based approach include (i) the relatively small
tagged corpus size sufficient for training, (ii) incremental learning, (iii)
explanation capabilities, (iv) flexible integration of information in case
representations, (v) its non-parametric nature, (vi) reasonably good results on
unknown words without morphological analysis, and (vii) fast learning and
tagging. In this paper we show that a large-scale application of the
memory-based approach is feasible: we obtain a tagging accuracy that is on a
par with that of known statistical approaches, and with attractive space and
time complexity properties when using {\em IGTree}, a tree-based formalism for
indexing and searching huge case bases.} The use of IGTree has as additional
advantage that optimal context size for disambiguation is dynamically computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607013</id><created>1996-07-11</created><authors><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>U. Tilburg, U. Antwerp</affiliation></author><author><keyname>Berck</keyname><forenames>Peter</forenames><affiliation>U. Antwerp</affiliation></author><author><keyname>Gillis</keyname><forenames>Steven</forenames><affiliation>U. Antwerp</affiliation></author></authors><title>Unsupervised Discovery of Phonological Categories through Supervised
  Learning of Morphological Rules</title><categories>cmp-lg cs.CL</categories><journal-ref>Proceedings COLING 1996, Copenhagen</journal-ref><abstract>  We describe a case study in the application of {\em symbolic machine
learning} techniques for the discovery of linguistic rules and categories. A
supervised rule induction algorithm is used to learn to predict the correct
diminutive suffix given the phonological representation of Dutch nouns. The
system produces rules which are comparable to rules proposed by linguists.
Furthermore, in the process of learning this morphological task, the phonemes
used are grouped into phonologically relevant categories. We discuss the
relevance of our method for linguistics and language technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607014</id><created>1996-07-12</created><authors><author><keyname>Linden</keyname><forenames>Keith Vander</forenames><affiliation>University of Brighton</affiliation></author><author><keyname>Di Eugenio</keyname><forenames>Barbara</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>A Corpus Study of Negative Imperatives in Natural Language Instructions</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses colap.sty and acl.bst</comments><journal-ref>Proceedings of COLING 96 (Copenhagen)</journal-ref><abstract>  In this paper, we define the notion of a preventative expression and discuss
a corpus study of such expressions in instructional text. We discuss our coding
schema, which takes into account both form and function features, and present
measures of inter-coder reliability for those features. We then discuss the
correlations that exist between the function and the form features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607015</id><created>1996-07-12</created><authors><author><keyname>Linden</keyname><forenames>Keith Vander</forenames><affiliation>ITRI, University of Brighton</affiliation></author><author><keyname>Di Eugenio</keyname><forenames>; Barbara</forenames><affiliation>Computational Linguistics, Carnegie Mellon University</affiliation></author></authors><title>Learning Micro-Planning Rules for Preventative Expressions</title><categories>cmp-lg cs.CL</categories><comments>8 pages, 4 Postscript figures, uses colap.sty</comments><journal-ref>INLG96 -- Eight International Natural Language Generation Workshop</journal-ref><abstract>  Building text planning resources by hand is time-consuming and difficult.
Certainly, a number of planning architectures and their accompanying plan
libraries have been implemented, but while the architectures themselves may be
reused in a new domain, the library of plans typically cannot. One way to
address this problem is to use machine learning techniques to automate the
derivation of planning resources for new domains. In this paper, we apply this
technique to build micro-planning rules for preventative expressions in
instructional text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607016</id><created>1996-07-13</created><authors><author><keyname>Pereira</keyname><forenames>Fernando C. N.</forenames><affiliation>AT&amp;T Research</affiliation></author><author><keyname>Singer</keyname><forenames>Yoram</forenames><affiliation>AT&amp;T Research</affiliation></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames><affiliation>Hebrew University</affiliation></author></authors><title>Beyond Word N-Grams</title><categories>cmp-lg cs.CL</categories><comments>15 pages, one PostScript figure, uses psfig.sty and fullname.sty.
  Revised version of a paper in the Proceedings of the Third Workshop on Very
  Large Corpora, MIT, 1995</comments><abstract>  We describe, analyze, and evaluate experimentally a new probabilistic model
for word-sequence prediction in natural language based on prediction suffix
trees (PSTs). By using efficient data structures, we extend the notion of PST
to unbounded vocabularies. We also show how to use a Bayesian approach based on
recursive priors over all possible PSTs to efficiently maintain tree mixtures.
These mixtures have provably and practically better performance than almost any
single model. We evaluate the model on several corpora. The low perplexity
achieved by relatively small PST mixture models suggests that they may be an
advantageous alternative, both theoretically and practically, to the widely
used n-gram models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607017</id><created>1996-07-13</created><authors><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames><affiliation>IBM Research, T. J. Watson Research Center, Yorktown Heights, NY, USA</affiliation></author></authors><title>Natural Language Processing: Structure and Complexity</title><categories>cmp-lg cs.CL</categories><comments>8 pp. Latex (documentstyle[ijcai89,named]). In: &quot;Proc. SEKE'96, 8th
  Int. Conf. on Software Engineering and Knowledge Engineering&quot;, Lake Tahoe,
  1996, pages 595-602</comments><abstract>  We introduce a method for analyzing the complexity of natural language
processing tasks, and for predicting the difficulty new NLP tasks.
  Our complexity measures are derived from the Kolmogorov complexity of a
class of automata --- {\it meaning automata}, whose purpose is to extract
relevant pieces of information from sentences. Natural language semantics
is defined only relative to the set of questions an automaton can answer.
  The paper shows examples of complexity estimates for various NLP programs
and tasks, and some recipes for complexity management. It positions natural
language processing as a subdomain of software engineering, and lays down
its formal foundation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607018</id><created>1996-07-15</created><authors><author><keyname>Lehmann</keyname><forenames>Sabine</forenames><affiliation>ISSCO, University of Geneva</affiliation></author><author><keyname>Oepen</keyname><forenames>Stephan</forenames><affiliation>DFKI, Saarbruecken</affiliation></author><author><keyname>Regnier-Prost</keyname><forenames>Sylvie</forenames><affiliation>Aerospatiale, Suresnes</affiliation></author><author><keyname>Netter</keyname><forenames>Klaus</forenames><affiliation>DFKI</affiliation></author><author><keyname>Lux</keyname><forenames>Veronika</forenames><affiliation>Aerospatiale</affiliation></author><author><keyname>Klein</keyname><forenames>Judith</forenames><affiliation>DFKI</affiliation></author><author><keyname>Falkedal</keyname><forenames>Kirsten</forenames><affiliation>GMS, Berlin</affiliation></author><author><keyname>Fouvry</keyname><forenames>Frederik</forenames><affiliation>University of Essex</affiliation></author><author><keyname>Estival</keyname><forenames>Dominique</forenames><affiliation>University of Melbourne</affiliation></author><author><keyname>Dauphin</keyname><forenames>Eva</forenames><affiliation>Aerospatiale</affiliation></author><author><keyname>Compagnion</keyname><forenames>Herve</forenames><affiliation>ISSCO</affiliation></author><author><keyname>Baur</keyname><forenames>Judith</forenames><affiliation>DFKI</affiliation></author><author><keyname>Baur</keyname><forenames>Judith</forenames><affiliation>DFKI</affiliation></author><author><keyname>Balkan</keyname><forenames>Lorna</forenames><affiliation>University of Essex</affiliation></author><author><keyname>Arnold</keyname><forenames>Doug</forenames><affiliation>University of Essex</affiliation></author></authors><title>TSNLP - Test Suites for Natural Language Processing</title><categories>cmp-lg cs.CL</categories><comments>7 pages, uses colap.sty and oe.sty. tar gzip uuencode. To appear in
  Proceedings of COLING-96</comments><abstract>  The TSNLP project has investigated various aspects of the construction,
maintenance and application of systematic test suites as diagnostic and
evaluation tools for NLP applications. The paper summarizes the motivation and
main results of the project: besides the solid methodological foundation, TSNLP
has produced substantial multi-purpose and multi-user test suites for three
European languages together with a set of specialized tools that facilitate the
construction, extension, maintenance, retrieval, and customization of the test
data. As TSNLP results, including the data and technology, are made publicly
available, the project presents a valuable linguistic resourc e that has the
potential of providing a wide-spread pre-standard diagnostic and evaluation
tool for both developers and users of NLP applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607019</id><created>1996-07-15</created><authors><author><keyname>Bouillon</keyname><forenames>Pierrette</forenames><affiliation>ISSCO, University of Geneva</affiliation></author></authors><title>Mental State Adjectives: the Perspective of Generative Lexicon</title><categories>cmp-lg cs.CL</categories><comments>6 pages, uses colap.sty. tar gzip uuencode. To appear in Proceedings
  of COLING-96</comments><abstract>  This paper focusses on mental state adjectives and offers a unified analysis
in the theory of Generative Lexicon (Pustejovsky, 1991, 1995). We show that,
instead of enumerating the various syntactic constructions they enter into,
with the different senses which arise, it is possible to give them a rich typed
semantic representation which will explain both their semantic and syntactic
polymorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607020</id><created>1996-07-15</created><authors><author><keyname>Shiuan</keyname><forenames>Peh Li</forenames><affiliation>Defence Science Organisation</affiliation></author><author><keyname>Ann</keyname><forenames>Christopher Ting Hian</forenames><affiliation>Defence Science Organisation and National University of Singapore</affiliation></author></authors><title>A Divide-and-Conquer Strategy for Parsing</title><categories>cmp-lg cs.CL</categories><comments>10 pages, 7 Postscript figures, uses fancyheadings.sty, psfig.sty,
  aclap.sty, uufiles package</comments><journal-ref>In Proceedings ACL/SIGPARSE, pp.57-66, 1996.</journal-ref><abstract>  In this paper, we propose a novel strategy which is designed to enhance the
accuracy of the parser by simplifying complex sentences before parsing. This
approach involves the separate parsing of the constituent sub-sentences within
a complex sentence. To achieve that, the divide-and-conquer strategy first
disambiguates the roles of the link words in the sentence and segments the
sentence based on these roles. The separate parse trees of the segmented
sub-sentences and the noun phrases within them are then synthesized to form the
final parse. To evaluate the effects of this strategy on parsing, we compare
the original performance of a dependency parser with the performance when it is
enhanced with the divide-and-conquer strategy. When tested on 600 sentences of
the IPSM'95 data sets, the enhanced parser saw a considerable error reduction
of 21.2% in its accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607021</id><created>1996-07-16</created><authors><author><keyname>Bosch</keyname><forenames>Antal van den</forenames><affiliation>University of Maastricht, the Netherlands</affiliation></author><author><keyname>Daelemans</keyname><forenames>Walter</forenames><affiliation>Tilburg University, the Netherlands</affiliation></author><author><keyname>Weijters</keyname><forenames>Ton</forenames><affiliation>University of Maastricht, the Netherlands</affiliation></author></authors><title>Morphological Analysis as Classification: an Inductive-Learning Approach</title><categories>cmp-lg cs.CL</categories><comments>11 pages, 5 encapsulated postscript figures, uses non-standard NeMLaP
  proceedings style nemlap.sty; inputs ipamacs (international phonetic
  alphabet) and epsf macros</comments><journal-ref>Proceedings of NEMLAP-2</journal-ref><abstract>  Morphological analysis is an important subtask in text-to-speech conversion,
hyphenation, and other language engineering tasks. The traditional approach to
performing morphological analysis is to combine a morpheme lexicon, sets of
(linguistic) rules, and heuristics to find a most probable analysis. In
contrast we present an inductive learning approach in which morphological
analysis is reformulated as a segmentation task. We report on a number of
experiments in which five inductive learning algorithms are applied to three
variations of the task of morphological analysis. Results show (i) that the
generalisation performance of the algorithms is good, and (ii) that the lazy
learning algorithm IB1-IG performs best on all three tasks. We conclude that
lazy learning of morphological analysis as a classification task is indeed a
viable approach; moreover, it has the strong advantages over the traditional
approach of avoiding the knowledge-acquisition bottleneck, being fast and
deterministic in learning and processing, and being language-independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607022</id><created>1996-07-16</created><authors><author><keyname>Andernach</keyname><forenames>Toine</forenames><affiliation>Parlevink Group, Department of Computer Science, University of Twente, The Netherlands</affiliation></author></authors><title>A Machine Learning Approach to the Classification of Dialogue Utterances</title><categories>cmp-lg cs.CL</categories><comments>12 pages, using nemlap.sty, harvard.sty and agsm.bst, to appear in
  Proceedings of NeMLaP-2, Bilkent University, Ankara, Turkey</comments><abstract>  The purpose of this paper is to present a method for automatic classification
of dialogue utterances and the results of applying that method to a corpus.
Superficial features of a set of training utterances (which we will call cues)
are taken as the basis for finding relevant utterance classes and for
extracting rules for assigning these classes to new utterances. Each cue is
assumed to partially contribute to the communicative function of an utterance.
Instead of relying on subjective judgments for the tasks of finding classes and
rules, we opt for using machine learning techniques to guarantee objectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607023</id><created>1996-07-17</created><authors><author><keyname>Lee</keyname><forenames>WonIl</forenames><affiliation>Pohang University of Science and Technology Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Geunbae</forenames><affiliation>Pohang University of Science and Technology Korea</affiliation></author><author><keyname>Lee</keyname><forenames>Jong-Hyeok</forenames><affiliation>Pohang University of Science and Technology Korea</affiliation></author></authors><title>Phonological modeling for continuous speech recognition in Korean</title><categories>cmp-lg cs.CL</categories><comments>5 pages, ACL96 sigphon workshop</comments><abstract>  A new scheme to represent phonological changes during continuous speech
recognition is suggested. A phonological tag coupled with its morphological tag
is designed to represent the conditions of Korean phonological changes. A
pairwise language model of these morphological and phonological tags is
implemented in Korean speech recognition system. Performance of the model is
verified through the TDNN-based speech recognition experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607024</id><created>1996-07-19</created><authors><author><keyname>Golding</keyname><forenames>Andrew R.</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>Applying Winnow to Context-Sensitive Spelling Correction</title><categories>cmp-lg cs.CL</categories><comments>9 pages</comments><abstract>  Multiplicative weight-updating algorithms such as Winnow have been studied
extensively in the COLT literature, but only recently have people started to
use them in applications. In this paper, we apply a Winnow-based algorithm to a
task in natural language: context-sensitive spelling correction. This is the
task of fixing spelling errors that happen to result in valid words, such as
substituting {\it to\/} for {\it too}, {\it casual\/} for {\it causal}, and so
on. Previous approaches to this problem have been statistics-based; we compare
Winnow to one of the more successful such approaches, which uses Bayesian
classifiers. We find that: (1)~When the standard (heavily-pruned) set of
features is used to describe problem instances, Winnow performs comparably to
the Bayesian method; (2)~When the full (unpruned) set of features is used,
Winnow is able to exploit the new features and convincingly outperform Bayes;
and (3)~When a test set is encountered that is dissimilar to the training set,
Winnow is better than Bayes at adapting to the unfamiliar test set, using a
strategy we will present for combining learning on the training set with
unsupervised learning on the (noisy) test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>cmp-lg/9607025</id><created>1996-07-23</created><authors><author><keyname>Cunningham</keyname><forenames>Hamish</forenames></author><author><keyname>Wilks</keyname><forenames>Yorick</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert J.</forenames></author></authors><title>New Methods, Current Trends and Software Infrastructure for NLP</title><categories>cmp-lg cs.CL</categories><comments>12 pages, LaTeX, uses nemlap.sty (included)</comments><journal-ref>Proceedings of NEMLAP-2</journal-ref><abstract>  The increasing use of `new methods' in NLP, which the NeMLaP conference
series exemplifies, occurs in the context of a wider shift in the nature and
concerns of the discipline. This paper begins with a short review of this
context and significant trends in the field. The review motivates and leads to
a set of requirements for support software of general utility for NLP research
and development workers. A freely-available system designed to meet these
requirements is described (called GATE - a General Architecture for Text
Engineering). Information Extraction (IE), in the sense defined by the Message
Understanding Conferences (ARPA \cite{Arp95}), is an NLP application in which
many of the new methods have found a home (Hobbs \cite{Hob93}; Jacobs ed.
\cite{Jac92}). An IE system based on GATE is also available for research
purposes, and this is described. Lastly we review related work.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="93000" completeListSize="102538">1122234|94001</resumptionToken>
</ListRecords>
</OAI-PMH>
