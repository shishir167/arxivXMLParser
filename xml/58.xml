<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T02:06:15Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|57001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4327</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4327</id><created>2014-02-18</created><updated>2014-03-27</updated><authors><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Bagnol</keyname><forenames>Marc</forenames></author></authors><title>Unification and Logarithmic Space</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present an algebraic characterization of the complexity classes Logspace
and NLogspace, using an algebra with a composition law based on unification.
This new bridge between unification and complexity classes is inspired from
proof theory and more specifically linear logic and Geometry of Interaction.
  We show how unification can be used to build a model of computation by means
of specific subalgebras associated to finite permutations groups. We then prove
that whether an observation (the algebraic counterpart of a program) accepts a
word can be decided within logarithmic space. We also show that the
construction can naturally represent pointer machines, an intuitive way of
understanding logarithmic space computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4337</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4337</id><created>2014-02-18</created><authors><author><keyname>Gasperin</keyname><forenames>Anthony</forenames></author><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A note on groups of a family of hyperbolic tessellations</title><categories>cs.FL</categories><comments>31 pages, 9 figures</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the word problem of groups corresponding to
tessellations of the hyperbolic plane. In particular using the Fibonacci
technology developed by the second author we show that groups corresponding to
the pentagrid or the heptagrid are not automatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4338</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4338</id><created>2014-02-18</created><authors><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author><author><keyname>Cr&#x103;ciun</keyname><forenames>Adrian</forenames></author></authors><title>Proof Complexity and the Kneser-Lov\'asz Theorem (I)</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the proof complexity of a class of propositional formulas
expressing a combinatorial principle known as the Kneser-Lov\'{a}sz Theorem.
This is a family of propositional tautologies, indexed by an nonnegative
integer parameter $k$ that generalizes the Pigeonhole Principle (obtained for
$k=1$).
  We show, for all fixed $k$, $2^{\Omega(n)}$ lower bounds on resolution
complexity and exponential lower bounds for bounded depth Frege proofs. These
results hold even for the more restricted class of formulas encoding
Schrijver's strenghtening of the Kneser-Lov\'{a}sz Theorem. On the other hand
for the cases $k=2,3$ (for which combinatorial proofs of the Kneser-Lov\'{a}sz
Theorem are known) we give polynomial size Frege ($k=2$), respectively extended
Frege ($k=3$) proofs. The paper concludes with a brief announcement of the
results (presented in subsequent work) on the proof complexity of the general
case of the Kneser-Lov\'{a}sz theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4343</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4343</id><created>2014-02-18</created><authors><author><keyname>Bonchi&#x15f;</keyname><forenames>Cosmin</forenames></author><author><keyname>Istrate</keyname><forenames>Gabriel</forenames></author></authors><title>Minimum Entropy Submodular Optimization (and Fairness in Cooperative
  Games)</title><categories>cs.DS cs.GT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study minimum entropy submodular optimization, a common generalization of
the minimum entropy set cover problem, studied earlier by Cardinal et al., and
the submodular set cover problem.
  We give a general bound of the approximation performance of the greedy
algorithm using an approach that can be interpreted in terms of a particular
type of biased network flows. As an application we rederive known results for
the Minimum Entropy Set Cover and Minimum Entropy Orientation problems, and
obtain a nontrivial bound for a new problem called the Minimum Entropy Spanning
Tree problem.
  The problem can be applied to (and is partly motivated by) the definition of
worst-case approaches to fairness in concave cooperative games, similar to the
notion of price of anarchy in noncooperative settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4346</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4346</id><created>2014-02-18</created><authors><author><keyname>Liu</keyname><forenames>Jingcheng</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>The Complexity of Ferromagnetic Two-spin Systems with External Fields</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximability of computing the partition function for
ferromagnetic two-state spin systems. The remarkable algorithm by Jerrum and
Sinclair showed that there is a fully polynomial-time randomized approximation
scheme (FPRAS) for the special ferromagnetic Ising model with any given uniform
external field. Later, Goldberg and Jerrum proved that it is #BIS-hard for
Ising model if we allow inconsistent external fields on different nodes. In
contrast to these two results, we prove that for any ferromagnetic two-state
spin systems except the Ising model, there exists a threshold for external
fields beyond which the problem is #BIS-hard, even if the external field is
uniform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4353</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4353</id><created>2014-02-18</created><authors><author><keyname>Blasco-Serrano</keyname><forenames>Ricardo</forenames></author><author><keyname>Thobaben</keyname><forenames>Ragnar</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Communication and Interference Coordination</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of controlling the interference created to an external
observer by a communication processes. We model the interference in terms of
its type (empirical distribution), and we analyze the consequences of placing
constraints on the admissible type. Considering a single interfering link, we
characterize the communication-interference capacity region. Then, we look at a
scenario where the interference is jointly created by two users allowed to
coordinate their actions prior to transmission. In this case, the trade-off
involves communication and interference as well as coordination. We establish
an achievable communication-interference region and show that efficiency is
significantly improved by coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4354</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4354</id><created>2014-02-18</created><authors><author><keyname>Teso</keyname><forenames>Stefano</forenames></author><author><keyname>Sebastiani</keyname><forenames>Roberto</forenames></author><author><keyname>Passerini</keyname><forenames>Andrea</forenames></author></authors><title>Hybrid SRL with Optimization Modulo Theories</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generally speaking, the goal of constructive learning could be seen as, given
an example set of structured objects, to generate novel objects with similar
properties. From a statistical-relational learning (SRL) viewpoint, the task
can be interpreted as a constraint satisfaction problem, i.e. the generated
objects must obey a set of soft constraints, whose weights are estimated from
the data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL)
as a description language, and on MAX-SAT solvers to perform inference. Alas,
FOL is unsuited for con- structive problems where the objects contain a mixture
of Boolean and numerical variables. It is in fact difficult to implement, e.g.
linear arithmetic constraints within the language of FOL. In this paper we
propose a novel class of hybrid SRL methods that rely on Satisfiability Modulo
Theories, an alternative class of for- mal languages that allow to describe,
and reason over, mixed Boolean-numerical objects and constraints. The resulting
methods, which we call Learning Mod- ulo Theories, are formulated within the
structured output SVM framework, and employ a weighted SMT solver as an
optimization oracle to perform efficient in- ference and discriminative max
margin weight learning. We also present a few examples of constructive learning
applications enabled by our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4360</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4360</id><created>2014-02-18</created><updated>2014-12-12</updated><authors><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Rane</keyname><forenames>Shantanu</forenames></author></authors><title>An Elementary Completeness Proof for Secure Two-Party Computation
  Primitives</title><categories>cs.CR cs.IT math.IT</categories><comments>6 pages, extended version of ITW 2014 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the secure two-party computation problem, two parties wish to compute a
(possibly randomized) function of their inputs via an interactive protocol,
while ensuring that neither party learns more than what can be inferred from
only their own input and output. For semi-honest parties and
information-theoretic security guarantees, it is well-known that, if only
noiseless communication is available, only a limited set of functions can be
securely computed; however, if interaction is also allowed over general
communication primitives (multi-input/output channels), there are &quot;complete&quot;
primitives that enable any function to be securely computed. The general set of
complete primitives was characterized recently by Maji, Prabhakaran, and
Rosulek leveraging an earlier specialized characterization by Kilian. Our
contribution in this paper is a simple, self-contained, alternative derivation
using elementary information-theoretic tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4364</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4364</id><created>2014-02-18</created><updated>2014-02-19</updated><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Roselli</keyname><forenames>Vincenzo</forenames></author></authors><title>Morphing Planar Graph Drawings Optimally</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an algorithm for computing a planar morph between any two planar
straight-line drawings of any $n$-vertex plane graph in $O(n)$ morphing steps,
thus improving upon the previously best known $O(n^2)$ upper bound. Further, we
prove that our algorithm is optimal, that is, we show that there exist two
planar straight-line drawings $\Gamma_s$ and $\Gamma_t$ of an $n$-vertex plane
graph $G$ such that any planar morph between $\Gamma_s$ and $\Gamma_t$ requires
$\Omega(n)$ morphing steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4370</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4370</id><created>2014-02-18</created><authors><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Wang</keyname><forenames>Menghui</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>FPTAS for Weighted Fibonacci Gates and Its Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fibonacci gate problems have severed as computation primitives to solve other
problems by holographic algorithm and play an important role in the dichotomy
of exact counting for Holant and CSP frameworks. We generalize them to weighted
cases and allow each vertex function to have different parameters, which is a
much boarder family and #P-hard for exactly counting. We design a fully
polynomial-time approximation scheme (FPTAS) for this generalization by
correlation decay technique. This is the first deterministic FPTAS for
approximate counting in the general Holant framework without a degree bound. We
also formally introduce holographic reduction in the study of approximate
counting and these weighted Fibonacci gate problems serve as computation
primitives for approximate counting. Under holographic reduction, we obtain
FPTAS for other Holant problems and spin problems. One important application is
developing an FPTAS for a large range of ferromagnetic two-state spin systems.
This is the first deterministic FPTAS in the ferromagnetic range for two-state
spin systems without a degree bound. Besides these algorithms, we also develop
several new tools and techniques to establish the correlation decay property,
which are applicable in other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4371</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4371</id><created>2014-02-18</created><authors><author><keyname>Nien</keyname><forenames>Hung</forenames></author><author><keyname>Fessler</keyname><forenames>Jeffrey A.</forenames></author></authors><title>A convergence proof of the split Bregman method for regularized
  least-squares problems</title><categories>math.OC cs.LG stat.ML</categories><comments>11 pages, 3 figures, submitted to SIAM J. Imaging Sci</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. Imaging
Sci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solves
image reconstruction problems with general l1, e.g., total-variation (TV) and
compressed sensing (CS), regularizations by introducing a single variable split
to decouple the data-fitting term and the regularization term, yielding simple
subproblems that are separable (or partially separable) and easy to minimize.
Several convergence proofs have been proposed, and these proofs either impose a
&quot;full column rank&quot; assumption to the split or assume exact updates in all
subproblems. However, these assumptions are impractical in many applications
such as the X-ray computed tomography (CT) image reconstructions, where the
inner least-squares problem usually cannot be solved efficiently due to the
highly shift-variant Hessian. In this paper, we show that when the data-fitting
term is quadratic, the SB method is a convergent alternating direction method
of multipliers (ADMM), and a straightforward convergence proof with inexact
updates is given using [J. Eckstein and D. P. Bertsekas, Mathematical
Programming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SB
method is just a special case of an ADMM algorithm, it seems likely that the
ADMM algorithm will be faster than the SB method if the augmented Largangian
(AL) penalty parameters are selected appropriately. To have a concrete example,
we conduct a convergence rate analysis of the ADMM algorithm using two splits
for image restoration problems with quadratic data-fitting term and
regularization term. According to our analysis, we can show that the two-split
ADMM algorithm can be faster than the SB method if the AL penalty parameter of
the SB method is suboptimal. Numerical experiments were conducted to verify our
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4376</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4376</id><created>2014-02-18</created><updated>2014-06-11</updated><authors><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author></authors><title>On Coloring Resilient Graphs</title><categories>cs.CC cs.DS</categories><comments>Appearing in MFCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new notion of resilience for constraint satisfaction problems,
with the goal of more precisely determining the boundary between NP-hardness
and the existence of efficient algorithms for resilient instances. In
particular, we study $r$-resiliently $k$-colorable graphs, which are those
$k$-colorable graphs that remain $k$-colorable even after the addition of any
$r$ new edges. We prove lower bounds on the NP-hardness of coloring resiliently
colorable graphs, and provide an algorithm that colors sufficiently resilient
graphs. We also analyze the corresponding notion of resilience for $k$-SAT.
This notion of resilience suggests an array of open questions for graph
coloring and other combinatorial problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4380</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4380</id><created>2014-02-18</created><authors><author><keyname>Danso</keyname><forenames>Samuel</forenames></author><author><keyname>Atwell</keyname><forenames>Eric</forenames></author><author><keyname>Johnson</keyname><forenames>Owen</forenames></author></authors><title>A Comparative Study of Machine Learning Methods for Verbal Autopsy Text
  Classification</title><categories>cs.CL</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Science Issues, Volume 10, Issue
  6, No 2, November 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Verbal Autopsy is the record of an interview about the circumstances of an
uncertified death. In developing countries, if a death occurs away from health
facilities, a field-worker interviews a relative of the deceased about the
circumstances of the death; this Verbal Autopsy can be reviewed off-site. We
report on a comparative study of the processes involved in Text Classification
applied to classifying Cause of Death: feature value representation; machine
learning classification algorithms; and feature reduction strategies in order
to identify the suitable approaches applicable to the classification of Verbal
Autopsy text. We demonstrate that normalised term frequency and the standard
TFiDF achieve comparable performance across a number of classifiers. The
results also show Support Vector Machine is superior to other classification
algorithms employed in this research. Finally, we demonstrate the effectiveness
of employing a &quot;locally-semi-supervised&quot; feature reduction strategy in order to
increase performance accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4381</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4381</id><created>2014-02-18</created><authors><author><keyname>Nien</keyname><forenames>Hung</forenames></author><author><keyname>Fessler</keyname><forenames>Jeffrey A.</forenames></author></authors><title>Fast X-ray CT image reconstruction using the linearized augmented
  Lagrangian method with ordered subsets</title><categories>math.OC cs.LG stat.ML</categories><comments>21 pages (including the supplementary material), 12 figures,
  submitted to IEEE Trans. Med. Imag</comments><journal-ref>IEEE Trans. Medical Imaging, 34(2):388-99, Feb. 2015</journal-ref><doi>10.1109/TMI.2014.2358499</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The augmented Lagrangian (AL) method that solves convex optimization problems
with linear constraints has drawn more attention recently in imaging
applications due to its decomposable structure for composite cost functions and
empirical fast convergence rate under weak conditions. However, for problems
such as X-ray computed tomography (CT) image reconstruction and large-scale
sparse regression with &quot;big data&quot;, where there is no efficient way to solve the
inner least-squares problem, the AL method can be slow due to the inevitable
iterative inner updates. In this paper, we focus on solving regularized
(weighted) least-squares problems using a linearized variant of the AL method
that replaces the quadratic AL penalty term in the scaled augmented Lagrangian
with its separable quadratic surrogate (SQS) function, thus leading to a much
simpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM,
for X-ray CT image reconstruction. To further accelerate the proposed
algorithm, we use a second-order recursive system analysis to design a
deterministic downward continuation approach that avoids tedious parameter
tuning and provides fast convergence. Experimental results show that the
proposed algorithm significantly accelerates the &quot;convergence&quot; of X-ray CT
image reconstruction with negligible overhead and greatly reduces the OS
artifacts in the reconstructed image when using many subsets for OS
acceleration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4385</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4385</id><created>2014-02-18</created><updated>2015-01-14</updated><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Estimating the resolution limit of the map equation in community
  detection</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><journal-ref>Phys. Rev. E 91, 012809 (2015)</journal-ref><doi>10.1103/PhysRevE.91.012809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A community detection algorithm is considered to have a resolution limit if
the scale of the smallest modules that can be resolved depends on the size of
the analyzed subnetwork. The resolution limit is known to prevent some
community detection algorithms from accurately identifying the modular
structure of a network. In fact, any global objective function for measuring
the quality of a two-level assignment of nodes into modules must have some sort
of resolution limit or an external resolution parameter. However, it is yet
unknown how the resolution limit affects the so-called map equation, which is
known to be an efficient objective function for community detection. We derive
an analytical estimate and conclude that the resolution limit of the map
equation is set by the total number of links between modules instead of the
total number of links in the full network as for modularity. This mechanism
makes the resolution limit much less restrictive for the map equation than for
modularity, and in practice orders of magnitudes smaller. Furthermore, we argue
that the effect of the resolution limit often results from shoehorning
multi-level modular structures into two-level descriptions. As we show, the
hierarchical map equation effectively eliminates the resolution limit for
networks with nested multi-level modular structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4388</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4388</id><created>2014-02-18</created><authors><author><keyname>Javed</keyname><forenames>Mohammed</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>B. B.</forenames></author></authors><title>Automatic Detection of Font Size Straight from Run Length Compressed
  Text Documents</title><categories>cs.CV</categories><comments>8 Pages</comments><journal-ref>(IJCSIT) International Journal of Computer Science and Information
  Technologies, Vol. 5 (1) , 2014, 818-825</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic detection of font size finds many applications in the area of
intelligent OCRing and document image analysis, which has been traditionally
practiced over uncompressed documents, although in real life the documents
exist in compressed form for efficient storage and transmission. It would be
novel and intelligent if the task of font size detection could be carried out
directly from the compressed data of these documents without decompressing,
which would result in saving of considerable amount of processing time and
space. Therefore, in this paper we present a novel idea of learning and
detecting font size directly from run-length compressed text documents at line
level using simple line height features, which paves the way for intelligent
OCRing and document analysis directly from compressed documents. In the
proposed model, the given mixed-case text documents of different font size are
segmented into compressed text lines and the features extracted such as line
height and ascender height are used to capture the pattern of font size in the
form of a regression line, using which the automatic detection of font size is
done during the recognition stage. The method is experimented with a dataset of
50 compressed documents consisting of 780 text lines of single font size and
375 text lines of mixed font size resulting in an overall accuracy of 99.67%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4410</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4410</id><created>2014-02-18</created><authors><author><keyname>Aghdam</keyname><forenames>Paniz Alipour</forenames></author><author><keyname>Ravanmehr</keyname><forenames>Reza</forenames></author></authors><title>A Novel Approach for Canvas Accessibility Problem in HTML5</title><categories>cs.HC</categories><comments>9 pages,8 Figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol 10,
  2013, Page 116</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canvas is a pixel-based inherently inaccessible element in HTML5.Therefore
web users with vision disabilities cannot benefit from Canvas and its desired
semantics and functionality. Regarding to the Canvas application in designing
interactive graphical user interface, vision-impaired users may miss important
information on web sites. This paper utilizes the content-based image retrieval
(CBIR) technique as well as code mapping embedded in a Firefox extension to
present a novel approach in order to make Canvas interactive user interface
accessible. This extension replaces Canvas with an accessible equivalent HTML
environment. Unlike previously done works on Canvas accessibility, the proposed
approach does not impose any rules on developers and designers during Canvas
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4413</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4413</id><created>2014-02-18</created><authors><author><keyname>Haim</keyname><forenames>Shai</forenames></author><author><keyname>Heule</keyname><forenames>Marijn</forenames></author></authors><title>Towards Ultra Rapid Restarts</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observe a trend regarding restart strategies used in SAT solvers. A few
years ago, most state-of-the-art solvers restarted on average after a few
thousands of backtracks. Currently, restarting after a dozen backtracks results
in much better performance. The main reason for this trend is that heuristics
and data structures have become more restart-friendly. We expect further
continuation of this trend, so future SAT solvers will restart even more
rapidly. Additionally, we present experimental results to support our
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4414</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4414</id><created>2014-02-18</created><updated>2014-05-22</updated><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author><author><keyname>Fauser</keyname><forenames>Bertfried</forenames></author></authors><title>Smooth coalgebra: testing vector analysis</title><categories>math.CT cs.LO math.FA</categories><comments>45 pages, 25 figures; to appear in Math. Struct. in Comp. Sci.; this
  version: affiliations updated, typos corrected</comments><msc-class>18F15, 46M35, 58A05</msc-class><acm-class>F.4.m</acm-class><doi>10.1017/S0960129515000511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Processes are often viewed as coalgebras, with the structure maps specifying
the state transitions. In the simplest case, the state spaces are discrete, and
the structure map simply takes each state to the next states. But the
coalgebraic view is also quite effective for studying processes over structured
state spaces, e.g. measurable, or continuous. In the present paper we consider
coalgebras over manifolds. This means that the captured processes evolve over
state spaces that are not just continuous, but also locally homeomorphic to
Banach spaces, and thus carry a differential structure. Both dynamical systems
and differential forms arise as coalgebras over such state spaces, for two
different endofunctors over manifolds. A duality induced by these two
endofunctors provides a formal underpinning for the informal geometric
intuitions linking differential forms and dynamical systems in the various
practical applications, e.g. in physics. This joint functorial reconstruction
of tangent bundles and cotangent bundles uncovers the universal properties and
a high level view of these fundamental structures, which are implemented rather
intricately in their standard form. The succinct coalgebraic presentation
provides unexpected insights even about the situations as familiar as Newton's
laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4417</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4417</id><created>2014-02-18</created><authors><author><keyname>Malhotra</keyname><forenames>Pankaj</forenames></author><author><keyname>Agarwal</keyname><forenames>Puneet</forenames></author><author><keyname>Shroff</keyname><forenames>Gautam</forenames></author></authors><title>Incremental Entity Resolution from Linked Documents</title><categories>cs.DB cs.IR</categories><comments>15 pages, 8 figures, patented work</comments><report-no>TR-DAIF-2014-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many government applications we often find that information about
entities, such as persons, are available in disparate data sources such as
passports, driving licences, bank accounts, and income tax records. Similar
scenarios are commonplace in large enterprises having multiple customer,
supplier, or partner databases. Each data source maintains different aspects of
an entity, and resolving entities based on these attributes is a well-studied
problem. However, in many cases documents in one source reference those in
others; e.g., a person may provide his driving-licence number while applying
for a passport, or vice-versa. These links define relationships between
documents of the same entity (as opposed to inter-entity relationships, which
are also often used for resolution). In this paper we describe an algorithm to
cluster documents that are highly likely to belong to the same entity by
exploiting inter-document references in addition to attribute similarity. Our
technique uses a combination of iterative graph-traversal, locality-sensitive
hashing, iterative match-merge, and graph-clustering to discover unique
entities based on a document corpus. A unique feature of our technique is that
new sets of documents can be added incrementally while having to re-resolve
only a small subset of a previously resolved entity-document collection. We
present performance and quality results on two data-sets: a real-world database
of companies and a large synthetically generated `population' database. We also
demonstrate benefit of using inter-document references for clustering in the
form of enhanced recall of documents for resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4419</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4419</id><created>2014-02-18</created><updated>2015-02-01</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LJK Laboratoire Jean Kuntzmann</affiliation></author></authors><title>Incremental Majorization-Minimization Optimization with Application to
  Large-Scale Machine Learning</title><categories>math.OC cs.LG stat.ML</categories><comments>to appear in SIAM Journal on Optimization; final author's version</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Majorization-minimization algorithms consist of successively minimizing a
sequence of upper bounds of the objective function. These upper bounds are
tight at the current estimate, and each iteration monotonically drives the
objective function downhill. Such a simple principle is widely applicable and
has been very popular in various scientific fields, especially in signal
processing and statistics. In this paper, we propose an incremental
majorization-minimization scheme for minimizing a large sum of continuous
functions, a problem of utmost importance in machine learning. We present
convergence guarantees for non-convex and convex optimization when the upper
bounds approximate the objective up to a smooth error; we call such upper
bounds &quot;first-order surrogate functions&quot;. More precisely, we study asymptotic
stationary point guarantees for non-convex problems, and for convex ones, we
provide convergence rates for the expected objective function value. We apply
our scheme to composite optimization and obtain a new incremental proximal
gradient algorithm with linear convergence rate for strongly convex functions.
In our experiments, we show that our method is competitive with the state of
the art for solving machine learning problems such as logistic regression when
the number of training samples is large enough, and we demonstrate its
usefulness for sparse estimation with non-convex penalties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4422</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4422</id><created>2014-02-18</created><authors><author><keyname>Varga</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Combinatorial Nullstellensatz modulo prime powers and the Parity
  Argument</title><categories>math.CO cs.CC math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new generalizations of Olson's theorem and of a consequence of
Alon's Combinatorial Nullstellensatz. These enable us to extend some of their
combinatorial applications with conditions modulo primes to conditions modulo
prime powers. We analyze computational search problems corresponding to these
kinds of combinatorial questions and we prove that the problem of finding
degree-constrained subgraphs modulo $2^d$ such as $2^d$-divisible subgraphs and
the search problem corresponding to the Combinatorial Nullstellensatz over
$\mathbb{F}_2$ belong to the complexity class Polynomial Parity Argument (PPA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4423</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4423</id><created>2014-02-18</created><authors><author><keyname>Jamadi</keyname><forenames>Mohammad</forenames></author><author><keyname>Merrikh-Bayat</keyname><forenames>Farshad</forenames></author></authors><title>New Method for Accurate Parameter Estimation of Induction Motors Based
  on Artificial Bee Colony Algorithm</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an effective method for estimating the parameters of
double-cage induction motors by using Artificial Bee Colony (ABC) algorithm.
For this purpose the unknown parameters in the electrical model of asynchronous
machine are calculated such that the sum of the square of differences between
full load torques, starting torques, maximum torques, starting currents, full
load currents, and nominal power factors obtained from model and provided by
manufacturer is minimized. In order to confirm the efficiency of the proposed
method the results are also compared with those achieved by using GA, PSO, and
PAMP. The simulations show that in the problem under consideration ABC
converges considerably faster than other algorithms and the results are as
accurate as PAMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4437</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4437</id><created>2014-02-18</created><updated>2014-05-25</updated><authors><author><keyname>Cohen</keyname><forenames>Taco</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Learning the Irreducible Representations of Commutative Lie Groups</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new probabilistic model of compact commutative Lie groups that
produces invariant-equivariant and disentangled representations of data. To
define the notion of disentangling, we borrow a fundamental principle from
physics that is used to derive the elementary particles of a system from its
symmetries. Our model employs a newfound Bayesian conjugacy relation that
enables fully tractable probabilistic inference over compact commutative Lie
groups -- a class that includes the groups that describe the rotation and
cyclic translation of images. We train the model on pairs of transformed image
patches, and show that the learned invariant representation is highly effective
for classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4442</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4442</id><created>2014-02-18</created><authors><author><keyname>Kateb</keyname><forenames>Donia El</forenames><affiliation>SnT</affiliation></author><author><keyname>Fouquet</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>SnT</affiliation></author><author><keyname>Bourcier</keyname><forenames>Johann</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Traon</keyname><forenames>Yves Le</forenames><affiliation>SnT, CSC</affiliation></author></authors><title>Artificial Mutation inspired Hyper-heuristic for Runtime Usage of
  Multi-objective Algorithms</title><categories>cs.SE cs.NE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years, multi-objective evolutionary algorithms (MOEA) have been
applied to different software engineering problems where many conflicting
objectives have to be optimized simultaneously. In theory, evolutionary
algorithms feature a nice property for runtime optimization as they can provide
a solution in any execution time. In practice, based on a Darwinian inspired
natural selection, these evolutionary algorithms produce many deadborn
solutions whose computation results in a computational resources wastage:
natural selection is naturally slow. In this paper, we reconsider this founding
analogy to accelerate convergence of MOEA, by looking at modern biology
studies: artificial selection has been used to achieve an anticipated specific
purpose instead of only relying on crossover and natural selection (i.e.,
Muller et al [18] research on artificial mutation of fruits with X-Ray).
Putting aside the analogy with natural selection , the present paper proposes
an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial
selective mutation to improve the convergence speed of MOEA. Sputnik leverages
the past history of mutation efficiency to select the most relevant mutations
to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives
on-demand provisioning while considering conflicting performance and cost
objectives. We have conducted experiments to highlight the significant
performance improvement of Sputnik in terms of resolution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4455</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4455</id><created>2014-02-18</created><authors><author><keyname>Mijnders</keyname><forenames>Sid</forenames></author><author><keyname>de Wilde</keyname><forenames>Boris</forenames></author><author><keyname>Heule</keyname><forenames>Marijn</forenames></author></authors><title>Symbiosis of Search and Heuristics for Random 3-SAT</title><categories>cs.DS cs.AI</categories><comments>Proceedings of the Third International Workshop on Logic and Search
  (LaSh 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When combined properly, search techniques can reveal the full potential of
sophisticated branching heuristics. We demonstrate this observation on the
well-known class of random 3-SAT formulae. First, a new branching heuristic is
presented, which generalizes existing work on this class. Much smaller search
trees can be constructed by using this heuristic. Second, we introduce a
variant of discrepancy search, called ALDS. Theoretical and practical evidence
support that ALDS traverses the search tree in a near-optimal order when
combined with the new heuristic. Both techniques, search and heuristic, have
been implemented in the look-ahead solver march. The SAT 2009 competition
results show that march is by far the strongest complete solver on random k-SAT
formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4465</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4465</id><created>2014-02-18</created><authors><author><keyname>van der Tak</keyname><forenames>Peter</forenames></author><author><keyname>Heule</keyname><forenames>Marijn J. H.</forenames></author><author><keyname>Biere</keyname><forenames>Armin</forenames></author></authors><title>Concurrent Cube-and-Conquer</title><categories>cs.DS cs.AI</categories><comments>Third International Workshop on Pragmatics of SAT (PoS 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work introduced the cube-and-conquer technique to solve hard SAT
instances. It partitions the search space into cubes using a lookahead solver.
Each cube is tackled by a conflict-driven clause learning (CDCL) solver.
Crucial for strong performance is the cutoff heuristic that decides when to
switch from lookahead to CDCL. Yet, this offline heuristic is far from ideal.
In this paper, we present a novel hybrid solver that applies the cube and
conquer steps simultaneously. A lookahead and a CDCL solver work together on
each cube, while communication is restricted to synchronization. Our concurrent
cube-and-conquer solver can solve many instances faster than pure lookahead,
pure CDCL and offline cube-and-conquer, and can abort early in favor of a pure
CDCL search if an instance is not suitable for cube-and-conquer techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4466</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4466</id><created>2014-02-18</created><updated>2014-08-15</updated><authors><author><keyname>Kaser</keyname><forenames>Owen</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Compressed bitmap indexes: beyond unions and intersections</title><categories>cs.DB cs.DS</categories><comments>Accepted for publication in Software: Practice and Experience on
  August 14th 2014. Note that arXiv:1402.4073 [cs:DB] is a companion to this
  paper; while they share some text, each contains many results not in the
  other</comments><journal-ref>Software: Practice &amp; Experience 46 (2), 2016</journal-ref><doi>10.1002/spe.2289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed bitmap indexes are used to speed up simple aggregate queries in
databases. Indeed, set operations like intersections, unions and complements
can be represented as logical operations (AND,OR,NOT) that are ideally suited
for bitmaps. However, it is less obvious how to apply bitmaps to more advanced
queries. For example, we might seek products in a store that meet some, but
maybe not all, criteria. Such threshold queries generalize intersections and
unions; they are often used in information-retrieval and data-mining
applications. We introduce new algorithms that are sometimes three orders of
magnitude faster than a naive approach. Our work shows that bitmap indexes are
more broadly applicable than is commonly believed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4467</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4467</id><created>2014-02-18</created><authors><author><keyname>Wecker</keyname><forenames>Dave</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>LIQUi|&gt;: A Software Design Architecture and Domain-Specific Language for
  Quantum Computing</title><categories>quant-ph cs.ET cs.PL</categories><comments>14 pages, 12 figures, comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Languages, compilers, and computer-aided design tools will be essential for
scalable quantum computing, which promises an exponential leap in our ability
to execute complex tasks. LIQUi|&gt; is a modular software architecture designed
to control quantum hardware. It enables easy programming, compilation, and
simulation of quantum algorithms and circuits, and is independent of a specific
quantum architecture. LIQUi|&gt; contains an embedded, domain-specific language
designed for programming quantum algorithms, with F# as the host language. It
also allows the extraction of a circuit data structure that can be used for
optimization, rendering, or translation. The circuit can also be exported to
external hardware and software environments. Two different simulation
environments are available to the user which allow a trade-off between number
of qubits and class of operations. LIQUi|&gt; has been implemented on a wide range
of runtimes as back-ends with a single user front-end. We describe the
significant components of the design architecture and how to express any given
quantum algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4488</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4488</id><created>2014-02-18</created><updated>2014-08-29</updated><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Sharma</keyname><forenames>Ankit</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>Privacy-Preserving Public Information for Sequential Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In settings with incomplete information, players can find it difficult to
coordinate to find states with good social welfare. For example, in financial
settings, if a collection of financial firms have limited information about
each other's strategies, some large number of them may choose the same
high-risk investment in hopes of high returns. While this might be acceptable
in some cases, the economy can be hurt badly if many firms make investments in
the same risky market segment and it fails. One reason why many firms might end
up choosing the same segment is that they do not have information about other
firms' investments (imperfect information may lead to `bad' game states).
Directly reporting all players' investments, however, raises confidentiality
concerns for both individuals and institutions.
  In this paper, we explore whether information about the game-state can be
publicly announced in a manner that maintains the privacy of the actions of the
players, and still suffices to deter players from reaching bad game-states. We
show that in many games of interest, it is possible for players to avoid these
bad states with the help of privacy-preserving, publicly-announced information.
We model behavior of players in this imperfect information setting in two ways
-- greedy and undominated strategic behaviours, and we prove guarantees on
social welfare that certain kinds of privacy-preserving information can help
attain. Furthermore, we design a counter with improved privacy guarantees under
continual observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4508</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4508</id><created>2014-02-18</created><authors><author><keyname>Noureddine</keyname><forenames>Haouari</forenames></author><author><keyname>Samira</keyname><forenames>Moussaoui</forenames></author></authors><title>Efficient Local Density Estimation Strategy for VANETs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local vehicle density estimation is increasingly becoming an essential factor
of many vehicular ad-hoc network applications such as congestion control and
traffic state estimation. This estimation is used to get an approximate number
of neighbors within the transmission range since beacons do not give accurate
accuracy about neighborhood. These is due to the special characteristics of
VANETs such as high mobility, high density variation. To enhance the
performance of these applications, an accurate estimation of the local density
with minimum of overhead is needed. Most of the proposed strategies address the
global traffic density estimation without a big attention on the local density
estimation. This paper proposes an improved approach for local density
estimation in VANETs in terms of accuracy and overhead. The simulation results
showed that our strategy allows an interesting precision of estimation with
acceptable overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4512</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4512</id><created>2014-02-18</created><updated>2014-09-04</updated><authors><author><keyname>Rao</keyname><forenames>Nikhil</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Cox</keyname><forenames>Christopher</forenames></author><author><keyname>Rogers</keyname><forenames>Timothy</forenames></author></authors><title>Classification with Sparse Overlapping Groups</title><categories>cs.LG stat.ML</categories><comments>Tighter result compared to the previous version. Some additional
  details and justification on the problem being solved</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification with a sparsity constraint on the solution plays a central
role in many high dimensional machine learning applications. In some cases, the
features can be grouped together so that entire subsets of features can be
selected or not selected. In many applications, however, this can be too
restrictive. In this paper, we are interested in a less restrictive form of
structured sparse feature selection: we assume that while features can be
grouped according to some notion of similarity, not all features in a group
need be selected for the task at hand. When the groups are comprised of
disjoint sets of features, this is sometimes referred to as the &quot;sparse group&quot;
lasso, and it allows for working with a richer class of models than traditional
group lasso methods. Our framework generalizes conventional sparse group lasso
further by allowing for overlapping groups, an additional flexiblity needed in
many applications and one that presents further challenges. The main
contribution of this paper is a new procedure called Sparse Overlapping Group
(SOG) lasso, a convex optimization program that automatically selects similar
features for classification in high dimensions. We establish model selection
error bounds for SOGlasso classification problems under a fairly general
setting. In particular, the error bounds are the first such results for
classification using the sparse group lasso. Furthermore, the general SOGlasso
bound specializes to results for the lasso and the group lasso, some known and
some new. The SOGlasso is motivated by multi-subject fMRI studies in which
functional activity is classified using brain voxels as features, source
localization problems in Magnetoencephalography (MEG), and analyzing gene
activation patterns in microarray data analysis. Experiments with real and
synthetic data demonstrate the advantages of SOGlasso compared to the lasso and
group lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4515</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4515</id><created>2014-02-18</created><updated>2014-03-06</updated><authors><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author></authors><title>The Power of Duples (in Self-Assembly): It's Not So Hip To Be Square</title><categories>cs.ET cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define the Dupled abstract Tile Assembly Model (DaTAM),
which is a slight extension to the abstract Tile Assembly Model (aTAM) that
allows for not only the standard square tiles, but also &quot;duple&quot; tiles which are
rectangles pre-formed by the joining of two square tiles. We show that the
addition of duples allows for powerful behaviors of self-assembling systems at
temperature 1, meaning systems which exclude the requirement of cooperative
binding by tiles (i.e., the requirement that a tile must be able to bind to at
least 2 tiles in an existing assembly if it is to attach). Cooperative binding
is conjectured to be required in the standard aTAM for Turing universal
computation and the efficient self-assembly of shapes, but we show that in the
DaTAM these behaviors can in fact be exhibited at temperature 1. We then show
that the DaTAM doesn't provide asymptotic improvements over the aTAM in its
ability to efficiently build thin rectangles. Finally, we present a series of
results which prove that the temperature-2 aTAM and temperature-1 DaTAM have
mutually exclusive powers. That is, each is able to self-assemble shapes that
the other can't, and each has systems which cannot be simulated by the other.
Beyond being of purely theoretical interest, these results have practical
motivation as duples have already proven to be useful in laboratory
implementations of DNA-based tiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4525</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4525</id><created>2014-02-18</created><authors><author><keyname>Abeyruwan</keyname><forenames>Saminda</forenames></author><author><keyname>Seekircher</keyname><forenames>Andreas</forenames></author><author><keyname>Visser</keyname><forenames>Ubbo</forenames></author></authors><title>Off-Policy General Value Functions to Represent Dynamic Role Assignments
  in RoboCup 3D Soccer Simulation</title><categories>cs.AI</categories><comments>18 pages, 8 figures</comments><msc-class>97R40</msc-class><acm-class>I.2.11</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Collecting and maintaining accurate world knowledge in a dynamic, complex,
adversarial, and stochastic environment such as the RoboCup 3D Soccer
Simulation is a challenging task. Knowledge should be learned in real-time with
time constraints. We use recently introduced Off-Policy Gradient Descent
algorithms within Reinforcement Learning that illustrate learnable knowledge
representations for dynamic role assignments. The results show that the agents
have learned competitive policies against the top teams from the RoboCup 2012
competitions for three vs three, five vs five, and seven vs seven agents. We
have explicitly used subsets of agents to identify the dynamics and the
semantics for which the agents learn to maximize their performance measures,
and to gather knowledge about different objectives, so that all agents
participate effectively and efficiently within the group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4535</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4535</id><created>2014-02-18</created><updated>2014-04-03</updated><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Han</keyname><forenames>Li</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Sampling and Representation Complexity of Revenue Maximization</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider (approximate) revenue maximization in auctions where the
distribution on input valuations is given via &quot;black box&quot; access to samples
from the distribution. We observe that the number of samples required -- the
sample complexity -- is tightly related to the representation complexity of an
approximately revenue-maximizing auction. Our main results are upper bounds and
an exponential lower bound on these complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4540</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4540</id><created>2014-02-18</created><updated>2014-12-01</updated><authors><author><keyname>Alstott</keyname><forenames>Jeff</forenames></author><author><keyname>Panzarasa</keyname><forenames>Pietro</forenames></author><author><keyname>Rubinov</keyname><forenames>Mikail</forenames></author><author><keyname>Bullmore</keyname><forenames>Ed</forenames></author><author><keyname>Vertes</keyname><forenames>Petra</forenames></author></authors><title>A Unifying Framework for Measuring Weighted Rich Clubs</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 3 figures</comments><journal-ref>Scientific Reports 4, 2014. Article number: 7258</journal-ref><doi>10.1038/srep07258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network analysis can help uncover meaningful regularities in the organization
of complex systems. Among these, rich clubs are a functionally important
property of a variety of social, technological and biological networks. Rich
clubs emerge when nodes that are somehow prominent or 'rich' (e.g., highly
connected) interact preferentially with one another. The identification of rich
clubs is non-trivial, especially in weighted networks, and to this end multiple
distinct metrics have been proposed. Here we describe a unifying framework for
detecting rich clubs which intuitively generalizes various metrics into a
single integrated method. This generalization rests upon the explicit
incorporation of randomized control networks into the measurement process. We
apply this framework to real-life examples, and show that, depending on the
selection of randomized controls, different kinds of rich-club structures can
be detected, such as topological and weighted rich clubs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4542</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4542</id><created>2014-02-18</created><authors><author><keyname>Li</keyname><forenames>Chun-Guo</forenames></author><author><keyname>Mei</keyname><forenames>Xing</forenames></author><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author></authors><title>Unsupervised Ranking of Multi-Attribute Objects Based on Principal
  Curves</title><categories>cs.LG cs.AI stat.ML</categories><comments>This paper has 14 pages and 9 figures. The paper has submitted to
  IEEE Transactions on Knowledge and Data Engineering (TKDE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised ranking faces one critical challenge in evaluation applications,
that is, no ground truth is available. When PageRank and its variants show a
good solution in related subjects, they are applicable only for ranking from
link-structure data. In this work, we focus on unsupervised ranking from
multi-attribute data which is also common in evaluation tasks. To overcome the
challenge, we propose five essential meta-rules for the design and assessment
of unsupervised ranking approaches: scale and translation invariance, strict
monotonicity, linear/nonlinear capacities, smoothness, and explicitness of
parameter size. These meta-rules are regarded as high level knowledge for
unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a
ranking principal curve (RPC) model, which learns a one-dimensional manifold
function to perform unsupervised ranking tasks on multi-attribute observations.
Furthermore, the RPC is modeled to be a cubic B\'ezier curve with control
points restricted in the interior of a hypercube, thereby complying with all
the five meta-rules to infer a reasonable ranking list. With control points as
the model parameters, one is able to understand the learned manifold and to
interpret the ranking list semantically. Numerical experiments of the presented
RPC model are conducted on two open datasets of different ranking applications.
In comparison with the state-of-the-art approaches, the new model is able to
show more reasonable ranking lists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4543</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4543</id><created>2014-02-18</created><authors><author><keyname>Zhu</keyname><forenames>Dengkui</forenames></author><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Liang</keyname><forenames>Ping</forenames></author></authors><title>Normalized Volume of Hyperball in Complex Grassmann Manifold and Its
  Application in Large-Scale MU-MIMO Communication Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a solution to a critical issue in large-scale Multi-User
Multiple-Input Multiple-Output (MU-MIMO) communication systems: how to estimate
the Signal-to-Interference-plus-Noise-Ratios (SINRs) and their expectations in
MU-MIMO mode at the Base Station (BS) side when only the Channel Quality
Information (CQI) in Single-User MIMO (SU-MIMO) mode and non-ideal Channel
State Information (CSI) are known? A solution to this problem would be very
beneficial for the BS to predict the capacity of MU-MIMO and choose the proper
modulation and channel coding for MU-MIMO. To that end, this paper derives a
normalized volume formula of a hyperball based on the probability density
function of the canonical angle between any two points in a complex Grassmann
manifold, and shows that this formula provides a solution to the aforementioned
issue. It enables the capability of a BS to predict the capacity loss due to
non-ideal CSI, group users in MU-MIMO mode, choose the proper modulation and
channel coding, and adaptively switch between SU-MIMO and MU-MIMO modes, as
well as between Conjugate Beamforming (CB) and Zero-Forcing (ZF) precoding.
Numerical results are provided to verify the validity and accuracy of the
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4556</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4556</id><created>2014-02-18</created><authors><author><keyname>Yin</keyname><forenames>Yitong</forenames></author></authors><title>Spatial Mixing of Coloring Random Graphs</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the strong spatial mixing (decay of correlation) property of proper
$q$-colorings of random graph $G(n, d/n)$ with a fixed $d$. The strong spatial
mixing of coloring and related models have been extensively studied on graphs
with bounded maximum degree. However, for typical classes of graphs with
bounded average degree, such as $G(n, d/n)$, an easy counterexample shows that
colorings do not exhibit strong spatial mixing with high probability.
Nevertheless, we show that for $q\ge\alpha d+\beta$ with $\alpha&gt;2$ and
sufficiently large $\beta=O(1)$, with high probability proper $q$-colorings of
random graph $G(n, d/n)$ exhibit strong spatial mixing with respect to an
arbitrarily fixed vertex. This is the first strong spatial mixing result for
colorings of graphs with unbounded maximum degree. Our analysis of strong
spatial mixing establishes a block-wise correlation decay instead of the
standard point-wise decay, which may be of interest by itself, especially for
graphs with unbounded degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4566</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4566</id><created>2014-02-19</created><updated>2014-03-17</updated><authors><author><keyname>De</keyname><forenames>Jaydeep</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaowei</forenames></author><author><keyname>Cheng</keyname><forenames>Li</forenames></author></authors><title>Transduction on Directed Graphs via Absorbing Random Walks</title><categories>cs.CV cs.LG stat.ML</categories><comments>The paper is withdrawn because of some violation in institute policy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of graph-based transductive
classification, and we are particularly interested in the directed graph
scenario which is a natural form for many real world applications. Different
from existing research efforts that either only deal with undirected graphs or
circumvent directionality by means of symmetrization, we propose a novel random
walk approach on directed graphs using absorbing Markov chains, which can be
regarded as maximizing the accumulated expected number of visits from the
unlabeled transient states. Our algorithm is simple, easy to implement, and
works with large-scale graphs. In particular, it is capable of preserving the
graph structure even when the input graph is sparse and changes over time, as
well as retaining weak signals presented in the directed edges. We present its
intimate connections to a number of existing methods, including graph kernels,
graph Laplacian based methods, and interestingly, spanning forest of graphs.
Its computational complexity and the generalization error are also studied.
Empirically our algorithm is systematically evaluated on a wide range of
applications, where it has shown to perform competitively comparing to a suite
of state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4570</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4570</id><created>2014-02-19</created><authors><author><keyname>Gopalkrishnan</keyname><forenames>Manoj</forenames></author><author><keyname>Varma</keyname><forenames>Girish</forenames></author></authors><title>Playing games in an uncertain world</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional game theory assumes that the players in the game are aware of the
rules of the game. However, in practice, often the players are unaware or have
only partial knowledge about the game they are playing. They may also have
knowledge that other players have only partial knowledge of the game they are
playing, which they can try to exploit. We present a novel mathematical
formulation of such games. We make use of Kripke semantics, which are a way to
keep track of what different players know and do not know about the world. We
propose a notion of equilibrium for such games, and show that equilibrium
always exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4572</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4572</id><created>2014-02-19</created><updated>2014-10-29</updated><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Tulino</keyname><forenames>Antonia M.</forenames></author><author><keyname>Llorca</keyname><forenames>Jaime</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Caching and Coded Multicasting: Multiple Groupcast Index Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, to appear in GlobalSIP14, Dec. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of caching networks has received considerable attention in the
past few years. A particularly studied setting is the case of a single server
(e.g., a base station) and multiple users, each of which caches segments of
files in a finite library. Each user requests one (whole) file in the library
and the server sends a common coded multicast message to satisfy all users at
once. The problem consists of finding the smallest possible codeword length to
satisfy such requests. In this paper we consider the generalization to the case
where each user places $L \geq 1$ requests. The obvious naive scheme consists
of applying $L$ times the order-optimal scheme for a single request, obtaining
a linear in $L$ scaling of the multicast codeword length. We propose a new
achievable scheme based on multiple groupcast index coding that achieves a
significant gain over the naive scheme. Furthermore, through an information
theoretic converse we find that the proposed scheme is approximately optimal
within a constant factor of (at most) $18$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4576</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4576</id><created>2014-02-19</created><updated>2014-07-02</updated><authors><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Tulino</keyname><forenames>Antonia M.</forenames></author><author><keyname>Llorca</keyname><forenames>Jaime</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>On the Average Performance of Caching and Coded Multicasting with Random
  Demands</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 3 figure, to appear in ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a network with one sender, $n$ receivers (users) and $m$ possible
messages (files), caching side information at the users allows to satisfy
arbitrary simultaneous demands by sending a common (multicast) coded message.
In the worst-case demand setting, explicit deterministic and random caching
strategies and explicit linear coding schemes have been shown to be order
optimal. In this work, we consider the same scenario where the user demands are
random i.i.d., according to a Zipf popularity distribution. In this case, we
pose the problem in terms of the minimum average number of equivalent message
transmissions. We present a novel decentralized random caching placement and a
coded delivery scheme which are shown to achieve order-optimal performance. As
a matter of fact, this is the first order-optimal result for the caching and
coded multicasting problem in the case of random demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4578</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4578</id><created>2014-02-19</created><updated>2014-05-08</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Mutz</keyname><forenames>Ruediger</forenames></author></authors><title>Growth rates of modern science: A bibliometric analysis based on the
  number of publications and cited references</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>Accepted for publication in the Journal of the Association for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies in information science have looked at the growth of science. In
this study, we re-examine the question of the growth of science. To do this we
(i) use current data up to publication year 2012 and (ii) analyse it across all
disciplines and also separately for the natural sciences and for the medical
and health sciences. Furthermore, the data are analysed with an advanced
statistical technique - segmented regression analysis - which can identify
specific segments with similar growth rates in the history of science. The
study is based on two different sets of bibliometric data: (1) The number of
publications held as source items in the Web of Science (WoS, Thomson Reuters)
per publication year and (2) the number of cited references in the publications
of the source items per cited reference year. We have looked at the rate at
which science has grown since the mid-1600s. In our analysis of cited
references we identified three growth phases in the development of science,
which each led to growth rates tripling in comparison with the previous phase:
from less than 1% up to the middle of the 18th century, to 2 to 3% up to the
period between the two world wars and 8 to 9% to 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4590</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4590</id><created>2014-02-19</created><authors><author><keyname>Jiang</keyname><forenames>Yupeng</forenames></author><author><keyname>Lin</keyname><forenames>DongDai</forenames></author></authors><title>On the distinctness of binary sequences derived from $2$-adic expansion
  of m-sequences over finite prime fields</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><msc-class>11B50, 94A55, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be an odd prime with $2$-adic expansion $\sum_{i=0}^kp_i\cdot2^i$.
For a sequence $\underline{a}=(a(t))_{t\ge 0}$ over $\mathbb{F}_{p}$, each
$a(t)$ belongs to $\{0,1,\ldots, p-1\}$ and has a unique $2$-adic expansion
$$a(t)=a_0(t)+a_1(t)\cdot 2+\cdots+a_{k}(t)\cdot2^k,$$ with $a_i(t)\in\{0,
1\}$. Let $\underline{a_i}$ denote the binary sequence $(a_i(t))_{t\ge 0}$ for
$0\le i\le k$. Assume $i_0$ is the smallest index $i$ such that $p_{i}=0$ and
$\underline{a}$ and $\underline{b}$ are two different m-sequences generated by
a same primitive characteristic polynomial over $\mathbb{F}_p$. We prove that
for $i\neq i_0$ and $0\le i\le k$, $\underline{a_i}=\underline{b_i}$ if and
only if $\underline{a}=\underline{b}$, and for $i=i_0$,
$\underline{a_{i_0}}=\underline{b_{i_0}}$ if and only if
$\underline{a}=\underline{b}$ or $\underline{a}=-\underline{b}$. Then the
period of $\underline{a_i}$ is equal to the period of $\underline{a}$ if $i\ne
i_0$ and half of the period of $\underline{a}$ if $i=i_0$. We also discuss a
possible application of the binary sequences $\underline{a_i}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4597</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4597</id><created>2014-02-19</created><authors><author><keyname>Becker-Kornstaedt</keyname><forenames>Ulrike</forenames></author><author><keyname>Boggio</keyname><forenames>Daniela</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author><author><keyname>Palladino</keyname><forenames>Gino</forenames></author></authors><title>Empirically Driven Design of Software Development Processes for Wireless
  Internet Services</title><categories>cs.SE</categories><comments>15 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F3-540-36209-6_30</comments><journal-ref>Product Focused Software Process Improvement, volume 2559 of
  Lecture Notes in Computer Science, pages 351-366. Springer Berlin Heidelberg,
  2002</journal-ref><doi>10.1007/3-540-36209-6_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of software for wireless services on the Internet is a
challenging task due to the extreme time-to-market pressure, the newness of the
application domain, and the quick evolution of the technical infrastructure.
Nevertheless, developing software of a predetermined quality in a predictable
fashion can only be achieved with systematic development processes and the use
of engineering principles. Thus, systematic development processes for this
domain are needed urgently. This article presents a method for the design of an
adaptable software development process based on existing practices from related
domains, industrial piloting, and expert knowledge. First results of the
application of the method for the wireless Internet services domain are
described. The benefit for the reader is twofold: the article describes a
validated method on how to gain process knowledge for an upcoming field fast
and incrementally. Furthermore, first results of the process design for the
wireless Internet services domain are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4600</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4600</id><created>2014-02-19</created><authors><author><keyname>Meyn</keyname><forenames>Sean</forenames></author><author><keyname>Barooah</keyname><forenames>Prabir</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Ehren</keyname><forenames>Jordan</forenames></author></authors><title>Ancillary Service to the Grid Using Intelligent Deferrable Loads</title><categories>math.OC cs.SY</categories><msc-class>93E20, 90C40,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable energy sources such as wind and solar power have a high degree of
unpredictability and time-variation, which makes balancing demand and supply
challenging. One possible way to address this challenge is to harness the
inherent flexibility in demand of many types of loads. Introduced in this paper
is a technique for decentralized control for automated demand response that can
be used by grid operators as ancillary service for maintaining demand-supply
balance.
  A Markovian Decision Process (MDP) model is introduced for an individual
load. A randomized control architecture is proposed, motivated by the need for
decentralized decision making, and the need to avoid synchronization that can
lead to large and detrimental spikes in demand. An aggregate model for a large
number of loads is then developed by examining the mean field limit. A key
innovation is an LTI-system approximation of the aggregate nonlinear model,
with a scalar signal as the input and a measure of the aggregate demand as the
output. This makes the approximation particularly convenient for control design
at the grid level.
  The second half of the paper contains a detailed application of these results
to a network of residential pools. Simulations are provided to illustrate the
accuracy of the approximations and effectiveness of the proposed control
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4612</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4612</id><created>2014-02-19</created><updated>2014-05-08</updated><authors><author><keyname>Zhao</keyname><forenames>Xiaochen</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author></authors><title>Power Allocation in Compressed Sensing of Non-uniformly Sparse Signals</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of power allocation in compressed sensing when
different components in the unknown sparse signal have different probability to
be non-zero. Given the prior information of the non-uniform sparsity and the
total power budget, we are interested in how to optimally allocate the power
across the columns of a Gaussian random measurement matrix so that the mean
squared reconstruction error is minimized. Based on the state evolution
technique originated from the work by Donoho, Maleki, and Montanari, we revise
the so called approximate message passing (AMP) algorithm for the
reconstruction and quantify the MSE performance in the asymptotic regime. Then
the closed form of the optimal power allocation is obtained. The results show
that in the presence of measurement noise, uniform power allocation, which
results in the commonly used Gaussian random matrix with i.i.d. entries, is not
optimal for non-uniformly sparse signals. Empirical results are presented to
demonstrate the performance gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4618</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4618</id><created>2014-02-19</created><updated>2014-09-24</updated><authors><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Passive Dynamics in Mean Field Control</title><categories>cs.SY</categories><comments>To appear IEEE CDC, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-field models are a popular tool in a variety of fields. They provide an
understanding of the impact of interactions among a large number of particles
or people or other &quot;self-interested agents&quot;, and are an increasingly popular
tool in distributed control.
  This paper considers a particular randomized distributed control architecture
introduced in our own recent work. In numerical results it was found that the
associated mean-field model had attractive properties for purposes of control.
In particular, when viewed as an input-output system, its linearization was
found to be minimum phase.
  In this paper we take a closer look at the control model. The results are
summarized as follows:
  (i) The Markov Decision Process framework of Todorov is extended to
continuous time models, in which the &quot;control cost&quot; is based on relative
entropy. This is the basis of the construction of a family of controlled
Markovian generators.
  (ii) A decentralized control architecture is proposed in which each agent
evolves as a controlled Markov process. A central authority broadcasts a common
control signal to each agent. The central authority chooses this signal based
on an aggregate scalar output of the Markovian agents.
  (iii) Provided the control-free system is a reversible Markov process, the
following identity holds for the linearization, \[ \text{Real} (G(j\omega)) =
\text{PSD}_Y(\omega)\ge 0, \quad \omega\in\Re, \] where the right hand side
denotes the power spectral density for the output of any one of the individual
(control-free) Markov processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4621</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4621</id><created>2014-02-19</created><authors><author><keyname>Nie</keyname><forenames>Dong</forenames></author><author><keyname>Hao</keyname><forenames>Bibo</forenames></author><author><keyname>Yan</keyname><forenames>Zheng</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Cyber Behavior of Microblog Users: Onlies Versus Others</title><categories>cs.CY</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much research has been conducted to investigate personality and daily
behavior of these only children ('Onlies') due to the Chinese
one-child-per-family policy, and report the singleton generation to be more
selfish. As Microblog becomes increasingly popular recently in China, we
studied cyber behavior of Onlies and children with siblings ('Others') on Sina
Microblog ('Weibo'), a leading Microblog service provider in China.
Participants were 1792 Weibo users. Their recorded data on Weibo were
downloaded to assess their cyber behaviors. The general results show that (1)
Onlies have a smaller social circle; (2)Onlies are more significantly active on
social platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4623</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4623</id><created>2014-02-19</created><authors><author><keyname>Berzano</keyname><forenames>Dario</forenames></author><author><keyname>Blomer</keyname><forenames>Jakob</forenames></author><author><keyname>Buncic</keyname><forenames>Predrag</forenames></author><author><keyname>Charalampidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Ganis</keyname><forenames>Gerardo</forenames></author><author><keyname>Lestaris</keyname><forenames>Georgios</forenames></author><author><keyname>Meusel</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>PROOF as a Service on the Cloud: a Virtual Analysis Facility based on
  the CernVM ecosystem</title><categories>cs.DC physics.data-an</categories><comments>Talk from Computing in High Energy and Nuclear Physics 2013
  (CHEP2013), Amsterdam (NL), October 2013, 7 pages, 4 figures</comments><doi>10.1088/1742-6596/513/3/032007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PROOF, the Parallel ROOT Facility, is a ROOT-based framework which enables
interactive parallelism for event-based tasks on a cluster of computing nodes.
Although PROOF can be used simply from within a ROOT session with no additional
requirements, deploying and configuring a PROOF cluster used to be not as
straightforward. Recently great efforts have been spent to make the
provisioning of generic PROOF analysis facilities with zero configuration, with
the added advantages of positively affecting both stability and scalability,
making the deployment operations feasible even for the end user. Since a
growing amount of large-scale computing resources are nowadays made available
by Cloud providers in a virtualized form, we have developed the Virtual
PROOF-based Analysis Facility: a cluster appliance combining the solid CernVM
ecosystem and PoD (PROOF on Demand), ready to be deployed on the Cloud and
leveraging some peculiar Cloud features such as elasticity. We will show how
this approach is effective both for sysadmins, who will have little or no
configuration to do to run it on their Clouds, and for the end users, who are
ultimately in full control of their PROOF cluster and can even easily restart
it by themselves in the unfortunate event of a major failure. We will also show
how elasticity leads to a more optimal and uniform usage of Cloud resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4624</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4624</id><created>2014-02-19</created><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Kambadur</keyname><forenames>Anju</forenames></author><author><keyname>Lozano</keyname><forenames>Aurelie C.</forenames></author><author><keyname>Luss</keyname><forenames>Ronny</forenames></author></authors><title>Sparse Quantile Huber Regression for Efficient and Robust Estimation</title><categories>stat.ML cs.DS math.OC stat.ME</categories><comments>9 pages</comments><msc-class>62F35, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider new formulations and methods for sparse quantile regression in
the high-dimensional setting. Quantile regression plays an important role in
many applications, including outlier-robust exploratory analysis in gene
selection. In addition, the sparsity consideration in quantile regression
enables the exploration of the entire conditional distribution of the response
variable given the predictors and therefore yields a more comprehensive view of
the important predictors. We propose a generalized OMP algorithm for variable
selection, taking the misfit loss to be either the traditional quantile loss or
a smooth version we call quantile Huber, and compare the resulting greedy
approaches with convex sparsity-regularized formulations. We apply a recently
proposed interior point methodology to efficiently solve all convex
formulations as well as convex subproblems in the generalized OMP setting, pro-
vide theoretical guarantees of consistent estimation, and demonstrate the
performance of our approach using empirical studies of simulated and genomic
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4642</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4642</id><created>2014-02-19</created><updated>2015-11-03</updated><authors><author><keyname>K&#xf6;bler</keyname><forenames>Johannes</forenames></author><author><keyname>Kuhnert</keyname><forenames>Sebastian</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>On the Isomorphism Problem for Helly Circular-Arc Graphs</title><categories>cs.CC cs.DM math.CO</categories><comments>22 pages, 5 figures. Section 5 is revised in this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The isomorphism problem is known to be efficiently solvable for interval
graphs, while for the larger class of circular-arc graphs its complexity status
stays open. We consider the intermediate class of intersection graphs for
families of circular arcs that satisfy the Helly property. We solve the
isomorphism problem for this class in logarithmic space. If an input graph has
a Helly circular-arc model, our algorithm constructs it canonically, which
means that the models constructed for isomorphic graphs are equal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4645</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4645</id><created>2014-02-19</created><authors><author><keyname>Prakash</keyname><forenames>V. Jothi</forenames></author><author><keyname>Nithya</keyname><forenames>Dr. L. M.</forenames></author></authors><title>A Survey on Semi-Supervised Learning Techniques</title><categories>cs.LG</categories><comments>5 Pages, 3 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  8(1):25-29, February 2014. ISSN:2231-2803.Published by Seventh Sense Research
  Group</journal-ref><doi>10.14445/22312803/IJCTT-V8P105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semisupervised learning is a learning standard which deals with the study of
how computers and natural systems such as human beings acquire knowledge in the
presence of both labeled and unlabeled data. Semisupervised learning based
methods are preferred when compared to the supervised and unsupervised learning
because of the improved performance shown by the semisupervised approaches in
the presence of large volumes of data. Labels are very hard to attain while
unlabeled data are surplus, therefore semisupervised learning is a noble
indication to shrink human labor and improve accuracy. There has been a large
spectrum of ideas on semisupervised learning. In this paper we bring out some
of the key approaches for semisupervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4648</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4648</id><created>2014-02-19</created><updated>2014-02-28</updated><authors><author><keyname>M&#x142;ynarski</keyname><forenames>Wiktor</forenames></author><author><keyname>Jost</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Natural statistics of binaural sounds</title><categories>q-bio.NC cs.SD</categories><comments>29 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binaural sound localization is usually considered a discrimination task,
where interaural time (ITD) and level (ILD) disparities at pure frequency
channels are utilized to identify a position of a sound source. In natural
conditions binaural circuits are exposed to a stimulation by sound waves
originating from multiple, often moving and overlapping sources. Therefore
statistics of binaural cues depend on acoustic properties and the spatial
configuration of the environment. In order to process binaural sounds
efficiently, the auditory system should be adapted to naturally encountered cue
distributions. Statistics of cues encountered naturally and their dependence on
the physical properties of an auditory scene have not been studied before.
Here, we performed binaural recordings of three auditory scenes with varying
spatial properties. We have analyzed empirical cue distributions from each
scene by fitting them with parametric probability density functions which
allowed for an easy comparison of different scenes. Higher order statistics of
binaural waveforms were analyzed by performing Independent Component Analysis
(ICA) and studying properties of learned basis functions. Obtained results can
be related to known neuronal mechanisms and suggest how binaural hearing can be
understood in terms of adaptation to the natural signal statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4653</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4653</id><created>2014-02-19</created><authors><author><keyname>Seth</keyname><forenames>Sohan</forenames></author><author><keyname>Shawe-Taylor</keyname><forenames>John</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Retrieval of Experiments by Efficient Estimation of Marginal Likelihood</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the task of retrieving relevant experiments given a query
experiment. By experiment, we mean a collection of measurements from a set of
`covariates' and the associated `outcomes'. While similar experiments can be
retrieved by comparing available `annotations', this approach ignores the
valuable information available in the measurements themselves. To incorporate
this information in the retrieval task, we suggest employing a retrieval metric
that utilizes probabilistic models learned from the measurements. We argue that
such a metric is a sensible measure of similarity between two experiments since
it permits inclusion of experiment-specific prior knowledge. However, accurate
models are often not analytical, and one must resort to storing posterior
samples which demands considerable resources. Therefore, we study strategies to
select informative posterior samples to reduce the computational load while
maintaining the retrieval performance. We demonstrate the efficacy of our
approach on simulated data with simple linear regression as the models, and
real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4662</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4662</id><created>2014-02-19</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeniy</forenames></author><author><keyname>Payain</keyname><forenames>Simon</forenames></author></authors><title>Optimal Control of Applications for Hybrid Cloud Services</title><categories>cs.DC cs.SY</categories><comments>4 pages, Proc. conf. (not published). arXiv admin note: text overlap
  with arXiv:1402.1469</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Development of cloud computing enables to move Big Data in the hybrid cloud
services. This requires research of all processing systems and data structures
for provide QoS. Due to the fact that there are many bottlenecks requires
monitoring and control system when performing a query. The models and
optimization criteria for the design of systems in a hybrid cloud
infrastructures are created. In this article suggested approaches and the
results of this build.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4663</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4663</id><created>2014-02-19</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeniy</forenames></author><author><keyname>Payain</keyname><forenames>Simon</forenames></author></authors><title>Concept of Feedback in Future Computing Models to Cloud Systems</title><categories>cs.DC cs.NI cs.SY</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1402.1469</comments><journal-ref>World Applied Sciences Journal 32 (7): 1394-1399, 2014</journal-ref><doi>10.5829/idosi.wasj.2014.32.07.588</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Currently, it is urgent to ensure QoS in distributed computing systems. This
became especially important to the development and spread of cloud services.
Big data structures become heavily distributed. Necessary to consider the
communication channels and data transmission systems and virtualization and
scalability in future design of computational models in problems of designing
cloud systems, evaluating the effectiveness of the algorithms, the assessment
of economic performance data centers. Requires not only the monitoring of data
flows and computing resources, but also the operational management of these
resources to QoS provide. Such a tool may be just the introduction of feedback
in computational models. The article presents the main dynamic model with
feedback as a basis for a new model of distributed computing processes. The
research results are presented here. Formulated in this work can be used for
other complex tasks - estimation of structural complexity of distributed
databases, evaluation of dynamic characteristics of systems operating in the
hybrid cloud, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4675</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4675</id><created>2014-02-19</created><updated>2014-10-05</updated><authors><author><keyname>Adame</keyname><forenames>T.</forenames></author><author><keyname>Bel</keyname><forenames>A.</forenames></author><author><keyname>Bellalta</keyname><forenames>B.</forenames></author><author><keyname>Barcelo</keyname><forenames>J.</forenames></author><author><keyname>Oliver</keyname><forenames>M.</forenames></author></authors><title>IEEE 802.11ah: The Wi-Fi Approach for M2M Communications</title><categories>cs.NI</categories><comments>IEEE Wireless Magazine, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  M2M communications are projected to be one of the fastest growing technology
segments of the IT sector in the next years. Sensor and actuator networks
connect communication machines and devices so that they automatically transmit
information, serving the growing demand for environmental data acquisition.
IEEE 802.11ah Task Group addresses the creation of a new standard for giving
response to the particular requirements of this type of networks: large number
of power-constrained stations, long transmission range, small and infrequent
data messages, low data-rates and non-critical delay. This article explores the
key features of this new standard under development, especially those related
to the reduction of energy consumption in the MAC Layer. In this direction, a
performance assessment of IEEE 802.11ah in four typical M2M scenarios has been
performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4678</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4678</id><created>2014-02-17</created><authors><author><keyname>Mandelshtam</keyname><forenames>Yelena</forenames></author><author><keyname>Komarova</keyname><forenames>Natalia</forenames></author></authors><title>When Learners Surpass their Sources: Mathematical Modeling of Learning
  from an Inconsistent Source</title><categories>cs.CL</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm to model and investigate the learning process of a
learner mastering a set of grammatical rules from an inconsistent source. The
compelling interest of human language acquisition is that the learning succeeds
in virtually every case, despite the fact that the input data are formally
inadequate to explain the success of learning. Our model explains how a learner
can successfully learn from or even surpass its imperfect source without
possessing any additional biases or constraints about the types of patterns
that exist in the language. We use the data collected by Singleton and Newport
(2004) on the performance of a 7-year boy Simon, who mastered the American Sign
Language (ASL) by learning it from his parents, both of whom were imperfect
speakers of ASL. We show that the algorithm possesses a frequency-boosting
property, whereby the frequency of the most common form of the source is
increased by the learner. We also explain several key features of Simon's ASL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4699</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4699</id><created>2014-02-19</created><authors><author><keyname>Liu</keyname><forenames>Shujia</forenames></author></authors><title>A Powerful Genetic Algorithm for Traveling Salesman Problem</title><categories>cs.NE cs.AI</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a powerful genetic algorithm(GA) to solve the traveling
salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)
with a local search procedure to determine good combinations of building blocks
of parent solutions for generating even better offspring solutions.
Experimental results on well studied TSP benchmarks demonstrate that the
proposed GA is competitive in finding very high quality solutions on instances
with up to 16,862 cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4702</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4702</id><created>2014-02-13</created><authors><author><keyname>Rawat</keyname><forenames>Nitin</forenames></author><author><keyname>Ni</keyname><forenames>Pavel</forenames></author><author><keyname>Kumar</keyname><forenames>Rajesh</forenames></author></authors><title>A Fast Compressive Sensing Based Digital Image Encryption Technique
  using Structurally Random Matrices and Arnold Transform</title><categories>cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new digital image encryption method based on fast compressed sensing
approach using structurally random matrices and Arnold transform is proposed.
Considering the natural images to be compressed in any domain, the fast
compressed sensing based approach saves computational time, increases the
quality of the image and reduces the dimension of the digital image by choosing
even 25 % of the measurements. First, dimension reduction is utilized to
compress the digital image with scrambling effect. Second, Arnold
transformation is used to give the reduced digital image into more complex
form. Then, the complex image is again encrypted by double random phase
encoding process embedded with a host image; two random keys with fractional
Fourier transform are been used as a secret keys. At the receiver, the
decryption process is recovered by using TwIST algorithm. Experimental results
including peak-to-peak signal-to-noise ratio between the original and
reconstructed image are shown to analyze the validity of this technique and
demonstrated our proposed method to be secure, fast, complex and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4707</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4707</id><created>2014-02-19</created><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Standard protocol complexes for the immediate snapshot read/write model</title><categories>cs.DC math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a family of abstract simplicial complexes which we
call immediate snapshot complexes. Their definition is motivated by theoretical
distributed computing. Specifically, these complexes appear as protocol
complexes in the general immediate snapshot execution model.
  In order to define and to analyze the immediate snapshot complexes we use the
novel language of witness structures. We develop the rigorous mathematical
theory of witness structures and use it to prove several combinatorial as well
as topological properties of the immediate snapshot complexes. In particular,
we prove that these complexes are simplicially homeomorphic to simplices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4718</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4718</id><created>2014-02-19</created><updated>2016-01-29</updated><authors><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author></authors><title>Turing Kernelization for Finding Long Paths and Cycles in Restricted
  Graph Classes</title><categories>cs.DS cs.CC</categories><comments>39 pages, 8 figures</comments><msc-class>05C85, 68Q25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-complete $k$-Path problem asks whether a given undirected graph has a
(simple) path of length at least $k$. We prove that $k$-Path has
polynomial-size Turing kernels when restricted to planar graphs, graphs of
bounded degree, claw-free graphs, or to $K_{3,t}$-minor-free graphs for some
constant $t$. This means that there is an algorithm that, given a $k$-Path
instance $(G,k)$ belonging to one of these graph classes, computes its answer
in polynomial time when given access to an oracle that solves $k$-Path
instances of size polynomial in $k$ in a single step. The difficulty of
$k$-Path can therefore be confined to subinstances whose size is independent of
the total input size, but is bounded by a polynomial in the parameter $k$
alone. These results contrast existing superpolynomial lower bounds for the
sizes of traditional kernels for the $k$-Path problem on these graph classes:
there is no polynomial-time algorithm that reduces any instance $(G,k)$ to a
single, equivalent instance $(G',k')$ of size polynomial in $k$ unless $NP
\subseteq coNP/poly$. The same positive and negative results apply to the
$k$-Cycle problem, which asks for the existence of a cycle of length at least
$k$. Our kernelization schemes are based on a new methodology called
Decompose-Query-Reduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4722</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4722</id><created>2014-02-19</created><updated>2015-12-17</updated><authors><author><keyname>da Fonseca</keyname><forenames>Guilherme D.</forenames></author><author><keyname>de S&#xe1;</keyname><forenames>Vin&#xed;cius G. Pereira</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina M. H.</forenames></author></authors><title>Linear-Time Approximation Algorithms for Geometric Intersection Graphs</title><categories>cs.DS</categories><comments>An extended abstract of this paper appeared in the 12th Workshop on
  Approximation and Online Algorithms (WAOA), part of ALGO~2014</comments><journal-ref>WAOA 2014, LNCS 8952:132-143, 2015</journal-ref><doi>10.1007/978-3-319-18263-6_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous approximation algorithms for problems on geometric intersection
graphs have been proposed in the literature, exhibiting a sharp trade-off
between running times and approximation ratios. We introduce a method to obtain
linear-time constant-factor approximation algorithms for such problems. To
illustrate its applicability, we obtain results for three well-known
optimization problems on two classes of geometric intersection graphs. Among
such results, our method yields linear-time (4+eps)-approximations for the
maximum-weight independent set and the minimum dominating set of unit disk
graphs, thus bringing significant performance improvements when compared to
previous algorithms that achieve the same approximation ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4724</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4724</id><created>2014-02-19</created><authors><author><keyname>Poobrasert</keyname><forenames>Onintra</forenames></author><author><keyname>Gestubtim</keyname><forenames>Waragorn</forenames></author></authors><title>Breaking Barriers: Assistive Technology Tool as Educational Software to
  support Writing</title><categories>cs.HC cs.CY</categories><comments>9 pages. IJCA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The preliminary report by Siriraj Hospital suggested that 6% of population
who are students in Thailand could be estimated to have learning disabilities.
It is therefore necessary for our institute to develop suitable ICT
technologies to assist the education of these learning disabilities children.
We therefore developed a program called Thai Word Prediction Program. Thai Word
Prediction program aims to assist students with learning disabilities in their
writing. After the usability engineering, we conducted the experiment with
students with learning disabilities at the School in Bangkok. Hence, the
results indicated that all three students with learning disabilities in this
study improved their ability of writing by 50%, 81.89% and 100% respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4729</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4729</id><created>2014-02-19</created><authors><author><keyname>Amuru</keyname><forenames>SaiDhiraj</forenames><affiliation>Shitz</affiliation></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On the Degrees-of-freedom of the 3-user MISO Broadcast Channel with
  Hybrid CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3-user multiple-input single-output (MISO) broadcast channel (BC) with
hybrid channel state information at the transmitter (CSIT) is considered. In
this framework, there is perfect and instantaneous CSIT from a subset of users
and delayed CSIT from the remaining users. We present new results on the
degrees of freedom (DoF) of the 3-user MISO BC with hybrid CSIT. In particular,
for the case of 2 transmit antennas, we show that with perfect CSIT from one
user and delayed CSIT from the remaining two users, the optimal DoF is 5/3. For
the case of 3 transmit antennas and the same hybrid CSIT setting, it is shown
that a higher DoF of 9/5 is achievable and this result improves upon the best
known bound. Furthermore, with 3 transmit antennas, and the hybrid CSIT setting
in which there is perfect CSIT from two users and delayed CSIT from the third
one, a novel scheme is presented which achieves 9/4 DoF. Our results also
reveal new insights on how to utilize hybrid channel knowledge for multi-user
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4732</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4732</id><created>2014-02-19</created><authors><author><keyname>Lasko</keyname><forenames>Thomas A.</forenames></author></authors><title>Efficient Inference of Gaussian Process Modulated Renewal Processes with
  Application to Medical Event Data</title><categories>stat.ML cs.LG stat.AP</categories><comments>8 pages, 4 figures</comments><report-no>VU-DBMI-2014-01-001</report-no><acm-class>G.3; I.2.1; I.5.1; I.5.2; I.5.4; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The episodic, irregular and asynchronous nature of medical data render them
difficult substrates for standard machine learning algorithms. We would like to
abstract away this difficulty for the class of time-stamped categorical
variables (or events) by modeling them as a renewal process and inferring a
probability density over continuous, longitudinal, nonparametric intensity
functions modulating that process. Several methods exist for inferring such a
density over intensity functions, but either their constraints and assumptions
prevent their use with our potentially bursty event streams, or their time
complexity renders their use intractable on our long-duration observations of
high-resolution events, or both. In this paper we present a new and efficient
method for inferring a distribution over intensity functions that uses direct
numeric integration and smooth interpolation over Gaussian processes. We
demonstrate that our direct method is up to twice as accurate and two orders of
magnitude more efficient than the best existing method (thinning). Importantly,
the direct method can infer intensity functions over the full range of bursty
to memoryless to regular events, which thinning and many other methods cannot.
Finally, we apply the method to clinical event data and demonstrate the
face-validity of the abstraction, which is now amenable to standard learning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4738</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4738</id><created>2014-02-19</created><authors><author><keyname>Fredlund</keyname><forenames>Richard M</forenames></author></authors><title>A measure of compression gain for new symbols in data-compression</title><categories>cs.IT math.IT</categories><comments>7 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In coding theory it is widely known that the optimal encoding for a given
alphabet of symbol codes is the Shannon entropy times the number of symbols to
be encoded. However, depending on the structure of the message to be encoded it
is possible to beat this optimal by including only frequently occurring
aggregates of symbols from the base alphabet. We prove that the change in
compressed message length by the introduction of a new aggregate symbol can be
expressed as the difference of two entropies, dependent only on the
probabilities of the characters within the aggregate plus a correction term
which involves only the probability and length of the introduced symbol. The
expression is independent of the probability of all other symbols in the
alphabet. This measure of information gain, for a new symbol, can be applied in
data compression methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4740</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4740</id><created>2014-02-19</created><updated>2014-05-13</updated><authors><author><keyname>Ichinose</keyname><forenames>Genki</forenames></author><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author></authors><title>Evolution of Fairness in the Not Quite Ultimatum Game</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ultimatum Game (UG) is an economic game where two players (proposer and
responder) decide how to split a certain amount of money. While traditional
economic theories based on rational decision making predict that the proposer
should make a minimal offer and the responder should accept it, human subjects
tend to behave more fairly in UG. Previous studies suggested that extra
information such as reputation, empathy, or spatial structure is needed for
fairness to evolve in UG. Here we show that fairness can evolve without
additional information if players make decisions probabilistically and may
continue interactions when the offer is rejected, which we call the Not Quite
Ultimatum Game (NQUG). Evolutionary simulations of NQUG showed that the
probabilistic decision making contributes to the increase of proposers' offer
amounts to avoid rejection, while the repetition of the game works to
responders' advantage because they can wait until a good offer comes. These
simple extensions greatly promote evolution of fairness in both proposers'
offers and responders' acceptance thresholds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4741</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4741</id><created>2014-02-19</created><authors><author><keyname>Lemos</keyname><forenames>Julio</forenames></author></authors><title>A normative account of defeasible and probabilistic inference</title><categories>cs.LO cs.AI</categories><journal-ref>Intuitio 6 (2), 2013, pp. 211-219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide more evidence for the contention that logical
consequence should be understood in normative terms. Hartry Field and John
MacFarlane covered the classical case. We extend their work, examining what it
means for an agent to be obliged to infer a conclusion when faced with
uncertain information or reasoning within a non-monotonic, defeasible, logical
framework (which allows e. g. for inference to be drawn from premises
considered true unless evidence to the contrary is presented).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4742</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4742</id><created>2014-02-19</created><authors><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Dowler</keyname><forenames>Patrick</forenames></author><author><keyname>Plante</keyname><forenames>Ray</forenames></author><author><keyname>Rixon</keyname><forenames>Guy</forenames></author><author><keyname>Taylor</keyname><forenames>Mark</forenames></author></authors><title>IVOA Recommendation: TAPRegExt: a VOResource Schema Extension for
  Describing TAP Services</title><categories>astro-ph.IM cs.DB</categories><report-no>REC-TAPRegExt-1.0-20120827</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes an XML encoding standard for metadata about services
implementing the table access protocol TAP [TAP], referred to as TAPRegExt.
Instance documents are part of the service's registry record or can be obtained
from the service itself. They deliver information to both humans and software
on the languages, output formats, and upload methods supported by the service,
as well as data models implemented by the exposed tables, optional language
features, and certain limits enforced by the service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4746</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4746</id><created>2014-02-19</created><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Jafarpour</keyname><forenames>Ashkan</forenames></author><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Suresh</keyname><forenames>Ananda Theertha</forenames></author></authors><title>Near-optimal-sample estimators for spherical Gaussian mixtures</title><categories>cs.LG cs.DS cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical and machine-learning algorithms are frequently applied to
high-dimensional data. In many of these applications data is scarce, and often
much more costly than computation time. We provide the first sample-efficient
polynomial-time estimator for high-dimensional spherical Gaussian mixtures.
  For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive an
intuitive spectral-estimator that uses
$\mathcal{O}_k\bigl(\frac{d\log^2d}{\epsilon^4}\bigr)$ samples and runs in time
$\mathcal{O}_{k,\epsilon}(d^3\log^5 d)$, both significantly lower than
previously known. The constant factor $\mathcal{O}_k$ is polynomial for sample
complexity and is exponential for the time complexity, again much smaller than
what was previously known. We also show that
$\Omega_k\bigl(\frac{d}{\epsilon^2}\bigr)$ samples are needed for any
algorithm. Hence the sample complexity is near-optimal in the number of
dimensions.
  We also derive a simple estimator for one-dimensional mixtures that uses
$\mathcal{O}\bigl(\frac{k \log \frac{k}{\epsilon} }{\epsilon^2} \bigr)$ samples
and runs in time
$\widetilde{\mathcal{O}}\left(\bigl(\frac{k}{\epsilon}\bigr)^{3k+1}\right)$.
Our other technical contributions include a faster algorithm for choosing a
density estimate from a set of distributions, that minimizes the $\ell_1$
distance to an unknown underlying distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4748</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4748</id><created>2014-02-19</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Managing information and knowledge sharing cultures in higher educations
  institutions</title><categories>cs.CY</categories><comments>The 11th International Research Conference on Quality, Innovation,
  and Knowledge Management (QIK2014); L. A. Abdillah, &quot;Managing information and
  knowledge sharing cultures in higher educations institutions,&quot; in The 11th
  International Research Conference on QIK, The Trans Luxury Hotel, Bandung,
  Indonesia, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information and knowledge (IK) are very important for any institution
including education higher institution. Those IK are stored in every single
individual in organization in the form of experiences, skills, etc. The growth
of the higher education institution nowadays relies on how an institution
manage the dissemination of those IK over the organization by using information
technology (IT). This article discusses several ways and tools for engaging
persons in the organization to build sharing cultures. This article gives some
view of freely availabe application over internet to be used for IK sharing
cultures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4750</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4750</id><created>2014-02-19</created><authors><author><keyname>Dowler</keyname><forenames>Patrick</forenames></author><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Taylor</keyname><forenames>Mark</forenames></author><author><keyname>Tody</keyname><forenames>Doug</forenames></author></authors><title>IVOA Recommendation: DALI: Data Access Layer Interface Version 1.0</title><categories>astro-ph.IM cs.SE</categories><report-no>REC-DALI-1.0-20131129</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes the Data Access Layer Interface (DALI). DALI defines
the base web service interface common to all Data Access Layer (DAL) services.
This standard defines the behaviour of common resources, the meaning and use of
common parameters, success and error responses, and DAL service registration.
The goal of this specification is to define the common elements that are shared
across DAL services in order to foster consistency across concrete DAL service
specifications and to enable standard re-usable client and service
implementations and libraries to be written and widely adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4758</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4758</id><created>2014-02-19</created><updated>2014-03-05</updated><authors><author><keyname>Householder</keyname><forenames>Rachel</forenames></author><author><keyname>Arnold</keyname><forenames>Scott</forenames></author><author><keyname>Green</keyname><forenames>Robert</forenames></author></authors><title>On Cloud-based Oversubscription</title><categories>cs.DC</categories><comments>7 pages, 3 figures</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V8(8),425-431 February 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V8P273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rising trends in the number of customers turning to the cloud for their
computing needs has made effective resource allocation imperative for cloud
service providers. In order to maximize profits and reduce waste, providers
have started to explore the role of oversubscribing cloud resources. However,
the benefits of cloud-based oversubscription are not without inherent risks.
This paper attempts to unveil the incentives, risks, and techniques behind
oversubscription in a cloud infrastructure. Additionally, an overview of the
current research that has been completed on this highly relevant topic is
reviewed, and suggestions are made regarding potential avenues for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4778</identifier>
 <datestamp>2014-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4778</id><created>2014-02-19</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Weijland</keyname><forenames>Peter</forenames></author></authors><title>Bitcoin: a Money-like Informational Commodity</title><categories>cs.CY</categories><report-no>Report TCS 1402</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question &quot;what is Bitcoin&quot; allows for many answers depending on the
objectives aimed at when providing such answers. The question addressed in this
paper is to determine a top-level classification, or type, for Bitcoin. We will
classify Bitcoin as a system of type money-like informational commodity (MLIC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4799</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4799</id><created>2014-02-19</created><updated>2014-06-02</updated><authors><author><keyname>Zivari-Fard</keyname><forenames>Hassan</forenames></author><author><keyname>Akhbari</keyname><forenames>Bahareh</forenames></author><author><keyname>Ahmadian-Attari</keyname><forenames>Mahmoud</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>On the Secrecy Capacity Region of Multiple Access Wire-tap Channel with
  Common Message</title><categories>cs.IT math.IT</categories><comments>12 pages, 4 figures, It contains detailed proofs of the paper
  submitted to ITW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of secret communication over a Multiple
Access Wire-tap Channel with Common Message. Here, we assume that two
transmitters have confidential messages which must be kept secret from the
eavesdropper (the second receiver), and both of them have access to a common
message which can be decoded by two receivers. For this setting, we derive
general inner and outer bounds on the secrecy capacity region for the discrete
memoryless case as well as for Gaussian one. Providing numerical examples for
the Gaussian case, we illustrate the comparison between derived achievable rate
regions and outer bounds for our considered model and the capacity region of
compound multiple access channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4802</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4802</id><created>2014-02-18</created><updated>2014-03-13</updated><authors><author><keyname>Sol&#xe9;</keyname><forenames>Ricard V.</forenames></author><author><keyname>Seoane</keyname><forenames>Lu&#xed;s F.</forenames></author></authors><title>Ambiguity in language networks</title><categories>physics.soc-ph cs.CL q-bio.NC</categories><comments>19 pages, 5 figures, review and book chapter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language defines the most complex outcomes of evolution. The emergence
of such an elaborated form of communication allowed humans to create extremely
structured societies and manage symbols at different levels including, among
others, semantics. All linguistic levels have to deal with an astronomic
combinatorial potential that stems from the recursive nature of languages. This
recursiveness is indeed a key defining trait. However, not all words are
equally combined nor frequent. In breaking the symmetry between less and more
often used and between less and more meaning-bearing units, universal scaling
laws arise. Such laws, common to all human languages, appear on different
stages from word inventories to networks of interacting words. Among these
seemingly universal traits exhibited by language networks, ambiguity appears to
be a specially relevant component. Ambiguity is avoided in most computational
approaches to language processing, and yet it seems to be a crucial element of
language architecture. Here we review the evidence both from language network
architecture and from theoretical reasonings based on a least effort argument.
Ambiguity is shown to play an essential role in providing a source of language
efficiency, and is likely to be an inevitable byproduct of network growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4826</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4826</id><created>2014-02-19</created><authors><author><keyname>Gianazza</keyname><forenames>Andrea</forenames></author><author><keyname>Maggi</keyname><forenames>Federico</forenames></author><author><keyname>Fattori</keyname><forenames>Aristide</forenames></author><author><keyname>Cavallaro</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zanero</keyname><forenames>Stefano</forenames></author></authors><title>PuppetDroid: A User-Centric UI Exerciser for Automatic Dynamic Analysis
  of Similar Android Applications</title><categories>cs.CR</categories><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popularity and complexity of malicious mobile applications are rising, making
their analysis difficult and labor intensive. Mobile application analysis is
indeed inherently different from desktop application analysis: In the latter,
the interaction of the user (i.e., victim) is crucial for the malware to
correctly expose all its malicious behaviors.
  We propose a novel approach to analyze (malicious) mobile applications. The
goal is to exercise the user interface (UI) of an Android application to
effectively trigger malicious behaviors, automatically. Our key intuition is to
record and reproduce the UI interactions of a potential victim of the malware,
so as to stimulate the relevant behaviors during dynamic analysis. To make our
approach scale, we automatically re-execute the recorded UI interactions on
apps that are similar to the original ones. These characteristics make our
system orthogonal and complementary to current dynamic analysis and
UI-exercising approaches.
  We developed our approach and experimentally shown that our stimulation
allows to reach a higher code coverage than automatic UI exercisers, so to
unveil interesting malicious behaviors that are not exposed when using other
approaches.
  Our approach is also suitable for crowdsourcing scenarios, which would push
further the collection of new stimulation traces. This can potentially change
the way we conduct dynamic analysis of (mobile) applications, from fully
automatic only, to user-centric and collaborative too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4827</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4827</id><created>2014-02-19</created><authors><author><keyname>Mansfield</keyname><forenames>Shane</forenames></author><author><keyname>Barbosa</keyname><forenames>Rui Soares</forenames></author></authors><title>Extendability in the Sheaf-theoretic Approach: Construction of Bell
  Models from Kochen-Specker Models</title><categories>quant-ph cs.LO math.CT</categories><comments>18 pages, presented at Quantum Physics and Logic X - 2013 (ICFO,
  Barcelona), submitted to Electronic Proceedings in Theoretical Computer
  Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extendability of an empirical model was shown by Abramsky &amp; Brandenburger to
correspond in a unified manner to both locality and non-contextuality. We
develop their approach by presenting a refinement of the notion of
extendability that can also be useful in characterising the properties of
sub-models. The refinement is found to have another useful application: it is
shown that a particular canonical extension, when well-defined, may be used for
the construction of Bell-type models from models of more general kinds in such
a way that the constructed model is equivalent to the original in terms of
non-locality/contextuality. This is important since on practical and
foundational levels, the notion of locality in Bell-type models can more easily
be motivated than the corresponding general notion of contextuality. We
consider examples of Bell-type models generated from some standard examples of
contextual models, including an entire class of Kochen-Specker-like models.
This exposes an intriguing relationship between the simplest possible
contextual model (the contextual triangle) and Popescu-Rohrlich no-signalling
correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4834</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4834</id><created>2014-02-19</created><authors><author><keyname>Sadati</keyname><forenames>Mir Ehsan Hesam</forenames></author><author><keyname>Mohasefi</keyname><forenames>Jamshid Bagherzadeh</forenames></author></authors><title>The Application of Imperialist Competitive Algorithm for Fuzzy Random
  Portfolio Selection Problem</title><categories>math.OC cs.AI</categories><comments>5 pages, 2 tables, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 79(9):10-14,
  October 2013</journal-ref><doi>10.5120/13767-1618</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper presents an implementation of the Imperialist Competitive
Algorithm (ICA) for solving the fuzzy random portfolio selection problem where
the asset returns are represented by fuzzy random variables. Portfolio
Optimization is an important research field in modern finance. By using the
necessity-based model, fuzzy random variables reformulate to the linear
programming and ICA will be designed to find the optimum solution. To show the
efficiency of the proposed method, a numerical example illustrates the whole
idea on implementation of ICA for fuzzy random portfolio selection problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4843</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4843</id><created>2014-02-19</created><updated>2014-03-21</updated><authors><author><keyname>Perisic</keyname><forenames>Aleksandar</forenames></author></authors><title>Exercise: +-1 bug and center of an array problem</title><categories>cs.PL</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem that is constantly cropping up in designing even the simplest
algorithm or a program is dealing with +-1 bug when we calculate positions
within an array, very noticeably while splitting it in half. This bug is often
found in buffer overflow type of bugs. While designing one complicated
algorithm, we needed various ways of splitting an array, and we found lack of
general guidance for this apparently minor problem. We present an exercise that
tracks the cause of the problem and leads to the solution. This problem looks
trivial because it seems obvious or insignificant, however treating it without
outmost precision can lead to subtle bugs, unbalanced solution, not transparent
expressions for various languages. Basically, the exercise is about dealing
with &lt;= &lt; as well as n/2, n/2-1, (n+1)/2, n-1 and similar expressions when they
are rounded down to the nearest integer and used to define a range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4844</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4844</id><created>2014-02-19</created><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Rosenbaum</keyname><forenames>Dan</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>The Sample Complexity of Subspace Learning with Partial Information</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of subspace learning is to find a $k$-dimensional subspace of
$\mathbb{R}^d$, such that the expected squared distance between instance
vectors and the subspace is as small as possible. In this paper we study the
sample complexity of subspace learning in a \emph{partial information} setting,
in which the learner can only observe $r \le d$ attributes from each instance
vector. We derive upper and lower bounds on the sample complexity in different
scenarios. In particular, our upper bounds involve a generalization of vector
sampling techniques, which are often used in bandit problems, to matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4845</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4845</id><created>2014-02-19</created><authors><author><keyname>Gelati</keyname><forenames>Jonathan</forenames></author><author><keyname>Kanna</keyname><forenames>Sithan</forenames></author></authors><title>Diffusion Least Mean Square: Simulations</title><categories>cs.LG cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report we analyse the performance of diffusion strategies
applied to the Least-Mean-Square adaptive filter. We configure a network of
cooperative agents running adaptive filters and discuss their behaviour when
compared with a non-cooperative agent which represents the average of the
network. The analysis provides conditions under which diversity in the filter
parameters is beneficial in terms of convergence and stability. Simulations
drive and support the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4855</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4855</id><created>2014-02-19</created><updated>2014-11-17</updated><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Jej&#x10d;i&#x10d;</keyname><forenames>Miha</forenames></author></authors><title>Shortest Paths in Intersection Graphs of Unit Disks</title><categories>cs.CG cs.DM cs.DS</categories><comments>An alternative approach for the unweighted case is added to the
  Introduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a unit disk graph in the plane defined by $n$ disks whose
positions are known. For the case when $G$ is unweighted, we give a simple
algorithm to compute a shortest path tree from a given source in $O(n\log n)$
time. For the case when $G$ is weighted, we show that a shortest path tree from
a given source can be computed in $O(n^{1+\varepsilon})$ time, improving the
previous best time bound of $O(n^{4/3+\varepsilon})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4861</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4861</id><created>2014-02-19</created><authors><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>A Quasi-Newton Method for Large Scale Support Vector Machines</title><categories>cs.LG</categories><comments>5 pages, To appear in International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper adapts a recently developed regularized stochastic version of the
Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the
solution of support vector machine classification problems. The proposed method
is shown to converge almost surely to the optimal classifier at a rate that is
linear in expectation. Numerical results show that the proposed method exhibits
a convergence rate that degrades smoothly with the dimensionality of the
feature vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4862</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4862</id><created>2014-02-19</created><authors><author><keyname>Affandi</keyname><forenames>Raja Hafiz</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Learning the Parameters of Determinantal Point Process Kernels</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinantal point processes (DPPs) are well-suited for modeling repulsion
and have proven useful in many applications where diversity is desired. While
DPPs have many appealing properties, such as efficient sampling, learning the
parameters of a DPP is still considered a difficult problem due to the
non-convex nature of the likelihood function. In this paper, we propose using
Bayesian methods to learn the DPP kernel parameters. These methods are
applicable in large-scale and continuous DPP settings even when the exact form
of the eigendecomposition is unknown. We demonstrate the utility of our DPP
learning methods in studying the progression of diabetic neuropathy based on
spatial distribution of nerve fibers, and in studying human perception of
diversity in images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4867</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4867</id><created>2014-02-19</created><authors><author><keyname>van Zuylen</keyname><forenames>Anke</forenames></author><author><keyname>Bieron</keyname><forenames>James</forenames></author><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author><author><keyname>Yu</keyname><forenames>Gexin</forenames></author></authors><title>An Upper Bound on the Number of Circular Transpositions to Sort a
  Permutation</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of upper bounding the number of circular
transpositions needed to sort a permutation. It is well known that any
permutation can be sorted using at most $n(n-1)/2$ adjacent transpositions. We
show that, if we allow all adjacent transpositions, as well as the
transposition that interchanges the element in position 1 with the element in
the last position, then the number of transpositions needed is at most $n^2/4$.
This answers an open question posed by Feng, Chitturi and Sudborough (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4869</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4869</id><created>2014-02-19</created><authors><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author></authors><title>The Impact of Cost and Network Topology on Urban Mobility: A Study of
  Public Bicycle Usage in 2 U.S. Cities</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 7 Figure, PLOS One</comments><journal-ref>PLoS ONE 8(11): e79396</journal-ref><doi>10.1371/journal.pone.0079396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the drivers of urban mobility is vital for epidemiology, urban
planning, and communication networks. Human movements have so far been studied
by observing people's positions in a given space and time, though most recent
models only implicitly account for expected costs and returns for movements.
This paper explores the explicit impact of cost and network topology on
mobility dynamics, using data from 2 city-wide public bicycle share systems in
the USA. User mobility is characterized through the distribution of trip
durations, while network topology is characterized through the pairwise
distances between stations and the popularity of stations and routes. Despite
significant differences in station density and physical layout between the 2
cities, trip durations follow remarkably similar distributions that exhibit
cost sensitive trends around pricing point boundaries, particularly with
long-term users of the system. Based on the results, recommendations for
dynamic pricing and incentive schemes are provided to positively influence
mobility patterns and guide improved planning and management of public bicycle
systems to increase uptake.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4876</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4876</id><created>2014-02-19</created><authors><author><keyname>Chan</keyname><forenames>Sze-Hang</forenames></author><author><keyname>Cheung</keyname><forenames>Jeanno</forenames></author><author><keyname>Wu</keyname><forenames>Edward</forenames></author><author><keyname>Wang</keyname><forenames>Heng</forenames></author><author><keyname>Liu</keyname><forenames>Chi-Man</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoqian</forenames></author><author><keyname>Peng</keyname><forenames>Shaoliang</forenames></author><author><keyname>Luo</keyname><forenames>Ruibang</forenames></author><author><keyname>Lam</keyname><forenames>Tak-Wah</forenames></author></authors><title>MICA: A fast short-read aligner that takes full advantage of Intel Many
  Integrated Core Architecture (MIC)</title><categories>cs.DC cs.CE q-bio.GN</categories><comments>2 pages, 1 figure, 2 tables, submitted to Bioinformatics as an
  application note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Short-read aligners have recently gained a lot of speed by
exploiting the massive parallelism of GPU. An uprising alternative to GPU is
Intel MIC; supercomputers like Tianhe-2, currently top of TOP500, is built with
48,000 MIC boards to offer ~55 PFLOPS. The CPU-like architecture of MIC allows
CPU-based software to be parallelized easily; however, the performance is often
inferior to GPU counterparts as an MIC board contains only ~60 cores (while a
GPU board typically has over a thousand cores). Results: To better utilize
MIC-enabled computers for NGS data analysis, we developed a new short-read
aligner MICA that is optimized in view of MICs limitation and the extra
parallelism inside each MIC core. Experiments on aligning 150bp paired-end
reads show that MICA using one MIC board is 4.9 times faster than the BWA-MEM
(using 6-core of a top-end CPU), and slightly faster than SOAP3-dp (using a
GPU). Furthermore, MICAs simplicity allows very efficient scale-up when
multiple MIC boards are used in a node (3 cards give a 14.1-fold speedup over
BWA-MEM). Summary: MICA can be readily used by MIC-enabled supercomputers for
production purpose. We have tested MICA on Tianhe-2 with 90 WGS samples (17.47
Tera-bases), which can be aligned in an hour less than 400 nodes. MICA has
impressive performance even though the current MIC is at its initial stage of
development (the next generation of MIC has been announced to release in late
2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4881</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4881</id><created>2014-02-19</created><updated>2014-04-21</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author></authors><title>Fixed Error Asymptotics For Erasure and List Decoding</title><categories>cs.IT math.IT</categories><comments>18 pages, 1 figure; Submitted to IEEE Transactions on Information
  Theory; Shorter version to be presented at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the optimum second-order coding rates, known as second-order
capacities, for erasure and list decoding. For erasure decoding for discrete
memoryless channels, we show that second-order capacity is
$\sqrt{V}\Phi^{-1}(\epsilon_t)$ where $V$ is the channel dispersion and
$\epsilon_t$ is the total error probability, i.e., the sum of the erasure and
undetected errors. We show numerically that the expected rate at finite
blocklength for erasures decoding can exceed the finite blocklength channel
coding rate. We also show that the analogous result also holds for lossless
source coding with decoder side information, i.e., Slepian-Wolf coding. For
list decoding, we consider list codes of deterministic size that scales as
$\exp(\sqrt{n}l)$ and show that the second-order capacity is
$l+\sqrt{V}\Phi^{-1}(\epsilon)$ where $\epsilon$ is the permissible error
probability. We also consider lists of polynomial size $n^\alpha$ and derive
bounds on the third-order coding rate in terms of the order of the polynomial
$\alpha$. These bounds are tight for symmetric and singular channels. The
direct parts of the coding theorems leverage on the simple threshold decoder
and converses are proved using variants of the hypothesis testing converse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4888</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4888</id><created>2014-02-19</created><authors><author><keyname>Johnvictor</keyname><forenames>D.</forenames></author><author><keyname>Selvavinayagam</keyname><forenames>G.</forenames></author></authors><title>Survey on Sparse Coded Features for Content Based Face Image Retrieval</title><categories>cs.IR cs.CV cs.LG stat.ML</categories><comments>4 pages,3 figures,1 table, Published with International Journal of
  Computer Trends and Technology (IJCTT)</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  8(1):30-33, February 2014. ISSN:2231-2803</journal-ref><doi>10.14445/22312803/IJCTT-V8P106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content based image retrieval, a technique which uses visual contents of
image to search images from large scale image databases according to users'
interests. This paper provides a comprehensive survey on recent technology used
in the area of content based face image retrieval. Nowadays digital devices and
photo sharing sites are getting more popularity, large human face photos are
available in database. Multiple types of facial features are used to represent
discriminality on large scale human facial image database. Searching and mining
of facial images are challenging problems and important research issues. Sparse
representation on features provides significant improvement in indexing related
images to query image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4892</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4892</id><created>2014-02-20</created><authors><author><keyname>Thekumparampil</keyname><forenames>Kiran Koshy</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Sub-Modularity of Waterfilling with Applications to Online Basestation
  Allocation</title><categories>cs.NI cs.DS cs.IT math.IT</categories><comments>5 pages, 2 figures; submitted to the International Symposium on
  Information Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the popular water-filling algorithm for maximizing the mutual
information in parallel Gaussian channels is sub-modular. The sub-modularity of
water-filling algorithm is then used to derive online basestation allocation
algorithms, where mobile users are assigned to one of many possible
basestations immediately and irrevocably upon arrival without knowing the
future user information. The goal of the allocation is to maximize the sum-rate
of the system under power allocation at each basestation. We present online
algorithms with competitive ratio of at most 2 when compared to offline
algorithms that have knowledge of all future user arrivals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4893</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4893</id><created>2014-02-20</created><updated>2015-11-29</updated><authors><author><keyname>Li</keyname><forenames>Xianping</forenames></author></authors><title>Anisotropic Mesh Adaptation for Image Representation</title><categories>cs.CV math.NA</categories><comments>19 pages, 12 figures</comments><acm-class>I.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triangular meshes have gained much interest in image representation and have
been widely used in image processing. This paper introduces a framework of
anisotropic mesh adaptation (AMA) methods to image representation and proposes
a GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme.
Different than many other methods that triangulate sample points to form the
mesh, the AMA methods start directly with a triangular mesh and then adapt the
mesh based on a user-defined metric tensor to represent the image. The AMA
methods have clear mathematical framework and provides flexibility for both
image representation and image reconstruction. A mesh patching technique is
developed for the implementation of the GPRAMA method, which leads to an
improved version of the popular GPRFS-ED method. The GPRAMA method can start
with less amount of initial points but achieve better quality than the GPRFS-ED
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4907</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4907</id><created>2014-02-20</created><authors><author><keyname>Romero</keyname><forenames>Leonardo</forenames></author><author><keyname>Lara</keyname><forenames>Carlos</forenames></author></authors><title>Line Maps in Cluttered Environments</title><categories>cs.RO</categories><comments>Advances in Artificial Intelligence Lecture Notes in Computer Science
  Volume 6437, 2010, pp 154-165</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses the smoothing and mapping framework to solve the SLAM problem
in indoor environments; focusing on how some key issues such as feature
extraction and data association can be handled by applying probabilistic
techniques. For feature extraction, an odds ratio approach to find multiple
lines from laser scans is proposed, this criterion allows to decide which model
must be merged and to output the best number of models. In addition, to solve
the data association problem a method based on the segments of each line is
proposed. Experimental results show that high quality indoor maps can be
obtained from noisy data
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4914</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4914</id><created>2014-02-20</created><authors><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Jonas</keyname><forenames>Eric</forenames></author></authors><title>Building fast Bayesian computing machines out of intentionally
  stochastic, digital parts</title><categories>cs.AI cs.AR stat.CO</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The brain interprets ambiguous sensory information faster and more reliably
than modern computers, using neurons that are slower and less reliable than
logic gates. But Bayesian inference, which underpins many computational models
of perception and cognition, appears computationally challenging even given
modern transistor speeds and energy budgets. The computational principles and
structures needed to narrow this gap are unknown. Here we show how to build
fast Bayesian computing machines using intentionally stochastic, digital parts,
narrowing this efficiency gap by multiple orders of magnitude. We find that by
connecting stochastic digital components according to simple mathematical
rules, one can build massively parallel, low precision circuits that solve
Bayesian inference problems and are compatible with the Poisson firing
statistics of cortical neurons. We evaluate circuits for depth and motion
perception, perceptual learning and causal reasoning, each performing inference
over 10,000+ latent variables in real time - a 1,000x speed advantage over
commodity microprocessors. These results suggest a new role for randomness in
the engineering and reverse-engineering of intelligent computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4926</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4926</id><created>2014-02-20</created><authors><author><keyname>Mouawad</keyname><forenames>Amer E.</forenames></author><author><keyname>Nishimura</keyname><forenames>Naomi</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author></authors><title>Vertex Cover Reconfiguration and Beyond</title><categories>cs.CC cs.DS</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Vertex Cover Reconfiguration (VCR) problem, given graph $G = (V, E)$,
positive integers $k$ and $\ell$, and two vertex covers $S$ and $T$ of $G$ of
size at most $k$, we determine whether $S$ can be transformed into $T$ by a
sequence of at most $\ell$ vertex additions or removals such that every
operation results in a vertex cover of size at most $k$. Motivated by recent
results establishing the W[1]-hardness of VCR when parameterized by $\ell$, we
delineate the complexity of the problem restricted to various graph classes. In
particular, we show that VCR remains W[1]-hard on bipartite graphs, is NP-hard
but fixed-parameter tractable on graphs of bounded degree, and is solvable in
time polynomial in $|V(G)|$ on cactus graphs. We prove W[1]-hardness and
fixed-parameter tractability via two new problems of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4929</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4929</id><created>2014-02-20</created><authors><author><keyname>Iliev</keyname><forenames>Asen Petkov</forenames></author></authors><title>Formal Description of Components in Operating Systems</title><categories>cs.OS</categories><journal-ref>IJITEE (ISSN: 2278 - 3075, Volume-3, Issue-4, September 2013) 96 -
  98</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contemporary development of hardware components is a prerequisite for
increasing the concentration of computing power. System software is developing
at a much slower pace. To use available resources efficiently modeling is
required. Formalization of elements, present in the material, provides the
basis for modeling. Examples are presented to demonstrate the efficiency of the
concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4933</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4933</id><created>2014-02-20</created><authors><author><keyname>Kaushik</keyname><forenames>Ankit</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>On the Estimation of Channel State Transitions for Cognitive Radio
  Systems</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coexistence by means of shared access is a cognitive radio application. The
secondary user models the slotted primary users channel access as a Markov
process. The model parameters, i.e, the state transition probabilities
(alpha,beta) help secondary user to determine the channel occupancy, thereby
enables secondary user to rank the primary user channels. These parameters are
unknown and need to be estimated by secondary users for each channel. To do so,
the secondary users have to sense all the primary user channels in every time
slot, which is unrealistic for a large and sparsely allocated primary user
spectrum. With no other choice left, the secondary user has to sense a channel
at random time intervals and estimate the parametric information for all the
channels using the observed slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4936</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4936</id><created>2014-02-20</created><authors><author><keyname>Saleh</keyname><forenames>Amira Mohammad Abdel-Mawgoud</forenames></author></authors><title>Enhanced Secure Algorithm for Fingerprint Recognition</title><categories>cs.CV</categories><comments>PhD Thesis, Ain Shams University, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprint recognition requires a minimal effort from the user, does not
capture other information than strictly necessary for the recognition process,
and provides relatively good performance. A critical step in fingerprint
identification system is thinning of the input fingerprint image. The
performance of a minutiae extraction algorithm relies heavily on the quality of
the thinning algorithm. So, a fast fingerprint thinning algorithm is proposed.
The algorithm works directly on the gray-scale image as binarization of
fingerprint causes many spurious minutiae and also removes many important
features. The performance of the thinning algorithm is evaluated and
experimental results show that the proposed thinning algorithm is both fast and
accurate. A new minutiae-based fingerprint matching technique is proposed. The
main idea is that each fingerprint is represented by a minutiae table of just
two columns in the database. The number of different minutiae types
(terminations and bifurcations) found in each track of a certain width around
the core point of the fingerprint is recorded in this table. Each row in the
table represents a certain track, in the first column, the number of
terminations in each track is recorded, in the second column, the number of
bifurcations in each track is recorded. The algorithm is rotation and
translation invariant, and needs less storage size. Experimental results show
that recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally,
the integrity of the data transmission via communication channels must be
secure all the way from the scanner to the application. After applying Gaussian
noise addition, and JPEG compression with high and moderate quality factors on
the watermarked fingerprint images, recognition accuracy decreases slightly to
reach 96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4946</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4946</id><created>2014-02-20</created><authors><author><keyname>Ahmed</keyname><forenames>Asrar</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Inequity aversion and the evolution of cooperation</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><journal-ref>Physical Review E (89) (2) (2014) 022802</journal-ref><doi>10.1103/PhysRevE.89.022802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution of cooperation is a widely studied problem in biology, social
science, economics, and artificial intelligence. Most of the existing
approaches that explain cooperation rely on some notion of direct or indirect
reciprocity. These reciprocity based models assume agents recognize their
partner and know their previous interactions, which requires advanced cognitive
abilities. In this paper we are interested in developing a model that produces
cooperation without requiring any explicit memory of previous game plays. Our
model is based on the notion of, a concept introduced within behavioral
economics, whereby individuals care about payoff equality in outcomes. Here we
explore the effect of using income inequality to guide partner selection and
interaction. We study our model by considering both the well-mixed and the
spatially structured population and present the conditions under which
cooperation becomes dominant. Our results support the hypothesis that inequity
aversion promotes cooperative relationship among nonkin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4950</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4950</id><created>2014-02-20</created><updated>2015-02-05</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>On algorithmic equivalence of instruction sequences for computing bit
  string functions</title><categories>cs.LO</categories><comments>27 pages, the preliminaries have textual overlaps with the
  preliminaries in arXiv:1308.0219 [cs.PL], arXiv:1312.1529 [cs.PL], and
  arXiv:1312.1812 [cs.PL]; 27 pages, three paragraphs about Milner's
  algorithmic equivalence hypothesis added to concluding remarks; 26 pages,
  several minor improvements of the presentation made</comments><acm-class>F.1.1; F.2.0</acm-class><journal-ref>Fundamenta Informaticae, 138(4):411--434, 2015</journal-ref><doi>10.3233/FI-2015-1219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every partial function from bit strings of a given length to bit strings of a
possibly different given length can be computed by a finite instruction
sequence that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. We look
for an equivalence relation on instruction sequences of this kind that captures
to a reasonable degree the intuitive notion that two instruction sequences
express the same algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4958</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4958</id><created>2014-02-20</created><authors><author><keyname>Androulaki</keyname><forenames>Elli</forenames></author><author><keyname>Cachin</keyname><forenames>Christian</forenames></author><author><keyname>Dobre</keyname><forenames>Dan</forenames></author><author><keyname>Vukolic</keyname><forenames>Marko</forenames></author></authors><title>Erasure-Coded Byzantine Storage with Separate Metadata</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many distributed storage protocols have been introduced, a solution
that combines the strongest properties in terms of availability, consistency,
fault-tolerance, storage complexity and the supported level of concurrency, has
been elusive for a long time. Combining these properties is difficult,
especially if the resulting solution is required to be efficient and incur low
cost. We present AWE, the first erasure-coded distributed implementation of a
multi-writer multi-reader read/write storage object that is, at the same time:
(1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (i.e., with data
nodes storing a bounded number of values) and (5) Byzantine fault-tolerant
(BFT) using the optimal number of nodes. Furthermore, AWE is efficient since it
does not use public-key cryptography and requires data nodes that support only
reads and writes, further reducing the cost of deployment and ownership of a
distributed storage solution. Notably, AWE stores metadata separately from
$k$-out-of-$n$ erasure-coded fragments. This enables AWE to be the first BFT
protocol that uses as few as $2t+k$ data nodes to tolerate $t$ Byzantine nodes,
for any $k \ge 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4963</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4963</id><created>2014-02-20</created><updated>2014-05-19</updated><authors><author><keyname>Hannink</keyname><forenames>Julius</forenames></author><author><keyname>Duits</keyname><forenames>Remco</forenames></author><author><keyname>Bekkers</keyname><forenames>Erik</forenames></author></authors><title>Vesselness via Multiple Scale Orientation Scores</title><categories>cs.CV</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-scale Frangi vesselness filter is an established tool in (retinal)
vascular imaging. However, it cannot cope with crossings or bifurcations, since
it only looks for elongated structures. Therefore, we disentangle crossing
structures in the image via (multiple scale) invertible orientation scores. The
described vesselness filter via scale-orientation scores performs considerably
better at enhancing vessels throughout crossings and bifurcations than the
Frangi version. Both methods are evaluated on a public dataset. Performance is
measured by comparing ground truth data to the segmentation results obtained by
basic thresholding and morphological component analysis of the filtered images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4986</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4986</id><created>2014-02-20</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tian</keyname><forenames>Hong</forenames></author></authors><title>Performance Impact of Data Layout on the GPU-accelerated IDW
  Interpolation</title><categories>cs.DC</categories><comments>Preprint version for reviewing. 9 pages</comments><journal-ref>SpringerPlus.2016, 5:104</journal-ref><doi>10.1186/s40064-016-1731-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on evaluating the performance impact of different data
layouts on the GPU-accelerated IDW interpolation. First, we redesign and
improve our previous GPU implementation that was performed by exploiting the
feature CUDA Dynamic Parallel (CDP). And then, we implement three versions of
GPU implementations, i.e., the naive version, the tiled version, and the
improved CDP version, based on five layouts including the Structure of Arrays
(SoA), the Array of Sturcutes (AoS), the Array of aligned Sturcutes (AoaS), the
Structure of Arrays of aligned Structures (SoAoS), and the Hybrid layout.
Experimental results show that: the layouts AoS and AoaS achieve better
performance than the layout SoA for both the naive version and tiled version,
while the layout SoA is the best choice for the improved CDP version. We also
observe that: for the two combined data layouts (the SoAoS and the Hybrid),
there are no notable performance gains when compared to other three basic
layouts. We recommend that: in practical applications, the layout AoaS is the
best choice since the tiled version is the fastest one among the three versions
of GPU implementations, especially on single precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4994</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4994</id><created>2014-02-20</created><updated>2014-04-28</updated><authors><author><keyname>Fuchs</keyname><forenames>Fabian</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Arbitrary Transmission Power in the SINR Model: Local Broadcasting,
  Coloring and MIS</title><categories>cs.DS</categories><comments>Improved Runtime of Local Broadcasting by a factor of \Gamma^\alpha</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the light of energy conservation and the expansion of existing networks,
wireless networks face the challenge of nodes with heterogeneous transmission
power. However, for more realistic models of wireless communication only few
algorithmic results are known. In this paper we consider nodes with arbitrary,
possibly variable, transmission power in the so-called physical or SINR model.
Our first result is a bound on the probabilistic interference from all
simultaneously transmitting nodes on receivers. This result implies that
current local broadcasting algorithms can be generalized to the case of
non-uniform transmission power with minor changes. The algorithms run in
$\O(\Gamma^{2} \Delta \log n)$ time slots if the maximal degree $\Delta$ is
known, and $\O((\Delta + \log n)\Gamma^{2} \log n)$ otherwise, where $\Gamma$
is the ratio between the maximal and the minimal transmission range. The broad
applicability of our result on bounding the interference is further
highlighted, by generalizing a distributed coloring algorithm to this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.4995</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.4995</id><created>2014-02-20</created><updated>2014-03-24</updated><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Kla&#x161;ka</keyname><forenames>David</forenames></author><author><keyname>Ku&#x10d;era</keyname><forenames>Anton&#xed;n</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Petr</forenames></author></authors><title>Minimizing Running Costs in Consumption Systems</title><categories>cs.SY</categories><comments>32 pages, corrections of typos and minor omissions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard approach to optimizing long-run running costs of discrete systems
is based on minimizing the mean-payoff, i.e., the long-run average amount of
resources (&quot;energy&quot;) consumed per transition. However, this approach inherently
assumes that the energy source has an unbounded capacity, which is not always
realistic. For example, an autonomous robotic device has a battery of finite
capacity that has to be recharged periodically, and the total amount of energy
consumed between two successive charging cycles is bounded by the capacity.
Hence, a controller minimizing the mean-payoff must obey this restriction. In
this paper we study the controller synthesis problem for consumption systems
with a finite battery capacity, where the task of the controller is to minimize
the mean-payoff while preserving the functionality of the system encoded by a
given linear-time property. We show that an optimal controller always exists,
and it may either need only finite memory or require infinite memory (it is
decidable in polynomial time which of the two cases holds). Further, we show
how to compute an effective description of an optimal controller in polynomial
time. Finally, we consider the limit values achievable by larger and larger
battery capacity, show that these values are computable in polynomial time, and
we also analyze the corresponding rate of convergence. To the best of our
knowledge, these are the first results about optimizing the long-run running
costs in systems with bounded energy stores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5003</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5003</id><created>2014-02-20</created><authors><author><keyname>Bodlaender</keyname><forenames>Marijke H. L.</forenames></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Magn&#xfa;s M.</forenames></author></authors><title>Beyond Geometry : Towards Fully Realistic Wireless Models</title><categories>cs.NI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal-strength models of wireless communications capture the gradual fading
of signals and the additivity of interference. As such, they are closer to
reality than other models. However, nearly all theoretic work in the SINR model
depends on the assumption of smooth geometric decay, one that is true in free
space but is far off in actual environments. The challenge is to model
realistic environments, including walls, obstacles, reflections and anisotropic
antennas, without making the models algorithmically impractical or analytically
intractable.
  We present a simple solution that allows the modeling of arbitrary static
situations by moving from geometry to arbitrary decay spaces. The complexity of
a setting is captured by a metricity parameter Z that indicates how far the
decay space is from satisfying the triangular inequality. All results that hold
in the SINR model in general metrics carry over to decay spaces, with the
resulting time complexity and approximation depending on Z in the same way that
the original results depends on the path loss term alpha. For distributed
algorithms, that to date have appeared to necessarily depend on the planarity,
we indicate how they can be adapted to arbitrary decay spaces.
  Finally, we explore the dependence on Z in the approximability of core
problems. In particular, we observe that the capacity maximization problem has
exponential upper and lower bounds in terms of Z in general decay spaces. In
Euclidean metrics and related growth-bounded decay spaces, the performance
depends on the exact metricity definition, with a polynomial upper bound in
terms of Z, but an exponential lower bound in terms of a variant parameter phi.
On the plane, the upper bound result actually yields the first approximation of
a capacity-type SINR problem that is subexponential in alpha.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5010</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5010</id><created>2014-02-20</created><authors><author><keyname>Abdallah</keyname><forenames>Hiba</forenames><affiliation>LJK</affiliation></author><author><keyname>M&#xe9;rigot</keyname><forenames>Quentin</forenames><affiliation>LJK</affiliation></author></authors><title>On the reconstruction of convex sets from random normal measurements</title><categories>cs.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of reconstructing a convex body using only a finite
number of measurements of outer normal vectors. More precisely, we suppose that
the normal vectors are measured at independent random locations uniformly
distributed along the boundary of our convex set. Given a desired Hausdorff
error eta, we provide an upper bounds on the number of probes that one has to
perform in order to obtain an eta-approximation of this convex set with high
probability. Our result rely on the stability theory related to Minkowski's
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5029</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5029</id><created>2014-02-20</created><updated>2014-08-24</updated><authors><author><keyname>Bordenabe</keyname><forenames>Nicol&#xe1;s E.</forenames></author><author><keyname>Chatzikokolakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Palamidessi</keyname><forenames>Catuscia</forenames></author></authors><title>Optimal Geo-Indistinguishable Mechanisms for Location Privacy</title><categories>cs.CR</categories><comments>13 pages</comments><acm-class>C.2.0; K.4.1</acm-class><doi>10.1145/2660267.2660345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the geo-indistinguishability approach to location privacy, and
the trade-off with respect to utility. We show that, given a desired degree of
geo-indistinguishability, it is possible to construct a mechanism that
minimizes the service quality loss, using linear programming techniques. In
addition we show that, under certain conditions, such mechanism also provides
optimal privacy in the sense of Shokri et al. Furthermore, we propose a method
to reduce the number of constraints of the linear program from cubic to
quadratic, maintaining the privacy guarantees and without affecting
significantly the utility of the generated mechanism. This reduces considerably
the time required to solve the linear program, thus enlarging significantly the
location sets for which the optimal mechanisms can be computed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5034</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5034</id><created>2014-02-20</created><authors><author><keyname>Sina</keyname><forenames>Sigal</forenames></author><author><keyname>Kraus</keyname><forenames>Sarit</forenames></author><author><keyname>Rosenfeld</keyname><forenames>Avi</forenames></author></authors><title>Using the Crowd to Generate Content for Scenario-Based Serious-Games</title><categories>cs.AI cs.HC</categories><report-no>IDGEI/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, scenario-based serious-games have become a main tool for
learning new skills and capabilities. An important factor in the development of
such systems is the overhead in time, cost and human resources to manually
create the content for these scenarios. We focus on how to create content for
scenarios in medical, military, commerce and gaming applications where
maintaining the integrity and coherence of the content is integral for the
system's success. To do so, we present an automatic method for generating
content about everyday activities through combining computer science techniques
with the crowd. We use the crowd in three basic ways: to capture a database of
scenarios of everyday activities, to generate a database of likely replacements
for specific events within that scenario, and to evaluate the resulting
scenarios. We found that the generated scenarios were rated as reliable and
consistent by the crowd when compared to the scenarios that were originally
captured. We also compared the generated scenarios to those created by
traditional planning techniques. We found that both methods were equally
effective in generated reliable and consistent scenarios, yet the main
advantages of our approach is that the content we generate is more varied and
much easier to create. We have begun integrating this approach within a
scenario-based training application for novice investigators within the law
enforcement departments to improve their questioning skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5037</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5037</id><created>2014-02-20</created><authors><author><keyname>Dunwell</keyname><forenames>Ian</forenames></author><author><keyname>Petridis</keyname><forenames>Panagiotis</forenames></author><author><keyname>Lameras</keyname><forenames>Petros</forenames></author><author><keyname>Hendrix</keyname><forenames>Maurice</forenames></author><author><keyname>Doukianou</keyname><forenames>Stella</forenames></author><author><keyname>Gaved</keyname><forenames>Mark</forenames></author></authors><title>Assessing the Reach and Impact of Game-Based Learning Approaches to
  Cultural Competency and Behavioural Change</title><categories>cs.AI</categories><report-no>IDGEI/2014/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As digital games continue to be explored as solutions to educational and
behavioural challenges, the need for evaluation methodologies which support
both the unique nature of the format and the need for comparison with other
approaches continues to increase. In this workshop paper, a range of challenges
are described related specifically to the case of cultural learning using
digital games, in terms of how it may best be assessed, understood, and
sustained through an iterative process supported by research. An evaluation
framework is proposed, identifying metrics for reach and impact and their
associated challenges, as well as presenting ethical considerations and the
means to utilize evaluation outcomes within an iterative cycle, and to provide
feedback to learners. Presenting as a case study a serious game from the Mobile
Assistance for Social Inclusion and Empowerment of Immigrants with Persuasive
Learning Technologies and Social Networks (MASELTOV) project, the use of the
framework in the context of an integrative project is discussed, with emphasis
on the need to view game-based learning as a blended component of the cultural
learning process, rather than a standalone solution. The particular case of
mobile gaming is also considered within this case study, providing a platform
by which to deliver and update content in response to evaluation outcomes.
Discussion reflects upon the general challenges related to the assessment of
cultural learning, and behavioural change in more general terms, suggesting
future work should address the need to provide sustainable, research-driven
platforms for game-based learning content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5039</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5039</id><created>2014-02-20</created><updated>2014-02-25</updated><authors><author><keyname>Jones</keyname><forenames>Hazael</forenames></author><author><keyname>Sabouret</keyname><forenames>Nicolas</forenames></author><author><keyname>Damian</keyname><forenames>Ionut</forenames></author><author><keyname>Baur</keyname><forenames>Tobias</forenames></author><author><keyname>Andr&#xe9;</keyname><forenames>Elisabeth</forenames></author><author><keyname>Porayska-Pomsta</keyname><forenames>Ka&#x15b;ka</forenames></author><author><keyname>Rizzo</keyname><forenames>Paola</forenames></author></authors><title>Interpreting social cues to generate credible affective reactions of
  virtual job interviewers</title><categories>cs.AI cs.CY</categories><report-no>IDGEI/2014/06</report-no><acm-class>I.6.5; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a mechanism of generating credible affective
reactions in a virtual recruiter during an interaction with a user. This is
done using communicative performance computation based on the behaviours of the
user as detected by a recognition module. The proposed software pipeline is
part of the TARDIS system which aims to aid young job seekers in acquiring job
interview related social skills. In this context, our system enables the
virtual recruiter to realistically adapt and react to the user in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5043</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5043</id><created>2014-02-20</created><authors><author><keyname>Belkaid</keyname><forenames>Marwen</forenames></author><author><keyname>Sabouret</keyname><forenames>Nicolas</forenames></author></authors><title>A logical model of Theory of Mind for virtual agents in the context of
  job interview simulation</title><categories>cs.AI</categories><report-no>IDGEI/2014/10</report-no><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Job interview simulation with a virtual agents aims at improving people's
social skills and supporting professional inclusion. In such simulators, the
virtual agent must be capable of representing and reasoning about the user's
mental state based on social cues that inform the system about his/her affects
and social attitude. In this paper, we propose a formal model of Theory of Mind
(ToM) for virtual agent in the context of human-agent interaction that focuses
on the affective dimension. It relies on a hybrid ToM that combines the two
major paradigms of the domain. Our framework is based on modal logic and
inference rules about the mental states, emotions and social relations of both
actors. Finally, we present preliminary results regarding the impact of such a
model on natural interaction in the context of job interviews simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5045</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5045</id><created>2014-02-20</created><authors><author><keyname>Sabouret</keyname><forenames>Nicolas</forenames></author><author><keyname>Jones</keyname><forenames>Haza&#xeb;l</forenames></author><author><keyname>Ochs</keyname><forenames>Magalie</forenames></author><author><keyname>Chollet</keyname><forenames>Mathieu</forenames></author><author><keyname>Pelachaud</keyname><forenames>Catherine</forenames></author></authors><title>Expressing social attitudes in virtual agents for social training games</title><categories>cs.HC cs.AI cs.CY</categories><report-no>IDGEI/2014/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of virtual agents in social coaching has increased rapidly in the
last decade. In order to train the user in different situations than can occur
in real life, the virtual agent should be able to express different social
attitudes. In this paper, we propose a model of social attitudes that enables a
virtual agent to reason on the appropriate social attitude to express during
the interaction with a user given the course of the interaction, but also the
emotions, mood and personality of the agent. Moreover, the model enables the
virtual agent to display its social attitude through its non-verbal behaviour.
The proposed model has been developed in the context of job interview
simulation. The methodology used to develop such a model combined a theoretical
and an empirical approach. Indeed, the model is based both on the literature in
Human and Social Sciences on social attitudes but also on the analysis of an
audiovisual corpus of job interviews and on post-hoc interviews with the
recruiters on their expressed attitudes during the job interview.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5047</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5047</id><created>2014-02-20</created><authors><author><keyname>Piana</keyname><forenames>Stefano</forenames></author><author><keyname>Staglian&#xf2;</keyname><forenames>Alessandra</forenames></author><author><keyname>Odone</keyname><forenames>Francesca</forenames></author><author><keyname>Verri</keyname><forenames>Alessandro</forenames></author><author><keyname>Camurri</keyname><forenames>Antonio</forenames></author></authors><title>Real-time Automatic Emotion Recognition from Body Gestures</title><categories>cs.HC cs.CV</categories><report-no>IDGEI/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although psychological research indicates that bodily expressions convey
important affective information, to date research in emotion recognition
focused mainly on facial expression or voice analysis. In this paper we propose
an approach to realtime automatic emotion recognition from body movements. A
set of postural, kinematic, and geometrical features are extracted from
sequences 3D skeletons and fed to a multi-class SVM classifier. The proposed
method has been assessed on data acquired through two different systems: a
professionalgrade optical motion capture system, and Microsoft Kinect. The
system has been assessed on a &quot;six emotions&quot; recognition problem, and using a
leave-one-subject-out cross validation strategy, reached an overall recognition
rate of 61.3% which is very close to the recognition rate of 61.9% obtained by
human observers. To provide further testing of the system, two games were
developed, where one or two users have to interact to understand and express
emotions with their body.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5051</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5051</id><created>2014-02-20</created><updated>2014-12-16</updated><authors><author><keyname>Iceland</keyname><forenames>Eran</forenames></author><author><keyname>Samorodnitsky</keyname><forenames>Alex</forenames></author></authors><title>On Coset Leader Graphs of LDPC Codes</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our main technical result is that, in the coset leader graph of a linear
binary code of block length n, the metric balls spanned by constant-weight
vectors grow exponentially slower than those in $\{0,1\}^n$.
  Following the approach of Friedman and Tillich (2006), we use this fact to
improve on the first linear programming bound on the rate of LDPC codes, as the
function of their minimal distance. This improvement, combined with the
techniques of Ben-Haim and Lytsin (2006), improves the rate vs distance bounds
for LDPC codes in a significant sub-range of relative distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5073</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5073</id><created>2014-02-20</created><updated>2014-02-21</updated><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing</title><categories>cs.CV cs.IT math.IT</categories><comments>RecPad 2013, Lisbon, Portugal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach, {\it two-dimensional fused binary compressive
sensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bit
measurements, exploiting 2D group sparsity for 1-bit compressive sensing
recovery. The proposed method is a modified 2D version of the previous {\it
binary iterative hard thresholding} (2DBIHT) algorithm, where the objective
function includes a 2D one-sided $\ell_1$ (or $\ell_2$) penalty function
encouraging agreement with the observed data, an indicator function of
$K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. The
subgradient of the 2D one-sided $\ell_1$ (or $\ell_2$) penalty and the
projection onto the $K$-sparsity and TV or MTV constraint can be computed
efficiently, allowing the appliaction of algorithms of the {\it
forward-backward splitting} (a.k.a. {\it iterative shrinkage-thresholding})
family. Experiments on the recovery of 2D sparse piece-wise smooth signals show
that the proposed approach is able to take advantage of the piece-wise
smoothness of the original signal, achieving more accurate recovery than
2DBIHT. More specifically, 2DFBCS with the MTV and the $\ell_2$ penalty
performs best amongst the algorithms tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5074</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5074</id><created>2014-02-20</created><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group
  Sparsity</title><categories>cs.CV cs.IT math.IT</categories><comments>Conf. on Telecommunications - ConfTele, Castelo Branco, Portugal,
  Vol. 1, pp. 65 - 68, May, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method, {\it binary fused compressive sensing} (BFCS), to
recover sparse piece-wise smooth signals from 1-bit compressive measurements.
The proposed algorithm is a modification of the previous {\it binary iterative
hard thresholding} (BIHT) algorithm, where, in addition to the sparsity
constraint, the total-variation of the recovered signal is upper constrained.
As in BIHT, the data term of the objective function is an one-sided $\ell_1$
(or $\ell_2$) norm. Experiments on the recovery of sparse piece-wise smooth
signals show that the proposed algorithm is able to take advantage of the
piece-wise smoothness of the original signal, achieving more accurate recovery
than BIHT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5076</identifier>
 <datestamp>2014-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5076</id><created>2014-02-20</created><updated>2014-03-20</updated><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Robust Binary Fused Compressive Sensing using Adaptive Outlier Pursuit</title><categories>cs.CV cs.IT math.IT</categories><comments>Accepted by ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method, {\it robust binary fused compressive sensing}
(RoBFCS), to recover sparse piece-wise smooth signals from 1-bit compressive
measurements. The proposed method is a modification of our previous {\it binary
fused compressive sensing} (BFCS) algorithm, which is based on the {\it binary
iterative hard thresholding} (BIHT) algorithm. As in BIHT, the data term of the
objective function is a one-sided $\ell_1$ (or $\ell_2$) norm. Experiments show
that the proposed algorithm is able to take advantage of the piece-wise
smoothness of the original signal and detect sign flips and correct them,
achieving more accurate recovery than BFCS and BIHT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5077</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5077</id><created>2014-02-20</created><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Group-sparse Matrix Recovery</title><categories>cs.LG cs.CV stat.ML</categories><comments>ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the OSCAR (octagonal selection and clustering algorithms for
regression) in recovering group-sparse matrices (two-dimensional---2D---arrays)
from compressive measurements. We propose a 2D version of OSCAR (2OSCAR)
consisting of the $\ell_1$ norm and the pair-wise $\ell_{\infty}$ norm, which
is convex but non-differentiable. We show that the proximity operator of 2OSCAR
can be computed based on that of OSCAR. The 2OSCAR problem can thus be
efficiently solved by state-of-the-art proximal splitting algorithms.
Experiments on group-sparse 2D array recovery show that 2OSCAR regularization
solved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm
(with debiasing) yields the most accurate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5078</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5078</id><created>2014-02-20</created><updated>2014-07-31</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Pr&#x16b;sis</keyname><forenames>Kri&#x161;j&#x101;nis</forenames></author></authors><title>A Tight Lower Bound on Certificate Complexity in Terms of Block
  Sensitivity and Sensitivity</title><categories>cs.CC</categories><comments>12 pages</comments><doi>10.1007/978-3-662-44465-8_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity, certificate complexity and block sensitivity are widely used
Boolean function complexity measures. A longstanding open problem, proposed by
Nisan and Szegedy, is whether sensitivity and block sensitivity are
polynomially related. Motivated by the constructions of functions which achieve
the largest known separations, we study the relation between 1-certificate
complexity and 0-sensitivity and 0-block sensitivity.
  Previously the best known lower bound was $C_1(f)\geq \frac{bs_0(f)}{2
s_0(f)}$, achieved by Kenyon and Kutin. We improve this to $C_1(f)\geq \frac{3
bs_0(f)}{2 s_0(f)}$. While this improvement is only by a constant factor, this
is quite important, as it precludes achieving a superquadratic separation
between $bs(f)$ and $s(f)$ by iterating functions which reach this bound. In
addition, this bound is tight, as it matches the construction of Ambainis and
Sun up to an additive constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5086</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5086</id><created>2014-02-20</created><authors><author><keyname>Krishnamoorthy</keyname><forenames>Aravindh</forenames></author></authors><title>Symmetric QR Algorithm with Permutations</title><categories>cs.NA cs.MS math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the QR Algorithm with Permutations that shows an
improved convergence rate compared to the classical QR algorithm. We determine
a bound for performance based on best instantaneous convergence, and develop
low complexity methods for computing the permutation matrices at every
iteration. We use simulations to verify the improvement, and to compare the
performance of proposed algorithms to the classical QR algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5110</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5110</id><created>2014-02-20</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Singular Layer Transmission for Continuous-Variable Quantum Key
  Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>48 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a singular layer transmission model for continuous-variable
quantum key distribution (CVQKD). In CVQKD, the transmit information is carried
by continuous-variable (CV) quantum states, particularly by Gaussian random
distributed position and momentum quadratures. The reliable transmission of the
quadrature components over a noisy link is a cornerstone of CVQKD protocols.
The proposed singular layer uses the singular value decomposition of the
Gaussian quantum channel, which yields an additional degree of freedom for the
phase space transmission. This additional degree of freedom can further be
exploited in a multiple-access scenario. The singular layer defines the
eigenchannels of the Gaussian physical link, which can be used for the
simultaneous reliable transmission of multiple user data streams. Our
transmission model also includes the singular interference avoider (SIA)
precoding scheme. The proposed SIA precoding scheme prevents the eigenchannel
interference to reach an optimal transmission over a Gaussian link. We
demonstrate the results through the adaptive multicarrier quadrature
division-multiuser quadrature allocation (AMQD-MQA) CVQKD multiple-access
scheme. We define the singular model of AMQD-MQA and characterize the
properties of the eigenchannel interference. We propose the SIA precoding of
Gaussian random quadratures and the optimal decoding at the receiver. We show a
random phase space constellation scheme for the Gaussian sub-channels. The
singular layer transmission provides improved simultaneous transmission rates
for the users with unconditional security in a multiple-access scenario,
particularly in crucial low signal-to-noise ratio regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5114</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5114</id><created>2014-02-18</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Abdulazeez</keyname><forenames>Hassan</forenames></author><author><keyname>Abraham</keyname><forenames>Ochoche</forenames></author><author><keyname>Mohammed</keyname><forenames>Umar</forenames></author></authors><title>Analysing Membership Profile Privacy Issues in Online Social Networks</title><categories>cs.SI cs.SY</categories><comments>20 pages</comments><journal-ref>IUP Journal of Information Technology 7 (3), 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A social networking site is an on-line service that attracts a society of
subscribers and provides such users with a multiplicity of tools for
distribution personal data and creating subscribers generated content directed
to a given users interest and personal life. Operators of online social
networks are gradually giving out potentially sensitive information about users
and their relationships with advertisers, application developers, and
data-mining researchers. Some criminals too uses information gathered through
membership profile in social networks to break peoples PINs and passwords. In
this paper, we looked at the field structure of membership profiles in ten
popular social networking sites. We also analysed how private information can
easily be made public in such sites. At the end recommendations and
countermeasures were made on how to safe guard subscribers personal data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5121</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5121</id><created>2014-02-18</created><updated>2014-10-27</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author></authors><title>Challenges in Selecting Software to be Reused</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This is a position paper for Sharing, Re-Use and Circulation of Resources in
Cooperative Scientific Work, a CSCW'14 workshop. It discusses the role of
software in NSF's CIF21 vision and the SI2 program, which is intended to
support that goal. SI2 primarily supports software projects that are proposed
in response to solicitations, and some of the criteria used by the
peer-reviewers and by NSF in evaluating these projects depend on predicting
scientific impact. This paper discusses some ideas on how the prediction of
scientific impact can be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5123</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5123</id><created>2014-02-20</created><authors><author><keyname>Amine</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Hamou</keyname><forenames>Reda Mohamed</forenames></author><author><keyname>Simonet</keyname><forenames>Michel</forenames></author></authors><title>Detecting Opinions in Tweets</title><categories>cs.CL cs.SI</categories><comments>13 pages, 2 figures</comments><journal-ref>International Journal Of Data Mining And Emerging Technologies,
  Year : 2013, Volume : 3, Issue : 1 First page : ( 23) Last page : ( 32) Print
  ISSN : 2249-3212. Online ISSN : 2249-3220</journal-ref><doi>10.5958/j.2249-3220.3.1.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the incessant growth of documents describing the opinions of different
people circulating on the web, including Web 2.0 has made it possible to give
an opinion on any product in the net. In this paper, we examine the various
opinions expressed in the tweets and classify them positive, negative or
neutral by using the emoticons for the Bayesian method and adjectives and
adverbs for the Turney's method
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5131</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5131</id><created>2014-02-20</created><updated>2015-07-06</updated><authors><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Jonckheere</keyname><forenames>Edmond</forenames></author></authors><title>Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse
  Optimization and Noisy Matrix Decomposition</title><categories>cs.LG math.OC stat.ML</categories><comments>appeared in Neural Information Processing Systems(NIPS) 2014. arXiv
  admin note: text overlap with arXiv:1207.4421 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient ADMM method with guarantees for high-dimensional
problems. We provide explicit bounds for the sparse optimization problem and
the noisy matrix decomposition problem. For sparse optimization, we establish
that the modified ADMM method has an optimal convergence rate of
$\mathcal{O}(s\log d/T)$, where $s$ is the sparsity level, $d$ is the data
dimension and $T$ is the number of steps. This matches with the minimax lower
bounds for sparse estimation. For matrix decomposition into sparse and low rank
components, we provide the first guarantees for any online method, and prove a
convergence rate of $\tilde{\mathcal{O}}((s+r)\beta^2(p) /T) +
\mathcal{O}(1/p)$ for a $p\times p$ matrix, where $s$ is the sparsity level,
$r$ is the rank and $\Theta(\sqrt{p})\leq \beta(p)\leq \Theta(p)$. Our
guarantees match the minimax lower bound with respect to $s,r$ and $T$. In
addition, we match the minimax lower bound with respect to the matrix dimension
$p$, i.e. $\beta(p)=\Theta(\sqrt{p})$, for many important statistical models
including the independent noise model, the linear Bayesian network and the
latent Gaussian graphical model under some conditions. Our ADMM method is based
on epoch-based annealing and consists of inexpensive steps which involve
projections on to simple norm balls. Experiments show that for both sparse
optimization and matrix decomposition problems, our algorithm outperforms the
state-of-the-art methods. In particular, we reach higher accuracy with same
time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5138</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5138</id><created>2014-02-19</created><updated>2014-06-12</updated><authors><author><keyname>Ahmed</keyname><forenames>Mahmuda</forenames></author><author><keyname>Karagiorgou</keyname><forenames>Sophia</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author><author><keyname>Wenk</keyname><forenames>Carola</forenames></author></authors><title>A Comparison and Evaluation of Map Construction Algorithms</title><categories>cs.CG</categories><doi>10.1007/s10707-014-0222-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Map construction methods automatically produce and/or update road network
datasets using vehicle tracking data. Enabled by the ubiquitous generation of
georeferenced tracking data, there has been a recent surge in map construction
algorithms coming from different computer science domains. A cross-comparison
of the various algorithms is still very rare, since (i) algorithms and
constructed maps are generally not publicly available and (ii) there is no
standard approach to assess the result quality, given the lack of benchmark
data and quantitative evaluation methods. This work represents a first
comprehensive attempt to benchmark map construction algorithms. We provide an
evaluation and comparison of seven algorithms using four datasets and four
different evaluation measures. In addition to this comprehensive comparison, we
make our datasets, source code of map construction algorithms and evaluation
measures publicly available on mapconstruction.org. This site has been
established as a repository for map con- struction data and algorithms and we
invite other researchers to contribute by uploading code and benchmark data
supporting their contributions to map construction algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5161</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5161</id><created>2014-02-20</created><updated>2014-08-14</updated><authors><author><keyname>Rossi</keyname><forenames>Roberto</forenames></author><author><keyname>Prestwich</keyname><forenames>Steven</forenames></author><author><keyname>Tarim</keyname><forenames>S. Armagan</forenames></author></authors><title>Statistical Constraints</title><categories>cs.AI stat.ME</categories><acm-class>F.4.1</acm-class><journal-ref>Proceedings of the 21st European Conference on Artificial
  Intelligence, ECAI 2014, August 18-22, 2014, Prague, Czech Republic, IOS
  Press, Volume 263, pp. 777-782, 2014</journal-ref><doi>10.3233/978-1-61499-419-0-777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce statistical constraints, a declarative modelling tool that links
statistics and constraint programming. We discuss two statistical constraints
and some associated filtering algorithms. Finally, we illustrate applications
to standard problems encountered in statistics and to a novel inspection
scheduling problem in which the aim is to find inspection plans with desirable
statistical properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5164</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5164</id><created>2014-02-20</created><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Distribution-Independent Reliable Learning</title><categories>cs.LG cs.CC cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study several questions in the reliable agnostic learning framework of
Kalai et al. (2009), which captures learning tasks in which one type of error
is costlier than others. A positive reliable classifier is one that makes no
false positive errors. The goal in the positive reliable agnostic framework is
to output a hypothesis with the following properties: (i) its false positive
error rate is at most $\epsilon$, (ii) its false negative error rate is at most
$\epsilon$ more than that of the best positive reliable classifier from the
class. A closely related notion is fully reliable agnostic learning, which
considers partial classifiers that are allowed to predict &quot;unknown&quot; on some
inputs. The best fully reliable partial classifier is one that makes no errors
and minimizes the probability of predicting &quot;unknown&quot;, and the goal in fully
reliable learning is to output a hypothesis that is almost as good as the best
fully reliable partial classifier from a class.
  For distribution-independent learning, the best known algorithms for PAC
learning typically utilize polynomial threshold representations, while the
state of the art agnostic learning algorithms use point-wise polynomial
approximations. We show that one-sided polynomial approximations, an
intermediate notion between polynomial threshold representations and point-wise
polynomial approximations, suffice for learning in the reliable agnostic
settings. We then show that majorities can be fully reliably learned and
disjunctions of majorities can be positive reliably learned, through
constructions of appropriate one-sided polynomial approximations. Our fully
reliable algorithm for majorities provides the first evidence that fully
reliable learning may be strictly easier than agnostic learning. Our algorithms
also satisfy strong attribute-efficiency properties, and provide smooth
tradeoffs between sample complexity and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5165</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5165</id><created>2014-02-20</created><authors><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author></authors><title>Axiomatic Approach to Solutions of Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider solutions of normal form games that are invariant under strategic
equivalence. We consider additional properties that can be expected (or be
desired) from a solution of a game, and we observe the following:
  - Even the weakest notion of individual rationality restricts the set of
solutions to be equilibria. This observation holds for all types of solutions:
in pure-strategies, in mixed strategies, and in correlated strategies where the
corresponding notions of equilibria are pure-Nash, Nash and coarse-correlated.
  An action profile is (strict) simultaneous maximizer if it simultaneously
globally (strictly) maximizes the payoffs of all players.
  - If we require that a simultaneous maximizer (if it exists) will be a
solution, then the solution contains the set of pure Nash equilibria.
  - There is no solution for which a strict simultaneous maximizer (if it
exists) is the unique solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5172</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5172</id><created>2014-02-20</created><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author></authors><title>Alternation in Quantum Programming: From Superposition of Data to
  Superposition of Programs</title><categories>cs.PL cs.LO quant-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.4379</comments><acm-class>D.3.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extract a novel quantum programming paradigm - superposition of programs -
from the design idea of a popular class of quantum algorithms, namely quantum
walk-based algorithms. The generality of this paradigm is guaranteed by the
universality of quantum walks as a computational model. A new quantum
programming language QGCL is then proposed to support the paradigm of
superposition of programs. This language can be seen as a quantum extension of
Dijkstra's GCL (Guarded Command Language). Surprisingly, alternation in GCL
splits into two different notions in the quantum setting: classical alternation
(of quantum programs) and quantum alternation, with the latter being introduced
in QGCL for the first time. Quantum alternation is the key program construct
for realizing the paradigm of superposition of programs.
  The denotational semantics of QGCL are defined by introducing a new
mathematical tool called the guarded composition of operator-valued functions.
Then the weakest precondition semantics of QGCL can straightforwardly derived.
Another very useful program construct in realizing the quantum programming
paradigm of superposition of programs, called quantum choice, can be easily
defined in terms of quantum alternation. The relation between quantum choices
and probabilistic choices is clarified through defining the notion of local
variables. We derive a family of algebraic laws for QGCL programs that can be
used in program verification, transformations and compilation. The expressive
power of QGCL is illustrated by several examples where various variants and
generalizations of quantum walks are conveniently expressed using quantum
alternation and quantum choice. We believe that quantum programming with
quantum alternation and choice will play an important role in further
exploiting the power of quantum computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5176</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5176</id><created>2014-02-20</created><authors><author><keyname>Hsiao</keyname><forenames>Ko-Jen</forenames></author><author><keyname>Calder</keyname><forenames>Jeff</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Pareto-depth for Multiple-query Image Retrieval</title><categories>cs.IR cs.LG stat.ML</categories><doi>10.1109/TIP.2014.2378057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most content-based image retrieval systems consider either one single query,
or multiple queries that include the same object or represent the same semantic
information. In this paper we consider the content-based image retrieval
problem for multiple query images corresponding to different image semantics.
We propose a novel multiple-query information retrieval algorithm that combines
the Pareto front method (PFM) with efficient manifold ranking (EMR). We show
that our proposed algorithm outperforms state of the art multiple-query
retrieval algorithms on real-world image databases. We attribute this
performance improvement to concavity properties of the Pareto fronts, and prove
a theoretical result that characterizes the asymptotic concavity of the fronts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5180</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5180</id><created>2014-02-20</created><updated>2015-03-04</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author></authors><title>Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$
  Updates</title><categories>cs.LG math.NA stat.ML</categories><comments>We have added an additional sub-algorithm to remove the (approximate)
  residual error left after the tensor power iteration</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide local and global convergence guarantees for
recovering CP (Candecomp/Parafac) tensor decomposition. The main step of the
proposed algorithm is a simple alternating rank-$1$ update which is the
alternating version of the tensor power iteration adapted for asymmetric
tensors. Local convergence guarantees are established for third order tensors
of rank $k$ in $d$ dimensions, when $k=o \bigl( d^{1.5} \bigr)$ and the tensor
components are incoherent. Thus, we can recover overcomplete tensor
decomposition. We also strengthen the results to global convergence guarantees
under stricter rank condition $k \le \beta d$ (for arbitrary constant $\beta &gt;
1$) through a simple initialization procedure where the algorithm is
initialized by top singular vectors of random tensor slices. Furthermore, the
approximate local convergence guarantees for $p$-th order tensors are also
provided under rank condition $k=o \bigl( d^{p/2} \bigr)$. The guarantees also
include tight perturbation analysis given noisy tensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5187</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5187</id><created>2014-02-20</created><authors><author><keyname>Lai</keyname><forenames>Chan-Yet</forenames></author><author><keyname>Zakaria</keyname><forenames>Nordin</forenames></author></authors><title>Towards an Intelligent Framework for Pressure-based 3D Curve Drawing</title><categories>cs.HC</categories><comments>This paper was rejected from GI 2014. Comment from the chief
  reviewer:All reviewers noted that the ideas behind this paper were promising,
  but felt that research was not quite sufficiently developed...Although all
  agreed that this idea is insightful and has the potential to lead to a
  valuable contribution,... the idea is not yet sufficiently developed to
  warrant publication</comments><acm-class>H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pen pressure is an input channel typically available in tablet pen device. To
date, little attention has been paid to the use of pressure in the domain of
graphical interaction, its usage largely limited to drawing and painting
program, typically for varying brush characteristic such as stroke width,
opacity and color. In this paper, we explore the use of pressure in 3D curve
drawing. The act of controlling pressure using pen, pencil and brush in real
life appears effortless, but to mimic this natural ability to control pressure
using a pressure sensitive pen in the realm of electronic medium is difficult.
Previous pressure based interaction work have proposed various signal
processing techniques to improve the accuracy in pressure control, but a
one-for-all signal processing solution tend not to work for different curve
types. We propose instead a framework which applies signal processing
techniques tuned to individual curve type. A neural network classifier is used
as a curve classifier. Based on the classification, a custom combination of
signal processing techniques is then applied. Results obtained point to the
feasibility and advantage of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5188</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5188</id><created>2014-02-20</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Collision free autonomous navigation and formation building for
  non-holonomic ground robots</title><categories>cs.RO math.OC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The primary objective of a safe navigation algorithm is to guide the object
from its current position to the target position while avoiding any collision
with the en-route obstacles, and the appropriate obstacle avoidance strategies
are the key factors to ensure safe navigation tasks in dynamic environments. In
this report, three different obstacle avoidance strategies for safe navigation
in dynamic environments have been presented. The biologically-inspired
navigation algorithm (BINA) is efficient in terms of avoidance time. The
equidistant based navigation algorithm (ENA) is able to achieve navigation task
with in uncertain dynamic environments. The navigation algorithm algorithm
based on an integrated environment representation (NAIER) allows the object to
seek a safe path through obstacles in unknown dynamic environment in a
human-like fashion. The performances and features of the proposed navigation
algorithms are confirmed by extensive simulation results and experiments with a
real non-holonomic mobile robot. The algorithms have been implemented on two
real control systems: intelligent wheelchair and robotic hospital bed. The
performance of the proposed algorithms with SAM and Flexbed demonstrate their
capabilities to achieve navigation tasks in complicated real time scenarios.
The proposed algorithms are easy to be implemented in real time and costly
efficient. An extra study on networked multi-robots formation building
algorithm is presented in this paper. A constructive and easy-to-implement
decentralised control is proposed for a formation building of a group of random
positioned objects. Furthermore, the problem of formation building with
anonymous objects is addressed. This randomised decentralised navigation
algorithm achieves the convergence to a desired configuration with probability
1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5192</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5192</id><created>2014-02-20</created><authors><author><keyname>Santipach</keyname><forenames>Wiroonsak</forenames></author><author><keyname>Mamat</keyname><forenames>Kritsada</forenames></author><author><keyname>Tonsirisittikun</keyname><forenames>Ake</forenames></author><author><keyname>Jiravanstit</keyname><forenames>Kaemmatat</forenames></author></authors><title>Power and Bit Allocation for Wireless OFDM Channels with Finite-Rate
  Feedback and Subcarrier Clustering</title><categories>cs.IT math.IT</categories><journal-ref>Kasetsart Journal (Natural Science), vol. 47, no. 6, pp. 898-908,
  Nov.-Dec. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study investigated the allocation of transmission power and bits for a
point-to-point orthogonal frequency-division multiplexing channel assuming
perfect channel information at the receiver, but imperfect channel information
at the transmitter. Channel information was quantized at the receiver and was
sent back to the transmitter via a finite-rate feedback channel. Based on
limited feedback from the receiver, the corresponding transmitter adapted the
power level and/or modulation across subcarriers. To reduce the amount of
feedback, subcarriers were partitioned into different clusters and an on/off
threshold-based power allocation was applied to subcarrier clusters. In
addition, two options were proposed to interpolate a channel frequency response
from a set of quantized channel gains and apply the optimal water-filling
allocation or a greedy bit allocation based on channel interpolation. Proposed
schemes with finite feedback rates were shown to perform close to the optimal
allocation without a feedback-rate constraint. In the numerical example,
channel capacity decreased about 6% from the optimum when one bit of feedback
per subcarrier was used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5194</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5194</id><created>2014-02-20</created><authors><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>Lu</keyname><forenames>Xiaoyi</forenames></author></authors><title>On Big Data Benchmarking</title><categories>cs.PF cs.DB</categories><comments>7 pages, 4 figures, 2 tables, accepted in BPOE-04
  (http://prof.ict.ac.cn/bpoe_4_asplos/)</comments><acm-class>D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data systems address the challenges of capturing, storing, managing,
analyzing, and visualizing big data. Within this context, developing benchmarks
to evaluate and compare big data systems has become an active topic for both
research and industry communities. To date, most of the state-of-the-art big
data benchmarks are designed for specific types of systems. Based on our
experience, however, we argue that considering the complexity, diversity, and
rapid evolution of big data systems, for the sake of fairness, big data
benchmarks must include diversity of data and workloads. Given this motivation,
in this paper, we first propose the key requirements and challenges in
developing big data benchmarks from the perspectives of generating data with 4V
properties (i.e. volume, velocity, variety and veracity) of big data, as well
as generating tests with comprehensive workloads for big data systems. We then
present the methodology on big data benchmarking designed to address these
challenges. Next, the state-of-the-art are summarized and compared, following
by our vision for future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5196</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5196</id><created>2014-02-20</created><authors><author><keyname>Nakanishi</keyname><forenames>Kensuke</forenames></author><author><keyname>Hara</keyname><forenames>Shinsuke</forenames></author><author><keyname>Matsuda</keyname><forenames>Takahiro</forenames></author><author><keyname>Takizawa</keyname><forenames>Kenichi</forenames></author><author><keyname>Ono</keyname><forenames>Fumie</forenames></author><author><keyname>Miura</keyname><forenames>Ryu</forenames></author></authors><title>Synchronization-Free Delay Tomography Based on Compressed Sensing</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay tomography has so far burdened source and receiver measurement nodes in
a network with two requirements such as path establishment and clock
synchronization between them. In this letter, we focus on the clock
synchronization problem in delay tomography and propose a synchronization-free
delay tomography scheme. The proposed scheme selects a path between source and
receiver measurement nodes as a reference path, which results in a loss of
equation in a conventional delay tomography problem. However, by utilizing
compressed sensing, the proposed scheme becomes robust to the loss. Simulation
experiments confirm that the proposed scheme works comparable to a conventional
delay tomography scheme in networks with no clock synchronization between
source and receiver measurement nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5205</identifier>
 <datestamp>2014-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5205</id><created>2014-02-21</created><authors><author><keyname>Thilagavathi</keyname><forenames>D.</forenames></author><author><keyname>Thanamani</keyname><forenames>Antony Selvadoss</forenames></author></authors><title>A Survey on Dynamic Job Scheduling in Grid Environment Based on
  Heuristic Algorithms</title><categories>cs.DC cs.AI</categories><comments>6 Pages, 1 FiguerE, &quot;Published with International Journal of Computer
  Trends and Technology (IJCTT)&quot;. arXiv admin note: contains excessive text
  overlap with other internet sources</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational Grids are a new trend in distributed computing systems. They
allow the sharing of geographically distributed resources in an efficient way,
extending the boundaries of what we perceive as distributed computing. Various
sciences can benefit from the use of grids to solve CPU-intensive problems,
creating potential benefits to the entire society. Job scheduling is an
integrated part of parallel and distributed computing. It allows selecting
correct match of resource for a particular job and thus increases the job
throughput and utilization of resources. Job should be scheduled in an
automatic way to make the system more reliable, accessible and less sensitive
to subsystem failures. This paper provides a survey on various heuristic
algorithms, used for scheduling in grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5207</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5207</id><created>2014-02-21</created><updated>2015-07-12</updated><authors><author><keyname>Bender</keyname><forenames>Michael A.</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Young</keyname><forenames>Maxwell</forenames></author></authors><title>How to Scale Exponential Backoff</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized exponential backoff is a widely deployed technique for
coordinating access to a shared resource. A good backoff protocol should,
arguably, satisfy three natural properties: (i) it should provide constant
throughput, wasting as little time as possible; (ii) it should require few
failed access attempts, minimizing the amount of wasted effort; and (iii) it
should be robust, continuing to work efficiently even if some of the access
attempts fail for spurious reasons. Unfortunately, exponential backoff has some
well-known limitations in two of these areas: it provides poor (sub-constant)
throughput (in the worst case), and is not robust (to resource acquisition
failures).
  The goal of this paper is to &quot;fix&quot; exponential backoff by making it scalable,
particularly focusing on the case where processes arrive in an on-line,
worst-case fashion. We present a relatively simple backoff
protocol~Re-Backoff~that has, at its heart, a version of exponential backoff.
It guarantees expected constant throughput with dynamic process arrivals and
requires only an expected polylogarithmic number of access attempts per
process.
  Re-Backoff is also robust to periods where the shared resource is unavailable
for a period of time. If it is unavailable for $D$ time slots, Re-Backoff
provides the following guarantees. When the number of packets is a finite $n$,
the average expected number of access attempts for successfully sending a
packet is $O(\log^2( n + D))$. In the infinite case, the average expected
number of access attempts for successfully sending a packet is $O( \log^2(\eta)
+ \log^2(D) )$ where $\eta$ is the maximum number of processes that are ever in
the system concurrently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5208</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5208</id><created>2014-02-21</created><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Kaligounder</keyname><forenames>Lakshmi</forenames></author></authors><title>Densely Entangled Financial Systems</title><categories>q-fin.RM cs.CE</categories><comments>to appear in Network Models in Economics and Finance, V. Kalyagin, P.
  M. Pardalos and T. M. Rassias (editors), Springer Optimization and Its
  Applications series, Springer, 2014</comments><msc-class>91G99, 91B30</msc-class><acm-class>J.1; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1] Zawadoski introduces a banking network model in which the asset and
counter-party risks are treated separately and the banks hedge their assets
risks by appropriate OTC contracts. In his model, each bank has only two
counter-party neighbors, a bank fails due to the counter-party risk only if at
least one of its two neighbors default, and such a counter-party risk is a low
probability event. Informally, the author shows that the banks will hedge their
asset risks by appropriate OTC contracts, and, though it may be socially
optimal to insure against counter-party risk, in equilibrium banks will {\em
not} choose to insure this low probability event.
  In this paper, we consider the above model for more general network
topologies, namely when each node has exactly 2r counter-party neighbors for
some integer r&gt;0. We extend the analysis of [1] to show that as the number of
counter-party neighbors increase the probability of counter-party risk also
increases, and in particular the socially optimal solution becomes privately
sustainable when each bank hedges its risk to at least n/2 banks, where n is
the number of banks in the network, i.e., when 2r is at least n/2, banks not
only hedge their asset risk but also hedge its counter-party risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5233</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5233</id><created>2014-02-21</created><authors><author><keyname>Rabindran</keyname><forenames>Dinesh</forenames></author><author><keyname>Tesar</keyname><forenames>Delbert</forenames></author></authors><title>Study of the Dynamic Coupling Term (\mu) in Parallel Force/Velocity
  Actuated Systems</title><categories>cs.RO</categories><comments>Proc. IEEE International Conference on Automation Science and
  Engineering, Sept. 2007, Scottsdale, Arizona, pp. 418-423</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Presented in this paper is an actuator concept, called a Parallel
Force/Velocity Actuator (PFVA), that combines two fundamentally distinct
actuators (one using low gear reduction or even direct drive, which we will
call a Force Actuator (FA) and the other with a high reduction gear train that
we will refer to as a Velocity Actuator (VA)). The objective of this work is to
evaluate the effect of the relative scale factor, RSF, (ratio of gear
reductions) between these inputs on their dynamic coupling. We conceptually
describe a Parallel Force/Velocity Actuator (PFVA) based on a
Dual-Input-Single- Output (DISO) epicyclic gear train. We then present an
analytical formulation for the variation of the dynamic coupling term w.r.t.
RSF. Conclusions from this formulation are illustrated through a numerical
example involving a 1-DOF four-bar linkage. It is shown, both analytically and
numerically, that as we increase the RSF, the two inputs to the PFVA are
decoupled w.r.t. the inertia torques. This understanding can serve as an
important design guideline for PFVAs. The paper also presents two limitations
of this study and suggests future work based on these caveats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5245</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5245</id><created>2014-02-21</created><authors><author><keyname>Anceaume</keyname><forenames>Emmanuelle</forenames><affiliation>IRISA, INRIA - SUPELEC</affiliation></author><author><keyname>Busnel</keyname><forenames>Yann</forenames><affiliation>LINA</affiliation></author><author><keyname>Sericola</keyname><forenames>Bruno</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>New results on a generalized coupon collector problem using Markov
  chains</title><categories>math.PR cs.DM</categories><comments>14 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper a generalized coupon collector problem, which consists
in determining the distribution and the moments of the time needed to collect a
given number of distinct coupons that are drawn from a set of coupons with an
arbitrary probability distribution. We suppose that a special coupon called the
null coupon can be drawn but never belongs to any collection. In this context,
we obtain expressions of the distribution and the moments of this time. We also
prove that the almost-uniform distribution, for which all the non-null coupons
have the same drawing probability, is the distribution which minimizes the
expected time to get a fixed subset of distinct coupons. This optimization
result is extended to the complementary distribution of that time when the full
collection is considered, proving by the way this well-known conjecture.
Finally, we propose a new conjecture which expresses the fact that the
almost-uniform distribution should minimize the complementary distribution of
the time needed to get any fixed number of distinct coupons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5255</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5255</id><created>2014-02-21</created><authors><author><keyname>von der Weth</keyname><forenames>Christian</forenames></author><author><keyname>Hauswirth</keyname><forenames>Manfred</forenames></author></authors><title>Analysing Parallel and Passive Web Browsing Behavior and its Effects on
  Website Metrics</title><categories>cs.HC cs.IR</categories><comments>22 pages, 11 figures, 3 tables, 29 references. arXiv admin note: text
  overlap with arXiv:1307.1542</comments><acm-class>H.1.2; H.5.3; H.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Getting deeper insights into the online browsing behavior of Web users has
been a major research topic since the advent of the WWW. It provides useful
information to optimize website design, Web browser design, search engines
offerings, and online advertisement. We argue that new technologies and new
services continue to have significant effects on the way how people browse the
Web. For example, listening to music clips on YouTube or to a radio station on
Last.fm does not require users to sit in front of their computer. Social media
and networking sites like Facebook or micro-blogging sites like Twitter have
attracted new types of users that previously were less inclined to go online.
These changes in how people browse the Web feature new characteristics which
are not well understood so far. In this paper, we provide novel and unique
insights by presenting first results of DOBBS, our long-term effort to create a
comprehensive and representative dataset capturing online user behavior. We
firstly investigate the concepts of parallel browsing and passive browsing,
showing that browsing the Web is no longer a dedicated task for many users.
Based on these results, we then analyze their impact on the calculation of a
user's dwell time -- i.e., the time the user spends on a webpage -- which has
become an important metric to quantify the popularity of websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5259</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5259</id><created>2014-02-21</created><updated>2014-05-04</updated><authors><author><keyname>Lv</keyname><forenames>Gattaca</forenames></author></authors><title>An Analysis of Rank Aggregation Algorithms</title><categories>cs.DS cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank aggregation is an essential approach for aggregating the preferences of
multiple agents. One rule of particular interest is the Kemeny rule, which
maximises the number of pairwise agreements between the final ranking and the
existing rankings. However, Kemeny rankings are NP-hard to compute. This has
resulted in the development of various algorithms. Fortunately, NP-hardness may
not reflect the difficulty of solving problems that arise in practice. As a
result, we aim to demonstrate that the Kemeny consensus can be computed
efficiently when aggregating different rankings in real case. In this paper, we
extend a dynamic programming algorithm originally for Kemeny scores. We also
provide details on the implementation of the algorithm. Finally, we present
results obtained from an empirical comparison of our algorithm and two other
popular algorithms based on real world and randomly generated problem
instances. Experimental results show the usefulness and efficiency of the
algorithm in practical settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5265</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5265</id><created>2014-02-21</created><updated>2014-11-12</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Coalitional Games in MISO Interference Channels: Epsilon-Core and
  Coalition Structure Stable Set</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Signal Processing, 14 pages, 14
  figures, 3 tables</comments><doi>10.1109/TSP.2014.2367466</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple-input single-output interference channel is considered. Each
transmitter is assumed to know the channels between itself and all receivers
perfectly and the receivers are assumed to treat interference as additive
noise. In this setting, noncooperative transmission does not take into account
the interference generated at other receivers which generally leads to
inefficient performance of the links. To improve this situation, we study
cooperation between the links using coalitional games. The players (links) in a
coalition either perform zero forcing transmission or Wiener filter precoding
to each other. The $\epsilon$-core is a solution concept for coalitional games
which takes into account the overhead required in coalition deviation. We
provide necessary and sufficient conditions for the strong and weak
$\epsilon$-core of our coalitional game not to be empty with zero forcing
transmission. Since, the $\epsilon$-core only considers the possibility of
joint cooperation of all links, we study coalitional games in partition form in
which several distinct coalitions can form. We propose a polynomial time
distributed coalition formation algorithm based on coalition merging and prove
that its solution lies in the coalition structure stable set of our coalition
formation game. Simulation results reveal the cooperation gains for different
coalition formation complexities and deviation overhead models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5267</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5267</id><created>2014-02-21</created><authors><author><keyname>Neu</keyname><forenames>Holger</forenames></author><author><keyname>Hanne</keyname><forenames>Thomas</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Nickel</keyname><forenames>Stefan</forenames></author><author><keyname>Wirsen</keyname><forenames>Andreas</forenames></author></authors><title>Simulation-Based Risk Reduction for Planning Inspections</title><categories>cs.SE</categories><comments>16 pages. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F3-540-36209-6_9</comments><journal-ref>Product Focused Software Process Improvement, volume 2559 of
  Lecture Notes in Computer Science, pages 78-93. Springer Berlin Heidelberg,
  2002</journal-ref><doi>10.1007/3-540-36209-6_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizations that develop software have recognized that software process
models are particularly useful for maintaining a high standard of quality. In
the last decade, simulations of software processes were used in several
settings and environments. This paper gives a short overview of the benefits of
software process simulation and describes the development of a discrete-event
model, a technique rarely used before in that field. The model introduced in
this paper captures the behavior of a detailed code inspection process. It aims
at reducing the risks inherent in implementing inspection processes and
techniques in the overall development process. The determination of the
underlying cause-effect relations using data mining techniques and empirical
data is explained. Finally, the paper gives an outlook on our future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5275</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5275</id><created>2014-02-21</created><authors><author><keyname>Chatterjee</keyname><forenames>Tanusree</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Abhishek</forenames></author></authors><title>VHDL Modeling of Intrusion Detection &amp; Prevention System (IDPS) A Neural
  Network Approach</title><categories>cs.CR</categories><doi>10.14445/22312803/IJCTT-V8P110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development and expansion of World Wide Web and network systems
have changed the computing world in the last decade and also equipped the
intruders and hackers with new facilities for their destructive purposes. The
cost of temporary or permanent damages caused by unauthorized access of the
intruders to computer systems has urged different organizations to increasingly
implement various systems to monitor data flow in their network. The systems
are generally known as Intrusion Detection System (IDS).Our objective is to
implement an artificial network approach to the design of intrusion detection
and prevention system and finally convert the designed model to a VHDL (Very
High Speed Integrated Circuit Hardware Descriptive Language) code. This feature
enables the system to suggest proper actions against possible attacks. The
promising results of the present study show the potential applicability of ANNs
for developing practical IDSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5284</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5284</id><created>2014-02-21</created><updated>2015-04-22</updated><authors><author><keyname>Schneider</keyname><forenames>Reinhold</forenames></author><author><keyname>Uschmajew</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Convergence results for projected line-search methods on varieties of
  low-rank matrices via \L{}ojasiewicz inequality</title><categories>math.OC cs.LG math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to derive convergence results for projected
line-search methods on the real-algebraic variety $\mathcal{M}_{\le k}$ of real
$m \times n$ matrices of rank at most $k$. Such methods extend Riemannian
optimization methods, which are successfully used on the smooth manifold
$\mathcal{M}_k$ of rank-$k$ matrices, to its closure by taking steps along
gradient-related directions in the tangent cone, and afterwards projecting back
to $\mathcal{M}_{\le k}$. Considering such a method circumvents the
difficulties which arise from the nonclosedness and the unbounded curvature of
$\mathcal{M}_k$. The pointwise convergence is obtained for real-analytic
functions on the basis of a \L{}ojasiewicz inequality for the projection of the
antigradient to the tangent cone. If the derived limit point lies on the smooth
part of $\mathcal{M}_{\le k}$, i.e. in $\mathcal{M}_k$, this boils down to more
or less known results, but with the benefit that asymptotic convergence rate
estimates (for specific step-sizes) can be obtained without an a priori
curvature bound, simply from the fact that the limit lies on a smooth manifold.
At the same time, one can give a convincing justification for assuming critical
points to lie in $\mathcal{M}_k$: if $X$ is a critical point of $f$ on
$\mathcal{M}_{\le k}$, then either $X$ has rank $k$, or $\nabla f(X) = 0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5287</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5287</id><created>2014-02-21</created><updated>2014-03-24</updated><authors><author><keyname>Beliakov</keyname><forenames>Gleb</forenames></author></authors><title>On fast matrix-vector multiplication with a Hankel matrix in
  multiprecision arithmetics</title><categories>math.NA cs.NA</categories><comments>11 pages</comments><msc-class>65F30, 65F15, 15B05</msc-class><acm-class>F.2.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two fast algorithms for matrix-vector multiplication $y=Ax$, where
$A$ is a Hankel matrix. The current asymptotically fastest method is based on
the Fast Fourier Transform (FFT), however in multiprecision arithmetics with
very high accuracy FFT method is actually slower than schoolbook multiplication
for matrix sizes up to $n=8000$. One method presented is based on a
decomposition of multiprecision numbers into sums, and applying standard or
double precision FFT. The second method, inspired by Karatsuba multiplication,
is based on recursively performing multiplications with matrices of half-size
of the original. Its complexity in terms of the matrix size $n$ is
$\Theta(n^{\log 3})$. Both methods are applicable to Toeplitz matrices and to
circulant matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5303</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5303</id><created>2014-02-21</created><updated>2015-05-17</updated><authors><author><keyname>Fox-Epstein</keyname><forenames>Eli</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Diffuse Reflection Radius in a Simple Polygon</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that every simple polygon in general position with $n$ walls can
be illuminated from a single point light source $s$ after at most $\lfloor
(n-2)/4\rfloor$ diffuse reflections, and this bound is the best possible. A
point $s$ with this property can be computed in $O(n\log n)$ time. It is also
shown that the minimum number of diffuse reflections needed to illuminate a
given simple polygon from a single point can be approximated up to an additive
constant in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5310</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5310</id><created>2014-02-21</created><updated>2014-02-27</updated><authors><author><keyname>Morrison</keyname><forenames>Donn</forenames></author></authors><title>Toward automatic censorship detection in microblogs</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages. Updated with example cascades figure and typo fixes. To
  appear at the International Workshop on Data Mining in Social Networks
  (PAKDD-SocNet) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media is an area where users often experience censorship through a
variety of means such as the restriction of search terms or active and
retroactive deletion of messages. In this paper we examine the feasibility of
automatically detecting censorship of microblogs. We use a network growing
model to simulate discussion over a microblog follow network and compare two
censorship strategies to simulate varying levels of message deletion. Using
topological features extracted from the resulting graphs, a classifier is
trained to detect whether or not a given communication graph has been censored.
The results show that censorship detection is feasible under empirically
measured levels of message deletion. The proposed framework can enable
automated censorship measurement and tracking, which, when combined with
aggregated citizen reports of censorship, can allow users to make informed
decisions about online communication habits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5311</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5311</id><created>2014-02-21</created><authors><author><keyname>Aharoni</keyname><forenames>Eldar</forenames></author><author><keyname>Kushilevitz</keyname><forenames>Eyal</forenames></author></authors><title>On the Power of Multiplexing in Number-on-the-Forehead Protocols</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the direct-sum problem for $k$-party ``Number On the Forehead''
(NOF) deterministic communication complexity. We prove several positive
results, showing that the complexity of computing a function $f$ in this model,
on $\ell$ instances, may be significantly cheaper than $\ell$ times the
complexity of computing $f$ on a single instance. Quite surprisingly, we show
that this is the case for ``most'' (boolean, $k$-argument) functions. We then
formalize two-types of sufficient conditions on a NOF protocol $Q$, for a
single instance, each of which guarantees some communication complexity savings
when appropriately extending $Q$ to work on $\ell$ instances. One such
condition refers to what each party needs to know about inputs of the other
parties, and the other condition, additionally, refers to the communication
pattern that the single-instance protocol $Q$ uses. In both cases, the tool
that we use is ``multiplexing'': we combine messages sent in parallel
executions of protocols for a single instance, into a single message for the
multi-instance (direct-sum) case, by xoring them with each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5323</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5323</id><created>2014-02-20</created><authors><author><keyname>Bell</keyname><forenames>Francis</forenames></author><author><keyname>Zhao</keyname><forenames>Chunyu</forenames></author><author><keyname>Sacan</keyname><forenames>Ahmet</forenames></author></authors><title>PDBCirclePlot: A Novel Visualization Method for Protein Structures</title><categories>q-bio.QM cs.CE q-bio.BM</categories><comments>Application note, 5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive molecular graphics applications facilitate analysis of three
dimensional protein structures. Naturally, non-interactive 2-D snapshots of the
protein structures do not convey the same level of geometric detail. Several
2-D visualization methods have been in use to summarize structural information,
including contact maps and 2-D cartoon views. We present a new approach for 2-D
visualization of protein structures where amino acid residues are displayed on
a circle and spatially close residues are depicted by links. Furthermore,
residue-specific properties, such as conservation, accessibility, temperature
factor, can be displayed as plots on the same circular view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5324</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5324</id><created>2014-02-21</created><updated>2015-07-07</updated><authors><author><keyname>Jones</keyname><forenames>Alex D.</forenames></author><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author></authors><title>On Asymptotic Incoherence and its Implications for Compressed Sensing of
  Inverse Problems</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been shown that incoherence is an unrealistic assumption for
compressed sensing when applied to many inverse problems. Instead, the key
property that permits efficient recovery in such problems is so-called local
incoherence. Similarly, the standard notion of sparsity is also inadequate for
many real world problems. In particular, in many applications, the optimal
sampling strategy depends on asymptotic incoherence and the signal sparsity
structure. The purpose of this paper is to study asymptotic incoherence and its
implications towards the design of optimal sampling strategies and efficient
sparsity bases. It is determined how fast asymptotic incoherence can decay in
general for isometries. Furthermore it is shown that Fourier sampling and
wavelet sparsity, whilst globally coherent, yield optimal asymptotic
incoherence as a power law up to a constant factor. Sharp bounds on the
asymptotic incoherence for Fourier sampling with polynomial bases are also
provided. A numerical experiment is also presented to demonstrate the role of
asymptotic incoherence in finding good subsampling strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5326</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5326</id><created>2014-02-21</created><updated>2015-08-31</updated><authors><author><keyname>Li</keyname><forenames>Cheuk Ting</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author></authors><title>Channel Diversity needed for Vector Interference Alignment</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures, Submitted to IEEE Transactions on Information
  Theory, under review. Short version presented at the International Symposium
  on Information Theory 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider vector space interference alignment strategies over the $K$-user
interference channel and derive an upper bound on the achievable degrees of
freedom as a function of the channel diversity $L$, where the channel diversity
is modeled by $L$ real-valued parallel channels with coefficients drawn from a
non-degenerate joint distribution. The seminal work of Cadambe and Jafar shows
that when $L$ is unbounded, vector space interference alignment can achieve
$1/2$ degrees of freedom per user independent of the number of users $K$.
However wireless channels have limited diversity in practice, dictated by their
coherence time and bandwidth, and an important question is the number of
degrees of freedom achievable at finite $L$. When $K=3$ and if $L$ is finite,
Bresler et al show that the number of degrees of freedom achievable with vector
space interference alignment is bounded away from $1/2$, and the gap decreases
inversely proportional to $L$. In this paper, we show that when $K\geq4$, the
gap is significantly larger. In particular, the gap to the optimal $1/2$
degrees of freedom per user can decrease at most like $1/\sqrt{L}$, and when
$L$ is smaller than the order of $2^{(K-2)(K-3)}$, it decays at most like
$1/\sqrt[4]{L}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5351</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5351</id><created>2014-02-21</created><updated>2014-02-28</updated><authors><author><keyname>Shu</keyname><forenames>Jian-Jun</forenames></author></authors><title>On generalized Tian Ji's horse racing strategy</title><categories>cs.GT</categories><journal-ref>Interdisciplinary Science Reviews, Vol. 37, No. 2, pp. 187-193,
  2012</journal-ref><doi>10.1179/0308018812Z.00000000014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tian Ji's horse racing strategy, a famous Chinese legend, constitutes a
promising concept to be applied to important issues in today's competitive
environment; this strategy is elaborated on and analyzed by examining the
general case. The mathematical formulation concerning the calculation of
winning, drawing or losing combinations and probabilities is presented to
illustrate the interesting insights on how ancient philosophies could promote
thinking in business competitiveness, in particular, the wisdom behind
sacrificing the part for the benefit of the whole or sacrificing the short-term
objectives in order to gain the long-term goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5358</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5358</id><created>2014-02-21</created><authors><author><keyname>K&#xe1;dek</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>P&#xe1;novics</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Extended Breadth-First Search Algorithm</title><categories>cs.AI</categories><comments>5 pages, 1 figure, 1 table</comments><msc-class>68T20</msc-class><acm-class>I.2.8</acm-class><journal-ref>International Journal of Computer Science Issues, Volume 10, Issue
  6, No 2, ISSN (Print): 1694-0814, ISSN (Online): 1694-0784, November 2013,
  pp. 78-82</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of artificial intelligence is to provide representation techniques
for describing problems, as well as search algorithms that can be used to
answer our questions. A widespread and elaborated model is state-space
representation, which, however, has some shortcomings. Classical search
algorithms are not applicable in practice when the state space contains even
only a few tens of thousands of states. We can give remedy to this problem by
defining some kind of heuristic knowledge. In case of classical state-space
representation, heuristic must be defined so that it qualifies an arbitrary
state based on its &quot;goodness,&quot; which is obviously not trivial. In our paper, we
introduce an algorithm that gives us the ability to handle huge state spaces
and to use a heuristic concept which is easier to embed into search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5359</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5359</id><created>2014-02-21</created><updated>2014-09-16</updated><authors><author><keyname>Mohapatra</keyname><forenames>Parthajit</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>On the Capacity of the 2-User Interference Channel with Transmitter
  Cooperation and Secrecy Constraints</title><categories>cs.IT math.IT</categories><comments>73 pages, part of it has been accepted for publication in SPAWC 2013
  and NCC 2014. Submitted to TIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies how cooperation between the transmitters can help manage
interference and simultaneously ensure secrecy, in the 2-user Gaussian
symmetric interference channel. First, the problem is studied in the linear
deterministic setting, and achievable schemes are proposed, which use a
combination of interference cancelation, relaying of the other user's data
bits, time sharing, and transmission of random bits, depending on the rate of
the cooperative link and the relative strengths of the signal and the
interference. Outer bounds on the secrecy rate are also derived. The novelty in
the derivation lies in the careful selection of side information, and
partitioning the encoded message/output depending on the relative strength of
the signal and the interference. The inner and outer bounds are derived under
all possible parameter settings. It is found that, for some parameter settings,
the inner and outer bounds match, yielding the capacity of the symmetric linear
deterministic IC (SLDIC) under transmitter cooperation and secrecy constraints.
In other scenarios, the achievable rate matches with the capacity region of the
2-user SLDIC without secrecy constraints derived by Wang and Tse; thus, the
proposed scheme offers secrecy for free. Inspired by the achievable schemes and
outer bounds in the deterministic case, achievable schemes and outer bounds are
derived in the Gaussian case. The proposed achievable scheme for the Gaussian
case uses a combination of Marton's coding scheme and stochastic encoding along
with dummy message transmission. For both the models, one of the key techniques
used in the achievable scheme is interference cancelation, which offers two
seemingly conflicting benefits simultaneously: it cancels interference and
ensures secrecy. The results show that limited transmitter cooperation can
greatly facilitate secure communications over 2-user ICs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5360</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5360</id><created>2014-02-21</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya M.</forenames></author></authors><title>Important Molecular Descriptors Selection Using Self Tuned Reweighted
  Sampling Method for Prediction of Antituberculosis Activity</title><categories>cs.LG stat.AP stat.ML</categories><comments>published 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, a new descriptor selection method for selecting an optimal
combination of important descriptors of sulfonamide derivatives data, named
self tuned reweighted sampling (STRS), is developed. descriptors are defined as
the descriptors with large absolute coefficients in a multivariate linear
regression model such as partial least squares(PLS). In this study, the
absolute values of regression coefficients of PLS model are used as an index
for evaluating the importance of each descriptor Then, based on the importance
level of each descriptor, STRS sequentially selects N subsets of descriptors
from N Monte Carlo (MC) sampling runs in an iterative and competitive manner.
In each sampling run, a fixed ratio (e.g. 80%) of samples is first randomly
selected to establish a regresson model. Next, based on the regression
coefficients, a two-step procedure including rapidly decreasing function (RDF)
based enforced descriptor selection and self tuned sampling (STS) based
competitive descriptor selection is adopted to select the important
descriptorss. After running the loops, a number of subsets of descriptors are
obtained and root mean squared error of cross validation (RMSECV) of PLS models
established with subsets of descriptors is computed. The subset of descriptors
with the lowest RMSECV is considered as the optimal descriptor subset. The
performance of the proposed algorithm is evaluated by sulfanomide derivative
dataset. The results reveal an good characteristic of STRS that it can usually
locate an optimal combination of some important descriptors which are
interpretable to the biologically of interest. Additionally, our study shows
that better prediction is obtained by STRS when compared to full descriptor set
PLS modeling, Monte Carlo uninformative variable elimination (MC-UVE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5365</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5365</id><created>2014-02-21</created><updated>2014-03-04</updated><authors><author><keyname>Bernardo</keyname><forenames>Marco</forenames><affiliation>University of Urbino</affiliation></author><author><keyname>De Nicola</keyname><forenames>Rocco</forenames><affiliation>IMT Lucca</affiliation></author><author><keyname>Loreti</keyname><forenames>Michele</forenames><affiliation>University of Firenze</affiliation></author></authors><title>Revisiting Trace and Testing Equivalences for Nondeterministic and
  Probabilistic Processes</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (March 3,
  2014) lmcs:1137</journal-ref><doi>10.2168/LMCS-10(1:16)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two of the most studied extensions of trace and testing equivalences to
nondeterministic and probabilistic processes induce distinctions that have been
questioned and lack properties that are desirable. Probabilistic
trace-distribution equivalence differentiates systems that can perform the same
set of traces with the same probabilities, and is not a congruence for parallel
composition. Probabilistic testing equivalence, which relies only on extremal
success probabilities, is backward compatible with testing equivalences for
restricted classes of processes, such as fully nondeterministic processes or
generative/reactive probabilistic processes, only if specific sets of tests are
admitted. In this paper, new versions of probabilistic trace and testing
equivalences are presented for the general class of nondeterministic and
probabilistic processes. The new trace equivalence is coarser because it
compares execution probabilities of single traces instead of entire trace
distributions, and turns out to be compositional. The new testing equivalence
requires matching all resolutions of nondeterminism on the basis of their
success probabilities, rather than comparing only extremal success
probabilities, and considers success probabilities in a trace-by-trace fashion,
rather than cumulatively on entire resolutions. It is fully backward compatible
with testing equivalences for restricted classes of processes; as a
consequence, the trace-by-trace approach uniformly captures the standard
probabilistic testing equivalences for generative and reactive probabilistic
processes. The paper discusses in full details the new equivalences and
provides a simple spectrum that relates them with existing ones in the setting
of nondeterministic and probabilistic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5371</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5371</id><created>2014-02-21</created><updated>2014-05-15</updated><authors><author><keyname>Cafaro</keyname><forenames>Massimo</forenames></author><author><keyname>Civino</keyname><forenames>Roberto</forenames></author><author><keyname>Masucci</keyname><forenames>Barbara</forenames></author></authors><title>On the Equivalence of Two Security Notions for Hierarchical Key
  Assignment Schemes in the Unconditional Setting</title><categories>cs.CR cs.IT math.IT</categories><journal-ref>IEEE Transactions on Dependable and Secure Computing, Volume 12,
  Issue 4 (2015), pp. 485 - 490</journal-ref><doi>10.1109/TDSC.2014.2355841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The access control problem in a hierarchy can be solved by using a
hierarchical key assignment scheme, where each class is assigned an encryption
key and some private information. A formal security analysis for hierarchical
key assignment schemes has been traditionally considered in two different
settings, i.e., the unconditionally secure and the computationally secure
setting, and with respect to two different notions: security against key
recovery (KR-security) and security with respect to key indistinguishability
(KI-security), with the latter notion being cryptographically stronger.
Recently, Freire, Paterson and Poettering proposed strong key
indistinguishability (SKI-security) as a new security notion in the
computationally secure setting, arguing that SKI-security is strictly stronger
than KI-security in such a setting. In this paper we consider the
unconditionally secure setting for hierarchical key assignment schemes. In such
a setting the security of the schemes is not based on specific unproven
computational assumptions, i.e., it relies on the theoretical impossibility of
breaking them, despite the computational power of an adversary coalition. We
prove that, in this setting, SKI-security is not stronger than KI-security,
i.e., the two notions are fully equivalent from an information-theoretic point
of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5379</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5379</id><created>2014-02-01</created><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author></authors><title>What Is It Like to Be a Brain Simulation?</title><categories>cs.AI</categories><comments>10 pages, draft of conference paper published in AGI 2012, also
  accepted to AISB 2012 but it was too late to arrange travel, unfortunately;
  Artificial General Intelligence, 5th International Conference, AGI 2012,
  Oxford, UK, December 8-11, 2012. Proceedings</comments><msc-class>68T01</msc-class><doi>10.1007/978-3-642-35506-6_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We frame the question of what kind of subjective experience a brain
simulation would have in contrast to a biological brain. We discuss the brain
prosthesis thought experiment. We evaluate how the experience of the brain
simulation might differ from the biological, according to a number of
hypotheses about experience and the properties of simulation. Then, we identify
finer questions relating to the original inquiry, and answer them from both a
general physicalist, and panexperientialist perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5380</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5380</id><created>2014-02-01</created><authors><author><keyname>&#xd6;zkural</keyname><forenames>Eray</forenames></author></authors><title>Godseed: Benevolent or Malevolent?</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that benign looking AI objectives may result in powerful AI
drives that may pose a risk to the human society. We examine the alternative
scenario of what happens when universal goals that are not human-centric are
used for designing AI agents. We follow a design approach that tries to exclude
malevolent motivations from AI's, however, we see that even objectives that
seem benevolent at first may pose significant risk to humanity. We also discuss
various solution approaches including selfless goals, hybrid designs, universal
constraints, and generalization of robot laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5388</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5388</id><created>2014-02-21</created><authors><author><keyname>Masucci</keyname><forenames>Antonia Maria</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Silva</keyname><forenames>Alonso</forenames><affiliation>LINCS</affiliation></author></authors><title>Strategic Resource Allocation for Competitive Influence in Social
  Networks</title><categories>cs.SI cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main objectives of data mining is to help companies determine to
which potential customers to market and how many resources to allocate to these
potential customers. Most previous works on competitive influence in social
networks focus on the first issue. In this work, our focus is on the second
issue, i.e., we are interested on the competitive influence of marketing
campaigns who need to simultaneously decide how many resources to allocate to
their potential customers to advertise their products. Using results from game
theory, we are able to completely characterize the optimal strategic resource
allocation for the voter model of social networks and prove that the price of
competition of this game is unbounded. This work is a step towards providing a
solid foundation for marketing advertising in more general scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5391</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5391</id><created>2014-02-21</created><authors><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Coupechoux</keyname><forenames>Emilie</forenames></author></authors><title>Exact Simulation for Assemble-To-Order Systems</title><categories>math.PR cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop exact simulation (also known as perfect sampling) algorithms for a
family of assemble-to-order systems. Due to the finite capacity, and coupling
in demands and replenishments, known solving techniques are inefficient for
larger problem instances. We first consider the case with individual
replenishments of items, and derive an event based representation of the Markov
chain that allows applying existing exact simulation techniques, using the
monotonicity properties or bounding chains. In the case of joint
replenishments, the state space becomes intractable for the existing methods.
We propose new exact simulation algorithms, based on aggregation and bounding
chains, that allow a significant reduction of the state space of the Markov
chain. We also discuss the coupling times of considered models and provide
sufficient conditions for linear (in the single server replenishment case) or
quadratic (many server case) complexity of our algorithms in terms of the total
capacity in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5422</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5422</id><created>2014-02-21</created><authors><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author></authors><title>Twofold Video Hashing with Automatic Synchronization</title><categories>cs.MM</categories><comments>submitted to Image Processing (ICIP), 2014 21st IEEE International
  Conference on</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video hashing finds a wide array of applications in content authentication,
robust retrieval and anti-piracy search. While much of the existing research
has focused on extracting robust and secure content descriptors, a significant
open challenge still remains: Most existing video hashing methods are fallible
to temporal desynchronization. That is, when the query video results by
deleting or inserting some frames from the reference video, most existing
methods assume the positions of the deleted (or inserted) frames are either
perfectly known or reliably estimated. This assumption may be okay under
typical transcoding and frame-rate changes but is highly inappropriate in
adversarial scenarios such as anti-piracy video search. For example, an illegal
uploader will try to bypass the 'piracy check' mechanism of YouTube/Dailymotion
etc by performing a cleverly designed non-uniform resampling of the video. We
present a new solution based on dynamic time warping (DTW), which can implement
automatic synchronization and can be used together with existing video hashing
methods. The second contribution of this paper is to propose a new robust
feature extraction method called flow hashing (FH), based on frame averaging
and optical flow descriptors. Finally, a fusion mechanism called distance
boosting is proposed to combine the information extracted by DTW and FH.
Experiments on real video collections show that such a hash extraction and
comparison enables unprecedented robustness under both spatial and temporal
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5428</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5428</id><created>2014-02-21</created><authors><author><keyname>jebari</keyname><forenames>Khalid</forenames></author><author><keyname>Madiafi</keyname><forenames>Mohammed</forenames></author><author><keyname>Elmoujahid</keyname><forenames>Abdelaziz</forenames></author></authors><title>An Evolutionary approach for solving Shr\&quot;odinger Equation</title><categories>cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.0523</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The purpose of this paper is to present a method of solving the Shr\&quot;odinger
Equation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms
generations of trial solutions expressed in an analytical form. We illustrate
the effectiveness of this method providing, for example, the results of its
application to a quantum system minimal energy, and we compare these results
with those produced by traditional analytical methods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5436</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5436</id><created>2014-02-21</created><authors><author><keyname>Brignoli</keyname><forenames>Gianpaolo</forenames></author><author><keyname>Costantini</keyname><forenames>Stefania</forenames></author><author><keyname>D'Antona</keyname><forenames>Ottavio</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Characterizing and computing stable models of logic programs: The
  non-stratified case</title><categories>cs.AI cs.LO</categories><comments>Proceedings of the Conference on Information Technology. Bhubaneswar,
  India, 1999. https://sites.google.com/site/citconference/</comments><acm-class>I.2; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stable Logic Programming (SLP) is an emergent, alternative style of logic
programming: each solution to a problem is represented by a stable model of a
deductive database/function-free logic program encoding the problem itself.
Several implementations now exist for stable logic programming, and their
performance is rapidly improving. To make SLP generally applicable, it should
be possible to check for consistency (i.e., existence of stable models) of the
input program before attempting to answer queries. In the literature, only
rather strong sufficient conditions have been proposed for consistency, e.g.,
stratification. This paper extends these results in several directions. First,
the syntactic features of programs, viz. cyclic negative dependencies,
affecting the existence of stable models are characterized, and their relevance
is discussed. Next, a new graph representation of logic programs, the Extended
Dependency Graph (EDG), is introduced, which conveys enough information for
reasoning about stable models (while the traditional Dependency Graph does
not). Finally, we show that the problem of the existence of stable models can
be reformulated in terms of coloring of the EDG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5440</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5440</id><created>2014-02-21</created><authors><author><keyname>Zheng</keyname><forenames>Youyi</forenames></author><author><keyname>Dorsey</keyname><forenames>Julie</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy</forenames></author></authors><title>Ergonomic-driven Geometric Exploration and Reshaping</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the following problem: given a set of man-made shapes,
e.g., chairs, can we quickly rank and explore the set of shapes with respect to
a given avatar pose? Answering this question requires identifying which shapes
are more suitable for the defined avatar and pose; and moreover, to provide
fast preview of how to alter the input geometry to better fit the deformed
shapes to the given avatar pose? The problem naturally links physical
proportions of human body and its interaction with object shapes in an attempt
to connect ergonomics with shape geometry. We designed an interaction system
that allows users to explore shape collections using the deformation of human
characters while at the same time providing interactive previews of how to
alter the shapes to better fit the user-specified character. We achieve this by
first mapping ergonomics guidelines into a set of simultaneous multi-part
constraints based on target contacts; and then, proposing a novel contact-based
deformation model to realize multi-contact constraints. We evaluate our
framework on various chair models and validate the results via a small user
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5443</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5443</id><created>2014-02-21</created><authors><author><keyname>Weng</keyname><forenames>Lilian</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>Topicality and Social Impact: Diverse Messages but Focused Messengers</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>9 pages, 7 figures, 6 tables</comments><doi>10.1371/journal.pone.0118410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Are users who comment on a variety of matters more likely to achieve high
influence than those who delve into one focused field? Do general Twitter
hashtags, such as #lol, tend to be more popular than novel ones, such as
#instantlyinlove? Questions like these demand a way to detect topics hidden
behind messages associated with an individual or a hashtag, and a gauge of
similarity among these topics. Here we develop such an approach to identify
clusters of similar hashtags by detecting communities in the hashtag
co-occurrence network. Then the topical diversity of a user's interests is
quantified by the entropy of her hashtags across different topic clusters. A
similar measure is applied to hashtags, based on co-occurring tags. We find
that high topical diversity of early adopters or co-occurring tags implies high
future popularity of hashtags. In contrast, low diversity helps an individual
accumulate social influence. In short, diverse messages and focused messengers
are more likely to gain impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5449</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5449</id><created>2014-02-21</created><authors><author><keyname>Gathen</keyname><forenames>Joachim von zur</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Circulant graphs and GCD and LCM of Subsets</title><categories>cs.CC math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two sets $A$ and $B$ of integers, we consider the problem of finding a
set $S \subseteq A$ of the smallest possible cardinality such the greatest
common divisor of the elements of $S \cup B$ equals that of those of $A \cup
B$. The particular cases of $B = \emptyset$ and $\#B = 1$ are of special
interest and have some links with graph theory. We also consider the
corresponding question for the least common multiple of the elements. We
establish NP-completeness and approximation results for these problems by
relating them to the Minimum Cover Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5450</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5450</id><created>2014-02-21</created><updated>2014-12-10</updated><authors><author><keyname>Rotella</keyname><forenames>Nicholas</forenames></author><author><keyname>Bloesch</keyname><forenames>Michael</forenames></author><author><keyname>Righetti</keyname><forenames>Ludovic</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>State Estimation for a Humanoid Robot</title><categories>cs.RO</categories><comments>IROS 2014 Submission, IEEE/RSJ International Conference on
  Intelligent Robots and Systems (2014) 952-958</comments><doi>10.1109/IROS.2014.6942674</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a framework for state estimation on a humanoid robot
platform using only common proprioceptive sensors and knowledge of leg
kinematics. The presented approach extends that detailed in [1] on a quadruped
platform by incorporating the rotational constraints imposed by the humanoid's
flat feet. As in previous work, the proposed Extended Kalman Filter (EKF)
accommodates contact switching and makes no assumptions about gait or terrain,
making it applicable on any humanoid platform for use in any task. The filter
employs a sensor-based prediction model which uses inertial data from an IMU
and corrects for integrated error using a kinematics-based measurement model
which relies on joint encoders and a kinematic model to determine the relative
position and orientation of the feet. A nonlinear observability analysis is
performed on both the original and updated filters and it is concluded that the
new filter significantly simplifies singular cases and improves the
observability characteristics of the system. Results on simulated walking and
squatting datasets demonstrate the performance gain of the flat-foot filter as
well as confirm the results of the presented observability analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5456</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5456</id><created>2014-02-21</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Chai</keyname><forenames>Bo</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy Management for a User Interactive Smart Community: A Stackelberg
  Game Approach</title><categories>cs.SY cs.GT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a three party energy management problem in a user
interactive smart community that consists of a large number of residential
units (RUs) with distributed energy resources (DERs), a shared facility
controller (SFC) and the main grid. A Stackelberg game is formulated to benefit
both the SFC and RUs, in terms of incurred cost and achieved utility
respectively, from their energy trading with each other and the grid. The
properties of the game are studied and it is shown that there exists a unique
Stackelberg equilibrium (SE). A novel algorithm is proposed that can be
implemented in a distributed fashion by both RUs and the SFC to reach the SE.
The convergence of the algorithm is also proven, and shown to always reach the
SE. Numerical examples are used to assess the properties and effectiveness of
the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5458</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5458</id><created>2014-02-21</created><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Kutty</keyname><forenames>Sindhu</forenames></author><author><keyname>Lahaie</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Sami</keyname><forenames>Rahul</forenames></author></authors><title>Information Aggregation in Exponential Family Markets</title><categories>cs.AI cs.GT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of prediction market mechanisms known as automated
market makers. We show that we can design these mechanisms via the mold of
\emph{exponential family distributions}, a popular and well-studied probability
distribution template used in statistics. We give a full development of this
relationship and explore a range of benefits. We draw connections between the
information aggregation of market prices and the belief aggregation of learning
agents that rely on exponential family distributions. We develop a very natural
analysis of the market behavior as well as the price equilibrium under the
assumption that the traders exhibit risk aversion according to exponential
utility. We also consider similar aspects under alternative models, such as
when traders are budget constrained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5461</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5461</id><created>2014-02-21</created><authors><author><keyname>Rabindran</keyname><forenames>Dinesh</forenames></author></authors><title>Preliminary Studies on Force/Motion Control of Intelligent Mechanical
  Systems</title><categories>cs.RO</categories><comments>Masters Thesis, Mechanical Engineering Department, The University of
  Texas at Austin, 2004</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  To rationalize the relatively high investment that industrial automation
systems entail, research in the field of intelligent machines should target
high value functions such as fettling, die-finishing, deburring, and
fixtureless manufacturing. For achieving this goal, past work has concentrated
on force control algorithms at the system level with limited focus on
performance expansion at the actuator level. We present a comprehensive
literature review on robot force control, including algorithms, specialized
actuators, and robot control software. A robot force control testbed was
developed using Schunk's PowerCube 6-DOF Arm and a six-axis ATI force/torque
sensor. Using parameter identification experiments, manipulator module inertias
and the motor torque constant were estimated. Experiments were conducted to
study the practical issues involved in implementing stable contact transitions
and programmable endpoint impedance. Applications to human augmentation,
virtual fixtures, and teleoperation are discussed. These experiments are used
as a vehicle to understand the performance improvement achievable at the
actuator level. The approach at UTRRG has been to maximize the choices within
the actuator to enhance its intelligence. Drawing on this 20-year research
history in electromechanical actuator architecture, we propose a new concept
that mixes two inputs, distinct in their velocity ratios, within the same dual
actuator called a Force/Motion Actuator (FMA). Detailed kinematic and dynamic
models of this dual actuator are developed. The actuator performance is
evaluated using simulations with an output velocity specification and resolving
input trajectories using a minimum-norm solution. It is shown that a design
choice of 14:1 motion scaling between the two inputs results in good
sensitivity to output force disturbances without compromising velocity tracking
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5466</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5466</id><created>2014-02-21</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasyya M.</forenames></author></authors><title>Predictive Comparative QSAR analysis of Sulfathiazole Analogues as
  Mycobacterium Tuberculosis H37RV Inhabitors</title><categories>cs.CE q-bio.QM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.2841</comments><journal-ref>published 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Antitubercular activity of Sulfathiazole Derivitives series were subjected to
Quantitative Structure Activity Relationship (QSAR) Analysis with an attempt to
derive and understand a correlation between the Biologically Activity as
dependent variable and various descriptors as independent variables. QSAR
models generated using 28 compounds. Several statistical regression expressions
were obtained using Partial Least Squares (PLS) Regression, Multiple Linear
Regression (MLR) and Principal Component Regression (PCR) methods. The among
these methods, Partial Least Square Regression (PLS) method has shown very
promising result as compare to other two methods. A QSAR model was generated by
a training set of 18 molecules with correlation coefficient r (r square) of
0.9191, significant cross validated correlation coefficient (q square) of
0.8300, F test of 53.5783, r square for external test set pred_r square
-3.6132, coefficient of correlation of predicted data set pred_r_se square
1.4859 and degree of freedom 14 by Partial Least Squares Regression Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5468</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5468</id><created>2014-02-21</created><authors><author><keyname>King</keyname><forenames>Ji</forenames></author></authors><title>Uncertainty Principle in Control Theory, Part I: Analysis of Performance
  Limitations</title><categories>cs.SY</categories><comments>20 pages, 6 figures</comments><msc-class>93Axx, 93Cxx</msc-class><acm-class>F.2.3; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates performance limitations and tradeoffs in the control
design for linear time-invariant systems. It is shown that control
specifications in time domain and in frequency domain are always mutually
exclusive determined by uncertainty relations. The uncertainty principle from
quantum mechanics and harmonic analysis therefore embeds itself inherently in
control theory. The relations among transient specifications, system bandwidth
and control energy are obtained within the framework of uncertainty principle.
If the control system is provided with a large bandwidth or great control
energy, then it can ensure transient specifications as good as it can be. Such
a control system could be approximated by prolate spheroidal wave functions.
The obtained results are also applicable to filter design due to the duality of
filtering and control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5472</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5472</id><created>2014-02-21</created><updated>2014-08-11</updated><authors><author><keyname>Zhou</keyname><forenames>Jianqin</forenames></author><author><keyname>Liu</keyname><forenames>Wanquan</forenames></author><author><keyname>Zhou</keyname><forenames>Guanglu</forenames></author></authors><title>On the $k$-error linear complexity for $p^n$-periodic binary sequences
  via hypercube theory</title><categories>cs.CR</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.1829, arXiv:1312.6927</comments><msc-class>94A55, 94A60, 11B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear complexity and the $k$-error linear complexity of a binary
sequence are important security measures for key stream strength. By studying
binary sequences with the minimum Hamming weight, a new tool named as hypercube
theory is developed for $p^n$-periodic binary sequences. In fact, hypercube
theory is based on a typical sequence decomposition and it is a very important
tool in investigating the critical error linear complexity spectrum proposed by
Etzion et al. To demonstrate the importance of hypercube theory, we first give
a standard hypercube decomposition based on a well-known algorithm for
computing linear complexity and show that the linear complexity of the first
hypercube in the decomposition is equal to the linear complexity of the
original sequence. Second, based on such decomposition, we give a complete
characterization for the first decrease of the linear complexity for a
$p^n$-periodic binary sequence $s$. This significantly improves the current
existing results in literature. As to the importance of the hypercube, we
finally derive a counting formula for the $m$-hypercubes with the same linear
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5475</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5475</id><created>2014-02-21</created><authors><author><keyname>Cai</keyname><forenames>Xiao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Li</keyname><forenames>Chunguang</forenames></author></authors><title>Soft Consistency Reconstruction: A Robust 1-bit Compressive Sensing
  Algorithm</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of recovering algorithms for 1-bit compressive sensing (CS) named
Soft Consistency Reconstructions (SCRs) are proposed. Recognizing that CS
recovery is essentially an optimization problem, we endeavor to improve the
characteristics of the objective function under noisy environments. With a
family of re-designed consistency criteria, SCRs achieve remarkable
counter-noise performance gain over the existing counterparts, thus acquiring
the desired robustness in many real-world applications. The benefits of soft
decisions are exemplified through structural analysis of the objective
function, with intuition described for better understanding. As expected,
through comparisons with existing methods in simulations, SCRs demonstrate
preferable robustness against noise in low signal-to-noise ratio (SNR) regime,
while maintaining comparable performance in high SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5477</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5477</id><created>2014-02-21</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author></authors><title>Mobile Conductance and Gossip-based Information Spreading in Mobile
  Networks</title><categories>cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.2586</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general analytical framework for information
spreading in mobile networks based on a new performance metric, mobile
conductance, which allows us to separate the details of mobility models from
the study of mobile spreading time. We derive a general result for the
information spreading time in mobile networks in terms of this new metric, and
instantiate it through several popular mobility models. Large scale network
simulation is conducted to verify our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5481</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5481</id><created>2014-02-22</created><updated>2015-02-09</updated><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Kallus</keyname><forenames>Nathan</forenames></author></authors><title>From Predictive to Prescriptive Analytics</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we combine ideas from machine learning (ML) and operations
research and management science (OR/MS) in developing a framework, along with
specific methods, for using data to prescribe decisions in OR/MS problems. In a
departure from other work on data-driven optimization and reflecting our
practical experience with the data available in applications of OR/MS, we
consider data consisting, not only of observations of quantities with direct
effect on costs/revenues, such as demand or returns, but predominantly of
observations of associated auxiliary quantities. The main problem of interest
is a conditional stochastic optimization problem, given imperfect observations,
where the joint probability distributions that specify the problem are unknown.
We demonstrate that our proposed solution methods are generally applicable to a
wide range of decision problems. We prove that they are computationally
tractable and asymptotically optimal under mild conditions even when data is
not independent and identically distributed (iid) and even for censored
observations. As an analogue to the coefficient of determination $R^2$, we
develop a metric $P$ termed the coefficient of prescriptiveness to measure the
prescriptive content of data and the efficacy of a policy from an operations
perspective. To demonstrate the power of our approach in a real-world setting
we study an inventory management problem faced by the distribution arm of an
international media conglomerate, which ships an average of 1 billion units per
year. We leverage both internal data and public online data harvested from
IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that
outperform baseline measures. Specifically, the data we collect, leveraged by
our methods, accounts for an 88% improvement as measured by our coefficient of
prescriptiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5483</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5483</id><created>2014-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Yin</keyname><forenames>Rui</forenames></author></authors><title>Energy Efficient Joint Source and Channel Sensing in Cognitive Radio
  Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel concept of Joint Source and Channel Sensing (JSCS) is introduced in
the context of Cognitive Radio Sensor Networks (CRSN). Every sensor node has
two basic tasks: application-oriented source sensing and ambient-oriented
channel sensing. The former is to collect the application-specific source
information and deliver it to the access point within some limit of distortion,
while the latter is to find the vacant channels and provide spectrum access
opportunities for the sensed source information. With in-depth exploration, we
find that these two tasks are actually interrelated when taking into account
the energy constraints. The main focus of this paper is to minimize the total
power consumed by these two tasks while bounding the distortion of the
application-specific source information. Firstly, we present a specific slotted
sensing and transmission scheme, and establish the multi-task power consumption
model. Secondly, we jointly analyze the interplay between these two sensing
tasks, and then propose a proper sensing and power allocation scheme to
minimize the total power consumption. Finally, Simulation results are given to
validate the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5486</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5486</id><created>2014-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author></authors><title>Rateless-Coding-Assisted Multi-Packet Spreading over Mobile Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel Rateless-coding-assisted Multi-Packet Relaying (RMPR) protocol is
proposed for large-size data spreading in mobile wireless networks. With this
lightweight and robust protocol, the packet redundancy is reduced by a factor
of $\sqrt n$, while the spreading time is reduced at least by a factor of $\ln
(n)$. Closed-form bounds and explicit non-asymptotic results are presented,
which are further validated through simulations. Besides, the packet
duplication phenomenon in the network setting is analyzed for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5488</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5488</id><created>2014-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author><author><keyname>Yin</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author></authors><title>Distributed Spectrum-Aware Clustering in Cognitive Radio Sensor Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel Distributed Spectrum-Aware Clustering (DSAC) scheme is proposed in
the context of Cognitive Radio Sensor Networks (CRSN). DSAC aims at forming
energy efficient clusters in a self-organized fashion while restricting
interference to Primary User (PU) systems. The spectrum-aware clustered
structure is presented where the communications consist of intra-cluster
aggregation and inter-cluster relaying. In order to save communication power,
the optimal number of clusters is derived and the idea of groupwise constrained
clustering is introduced to minimize intra-cluster distance under
spectrum-aware constraint. In terms of practical implementation, DSAC
demonstrates preferable scalability and stability because of its low complexity
and quick convergence under dynamic PU activity. Finally, simulation results
are given to validate the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5492</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5492</id><created>2014-02-22</created><updated>2014-07-01</updated><authors><author><keyname>Davoodi</keyname><forenames>Pooya</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>&#xd6;zkan</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>Cache-Oblivious Persistence</title><categories>cs.DS</categories><acm-class>F.2.2; E.1; E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial persistence is a general transformation that takes a data structure
and allows queries to be executed on any past state of the structure. The
cache-oblivious model is the leading model of a modern multi-level memory
hierarchy.We present the first general transformation for making
cache-oblivious model data structures partially persistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5495</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5495</id><created>2014-02-22</created><updated>2014-08-22</updated><authors><author><keyname>Dzik</keyname><forenames>Wojciech</forenames></author><author><keyname>Stronkowski</keyname><forenames>Michal M.</forenames></author></authors><title>Almost structural completeness; an algebraic approach</title><categories>math.LO cs.LO math.RA</categories><msc-class>08C15, 03G27, 03B45, 03B22, 06E25</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A deductive system is structurally complete if its admissible inference rules
are derivable. For several important systems, like modal logic S5, failure of
structural completeness is caused only by the underivability of passive rules,
i.e. rules that can not be applied to theorems of the system. Neglecting
passive rules leads to the notion of almost structural completeness, that
means, derivablity of admissible non-passive rules. Almost structural
completeness for quasivarieties and varieties of general algebras is
investigated here by purely algebraic means. The results apply to all
algebraizable deductive systems.
  Firstly, various characterizations of almost structurally complete
quasivarieties are presented. Two of them are general: expressed with finitely
presented algebras, and with subdirectly irreducible algebras. One is
restricted to quasivarieties with finite model property and equationally
definable principal relative congruences, where the condition is verifiable on
finite subdirectly irreducible algebras.
  Secondly, examples of almost structurally complete varieties are provided
Particular emphasis is put on varieties of closure algebras, that are known to
constitute adequate semantics for normal extensions of S4 modal logic. A
certain infinite family of such almost structurally complete, but not
structurally complete, varieties is constructed. Every variety from this family
has a finitely presented unifiable algebra which does not embed into any free
algebra for this variety. Hence unification in it is not unitary. This shows
that almost structural completeness is strictly weaker than projective
unification for varieties of closure algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5497</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5497</id><created>2014-02-22</created><authors><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Hanzi</forenames></author></authors><title>Efficient Semidefinite Spectral Clustering via Lagrange Duality</title><categories>cs.LG cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient approach to semidefinite spectral clustering (SSC),
which addresses the Frobenius normalization with the positive semidefinite
(p.s.d.) constraint for spectral clustering. Compared with the original
Frobenius norm approximation based algorithm, the proposed algorithm can more
accurately find the closest doubly stochastic approximation to the affinity
matrix by considering the p.s.d. constraint. In this paper, SSC is formulated
as a semidefinite programming (SDP) problem. In order to solve the high
computational complexity of SDP, we present a dual algorithm based on the
Lagrange dual formalization. Two versions of the proposed algorithm are
proffered: one with less memory usage and the other with faster convergence
rate. The proposed algorithm has much lower time complexity than that of the
standard interior-point based SDP solvers. Experimental results on both UCI
data sets and real-world image data sets demonstrate that 1) compared with the
state-of-the-art spectral clustering methods, the proposed algorithm achieves
better clustering performance; and 2) our algorithm is much more efficient and
can solve larger-scale SSC problems than those standard interior-point SDP
solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5500</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5500</id><created>2014-02-22</created><updated>2014-09-12</updated><authors><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Handbook of Network Analysis [KONECT -- the Koblenz Network Collection]</title><categories>cs.SI physics.soc-ph</categories><comments>53 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This is the Handbook of Network Analysis, the companion article to the KONECT
(Koblenz Network Collection) project. This project is intended to collect
network datasets, analyse them systematically, and provide both datasets and
the underlying network analysis code to researchers. This article outlines the
project, gives all definitions used within the project, reviews all network
statistics used, reviews all network plots used, and gives a brief overview of
the API used by KONECT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5503</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5503</id><created>2014-02-22</created><authors><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Chau</keyname><forenames>Yuen</forenames></author></authors><title>Distributed Compressed Wideband Sensing in Cognitive Radio Sensor
  Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel distributed compressed wideband sensing scheme for Cognitive Radio
Sensor Networks (CRSN) is proposed in this paper. Taking advantage of the
distributive nature of CRSN, the proposed scheme deploys only one single
narrowband sampler with ultra-low sampling rate at each nodes to accomplish the
wideband spectrum sensing. First, the practical structure of the compressed
sampler at each node is described in detail. Second, we show how the Fusion
Center (FC) exploits the sampled signals with their spectrum randomly-aliased
to detect the global wideband spectrum activity. Finally, the proposed scheme
is validated through extensive simulations, which shows that it is particularly
suitable for CRSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5511</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5511</id><created>2014-02-22</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author></authors><title>A Generalized Robust Filtering Framework for Nonlinear
  Differential-Algebraic Systems</title><categories>cs.SY math.OC</categories><comments>26 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized dynamical robust nonlinear filtering framework is established
for a class of Lipschitz differential algebraic systems, in which the
nonlinearities appear both in the state and measured output equations. The
system is assumed to be affected by norm-bounded disturbance and to have both
norm-bounded uncertainties in the realization matrices as well as nonlinear
model uncertainties. We synthesize a robust H_infty filter through semidefinite
programming and strict linear matrix inequalities (LMIs). The admissible
Lipschitz constants of the nonlinear functions are maximized through LMI
optimization. The resulting H_infty filter guarantees asymptotic stability of
the estimation error dynamics with prespecified disturbance attenuation level
and is robust against time-varying parametric uncertainties as well as
Lipschitz nonlinear additive uncertainty. Explicit bound on the tolerable
nonlinear uncertainty is derived based on a norm-wise robustness analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5516</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5516</id><created>2014-02-22</created><updated>2014-06-18</updated><authors><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Wang</keyname><forenames>Yajun</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>Minimizing Seed Set Selection with Probabilistic Coverage Guarantee in a
  Social Network</title><categories>cs.SI cs.DS</categories><comments>Conference version will appear in KDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A topic propagating in a social network reaches its tipping point if the
number of users discussing it in the network exceeds a critical threshold such
that a wide cascade on the topic is likely to occur. In this paper, we consider
the task of selecting initial seed users of a topic with minimum size so that
with a guaranteed probability the number of users discussing the topic would
reach a given threshold. We formulate the task as an optimization problem
called seed minimization with probabilistic coverage guarantee (SM-PCG). This
problem departs from the previous studies on social influence maximization or
seed minimization because it considers influence coverage with probabilistic
guarantees instead of guarantees on expected influence coverage. We show that
the problem is not submodular, and thus is harder than previously studied
problems based on submodular function optimization. We provide an approximation
algorithm and show that it approximates the optimal solution with both a
multiplicative ratio and an additive error. The multiplicative ratio is tight
while the additive error would be small if influence coverage distributions of
certain seed sets are well concentrated. For one-way bipartite graphs we
analytically prove the concentration condition and obtain an approximation
algorithm with an $O(\log n)$ multiplicative ratio and an $O(\sqrt{n})$
additive error, where $n$ is the total number of nodes in the social graph.
Moreover, we empirically verify the concentration condition in real-world
networks and experimentally demonstrate the effectiveness of our proposed
algorithm comparing to commonly adopted benchmark algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5521</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5521</id><created>2014-02-22</created><updated>2014-12-08</updated><authors><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Sagratella</keyname><forenames>Simone</forenames></author></authors><title>Parallel Selective Algorithms for Big Data Optimization</title><categories>cs.DC cs.IT cs.NA math.IT math.OC</categories><comments>This work is an extended version of the conference paper that has
  been presented at IEEE ICASSP'14. The first and the second author contributed
  equally to the paper. This revised version contains new numerical results on
  non convex quadratic problems</comments><doi>10.1109/TSP.2015.2399858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a decomposition framework for the parallel optimization of the sum
of a differentiable (possibly nonconvex) function and a (block) separable
nonsmooth, convex one. The latter term is usually employed to enforce structure
in the solution, typically sparsity. Our framework is very flexible and
includes both fully parallel Jacobi schemes and Gauss- Seidel (i.e.,
sequential) ones, as well as virtually all possibilities &quot;in between&quot; with only
a subset of variables updated at each iteration. Our theoretical convergence
results improve on existing ones, and numerical results on LASSO, logistic
regression, and some nonconvex quadratic problems show that the new method
consistently outperforms existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5524</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5524</id><created>2014-02-22</created><authors><author><keyname>Ohrimenko</keyname><forenames>Olga</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Tamassia</keyname><forenames>Roberto</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>The Melbourne Shuffle: Improving Oblivious Storage in the Cloud</title><categories>cs.CR cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple, efficient, and secure data-oblivious randomized shuffle
algorithm. This is the first secure data-oblivious shuffle that is not based on
sorting. Our method can be used to improve previous oblivious storage solutions
for network-based outsourcing of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5557</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5557</id><created>2014-02-22</created><updated>2014-07-02</updated><authors><author><keyname>Veneziano</keyname><forenames>Vito</forenames></author><author><keyname>Rainer</keyname><forenames>Austen W.</forenames></author><author><keyname>Haider</keyname><forenames>Sheraz</forenames></author></authors><title>When Agile Is Not Good Enough: an initial attempt at understanding how
  to make the right decision</title><categories>cs.SE cs.CY</categories><acm-class>D.2.9; K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particularly over the last ten years, Agile has attracted not only the
praises of a broad range of enthusiast software developers, but also the
criticism of others. Either way, adoption or rejection of Agile seems sometimes
to be based more on a questionable understanding than on a critical,
well-informed decision making process. In this paper, the dual nature of the
above criticism is discussed, and the arguments against Agile have been
classified within a critical taxonomy of risk factors. A decisional model and
tool based on such taxonomy are consequently proposed for supporting software
engineers and other stakeholders in the decision-making about whether or not to
use Agile. The tool, which is freely available online, comes with a set of
guidelines: its purpose is to facilitate the community of software developers
to contribute to further assessing the potential and the criticalities of Agile
Methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5564</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5564</id><created>2014-02-22</created><updated>2014-12-26</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>Structure Tensor Based Image Interpolation Method</title><categories>cs.CV</categories><comments>Accepted for publication in AEU - International Journal of
  Electronics and Communications</comments><doi>10.1016/j.aeue.2014.10.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature preserving image interpolation is an active area in image processing
field. In this paper a new direct edge directed image super-resolution
algorithm based on structure tensors is proposed. Using an isotropic Gaussian
filter, the structure tensor at each pixel of the input image is computed and
the pixels are classified to three distinct classes; uniform region, corners
and edges, according to the eigenvalues of the structure tensor. Due to
application of the isotropic Gaussian filter, the classification is robust to
noise presented in image. Based on the tangent eigenvector of the structure
tensor, the edge direction is determined and used for interpolation along the
edges. In comparison to some previous edge directed image interpolation
methods, the proposed method achieves higher quality in both subjective and
objective aspects. Also the proposed method outperforms previous methods in
case of noisy and JPEG compressed images. Furthermore, without the need for
optimization in the process, the algorithm can achieve higher speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5565</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5565</id><created>2014-02-22</created><authors><author><keyname>Johnson</keyname><forenames>David M.</forenames></author><author><keyname>Xiong</keyname><forenames>Caiming</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Semi-Supervised Nonlinear Distance Metric Learning via Forests of
  Max-Margin Cluster Hierarchies</title><categories>stat.ML cs.IR cs.LG</categories><comments>Manuscript submitted to SIGKDD on 21 Feb 2014</comments><msc-class>68T10 (Primary) 62H30 (Secondary)</msc-class><acm-class>I.5.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric learning is a key problem for many data mining and machine learning
applications, and has long been dominated by Mahalanobis methods. Recent
advances in nonlinear metric learning have demonstrated the potential power of
non-Mahalanobis distance functions, particularly tree-based functions. We
propose a novel nonlinear metric learning method that uses an iterative,
hierarchical variant of semi-supervised max-margin clustering to construct a
forest of cluster hierarchies, where each individual hierarchy can be
interpreted as a weak metric over the data. By introducing randomness during
hierarchy training and combining the output of many of the resulting
semi-random weak hierarchy metrics, we can obtain a powerful and robust
nonlinear metric model. This method has two primary contributions: first, it is
semi-supervised, incorporating information from both constrained and
unconstrained points. Second, we take a relaxed approach to constraint
satisfaction, allowing the method to satisfy different subsets of the
constraints at different levels of the hierarchy rather than attempting to
simultaneously satisfy all of them. This leads to a more robust learning
algorithm. We compare our method to a number of state-of-the-art benchmarks on
$k$-nearest neighbor classification, large-scale image retrieval and
semi-supervised clustering problems, and find that our algorithm yields results
comparable or superior to the state-of-the-art, and is significantly more
robust to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5572</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5572</id><created>2014-02-22</created><updated>2014-02-25</updated><authors><author><keyname>Wang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Hori</keyname><forenames>Yutaka</forenames></author><author><keyname>Hara</keyname><forenames>Shinji</forenames></author><author><keyname>Doyle</keyname><forenames>Francis J.</forenames><suffix>III</suffix></author></authors><title>Collective oscillation period of inter-coupled biological negative
  cyclic feedback oscillators</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1203.1251</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of biological rhythms originate from networks comprised of multiple
cellular oscillators. But analytical results are still lacking on the
collective oscillation period of inter-coupled gene regulatory oscillators,
which, as has been reported, may be different from that of an autonomous
oscillator. Based on cyclic feedback oscillators, we analyze the collective
oscillation pattern of coupled cellular oscillators. First we give a condition
under which the oscillator network exhibits oscillatory and synchronized
behavior. Then we estimate the collective oscillation period based on a novel
multivariable harmonic balance technique. Analytical results are derived in
terms of biochemical parameters, thus giving insight into the basic mechanism
of biological oscillation and providing guidance in synthetic biology design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5584</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5584</id><created>2014-02-23</created><authors><author><keyname>Vats</keyname><forenames>Divyanshu</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse
  Regression</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>AISTATS 2014</comments><journal-ref>Proceedings of the 17th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&amp;CP
  volume 33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the challenging problem of selecting tuning
parameters for high-dimensional sparse regression. We propose a simple and
computationally efficient method, called path thresholding (PaTh), that
transforms any tuning parameter-dependent sparse regression algorithm into an
asymptotically tuning-free sparse regression algorithm. More specifically, we
prove that, as the problem size becomes large (in the number of variables and
in the number of observations), PaTh performs accurate sparse regression, under
appropriate conditions, without specifying a tuning parameter. In
finite-dimensional settings, we demonstrate that PaTh can alleviate the
computational burden of model selection algorithms by significantly reducing
the search space of tuning parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5586</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5586</id><created>2014-02-23</created><updated>2015-12-14</updated><authors><author><keyname>Xu</keyname><forenames>Shuanfeng</forenames></author><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author><author><keyname>Zhang</keyname><forenames>Duzhou</forenames></author><author><keyname>Yang</keyname><forenames>Baohua</forenames></author></authors><title>Adaptive Zero Reaction Motion Control for Free-Floating Space
  Manipulators</title><categories>cs.SY</categories><comments>17 pages, 9 figures, revised for more rigorously presenting the proof
  and for improving the presentation, highlighting the contribution, and
  correcting some typos based on the reviewers' and AE's comments from IEEE
  Transactions on Aerospace and Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates adaptive zero reaction motion control for
free-floating space manipulators with uncertain kinematics and dynamics. The
challenge in deriving the adaptive reaction null-space (RNS) based control
scheme is that it is difficult to obtain a linear expression, which is the
basis of the adaptive control. The main contribution of this paper is that we
skillfully obtain such a linear expression, based on which, an adaptive version
of the RNS-based controller (referred to as the adaptive zero reaction motion
controller in the sequel) is developed at the velocity level, taking into
account both the kinematic and dynamic uncertainties. It is shown that the
proposed controller achieves both the spacecraft attitude regulation and
end-effector trajectory tracking. The performance of the proposed adaptive
controller is shown by numerical simulations with a planar 3-DOF
(degree-of-freedom) space manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5592</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5592</id><created>2014-02-23</created><authors><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author><author><keyname>Uddin</keyname><forenames>Mohammad Salah</forenames></author><author><keyname>Barua</keyname><forenames>Aoyan</forenames></author></authors><title>Web Service Composition - BPEL vs cCSP Process Algebra</title><categories>cs.SE</categories><comments>6 pages, 4 figures, Advanced Computer Science Applications and
  Technologies (ACSAT), 2012 International Conference on</comments><doi>10.1109/ACSAT.2012.47</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web services technology provides a platform on which we can develop
distributed services. The interoperability among these services is achieved by
various standard protocols. In recent years, several researches suggested that
process algebras provide a satisfactory assistance to the whole process of web
services development. Business transactions, on the other hand, involve the
coordination and interaction between multiple partners. With the emergence of
web services, business transactions are conducted using these services. The
coordination among the business processes is crucial, so is the handling of
faults that can arise at any stage of a transaction. BPEL models the behavior
of business process interaction by providing a XML based grammar to describe
the control logic required to coordinate the web services participating in a
process flow. However BPEL lacks a proper formal description where the
composition of business processes cannot be formally verified. Process algebra,
on the other hand, facilitates a formal foundation for rigorous verification of
the composition. This paper presents a comparison of web service composition
between BPEL and process algebra, cCSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5593</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5593</id><created>2014-02-23</created><authors><author><keyname>Tagiew</keyname><forenames>Rustam</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>Reciprocity in Gift-Exchange-Games</title><categories>cs.AI</categories><comments>6 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analysis of data from a gift-exchange-game experiment.
The experiment was described in `The Impact of Social Comparisons on
Reciprocity' by G\&quot;achter et al. 2012. Since this paper uses state-of-art data
science techniques, the results provide a different point of view on the
problem. As already shown in relevant literature from experimental economics,
human decisions deviate from rational payoff maximization. The average gift
rate was $31$%. Gift rate was under no conditions zero. Further, we derive some
special findings and calculate their significance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5595</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5595</id><created>2014-02-23</created><authors><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author><author><keyname>Hossain</keyname><forenames>Sk. Jahir</forenames></author><author><keyname>Azad</keyname><forenames>Keya</forenames></author><author><keyname>Hassan</keyname><forenames>Mehidee</forenames></author></authors><title>Logic Verification of Product-Line Variant Requirements</title><categories>cs.SE</categories><comments>4 pages, 4 figures, Sofware Engineering and Applied Computing
  (ACSEAC), 2012 African Conference on</comments><doi>10.1109/ACSEAC.2012.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal verification of variant requirements has gained much interest in the
software product line (SPL) community. Feature diagrams are widely used to
model product line variants. However, there is a lack of precisely defined
formal notation for representing and verifying such models. This paper presents
an approach to modeling and verifying SPL variant feature diagrams using
first-order logic. It provides a precise and rigorous formal interpretation of
the feature diagrams. Logical expressions can be built by modeling variants and
their dependencies by using propositional connectives. These expressions can
then be validated by any suitable verification tool. A case study of a Computer
Aided Dispatch (CAD) system variant feature model is presented to illustrate
the verification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5596</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5596</id><created>2014-02-23</created><updated>2014-02-27</updated><authors><author><keyname>Lee</keyname><forenames>Jason D</forenames></author><author><keyname>Taylor</keyname><forenames>Jonathan E</forenames></author></authors><title>Exact Post Model Selection Inference for Marginal Screening</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for post model selection inference, via marginal
screening, in linear regression. At the core of this framework is a result that
characterizes the exact distribution of linear functions of the response $y$,
conditional on the model being selected (``condition on selection&quot; framework).
This allows us to construct valid confidence intervals and hypothesis tests for
regression coefficients that account for the selection procedure. In contrast
to recent work in high-dimensional statistics, our results are exact
(non-asymptotic) and require no eigenvalue-like assumptions on the design
matrix $X$. Furthermore, the computational cost of marginal regression,
constructing confidence intervals and hypothesis testing is negligible compared
to the cost of linear regression, thus making our methods particularly suitable
for extremely large datasets. Although we focus on marginal screening to
illustrate the applicability of the condition on selection framework, this
framework is much more broadly applicable. We show how to apply the proposed
framework to several other selection procedures including orthogonal matching
pursuit, non-negative least squares, and marginal screening+Lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5599</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5599</id><created>2014-02-23</created><updated>2014-10-22</updated><authors><author><keyname>Peng</keyname><forenames>Zhaoguang</forenames></author><author><keyname>Lu</keyname><forenames>Yu</forenames></author><author><keyname>Miller</keyname><forenames>Alice</forenames></author><author><keyname>Zhao</keyname><forenames>Tingdi</forenames></author><author><keyname>Johnson</keyname><forenames>Chris</forenames></author></authors><title>Formal Specification and Quantitative Analysis of a Constellation of
  Navigation Satellites</title><categories>cs.SY</categories><doi>10.1002/qre.1754</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigation satellites are a core component of navigation satellite based
systems such as GPS, GLONASS and Galileo which provide location and timing
information for a variety of uses. Such satellites are designed for operating
on orbit to perform tasks and have lifetimes of 10 years or more. Reliability,
availability and maintainability (RAM) analysis of systems has been
indispensable in the design phase of satellites in order to achieve minimum
failures or to increase mean time between failures (MTBF) and thus to plan
maintenance strategies, optimise reliability and maximise availability. In this
paper, we present formal models of both a single satellite and a navigation
satellite constellation and logical specification of their reliability,
availability and maintainability properties respectively. The probabilistic
model checker PRISM has been used to perform automated analysis of these
quantitative properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5604</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5604</id><created>2014-02-23</created><authors><author><keyname>Yan</keyname><forenames>Han</forenames></author><author><keyname>Hou</keyname><forenames>Mingzhe</forenames></author></authors><title>Three-Dimensional Integrated Guidance and Control Based on Small-Gain
  Theorem</title><categories>cs.SY</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A three-dimensional (3D) integrated guidance and control (IGC) design
approach is proposed by using small-gain theorem in this paper. The 3D IGC
model is formulated by combining nonlinear pursuer dynamics with the nonlinear
dynamics describing pursuitevasion motion. Small-gain theorem and ISS theory
are iteratively utilized to design desired attack angle, sideslip angle and
attitude angular rates (virtual controls), and eventually an IGC law is
proposed. Theoretical analysis shows that the IGC approach can make the LOS
rate converge into a small neighborhood of zero, and the stability of the
overall system can be guaranteed as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5613</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5613</id><created>2014-02-23</created><authors><author><keyname>Peng</keyname><forenames>Bo</forenames></author><author><keyname>Lu</keyname><forenames>Zhipeng</forenames></author><author><keyname>Cheng</keyname><forenames>T. C. E.</forenames></author></authors><title>A Tabu Search/Path Relinking Algorithm to Solve the Job Shop Scheduling
  Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that incorporates a tabu search procedure into the
framework of path relinking to tackle the job shop scheduling problem (JSP).
This tabu search/path relinking (TS/PR) algorithm comprises several
distinguishing features, such as a specific relinking procedure and a reference
solution determination method. To test the performance of TS/PR, we apply it to
tackle almost all of the benchmark JSP instances available in the literature.
The test results show that TS/PR obtains competitive results compared with
state-of-the-art algorithms for JSP in the literature, demonstrating its
efficacy in terms of both solution quality and computational efficiency. In
particular, TS/PR is able to improve the upper bounds for 49 out of the 205
tested instances and it solves a challenging instance that has remained
unsolved for over 20 years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5617</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5617</id><created>2014-02-23</created><authors><author><keyname>Vijayalakshmi</keyname><forenames>V. Karthikeyan V. J.</forenames></author></authors><title>Control Loop Feedback Mechanism for Generic Array Logic Chip
  Multiprocessor</title><categories>cs.OH</categories><comments>4 pages and 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control Loop Feedback Mechanism for Generic Array Logic Chip Multiprocessor
is presented. The approach is based on control-loop feedback mechanism to
maximize the efficiency on exploiting available resources such as CPU time,
operating frequency, etc. Each Processing Element (PE) in the architecture is
equipped with a frequency scaling module responsible for tuning the frequency
of processors at run-time according to the application requirements. We show
that generic array logic Chip Multiprocessors with large inter-processor First
In First Outputs (First In First Outs) buffers can inherently hide much of the
Generic Array Logic performance penalty while executing applications that have
been mapped with few communication loops. In fact, the penalty can be driven to
zero with sufficiently large First In First Outs and the removal of
multiple-loop communication links. We present an example mesh-connected Generic
Array Logic chip multiprocessor and show it has a less than 1% performance
(throughput) reduction on average compared to the corresponding synchronous
system for many DSP workloads. Furthermore, adaptive clock and voltage scaling
for each processor provides an approximately 40% power savings without any
performance reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5619</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5619</id><created>2014-02-23</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author></authors><title>A Novel Histogram Based Robust Image Registration Technique</title><categories>cs.CV</categories><comments>5 pages and 6 figures. submit/0850305</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method for Automatic Image Registration (AIR) through
histogram is proposed. Automatic image registration is one of the crucial steps
in the analysis of remotely sensed data. A new acquired image must be
transformed, using image registration techniques, to match the orientation and
scale of previous related images. This new approach combines several
segmentations of the pair of images to be registered. A relaxation parameter on
the histogram modes delineation is introduced. It is followed by
characterization of the extracted objects through the objects area, axis ratio,
and perimeter and fractal dimension. The matched objects are used for rotation
and translation estimation. It allows for the registration of pairs of images
with differences in rotation and translation. This method contributes to
subpixel accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5623</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5623</id><created>2014-02-23</created><authors><author><keyname>Karthikeyan</keyname><forenames>V.</forenames></author><author><keyname>Vijayalakshmi</keyname><forenames>V. J.</forenames></author></authors><title>Localization of License Plate Using Morphological Operations</title><categories>cs.CV</categories><comments>6 PAGES 12 FIGURES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is believed that there are currently millions of vehicles on the roads
worldwide. The over speed of vehicles,theft of vehicles, disobeying traffic
rules in public, an unauthorized person entering the restricted area are keep
on increasing. In order restrict against these criminal activities, we need an
automatic public security system. Each vehicle has their own Vehicle
Identification Number (VIN) as their primary identifier. The VIN is actually a
License Number which states a legal license to participate in the public
traffic. The proposed paper is to identify the vehicle with the help of
vehicles License Plate (LP).LPRS is one the most important part of the
Intelligent Transportation System (ITS) to locate the LP. In this paper certain
existing algorithm drawbacks are overcome by the proposed morphological
operations for LPRS. Morphological operation is chosen due to its higher
efficiency, noise filter capacity, accuracy, exact localization of LP and
speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5634</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5634</id><created>2014-02-23</created><authors><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>To go deep or wide in learning?</title><categories>cs.LG</categories><comments>9 pages, 1 figure, Accepted for publication in Seventeenth
  International Conference on Artificial Intelligence and Statistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve acceptable performance for AI tasks, one can either use
sophisticated feature extraction methods as the first layer in a two-layered
supervised learning model, or learn the features directly using a deep
(multi-layered) model. While the first approach is very problem-specific, the
second approach has computational overheads in learning multiple layers and
fine-tuning of the model. In this paper, we propose an approach called wide
learning based on arc-cosine kernels, that learns a single layer of infinite
width. We propose exact and inexact learning strategies for wide learning and
show that wide learning with single layer outperforms single layer as well as
deep architectures of finite width for some benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5639</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5639</id><created>2014-02-23</created><authors><author><keyname>Kan</keyname><forenames>Zhen</forenames></author><author><keyname>Klotz</keyname><forenames>Justin</forenames></author><author><keyname>Pasiliao</keyname><forenames>Eduardo L.</forenames><suffix>Jr</suffix></author><author><keyname>Shea</keyname><forenames>John M.</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Decentralized Rendezvous of Nonholonomic Robots with Sensing and
  Connectivity Constraints</title><categories>cs.SY cs.RO</categories><comments>9 pages, 5 figures, submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A group of wheeled robots with nonholonomic constraints is considered to
rendezvous at a common specified setpoint with a desired orientation while
maintaining network connectivity and ensuring collision avoidance within the
robots. Given communication and sensing constraints for each robot, only a
subset of the robots are aware or informed of the global destination, and the
remaining robots must move within the network connectivity constraint so that
the informed robots can guide the group to the goal. The mobile robots are also
required to avoid collisions with each other outside a neighborhood of the
common rendezvous point. To achieve the rendezvous control objective,
decentralized time-varying controllers are developed based on a navigation
function framework to steer the robots to perform rendezvous while preserving
network connectivity and ensuring collision avoidance. Only local sensing
feedback, which includes position feedback from immediate neighbors and
absolute orientation measurement, is used to navigate the robots and enables
radio silence during navigation. Simulation results demonstrate the performance
of the developed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5642</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5642</id><created>2014-02-23</created><authors><author><keyname>Sahai</keyname><forenames>Ankur</forenames></author></authors><title>VM Power Prediction in Distributed Systems for Maximizing Renewable
  Energy Usage</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of GreenPAD project it is important to predict the energy
consumption of individual (and mixture of) VMs / workload for optimal
scheduling (running those VMs which require higher energy when there is more
green energy available and vice-versa) in order to maximize green energy
utilization.
  For this we execute the following experiments on an Openstack cloud testbed
consisting of Fujitsu servers: VM energy measurement for different
configurations (flavor + workload) and VM energy prediction for a new
configuration. The automation framework for running these experiments uses bash
scripts which call tools like 'stress' (simulating workloads), 'collected'
(resource usage) and 'IPMI' (power measurement).
  We propose a linear model for predicting the power usage of the VMs based on
regression. We first collect the resource usage (using collected) and the
associated power usage (using IPMI) for different VM configurations and use
this to build a (multi-) regression model (between resource usage and VM energy
consumption). Then we use the information about the resource usage patterns of
the new workload to predict the power usage. For predicting power for mix of
workloads we execute (build a regression model based on) experiments with
random workloads. We observe the highest energy usage for CPU-intensive
workloads followed by memory-intensive workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5644</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5644</id><created>2014-02-23</created><authors><author><keyname>Kan</keyname><forenames>Zhen</forenames></author><author><keyname>Klotz</keyname><forenames>Justin</forenames></author><author><keyname>Pasiliao</keyname><forenames>Eduardo L.</forenames><suffix>Jr</suffix></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Containment Control for a Social Network with State-Dependent
  Connectivity</title><categories>cs.SY</categories><comments>9 pages, 2 figures, submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social interactions influence our thoughts, opinions and actions. In this
paper, social interactions are studied within a group of individuals composed
of influential social leaders and followers. Each person is assumed to maintain
a social state, which can be an emotional state or an opinion. Followers update
their social states based on the states of local neighbors, while social
leaders maintain a constant desired state. Social interactions are modeled as a
general directed graph where each directed edge represents an influence from
one person to another. Motivated by the non-local property of fractional-order
systems, the social response of individuals in the network are modeled by
fractional-order dynamics whose states depend on influences from local
neighbors and past experiences. A decentralized influence method is then
developed to maintain existing social influence between individuals (i.e.,
without isolating peers in the group) and to influence the social group to a
common desired state (i.e., within a convex hull spanned by social leaders).
Mittag-Leffler stability methods are used to prove asymptotic stability of the
networked fractional-order system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5645</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5645</id><created>2014-02-23</created><authors><author><keyname>Soliman</keyname><forenames>Omar S.</forenames></author><author><keyname>Elgendi</keyname><forenames>Elshimaa A. R.</forenames></author></authors><title>A Hybrid Estimation of Distribution Algorithm with Random Walk local
  Search for Multi-mode Resource-Constrained Project Scheduling problems</title><categories>cs.OH</categories><comments>8 pages,0 figures, Published with International Journal of Computer
  Trends and Technology (IJCTT). Omar S. Soliman, Elshimaa A.R. Elgendi. A
  Hybrid Estimation of Distribution Algorithm with Random Walk local Search for
  Multi-mode Resource-Constrained Project Scheduling problems. Inter J. of
  Computer Trends and Tech(IJCTT)8(2):57-64,2014</comments><doi>10.14445/22312803/IJCTT-V8P111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-mode resource-constrained project scheduling problems (MRCPSPs) are
classified as NP-hard problems, in which a task has different execution modes
characterized by different resource requirements. Estimation of distribution
algorithm (EDA) has shown an effective performance for solving such real-world
optimization problems but it fails to find the desired optima. This paper
integrates a novel hybrid local search technique with EDA to enhance their
local search ability. The new local search is based on delete-then-insert
operator and a random walk (DIRW) to enhance exploitation abilities of EDA in
the neighborhoods of the search space. The proposed algorithm is capable to
explore and exploit the search mechanism in the search space through its outer
and inner loops. The proposed algorithm is tested and evaluated using benchmark
test problems of the project scheduling problem library PSPLIB. Simulation
results of the proposed algorithm are compared with the classical EDA
algorithm. The obtained results showed that the effectiveness of the proposed
algorithm and outperformed the compared EDA algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5646</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5646</id><created>2014-02-23</created><updated>2014-07-24</updated><authors><author><keyname>Morrison</keyname><forenames>Natasha</forenames></author><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Scott</keyname><forenames>Alex</forenames></author></authors><title>On Saturated $k$-Sperner Systems</title><categories>math.CO cs.DM</categories><comments>17 pages</comments><journal-ref>Electron. J. Combin. 21(3) (2014), #P3.22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $X$, a collection $\mathcal{F}\subseteq\mathcal{P}(X)$ is said to
be $k$-Sperner if it does not contain a chain of length $k+1$ under set
inclusion and it is saturated if it is maximal with respect to this property.
Gerbner et al. conjectured that, if $|X|$ is sufficiently large with respect to
$k$, then the minimum size of a saturated $k$-Sperner system
$\mathcal{F}\subseteq\mathcal{P}(X)$ is $2^{k-1}$. We disprove this conjecture
by showing that there exists $\varepsilon&gt;0$ such that for every $k$ and $|X|
\geq n_0(k)$ there exists a saturated $k$-Sperner system
$\mathcal{F}\subseteq\mathcal{P}(X)$ with cardinality at most
$2^{(1-\varepsilon)k}$.
  A collection $\mathcal{F}\subseteq \mathcal{P}(X)$ is said to be an
oversaturated $k$-Sperner system if, for every
$S\in\mathcal{P}(X)\setminus\mathcal{F}$, $\mathcal{F}\cup\{S\}$ contains more
chains of length $k+1$ than $\mathcal{F}$. Gerbner et al. proved that, if
$|X|\geq k$, then the smallest such collection contains between $2^{k/2-1}$ and
$O\left(\frac{\log{k}}{k}2^k\right)$ elements. We show that if $|X|\geq k^2+k$,
then the lower bound is best possible, up to a polynomial factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5647</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5647</id><created>2014-02-23</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Aleisa</keyname><forenames>Eisa A.</forenames></author></authors><title>A new model for Context-Oriented Programs</title><categories>cs.PL cs.SE</categories><comments>9 pages, 6 figures</comments><journal-ref>Mohamed A. El-Zawawy, Eisa A. Aleisa. A new model for
  Context-Oriented Programs. Life Science Journal, 2013, 10(2), pp: 2515-2523</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-oriented programming (COP) is a new technique for programming that
allows changing the context in which commands execute as a program executes.
Compared to object-oriented programming (aspect-oriented programming), COP is
more flexible (modular and structured). This paper presents a precise
syntax-directed operational semantics for context-oriented programming with
layers, as realized by COP languages like ContextJ* and ContextL. Our language
model is built on Java enriched with layer concepts and activation and
deactivation of layer scopes. The paper also presents a static type system that
guarantees that typed programs do not get stuck. Using the means of the
proposed semantics, the mathematical correctness of the type system is
presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5666</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5666</id><created>2014-02-23</created><updated>2014-05-12</updated><authors><author><keyname>Combes</keyname><forenames>Richard</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Dynamic Rate and Channel Selection in Cognitive Radio Systems</title><categories>cs.IT cs.LG math.IT</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate dynamic channel and rate selection in cognitive
radio systems which exploit a large number of channels free from primary users.
In such systems, transmitters may rapidly change the selected (channel, rate)
pair to opportunistically learn and track the pair offering the highest
throughput. We formulate the problem of sequential channel and rate selection
as an online optimization problem, and show its equivalence to a {\it
structured} Multi-Armed Bandit problem. The structure stems from inherent
properties of the achieved throughput as a function of the selected channel and
rate. We derive fundamental performance limits satisfied by {\it any} channel
and rate adaptation algorithm, and propose algorithms that achieve (or
approach) these limits. In turn, the proposed algorithms optimally exploit the
inherent structure of the throughput. We illustrate the efficiency of our
algorithms using both test-bed and simulation experiments, in both stationary
and non-stationary radio environments. In stationary environments, the packet
successful transmission probabilities at the various channel and rate pairs do
not evolve over time, whereas in non-stationary environments, they may evolve.
In practical scenarios, the proposed algorithms are able to track the best
channel and rate quite accurately without the need of any explicit measurement
and feedback of the quality of the various channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5681</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5681</id><created>2014-02-23</created><updated>2014-12-02</updated><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Kostitsyna</keyname><forenames>Irina</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Silveira</keyname><forenames>Rodrigo I.</forenames></author></authors><title>Region-based approximation of probability distributions (for visibility
  between imprecise points among obstacles)</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ and $q$ be two imprecise points, given as probability density
functions on $\mathbb R^2$, and let $\cal R$ be a set of $n$ line segments
(obstacles) in $\mathbb R^2$. We study the problem of approximating the
probability that $p$ and $q$ can see each other; that is, that the segment
connecting $p$ and $q$ does not cross any segment of $\cal R$. To solve this
problem, we approximate each density function by a weighted set of polygons; a
novel approach to dealing with probability density functions in computational
geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5684</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5684</id><created>2014-02-23</created><updated>2014-03-06</updated><authors><author><keyname>Firat</keyname><forenames>Orhan</forenames></author><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Oztekin</keyname><forenames>Ilke</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author></authors><title>Discriminative Functional Connectivity Measures for Brain Decoding</title><categories>cs.AI cs.CE cs.CV cs.LG</categories><comments>This paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a statistical learning model for classifying cognitive processes
based on distributed patterns of neural activation in the brain, acquired via
functional magnetic resonance imaging (fMRI). In the proposed learning method,
local meshes are formed around each voxel. The distance between voxels in the
mesh is determined by using a functional neighbourhood concept. In order to
define the functional neighbourhood, the similarities between the time series
recorded for voxels are measured and functional connectivity matrices are
constructed. Then, the local mesh for each voxel is formed by including the
functionally closest neighbouring voxels in the mesh. The relationship between
the voxels within a mesh is estimated by using a linear regression model. These
relationship vectors, called Functional Connectivity aware Local Relational
Features (FC-LRF) are then used to train a statistical learning machine. The
proposed method was tested on a recognition memory experiment, including data
pertaining to encoding and retrieval of words belonging to ten different
semantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)
and Support Vector Machine (SVM), are trained in order to predict the semantic
category of the item being retrieved, based on activation patterns during
encoding. The classification performance of the Functional Mesh Learning model,
which range in 62%-71% is superior to the classical multi-voxel pattern
analysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5687</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5687</id><created>2014-02-23</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Monoidal computer II: Normal complexity by string diagrams</title><categories>cs.LO cs.CC math.CT math.LO</categories><comments>11 pages, 18 figures</comments><msc-class>03D75, 03D15, 18D10, 18B20</msc-class><acm-class>F.1.3; F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Monoidal Computer I, we introduced a categorical model of computation
where the formal reasoning about computability was supported by the simple and
popular diagrammatic language of string diagrams. In the present paper, we
refine and extend that model of computation to support a formal complexity
theory as well. This formalization brings to the foreground the concept of
normal complexity measures, which allow decompositions akin to Kleene's normal
form. Such measures turn out to be just those where evaluating the complexity
of a program does not require substantially more resources than evaluating the
program itself. The usual time and space complexity are thus normal measures,
whereas the average and the randomized complexity measures are not. While the
measures that are not normal provide important design time information about
algorithms, and for theoretical analyses, normal measures can also be used at
run time, as practical tools of computation, e.g. to set the bounds for
hypothesis testing, inductive inference and algorithmic learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5691</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5691</id><created>2014-02-23</created><authors><author><keyname>Somasundaram</keyname><forenames>S.</forenames></author><author><keyname>Li</keyname><forenames>P.</forenames></author><author><keyname>Parsons</keyname><forenames>N.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Data-Adaptive Reduced-Dimension Robust Beamforming Algorithms</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present low complexity, quickly converging robust adaptive beamformers
that combine robust Capon beamformer (RCB) methods and data-adaptive Krylov
subspace dimensionality reduction techniques. We extend a recently proposed
reduced-dimension RCB framework, which ensures proper combination of RCBs with
any form of dimensionality reduction that can be expressed using a full-rank
dimension reducing transform, providing new results for data-adaptive
dimensionality reduction. We consider Krylov subspace methods computed with the
Powers-of-R (PoR) and Conjugate Gradient (CG) techniques, illustrating how a
fast CG-based algorithm can be formed by beneficially exploiting that the
CG-algorithm diagonalizes the reduced-dimension covariance. Our simulations
show the benefits of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5692</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5692</id><created>2014-02-23</created><authors><author><keyname>Uchoa</keyname><forenames>A. G. D.</forenames></author><author><keyname>Healy</keyname><forenames>C. T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Repeat Accumulate Based Designs for LDPC Codes on Fading Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Irregular repeat-accumulate Root-Check LDPC codes based on Progressive Edge
Growth (PEG) techniques for block-fading channels are proposed. The proposed
Root-Check LDPC codes are {both suitable for channels under $F = 2, 3$
independent fadings per codeword and} for fast fading channels. An IRA(A)
Root-Check structure is devised for $F = 2, 3$ independent fadings. The
performance of the new codes is investigated in terms of the Frame Error Rate
(FER). Numerical results show that the IRAA LDPC codes constructed by the
proposed algorithm {outperform by about 1dB the existing} IRA Root-Check LDPC
codes under fast-fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5693</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5693</id><created>2014-02-23</created><updated>2014-10-17</updated><authors><author><keyname>Parseh</keyname><forenames>Reza</forenames></author><author><keyname>Kansanen</keyname><forenames>Kimmo</forenames></author></authors><title>On Estimation Error Outage for Scalar Gauss-Markov Signals Sent Over
  Fading Channels</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 5 figures, conference version at EUSIPCO 2014, accepted for
  publication in IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2014.2360820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurements of a scalar linear Gauss-Markov process are sent over a fading
channel. The fading channel is modeled as independent and identically
distributed random variables with known realization at the receiver. The
optimal estimator at the receiver is the Kalman filter. In contrast to the
classical Kalman filter theory, given a random channel, the Kalman gain and the
error covariance become random. Then the probability distribution function of
expected estimation error and its outage probability can be chosen for
estimation quality assessment. In this paper and in order to get the estimation
error outage, we provide means to characterize the stationary probability
density function of the random expected estimation error. Furthermore and for
the particular case of the i.i.d. Rayleigh fading channels, upper and lower
bounds for the outage probability are derived which provide insight and simpler
means for design purposes. We also show that the bounds are tight for the high
SNR regime, and that the outage probability decreases linearly with the inverse
of the average channel SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5697</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5697</id><created>2014-02-23</created><authors><author><keyname>Gao</keyname><forenames>Changxin</forenames></author><author><keyname>Chen</keyname><forenames>Feifei</forenames></author><author><keyname>Yu</keyname><forenames>Jin-Gang</forenames></author><author><keyname>Huang</keyname><forenames>Rui</forenames></author><author><keyname>Sang</keyname><forenames>Nong</forenames></author></authors><title>Exemplar-based Linear Discriminant Analysis for Robust Object Tracking</title><categories>cs.CV</categories><comments>ICIP2014</comments><doi>10.1109/ICIP.2014.7025077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking-by-detection has become an attractive tracking technique, which
treats tracking as a category detection problem. However, the task in tracking
is to search for a specific object, rather than an object category as in
detection. In this paper, we propose a novel tracking framework based on
exemplar detector rather than category detector. The proposed tracker is an
ensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Each
detector is quite specific and discriminative, because it is trained by a
single object instance and massive negatives. To improve its adaptivity, we
update both object and background models. Experimental results on several
challenging video sequences demonstrate the effectiveness and robustness of our
tracking algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5708</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5708</id><created>2014-02-23</created><authors><author><keyname>Kurtaj</keyname><forenames>Lavdim</forenames></author><author><keyname>Limani</keyname><forenames>Ilir</forenames></author><author><keyname>Shatri</keyname><forenames>Vjosa</forenames></author><author><keyname>Skeja</keyname><forenames>Avni</forenames></author></authors><title>The Cerebellum: New Computational Model that Reveals its Primary
  Function to Calculate Multibody Dynamics Conform to Lagrange-Euler
  Formulation</title><categories>cs.NE cs.CE cs.RO q-bio.NC</categories><comments>18 pages, 4 figures</comments><acm-class>C.1.3; F.1.1; I.2.6</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 5, No 2, September 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cerebellum is part of the brain that occupies only 10% of the brain volume,
but it contains about 80% of total number of brain neurons. New cerebellar
function model is developed that sets cerebellar circuits in context of
multibody dynamics model computations, as important step in controlling balance
and movement coordination, functions performed by two oldest parts of the
cerebellum. Model gives new functional interpretation for granule cells-Golgi
cell circuit, including distinct function for upper and lower Golgi cell
dendritc trees, and resolves issue of sharing Granule cells between Purkinje
cells. Sets new function for basket cells, and for stellate cells according to
position in molecular layer. New model enables easily and direct integration of
sensory information from vestibular system and cutaneous mechanoreceptors, for
balance, movement and interaction with environments. Model gives explanation of
Purkinje cells convergence on deep-cerebellar nuclei.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5715</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5715</id><created>2014-02-23</created><updated>2015-12-05</updated><authors><author><keyname>Saeedi</keyname><forenames>Ardavan</forenames></author><author><keyname>Kulkarni</keyname><forenames>Tejas D</forenames></author><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Gershman</keyname><forenames>Samuel</forenames></author></authors><title>Variational Particle Approximations</title><categories>stat.ML cs.LG</categories><comments>First two authors contributed equally to this work</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Approximate inference in high-dimensional, discrete probabilistic models is a
central problem in computational statistics and machine learning. This paper
describes discrete particle variational inference (DPVI), a new approach that
combines key strengths of Monte Carlo, variational and search-based techniques.
DPVI is based on a novel family of particle-based variational approximations
that can be fit using simple, fast, deterministic search techniques. Like Monte
Carlo, DPVI can handle multiple modes, and yields exact results in a
well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a
lower bound on the partition function; when this quantity is not of intrinsic
interest, it facilitates convergence assessment and debugging. Like both Monte
Carlo and combinatorial search, DPVI can take advantage of factorization,
sequential structure, and custom search operators. This paper defines DPVI
particle-based approximation family and partition function lower bounds, along
with the sequential DPVI and local DPVI algorithm templates for optimizing
them. DPVI is illustrated and evaluated via experiments on lattice Markov
Random Fields, nonparametric Bayesian mixtures and block-models, and parametric
as well as non-parametric hidden Markov models. Results include applications to
real-world spike-sorting and relational modeling problems, and show that DPVI
can offer appealing time/accuracy trade-offs as compared to multiple
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5726</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5726</id><created>2014-02-24</created><updated>2014-08-25</updated><authors><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>On Power and Load Coupling in Cellular Networks for Energy Optimization</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimization of sum transmission energy in
cellular networks where coupling occurs between cells due to mutual
interference. The coupling relation is characterized by the
signal-to-interference-and-noise-ratio (SINR) coupling model. Both cell load
and transmission power, where cell load measures the average level of resource
usage in the cell, interact via the coupling model. The coupling is implicitly
characterized with load and power as the variables of interest using two
equivalent equations, namely, non-linear load coupling equation (NLCE) and
non-linear power coupling equation (NPCE), respectively. By analyzing the NLCE
and NPCE, we prove that operating at full load is optimal in minimizing sum
energy, and provide an iterative power adjustment algorithm to obtain the
corresponding optimal power solution with guaranteed convergence, where in each
iteration a standard bisection search is employed. To obtain the algorithmic
result, we use the properties of the so-called standard interference function;
the proof is non-standard because the NPCE cannot even be expressed as a
closed-form expression with power as the implicit variable of interest. We
present numerical results illustrating the theoretical findings for a real-life
and large-scale cellular network, showing the advantage of our solution
compared to the conventional solution of deploying uniform power for base
stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5728</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5728</id><created>2014-02-24</created><authors><author><keyname>Vidyasagar</keyname><forenames>Mathukumalli</forenames></author></authors><title>Machine Learning Methods in the Computational Biology of Cancer</title><categories>q-bio.QM cs.LG stat.ML</categories><comments>35 pages, three figures</comments><msc-class>62P10</msc-class><doi>10.1098/rspa.2014.0081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objectives of this &quot;perspective&quot; paper are to review some recent advances
in sparse feature selection for regression and classification, as well as
compressed sensing, and to discuss how these might be used to develop tools to
advance personalized cancer therapy. As an illustration of the possibilities, a
new algorithm for sparse regression is presented, and is applied to predict the
time to tumor recurrence in ovarian cancer. A new algorithm for sparse feature
selection in classification problems is presented, and its validation in
endometrial cancer is briefly discussed. Some open problems are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5730</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5730</id><created>2014-02-24</created><authors><author><keyname>Leng</keyname><forenames>Shiyang</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Power Efficient and Secure Multiuser Communication Systems with Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at the IEEE International Conference on
  Communications (ICC), Sydney, Australia, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study resource allocation algorithm design for power
efficient secure communication with simultaneous wireless information and power
transfer (WIPT) in multiuser communication systems. In particular, we focus on
power splitting receivers which are able to harvest energy and decode
information from the received signals. The considered problem is modeled as an
optimization problem which takes into account a minimum required
signal-to-interference-plus-noise ratio (SINR) at multiple desired receivers, a
maximum tolerable data rate at multiple multi-antenna potential eavesdroppers,
and a minimum required power delivered to the receivers. The proposed problem
formulation facilitates the dual use of artificial noise in providing efficient
energy transfer and guaranteeing secure communication. We aim at minimizing the
total transmit power by jointly optimizing transmit beamforming vectors, power
splitting ratios at the desired receivers, and the covariance of the artificial
noise. The resulting non-convex optimization problem is transformed into a
semidefinite programming (SDP) and solved by SDP relaxation. We show that the
adopted SDP relaxation is tight and achieves the global optimum of the original
problem. Simulation results illustrate the significant power saving obtained by
the proposed optimal algorithm compared to suboptimal baseline schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5731</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5731</id><created>2014-02-24</created><updated>2014-04-29</updated><authors><author><keyname>Aksoylar</keyname><forenames>Cem</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Information-Theoretic Bounds for Adaptive Sparse Recovery</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Accepted to IEEE ISIT 2014. Better presentation and fixed errors
  compared to the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive an information-theoretic lower bound for sample complexity in
sparse recovery problems where inputs can be chosen sequentially and
adaptively. This lower bound is in terms of a simple mutual information
expression and unifies many different linear and nonlinear observation models.
Using this formula we derive bounds for adaptive compressive sensing (CS),
group testing and 1-bit CS problems. We show that adaptivity cannot decrease
sample complexity in group testing, 1-bit CS and CS with linear sparsity. In
contrast, we show there might be mild performance gains for CS in the sublinear
regime. Our unified analysis also allows characterization of gains due to
adaptivity from a wider perspective on sparse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5734</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5734</id><created>2014-02-24</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Qu</keyname><forenames>Longjiang</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author><author><keyname>Yuan</keyname><forenames>Jin</forenames></author><author><keyname>Yuan</keyname><forenames>Pingzhi</forenames></author></authors><title>Permutation trinomials over finite fields with even characteristic</title><categories>cs.IT math.IT</categories><msc-class>11C08, 05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation polynomials have been a subject of study for a long time and have
applications in many areas of science and engineering. However, only a small
number of specific classes of permutation polynomials are described in the
literature so far. In this paper we present a number of permutation trinomials
over finite fields, which are of different forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5742</identifier>
 <datestamp>2014-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5742</id><created>2014-02-24</created><updated>2014-07-17</updated><authors><author><keyname>Turan</keyname><forenames>Ugur</forenames></author><author><keyname>Toroslu</keyname><forenames>Ismail Hakki</forenames></author></authors><title>Secure Logical Schema and Decomposition Algorithm for Proactive Context
  Dependent Attribute Based Access Control</title><categories>cs.DB cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional database access control mechanisms use role based methods, with
generally row based and attribute based constraints for granularity, and
privacy is achieved mainly by using views. However if only a set of views
according to policy are made accessible to users, then this set should be
checked against the policy for the whole probable query history. The aim of
this work is to define a proactive decomposition algorithm according to the
attribute based policy rules and build a secure logical schema in which
relations are decomposed into several ones in order to inhibit joins or
inferences that may violate predefined privacy constraints. The attributes
whose association should not be inferred, are defined as having security
dependency among them and they form a new kind of context dependent attribute
based policy rule named as security dependent set. The decomposition algorithm
works on a logical schema with given security dependent sets and aims to
prohibit the inference of the association among the elements of these sets. It
is also proven that the decomposition technique generates a secure logical
schema that is in compliance with the given security dependent set constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5743</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5743</id><created>2014-02-24</created><authors><author><keyname>Huang</keyname><forenames>Shen</forenames></author><author><keyname>Cui</keyname><forenames>Hongfei</forenames></author><author><keyname>Ding</keyname><forenames>Yiming</forenames></author></authors><title>Evaluation of node importance in complex networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The assessment of node importance has been a fundamental issue in the
research of complex networks. In this paper, we propose to use the
Shannon-Parry measure (SPM) to evaluate the importance of a node
quantitatively, because SPM is the stationary distribution of the most
unprejudiced random walk on the network. We demonstrate the accuracy and
robustness of SPM compared with several popular methods in the Zachary karate
club network and three toy networks. We apply SPM to analyze the city
importance of China Railways High-speed (CRH) network, and obtain reasonable
results. Since SPM can be used effectively in weighted and directed network, we
believe it is a relevant method to identify key nodes in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5745</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5745</id><created>2014-02-24</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author></authors><title>Distributed Data and Programs Slicing</title><categories>cs.PL cs.SE</categories><comments>9 pages, 8 figures</comments><journal-ref>Life Science Journal, 2013, 10(4), pp. 1361--1369</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new technique for data slicing of distributed programs
running on a hierarchy of machines. Data slicing can be realized as a program
transformation that partitions heaps of machines in a hierarchy into
independent regions. Inside each region of each machine, pointers preserve the
original pointer structures in the original heap hierarchy. Each heap component
of the base type (e.g., the integer type) goes only to a region of one of the
heaps. The proposed technique has the shape of a system of inference rules. In
addition, this paper presents a simply structure type system to decide type
soundness of distributed programs. Using this type system, a mathematical proof
that the proposed slicing technique preserves typing properties is outlined in
this paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5748</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5748</id><created>2014-02-24</created><authors><author><keyname>Aarti</keyname></author><author><keyname>Tomar</keyname><forenames>Sanjiv Kumar</forenames></author></authors><title>A Parametric Chain based Routing Approach for Underwater Sensor Network</title><categories>cs.NI cs.DC</categories><comments>4 pages,1 figure</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT) ,
  4(1): 1492-1495, May 2013, Published by Seventh Sense Research Group</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sensor network is one of the critical networks that is based on hardware
components as well the energy parameters. Because of this, such network
requires the optimization in all kind communication to improve the network
life. In case of underwater sensor network, the criticality of network is also
increased because of the random floating movement of the nodes. In this work, a
composition of the multicast or broadcast communication is presented by the
generation of aggregative path. The presented work is about to define a new
chain based aggregative routing approach to provide the effective communication
over the network. In this work, an effective aggregative path is suggested
under the different parameters of energy, distance and congestion analysis.
Based on these parameters a trustful aggregative route will be generated so
that the network life will be improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5749</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5749</id><created>2014-02-24</created><authors><author><keyname>McClatchey</keyname><forenames>R.</forenames></author><author><keyname>Branson</keyname><forenames>A.</forenames></author><author><keyname>Anjum</keyname><forenames>A.</forenames></author><author><keyname>Bloodsworth</keyname><forenames>P.</forenames></author><author><keyname>Habib</keyname><forenames>I.</forenames></author><author><keyname>Munir</keyname><forenames>K.</forenames></author><author><keyname>Shamdasani</keyname><forenames>J.</forenames></author><author><keyname>Soomro</keyname><forenames>K.</forenames></author><author><keyname>Consortium</keyname><forenames>the neuGRID</forenames></author></authors><title>Providing Traceability for Neuroimaging Analyses</title><categories>cs.SE</categories><comments>17 pages, 9 figures, 2 tables</comments><journal-ref>International Journal of Medical Informatics, 82 (2013) pp 882-894
  Elsevier publishers</journal-ref><doi>10.1016/j.ijmedinf.2013.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasingly digital nature of biomedical data and as the complexity
of analyses in medical research increases, the need for accurate information
capture, traceability and accessibility has become crucial to medical
researchers in the pursuance of their research goals. Grid- or Cloud-based
technologies, often based on so-called Service Oriented Architectures (SOA),
are increasingly being seen as viable solutions for managing distributed data
and algorithms in the bio-medical domain. For neuroscientific analyses,
especially those centred on complex image analysis, traceability of processes
and datasets is essential but up to now this has not been captured in a manner
that facilitates collaborative study. Over the past decade, we have been
working with mammographers, paediatricians and neuroscientists in three
generations of projects to provide the data management and provenance services
now required for 21st century medical research. This paper outlines the finding
of a requirements study and a resulting system architecture for the production
of services to support neuroscientific studies of biomarkers for Alzheimers
Disease. The paper proposes a software infrastructure and services that provide
the foundation for such support. It introduces the use of the CRISTAL software
to provide provenance management as one of a number of services delivered on a
SOA, deployed to manage neuroimaging projects that have been studying
biomarkers for Alzheimers disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5750</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5750</id><created>2014-02-24</created><authors><author><keyname>Sun</keyname><forenames>Yuli</forenames></author><author><keyname>Tao</keyname><forenames>Jinxu</forenames></author></authors><title>A new inexact iterative hard thresholding algorithm for compressed
  sensing</title><categories>cs.IT math.IT</categories><comments>9 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Compressed sensing (CS) demonstrates that a sparse, or compressible signal
can be acquired using a low rate acquisition process below the Nyquist rate,
which projects the signal onto a small set of vectors incoherent with the
sparsity basis. In this paper, we propose a new framework for compressed
sensing recovery problem using iterative approximation method via L0
minimization. Instead of directly solving the unconstrained L0 norm
optimization problem, we use the linearization and proximal points techniques
to approximate the penalty function at each iteration. The proposed algorithm
is very simple, efficient, and proved to be convergent. Numerical simulation
demonstrates our conclusions and indicates that the algorithm can improve the
reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5753</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5753</id><created>2014-02-24</created><authors><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Goff</keyname><forenames>Jean-Marie Le</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author></authors><title>CRISTAL : A Practical Study in Designing Systems to Cope with Change</title><categories>cs.SE</categories><comments>20mpages, 5 figures, 1 table</comments><journal-ref>Information Systems 42 (2014), pp 139-152 Elsevier Publishers</journal-ref><doi>10.1016/j.is.2013.12.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software engineers frequently face the challenge of developing systems whose
requirements are likely to change in order to adapt to organizational
reconfigurations or other external pressures. Evolving requirements present
difficulties, especially in environments in which business agility demands
shorter development times and responsive prototyping. This paper uses a study
from CERN in Geneva to address these research questions by employing a
description-driven approach that is responsive to changes in user requirements
and that facilitates dynamic system reconfiguration. The study describes how
handling descriptions of objects in practice alongside their instances (making
the objects self-describing) can mediate the effects of evolving user
requirements on system development. This paper reports on and draws lessons
from the practical use of a description-driven system over time. It also
identifies lessons that can be learned from adopting such a self-describing
description-driven approach in future software development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5757</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5757</id><created>2014-02-24</created><authors><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>Kiani</keyname><forenames>Saad Liaquat</forenames></author><author><keyname>Hasham</keyname><forenames>Khawar</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Consortium</keyname><forenames>the N4U</forenames></author></authors><title>An Integrated e-science Analysis Base for Computation Neuroscience
  Experiments and Analysis</title><categories>cs.SE cs.CE</categories><comments>8 pages &amp; 4 figures</comments><journal-ref>Procedia - Social and Behavioral Sciences. Vol 73 pp 85-92 (2013)
  Elsevier Publishers</journal-ref><doi>10.1016/j.sbspro.2013.02.026.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in data management and imaging technologies have
significantly affected diagnostic and extrapolative research in the
understanding of neurodegenerative diseases. However, the impact of these new
technologies is largely dependent on the speed and reliability with which the
medical data can be visualised, analysed and interpreted. The EUs neuGRID for
Users (N4U) is a follow-on project to neuGRID, which aims to provide an
integrated environment to carry out computational neuroscience experiments.
This paper reports on the design and development of the N4U Analysis Base and
related Information Services, which addresses existing research and practical
challenges by offering an integrated medical data analysis environment with the
necessary building blocks for neuroscientists to optimally exploit neuroscience
workflows, large image datasets and algorithms in order to conduct analyses.
The N4U Analysis Base enables such analyses by indexing and interlinking the
neuroimaging and clinical study datasets stored on the N4U Grid infrastructure,
algorithms and scientific workflow definitions along with their associated
provenance information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5758</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5758</id><created>2014-02-24</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author></authors><title>Bandits with concave rewards and convex knapsacks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a very general model for exploration-exploitation
tradeoff which allows arbitrary concave rewards and convex constraints on the
decisions across time, in addition to the customary limitation on the time
horizon. This model subsumes the classic multi-armed bandit (MAB) model, and
the Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also
consider an extension of this model to allow linear contexts, similar to the
linear contextual extension of the MAB model. We demonstrate that a natural and
simple extension of the UCB family of algorithms for MAB provides a polynomial
time algorithm that has near-optimal regret guarantees for this substantially
more general model, and matches the bounds provided by Badanidiyuru et
al.[2013] for the special case of BwK, which is quite surprising. We also
provide computationally more efficient algorithms by establishing interesting
connections between this problem and other well studied problems/algorithms
such as the Blackwell approachability problem, online convex optimization, and
the Frank-Wolfe technique for convex optimization. We give examples of several
concrete applications, where this more general model of bandits allows for
richer and/or more efficient formulations of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5759</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5759</id><created>2014-02-24</created><authors><author><keyname>Schmuck</keyname><forenames>Anne-Kathrin</forenames></author><author><keyname>Raisch</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Asynchronous $l$-Complete Approximations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the $l$-complete approximation method developed for time
invariant systems to a larger system class, ensuring that the resulting
approximation can be realized by a finite state machine. To derive the new
abstraction method, called asynchronous $l$-complete approximation, an
asynchronous version of the well-known concepts of state property, memory span
and $l$-completeness is introduced, extending the behavioral systems theory in
a consistent way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5761</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5761</id><created>2014-02-24</created><updated>2015-08-25</updated><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>A Technique for Deriving Equational Conditions on the Denavit-Hartenberg
  Parameters of 6R Linkages that are Necessary for Movability</title><categories>cs.RO cs.SC</categories><journal-ref>Mechanism and Machine Theory 94 (2015)</journal-ref><doi>10.1016/j.mechmachtheory.2015.07.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A closed 6R linkage is generically rigid. Special cases may be mobile. Many
families of mobile 6R linkages have been characterised in terms of the
invariant Denavit-Hartenberg parameters of the linkage. In other words, many
sufficient conditions for mobility are known. In this paper we give, for the
first time, equational conditions on the invariant Denavit-Hartenberg
parameters that are necessary for mobility. The method is based on the theory
of bonds. We illustrate the method by deriving the equational conditions for
various well-known linkages (Bricard's line symmetric linkage, Hooke's linkage,
Dietmaier's linkage, and recent a generalization of Bricard's orthogonal
linkage), starting from their bond diagrams; and by deriving the equations for
another bond diagram, thereby discovering a new mobile 6R linkage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5764</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5764</id><created>2014-02-24</created><authors><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author></authors><title>Designing Reusable Systems that Can Handle Change - Description-Driven
  Systems : Revisiting Object-Oriented Principles</title><categories>cs.SE</categories><comments>8 pages, 1 figure and 1 table. Accepted by the 9th Int Conf on the
  Evaluation of Novel Approaches to Software Engineering (ENASE'14). Lisbon,
  Portugal. April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the age of the Cloud and so-called Big Data systems must be increasingly
flexible, reconfigurable and adaptable to change in addition to being developed
rapidly. As a consequence, designing systems to cater for evolution is becoming
critical to their success. To be able to cope with change, systems must have
the capability of reuse and the ability to adapt as and when necessary to
changes in requirements. Allowing systems to be self-describing is one way to
facilitate this. To address the issues of reuse in designing evolvable systems,
this paper proposes a so-called description-driven approach to systems design.
This approach enables new versions of data structures and processes to be
created alongside the old, thereby providing a history of changes to the
underlying data models and enabling the capture of provenance data. The
efficacy of the description-driven approach is exemplified by the CRISTAL
project. CRISTAL is based on description-driven design principles; it uses
versions of stored descriptions to define various versions of data which can be
stored in diverse forms. This paper discusses the need for capturing holistic
system description when modelling large-scale distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5766</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5766</id><created>2014-02-24</created><authors><author><keyname>Romero</keyname><forenames>Adriana</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author><author><keyname>Gatta</keyname><forenames>Carlo</forenames></author></authors><title>No more meta-parameter tuning in unsupervised sparse feature learning</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised
feature learning algorithm, which exploits a new way of optimizing for
sparsity. Experiments on STL-10 show that the method presents state-of-the-art
performance and provides discriminative features that generalize well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5768</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5768</id><created>2014-02-24</created><authors><author><keyname>Manset</keyname><forenames>David</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Verjus</keyname><forenames>Herve</forenames></author></authors><title>Model Driven Engineering for Science Gateways</title><categories>cs.SE</categories><comments>10 pages, 6 figures and 1 table. 14th International Conference on
  Enterprise Information Systems (ICEIS) Vol 2 pp 421-431 ISBN
  978-989-8565-11-2 . Wroclaw, Poland. July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From n-Tier client/server applications, to more complex academic Grids, or
even the most recent and promising industrial Clouds, the last decade has
witnessed significant developments in distributed computing. In spite of this
conceptual heterogeneity, Service-Oriented Architectures (SOA) seem to have
emerged as the common underlying abstraction paradigm. Suitable access to data
and applications resident in SOAs via so-called Science Gateways has thus
become a pressing need in various fields of science, in order to realize the
benefits of Grid and Cloud infrastructures. In this context, authors have
consolidated work from three complementary experiences in European projects,
which have developed and deployed large-scale production quality
infrastructures as Science Gateways to support research in breast cancer,
paediatric diseases and neurodegenerative pathologies respectively. In
analysing the requirements from these biomedical applications the authors were
able to elaborate on commonly faced Grid development issues, while proposing an
adaptable and extensible engineering framework for Science Gateways. This paper
thus proposes the application of an architecture-centric Model-Driven
Engineering (MDE) approach to service-oriented developments, making it possible
to define Science Gateways that satisfy quality of service requirements,
execution platform and distribution criteria at design time. An novel
investigation is presented on the applicability of the resulting grid MDE
(gMDE) to specific examples, and conclusions are drawn on the benefits of this
approach and its possible application to other areas, in particular that of
Distributed Computing Infrastructures (DCI) interoperability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5769</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5769</id><created>2014-02-24</created><authors><author><keyname>Matsui</keyname><forenames>Tomomi</forenames></author><author><keyname>Sukegawa</keyname><forenames>Noriyoshi</forenames></author><author><keyname>Miyauchi</keyname><forenames>Atsushi</forenames></author></authors><title>Fractional programming formulation for the vertex coloring problem</title><categories>cs.DS</categories><comments>6 pages, 5 tables</comments><journal-ref>Information Processing Letters 114, 706-709 (2014)</journal-ref><doi>10.1016/j.ipl.2014.06.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a new formulation for the vertex coloring problem. Different from
other formulations, decision variables are associated with the pairs of
vertices. Consequently, colors will be distinguishable. Although the objective
function is fractional, it can be replaced by a piece-wise linear convex
function. Numerical experiments show that our formulation has significantly
good performance for dense graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5770</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5770</id><created>2014-02-24</created><authors><author><keyname>Lynn</keyname><forenames>Theo</forenames></author><author><keyname>Healy</keyname><forenames>Philip</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Morrison</keyname><forenames>John</forenames></author><author><keyname>Pahl</keyname><forenames>Claus</forenames></author><author><keyname>Lee</keyname><forenames>Brian</forenames></author></authors><title>The Case for Cloud Service Trustmarks and Assurance-as-a-Service</title><categories>cs.DC cs.CY</categories><comments>6 pages and 1 figure</comments><report-no>3rd Int Conf on Cloud Computing and Services Science (CLOSER).
  Aachen, Germany May 2013. SciTePress</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing represents a significant economic opportunity for Europe.
However, this growth is threatened by adoption barriers largely related to
trust. This position paper examines trust and confidence issues in cloud
computing and advances a case for addressing them through the implementation of
a novel trustmark scheme for cloud service providers. The proposed trustmark
would be both active and dynamic featuring multi-modal information about the
performance of the underlying cloud service. The trustmarks would be informed
by live performance data from the cloud service provider, or ideally an
independent third-party accountability and assurance service that would
communicate up-to-date information relating to service performance and
dependability. By combining assurance measures with a remediation scheme, cloud
service providers could both signal dependability to customers and the wider
marketplace and provide customers, auditors and regulators with a mechanism for
determining accountability in the event of failure or non-compliance. As a
result, the trustmarks would convey to consumers of cloud services and other
stakeholders that strong assurance and accountability measures are in place for
the service in question and thereby address trust and confidence issues in
cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5773</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5773</id><created>2014-02-24</created><authors><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Data Management Challenges in Paediatric Information Systems</title><categories>cs.DB cs.CY</categories><comments>16 pages, 4 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:0812.2874, arXiv:cs/0603036, arXiv:0707.0763</comments><journal-ref>Chapter in From Physics into Daily Life - How Knowledge Transfer
  Changed Biology, Medicine and Healthcare. Ed B. Bressan. Wiley Publishers,
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a compelling demand for the data integration and exploitation of
heterogeneous biomedical information for improved clinical practice, medical
research, and personalised healthcare across the EU. The area of paediatric
information integration is particularly challenging since the patients
physiology changes with growth and different aspects of health being regularly
monitored over extended periods of time. Paediatricians require access to
heterogeneous data sets, often collected in different locations with different
apparatus and over extended timescales. Using a Grid platform originally
developed for physics at CERN and a novel integrated semantic data model the
Health-e-Child project has developed an integrated healthcare platform for
European paediatrics, providing seamless integration of traditional and
emerging sources of biomedical data. The long-term goal of the project was to
provide uninhibited access to universal biomedical knowledge repositories for
personalised and preventive healthcare, large-scale information-based
biomedical research and training, and informed policy making. The project built
a Grid-enabled european network of leading clinical centres that can share and
annotate paediatric data, can validate systems clinically, and diffuse clinical
excellence across Europe by setting up new technologies, clinical workflows,
and standards. The Health-e-Child project highlights data management challenges
for the future of European paediatric healthcare and is the subject of this
chapter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5774</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5774</id><created>2014-02-24</created><authors><author><keyname>Nie</keyname><forenames>Da-Cheng</forenames></author><author><keyname>An</keyname><forenames>Ya-Hui</forenames></author><author><keyname>Dong</keyname><forenames>Qiang</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Information Filtering via Balanced Diffusion on Bipartite Networks</title><categories>cs.IR</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent decade has witnessed the increasing popularity of recommender systems,
which help users acquire relevant commodities and services from overwhelming
resources on Internet. Some simple physical diffusion processes have been used
to design effective recommendation algorithms for user-object bipartite
networks, typically mass diffusion (MD) and heat conduction (HC) algorithms
which have different advantages respectively on accuracy and diversity. In this
paper, we investigate the effect of weight assignment in the hybrid of MD and
HC, and find that a new hybrid algorithm of MD and HC with balanced weights
will achieve the optimal recommendation results, we name it balanced diffusion
(BD) algorithm. Numerical experiments on three benchmark data sets, MovieLens,
Netflix and RateYourMusic (RYM), show that the performance of BD algorithm
outperforms the existing diffusion-based methods on the three important
recommendation metrics, accuracy, diversity and novelty. Specifically, it can
not only provide accurately recommendation results, but also yield higher
diversity and novelty in recommendations by accurately recommending unpopular
objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5781</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5781</id><created>2014-02-24</created><authors><author><keyname>Ramachandra</keyname><forenames>Karthik</forenames></author><author><keyname>Chavan</keyname><forenames>Mahendra</forenames></author><author><keyname>Guravannavar</keyname><forenames>Ravindra</forenames></author><author><keyname>Sudarshan</keyname><forenames>S</forenames></author></authors><title>Program Transformations for Asynchronous and Batched Query Submission</title><categories>cs.DB</categories><comments>14 pages</comments><doi>10.1109/TKDE.2014.2334302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of database/Web-service backed applications can be
significantly improved by asynchronous submission of queries/requests well
ahead of the point where the results are needed, so that results are likely to
have been fetched already when they are actually needed. However, manually
writing applications to exploit asynchronous query submission is tedious and
error-prone. In this paper we address the issue of automatically transforming a
program written assuming synchronous query submission, to one that exploits
asynchronous query submission. Our program transformation method is based on
data flow analysis and is framed as a set of transformation rules. Our rules
can handle query executions within loops, unlike some of the earlier work in
this area. We also present a novel approach that, at runtime, can combine
multiple asynchronous requests into batches, thereby achieving the benefits of
batching in addition to that of asynchronous submission. We have built a tool
that implements our transformation techniques on Java programs that use JDBC
calls; our tool can be extended to handle Web service calls. We have carried
out a detailed experimental study on several real-life applications, which
shows the effectiveness of the proposed rewrite techniques, both in terms of
their applicability and the performance gains achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5783</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5783</id><created>2014-02-24</created><authors><author><keyname>Abu-Affash</keyname><forenames>Karim</forenames></author><author><keyname>Carmi</keyname><forenames>Paz</forenames></author><author><keyname>Tzur</keyname><forenames>Anat Parush</forenames></author></authors><title>Dual Power Assignment via Second Hamiltonian Cycle</title><categories>cs.DM cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A power assignment is an assignment of transmission power to each of the
wireless nodes of a wireless network, so that the induced graph satisfies some
desired properties. The cost of a power assignment is the sum of the assigned
powers. In this paper, we consider the dual power assignment problem, in which
each wireless node is assigned a high- or low-power level, so that the induced
graph is strongly connected and the cost of the assignment is minimized. We
improve the best known approximation ratio from
$\frac{\pi^2}{6}-\frac{1}{36}+\epsilon\thickapprox 1.617$ to
$\frac{11}{7}\thickapprox 1.571$.
  Moreover, we show that the algorithm of Khuller et al. for the strongly
connected spanning subgraph problem, which achieves an approximation ratio of
$1.61$, is $1.522$-approximation algorithm for symmetric directed graphs.
  The innovation of this paper is in achieving these results via utilizing
interesting properties for the existence of a second Hamiltonian cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5784</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5784</id><created>2014-02-24</created><authors><author><keyname>Li</keyname><forenames>Yuzhe</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Transmission Power Scheduling for Energy Harvesting Sensor in Remote
  State Estimation</title><categories>math.OC cs.SY</categories><comments>Extended version of article to be published in the Proceedings of the
  19th IFAC World Congress, 2014</comments><msc-class>93E11, 93E20, 93C41</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study remote estimation in a wireless sensor network. Instead of using a
conventional battery-powered sensor, a sensor equipped with an energy harvester
which can obtain energy from the external environment is utilized. We formulate
this problem into an infinite time-horizon Markov decision process and provide
the optimal sensor transmission power control strategy. In addition, a
sub-optimal strategy which is easier to implement and requires less computation
is presented. A numerical example is provided to illustrate the implementation
of the sub-optimal policy and evaluation of its estimation performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5792</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5792</id><created>2014-02-24</created><updated>2014-09-29</updated><authors><author><keyname>Kia</keyname><forenames>Seyed Mostafa</forenames></author><author><keyname>Rahmani</keyname><forenames>Hossein</forenames></author><author><keyname>Mortezaei</keyname><forenames>Reza</forenames></author><author><keyname>Moghaddam</keyname><forenames>Mohsen Ebrahimi</forenames></author><author><keyname>Namazi</keyname><forenames>Amer</forenames></author></authors><title>A Novel Scheme for Intelligent Recognition of Pornographic Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harmful contents are rising in internet day by day and this motivates the
essence of more research in fast and reliable obscene and immoral material
filtering. Pornographic image recognition is an important component in each
filtering system. In this paper, a new approach for detecting pornographic
images is introduced. In this approach, two new features are suggested. These
two features in combination with other simple traditional features provide
decent difference between porn and non-porn images. In addition, we applied
fuzzy integral based information fusion to combine MLP (Multi-Layer Perceptron)
and NF (Neuro-Fuzzy) outputs. To test the proposed method, performance of
system was evaluated over 18354 download images from internet. The attained
precision was 93% in TP and 8% in FP on training dataset, and 87% and 5.5% on
test dataset. Achieved results verify the performance of proposed system versus
other related works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5803</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5803</id><created>2014-02-24</created><authors><author><keyname>Lauer</keyname><forenames>Fabien</forenames><affiliation>LORIA</affiliation></author><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author></authors><title>Sparse phase retrieval via group-sparse optimization</title><categories>cs.IT cs.LG math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with sparse phase retrieval, i.e., the problem of estimating
a vector from quadratic measurements under the assumption that few components
are nonzero. In particular, we consider the problem of finding the sparsest
vector consistent with the measurements and reformulate it as a group-sparse
optimization problem with linear constraints. Then, we analyze the convex
relaxation of the latter based on the minimization of a block l1-norm and show
various exact recovery and stability results in the real and complex cases.
Invariance to circular shifts and reflections are also discussed for real
vectors measured via complex matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5805</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5805</id><created>2014-02-24</created><authors><author><keyname>Hitimana</keyname><forenames>Eric</forenames></author><author><keyname>Gwun</keyname><forenames>Oubong</forenames></author></authors><title>Automatic Estimation of Live Coffee Leaf Infection based on Image
  Processing Techniques</title><categories>cs.CV</categories><comments>SIPP 2014 : Second International Conference on Signal, Image
  Processing and Pattern Recognition,Sydney, Australia</comments><doi>10.5121/csit.2014.4221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is the most challenging issue in computer vision
applications. And most difficulties for crops management in agriculture are the
lack of appropriate methods for detecting the leaf damage for pests treatment.
In this paper we proposed an automatic method for leaf damage detection and
severity estimation of coffee leaf by avoiding defoliation. After enhancing the
contrast of the original image using LUT based gamma correction, the image is
processed to remove the background, and the output leaf is clustered using
Fuzzy c-means segmentation in V channel of YUV color space to maximize all leaf
damage detection, and finally, the severity of leaf is estimated in terms of
ratio for leaf pixel distribution between the normal and the detected leaf
damage. The results in each proposed method was compared to the current
researches and the accuracy is obvious either in the background removal or
damage detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5818</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5818</id><created>2014-02-24</created><authors><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author><author><keyname>Bozkurt</keyname><forenames>Alican</forenames></author><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author></authors><title>Deconvolution Using Projections Onto The Epigraph Set of a Convex Cost
  Function</title><categories>cs.DS math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1309.0700, arXiv:1402.2088</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new deconvolution algorithm based on orthogonal projections onto the
epigraph set of a convex cost function is presented. In this algorithm, the
dimension of the minimization problem is lifted by one and sets corresponding
to the cost function are defined. As the utilized cost function is a convex
function in $R^N$, the corresponding epigraph set is also a convex set in
$R^{N+1}$. The deconvolution algorithm starts with an arbitrary initial
estimate in $R^{N+1}$. At each step of the iterative algorithm, first
deconvolution projections are performed onto the epigraphs, later an orthogonal
projection is performed onto one of the constraint sets associated with the
cost function in a sequential manner. The method provides globally optimal
solutions for total-variation, $\ell_1$, $\ell_2$, and entropic cost functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5830</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5830</id><created>2014-02-24</created><authors><author><keyname>Ampellio</keyname><forenames>Enrico</forenames></author><author><keyname>Vassio</keyname><forenames>Luca</forenames></author></authors><title>Artificial super-Bee enhanced Colony (AsBeC) algorithm for numerical
  optimization with limited function evaluations Part 1: technologies and
  benchmark validation</title><categories>math.OC cs.AI cs.DC cs.NE</categories><comments>19 pages, 10 figures, submitted to Springer Swarm Intelligence
  journal</comments><report-no>POLITO-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Artificial Bee Colony (ABC) algorithm is a very effective, simple and
robust nature-based metaheuristic optimization procedure. The standard ABC has
been recently developed and it currently raises a lot of interest and upgrading
efforts, since it finds attractive applications in many fields of the
scientific research. One of these application is represented by optimizations
that employ very expensive numerical simulations in terms of resources. In this
framework there is a strong need for methods that assure the best improvement
with the shortest analyses time. In this paper, the authors propose and recall
several modifications to the standard ABC in order to improve its speed and
solution accuracy for problems where function evaluations have to be limited
around 103. These technologies consist in enhancements of the basic structure
and hybridizations with other optimization strategies. Moreover three different
kinds of parallelization in the code are analysed. Each modification as well as
their combinations are studied and explained; the performance of modified ABC
is evaluated through extensive simulations over different analytical test
functions. Moreover, standard settings for this new algorithm, called
Artificial super-Bee enhanced Colony (AsBeC), are given in order to maintain
the simplicity of ABC. The present article explores all the technical issues
involved about AsBeC, while the second part will consider a real-like
application to the engineering optimal design of turbomachinery through
Computational Fluid Dynamics, environment in which the improved algorithm was
originally conceived by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5835</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5835</id><created>2014-02-24</created><authors><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Polcovar: Software for Computing the Mean and Variance of Subgraph
  Counts in Random Graphs</title><categories>cs.MS</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The mean and variance of the number of appearances of a given subgraph $H$ in
an Erd\H{o}s--R\'enyi random graph over $n$ nodes are rational polynomials in
$n$. We present a piece of software named Polcovar (from &quot;polynomial&quot; and
&quot;covariance&quot;) that computes the exact rational coefficients of these
polynomials in function of $H$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5836</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5836</id><created>2014-02-24</created><updated>2014-09-14</updated><authors><author><keyname>Duvenaud</keyname><forenames>David</forenames></author><author><keyname>Rippel</keyname><forenames>Oren</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Avoiding pathologies in very deep networks</title><categories>stat.ML cs.LG</categories><comments>20 pages, 14 figures. Appeared in AISTATS 2014. This version has many
  minor fixes, and nicer figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Choosing appropriate architectures and regularization strategies for deep
networks is crucial to good predictive performance. To shed light on this
problem, we analyze the analogous problem of constructing useful priors on
compositions of functions. Specifically, we study the deep Gaussian process, a
type of infinitely-wide, deep neural network. We show that in standard
architectures, the representational capacity of the network tends to capture
fewer degrees of freedom as the number of layers increases, retaining only a
single degree of freedom in the limit. We propose an alternate network
architecture which does not suffer from this pathology. We also examine deep
covariance functions, obtained by composing infinitely many feature transforms.
Lastly, we characterize the class of models obtained by performing dropout on
Gaussian processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5843</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5843</id><created>2014-02-24</created><updated>2015-05-26</updated><authors><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Sciortino</keyname><forenames>Marinella</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Cyclic Complexity of Words</title><categories>cs.FL cs.DM math.CO</categories><comments>Full version</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study a complexity function on words $c_\omega(n),$ we call
\emph{cyclic complexity}, which counts the number of conjugacy classes of
factors of length $n$ of an infinite word $\omega.$ We extend the well known
Morse-Hedlund theorem to the setting of cyclic complexity by showing that a
word is ultimately periodic if and only if it has bounded cyclic complexity.
Unlike most complexity functions, cyclic complexity distinguishes between
Sturmian words of different slopes. We prove that if $x$ is a Sturmian word and
$y$ is a word having the same cyclic complexity of $x,$ then up to renaming
letters, $x$ and $y$ have the same set of factors. In particular, $y$ is also
Sturmian of slope equal to that of $x.$ Since $c_\omega(n)=1$ for some $n\geq
1$ implies $\omega$ is periodic, it is natural the consider the
$\liminf_{n\rightarrow \infty} c_\omega(n).$ We show that if $\omega$ is a
Sturmian word, then $\liminf_{n\rightarrow \infty} c_\omega(n)=2.$ We prove
however that this is not a characterization of Sturmian words by exhibiting a
restricted class of Toeplitz words, including the period doubling word, which
also verify this same condition on the limit infimum. In contrast we show that
for the Thue-Morse word the $\liminf_{n\rightarrow \infty}
c_\omega(n)=+\infty.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5845</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5845</id><created>2014-02-24</created><authors><author><keyname>Sharma</keyname><forenames>Priyanka</forenames></author><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Mathematical Modelling of Energy Wastage in Absence of Levelling and
  Sectoring in Wireless Sensor Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we quantitatively (mathematically) reason the energy savings
achieved by the Leveling and Sectoring protocol. Due to the energy constraints
on the sensor nodes (in terms of supply of energy) energy awareness has become
crucial in networking protocol stack. The understanding of routing protocols
along with energy awareness in a network would help in energy opti-mization
with efficient routing .We provide analytical modelling of the energy wastage
in the absence of Leveling and Sectoring protocol by considering the network in
the form of binary tree, nested tree and Q-ary tree. The simulation results
reflect the energy wastage in the absence of Levelling and Sectoring based
hybrid protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5857</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5857</id><created>2014-02-24</created><updated>2015-05-29</updated><authors><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author></authors><title>The challenges of unbounded treewidth in parameterised subgraph counting
  problems</title><categories>cs.CC cs.DM math.CO</categories><comments>Survey part of paper substantially extended and reorganised; some
  additional figures added to illustrate proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameterised subgraph counting problems are the most thoroughly studied
topic in the theory of parameterised counting, and there has been significant
recent progress in this area. Many of the existing tractability results for
parameterised problems which involve finding or counting subgraphs with
particular properties rely on bounding the treewidth of these subgraphs in some
sense; here, we prove a number of hardness results for the situation in which
this bounded treewidth condition does not hold, resulting in dichotomies for
some special cases of the general subgraph counting problem. The paper also
gives a thorough survey of known results on this subject and the methods used,
as well as discussing the relationships both between multicolour and uncoloured
versions of subgraph counting problems, and between exact counting, approximate
counting and the corresponding decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5859</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5859</id><created>2014-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Huanguo</forenames></author><author><keyname>Lv</keyname><forenames>Sha</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Qu</keyname><forenames>Xun</forenames></author></authors><title>A Novel Face Recognition Method using Nearest Line Projection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition is a popular application of pat- tern recognition methods,
and it faces challenging problems including illumination, expression, and pose.
The most popular way is to learn the subspaces of the face images so that it
could be project to another discriminant space where images of different
persons can be separated. In this paper, a nearest line projection algorithm is
developed to represent the face images for face recognition. Instead of
projecting an image to its nearest image, we try to project it to its nearest
line spanned by two different face images. The subspaces are learned so that
each face image to its nearest line is minimized. We evaluated the proposed
algorithm on some benchmark face image database, and also compared it to some
other image projection algorithms. The experiment results showed that the
proposed algorithm outperforms other ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5869</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5869</id><created>2014-02-24</created><authors><author><keyname>Zivari-Fard</keyname><forenames>Hassan</forenames></author><author><keyname>Akhbari</keyname><forenames>Bahareh</forenames></author><author><keyname>Ahmadian-Attari</keyname><forenames>Mahmoud</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Compound Multiple Access Channel with Confidential Messages</title><categories>cs.IT math.IT</categories><comments>Accepted at IEEE ICC 2014. arXiv admin note: substantial text overlap
  with arXiv:1402.4799</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of secret communication over a Compound
Multiple Access Channel (MAC). In this channel, we assume that one of the
transmitted messages is confidential that is only decoded by its corresponding
receiver and kept secret from the other receiver. For this proposed setting
(compound MAC with confidential messages), we derive general inner and outer
bounds on the secrecy capacity region. Also, as examples, we investigate 'Less
noisy' and 'Gaussian' versions of this channel, and extend the results of the
discrete memoryless version to these cases. Moreover, providing numerical
examples for the Gaussian case, we illustrate the comparison between achievable
rate regions of compound MAC and compound MAC with confidential messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5874</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5874</id><created>2014-02-24</created><authors><author><keyname>Hamed</keyname><forenames>Mohammad Ghasemi</forenames></author><author><keyname>Serrurier</keyname><forenames>Mathieu</forenames></author><author><keyname>Durand</keyname><forenames>Nicolas</forenames></author></authors><title>Predictive Interval Models for Non-parametric Regression</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having a regression model, we are interested in finding two-sided intervals
that are guaranteed to contain at least a desired proportion of the conditional
distribution of the response variable given a specific combination of
predictors. We name such intervals predictive intervals. This work presents a
new method to find two-sided predictive intervals for non-parametric least
squares regression without the homoscedasticity assumption. Our predictive
intervals are built by using tolerance intervals on prediction errors in the
query point's neighborhood. We proposed a predictive interval model test and we
also used it as a constraint in our hyper-parameter tuning algorithm. This
gives an algorithm that finds the smallest reliable predictive intervals for a
given dataset. We also introduce a measure for comparing different interval
prediction methods yielding intervals having different size and coverage. These
experiments show that our methods are more reliable, effective and precise than
other interval prediction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5876</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5876</id><created>2014-02-24</created><updated>2014-05-08</updated><authors><author><keyname>Calandra</keyname><forenames>Roberto</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl Edward</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Manifold Gaussian Processes for Regression</title><categories>stat.ML cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness
assumptions on the structure of the function to be modeled. To model complex
and non-differentiable functions, these smoothness assumptions are often too
restrictive. One way to alleviate this limitation is to find a different
representation of the data by introducing a feature space. This feature space
is often learned in an unsupervised way, which might lead to data
representations that are not useful for the overall regression task. In this
paper, we propose Manifold Gaussian Processes, a novel supervised method that
learns jointly a transformation of the data into a feature space and a GP
regression from the feature space to observed space. The Manifold GP is a full
GP, and it allows to learn data representations, which are useful for the
overall regression task. As a proof-of-concept, we evaluate our approach on
complex non-smooth functions where standard GPs perform poorly, such as step
functions and effects of ground contacts in a robotics application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5878</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5878</id><created>2014-02-20</created><authors><author><keyname>Cetto</keyname><forenames>Alexandra</forenames></author><author><keyname>Netter</keyname><forenames>Michael</forenames></author><author><keyname>Pernul</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Richthammer</keyname><forenames>Christian</forenames></author><author><keyname>Riesner</keyname><forenames>Moritz</forenames></author><author><keyname>Roth</keyname><forenames>Christian</forenames></author><author><keyname>S&#xe4;nger</keyname><forenames>Johannes</forenames></author></authors><title>Friend Inspector: A Serious Game to Enhance Privacy Awareness in Social
  Networks</title><categories>cs.CY cs.AI</categories><report-no>IDGEI/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, many users of Social Network Sites are insufficiently aware of who
can see their shared personal items. Nonetheless, most approaches focus on
enhancing privacy in Social Networks through improved privacy settings,
neglecting the fact that privacy awareness is a prerequisite for privacy
control. Social Network users first need to know about privacy issues before
being able to make adjustments. In this paper, we introduce Friend Inspector, a
serious game that allows its users to playfully increase their privacy
awareness on Facebook. Since its launch, Friend Inspector has attracted a
significant number of visitors, emphasising the need for better tools to
understand privacy settings on Social Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5881</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5881</id><created>2014-02-24</created><authors><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Filter Bank Multicarrier for Massive MIMO</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces filter bank multicarrier (FBMC) as a potential
candidate in the application of massive MIMO communication. It also points out
the advantages of FBMC over OFDM (orthogonal frequency division multiplexing)
in the application of massive MIMO. The absence of cyclic prefix in FBMC
increases the bandwidth efficiency. In addition, FBMC allows carrier
aggregation straightforwardly. Self-equalization, a property of FBMC in massive
MIMO that is introduced in this paper, has the impact of reducing (i)
complexity; (ii) sensitivity to carrier frequency offset (CFO); (iii)
peak-to-average power ratio (PAPR); (iv) system latency; and (v) increasing
bandwidth efficiency. The numerical results that corroborate these claims are
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5886</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5886</id><created>2014-02-24</created><authors><author><keyname>Javdani</keyname><forenames>Shervin</forenames></author><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha</forenames></author></authors><title>Near Optimal Bayesian Active Learning for Decision Making</title><categories>cs.LG cs.AI</categories><comments>Extended version of work appearing in the International conference on
  Artificial Intelligence and Statistics (AISTATS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How should we gather information to make effective decisions? We address
Bayesian active learning and experimental design problems, where we
sequentially select tests to reduce uncertainty about a set of hypotheses.
Instead of minimizing uncertainty per se, we consider a set of overlapping
decision regions of these hypotheses. Our goal is to drive uncertainty into a
single decision region as quickly as possible.
  We identify necessary and sufficient conditions for correctly identifying a
decision region that contains all hypotheses consistent with observations. We
develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove
that is competitive with the intractable optimal policy. Our efficient
implementation of the algorithm relies on computing subsets of the complete
homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on
two practical applications: approximate comparison-based learning and active
localization using a robot manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5897</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5897</id><created>2014-02-21</created><authors><author><keyname>Peise</keyname><forenames>Elmar</forenames><affiliation>AICES, RWTH Aachen</affiliation></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames><affiliation>AICES, RWTH Aachen</affiliation></author></authors><title>A Study on the Influence of Caching: Sequences of Dense Linear Algebra
  Kernels</title><categories>cs.MS cs.NA cs.PF</categories><comments>Submitted to the Ninth International Workshop on Automatic
  Performance Tuning (iWAPT2014)</comments><report-no>AICES-2014/02-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is universally known that caching is critical to attain high- performance
implementations: In many situations, data locality (in space and time) plays a
bigger role than optimizing the (number of) arithmetic floating point
operations. In this paper, we show evidence that at least for linear algebra
algorithms, caching is also a crucial factor for accurate performance modeling
and performance prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5902</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5902</id><created>2014-02-24</created><updated>2015-02-11</updated><authors><author><keyname>Yu</keyname><forenames>Felix X.</forenames></author><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Jebara</keyname><forenames>Tony</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>On Learning from Label Proportions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning from Label Proportions (LLP) is a learning setting, where the
training data is provided in groups, or &quot;bags&quot;, and only the proportion of each
class in each bag is known. The task is to learn a model to predict the class
labels of the individual instances. LLP has broad applications in political
science, marketing, healthcare, and computer vision. This work answers the
fundamental question, when and why LLP is possible, by introducing a general
framework, Empirical Proportion Risk Minimization (EPRM). EPRM learns an
instance label classifier to match the given label proportions on the training
data. Our result is based on a two-step analysis. First, we provide a VC bound
on the generalization error of the bag proportions. We show that the bag sample
complexity is only mildly sensitive to the bag size. Second, we show that under
some mild assumptions, good bag proportion prediction guarantees good instance
label prediction. The results together provide a formal guarantee that the
individual labels can indeed be learned in the LLP setting. We discuss
applications of the analysis, including justification of LLP algorithms,
learning with population proportions, and a paradigm for learning algorithms
with privacy guarantees. We also demonstrate the feasibility of LLP based on a
case study in real-world setting: predicting income based on census data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5904</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5904</id><created>2014-02-24</created><updated>2014-08-24</updated><authors><author><keyname>Hougardy</keyname><forenames>Stefan</forenames></author></authors><title>On the Integrality Ratio of the Subtour LP for Euclidean TSP</title><categories>cs.DM cs.DS math.CO</categories><msc-class>90C27 90C59 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long standing conjecture says that the integrality ratio of the subtour LP
for metric TSP is $4/3$. A well known family of graphic TSP instances achieves
this lower bound asymptotically. For Euclidean TSP the best known lower bound
on the integrality ratio was $8/7$. We improve this value by presenting a
family of Euclidean TSP instances for which the integrality ratio of the
subtour LP converges to 4/3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5912</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5912</id><created>2014-02-24</created><updated>2014-04-20</updated><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author><author><keyname>Jafar</keyname><forenames>Syed Ali</forenames></author></authors><title>On the Vector Broadcast Channel with Alternating CSIT: A Topological
  Perspective</title><categories>cs.IT math.IT</categories><comments>Shorter version will be presented at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many wireless networks, link strengths are affected by many topological
factors such as different distances, shadowing and inter-cell interference,
thus resulting in some links being generally stronger than other links. From an
information theoretic point of view, accounting for such topological aspects
has remained largely unexplored, despite strong indications that such aspects
can crucially affect transceiver and feedback design, as well as the overall
performance.
  The work here takes a step in exploring this interplay between topology,
feedback and performance. This is done for the two user broadcast channel with
random fading, in the presence of a simple two-state topological setting of
statistically strong vs. weaker links, and in the presence of a practical
ternary feedback setting of alternating channel state information at the
transmitter (alternating CSIT) where for each channel realization, this CSIT
can be perfect, delayed, or not available.
  In this setting, the work derives generalized degrees-of-freedom bounds and
exact expressions, that capture performance as a function of feedback
statistics and topology statistics. The results are based on novel topological
signal management (TSM) schemes that account for topology in order to fully
utilize feedback. This is achieved for different classes of feedback mechanisms
of practical importance, from which we identify specific feedback mechanisms
that are best suited for different topologies. This approach offers further
insight on how to split the effort --- of channel learning and feeding back
CSIT --- for the strong versus for the weaker link. Further intuition is
provided on the possible gains from topological spatio-temporal diversity,
where topology changes in time and across users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5922</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5922</id><created>2014-02-24</created><updated>2015-09-21</updated><authors><author><keyname>Balan</keyname><forenames>Adriana</forenames><affiliation>University Politehnica of Bucharest</affiliation></author><author><keyname>Kurz</keyname><forenames>Alexander</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Velebil</keyname><forenames>Ji&#x159;&#xed;</forenames><affiliation>Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic</affiliation></author></authors><title>Positive fragments of coalgebraic logics</title><categories>math.CT cs.LO</categories><comments>51 pages; accepted for publication; expanded and improved version of
  the previous submission. Proposition 4.15 is new; Section 6 was rewritten in
  view of new results (theorem 6.9, proposition 6.14, paragraphs A-D);
  references added</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:18) 2015</journal-ref><doi>10.2168/LMCS-11(3:18)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positive modal logic was introduced in an influential 1995 paper of Dunn as
the positive fragment of standard modal logic. His completeness result consists
of an axiomatization that derives all modal formulas that are valid on all
Kripke frames and are built only from atomic propositions, conjunction,
disjunction, box and diamond. In this paper, we provide a coalgebraic analysis
of this theorem, which not only gives a conceptual proof based on duality
theory, but also generalizes Dunn's result from Kripke frames to coalgebras for
weak-pullback preserving functors. To facilitate this analysis we prove a
number of category theoretic results on functors on the categories
$\mathsf{Set}$ of sets and $\mathsf{Pos}$ of posets: Every functor
$\mathsf{Set} \to \mathsf{Pos}$ has a $\mathsf{Pos}$-enriched left Kan
extension $\mathsf{Pos} \to \mathsf{Pos}$. Functors arising in this way are
said to have a presentation in discrete arities. In the case that $\mathsf{Set}
\to \mathsf{Pos}$ is actually $\mathsf{Set}$-valued, we call the corresponding
left Kan extension $\mathsf{Pos} \to \mathsf{Pos}$ its posetification. A
$\mathsf{Set}$-functor preserves weak pullbacks if and only if its
posetification preserves exact squares. A $\mathsf{Pos}$-functor with a
presentation in discrete arities preserves surjections. The inclusion
$\mathsf{Set} \to \mathsf{Pos}$ is dense. A functor $\mathsf{Pos} \to
\mathsf{Pos}$ has a presentation in discrete arities if and only if it
preserves coinserters of `truncated nerves of posets'. A functor $\mathsf{Pos}
\to \mathsf{Pos}$ is a posetification if and only if it preserves coinserters
of truncated nerves of posets and discrete posets. A locally monotone
endofunctor of an ordered variety has a presentation by monotone operations and
equations if and only if it preserves $\mathsf{Pos}$-enriched sifted colimits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5923</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5923</id><created>2014-02-24</created><authors><author><keyname>Tommasi</keyname><forenames>Tatiana</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author><author><keyname>Caputo</keyname><forenames>Barbara</forenames></author></authors><title>A Testbed for Cross-Dataset Analysis</title><categories>cs.CV</categories><report-no>December 2013, Technical Report: KUL/ESAT/PSI/1304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its beginning visual recognition research has tried to capture the huge
variability of the visual world in several image collections. The number of
available datasets is still progressively growing together with the amount of
samples per object category. However, this trend does not correspond directly
to an increasing in the generalization capabilities of the developed
recognition systems. Each collection tends to have its specific characteristics
and to cover just some aspects of the visual world: these biases often narrow
the effect of the methods defined and tested separately over each image set.
Our work makes a first step towards the analysis of the dataset bias problem on
a large scale. We organize twelve existing databases in a unique corpus and we
present the visual community with a useful feature repository for future
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5927</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5927</id><created>2014-02-24</created><updated>2015-03-10</updated><authors><author><keyname>B&#xe4;uml</keyname><forenames>Stefan</forenames></author><author><keyname>Christandl</keyname><forenames>Matthias</forenames></author><author><keyname>Horodecki</keyname><forenames>Karol</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Limitations on Quantum Key Repeaters</title><categories>quant-ph cs.IT math.IT</categories><comments>11+38 pages, 4 figures, Statements for exact p-bits weakened as
  non-locking bound on measured relative entropy distance contained an error</comments><journal-ref>Nature Communications 6, Article number: 6908, 2015</journal-ref><doi>10.1038/ncomms7908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major application of quantum communication is the distribution of entangled
particles for use in quantum key distribution (QKD). Due to noise in the
communication line, QKD is in practice limited to a distance of a few hundred
kilometres, and can only be extended to longer distances by use of a quantum
repeater, a device which performs entanglement distillation and quantum
teleportation. The existence of noisy entangled states that are undistillable
but nevertheless useful for QKD raises the question of the feasibility of a
quantum key repeater, which would work beyond the limits of entanglement
distillation, hence possibly tolerating higher noise levels than existing
protocols. Here we exhibit fundamental limits on such a device in the form of
bounds on the rate at which it may extract secure key. As a consequence, we
give examples of states suitable for QKD but unsuitable for the most general
quantum key repeater protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5945</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5945</id><created>2014-02-24</created><authors><author><keyname>Ziegler</keyname><forenames>Konstantin</forenames></author></authors><title>Tame Decompositions and Collisions</title><categories>math.AC cs.SC</categories><msc-class>11T06, 12Y05</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A univariate polynomial f over a field is decomposable if f = g o h = g(h)
for nonlinear polynomials g and h. It is intuitively clear that the
decomposable polynomials form a small minority among all polynomials over a
finite field. The tame case, where the characteristic p of Fq does not divide n
= deg f, is fairly well-understood, and we have reasonable bounds on the number
of decomposables of degree n. Nevertheless, no exact formula is known if $n$
has more than two prime factors. In order to count the decomposables, one wants
to know, under a suitable normalization, the number of collisions, where
essentially different (g, h) yield the same f. In the tame case, Ritt's Second
Theorem classifies all 2-collisions.
  We introduce a normal form for multi-collisions of decompositions of
arbitrary length with exact description of the (non)uniqueness of the
parameters. We obtain an efficiently computable formula for the exact number of
such collisions at degree n over a finite field of characteristic coprime to p.
This leads to an algorithm for the exact number of decomposable polynomials at
degree n over a finite field Fq in the tame case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5950</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5950</id><created>2014-02-23</created><updated>2014-04-11</updated><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>A generalization of extension complexity that captures $P$</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a generalization of the extension complexity of a
polyhedron $Q$. On the one hand it is general enough so that all problems in
$P$ can be formulated as linear programs with polynomial size extension
complexity. On the other hand it still allows non-polynomial lower bounds to be
proved for $NP$-hard problems independently of whether or not $P=NP$. The
generalization, called $H$-free extension complexity, allows for a set of valid
inequalities $H$ to be excluded in computing the extension complexity of $Q$.
We give results on the $H$-free extension complexity of hard matching problems
(when $H$ are the odd set inequalities) and the traveling salesman problem
(when $H$ are the subtour elimination constraints).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5951</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5951</id><created>2014-02-23</created><authors><author><keyname>Kan</keyname><forenames>Zhen</forenames></author><author><keyname>Shea</keyname><forenames>John M.</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Navigation Function Based Decentralized Control of A Multi-Agent System
  with Network Connectivity Constraints</title><categories>cs.SY</categories><comments>16 pages, 9 figures, submitted to NATO Science for Peace and Security
  Series by IOS Press. arXiv admin note: substantial text overlap with
  arXiv:1402.5639</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of applications require or can benefit from collaborative
behavior of a group of agents. The technical challenge addressed in this
chapter is the development of a decentralized control strategy that enables
each agent to independently navigate to ensure agents achieve a collective goal
while maintaining network connectivity. Specifically, cooperative controllers
are developed for networked agents with limited sensing and network
connectivity constraints. By modeling the interaction among the agents as a
graph, several different approaches to address the problems of preserving
network connectivity are presented, with the focus on a method that utilizes
navigation function frameworks. By modeling network connectivity constraints as
artificial obstacles in navigation functions, a decentralized control strategy
is presented in two particular applications, formation control and rendezvous
for a system of autonomous agents, which ensures global convergence to the
unique minimum of the potential field (i.e., desired formation or desired
destination) while preserving network connectivity. Simulation results are
provided to demonstrate the developed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5953</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5953</id><created>2014-02-24</created><authors><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>A Description Driven Approach for Flexible Metadata Tracking</title><categories>cs.SE</categories><comments>10 pages and 3 figures. arXiv admin note: text overlap with
  arXiv:1402.5753, arXiv:1402.5764</comments><journal-ref>7th ESA International Conference on Ensuring Long-Term
  Preservation and Adding Value to Scientific and Technical Data (PV 2013)
  4--6th November 2013. Frascati, Italy</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolving user requirements presents a considerable software engineering
challenge, all the more so in an environment where data will be stored for a
very long time, and must remain usable as the system specification evolves
around it. Capturing the description of the system addresses this issue since a
description-driven approach enables new versions of data structures and
processes to be created alongside the old, thereby providing a history of
changes to the underlying data models and enabling the capture of provenance
data. This description-driven approach is advocated in this paper in which a
system called CRISTAL is presented. CRISTAL is based on description-driven
principles; it can use previous versions of stored descriptions to define
various versions of data which can be stored in various forms. To demonstrate
the efficacy of this approach the history of the project at CERN is presented
where CRISTAL was used to track data and process definitions and their
associated provenance data in the construction of the CMS ECAL detector, how it
was applied to handle analysis tracking and data index provenance in the
neuGRID and N4U projects, and how it will be matured further in the CRISTAL-ISE
project. We believe that the CRISTAL approach could be invaluable in handling
the evolution, indexing and tracking of large datasets, and are keen to apply
it further in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5979</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5979</id><created>2014-02-24</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author><author><keyname>Coutinho</keyname><forenames>V. A.</forenames></author><author><keyname>Kulasekera</keyname><forenames>S.</forenames></author><author><keyname>Madanayake</keyname><forenames>A.</forenames></author></authors><title>DCT-like Transform for Image and Video Compression Requires 10 Additions
  Only</title><categories>cs.MM cs.CV stat.ME</categories><comments>11 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiplierless pruned approximate 8-point discrete cosine transform (DCT)
requiring only 10 additions is introduced. The proposed algorithm was assessed
in image and video compression, showing competitive performance with
state-of-the-art methods. Digital implementation in 45 nm CMOS technology up to
place-and-route level indicates clock speed of 255 MHz at a 1.1 V supply. The
8x8 block rate is 31.875 MHz.The DCT approximation was embedded into HEVC
reference software; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC,
presented negligible image degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5987</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5987</id><created>2014-02-24</created><authors><author><keyname>Berger</keyname><forenames>Daniel S.</forenames></author><author><keyname>Gland</keyname><forenames>Philipp</forenames></author><author><keyname>Singla</keyname><forenames>Sahil</forenames></author><author><keyname>Ciucu</keyname><forenames>Florin</forenames></author></authors><title>Exact Analysis of TTL Cache Networks: The Case of Caching Policies
  driven by Stopping Times</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TTL caching models have recently regained significant research interest,
largely due to their ability to fit popular caching policies such as LRU. This
paper advances the state-of-the-art analysis of TTL-based cache networks by
developing two exact methods with orthogonal generality and computational
complexity. The first method generalizes existing results for line networks
under renewal requests to the broad class of caching policies whereby evictions
are driven by stopping times. The obtained results are further generalized,
using the second method, to feedforward networks with Markov arrival processes
(MAP) requests. MAPs are particularly suitable for non-line networks because
they are closed not only under superposition and splitting, as known, but also
under input-output caching operations as proven herein for phase-type TTL
distributions. The crucial benefit of the two closure properties is that they
jointly enable the first exact analysis of feedforward networks of TTL caches
in great generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5988</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5988</id><created>2014-02-24</created><updated>2014-11-22</updated><authors><author><keyname>Katzouris</keyname><forenames>Nikos</forenames></author><author><keyname>Artikis</keyname><forenames>Alexander</forenames></author><author><keyname>Paliouras</keyname><forenames>George</forenames></author></authors><title>Incremental Learning of Event Definitions with Inductive Logic
  Programming</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event recognition systems rely on properly engineered knowledge bases of
event definitions to infer occurrences of events in time. The manual
development of such knowledge is a tedious and error-prone task, thus
event-based applications may benefit from automated knowledge construction
techniques, such as Inductive Logic Programming (ILP), which combines machine
learning with the declarative and formal semantics of First-Order Logic.
However, learning temporal logical formalisms, which are typically utilized by
logic-based Event Recognition systems is a challenging task, which most ILP
systems cannot fully undertake. In addition, event-based data is usually
massive and collected at different times and under various circumstances.
Ideally, systems that learn from temporal data should be able to operate in an
incremental mode, that is, revise prior constructed knowledge in the face of
new evidence. Most ILP systems are batch learners, in the sense that in order
to account for new evidence they have no alternative but to forget past
knowledge and learn from scratch. Given the increased inherent complexity of
ILP and the volumes of real-life temporal data, this results to algorithms that
scale poorly. In this work we present an incremental method for learning and
revising event-based knowledge, in the form of Event Calculus programs. The
proposed algorithm relies on abductive-inductive learning and comprises a
scalable clause refinement methodology, based on a compressive summarization of
clause coverage in a stream of examples. We present an empirical evaluation of
our approach on real and synthetic data from activity recognition and city
transport applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5991</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5991</id><created>2014-02-24</created><updated>2014-03-12</updated><authors><author><keyname>Shams</keyname><forenames>Issac</forenames></author><author><keyname>Ajorlou</keyname><forenames>Saeede</forenames></author><author><keyname>Yang</keyname><forenames>Kai</forenames></author></authors><title>A predictive analytics approach to reducing avoidable hospital
  readmission</title><categories>stat.AP cs.AI</categories><comments>30 pages, 4 figures, 7 tables</comments><msc-class>60J28, 62P10, 68T05, 62G86, 90B22</msc-class><acm-class>I.2.1; I.2.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hospital readmission has become a critical metric of quality and cost of
healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20%
of patients who are readmitted within 30 days of discharge. Although several
interventions such as transition care management and discharge reengineering
have been practiced in recent years, the effectiveness and sustainability
depends on how well they can identify and target patients at high risk of
rehospitalization. Based on the literature, most current risk prediction models
fail to reach an acceptable accuracy level; none of them considers patient's
history of readmission and impacts of patient attribute changes over time; and
they often do not discriminate between planned and unnecessary readmissions.
Tackling such drawbacks, we develop a new readmission metric based on
administrative data that can identify potentially avoidable readmissions from
all other types of readmission. We further propose a tree based classification
method to estimate the predicted probability of readmission that can directly
incorporate patient's history of readmission and risk factors changes over
time. The proposed methods are validated with 2011-12 Veterans Health
Administration data from inpatients hospitalized for heart failure, acute
myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in
the State of Michigan. Results shows improved discrimination power compared to
the literature (c-statistics&gt;80%) and good calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.5992</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.5992</id><created>2014-02-24</created><updated>2015-04-23</updated><authors><author><keyname>&#x15e;tef&#x103;nescu</keyname><forenames>R&#x103;zvan</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Navon</keyname><forenames>Ionel Michael</forenames></author></authors><title>POD/DEIM Reduced-Order Strategies for Efficient Four Dimensional
  Variational Data Assimilation</title><categories>cs.SY math.NA</categories><comments>49 pages, 7 figures</comments><report-no>CSTR-17/2015</report-no><doi>10.1016/j.jcp.2015.04.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies reduced order modeling (ROM) approaches to speed up the
solution of variational data assimilation problems with large scale nonlinear
dynamical models. It is shown that a key requirement for a successful reduced
order solution is that reduced order Karush-Kuhn-Tucker conditions accurately
represent their full order counterparts. In particular, accurate reduced order
approximations are needed for the forward and adjoint dynamical models, as well
as for the reduced gradient. New strategies to construct reduced order based
are developed for Proper Orthogonal Decomposition (POD) ROM data assimilation
using both Galerkin and Petrov-Galerkin projections. For the first time POD,
tensorial POD, and discrete empirical interpolation method (DEIM) are employed
to develop reduced data assimilation systems for a geophysical flow model,
namely, the two dimensional shallow water equations. Numerical experiments
confirm the theoretical framework for Galerkin projection. In the case of
Petrov-Galerkin projection, stabilization strategies must be considered for the
reduced order models. The new reduced order shallow water data assimilation
system provides analyses similar to those produced by the full resolution data
assimilation system in one tenth of the computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6005</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6005</id><created>2014-02-24</created><authors><author><keyname>Valderrama-Cuervo</keyname><forenames>Juan Camilo</forenames></author><author><keyname>L&#xf3;pez-Parrado</keyname><forenames>Alexander</forenames></author></authors><title>Open Cores for Digital Signal Processing</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the design and implementation of three System on Chip
(SoC) cores, which implement the Digital Signal Processing (DSP) functions:
Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter
and Fast Fourier Transform (FFT). The FIR filter core is based on the
symmetrical realization form, the IIR filter core is based on the Second Order
Sections (SOS) architecture and the FFT core is based on the Radix $2^2$ Single
Delay Feedback (R$2^2$SDF) architecture. The three cores are compatible with
the Wishbone SoC bus and they were described using generic and structural VHDL.
In system hardware verification was performed by using an OpenRisc-based SoC
synthesized on an Altera FPGA, the tests showed that the designed DSP cores are
suitable for building SoC based on the OpenRisc processor and the Wishbone bus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6010</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6010</id><created>2014-02-24</created><updated>2014-06-12</updated><authors><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social
  Media</title><categories>cs.SI cs.CL cs.IR</categories><comments>A short version is in Proceeding of the 2014 ACM SIGMOD International
  Conference on Management of data</comments><doi>10.1145/2588555.2593682</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing popularity of social media (e.g, Twitter) allows users to easily
share information with each other and influence others by expressing their own
sentiments on various subjects. In this work, we propose an unsupervised
\emph{tri-clustering} framework, which analyzes both user-level and tweet-level
sentiments through co-clustering of a tripartite graph. A compelling feature of
the proposed framework is that the quality of sentiment clustering of tweets,
users, and features can be mutually improved by joint clustering. We further
investigate the evolution of user-level sentiments and latent feature vectors
in an online framework and devise an efficient online algorithm to sequentially
update the clustering of tweets, users and features with newly arrived data.
The online framework not only provides better quality of both dynamic
user-level and tweet-level sentiment analysis, but also improves the
computational and storage efficiency. We verified the effectiveness and
efficiency of the proposed approaches on the November 2012 California ballot
Twitter data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6013</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6013</id><created>2014-02-24</created><authors><author><keyname>Vanschoren</keyname><forenames>Joaquin</forenames></author><author><keyname>Braun</keyname><forenames>Mikio L.</forenames></author><author><keyname>Ong</keyname><forenames>Cheng Soon</forenames></author></authors><title>Open science in machine learning</title><categories>cs.LG cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present OpenML and mldata, open science platforms that provides easy
access to machine learning data, software and results to encourage further
study and application. They go beyond the more traditional repositories for
data sets and software packages in that they allow researchers to also easily
share the results they obtained in experiments and to compare their solutions
with those of others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6016</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6016</id><created>2014-02-24</created><updated>2014-07-14</updated><authors><author><keyname>Arslan</keyname><forenames>Suayb S.</forenames></author></authors><title>Incremental Redundancy, Fountain Codes and Advanced Topics</title><categories>cs.IT math.IT</categories><comments>57 pages, 22 figures, Version 0.2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document is written in order to establish a common base ground on which
the majority of the relevant research about linear fountain codes can be
analyzed and compared. As far as I am concerned, there is no unified approach
that outlines and compares most of the published linear fountain codes in a
single and self-contained framework. This written document has not only
resulted in the review of theoretical fundamentals of efficient coding
techniques for incremental redundancy and linear fountain coding, but also
helped me have a comprehensive reference document and hopefully for many other
graduate students who would like to have some background to pursue a research
career regarding fountain codes and their various applications. Some background
in information, coding, graph and probability theory is expected. Although
various aspects of this topic and many other relevant research are deliberately
left out, I still hope that this document shall serve researchers' need well. I
have also included several exercises to warm up. The presentation style is
usually informal and the presented material is not necessarily rigorous. There
are many spots in the text that are product of my coauthors and myself,
although some of which have not been published yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6028</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6028</id><created>2014-02-24</created><authors><author><keyname>Kuleshov</keyname><forenames>Volodymyr</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Algorithms for multi-armed bandit problems</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although many algorithms for the multi-armed bandit problem are
well-understood theoretically, empirical confirmation of their effectiveness is
generally scarce. This paper presents a thorough empirical study of the most
popular multi-armed bandit algorithms. Three important observations can be made
from our results. Firstly, simple heuristics such as epsilon-greedy and
Boltzmann exploration outperform theoretically sound algorithms on most
settings by a significant margin. Secondly, the performance of most algorithms
varies dramatically with the parameters of the bandit problem. Our study
identifies for each algorithm the settings where it performs well, and the
settings where it performs poorly. Thirdly, the algorithms' performance
relative each to other is affected only by the number of bandit arms and the
variance of the rewards. This finding may guide the design of subsequent
empirical evaluations. In the second part of the paper, we turn our attention
to an important area of application of bandit algorithms: clinical trials.
Although the design of clinical trials has been one of the principal practical
problems motivating research on multi-armed bandits, bandit algorithms have
never been evaluated as potential treatment allocation strategies. Using data
from a real study, we simulate the outcome that a 2001-2002 clinical trial
would have had if bandit algorithms had been used to allocate patients to
treatments. We find that an adaptive trial would have successfully treated at
least 50% more patients, while significantly reducing the number of adverse
effects and increasing patient retention. At the end of the trial, the best
treatment could have still been identified with a high level of statistical
confidence. Our findings demonstrate that bandit algorithms are attractive
alternatives to current adaptive treatment allocation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6034</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6034</id><created>2014-02-24</created><authors><author><keyname>Cintra</keyname><forenames>R. J.</forenames></author><author><keyname>Bayer</keyname><forenames>F. M.</forenames></author></authors><title>A DCT Approximation for Image Compression</title><categories>cs.MM cs.CV stat.ME</categories><comments>10 pages, 6 figures</comments><journal-ref>IEEE Signal Processing Letters, 18(10):579-582, October 2011</journal-ref><doi>10.1109/LSP.2011.2163394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An orthogonal approximation for the 8-point discrete cosine transform (DCT)
is introduced. The proposed transformation matrix contains only zeros and ones;
multiplications and bit-shift operations are absent. Close spectral behavior
relative to the DCT was adopted as design criterion. The proposed algorithm is
superior to the signed discrete cosine transform. It could also outperform
state-of-the-art algorithms in low and high image compression scenarios,
exhibiting at the same time a comparable computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6044</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6044</id><created>2014-02-24</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author></authors><title>Generalized Nonlinear Robust Energy-to-Peak Filtering for Differential
  Algebraic Systems</title><categories>cs.SY math.OC</categories><comments>16 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1402.5511</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of robust nonlinear energy-to-peak filtering for nonlinear
descriptor systems with model uncertainties is addressed. The system is assumed
to have nonlinearities both in the state and output equations as well as
norm-bounded time-varying uncertainties in the realization matrices. A
generalized nonlinear dynamic filtering structure is proposed for such a class
of systems with more degrees of freedom than the conventional static-gain and
dynamic filtering structures. The L2-Linfty filter is synthesized through
semidefinite programming and strict LMIs, in which the energy-to-peak filtering
performance in optimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6045</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6045</id><created>2014-02-24</created><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Multi-Dimensional Customization Modelling Based On Metagraph For Saas
  Multi-Tenant Applications</title><categories>cs.SE</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software as a Service (SaaS) is a new software delivery model in which
pre-built applications are delivered to customers as a service. SaaS providers
aim to attract a large number of tenants (users) with minimal system
modifications to meet economics of scale. To achieve this aim, SaaS
applications have to be customizable to meet requirements of each tenant.
However, due to the rapid growing of the SaaS, SaaS applications could have
thousands of tenants with a huge number of ways to customize applications.
Modularizing such customizations still is a highly complex task. Additionally,
due to the big variation of requirements for tenants, no single customization
model is appropriate for all tenants. In this paper, we propose a
multi-dimensional customization model based on metagraph. The proposed mode
addresses the modelling variability among tenants, describes customizations and
their relationships, and guarantees the correctness of SaaS customizations made
by tenants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6046</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6046</id><created>2014-02-24</created><authors><author><keyname>Dam</keyname><forenames>Hoa Khanh</forenames></author><author><keyname>Ghose</keyname><forenames>Aditya</forenames></author></authors><title>Towards rational and minimal change propagation in model evolution</title><categories>cs.SE</categories><acm-class>D.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A critical issue in the evolution of software models is change propagation:
given a primary change that is made to a model in order to meet a new or
changed requirement, what additional secondary changes are needed to maintain
consistency within the model, and between the model and other models in the
system? In practice, there are many ways of propagating changes to fix a given
inconsistency, and how to justify and automate the selection between such
change options remains a critical challenge. In this paper, we propose a number
of postulates, inspired by the mature belief revision theory, that a change
propagation process should satisfy to be considered rational and minimal. Such
postulates enable us to reason about selecting alternative change options, and
consequently to develop a machinery that automatically performs this task. We
further argue that a possible implementation of such a change propagation
process can be considered as a classical state space search in which each state
represents a snapshot of the model in the process. This view naturally reflects
the cascading nature of change propagation, where each change can require
further changes to be made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6050</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6050</id><created>2014-02-24</created><authors><author><keyname>Ariyan</keyname><forenames>Saurabh Kumar</forenames></author><author><keyname>Bagela</keyname><forenames>Eshant</forenames></author><author><keyname>Priyadarshin</keyname><forenames>Akanksha</forenames></author></authors><title>Abiot: A Low cost agile sonic pest control tricopter</title><categories>cs.RO</categories><comments>4 pages; workshop paper</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we introduce the concept of an agile electronic pest control
intelligent device for commercial usage and we have evaluated its performance
in comparison with other existing similar technologies. The frequency and
intensities are changed with respect to the target pest however human behavior
has been found to be inert with their exposure. The unit has been tested in lab
conditions as well as field testing done have given encouraging results. The
device can be a standalone unit and hence work for small scale viz. kitchen
garden on the other hand multiple devices acting in coordination with each
other give the desired output on a larger scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6065</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6065</id><created>2014-02-25</created><updated>2014-09-10</updated><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author></authors><title>Multi-Agent Distributed Optimization via Inexact Consensus ADMM</title><categories>cs.SY math.OC</categories><comments>submitted to IEEE Trans. Signal Processing; Revised April 2014 and
  August 2014</comments><doi>10.1109/TSP.2014.2367458</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent distributed consensus optimization problems arise in many signal
processing applications. Recently, the alternating direction method of
multipliers (ADMM) has been used for solving this family of problems. ADMM
based distributed optimization method is shown to have faster convergence rate
compared with classic methods based on consensus subgradient, but can be
computationally expensive, especially for problems with complicated structures
or large dimensions. In this paper, we propose low-complexity algorithms that
can reduce the overall computational cost of consensus ADMM by an order of
magnitude for certain large-scale problems. Central to the proposed algorithms
is the use of an inexact step for each ADMM update, which enables the agents to
perform cheap computation at each iteration. Our convergence analyses show that
the proposed methods converge well under some convexity assumptions. Numerical
results show that the proposed algorithms offer considerably lower
computational complexity than the standard ADMM based distributed optimization
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6067</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6067</id><created>2014-02-25</created><authors><author><keyname>Wu</keyname><forenames>Zhilin</forenames></author></authors><title>Regular path queries on graphs with data: A rigid approach</title><categories>cs.LO cs.DB cs.FL</categories><comments>25 pages, 2 figures</comments><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular path queries (RPQ) is a classical navigational query formalism for
graph databases to specify constraints on labeled paths. Recently, RPQs have
been extended by Libkin and Vrgo$\rm \check{c}$ to incorporate data value
comparisons among different nodes on paths, called regular path queries with
data (RDPQ). It has been shown that the evaluation problem of RDPQs is
PSPACE-complete and NLOGSPACE-complete in data complexity. On the other hand,
the containment problem of RDPQs is in general undecidable. In this paper, we
propose a novel approach to extend regular path queries with data value
comparisons, called rigid regular path queries with data (RRDPQ). The main
ingredient of this approach is an automata model called nondeterministic rigid
register automata (NRRA), in which the data value comparisons are \emph{rigid},
in the sense that if the data value in the current position $x$ is compared to
a data value in some other position $y$, then by only using the labels (but not
data values), the position $y$ can be uniquely determined from $x$. We show
that NRRAs are robust in the sense that nondeterministic, deterministic and
two-way variant of NRRAs, as well as an extension of regular expressions, are
all of the same expressivity. We then argue that the expressive power of RDPQs
are reasonable by demonstrating that for every graph database, there is a
localized transformation of the graph database so that every RDPQ in the
original graph database can be turned into an equivalent RRDPQ over the
transformed one. Finally, we investigate the computational properties of RRDPQs
and conjunctive RRDPQs (CRRDPQ). In particular, we show that the containment of
CRRDPQs (and RRDPQs) can be decided in 2EXPSPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6076</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6076</id><created>2014-02-25</created><authors><author><keyname>Izrailev</keyname><forenames>Sergei</forenames></author><author><keyname>Stanley</keyname><forenames>Jeremy M.</forenames></author></authors><title>Machine Learning at Scale</title><categories>cs.LG cs.MS stat.ML</categories><comments>Submitted to KDD'14</comments><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It takes skill to build a meaningful predictive model even with the abundance
of implementations of modern machine learning algorithms and readily available
computing resources. Building a model becomes challenging if hundreds of
terabytes of data need to be processed to produce the training data set. In a
digital advertising technology setting, we are faced with the need to build
thousands of such models that predict user behavior and power advertising
campaigns in a 24/7 chaotic real-time production environment. As data
scientists, we also have to convince other internal departments critical to
implementation success, our management, and our customers that our machine
learning system works. In this paper, we present the details of the design and
implementation of an automated, robust machine learning platform that impacts
billions of advertising impressions monthly. This platform enables us to
continuously optimize thousands of campaigns over hundreds of millions of
users, on multiple continents, against varying performance objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6077</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6077</id><created>2014-02-25</created><authors><author><keyname>Dai</keyname><forenames>Wang-Zhou</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Inductive Logic Boosting</title><categories>cs.LG cs.AI</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen a surge of interest in Probabilistic Logic Programming
(PLP) and Statistical Relational Learning (SRL) models that combine logic with
probabilities. Structure learning of these systems is an intersection area of
Inductive Logic Programming (ILP) and statistical learning (SL). However, ILP
cannot deal with probabilities, SL cannot model relational hypothesis. The
biggest challenge of integrating these two machine learning frameworks is how
to estimate the probability of a logic clause only from the observation of
grounded logic atoms. Many current methods models a joint probability by
representing clause as graphical model and literals as vertices in it. This
model is still too complicate and only can be approximate by pseudo-likelihood.
We propose Inductive Logic Boosting framework to transform the relational
dataset into a feature-based dataset, induces logic rules by boosting Problog
Rule Trees and relaxes the independence constraint of pseudo-likelihood.
Experimental evaluation on benchmark datasets demonstrates that the AUC-PR and
AUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6081</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6081</id><created>2014-02-25</created><updated>2014-11-07</updated><authors><author><keyname>Liska</keyname><forenames>Sebastian</forenames></author><author><keyname>Colonius</keyname><forenames>Tim</forenames></author></authors><title>A parallel fast multipole method for elliptic difference equations</title><categories>physics.comp-ph cs.NA</categories><comments>47 pages, 4 figures, accepted by Journal of Computational Physics</comments><journal-ref>Journal of Computational Physics 278 (2014), 76-91</journal-ref><doi>10.1016/j.jcp.2014.07.048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new fast multipole formulation for solving elliptic difference equations on
unbounded domains and its parallel implementation are presented. These
difference equations can arise directly in the description of physical systems,
e.g. crystal structures, or indirectly through the discretization of PDEs. In
the analog to solving continuous inhomogeneous differential equations using
Green's functions, the proposed method uses the fundamental solution of the
discrete operator on an infinite grid, or lattice Green's function. Fast
solutions $\mathcal{O}(N)$ are achieved by using a kernel-independent
interpolation-based fast multipole method. Unlike other fast multipole
algorithms, our approach exploits the regularity of the underlying Cartesian
grid and the efficiency of FFTs to reduce the computation time. Our parallel
implementation allows communications and computations to be overlapped and
requires minimal global synchronization. The accuracy, efficiency, and parallel
performance of the method are demonstrated through numerical experiments on the
discrete 3D Poisson equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6083</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6083</id><created>2014-02-25</created><updated>2014-10-13</updated><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Syrj&#xe4;l&#xe4;</keyname><forenames>Ville</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Widely-Linear Digital Self-Interference Cancellation in
  Direct-Conversion Full-Duplex Transceiver</title><categories>cs.IT math.IT</categories><comments>IEEE Journal on Selected Areas in Communications, vol. 32, no. 9, pp.
  1674-1687, September 2014</comments><doi>10.1109/JSAC.2014.2330093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the modeling and cancellation of self-interference in
full-duplex direct-conversion radio transceivers, operating under practical
imperfect radio frequency (RF) components. Firstly, detailed self-interference
signal modeling is carried out, taking into account the most important RF
imperfections, namely transmitter power amplifier nonlinear distortion as well
as transmitter and receiver IQ mixer amplitude and phase imbalances. The
analysis shows that after realistic antenna isolation and RF cancellation, the
dominant self-interference waveform at receiver digital baseband can be modeled
through a widely-linear transformation of the original transmit data, opposed
to classical purely linear models. Such widely-linear self-interference
waveform is physically stemming from the transmitter and receiver IQ imaging,
and cannot be efficiently suppressed by classical linear digital cancellation.
Motivated by this, novel widely-linear digital self-interference cancellation
processing is then proposed and formulated, combined with efficient parameter
estimation methods. Extensive simulation results demonstrate that the proposed
widely-linear cancellation processing clearly outperforms the existing linear
solutions, hence enabling the use of practical low-cost RF front-ends utilizing
IQ mixing in full-duplex transceivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6096</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6096</id><created>2014-02-25</created><authors><author><keyname>Aschner</keyname><forenames>Rom</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author></authors><title>Bounded-Angle Spanning Tree: Modeling Networks with Angular Constraints</title><categories>cs.CG cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new structure for a set of points in the plane and an angle
$\alpha$, which is similar in flavor to a bounded-degree MST. We name this
structure $\alpha$-MST. Let $P$ be a set of points in the plane and let $0 &lt;
\alpha \le 2\pi$ be an angle. An $\alpha$-ST of $P$ is a spanning tree of the
complete Euclidean graph induced by $P$, with the additional property that for
each point $p \in P$, the smallest angle around $p$ containing all the edges
adjacent to $p$ is at most $\alpha$. An $\alpha$-MST of $P$ is then an
$\alpha$-ST of $P$ of minimum weight. For $\alpha &lt; \pi/3$, an $\alpha$-ST does
not always exist, and, for $\alpha \ge \pi/3$, it always exists. In this paper,
we study the problem of computing an $\alpha$-MST for several common values of
$\alpha$.
  Motivated by wireless networks, we formulate the problem in terms of
directional antennas. With each point $p \in P$, we associate a wedge $W_p$ of
angle $\alpha$ and apex $p$. The goal is to assign an orientation and a radius
$r_p$ to each wedge $W_p$, such that the resulting graph is connected and its
MST is an $\alpha$-MST. (We draw an edge between $p$ and $q$ if $p \in W_q$, $q
\in W_p$, and $|pq| \le r_p, r_q$.) Unsurprisingly, the problem of computing an
$\alpha$-MST is NP-hard, at least for $\alpha=\pi$ and $\alpha=2\pi/3$. We
present constant-factor approximation algorithms for $\alpha = \pi/2, 2\pi/3,
\pi$.
  One of our major results is a surprising theorem for $\alpha = 2\pi/3$,
which, besides being interesting from a geometric point of view, has important
applications. For example, the theorem guarantees that given any set $P$ of
$3n$ points in the plane and any partitioning of the points into $n$ triplets,
one can orient the wedges of each triplet {\em independently}, such that the
graph induced by $P$ is connected. We apply the theorem to the {\em antenna
conversion} problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6109</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6109</id><created>2014-02-25</created><authors><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>The Complexity of Repairing, Adjusting, and Aggregating of Extensions in
  Abstract Argumentation</title><categories>cs.DS cs.AI</categories><journal-ref>Proc. TAFA 2013, pp. 158-175, Springer LNCS</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of problems that arise in abstract
argumentation in the context of dynamic argumentation, minimal change, and
aggregation. In particular, we consider the following problems where always an
argumentation framework F and a small positive integer k are given.
  - The Repair problem asks whether a given set of arguments can be modified
into an extension by at most k elementary changes (i.e., the extension is of
distance k from the given set).
  - The Adjust problem asks whether a given extension can be modified by at
most k elementary changes into an extension that contains a specified argument.
  - The Center problem asks whether, given two extensions of distance k,
whether there is a &quot;center&quot; extension that is a distance at most (k-1) from
both given extensions.
  We study these problems in the framework of parameterized complexity, and
take the distance k as the parameter. Our results covers several different
semantics, including admissible, complete, preferred, semi-stable and stable
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6114</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6114</id><created>2014-02-25</created><authors><author><keyname>Fioriti</keyname><forenames>Vincenzo</forenames></author><author><keyname>Chinnici</keyname><forenames>Marta</forenames></author></authors><title>Node seniority ranking</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in graph theory suggest that is possible to identify the
oldest nodes of a network using only the graph topology. Here we report on
applications to heterogeneous real world networks. To this end, and in order to
gain new insights, we propose the theoretical framework of the Estrada
communicability. We apply it to two technological networks (an underground, the
diffusion of a software worm in a LAN) and to a third network representing a
cholera outbreak. In spite of errors introduced in the adjacency matrix of
their graphs, the identification of the oldest nodes is feasible, within a
small margin of error, and extremely simple. Utilizations include the search of
the initial disease-spreader (patient zero problem), rumors in social networks,
malware in computer networks, triggering events in blackouts, oldest urban
sites recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6124</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6124</id><created>2014-02-25</created><authors><author><keyname>Holohan</keyname><forenames>Naoise</forenames></author><author><keyname>Leith</keyname><forenames>Douglas</forenames></author><author><keyname>Mason</keyname><forenames>Oliver</forenames></author></authors><title>Differential Privacy in Metric Spaces: Numerical, Categorical and
  Functional Data Under the One Roof</title><categories>cs.DB cs.IT math.IT math.PR</categories><comments>18 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Differential Privacy in the abstract setting of Probability on
metric spaces. Numerical, categorical and functional data can be handled in a
uniform manner in this setting. We demonstrate how mechanisms based on data
sanitisation and those that rely on adding noise to query responses fit within
this framework. We prove that once the sanitisation is differentially private,
then so is the query response for any query. We show how to construct
sanitisations for high-dimensional databases using simple 1-dimensional
mechanisms. We also provide lower bounds on the expected error for
differentially private sanitisations in the general metric space setting.
Finally, we consider the question of sufficient sets for differential privacy
and show that for relaxed differential privacy, any algebra generating the
Borel $\sigma$-algebra is a sufficient set for relaxed differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6132</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6132</id><created>2014-02-25</created><authors><author><keyname>Zeng</keyname><forenames>Wei</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Liu</keyname><forenames>Hao</forenames></author><author><keyname>Shang</keyname><forenames>Ming-Sheng</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Uncovering the information core in recommender systems</title><categories>cs.IR</categories><comments>14pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of the Internet and overwhelming amount of information
that people are confronted with, recommender systems have been developed to
effiectively support users' decision-making process in online systems. So far,
much attention has been paid to designing new recommendation algorithms and
improving existent ones. However, few works considered the different
contributions from different users to the performance of a recommender system.
Such studies can help us improve the recommendation efficiency by excluding
irrelevant users. In this paper, we argue that in each online system there
exists a group of core users who carry most of the information for
recommendation. With them, the recommender systems can already generate
satisfactory recommendation. Our core user extraction method enables the
recommender systems to achieve 90% of the accuracy by taking only 20% of the
data into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6133</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6133</id><created>2014-02-25</created><authors><author><keyname>Sahu</keyname><forenames>Siddhant</forenames></author><author><keyname>Sugumaran</keyname><forenames>V.</forenames></author></authors><title>Bayesian Sample Size Determination of Vibration Signals in Machine
  Learning Approach to Fault Diagnosis of Roller Bearings</title><categories>stat.ML cs.LG</categories><comments>14 pages, 1 table, 6 figures</comments><journal-ref>Intentional Journal of Research in Mechanical Engineering, Volume
  1, Issue 1, July-September, 2013, pp. 55-63, IASTER</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sample size determination for a data set is an important statistical process
for analyzing the data to an optimum level of accuracy and using minimum
computational work. The applications of this process are credible in every
domain which deals with large data sets and high computational work. This study
uses Bayesian analysis for determination of minimum sample size of vibration
signals to be considered for fault diagnosis of a bearing using pre-defined
parameters such as the inverse standard probability and the acceptable margin
of error. Thus an analytical formula for sample size determination is
introduced. The fault diagnosis of the bearing is done using a machine learning
approach using an entropy-based J48 algorithm. The following method will help
researchers involved in fault diagnosis to determine minimum sample size of
data for analysis for a good statistical stability and precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6136</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6136</id><created>2014-02-25</created><authors><author><keyname>Kehagias</keyname><forenames>Athanasios</forenames></author><author><keyname>Mitsche</keyname><forenames>Dieter</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author></authors><title>The Role of Visibility in Pursuit / Evasion Games</title><categories>cs.DM</categories><msc-class>91A80, 97K30, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cops-and-robber (CR) game has been used in mobile robotics as a
discretized model (played on a graph G) of pursuit/evasion problems. The
&quot;classic&quot; CR version is a perfect information game: the cops' (pursuer's)
location is always known to the robber (evader) and vice versa. Many variants
of the classic game can be defined: the robber can be invisible and also the
robber can be either adversarial (tries to avoid capture) or drunk (performs a
random walk). Furthermore, the cops and robber can reside in either nodes or
edges of G. Several of these variants are relevant as models or robotic pursuit
/ evasion. In this paper, we first define carefully several of the variants
mentioned above and related quantities such as the cop number and the capture
time. Then we introduce and study the cost of visibility (COV), a quantitative
measure of the increase in difficulty (from the cops' point of view) when the
robber is invisible. In addition to our theoretical results, we present
algorithms which can be used to compute capture times and COV of graphs which
are analytically intractable. Finally, we present the results of applying these
algorithms to the numerical computation of COV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6138</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6138</id><created>2014-02-25</created><updated>2015-08-17</updated><authors><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Garimella</keyname><forenames>Kiran</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author><author><keyname>Tsang</keyname><forenames>Dominic</forenames></author></authors><title>Discovering the Network Backbone from Traffic Activity Data</title><categories>cs.SI</categories><comments>Submitted for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new computational problem, the BackboneDiscovery problem,
which encapsulates both functional and structural aspects of network analysis.
  While the topology of a typical road network has been available for a long
time (e.g., through maps), it is only recently that fine-granularity functional
(activity and usage) information about the network (like source-destination
traffic information) is being collected and is readily available. The
combination of functional and structural information provides an efficient way
to explore and understand usage patterns of networks and aid in design and
decision making. We propose efficient algorithms for the BackboneDiscovery
problem including a novel use of edge centrality. We observe that for many real
world networks, our algorithm produces a backbone with a small subset of the
edges that support a large percentage of the network activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6148</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6148</id><created>2014-02-25</created><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Devillers</keyname><forenames>Olivier</forenames></author><author><keyname>Hemsley</keyname><forenames>Ross</forenames></author></authors><title>Efficiently navigating a random Delaunay triangulation</title><categories>math.PR cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planar graph navigation is an important problem with significant implications
to both point location in geometric data structures and routing in networks.
However, whilst a number of algorithms and existence proofs have been proposed,
very little analysis is available for the properties of the paths generated and
the computational resources required to generate them under a random
distribution hypothesis for the input. In this paper we analyse a new
deterministic planar navigation algorithm with constant competitiveness which
follows vertex adjacencies in the Delaunay triangulation. We call this strategy
cone walk. We prove that given $n$ uniform points in a smooth convex domain of
unit area, and for any start point $z$ and query point $q$; cone walk applied
to $z$ and $q$ will access at most $O(|zq|\sqrt{n} +\log^7 n)$ sites with
complexity $O(|zq|\sqrt{n} \log \log n + \log^7 n)$ with probability tending to
1 as $n$ goes to infinity. We additionally show that in this model, cone walk
is $(\log ^{3+\xi} n)$-memoryless with high probability for any pair of start
and query point in the domain, for any positive $\xi$. We take special care
throughout to ensure our bounds are valid even when the query points are
arbitrarily close to the border.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6190</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6190</id><created>2014-02-25</created><updated>2014-04-29</updated><authors><author><keyname>Dudek</keyname><forenames>Andrzej</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Ruci&#x144;ski</keyname><forenames>Andrzej</forenames></author><author><keyname>Szyma&#x144;ska</keyname><forenames>Edyta</forenames></author></authors><title>Approximate Counting of Matchings in $(3,3)$-Hypergraphs</title><categories>math.CO cs.DS</categories><comments>13 pp, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a fully polynomial time approximation scheme (FPTAS) for counting
the number of matchings (packings) in arbitrary 3-uniform hypergraphs of
maximum degree three, referred to as $(3,3)$-hypergraphs. It is the first
polynomial time approximation scheme for that problem, which includes also, as
a special case, the 3D Matching counting problem for 3-partite
$(3,3)$-hypergraphs. The proof technique of this paper uses the general
correlation decay technique and a new combinatorial analysis of the underlying
structures of the intersection graphs. The proof method could be also of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6208</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6208</id><created>2014-02-25</created><authors><author><keyname>Flaounas</keyname><forenames>Ilias</forenames></author><author><keyname>Lansdall-Welfare</keyname><forenames>Thomas</forenames></author><author><keyname>Antonakaki</keyname><forenames>Panagiota</forenames></author><author><keyname>Cristianini</keyname><forenames>Nello</forenames></author></authors><title>The Anatomy of a Modular System for Media Content Analysis</title><categories>cs.MA cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent systems for the annotation of media content are increasingly
being used for the automation of parts of social science research. In this
domain the problem of integrating various Artificial Intelligence (AI)
algorithms into a single intelligent system arises spontaneously. As part of
our ongoing effort in automating media content analysis for the social
sciences, we have built a modular system by combining multiple AI modules into
a flexible framework in which they can cooperate in complex tasks. Our system
combines data gathering, machine translation, topic classification, extraction
and annotation of entities and social networks, as well as many other tasks
that have been perfected over the past years of AI research. Over the last few
years, it has allowed us to realise a series of scientific studies over a vast
range of applications including comparative studies between news outlets and
media content in different countries, modelling of user preferences, and
monitoring public mood. The framework is flexible and allows the design and
implementation of modular agents, where simple modules cooperate in the
annotation of a large dataset without central coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6219</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6219</id><created>2014-01-21</created><authors><author><keyname>Hegazy</keyname><forenames>Ola M.</forenames></author><author><keyname>Bahaa-Eldin</keyname><forenames>Ayman M.</forenames></author><author><keyname>Dakroury</keyname><forenames>Yasser H.</forenames></author></authors><title>Quantum Secure Direct Communication using Entanglement and Super Dense
  Coding</title><categories>quant-ph cs.CR</categories><comments>SECRYPT 2009. arXiv admin note: text overlap with
  arXiv:quant-ph/0612114 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new quantum protocol for secure direct communication.
This protocol is based on Entanglement and Super-Dense coding. In this paper we
present some basic definitions of entanglement in quantum mechanics, present
how to use the maximally entangled states known as Bell States, and super dense
coding technique to achieve secure direct message communication. Finally, we
will apply some error models that could affect the transmission of the quantum
data on the quantum channels, and how to treat these errors and acquire a safe
transmission of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6225</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6225</id><created>2014-02-25</created><updated>2015-02-03</updated><authors><author><keyname>Zhu</keyname><forenames>Xuzhen</forenames></author><author><keyname>Tian</keyname><forenames>Hui</forenames></author><author><keyname>Cai</keyname><forenames>Shimin</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Predicting missing links via significant paths</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 pages, 2 figures</comments><doi>10.1209/0295-5075/106/18008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction plays an important role in understanding intrinsic evolving
mechanisms of networks. With the belief that the likelihood of the existence of
a link between two nodes is strongly related with their similarity, many
methods have been proposed to calculate node similarity based on node
attributes and/or topological structures. Among a large variety of methods that
take into account paths connecting the target pair of nodes, most of which
neglect the heterogeneity of those paths. Our hypothesis is that a path
consisting of small-degree nodes provides a strong evidence of similarity
between two ends, accordingly, we propose a so-called sig- nificant path index
in this Letter to leverage intermediate nodes' degrees in similarity
calculation. Empirical experiments on twelve disparate real networks
demonstrate that the proposed index outperforms the mainstream link prediction
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6238</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6238</id><created>2014-02-25</created><authors><author><keyname>Wilson</keyname><forenames>Jobin</forenames></author><author><keyname>Chaudhury</keyname><forenames>Santanu</forenames></author><author><keyname>Lall</keyname><forenames>Brejesh</forenames></author><author><keyname>Kapadia</keyname><forenames>Prateek</forenames></author></authors><title>Improving Collaborative Filtering based Recommenders using Topic
  Modelling</title><categories>cs.IR cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard Collaborative Filtering (CF) algorithms make use of interactions
between users and items in the form of implicit or explicit ratings alone for
generating recommendations. Similarity among users or items is calculated
purely based on rating overlap in this case,without considering explicit
properties of users or items involved, limiting their applicability in domains
with very sparse rating spaces. In many domains such as movies, news or
electronic commerce recommenders, considerable contextual data in text form
describing item properties is available along with the rating data, which could
be utilized to improve recommendation quality.In this paper, we propose a novel
approach to improve standard CF based recommenders by utilizing latent
Dirichlet allocation (LDA) to learn latent properties of items, expressed in
terms of topic proportions, derived from their textual description. We infer
user's topic preferences or persona in the same latent space,based on her
historical ratings. While computing similarity between users, we make use of a
combined similarity measure involving rating overlap as well as similarity in
the latent topic space. This approach alleviates sparsity problem as it allows
calculation of similarity between users even if they have not rated any items
in common. Our experiments on multiple public datasets indicate that the
proposed hybrid approach significantly outperforms standard user Based and item
Based CF recommenders in terms of classification accuracy metrics such as
precision, recall and f-measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6239</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6239</id><created>2014-02-25</created><authors><author><keyname>Hartung</keyname><forenames>Sepp</forenames></author><author><keyname>Hoffmann</keyname><forenames>Clemens</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Improved Upper and Lower Bound Heuristics for Degree Anonymization in
  Social Networks</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a strongly growing interest in anonymizing social network data,
we investigate the NP-hard Degree Anonymization problem: given an undirected
graph, the task is to add a minimum number of edges such that the graph becomes
k-anonymous. That is, for each vertex there have to be at least k-1 other
vertices of exactly the same degree. The model of degree anonymization has been
introduced by Liu and Terzi [ACM SIGMOD'08], who also proposed and evaluated a
two-phase heuristic. We present an enhancement of this heuristic, including new
algorithms for each phase which significantly improve on the previously known
theoretical and practical running times. Moreover, our algorithms are optimized
for large-scale social networks and provide upper and lower bounds for the
optimal solution. Notably, on about 26 % of the real-world data we provide
(provably) optimal solutions; whereas in the other cases our upper bounds
significantly improve on known heuristic solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6243</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6243</id><created>2014-02-25</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Nasr</keyname><forenames>Omar A.</forenames></author></authors><title>Globally Optimal Cooperation in Dense Cognitive Radio Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The problem of calculating the local and global decision thresholds in hard
decisions based cooperative spectrum sensing is well known for its mathematical
intractability. Previous work relied on simple suboptimal counting rules for
decision fusion in order to avoid the exhaustive numerical search required for
obtaining the optimal thresholds. However, these simple rules are not globally
optimal as they do not maximize the overall global detection probability by
jointly selecting local and global thresholds. Instead, they maximize the
detection probability for a specific global threshold. In this paper, a
globally optimal decision fusion rule for Primary User signal detection based
on the Neyman- Pearson (NP) criterion is derived. The algorithm is based on a
novel representation for the global performance metrics in terms of the
regularized incomplete beta function. Based on this mathematical
representation, it is shown that the globally optimal NP hard decision fusion
test can be put in the form of a conventional one dimensional convex
optimization problem. A binary search for the global threshold can be applied
yielding a complexity of O(log2(N)), where N represents the number of
cooperating users. The logarithmic complexity is appreciated because we are
concerned with dense networks, and thus N is expected to be large. The proposed
optimal scheme outperforms conventional counting rules, such as the OR, AND,
and MAJORITY rules. It is shown via simulations that, although the optimal rule
tends to the simple OR rule when the number of cooperating secondary users is
small, it offers significant SNR gain in dense cognitive radio networks with
large number of cooperating users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6245</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6245</id><created>2014-02-24</created><authors><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Towards Provenance and Traceability in CRISTAL for HEP</title><categories>cs.SE physics.comp-ph physics.data-an</categories><comments>5 pages and 1 figure. 20th International Conference on Computing in
  High Energy and Nuclear Physics (CHEP13). 14-18th October 2013. Amsterdam,
  Netherlands. To appear in Journal of Physics Conference Series</comments><doi>10.1088/1742-6596/513/3/032091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the CRISTAL object lifecycle management system and its
use in provenance data management and the traceability of system events. This
software was initially used to capture the construction and calibration of the
CMS ECAL detector at CERN for later use by physicists in their data analysis.
Some further uses of CRISTAL in different projects (CMS, neuGRID and N4U) are
presented as examples of its flexible data model. From these examples,
applications are drawn for the High Energy Physics domain and some initial
ideas for its use in data preservation HEP are outlined in detail in this
paper. Currently investigations are underway to gauge the feasibility of using
the N4U Analysis Service or a derivative of it to address the requirements of
data and analysis logging and provenance capture within the HEP long term data
analysis environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6246</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6246</id><created>2014-01-22</created><updated>2016-01-03</updated><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>An experimental exploration of Marsaglia's xorshift generators,
  scrambled</title><categories>cs.DS cs.CR cs.MS</categories><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marsaglia proposed recently xorshift generators as a class of very fast,
good-quality pseudorandom number generators. Subsequent analysis by Panneton
and L'Ecuyer has lowered the expectations raised by Marsaglia's paper, showing
several weaknesses of such generators, verified experimentally using the
TestU01 suite. Nonetheless, many of the weaknesses of xorshift generators fade
away if their result is scrambled by a non-linear operation (as originally
suggested by Marsaglia). In this paper we explore the space of possible
generators obtained by multiplying the result of a xorshift generator by a
suitable constant. We sample generators at 100 equispaced points of their state
space and obtain detailed statistics that lead us to choices of parameters that
improve on the current ones. We then explore for the first time the space of
high-dimensional xorshift generators, following another suggestion in
Marsaglia's paper, finding choices of parameters providing periods of length
$2^{1024} - 1$ and $2^{4096} - 1$. The resulting generators are of extremely
high quality, faster than current similar alternatives, and generate
long-period sequences passing strong statistical tests using only eight logical
operations, one addition and one multiplication by a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6273</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6273</id><created>2014-02-25</created><updated>2014-04-25</updated><authors><author><keyname>Askalidis</keyname><forenames>Georgios</forenames></author><author><keyname>Berry</keyname><forenames>Randall A.</forenames></author><author><keyname>Subramanian</keyname><forenames>Vijay G.</forenames></author></authors><title>Explaining Snapshots of Network Diffusions: Structural and Hardness
  Results</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much research has been done on studying the diffusion of ideas or
technologies on social networks including the \textit{Influence Maximization}
problem and many of its variations. Here, we investigate a type of inverse
problem. Given a snapshot of the diffusion process, we seek to understand if
the snapshot is feasible for a given dynamic, i.e., whether there is a limited
number of nodes whose initial adoption can result in the snapshot in finite
time. While similar questions have been considered for epidemic dynamics, here,
we consider this problem for variations of the deterministic Linear Threshold
Model, which is more appropriate for modeling strategic agents. Specifically,
we consider both sequential and simultaneous dynamics when deactivations are
allowed and when they are not. Even though we show hardness results for all
variations we consider, we show that the case of sequential dynamics with
deactivations allowed is significantly harder than all others. In contrast,
sequential dynamics make the problem trivial on cliques even though it's
complexity for simultaneous dynamics is unknown. We complement our hardness
results with structural insights that can help better understand diffusions of
social networks under various dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6276</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6276</id><created>2014-02-25</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>Leonardo</forenames></author><author><keyname>Rold&#xe1;n-Pensado</keyname><forenames>Edgardo</forenames></author></authors><title>Points defining triangles with distinct circumradii</title><categories>math.MG cs.CG</categories><journal-ref>Acta Mathematica Hungarica, 2015, 145</journal-ref><doi>10.1007/s10474-014-0443-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paul Erdos asked if, among sufficiently many points in general position,
there are always $k$ points such that all the circles through $3$ of these $k$
points have different radii. He later proved that this is indeed the case.
However, he overlooked a non-trivial case in his proof. In this note we deal
with this case using B\'ezout's Theorem on the number of intersection points of
two curves and obtain a polynomial bound for the needed number of points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6278</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6278</id><created>2014-02-25</created><updated>2015-09-13</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Xiao</keyname><forenames>David</forenames></author></authors><title>Sample Complexity Bounds on Differentially Private Learning via
  Communication Complexity</title><categories>cs.DS cs.CC cs.LG</categories><comments>Extended abstract appears in Conference on Learning Theory (COLT)
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze the sample complexity of classification by
differentially private algorithms. Differential privacy is a strong and
well-studied notion of privacy introduced by Dwork et al. (2006) that ensures
that the output of an algorithm leaks little information about the data point
provided by any of the participating individuals. Sample complexity of private
PAC and agnostic learning was studied in a number of prior works starting with
(Kasiviswanathan et al., 2008) but a number of basic questions still remain
open, most notably whether learning with privacy requires more samples than
learning without privacy.
  We show that the sample complexity of learning with (pure) differential
privacy can be arbitrarily higher than the sample complexity of learning
without the privacy constraint or the sample complexity of learning with
approximate differential privacy. Our second contribution and the main tool is
an equivalence between the sample complexity of (pure) differentially private
learning of a concept class $C$ (or $SCDP(C)$) and the randomized one-way
communication complexity of the evaluation problem for concepts from $C$. Using
this equivalence we prove the following bounds:
  1. $SCDP(C) = \Omega(LDim(C))$, where $LDim(C)$ is the Littlestone's (1987)
dimension characterizing the number of mistakes in the online-mistake-bound
learning model. Known bounds on $LDim(C)$ then imply that $SCDP(C)$ can be much
higher than the VC-dimension of $C$.
  2. For any $t$, there exists a class $C$ such that $LDim(C)=2$ but $SCDP(C)
\geq t$.
  3. For any $t$, there exists a class $C$ such that the sample complexity of
(pure) $\alpha$-differentially private PAC learning is $\Omega(t/\alpha)$ but
the sample complexity of the relaxed $(\alpha,\beta)$-differentially private
PAC learning is $O(\log(1/\beta)/\alpha)$. This resolves an open problem of
Beimel et al. (2013b).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6281</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6281</id><created>2014-02-25</created><authors><author><keyname>Brengos</keyname><forenames>Tomasz</forenames></author></authors><title>On coalgebras with internal moves</title><categories>cs.LO</categories><comments>Article: 23 pages, Appendix: 3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of the paper we recall the coalgebraic approach to handling
the so-called invisible transitions that appear in different state-based
systems semantics. We claim that these transitions are always part of the unit
of a certain monad. Hence, coalgebras with internal moves are exactly
coalgebras over a monadic type. The rest of the paper is devoted to supporting
our claim by studying two important behavioural equivalences for state-based
systems with internal moves, namely: weak bisimulation and trace semantics.
  We continue our research on weak bisimulations for coalgebras over order
enriched monads. The key notions used in this paper and proposed by us in our
previous work are the notions of an order saturation monad and a saturator. A
saturator operator can be intuitively understood as a reflexive, transitive
closure operator. There are two approaches towards defining saturators for
coalgebras with internal moves. Here, we give necessary conditions for them to
yield the same notion of weak bisimulation.
  Finally, we propose a definition of trace semantics for coalgebras with
silent moves via a uniform fixed point operator. We compare strong and weak
bisimilation together with trace semantics for coalgebras with internal steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6282</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6282</id><created>2014-02-25</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Hamead</keyname><forenames>Nur Gaylan</forenames></author></authors><title>Mobile GIS and Open Source Platform Based on Android: Technology for
  System Pregnant Women</title><categories>cs.CY</categories><comments>8 pages, 16 figures, 1 Table</comments><journal-ref>International Journal of Scientific &amp; Engineering Research, Volume
  5, Issue 2, February-2014, Pages 847-854</journal-ref><doi>10.14299/000000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistic of World Health Organization shows at one year about 287000
women died most of them during and following pregnancy and childbirth in Africa
and south Asia. This paper suggests system for serving pregnant women using
open source based on Android technology, the proposed system works based on
mobile GIS to select closest care centre or hospital maternity on Google map
for the pregnant woman, which completed an online registration by sending SMS
via GPRS network (or internet) contains her name and phone number and region
(Longitude and Latitude) and other required information the server will save
the information in server database then find the closest care centre and call
her for first review at the selected care centre, the proposed system allowed
the pregnant women from her location (home, market, etc) can send a help
request in emergency cases (via SMS by click one button) contains the ID for
this pregnant woman, and her coordinates (Longitude and Latitude) via GPRS
network, then the server will locate the pregnant on Google map and retrieve
the pregnant information from the database. This information will be used by
the server to send succoring to pregnant woman at her location and at the same
time notify the nearest hospital and moreover, the server will send SMS over IP
to inform her husband and the hospital doctors. Implement and applied this
proposed system of pregnant women shows more effective cost than other systems
because it works in economic mode (SMS), and the services of proposed system
are flexible (open source platform) as well as rapidly (mobile GIS based on
Android) achieved locally registration, succoring in emergency cases, change
the review date of pregnant woman, addition to different types of advising
according to pregnancy. Index Terms: Build-in GPS; GPRS; Mobile GIS; SoIP; Open
Source; Google Maps API ; Android Technology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6286</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6286</id><created>2014-02-25</created><updated>2016-01-26</updated><authors><author><keyname>Gross</keyname><forenames>David</forenames></author><author><keyname>Krahmer</keyname><forenames>Felix</forenames></author><author><keyname>Kueng</keyname><forenames>Richard</forenames></author></authors><title>Improved Recovery Guarantees for Phase Retrieval from Coded Diffraction
  Patterns</title><categories>cs.IT math.IT quant-ph</categories><comments>28 pages, in press, Applied and Computational Harmonic Analysis
  (2015)</comments><doi>10.1016/j.acha.2015.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze the problem of phase retrieval from Fourier
measurements with random diffraction patterns. To this end, we consider the
recently introduced PhaseLift algorithm, which expresses the problem in the
language of convex optimization. We provide recovery guarantees which require
O(log^2 d) different diffraction patterns, thus improving on recent results by
Candes et al. [arXiv:1310.3240], which require O(log^4 d) different patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6288</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6288</id><created>2014-02-25</created><authors><author><keyname>Mitter</keyname><forenames>Silvia</forenames></author><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>A categorization scheme for socialbot attacks in online social networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past, online social networks (OSN) like Facebook and Twitter became
powerful instruments for communication and networking. Unfortunately, they have
also become a welcome target for socialbot attacks. Therefore, a deep
understanding of the nature of such attacks is important to protect the
Eco-System of OSNs. In this extended abstract we propose a categorization
scheme of social bot attacks that aims at providing an overview of the state of
the art of techniques in this emerging field. Finally, we demonstrate the
usefulness of our categorization scheme by characterizing recent socialbot
attacks according to our categorization scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6289</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6289</id><created>2014-02-25</created><authors><author><keyname>Mitter</keyname><forenames>Silvia</forenames></author><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Understanding the impact of socialbot attacks in online social networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks (OSN) like Twitter or Facebook are popular and
powerful since they allow reaching millions of users online. They are also a
popular target for socialbot attacks. Without a deep understanding of the
impact of such attacks, the potential of online social networks as an
instrument for facilitating discourse or democratic processes is in jeopardy.
In this extended abstract we present insights from a live lab experiment in
which social bots aimed at manipulating the social graph of an online social
network, in our case Twitter. We explored the link creation behavior between
targeted human users and our results suggest that socialbots may indeed have
the ability to shape and influence the social graph in online social networks.
However, our results also show that external factors may play an important role
in the creation of social links in OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6294</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6294</id><created>2014-02-25</created><authors><author><keyname>Keevash</keyname><forenames>Peter</forenames></author><author><keyname>Long</keyname><forenames>Eoin</forenames></author></authors><title>Frankl-R\&quot;odl type theorems for codes and permutations</title><categories>math.CO cs.IT math.IT</categories><comments>18 pages</comments><msc-class>05D05, 05D40, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of the Frankl-R\&quot;odl theorem on forbidden intersections,
via the probabilistic method of dependent random choice. Our method extends to
codes with forbidden distances, where over large alphabets our bound is
significantly better than that obtained by Frankl and R\&quot;odl. We also apply our
bound to a question of Ellis on sets of permutations with forbidden distances,
and to establish a weak form of a conjecture of Alon, Shpilka and Umans on
sunflowers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6299</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6299</id><created>2014-02-25</created><updated>2014-03-02</updated><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author></authors><title>Necessary and sufficient optimality conditions for classical simulations
  of quantum communication processes</title><categories>quant-ph cs.IT math.IT</categories><comments>Corrected some typos and replaced the (dim) proof of Theorem 1 with a
  much more elegant one. This paper is the extended journal version of the
  conference version arXiv:1401.4126. The journal version contains considerable
  new material. See comment to the first version for a list of the new material</comments><journal-ref>Phys. Rev. A 90, 012309 (2014)</journal-ref><doi>10.1103/PhysRevA.90.012309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the process consisting of preparation, transmission through a
quantum channel, and subsequent measurement of quantum states. The
communication complexity of the channel is the minimal amount of classical
communication required for classically simulating it. Recently, we reduced the
computation of this quantity to a convex minimization problem with linear
constraints. Every solution of the constraints provides an upper bound on the
communication complexity. In this paper, we derive the dual maximization
problem of the original one. The feasible points of the dual constraints, which
are inequalities, give lower bounds on the communication complexity, as
illustrated with an example. The optimal values of the two problems turn out to
be equal (zero duality gap). By this property, we provide necessary and
sufficient conditions for optimality in terms of a set of equalities and
inequalities. We use these conditions and two reasonable but unproven
hypotheses to derive the lower bound $n 2^{n-1}$ for a noiseless quantum
channel with capacity equal to $n$ qubits. This lower bound can have
interesting consequences in the context of the recent debate on the reality of
the quantum state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6305</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6305</id><created>2014-02-25</created><updated>2015-04-17</updated><authors><author><keyname>Stephane</keyname><forenames>Boucheron</forenames></author><author><keyname>Gassiat</keyname><forenames>Elisabeth</forenames></author><author><keyname>Ohannessian</keyname><forenames>Mesrob I.</forenames></author></authors><title>About Adaptive Coding on Countable Alphabets: Max-Stable Envelope
  Classes</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of lossless universal source coding for
stationary memoryless sources on countably infinite alphabets. This task is
generally not achievable without restricting the class of sources over which
universality is desired. Building on our prior work, we propose natural
families of sources characterized by a common dominating envelope. We
particularly emphasize the notion of adaptivity, which is the ability to
perform as well as an oracle knowing the envelope, without actually knowing it.
This is closely related to the notion of hierarchical universal source coding,
but with the important difference that families of envelope classes are not
discretely indexed and not necessarily nested.
  Our contribution is to extend the classes of envelopes over which adaptive
universal source coding is possible, namely by including max-stable
(heavy-tailed) envelopes which are excellent models in many applications, such
as natural language modeling. We derive a minimax lower bound on the redundancy
of any code on such envelope classes, including an oracle that knows the
envelope. We then propose a constructive code that does not use knowledge of
the envelope. The code is computationally efficient and is structured to use an
{E}xpanding {T}hreshold for {A}uto-{C}ensoring, and we therefore dub it the
\textsc{ETAC}-code. We prove that the \textsc{ETAC}-code achieves the lower
bound on the minimax redundancy within a factor logarithmic in the sequence
length, and can be therefore qualified as a near-adaptive code over families of
heavy-tailed envelopes. For finite and light-tailed envelopes the penalty is
even less, and the same code follows closely previous results that explicitly
made the light-tailed assumption. Our technical results are founded on methods
from regular variation theory and concentration of measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6310</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6310</id><created>2014-02-25</created><authors><author><keyname>Babu</keyname><forenames>Jasine</forenames></author><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Chandran</keyname><forenames>L Sunil</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author><author><keyname>Sivadasan</keyname><forenames>Naveen</forenames></author></authors><title>Approximating the Cubicity of Trees</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cubicity of a graph $G$ is the smallest dimension $d$, for which $G$ is a
unit disc graph in ${\mathbb{R}}^d$, under the $l^\infty$ metric, i.e. $G$ can
be represented as an intersection graph of $d$-dimensional (axis-parallel) unit
hypercubes. We call such an intersection representation a $d$-dimensional cube
representation of $G$. Computing cubicity is known to be inapproximable in
polynomial time, within an $O(n^{1-\epsilon})$ factor for any $\epsilon &gt;0$,
unless NP=ZPP.
  In this paper, we present a randomized algorithm that runs in polynomial time
and computes cube representations of trees, of dimension within a constant
factor of the optimum. It is also shown that the cubicity of trees can be
approximated within a constant factor in deterministic polynomial time, if the
cube representation is not required to be computed. As far as we know, this is
the first constant factor approximation algorithm for computing the cubicity of
trees. It is not yet clear whether computing the cubicity of trees is NP-hard
or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6317</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6317</id><created>2014-02-25</created><authors><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Maria Isabel</forenames></author><author><keyname>Santos-Penate</keyname><forenames>Dolores Rosa</forenames></author><author><keyname>Suarez-Vega</keyname><forenames>Rafael</forenames></author></authors><title>Journal topic citation potential and between-field comparisons: The
  topic normalized impact factor</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The journal impact factor is not comparable among fields of science and
social science because of systematic differences in publication and citation
behaviour across disciplines. In this work, a source normalization of the
journal impact factor is proposed. We use the aggregate impact factor of the
citing journals as a measure of the citation potential in the journal topic,
and we employ this citation potential in the normalization of the journal
impact factor to make it comparable between scientific fields. An empirical
application comparing some impact indicators with our topic normalized impact
factor in a set of 224 journals from four different fields shows that our
normalization, using the citation potential in the journal topic, reduces the
between-group variance with respect to the within-group variance in a higher
proportion than the rest of indicators analysed. The effect of journal
self-citations over the normalization process is also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6361</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6361</id><created>2014-02-25</created><authors><author><keyname>Ben-Tal</keyname><forenames>Aharon</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Oracle-Based Robust Optimization via Online Learning</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimization is a common framework in optimization under uncertainty
when the problem parameters are not known, but it is rather known that the
parameters belong to some given uncertainty set. In the robust optimization
framework the problem solved is a min-max problem where a solution is judged
according to its performance on the worst possible realization of the
parameters. In many cases, a straightforward solution of the robust
optimization problem of a certain type requires solving an optimization problem
of a more complicated type, and in some cases even NP-hard. For example,
solving a robust conic quadratic program, such as those arising in robust SVM,
ellipsoidal uncertainty leads in general to a semidefinite program. In this
paper we develop a method for approximately solving a robust optimization
problem using tools from online convex optimization, where in every stage a
standard (non-robust) optimization program is solved. Our algorithms find an
approximate robust solution using a number of calls to an oracle that solves
the original (non-robust) problem that is inversely proportional to the square
of the target accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6366</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6366</id><created>2014-02-25</created><authors><author><keyname>Hegazy</keyname><forenames>Osman</forenames></author><author><keyname>Soliman</keyname><forenames>Omar S.</forenames></author><author><keyname>Salam</keyname><forenames>Mustafa Abdul</forenames></author></authors><title>LSSVM-ABC Algorithm for Stock Price prediction</title><categories>cs.CE cs.NE</categories><comments>12 pages. International Journal of Computer Trends and Technology
  (IJCTT)2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, Artificial Bee Colony (ABC) algorithm which inspired from the
behavior of honey bees swarm is presented. ABC is a stochastic population-based
evolutionary algorithm for problem solving. ABC algorithm, which is considered
one of the most recently swarm intelligent techniques, is proposed to optimize
least square support vector machine (LSSVM) to predict the daily stock prices.
The proposed model is based on the study of stocks historical data, technical
indicators and optimizing LSSVM with ABC algorithm. ABC selects best free
parameters combination for LSSVM to avoid over-fitting and local minima
problems and improve prediction accuracy. LSSVM optimized by Particle swarm
optimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparison
with proposed model. Proposed model tested with twenty datasets representing
different sectors in S&amp;P 500 stock market. Results presented in this paper show
that the proposed model has fast convergence speed, and it also achieves better
accuracy than compared techniques in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6383</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6383</id><created>2014-02-25</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Large-margin Learning of Compact Binary Image Encodings</title><categories>cs.CV</categories><comments>13 pages</comments><doi>10.1109/TIP.2014.2337759</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of high-dimensional features has become a normal practice in many
computer vision applications. The large dimension of these features is a
limiting factor upon the number of data points which may be effectively stored
and processed, however. We address this problem by developing a novel approach
to learning a compact binary encoding, which exploits both pair-wise proximity
and class-label information on training data set. Exploiting this extra
information allows the development of encodings which, although compact,
outperform the original high-dimensional features in terms of final
classification or retrieval performance. The method is general, in that it is
applicable to both non-parametric and parametric learning methods. This
generality means that the embedded features are suitable for a wide variety of
computer vision tasks, such as image classification and content-based image
retrieval. Experimental results demonstrate that the new compact descriptor
achieves an accuracy comparable to, and in some cases better than, the visual
descriptor in the original space despite being significantly more compact.
Moreover, any convex loss function and convex regularization penalty (e.g., $
\ell_p $ norm with $ p \ge 1 $) can be incorporated into the framework, which
provides future flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6387</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6387</id><created>2014-02-25</created><authors><author><keyname>Tan</keyname><forenames>Jen Hong</forenames></author><author><keyname>Acharya</keyname><forenames>U. Rajendra</forenames></author></authors><title>Active spline model: A shape based model-interactive segmentation</title><categories>cs.CV</categories><comments>submitted to Computers in biology and Medicine, second revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rarely in literature a method of segmentation cares for the edit after the
algorithm delivers. They provide no solution when segmentation goes wrong. We
propose to formulate point distribution model in terms of
centripetal-parameterized Catmull-Rom spline. Such fusion brings interactivity
to model-based segmentation, so that edit is better handled. When the delivered
segment is unsatisfactory, user simply shifts points to vary the curve. We ran
the method on three disparate imaging modalities and achieved an average
overlap of 0.879 for automated lung segmentation on chest radiographs. The edit
afterward improved the average overlap to 0.945, with a minimum of 0.925. The
source code and the demo video are available at http://wp.me/p3vCKy-2S
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6399</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6399</id><created>2014-02-25</created><updated>2014-03-31</updated><authors><author><keyname>Li</keyname><forenames>Ruihu</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Mao</keyname><forenames>Yaping</forenames></author><author><keyname>Wei</keyname><forenames>Meiqin</forenames></author></authors><title>Formally self-dual linear binary codes from circulant graphs</title><categories>math.CO cs.IT math.IT</categories><comments>15 pages</comments><msc-class>94B05, 05C50, 05C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2002, Tonchev first constructed some linear binary codes defined by the
adjacency matrices of undirected graphs. So, graph is an important tool for
searching optimum codes. In this paper, we introduce a new method of searching
(proposed) optimum formally self-dual linear binary codes from circulant
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6404</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6404</id><created>2014-02-25</created><authors><author><keyname>Conti</keyname><forenames>David</forenames></author><author><keyname>Boston</keyname><forenames>Nigel</forenames></author></authors><title>On the Algebraic Structure of Linear Trellises</title><categories>cs.IT cs.DM math.IT</categories><comments>53 pages. Submitted to IEEE Transactions on Information Theory. Some
  parts of this paper were presented at the 2012 International Zurich Seminar
  on Communications and the 2012 Allerton conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trellises are crucial graphical representations of codes. While conventional
trellises are well understood, the general theory of (tail-biting) trellises is
still under development. Iterative decoding concretely motivates such theory.
In this paper we first develop a new algebraic framework for a systematic
analysis of linear trellises which enables us to address open foundational
questions. In particular, we present a useful and powerful characterization of
linear trellis isomorphy. We also obtain a new proof of the Factorization
Theorem of Koetter/Vardy and point out unnoticed problems for the group case.
  Next, we apply our work to: describe all the elementary trellis
factorizations of linear trellises and consequently to determine all the
minimal linear trellises for a given code; prove that nonmergeable one-to-one
linear trellises are strikingly determined by the edge-label sequences of
certain closed paths; prove self-duality theorems for minimal linear trellises;
analyze quasi-cyclic linear trellises and consequently extend results on
reduced linear trellises to nonreduced ones. To achieve this, we also provide
new insight into mergeability and path connectivity properties of linear
trellises.
  Our classification results are important for iterative decoding as we show
that minimal linear trellises can yield different pseudocodewords even if they
have the same graph structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6407</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6407</id><created>2014-02-25</created><updated>2015-03-28</updated><authors><author><keyname>Chambi</keyname><forenames>Samy</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Kaser</keyname><forenames>Owen</forenames></author><author><keyname>Godin</keyname><forenames>Robert</forenames></author></authors><title>Better bitmap performance with Roaring bitmaps</title><categories>cs.DB</categories><doi>10.1002/spe.2325</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Bitmap indexes are commonly used in databases and search engines. By
exploiting bit-level parallelism, they can significantly accelerate queries.
However, they can use much memory, and thus we might prefer compressed bitmap
indexes. Following Oracle's lead, bitmaps are often compressed using run-length
encoding (RLE). Building on prior work, we introduce the Roaring compressed
bitmap format: it uses packed arrays for compression instead of RLE. We compare
it to two high-performance RLE-based bitmap encoding techniques: WAH (Word
Aligned Hybrid compression scheme) and Concise (Compressed `n' Composable
Integer Set). On synthetic and real data, we find that Roaring bitmaps (1)
often compress significantly better (e.g., 2 times) and (2) are faster than the
compressed alternatives (up to 900 times faster for intersections). Our results
challenge the view that RLE-based bitmap compression is best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6416</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6416</id><created>2014-02-26</created><authors><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Bastian</keyname><forenames>John</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Fleming</keyname><forenames>Lachlan</forenames></author></authors><title>Deconstruction of compound objects from image sets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to recover the structure of a compound object from
multiple silhouettes. Structure is expressed as a collection of 3D primitives
chosen from a pre-defined library, each with an associated pose. This has
several advantages over a volume or mesh representation both for estimation and
the utility of the recovered model. The main challenge in recovering such a
model is the combinatorial number of possible arrangements of parts. We address
this issue by exploiting the sparse nature of the problem, and show that our
method scales to objects constructed from large libraries of parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6421</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6421</id><created>2014-02-26</created><authors><author><keyname>Moro</keyname><forenames>Nicolas</forenames><affiliation>LIP6, SAS-ENSMSE, LETI</affiliation></author><author><keyname>Dehbaoui</keyname><forenames>Amine</forenames><affiliation>SAS-ENSMSE</affiliation></author><author><keyname>Heydemann</keyname><forenames>Karine</forenames><affiliation>LIP6</affiliation></author><author><keyname>Robisson</keyname><forenames>Bruno</forenames><affiliation>SAS-ENSMSE, LETI</affiliation></author><author><keyname>Encrenaz</keyname><forenames>Emmanuelle</forenames><affiliation>LIP6</affiliation></author></authors><title>Electromagnetic fault injection: towards a fault model on a 32-bit
  microcontroller</title><categories>cs.CR</categories><comments>10 pages</comments><proxy>ccsd</proxy><journal-ref>10th workshop on Fault Diagnosis and Tolerance in Cryptography -
  FDTC 2013, Santa-Barbara : United States (2013)</journal-ref><doi>10.1109/FDTC.2013.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Injection of transient faults as a way to attack cryptographic
implementations has been largely studied in the last decade. Several attacks
that use electromagnetic fault injection against hardware or software
architectures have already been presented. On microcontrollers, electromagnetic
fault injection has mostly been seen as a way to skip assembly instructions or
subroutine calls. However, to the best of our knowledge, no precise study about
the impact of an electromagnetic glitch fault injection on a microcontroller
has been proposed yet. The aim of this paper is twofold: providing a more
in-depth study of the effects of electromagnetic glitch fault injection on a
state-of-the-art microcontroller and building an associated register-transfer
level fault model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6422</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6422</id><created>2014-02-26</created><updated>2014-09-23</updated><authors><author><keyname>Islam</keyname><forenames>Shama N.</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>A Novel User Pairing Scheme for Functional Decode-and-Forward Multi-way
  Relay Network</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, submitted for journal publication</comments><msc-class>94A05, 94B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a functional decode and forward (FDF) multi-way
relay network (MWRN) where a common user facilitates each user in the network
to obtain messages from all other users. We propose a novel user pairing
scheme, which is based on the principle of selecting a common user with the
best average channel gain. This allows the user with the best channel
conditions to contribute to the overall system performance. Assuming lattice
code based transmissions, we derive upper bounds on the average common rate and
the average sum rate with the proposed pairing scheme. Considering M-ary
quadrature amplitude modulation with square constellation as a special case of
lattice code transmission, we derive asymptotic average symbol error rate (SER)
of the MWRN. We show that in terms of the achievable rates, the proposed
pairing scheme outperforms the existing pairing schemes under a wide range of
channel scenarios. The proposed pairing scheme also has lower average SER
compared to existing schemes. We show that overall, the MWRN performance with
the proposed pairing scheme is more robust, compared to existing pairing
schemes, especially under worst case channel conditions when majority of users
have poor average channel gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6428</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6428</id><created>2014-02-26</created><authors><author><keyname>Ghorpade-Aher</keyname><forenames>Jayshree</forenames></author><author><keyname>Metre</keyname><forenames>Vishakha A.</forenames></author></authors><title>Clustering Multidimensional Data with PSO based Algorithm</title><categories>cs.NE</categories><comments>6 pages,6 figures,3 tables, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering is a recognized data analysis method in data mining whereas
K-Means is the well known partitional clustering method, possessing pleasant
features. We observed that, K-Means and other partitional clustering techniques
suffer from several limitations such as initial cluster centre selection,
preknowledge of number of clusters, dead unit problem, multiple cluster
membership and premature convergence to local optima. Several optimization
methods are proposed in the literature in order to solve clustering
limitations, but Swarm Intelligence (SI) has achieved its remarkable position
in the concerned area. Particle Swarm Optimization (PSO) is the most popular SI
technique and one of the favorite areas of researchers. In this paper, we
present a brief overview of PSO and applicability of its variants to solve
clustering challenges. Also, we propose an advanced PSO algorithm named as
Subtractive Clustering based Boundary Restricted Adaptive Particle Swarm
Optimization (SC-BR-APSO) algorithm for clustering multidimensional data. For
comparison purpose, we have studied and analyzed various algorithms such as
K-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposed
algorithm on nine different datasets. The motivation behind proposing
SC-BR-APSO algorithm is to deal with multidimensional data clustering, with
minimum error rate and maximum convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6430</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6430</id><created>2014-02-26</created><updated>2014-10-18</updated><authors><author><keyname>Bai</keyname><forenames>Tianyang</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Coverage and Rate Analysis for Millimeter Wave Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>16 pages, 10 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) holds promise as a carrier frequency for fifth
generation cellular networks. Because mmWave signals are sensitive to blockage,
prior models for cellular networks operated in the ultra high frequency (UHF)
band do not apply to analyze mmWave cellular networks directly. Leveraging
concepts from stochastic geometry, this paper proposes a general framework to
evaluate the coverage and rate performance in mmWave cellular networks. Using a
distance-dependent line-of-site (LOS) probability function, the locations of
the LOS and non-LOS base stations are modeled as two independent
non-homogeneous Poisson point processes, to which different path loss laws are
applied. Based on the proposed framework, expressions for the
signal-to-noise-and-interference ratio (SINR) and rate coverage probability are
derived. The mmWave coverage and rate performance are examined as a function of
the antenna geometry and base station density. The case of dense networks is
further analyzed by applying a simplified system model, in which the LOS region
of a user is approximated as a fixed LOS ball. The results show that dense
mmWave networks can achieve comparable coverage and much higher data rates than
conventional UHF cellular systems, despite the presence of blockages. The
results suggest that the cell size to achieve the optimal SINR scales with the
average size of the area that is LOS to a user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6441</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6441</id><created>2014-02-26</created><updated>2014-08-29</updated><authors><author><keyname>Lee</keyname><forenames>Seunghyun</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Collaborative Wireless Energy and Information Transfer in Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the simultaneous wireless information and power transfer
(SWIPT) in a multiuser wireless system, in which distributed transmitters send
independent messages to their respective receivers, and at the same time
cooperatively transmit wireless power to the receivers via energy beamforming.
Accordingly, from the wireless information transmission (WIT) perspective, the
system of interest can be modeled as the classic interference channel, while it
also can be regarded as a distributed multiple-input multiple-output (MIMO)
system for collaborative wireless energy transmission (WET). To enable both
information decoding (ID) and energy harvesting (EH) in SWIPT, we adopt the
low-complexity time switching operation at each receiver to switch between the
ID and EH modes over scheduled time. Based on this hybrid model, we aim to
characterize the achievable rate-energy (R-E) trade-offs in the multiuser SWIPT
system under various transmitter-side collaboration schemes. Specifically, to
facilitate the collaborative energy beamforming, we propose a new signal
splitting scheme at the transmitters, where each transmit signal is generally
composed of an information signal component and an energy signal component for
WIT and WET, respectively. With this new scheme, first, we study the two-user
SWIPT system and derive the optimal mode switching rule at the receivers and
the corresponding transmit signal optimization to achieve various R-E
trade-offs over the fading channel. We also compare the R-E performance of our
proposed scheme with transmit energy beamforming and signal splitting against
two existing schemes with partial or no cooperation of the transmitters, and
show remarkable gains over these baseline schemes. Finally, the general case of
SWIPT systems with more than two users is studied, for which we propose and
compare two practical transmit collaboration schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6457</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6457</id><created>2014-02-26</created><authors><author><keyname>Kuo</keyname><forenames>Tung-Wei</forenames></author><author><keyname>Lin</keyname><forenames>Kate Ching-Ju</forenames></author><author><keyname>Tsai</keyname><forenames>Ming-Jer</forenames></author></authors><title>On the Construction of Data Aggregation Tree with Minimum Energy Cost in
  Wireless Sensor Networks: NP-Completeness and Approximation Algorithms</title><categories>cs.NI</categories><comments>19 pages, 9 figures, 1 table, submitted to IEEE Transactions on
  Information Theory, Feb 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, it is a basic operation for the sink to periodically
collect reports from all sensors. Since the data gathering process usually
proceeds for many rounds, it is important to collect these data efficiently,
that is, to reduce the energy cost of data transmission. Under such
applications, a tree is usually adopted as the routing structure to save the
computation costs for maintaining the routing tables of sensors. In this paper,
we work on the problem of constructing a data aggregation tree that minimizes
the total energy cost of data transmission in a wireless sensor network. In
addition, we also address such a problem in the wireless sensor network where
relay nodes exist. We show these two problems are NP-complete, and propose
O(1)-approximation algorithms for each of them. Simulations show that the
proposed algorithms each have good performance in terms of the energy cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6459</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6459</id><created>2014-02-26</created><updated>2014-09-13</updated><authors><author><keyname>Beffara</keyname><forenames>Emmanuel</forenames><affiliation>I2M</affiliation></author></authors><title>A proof-theoretic view on scheduling in concurrency</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper elaborates on a new approach of the question of the
proof-theoretic study of concurrent interaction called &quot;proofs as schedules&quot;.
Observing that proof theory is well suited to the description of confluent
systems while concurrency has non-determinism as a fundamental feature, we
develop a correspondence where proofs provide what is needed to make concurrent
systems confluent, namely scheduling. In our logical system, processes and
schedulers appear explicitly as proofs in different fragments of the proof
language and cut elimination between them does correspond to execution of a
concurrent system. This separation of roles suggests new insights for the
denotational semantics of processes and new methods for the translation of
pi-calculi into prefix-less formalisms (like solos) as the operational
counterpart of translations between proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6461</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6461</id><created>2014-02-26</created><authors><author><keyname>Moro</keyname><forenames>Nicolas</forenames><affiliation>LIP6, SAS-ENSMSE, LETI</affiliation></author><author><keyname>Heydemann</keyname><forenames>Karine</forenames><affiliation>LIP6</affiliation></author><author><keyname>Encrenaz</keyname><forenames>Emmanuelle</forenames><affiliation>LIP6</affiliation></author><author><keyname>Robisson</keyname><forenames>Bruno</forenames><affiliation>SAS-ENSMSE, LETI</affiliation></author></authors><title>Formal verification of a software countermeasure against instruction
  skip attacks</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>Journal of Cryptographic Engineering (2014) 1-12</journal-ref><doi>10.1007/s13389-014-0077-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault attacks against embedded circuits enabled to define many new attack
paths against secure circuits. Every attack path relies on a specific fault
model which defines the type of faults that the attacker can perform. On
embedded processors, a fault model consisting in an assembly instruction skip
can be very useful for an attacker and has been obtained by using several fault
injection means. To avoid this threat, some countermeasure schemes which rely
on temporal redundancy have been proposed. Nevertheless, double fault injection
in a long enough time interval is practical and can bypass those countermeasure
schemes. Some fine-grained countermeasure schemes have also been proposed for
specific instructions. However, to the best of our knowledge, no approach that
enables to secure a generic assembly program in order to make it fault-tolerant
to instruction skip attacks has been formally proven yet. In this paper, we
provide a fault-tolerant replacement sequence for almost all the instructions
of the Thumb-2 instruction set and provide a formal verification for this fault
tolerance. This simple transformation enables to add a reasonably good security
level to an embedded program and makes practical fault injection attacks much
harder to achieve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6474</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6474</id><created>2014-02-26</created><authors><author><keyname>Chin</keyname><forenames>Woon Hau</forenames></author><author><keyname>Fan</keyname><forenames>Zhong</forenames></author><author><keyname>Haines</keyname><forenames>Russell J.</forenames></author></authors><title>Emerging Technologies and Research Challenges for 5G Wireless Networks</title><categories>cs.NI</categories><comments>Accepted for publication in IEEE Wireless Communications April 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the take-up of Long Term Evolution (LTE)/4G cellular accelerates, there is
increasing interest in technologies that will define the next generation (5G)
telecommunication standard. This paper identifies several emerging technologies
which will change and define the future generations of telecommunication
standards. Some of these technologies are already making their way into
standards such as 3GPP LTE, while others are still in development.
Additionally, we will look at some of the research problems that these new
technologies pose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6478</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6478</id><created>2014-02-26</created><authors><author><keyname>de Aledo</keyname><forenames>Pablo Gonz&#xe1;lez</forenames></author></authors><title>Estimating verification time</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This essay is divided in four parts: In section (I) I explain why I think
detecting hot-spots in verification is complicated and in particular, more
complicated than detection when developing crude software. In section (II) I
introduce the factors I think mostly affect performance in verification. In
section (III) I propose a method to find functions that are most promising to
be optimized. Finally, in section (IV) I draw some conclusions and discuss
pros/cons of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6485</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6485</id><created>2014-02-26</created><authors><author><keyname>S&#xe6;ther</keyname><forenames>Sigve Hortemo</forenames></author><author><keyname>Telle</keyname><forenames>Jan Arne</forenames></author><author><keyname>Vatshelle</keyname><forenames>Martin</forenames></author></authors><title>Solving MaxSAT and #SAT on structured CNF formulas</title><categories>cs.DS cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a structural parameter of CNF formulas and use it to
identify instances of weighted MaxSAT and #SAT that can be solved in polynomial
time. Given a CNF formula we say that a set of clauses is precisely satisfiable
if there is some complete assignment satisfying these clauses only. Let the
ps-value of the formula be the number of precisely satisfiable sets of clauses.
Applying the notion of branch decompositions to CNF formulas and using ps-value
as cut function, we define the ps-width of a formula. For a formula given with
a decomposition of polynomial ps-width we show dynamic programming algorithms
solving weighted MaxSAT and #SAT in polynomial time. Combining with results of
'Belmonte and Vatshelle, Graph classes with structured neighborhoods and
algorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get
polynomial-time algorithms solving weighted MaxSAT and #SAT for some classes of
structured CNF formulas. For example, we get $O(m^2(m + n)s)$ algorithms for
formulas $F$ of $m$ clauses and $n$ variables and size $s$, if $F$ has a linear
ordering of the variables and clauses such that for any variable $x$ occurring
in clause $C$, if $x$ appears before $C$ then any variable between them also
occurs in $C$, and if $C$ appears before $x$ then $x$ occurs also in any clause
between them. Note that the class of incidence graphs of such formulas do not
have bounded clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6489</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6489</id><created>2014-02-26</created><authors><author><keyname>Kasthurirathna</keyname><forenames>Dharshana</forenames></author><author><keyname>Piraveenan</keyname><forenames>Mahendra</forenames></author><author><keyname>Thedchanamoorthy</keyname><forenames>Gnanakumar</forenames></author></authors><title>On the influence of topological characteristics on robustness of complex
  networks</title><categories>physics.soc-ph cs.SI nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the relationship between the topological
characteristics of a complex network and its robustness to sustained targeted
attacks. Using synthesised scale-free, small-world and random networks, we look
at a number of network measures, including assortativity, modularity, average
path length, clustering coefficient, rich club profiles and scale-free exponent
(where applicable) of a network, and how each of these influence the robustness
of a network under targeted attacks. We use an established robustness
coefficient to measure topological robustness, and consider sustained targeted
attacks by order of node degree. With respect to scale-free networks, we show
that assortativity, modularity and average path length have a positive
correlation with network robustness, whereas clustering coefficient has a
negative correlation. We did not find any correlation between scale-free
exponent and robustness, or rich-club profiles and robustness. The robustness
of small-world networks on the other hand, show substantial positive
correlations with assortativity, modularity, clustering coefficient and average
path length. In comparison, the robustness of Erdos-Renyi random networks did
not have any significant correlation with any of the network properties
considered. A significant observation is that high clustering decreases
topological robustness in scale-free networks, yet it increases topological
robustness in small-world networks. Our results highlight the importance of
topological characteristics in influencing network robustness, and illustrate
design strategies network designers can use to increase the robustness of
scale-free and small-world networks under sustained targeted attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6497</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6497</id><created>2014-02-26</created><authors><author><keyname>Aghav</keyname><forenames>Sharayu A.</forenames></author><author><keyname>Bedi</keyname><forenames>RajneeshKaur</forenames></author></authors><title>Authentication Mechanism for Resistance to Password Stealing and Reuse
  Attack</title><categories>cs.CR</categories><comments>6 pages, 3 figures, Third Post Graduate Symposium for Computer
  Engineering cPGCON 2014, 28-29 March, 2014, Nashik, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering computer systems, security is the major concern with usability.
Security policies need to be developed to protect information from unauthorized
access. Passwords and secrete codes used between users and information systems
for secure user authentication with the system. Playing a vital role in
security, easily guessed passwords are links to vulnerability. They allow
invader to put system resources significantly closer to access them, other
accounts on nearby machines and possibly even administrative privileges with
different threats and vulnerabilities (e.g., phishing, key logging and
malwares). The purpose of this system is to introduce the concept and
methodology which helps organization and users to implement stronger password
policies. This paper studies a password stealing and reuse issues of password
based authentication systems. Techniques and concepts of authentication are
discussed which gives rise to a novel approach of two-factor authentication.
Avoiding password reuse is a crucial issue in information systems which can at
some extent contribute to password stealing issue also. In the proposed system,
each participating website possesses a user's unique phone number,
telecommunication services in registration and recovery phases and a long-term
password used to generate one-time password for each login session on all
websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6500</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6500</id><created>2014-02-26</created><updated>2014-03-24</updated><authors><author><keyname>Zhong</keyname><forenames>Changtao</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author><author><keyname>Shah</keyname><forenames>Sunil</forenames></author><author><keyname>Cobzarenco</keyname><forenames>Marius</forenames></author><author><keyname>Sastry</keyname><forenames>Nishanth</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author></authors><title>Social Bootstrapping: How Pinterest and Last.fm Social Communities
  Benefit by Borrowing Links from Facebook</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Proc. 23rd International World Wide Web Conference (WWW), 2014</comments><acm-class>H.3.5; J.4</acm-class><doi>10.1145/2566486.2568031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does one develop a new online community that is highly engaging to each
user and promotes social interaction? A number of websites offer friend-finding
features that help users bootstrap social networks on the website by copying
links from an established network like Facebook or Twitter. This paper
quantifies the extent to which such social bootstrapping is effective in
enhancing a social experience of the website. First, we develop a stylised
analytical model that suggests that copying tends to produce a giant connected
component (i.e., a connected community) quickly and preserves properties such
as reciprocity and clustering, up to a linear multiplicative factor. Second, we
use data from two websites, Pinterest and Last.fm, to empirically compare the
subgraph of links copied from Facebook to links created natively. We find that
the copied subgraph has a giant component, higher reciprocity and clustering,
and confirm that the copied connections see higher social interactions.
However, the need for copying diminishes as users become more active and
influential. Such users tend to create links natively on the website, to users
who are more similar to them than their Facebook friends. Our findings give new
insights into understanding how bootstrapping from established social networks
can help engage new users by enhancing social interactivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6508</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6508</id><created>2014-02-26</created><updated>2014-02-27</updated><authors><author><keyname>Bisconti</keyname><forenames>Cristian</forenames></author><author><keyname>Corallo</keyname><forenames>Angelo</forenames></author><author><keyname>Fortunato</keyname><forenames>Laura</forenames></author><author><keyname>Gentile</keyname><forenames>Antonio A.</forenames></author></authors><title>Considerations about multistep community detection</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem and implications of community detection in networks have raised a
huge attention, for its important applications in both natural and social
sciences. A number of algorithms has been developed to solve this problem,
addressing either speed optimization or the quality of the partitions
calculated. In this paper we propose a multi-step procedure bridging the
fastest, but less accurate algorithms (coarse clustering), with the slowest,
most effective ones (refinement). By adopting heuristic ranking of the nodes,
and classifying a fraction of them as `critical', a refinement step can be
restricted to this subset of the network, thus saving computational time.
Preliminary numerical results are discussed, showing improvement of the final
partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6510</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6510</id><created>2014-02-26</created><updated>2014-12-07</updated><authors><author><keyname>Jan&#x10d;i&#x107;</keyname><forenames>Zorana</forenames></author><author><keyname>Mici&#x107;</keyname><forenames>Ivana</forenames></author><author><keyname>Ignjatovi&#x107;</keyname><forenames>Jelena</forenames></author><author><keyname>&#x106;iri&#x107;</keyname><forenames>Miroslav</forenames></author></authors><title>Further improvements of determinization methods for fuzzy finite
  automata</title><categories>cs.FL</categories><comments>Preprint submitted to Fuzzy Sets and Systems</comments><msc-class>68Q45, 68Q70, 68T37, 03E72</msc-class><acm-class>F.1.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we combine determinization and state reduction methods into
two-in-one algorithms that simultaneously perform determinization and state
reduction. These algorithms perform better than all previous determinization
algorithms for fuzzy finite automata, developed by Belohlavek [Inform Sciences
143 (2002) 205-209], Li and Pedrycz [Fuzzy Set Syst 156 (2005) 68-92],
Ignjatovi\'c et al. [Inform Sciences 178 (2008) 164-180], and Jan\v{c}i\'c et
al. [Inform Sciences 181 (2011) 1358-1368], in the sense that they produce
smaller automata, while require the same computation time. The only exception
is the Brzozowski type determinization algorithm developed recently by
Jan\v{c}i\'c and \'Ciri\'c [Fuzzy Set Syst (2014), to appear], which produces a
minimal crisp-deterministic fuzzy automaton, but the algorithms created here
can also be used within the Brzozowski type algorithm and improve its
performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6515</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6515</id><created>2014-02-26</created><authors><author><keyname>Kushwah</keyname><forenames>Atul Singh</forenames></author></authors><title>Performance Analysis of 2*4 MIMO-MC-CDMA in Rayleigh Fading Channel
  Using ZF-Decoder</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 2 figures, 2 tables Published with International Journal of
  Engineering Trends and Technology (IJETT) Volume-8 Number-4 Year of
  Publication : 2014</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V8(4),204-207 February 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V8P237</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the performance of 2*4 MIMO-MC-CDMA system in MATLAB
which highly reduces BER. In this paper we combine MIMO and MC-CDMA system to
reduce bit error rate in which MC-CDMA is multi user and multiple access
schemes which is used to increase the data rate of the system. MC-CDMA system
is a single wideband frequency selective carrier which converts frequency
selective to parallel narrowband flat fading multiple sub-carriers to enhance
the performance of system. Now MC-CDMA system further improved by grouping with
2*4 MIMO system which uses ZF (Zero Forcing) decoder at the receiver to
decrease BER with half rate convolutionally encoded Alamouti STBC block code is
used as transmit diversity of MIMO through multiple transmit antenna.
Importance of using MIMO-MC-CDMA using convolution code is firstly to reduce
the complexity of system secondary to reduce BER and lastly to increase gain.
In this paper we examine system performance in diverse modulation techniques
like, 8-PSK, 16-QAM, QPSK, 32-QAM, 8-QAM and 64-QAM in Rayleigh fading channel
using MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6516</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6516</id><created>2014-02-26</created><authors><author><keyname>Dubbin</keyname><forenames>Greg</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Modelling the Lexicon in Unsupervised Part of Speech Induction</title><categories>cs.CL</categories><comments>To be presented at the 14th Conference of the European Chapter of the
  Association for Computational Linguistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically inducing the syntactic part-of-speech categories for words in
text is a fundamental task in Computational Linguistics. While the performance
of unsupervised tagging models has been slowly improving, current
state-of-the-art systems make the obviously incorrect assumption that all
tokens of a given word type must share a single part-of-speech tag. This
one-tag-per-type heuristic counters the tendency of Hidden Markov Model based
taggers to over generate tags for a given word type. However, it is clearly
incompatible with basic syntactic theory. In this paper we extend a
state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model
of the lexicon. In doing so we are able to incorporate a soft bias towards
inducing few tags per type. We develop a particle filter for drawing samples
from the posterior of our model and present empirical results that show that
our model is competitive with and faster than the state-of-the-art without
making any unrealistic restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6519</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6519</id><created>2014-02-26</created><updated>2014-03-04</updated><authors><author><keyname>Xia</keyname><forenames>Xiaochen</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Performance Analysis of Interference-Limited Three-Phase Two-Way
  Relaying with Direct Channel</title><categories>cs.IT math.IT</categories><comments>31 pages, 10 figures, 1 table. Complementary Material, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the performance of interference-limited three-phase
two-way relaying with direct channel between two terminals in Rayleigh fading
channels. The outage probability, sum bit error rate (BER) and ergodic sum rate
are analyzed for a general model that both terminals and relay are corrupted by
co-channel interference. We first derive the closed-form expressions of
cumulative distribution function (CDF) for received
signal-to-interference-plus-noise ratio (SINR) at the terminal. Based on the
results for CDF, the lower bounds, approximate expressions as well as the
asymptotic expressions for outage probability and sum BER are derived in
closed-form with different computational complexities and accuracies. The
approximate expression for ergodic sum rate is also presented. With the
theoretic results, we consider the optimal power allocation at the relay and
optimal relay location problems that aiming to minimize the outage and sum BER
performances of the protocol. It is shown that jointly optimization of power
and relay location can provide the best performance. Simulation results are
presented to study the effect of system parameters while verify the theoretic
analysis. The results show that three-phase TWR protocol can outperform
two-phase TWR protocol in ergodic sum rate when the interference power at the
relay is much larger than that at the terminals. This is in sharp contrast with
the conclusion in interference free scenario. Moreover, we show that an
estimation error on the interference channel will not affect the system
performance significantly, while a very small estimation error on the desired
channels can degrade the performance considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6552</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6552</id><created>2014-02-26</created><authors><author><keyname>Sahai</keyname><forenames>Ankur</forenames></author></authors><title>Renewable Energy Prediction using Weather Forecasts for Optimal
  Scheduling in HPC Systems</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of the GreenPAD project is to use green energy (wind, solar and
biomass) for powering data-centers that are used to run HPC jobs. As a part of
this it is important to predict the Renewable (Wind) energy for efficient
scheduling (executing jobs that require higher energy when there is more green
energy available and vice-versa). For predicting the wind energy we first
analyze the historical data to find a statistical model that gives relation
between wind energy and weather attributes. Then we use this model based on the
weather forecast data to predict the green energy availability in the future.
Using the green energy prediction obtained from the statistical model we are
able to precompute job schedules for maximizing the green energy utilization in
the future. We propose a model which uses live weather data in addition to
machine learning techniques (which can predict future deviations in weather
conditions based on current deviations from the forecast) to make on-the-fly
changes to the precomputed schedule (based on green energy prediction).
  For this we first analyze the data using histograms and simple statistical
tools such as correlation. In addition we build (correlation) regression model
for finding the relation between wind energy availability and weather
attributes (temperature, cloud cover, air pressure, wind speed / direction,
precipitation and sunshine). We also analyze different algorithms and machine
learning techniques for optimizing the job schedules for maximizing the green
energy utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6555</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6555</id><created>2014-02-26</created><authors><author><keyname>Jiang</keyname><forenames>J.</forenames></author><author><keyname>Li</keyname><forenames>W.</forenames></author><author><keyname>Cai</keyname><forenames>X.</forenames></author></authors><title>The effect of interdependence on the percolation of interdependent
  networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1016/j.physa.2014.05.065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two stochastic models are proposed to generate a system composed of two
interdependent scale-free (SF) or Erd\H{o}s-R\'{e}nyi (ER) networks where
interdependent nodes are connected with exponential or power-law relation, as
well as different dependence strength, respectively. Each subnetwork grows
through the addition of new nodes with constant accelerating random attachment
in the first model but with preferential attachment in the second model. Two
subnetworks interact with multi-support and undirectional dependence links. The
effect of dependence relations and strength between subnetworks are analyzed in
the percolation behavior of fully interdependent networks against random
failure, both theoretically and numerically, and as a result, for both
relations: interdependent SF networks show a second-order percolation phase
transition and increased dependence strength decreases the robustness of the
system, whereas, interdependent ER networks show the opposite results. In
addition, power-law relation between networks yields greater robustness than
exponential one at given dependence strength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6556</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6556</id><created>2014-02-26</created><authors><author><keyname>Patcas</keyname><forenames>Csaba</forenames></author><author><keyname>Bartha</keyname><forenames>Attila</forenames></author></authors><title>Evolutionary solving of the debts' clearing problem</title><categories>cs.NE cs.AI</categories><comments>13 pages, 5 figures</comments><msc-class>97R40</msc-class><acm-class>I.2.8; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The debts' clearing problem is about clearing all the debts in a group of n
entities (persons, companies etc.) using a minimal number of money transaction
operations. The problem is known to be NP-hard in the strong sense. As for many
intractable problems, techniques from the field of artificial intelligence are
useful in finding solutions close to optimum for large inputs. An evolutionary
algorithm for solving the debts' clearing problem is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6560</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6560</id><created>2014-02-26</created><authors><author><keyname>Roca-Lacostena</keyname><forenames>Jordi</forenames></author><author><keyname>Cerquides</keyname><forenames>Jesus</forenames></author></authors><title>Even more generic solution construction in Valuation-Based Systems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valuation algebras abstract a large number of formalisms for automated
reasoning and enable the definition of generic inference procedures. Many of
these formalisms provide some notions of solutions. Typical examples are
satisfying assignments in constraint systems, models in logics or solutions to
linear equation systems.
  Recently, formal requirements for the presence of solutions and a generic
algorithm for solution construction based on the results of a previously
executed inference scheme have been proposed in the literature. Unfortunately,
the formalization of Pouly and Kohlas relies on a theorem for which we provide
a counter example. In spite of that, the mainline of the theory described is
correct, although some of the necessary conditions to apply some of the
algorithms have to be revised. To fix the theory, we generalize some of their
definitions and provide correct sufficient conditions for the algorithms. As a
result, we get a more general and corrected version of the already existing
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6573</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6573</id><created>2014-02-25</created><updated>2014-05-06</updated><authors><author><keyname>Li</keyname><forenames>Ming-Xia</forenames><affiliation>ECUST</affiliation></author><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames><affiliation>ECUST</affiliation></author><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames><affiliation>ECUST</affiliation></author><author><keyname>Miccich&#xe8;</keyname><forenames>Salvatore</forenames><affiliation>Univ Palermo</affiliation></author><author><keyname>Tumminello</keyname><forenames>Michele</forenames><affiliation>Univ Palermo</affiliation></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames><affiliation>ECUST</affiliation></author><author><keyname>Mantegna</keyname><forenames>Rosario N.</forenames><affiliation>Univ Palermo and CEU</affiliation></author></authors><title>A comparative analysis of the statistical properties of large mobile
  phone calling networks</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Scientific Reports 4, 5132 (2014)</journal-ref><doi>10.1038/srep05132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phone calling is one of the most widely used communication methods in
modern society. The records of calls among mobile phone users provide us a
valuable proxy for the understanding of human communication patterns embedded
in social networks. Mobile phone users call each other forming a directed
calling network. If only reciprocal calls are considered, we obtain an
undirected mutual calling network. The preferential communication behavior
between two connected users can be statistically tested and it results in two
Bonferroni networks with statistically validated edges. We perform a
comparative analysis of the statistical properties of these four networks,
which are constructed from the calling records of more than nine million
individuals in Shanghai over a period of 110 days. We find that these networks
share many common structural properties and also exhibit idiosyncratic features
when compared with previously studied large mobile calling networks. The
empirical findings provide us an intriguing picture of a representative large
social network that might shed new lights on the modelling of large social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6601</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6601</id><created>2014-02-26</created><updated>2014-04-25</updated><authors><author><keyname>Bleuse</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Gautier</keyname><forenames>Thierry</forenames></author><author><keyname>Lima</keyname><forenames>Jo&#xe3;o V. F.</forenames></author><author><keyname>Mouni&#xe9;</keyname><forenames>Gr&#xe9;gory</forenames></author><author><keyname>Trystram</keyname><forenames>Denis</forenames></author></authors><title>Scheduling data flow program in xkaapi: A new affinity based Algorithm
  for Heterogeneous Architectures</title><categories>cs.DC</categories><doi>10.1007/978-3-319-09873-9_47</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Efficient implementations of parallel applications on heterogeneous hybrid
architectures require a careful balance between computations and communications
with accelerator devices. Even if most of the communication time can be
overlapped by computations, it is essential to reduce the total volume of
communicated data. The literature therefore abounds with ad-hoc methods to
reach that balance, but that are architecture and application dependent. We
propose here a generic mechanism to automatically optimize the scheduling
between CPUs and GPUs, and compare two strategies within this mechanism: the
classical Heterogeneous Earliest Finish Time (HEFT) algorithm and our new,
parametrized, Distributed Affinity Dual Approximation algorithm (DADA), which
consists in grouping the tasks by affinity before running a fast dual
approximation. We ran experiments on a heterogeneous parallel machine with six
CPU cores and eight NVIDIA Fermi GPUs. Three standard dense linear algebra
kernels from the PLASMA library have been ported on top of the Xkaapi runtime.
We report their performances. It results that HEFT and DADA perform well for
various experimental conditions, but that DADA performs better for larger
systems and number of GPUs, and, in most cases, generates much lower data
transfers than HEFT to achieve the same performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6610</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6610</id><created>2014-02-23</created><authors><author><keyname>Holik</keyname><forenames>Lukas</forenames></author><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author></authors><title>Proceedings 15th International Workshop on Verification of
  Infinite-State Systems</title><categories>cs.FL cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014</journal-ref><doi>10.4204/EPTCS.140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of Infinity'13, the 15th International
Workshop on Verification of Infinite-State Systems, which was held in Hanoi,
Vietnam on the 14th of October 2013 as a satellite event of ATVA'13. The aim of
the INFINITY workshop is to provide a forum for researchers interested in the
development of formal methods and algorithmic techniques for the analysis of
systems with infinitely many states, and their application in automated
verification of complex software and hardware systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6633</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6633</id><created>2014-02-26</created><authors><author><keyname>Nourian</keyname><forenames>Mojtaba</forenames></author><author><keyname>Leong</keyname><forenames>Alex S.</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author></authors><title>An Optimal Transmission Strategy for Kalman Filtering over Packet
  Dropping Links with Imperfect Acknowledgements</title><categories>math.OC cs.IT math.IT</categories><comments>Conditionally accepted in IEEE Transactions on Control of Network
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel design methodology for optimal transmission
policies at a smart sensor to remotely estimate the state of a stable linear
stochastic dynamical system. The sensor makes measurements of the process and
forms estimates of the state using a local Kalman filter. The sensor transmits
quantized information over a packet dropping link to the remote receiver. The
receiver sends packet receipt acknowledgments back to the sensor via an
erroneous feedback communication channel which is itself packet dropping. The
key novelty of this formulation is that the smart sensor decides, at each
discrete time instant, whether to transmit a quantized version of either its
local state estimate or its local innovation. The objective is to design
optimal transmission policies in order to minimize a long term average cost
function as a convex combination of the receiver's expected estimation error
covariance and the energy needed to transmit the packets. The optimal
transmission policy is obtained by the use of dynamic programming techniques.
Using the concept of submodularity, the optimality of a threshold policy in the
case of scalar systems with perfect packet receipt acknowledgments is proved.
Suboptimal solutions and their structural results are also discussed. Numerical
results are presented illustrating the performance of the optimal and
suboptimal transmission policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6635</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6635</id><created>2014-02-22</created><authors><author><keyname>Korolkova</keyname><forenames>A. V.</forenames></author><author><keyname>Kulyabov</keyname><forenames>D. S.</forenames></author><author><keyname>Sevastyanov</keyname><forenames>L. A.</forenames></author></authors><title>Tensor computations in computer algebra systems</title><categories>cs.SC cs.MS gr-qc</categories><comments>in Russian; in English</comments><journal-ref>A. V. Korol'kova, D. S. Kulyabov, and L. A. Sevast'yanov. Tensor
  computations in computer algebra systems. Programming and Computer Software,
  39(3):135--142, 2013</journal-ref><doi>10.1134/S0361768813030031</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper considers three types of tensor computations. On their basis, we
attempt to formulate criteria that must be satisfied by a computer algebra
system dealing with tensors. We briefly overview the current state of tensor
computations in different computer algebra systems. The tensor computations are
illustrated with appropriate examples implemented in specific systems: Cadabra
and Maxima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6636</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6636</id><created>2014-02-19</created><authors><author><keyname>Rice</keyname><forenames>Iain</forenames></author><author><keyname>Benton</keyname><forenames>Roger</forenames></author><author><keyname>Hart</keyname><forenames>Les</forenames></author><author><keyname>Lowe</keyname><forenames>David</forenames></author></authors><title>Analysis of Multibeam SONAR Data using Dissimilarity Representations</title><categories>cs.CE stat.ML</categories><comments>Presented at IMA Mathematics in Defence 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of low-dimensional visualisation of very
high dimensional information sources for the purpose of situation awareness in
the maritime environment. In response to the requirement for human decision
support aids to reduce information overload (and specifically, data amenable to
inter-point relative similarity measures) appropriate to the below-water
maritime domain, we are investigating a preliminary prototype topographic
visualisation model. The focus of the current paper is on the mathematical
problem of exploiting a relative dissimilarity representation of signals in a
visual informatics mapping model, driven by real-world sonar systems. An
independent source model is used to analyse the sonar beams from which a simple
probabilistic input model to represent uncertainty is mapped to a latent
visualisation space where data uncertainty can be accommodated. The use of
euclidean and non-euclidean measures are used and the motivation for future use
of non-euclidean measures is made. Concepts are illustrated using a simulated
64 beam weak SNR dataset with realistic sonar targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6650</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6650</id><created>2014-02-26</created><authors><author><keyname>Sahlol</keyname><forenames>Ahmed</forenames></author><author><keyname>Suen</keyname><forenames>Cheng</forenames></author></authors><title>A Novel Method for the Recognition of Isolated Handwritten Arabic
  Characters</title><categories>cs.CV</categories><comments>Indicate 13 pages, 5 figures</comments><msc-class>68T10</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are many difficulties facing a handwritten Arabic recognition system
such as unlimited variation in human handwriting, similarities of distinct
character shapes, interconnections of neighbouring characters and their
position in the word. The typical Optical Character Recognition (OCR) systems
are based mainly on three stages, preprocessing, features extraction and
recognition. This paper proposes new methods for handwritten Arabic character
recognition which is based on novel preprocessing operations including
different kinds of noise removal also different kind of features like
structural, Statistical and Morphological features from the main body of the
character and also from the secondary components. Evaluation of the accuracy of
the selected features is made. The system was trained and tested by back
propagation neural network with CENPRMI dataset. The proposed algorithm
obtained promising results as it is able to recognize 88% of our test set
accurately. In Comparable with other related works we find that our result is
the highest among other published works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6658</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6658</id><created>2014-02-26</created><authors><author><keyname>Huang</keyname><forenames>Ming-Deh</forenames></author><author><keyname>Narayanan</keyname><forenames>Anand Kumar</forenames></author></authors><title>Computing discrete logarithms in subfields of residue class rings</title><categories>cs.CC cs.CR cs.SC math.NT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.1674</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent breakthrough methods \cite{gggz,joux,bgjt} on computing discrete
logarithms in small characteristic finite fields share an interesting feature
in common with the earlier medium prime function field sieve method \cite{jl}.
To solve discrete logarithms in a finite extension of a finite field $\F$, a
polynomial $h(x) \in \F[x]$ of a special form is constructed with an
irreducible factor $g(x) \in \F[x]$ of the desired degree. The special form of
$h(x)$ is then exploited in generating multiplicative relations that hold in
the residue class ring $\F[x]/h(x)\F[x]$ hence also in the target residue class
field $\F[x]/g(x)\F[x]$. An interesting question in this context and addressed
in this paper is: when and how does a set of relations on the residue class
ring determine the discrete logarithms in the finite fields contained in it? We
give necessary and sufficient conditions for a set of relations on the residue
class ring to determine discrete logarithms in the finite fields contained in
it. We also present efficient algorithms to derive discrete logarithms from the
relations when the conditions are met. The derived necessary conditions allow
us to clearly identify structural obstructions intrinsic to the special
polynomial $h(x)$ in each of the aforementioned methods, and propose
modifications to the selection of $h(x)$ so as to avoid obstructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6663</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6663</id><created>2014-02-26</created><authors><author><keyname>De Loor</keyname><forenames>Pierre</forenames></author><author><keyname>Manach</keyname><forenames>Kristen</forenames></author><author><keyname>Tisseau</keyname><forenames>Jacques</forenames></author></authors><title>Enaction-Based Artificial Intelligence: Toward Coevolution with Humans
  in the Loop</title><categories>cs.AI nlin.AO</categories><journal-ref>Minds and Machine, num 19, pp 319-343, 2009</journal-ref><doi>10.1007/s11023-009-9165-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the links between the enaction paradigm and
artificial intelligence. Enaction is considered a metaphor for artificial
intelligence, as a number of the notions which it deals with are deemed
incompatible with the phenomenal field of the virtual. After explaining this
stance, we shall review previous works regarding this issue in terms of
artifical life and robotics. We shall focus on the lack of recognition of
co-evolution at the heart of these approaches. We propose to explicitly
integrate the evolution of the environment into our approach in order to refine
the ontogenesis of the artificial system, and to compare it with the enaction
paradigm. The growing complexity of the ontogenetic mechanisms to be activated
can therefore be compensated by an interactive guidance system emanating from
the environment. This proposition does not however resolve that of the
relevance of the meaning created by the machine (sense-making). Such
reflections lead us to integrate human interaction into this environment in
order to construct relevant meaning in terms of participative artificial
intelligence. This raises a number of questions with regards to setting up an
enactive interaction. The article concludes by exploring a number of issues,
thereby enabling us to associate current approaches with the principles of
morphogenesis, guidance, the phenomenology of interactions and the use of
minimal enactive interfaces in setting up experiments which will deal with the
problem of artificial intelligence in a variety of enaction-based ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6675</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6675</id><created>2014-02-26</created><updated>2015-09-29</updated><authors><author><keyname>Vaccon</keyname><forenames>Tristan</forenames><affiliation>IRMAR</affiliation></author></authors><title>Matrix-F5 algorithms and tropical Gr\&quot;obner bases computation</title><categories>cs.SC math.AC</categories><proxy>ccsd</proxy><journal-ref>International Symposium on Symbolic and Algebraic Computation,
  ISSAC 2015, Jul 2015, Bath, United Kingdom</journal-ref><doi>10.1145/2755996.2756665</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $K$ be a field equipped with a valuation. Tropical varieties over $K$ can
be defined with a theory of Gr\&quot;obner bases taking into account the valuation
of $K$. Because of the use of the valuation, this theory is promising for
stable computations over polynomial rings over a $p$-adic fields.We design a
strategy to compute such tropical Gr\&quot;obner bases by adapting the Matrix-F5
algorithm. Two variants of the Matrix-F5 algorithm, depending on how the
Macaulay matrices are built, are available to tropical computation with
respective modifications. The former is more numerically stable while the
latter is faster.Our study is performed both over any exact field with
valuation and some inexact fields like $\mathbb{Q}\_p$ or $\mathbb{F}\_q
\llbracket t \rrbracket.$ In the latter case, we track the loss in precision,
and show that the numerical stability can compare very favorably to the case of
classical Gr\&quot;obner bases when the valuation is non-trivial. Numerical examples
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6690</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6690</id><created>2014-02-26</created><authors><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Chen</keyname><forenames>Jilin</forenames></author><author><keyname>Nichols</keyname><forenames>Jeffrey</forenames></author></authors><title>Why Are You More Engaged? Predicting Social Engagement from Word Use</title><categories>cs.SI cs.CL cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a study to analyze how word use can predict social engagement
behaviors such as replies and retweets in Twitter. We compute psycholinguistic
category scores from word usage, and investigate how people with different
scores exhibited different reply and retweet behaviors on Twitter. We also
found psycholinguistic categories that show significant correlations with such
social engagement behaviors. In addition, we have built predictive models of
replies and retweets from such psycholinguistic category based features. Our
experiments using a real world dataset collected from Twitter validates that
such predictions can be done with reasonable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6692</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6692</id><created>2014-02-26</created><authors><author><keyname>Sutar</keyname><forenames>Shiv H.</forenames></author><author><keyname>Khade</keyname><forenames>Akshata H.</forenames></author></authors><title>Recommendation System for Outfit Selection (RSOS)</title><categories>cs.OH</categories><comments>6 pages,9 figures,5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a system which will be able to recommend the user to choose
appropriate outfits suits to their personality. The necessity of this system is
to reduce the outfit selection and purchasing time; this will also help to
create tailor made outfits as per the personality traits. The guidelines for
selection of their respective outfits are based upon various bodily parameters
that evolve with the learning of available labeled and unlabeled data. The
system is based on two modules of processes; first one is to recognize the
features for usage of outfits like traditional, western, functional, daytime or
night etc, second is to calculate the body measurement parameters. The proposed
system will have image capturing by using HAAR feature or input device for
getting body parameters. We intend to classify and extract the best possible
outfits from the system by using HIGEN MINER algorithm. The applications of
outfit selection will be ranging from manual gender selection, image processing
with body feature extractions, Value comparison with database by using
different statistical techniques and data mining algorithms. After that it will
recommend best outfits as per body parameters, inputs and availability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6693</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6693</id><created>2014-02-26</created><authors><author><keyname>Nourian</keyname><forenames>Mojtaba</forenames></author><author><keyname>Leong</keyname><forenames>Alex S.</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Optimal Energy Allocation for Kalman Filtering over Packet Dropping
  Links with Imperfect Acknowledgments and Energy Harvesting Constraints</title><categories>math.OC cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Automatic Control. arXiv admin
  note: text overlap with arXiv:1402.6633</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a design methodology for optimal transmission energy
allocation at a sensor equipped with energy harvesting technology for remote
state estimation of linear stochastic dynamical systems. In this framework, the
sensor measurements as noisy versions of the system states are sent to the
receiver over a packet dropping communication channel. The packet dropout
probabilities of the channel depend on both the sensor's transmission energies
and time varying wireless fading channel gains. The sensor has access to an
energy harvesting source which is an everlasting but unreliable energy source
compared to conventional batteries with fixed energy storages. The receiver
performs optimal state estimation with random packet dropouts to minimize the
estimation error covariances based on received measurements. The receiver also
sends packet receipt acknowledgments to the sensor via an erroneous feedback
communication channel which is itself packet dropping.
  The objective is to design optimal transmission energy allocation at the
energy harvesting sensor to minimize either a finite-time horizon sum or a long
term average (infinite-time horizon) of the trace of the expected estimation
error covariance of the receiver's Kalman filter. These problems are formulated
as Markov decision processes with imperfect state information. The optimal
transmission energy allocation policies are obtained by the use of dynamic
programming techniques. Using the concept of submodularity, the structure of
the optimal transmission energy policies are studied. Suboptimal solutions are
also discussed which are far less computationally intensive than optimal
solutions. Numerical simulation results are presented illustrating the
performance of the energy allocation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6712</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6712</id><created>2014-02-24</created><authors><author><keyname>Franceschet</keyname><forenames>Massimo</forenames></author></authors><title>Complex Beauty</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems and their underlying convoluted networks are ubiquitous, all
we need is an eye for them. They pose problems of organized complexity which
cannot be approached with a reductionist method. Complexity science and its
emergent sister network science both come to grips with the inherent complexity
of complex systems with an holistic strategy. The relevance of complexity,
however, transcends the sciences. Complex systems and networks are the focal
point of a philosophical, cultural and artistic turn of our tightly
interrelated and interdependent postmodern society. Here I take a different,
aesthetic perspective on complexity. I argue that complex systems can be
beautiful and can the object of artification - the neologism refers to
processes in which something that is not regarded as art in the traditional
sense of the word is changed into art. Complex systems and networks are
powerful sources of inspiration for the generative designer, for the artful
data visualizer, as well as for the traditional artist. I finally discuss the
benefits of a cross-fertilization between science and art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6713</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6713</id><created>2014-02-26</created><authors><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames><affiliation>Sterrewacht Leiden</affiliation></author><author><keyname>Boekholt</keyname><forenames>Tjarda</forenames><affiliation>Sterrewacht Leiden</affiliation></author></authors><title>On the minimal accuracy required for simulating self-gravitating systems
  by means of direct N-body methods</title><categories>astro-ph.IM cs.CC</categories><comments>ApJ Letters (accepted for publication)</comments><doi>10.1088/2041-8205/785/1/L3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conservation of energy, linear momentum and angular momentum are
important drivers for our physical understanding of the evolution of the
Universe. These quantities are also conserved in Newton's laws of motion under
gravity \citep{Newton:1687}. Numerical integration of the associated equations
of motion is extremely challenging, in particular due to the steady growth of
numerical errors (by round-off and discrete time-stepping,
\cite{1981PAZh....7..752B,1993ApJ...415..715G,1993ApJ...402L..85H,1994LNP...430..131M})
and the exponential divergence \citep{1964ApJ...140..250M,2009MNRAS.392.1051U}
between two nearby solution. As a result, numerical solutions to the general
N-body problem are intrinsically questionable
\citep{2003gmbp.book.....H,1994JAM....61..226L}. Using brute force integrations
to arbitrary numerical precision we demonstrate empirically that ensembles of
different realizations of resonant 3-body interactions produce statistically
indistinguishable results. Although individual solutions using common
integration methods are notoriously unreliable, we conjecture that an ensemble
of approximate 3-body solutions accurately represents an ensemble of true
solutions, so long as the energy during integration is conserved to better than
1/10. We therefore provide an independent confirmation that previous work on
self-gravitating systems can actually be trusted, irrespective of the intrinsic
chaotic nature of the N-body problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6742</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6742</id><created>2014-02-26</created><authors><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Blanc</keyname><forenames>Coralie</forenames></author><author><keyname>Martin</keyname><forenames>Florent</forenames></author><author><keyname>Bornand</keyname><forenames>Pierre</forenames></author><author><keyname>Massonnat</keyname><forenames>Sandra</forenames></author><author><keyname>Gattaz</keyname><forenames>Olivier</forenames></author><author><keyname>Emin</keyname><forenames>Patrick</forenames></author></authors><title>CRISTAL-ISE : Provenance Applied in Industry</title><categories>cs.DB cs.SE</categories><comments>6 pages, 3 figures; Presented at the 16th International Conference on
  Enterprise Information Systems (ICEIS 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the CRISTAL-iSE project as a framework for the management
of provenance information in industry. The project itself is a research
collaboration between academia and industry. A key factor in the project is the
use of a system known as CRISTAL which is a mature system based on proven
description driven principles. A crucial element in the description driven
approach is that the fact that objects (Items) are described at runtime
enabling managed systems to be both dynamic and flexible. Another factor is the
notion that all Items in CRISTAL are stored and versioned, therefore enabling a
provenance collection system. In this paper a concrete application, called
Agilium, is briefly described and a future application CIMAG-RA is presented
which will harness the power of both CRISTAL and Agilium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6757</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6757</id><created>2014-02-26</created><updated>2015-07-27</updated><authors><author><keyname>James</keyname><forenames>Oliver</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author></authors><title>Concise Probability Distributions of Eigenvalues of Real-Valued Wishart
  Matrices</title><categories>cs.IT math.IT</categories><comments>Submitted to Math Journal, 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of deriving new eigenvalue
distributions of real-valued Wishart matrices that arises in many scientific
and engineering applications. The distributions are derived using the tools
from the theory of skew symmetric matrices. In particular, we relate the
multiple integrals of a determinant, which arises while finding the eigenvalue
distributions, in terms of the Pfaffian of skew-symmetric matrices. Pfaffians
being the square root of skew symmetric matrices are easy to compute than the
conventional distributions that involve Zonal polynomials or beta integrals. We
show that the plots of the derived distributions are exactly coinciding with
the numerically simulated plots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6763</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6763</id><created>2014-02-26</created><authors><author><keyname>Abbasi-Yadkori</keyname><forenames>Yasin</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author><author><keyname>Malek</keyname><forenames>Alan</forenames></author></authors><title>Linear Programming for Large-Scale Markov Decision Problems</title><categories>math.OC cs.AI cs.NA</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of controlling a Markov decision process (MDP) with a
large state space, so as to minimize average cost. Since it is intractable to
compete with the optimal policy for large scale problems, we pursue the more
modest goal of competing with a low-dimensional family of policies. We use the
dual linear programming formulation of the MDP average cost problem, in which
the variable is a stationary distribution over state-action pairs, and we
consider a neighborhood of a low-dimensional subset of the set of stationary
distributions (defined in terms of state-action features) as the comparison
class. We propose two techniques, one based on stochastic convex optimization,
and one based on constraint sampling. In both cases, we give bounds that show
that the performance of our algorithms approaches the best achievable by any
policy in the comparison class. Most importantly, these results depend on the
size of the comparison class, but not on the size of the state space.
Preliminary experiments show the effectiveness of the proposed algorithms in a
queuing application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6764</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6764</id><created>2014-02-26</created><authors><author><keyname>Haron</keyname><forenames>Hazlina</forenames></author><author><keyname>Ghani</keyname><forenames>Abdul Azim Abd.</forenames></author></authors><title>A method to identify potential ambiguous Malay words through Ambiguity
  Attributes mapping: An exploratory Study</title><categories>cs.SE cs.CL</categories><comments>Paper was presented at The Fourth International Conference of
  Computer Science and Information Technology (CCSIT2014)in Sydney, Australia
  on Feb 22, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe here a methodology to identify a list of ambiguous Malay words
that are commonly being used in Malay documentations such as Requirement
Specification. We compiled several relevant and appropriate requirement quality
attributes and sentence rules from previous literatures and adopt it to come
out with a set of ambiguity attributes that most suit Malay words. The
extracted Malay ambiguous words (potential) are then being mapped onto the
constructed ambiguity attributes to confirm their vagueness. The list is then
verified by Malay linguist experts. This paper aims to identify a list of
potential ambiguous words in Malay as an attempt to assist writers to avoid
using the vague words while documenting Malay Requirement Specification as well
as to any other related Malay documentation. The result of this study is a list
of 120 potential ambiguous Malay words that could act as guidelines in writing
Malay sentences
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6771</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6771</id><created>2014-02-26</created><updated>2014-12-31</updated><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Gao</keyname><forenames>Yun</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>On Linear Codes over $\mathbb{Z}_4+v\mathbb{Z}_4$</title><categories>cs.IT math.IT</categories><comments>25 pages</comments><msc-class>94B05, 94B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes are considered over the ring $\mathbb{Z}_4+v\mathbb{Z}_4$, where
$v^2=v$. Gray weight, Gray maps for linear codes are defined and MacWilliams
identity for the Gray weight enumerator is given. Self-dual codes, construction
of Euclidean isodual codes, unimodular complex lattices, MDS codes and MGDS
codes over $\mathbb{Z}_4+v\mathbb{Z}_4$ are studied. Cyclic codes and quadratic
residue codes are also considered. Finally, some examples for illustrating the
main work are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6775</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6775</id><created>2014-02-26</created><authors><author><keyname>Sarkar</keyname><forenames>Chandrima</forenames></author><author><keyname>Deshpande</keyname><forenames>Raamesh</forenames></author><author><keyname>Myers</keyname><forenames>Chad</forenames></author></authors><title>Analysis of Barcode sequence features to find anomalies due to
  amplification Bias</title><categories>cs.CE q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we aim at investigating whether barcode sequence features can
predict the read count ambiguities caused during PCR based next generation
sequencing techniques. The methodologies we used are mutual information based
motif discovery and Lasso regression technique using features generated from
the barcode sequence. The results indicate that there is a certain degree of
correlation between motifs discovered in the sequences and the read counts. Our
main contribution in this paper is a thorough investigation of the barcode
features that gave us useful information regarding the significance of the
sequence features and the sequence containing the discovered motifs in
prediction of read counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6778</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6778</id><created>2014-02-26</created><authors><author><keyname>Kwong</keyname><forenames>Man Kam</forenames></author></authors><title>Nonnegative Trigonometric Polynomials, Sturms Theorem, and Symbolic
  Computation</title><categories>math.CA cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explain a procedure based on a classical result of Sturm
that can be used to determine rigorously whether a given trigonometric
polynomial is nonnegative in a certain interval or not. Many examples are
given. This technique has been employed by the author in several recent works.
  The procedure often involves tedious computations that are time-consuming and
error-prone. Fortunately, symbolic computation software is available to
automate the procedure. In this paper, we give the details of its
implementation in MAPLE 13. Some who are strongly attached to a more
traditional theoretical research framework may find such details boring or even
consider computer-assisted proofs suspicious. However, we emphasize again that
the procedure is completely mathematically rigorous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6779</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6779</id><created>2014-02-26</created><updated>2015-07-31</updated><authors><author><keyname>Badanidiyuru</keyname><forenames>Ashwinkumar</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author></authors><title>Resourceful Contextual Bandits</title><categories>cs.LG cs.DS cs.GT</categories><comments>This is the full version of a paper in COLT 2014. Version history:
  (v2) Added some details to one of the proofs, (v3) a big revision following
  comments from COLT reviewers (but no new results), (v4) edits in related
  work, minor edits elsewhere. (v6) A correction for Theorem 3, corollary for
  contextual dynamic pricing with discretization; updated follow-up work &amp; open
  questions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study contextual bandits with ancillary constraints on resources, which
are common in real-world applications such as choosing ads or dynamic pricing
of items. We design the first algorithm for solving these problems that handles
constrained resources other than time, and improves over a trivial reduction to
the non-contextual case. We consider very general settings for both contextual
bandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits with
resource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)),
and prove a regret guarantee with near-optimal statistical properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6782</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6782</id><created>2014-02-26</created><authors><author><keyname>Schuster</keyname><forenames>Johann</forenames><affiliation>University of the Federal Armed Forces Munich Neubiberg, Germany</affiliation></author><author><keyname>Siegle</keyname><forenames>Markus</forenames><affiliation>University of the Federal Armed Forces Munich Neubiberg, Germany</affiliation></author></authors><title>Lattice structures for bisimilar Probabilistic Automata</title><categories>cs.FL cs.LO</categories><comments>In Proceedings INFINITY 2013, arXiv:1402.6610</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014, pp. 1-15</journal-ref><doi>10.4204/EPTCS.140.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper shows that there is a deep structure on certain sets of bisimilar
Probabilistic Automata (PA). The key prerequisite for these structures is a
notion of compactness of PA. It is shown that compact bisimilar PA form
lattices. These results are then used in order to establish normal forms not
only for finite automata, but also for infinite automata, as long as they are
compact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6783</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6783</id><created>2014-02-26</created><authors><author><keyname>Chen</keyname><forenames>Yu-Fang</forenames><affiliation>Academia Sinica, Taiwan</affiliation></author><author><keyname>Wang</keyname><forenames>Bow-Yaw</forenames><affiliation>Academia Sinica, Taiwan</affiliation></author><author><keyname>Yen</keyname><forenames>Di-De</forenames><affiliation>Academia Sinica, Taiwan</affiliation></author></authors><title>A Finite Exact Representation of Register Automata Configurations</title><categories>cs.FL cs.LO</categories><comments>In Proceedings INFINITY 2013, arXiv:1402.6610</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014, pp. 16-34</journal-ref><doi>10.4204/EPTCS.140.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A register automaton is a finite automaton with finitely many registers
ranging from an infinite alphabet. Since the valuations of registers are
infinite, there are infinitely many configurations. We describe a technique to
classify infinite register automata configurations into finitely many exact
representative configurations. Using the finitary representation, we give an
algorithm solving the reachability problem for register automata. We moreover
define a computation tree logic for register automata and solve its model
checking problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6784</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6784</id><created>2014-02-26</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Stenman</keyname><forenames>Jari</forenames><affiliation>Uppsala University</affiliation></author></authors><title>Zenoness for Timed Pushdown Automata</title><categories>cs.FL</categories><comments>In Proceedings INFINITY 2013, arXiv:1402.6610</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014, pp. 35-47</journal-ref><doi>10.4204/EPTCS.140.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timed pushdown automata are pushdown automata extended with a finite set of
real-valued clocks. Additionaly, each symbol in the stack is equipped with a
value representing its age. The enabledness of a transition may depend on the
values of the clocks and the age of the topmost symbol. Therefore, dense-timed
pushdown automata subsume both pushdown automata and timed automata. We have
previously shown that the reachability problem for this model is decidable. In
this paper, we study the zenoness problem and show that it is EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6785</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6785</id><created>2014-02-26</created><authors><author><keyname>Katz</keyname><forenames>Gal</forenames><affiliation>Bar Ilan University</affiliation></author><author><keyname>Peled</keyname><forenames>Doron</forenames><affiliation>Bar Ilan University</affiliation></author></authors><title>Synthesis of Parametric Programs using Genetic Programming and Model
  Checking</title><categories>cs.SE cs.AI cs.NE</categories><comments>In Proceedings INFINITY 2013, arXiv:1402.6610</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014, pp. 70-84</journal-ref><doi>10.4204/EPTCS.140.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal methods apply algorithms based on mathematical principles to enhance
the reliability of systems. It would only be natural to try to progress from
verification, model checking or testing a system against its formal
specification into constructing it automatically. Classical algorithmic
synthesis theory provides interesting algorithms but also alarming high
complexity and undecidability results. The use of genetic programming, in
combination with model checking and testing, provides a powerful heuristic to
synthesize programs. The method is not completely automatic, as it is fine
tuned by a user that sets up the specification and parameters. It also does not
guarantee to always succeed and converge towards a solution that satisfies all
the required properties. However, we applied it successfully on quite
nontrivial examples and managed to find solutions to hard programming
challenges, as well as to improve and to correct code. We describe here several
versions of our method for synthesizing sequential and concurrent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6787</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6787</id><created>2014-02-26</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Riquelme</keyname><forenames>Carlos</forenames></author><author><keyname>Schmit</keyname><forenames>Sven</forenames></author></authors><title>Learning multifractal structure in large networks</title><categories>cs.SI</categories><acm-class>H.4.0; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating random graphs to model networks has a rich history. In this paper,
we analyze and improve upon the multifractal network generator (MFNG)
introduced by Palla et al. We provide a new result on the probability of
subgraphs existing in graphs generated with MFNG. From this result it follows
that we can quickly compute moments of an important set of graph properties,
such as the expected number of edges, stars, and cliques. Specifically, we show
how to compute these moments in time complexity independent of the size of the
graph and the number of recursive levels in the generative model. We leverage
this theory to a new method of moments algorithm for fitting large networks to
MFNG. Empirically, this new approach effectively simulates properties of
several social and information networks. In terms of matching subgraph counts,
our method outperforms similar algorithms used with the Stochastic Kronecker
Graph model. Furthermore, we present a fast approximation algorithm to generate
graph instances following the multi- fractal structure. The approximation
scheme is an improvement over previous methods, which ran in time complexity
quadratic in the number of vertices. Combined, our method of moments and fast
sampling scheme provide the first scalable framework for effectively modeling
large networks with MFNG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6792</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6792</id><created>2014-02-27</created><authors><author><keyname>Adamic</keyname><forenames>Lada A.</forenames></author><author><keyname>Lento</keyname><forenames>Thomas M.</forenames></author><author><keyname>Adar</keyname><forenames>Eytan</forenames></author><author><keyname>Ng</keyname><forenames>Pauline C.</forenames></author></authors><title>Information Evolution in Social Networks</title><categories>cs.SI cs.CL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks readily transmit information, albeit with less than perfect
fidelity. We present a large-scale measurement of this imperfect information
copying mechanism by examining the dissemination and evolution of thousands of
memes, collectively replicated hundreds of millions of times in the online
social network Facebook. The information undergoes an evolutionary process that
exhibits several regularities. A meme's mutation rate characterizes the
population distribution of its variants, in accordance with the Yule process.
Variants further apart in the diffusion cascade have greater edit distance, as
would be expected in an iterative, imperfect replication process. Some text
sequences can confer a replicative advantage; these sequences are abundant and
transfer &quot;laterally&quot; between different memes. Subpopulations of the social
network can preferentially transmit a specific variant of a meme if the variant
matches their beliefs or culture. Understanding the mechanism driving change in
diffusing information has important implications for how we interpret and
harness the information that reaches us through our social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6794</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6794</id><created>2014-02-27</created><updated>2014-11-24</updated><authors><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Kim</keyname><forenames>Taeyoung</forenames></author></authors><title>Trellis-Extended Codebooks and Successive Phase Adjustment: A Path from
  LTE-Advanced to FDD Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 11 figures, accepted to IEEE Transactions on Wireless
  Communications, Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is of great interest to develop efficient ways to acquire accurate channel
state information (CSI) for frequency division duplexing (FDD) massive
multiple-input multiple-output (MIMO) systems for backward compatibility. It is
theoretically well known that the codebook size for CSI quantization should be
increased as the number of transmit antennas becomes larger, and 3GPP long term
evolution (LTE) and LTE-Advanced codebooks also follow this trend. Thus, in
massive MIMO, it is hard to apply the conventional approach of using
pre-defined vector-quantized codebooks for CSI quantization mainly because of
codeword search complexity. In this paper, we propose a trellis-extended
codebook (TEC) that can be easily harmonized with current wireless standards
such as LTE or LTE-Advanced by extending standardized codebooks designed for 2,
4, or 8 antennas with trellis structures. TEC exploits a Viterbi decoder and
convolutional encoder in channel coding as the CSI quantizer and the CSI
reconstructer, respectively. By quantizing multiple channel entries
simultaneously using standardized codebooks in a state transition of trellis
search, TEC can achieve fractional bits per channel entry quantization to have
a practical feedback overhead. Thus, TEC can solve both the complexity and the
feedback overhead issues of CSI quantization in massive MIMO systems. We also
develop trellis-extended successive phase adjustment (TE-SPA) which works as a
differential codebook of TEC. This is similar to the dual codebook concept of
LTE-Advanced. TE-SPA can reduce CSI quantization error even with lower feedback
overhead in temporally correlated channels. Numerical results verify the
effectiveness of the proposed schemes in FDD massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6799</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6799</id><created>2014-02-27</created><authors><author><keyname>Garner</keyname><forenames>Richard</forenames></author></authors><title>Combinatorial structure of type dependency</title><categories>math.LO cs.LO math.CT</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an account of the basic combinatorial structure underlying the notion
of type dependency. We do so by considering the category of all dependent
sequent calculi, and exhibiting it as the category of algebras for a monad on a
presheaf category. The objects of the presheaf category encode the basic
judgements of a dependent sequent calculus, while the action of the monad
encodes the deduction rules; so by giving an explicit description of the monad,
we obtain an explicit account of the combinatorics of type dependency. We find
that this combinatorics is controlled by a particular kind of decorated ordered
tree, familiar from computer science and from innocent game semantics.
Furthermore, we find that the monad at issue is of a particularly well-behaved
kind: it is local right adjoint in the sense of Street--Weber. In future work,
we will use this fact to describe nerves for dependent type theories, and to
study the coherence problem for dependent type theory using the tools of
two-dimensional monad theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6800</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6800</id><created>2014-02-27</created><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>Prakash</keyname><forenames>Ved</forenames></author></authors><title>An Improved Interactive Streaming Algorithm for the Distinct Elements
  Problem</title><categories>cs.CC</categories><comments>Submitted to ICALP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exact computation of the number of distinct elements (frequency moment
$F_0$) is a fundamental problem in the study of data streaming algorithms. We
denote the length of the stream by $n$ where each symbol is drawn from a
universe of size $m$. While it is well known that the moments $F_0,F_1,F_2$ can
be approximated by efficient streaming algorithms, it is easy to see that exact
computation of $F_0,F_2$ requires space $\Omega(m)$. In previous work, Cormode
et al. therefore considered a model where the data stream is also processed by
a powerful helper, who provides an interactive proof of the result. They gave
such protocols with a polylogarithmic number of rounds of communication between
helper and verifier for all functions in NC. This number of rounds
$\left(O(\log^2 m) \;\text{in the case of} \;F_0 \right)$ can quickly make such
protocols impractical.
  Cormode et al. also gave a protocol with $\log m +1$ rounds for the exact
computation of $F_0$ where the space complexity is $O\left(\log m \log n+\log^2
m\right)$ but the total communication $O\left(\sqrt{n}\log m\left(\log n+ \log
m \right)\right)$. They managed to give $\log m$ round protocols with
$\operatorname{polylog}(m,n)$ complexity for many other interesting problems
including $F_2$, Inner product, and Range-sum, but computing $F_0$ exactly with
polylogarithmic space and communication and $O(\log m)$ rounds remained open.
  In this work, we give a streaming interactive protocol with $\log m$ rounds
for exact computation of $F_0$ using $O\left(\log m \left(\,\log n + \log m
\log\log m\,\right)\right)$ bits of space and the communication is $O\left(
\log m \left(\,\log n +\log^3 m (\log\log m)^2 \,\right)\right)$. The update
time of the verifier per symbol received is $O(\log^2 m)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6804</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6804</id><created>2014-02-27</created><updated>2014-07-09</updated><authors><author><keyname>Kotek</keyname><forenames>Tomer</forenames></author><author><keyname>Simkus</keyname><forenames>Mantas</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author></authors><title>Extending ALCQIO with reachability</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a description logic ALCQIO_{b,Re} which adds reachability
assertions to ALCQIO, a sub-logic of the two-variable fragment of first order
logic with counting quantifiers. ALCQIO_{b,Re} is well-suited for applications
in software verification and shape analysis. Shape analysis requires expressive
logics which can express reachability and have good computational properties.
We show that ALCQIO_{b,Re} can describe complex data structures with a high
degree of sharing and allows compositions such as list of trees.
  We show that the finite satisfiability and implication problems of
ALCQIO_{b,Re}-formulae are polynomial-time reducible to finite satisfiability
of ALCQIO-formulae. As a consequence, we get that finite satisfiability and
finite implication in ALCQIO_{b,Re} are NEXPTIME-complete. Description logics
with transitive closure constructors have been studied before, but
ALCQIO_{b,Re} is the first description logic that remains decidable on finite
structures while allowing at the same time nominals, inverse roles, counting
quantifiers and reachability assertions,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6809</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6809</id><created>2014-02-27</created><authors><author><keyname>Ruj</keyname><forenames>Sushmita</forenames></author><author><keyname>Pal</keyname><forenames>Arindam</forenames></author></authors><title>Analyzing Cascading Failures in Smart Grids under Random and Targeted
  Attacks</title><categories>cs.SI cs.DM cs.NI math.CO physics.soc-ph</categories><comments>Accepted for publication in 28th IEEE International Conference on
  Advanced Information Networking and Applications (AINA) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model smart grids as complex interdependent networks, and study targeted
attacks on smart grids for the first time. A smart grid consists of two
networks: the power network and the communication network, interconnected by
edges. Occurrence of failures (attacks) in one network triggers failures in the
other network, and propagates in cascades across the networks. Such cascading
failures can result in disintegration of either (or both) of the networks.
Earlier works considered only random failures. In practical situations, an
attacker is more likely to compromise nodes selectively.
  We study cascading failures in smart grids, where an attacker selectively
compromises the nodes with probabilities proportional to their degrees; high
degree nodes are compromised with higher probability. We mathematically analyze
the sizes of the giant components of the networks under targeted attacks, and
compare the results with the corresponding sizes under random attacks. We show
that networks disintegrate faster for targeted attacks compared to random
attacks. A targeted attack on a small fraction of high degree nodes
disintegrates one or both of the networks, whereas both the networks contain
giant components for random attack on the same fraction of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6835</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6835</id><created>2014-02-27</created><authors><author><keyname>Saito</keyname><forenames>Hiroshi</forenames></author></authors><title>Spatial Design of Physical Network Robust against Earthquakes</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1312.7187</comments><doi>10.1109/JLT.2014.2385100</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the survivability of a physical network against
earthquakes and proposes spatial network design rules to make a network robust
against earthquakes. The disaster area model used is fairly generic and
bounded. The proposed design rules for physical networks include: (i) a shorter
zigzag route can reduce the probability that a network intersects a disaster
area, (ii) an additive performance metric, such as repair cost, is independent
of the network shape if the route length is fixed, and (iii) additional routes
within a ring network does not decrease the probability that all the routes
between a given pair of nodes intersect the disaster area, but a wider detour
route decreases it. Formulas for evaluating the probability of disconnecting
two given nodes are also derived. An optimal server placement is shown as an
application of the theoretical results. These analysis results are validated
through empirical earthquake data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6859</identifier>
 <datestamp>2014-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6859</id><created>2014-02-27</created><authors><author><keyname>Marghny</keyname><forenames>M. H.</forenames></author><author><keyname>Taloba</keyname><forenames>Ahmed I.</forenames></author></authors><title>Outlier Detection using Improved Genetic K-means</title><categories>cs.LG cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The outlier detection problem in some cases is similar to the classification
problem. For example, the main concern of clustering-based outlier detection
algorithms is to find clusters and outliers, which are often regarded as noise
that should be removed in order to make more reliable clustering. In this
article, we present an algorithm that provides outlier detection and data
clustering simultaneously. The algorithmimprovesthe estimation of centroids of
the generative distribution during the process of clustering and outlier
discovery. The proposed algorithm consists of two stages. The first stage
consists of improved genetic k-means algorithm (IGK) process, while the second
stage iteratively removes the vectors which are far from their cluster
centroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6862</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6862</id><created>2014-02-27</created><updated>2014-03-24</updated><authors><author><keyname>Keshtkaran</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Yang</keyname><forenames>Zhi</forenames></author></authors><title>A Fast, robust algorithm for power line interference cancellation in
  neural recording</title><categories>cs.SY physics.med-ph</categories><comments>18 pages, 21 figures, to appear in Journal of Neural Engineering</comments><journal-ref>M. R. Keshtkaran and Z. Yang, &quot;A fast, robust algorithm for power
  line interference cancellation in neural recording,&quot; J. Neural Eng., vol. 11,
  no. 2, p. 026017, Apr. 2014</journal-ref><doi>10.1088/1741-2560/11/2/026017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power line interference may severely corrupt neural recordings at 50/60 Hz
and harmonic frequencies. In this paper, we present a robust and
computationally efficient algorithm for removing power line interference from
neural recordings. The algorithm includes four steps. First, an adaptive notch
filter is used to estimate the fundamental frequency of the interference.
Subsequently, based on the estimated frequency, harmonics are generated by
using discrete-time oscillators, and then the amplitude and phase of each
harmonic are estimated through using a modified recursive least squares
algorithm. Finally, the estimated interference is subtracted from the recorded
data. The algorithm does not require any reference signal, and can track the
frequency, phase, and amplitude of each harmonic. When benchmarked with other
popular approaches, our algorithm performs better in terms of noise immunity,
convergence speed, and output signal-to-noise ratio (SNR). While minimally
affecting the signal bands of interest, the algorithm consistently yields fast
convergence and substantial interference rejection in different conditions of
interference strengths (input SNR from -30 dB to 30 dB), power line frequencies
(45-65 Hz), and phase and amplitude drifts. In addition, the algorithm features
a straightforward parameter adjustment since the parameters are independent of
the input SNR, input signal power, and the sampling rate. The proposed
algorithm features a highly robust operation, fast adaptation to interference
variations, significant SNR improvement, low computational complexity and
memory requirement, and straightforward parameter adjustment. These features
render the algorithm suitable for wearable and implantable sensor applications,
where reliable and real-time cancellation of the interference is desired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6865</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6865</id><created>2014-02-27</created><authors><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Applications of Structural Balance in Signed Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>37 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present measures, models and link prediction algorithms based on the
structural balance in signed social networks. Certain social networks contain,
in addition to the usual 'friend' links, 'enemy' links. These networks are
called signed social networks. A classical and major concept for signed social
networks is that of structural balance, i.e., the tendency of triangles to be
'balanced' towards including an even number of negative edges, such as
friend-friend-friend and friend-enemy-enemy triangles. In this article, we
introduce several new signed network analysis methods that exploit structural
balance for measuring partial balance, for finding communities of people based
on balance, for drawing signed social networks, and for solving the problem of
link prediction. Notably, the introduced methods are based on the signed graph
Laplacian and on the concept of signed resistance distances. We evaluate our
methods on a collection of four signed social network datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6880</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6880</id><created>2014-02-27</created><authors><author><keyname>Keane</keyname><forenames>M. T.</forenames></author><author><keyname>Gerow</keyname><forenames>A.</forenames></author></authors><title>It's distributions all the way down!: Second order changes in
  statistical distributions also occur</title><categories>cs.CL</categories><journal-ref>Behavioral &amp; Brain Sciences, 2014, 37(1), 87</journal-ref><doi>10.1017/S0140525X13001763</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The textual, big-data literature misses Bentley, OBrien, &amp; Brocks (Bentley et
als) message on distributions; it largely examines the first-order effects of
how a single, signature distribution can predict population behaviour,
neglecting second-order effects involving distributional shifts, either between
signature distributions or within a given signature distribution. Indeed,
Bentley et al. themselves under-emphasise the potential richness of the latter,
within-distribution effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6882</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6882</id><created>2014-02-27</created><updated>2014-07-08</updated><authors><author><keyname>Zhang</keyname><forenames>Minglong</forenames></author><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Liew</keyname><forenames>Soung-Chang</forenames></author></authors><title>An Optimal Decoding Strategy for Physical-layer Network Coding over
  Multipath Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an optimal decoder for physical-layer network coding (PNC) in a
multipath fading channels. Previous studies on PNC have largely focused on the
single path case. For PNC, multipath not only introduces inter-symbol
interference (ISI), but also cross-symbol interference (Cross-SI) between
signals simultaneously transmitted by multiple users. In this paper, we assume
the transmitters do not have channel state information (CSI). The relay in the
PNC system, however, has CSI. The relay makes use of a belief propagation (BP)
algorithm to decode the multipath-distorted signals received from multiple
users into a network-coded packet. We refer to our multipath decoding algorithm
as MP-PNC. Our simulation results show that, benchmarked against synchronous
PNC over a one-path channel, the bit error rate (BER) performance penalty of
MP-PNC under a two-tap ITU channel model can be kept within 0.5dB. Moreover, it
outperforms a MUD-XOR algorithm by 3dB -- MUD-XOR decodes the individual
information from both users explicitly before performing the XOR network-coding
mapping. Although the framework of fading-channel PNC presented in this paper
is demonstrated based on two-path and three-path channel models, our algorithm
can be easily extended to cases with more than three paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6888</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6888</id><created>2014-02-27</created><authors><author><keyname>Erskine</keyname><forenames>Adam</forenames></author><author><keyname>Herrmann</keyname><forenames>J Michael</forenames></author></authors><title>CriPS: Critical Dynamics in Particle Swarm Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle Swarm Optimisation (PSO) makes use of a dynamical system for solving
a search task. Instead of adding search biases in order to improve performance
in certain problems, we aim to remove algorithm-induced scales by controlling
the swarm with a mechanism that is scale-free except possibly for a suppression
of scales beyond the system size. In this way a very promising performance is
achieved due to the balance of large-scale exploration and local search. The
resulting algorithm shows evidence for self-organised criticality, brought
about via the intrinsic dynamics of the swarm as it interacts with the
objective function, rather than being explicitly specified. The Critical
Particle Swarm (CriPS) can be easily combined with many existing extensions
such as chaotic exploration, additional force terms or non-trivial topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6889</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6889</id><created>2014-02-27</created><updated>2015-02-03</updated><authors><author><keyname>De Cat</keyname><forenames>Broes</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter</forenames></author><author><keyname>Bruynooghe</keyname><forenames>Maurice</forenames></author></authors><title>Lazy Model Expansion: Interleaving Grounding with Search</title><categories>cs.LO</categories><journal-ref>Journal of Artificial Intelligence Research, feb 2015, volume 52,
  pages 235-286</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding satisfying assignments for the variables involved in a set of
constraints can be cast as a (bounded) model generation problem: search for
(bounded) models of a theory in some logic. The state-of-the-art approach for
bounded model generation for rich knowledge representation languages, like ASP,
FO(.) and Zinc, is ground-and-solve: reduce the theory to a ground or
propositional one and apply a search algorithm to the resulting theory.
  An important bottleneck is the blowup of the size of the theory caused by the
reduction phase. Lazily grounding the theory during search is a way to overcome
this bottleneck. We present a theoretical framework and an implementation in
the context of the FO(.) knowledge representation language. Instead of
grounding all parts of a theory, justifications are derived for some parts of
it. Given a partial assignment for the grounded part of the theory and valid
justifications for the formulas of the non-grounded part, the justifications
provide a recipe to construct a complete assignment that satisfies the
non-grounded part. When a justification for a particular formula becomes
invalid during search, a new one is derived; if that fails, the formula is
split in a part to be grounded and a part that can be justified.
  The theoretical framework captures existing approaches for tackling the
grounding bottleneck such as lazy clause generation and grounding-on-the-fly,
and presents a generalization of the 2-watched literal scheme. We present an
algorithm for lazy model expansion and integrate it in a model generator for
FO(ID), a language extending first-order logic with inductive definitions. The
algorithm is implemented as part of the state-of-the-art FO(ID) Knowledge-Base
System IDP. Experimental results illustrate the power and generality of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6903</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6903</id><created>2014-02-27</created><authors><author><keyname>Sethia</keyname><forenames>Seema</forenames></author><author><keyname>Chatterjee</keyname><forenames>Shouri</forenames></author><author><keyname>Kale</keyname><forenames>Sunil</forenames></author><author><keyname>Gupta</keyname><forenames>Amit</forenames></author><author><keyname>Sarangi</keyname><forenames>Smruti R.</forenames></author></authors><title>Three Experiments to Analyze the Nature of the Heat Spreader</title><categories>cs.OH</categories><comments>4 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we describe ongoing work to investigate the properties of the
heat spreader, and its implication on architecture research. In specific, we
conduct two experiments to quantify the heat distribution across the surface of
a spreader during normal operation. The first experiment uses T-type
thermocouples, to find the temperature difference across different points on
the spreader. We observe about a 6 degree celsius temperature difference on
average. In the second experiment, we try to capture the temperature gradients
using an infrared camera. However, this experiment was inconclusive because of
some practical constraints such as the low emissivity of the spreader. We
conclude that to properly model the spreader, it is necessary to conduct
detailed finite element simulations. We describe a method to accurately measure
the thermal conductivity of the heat spreader such that it can be used to
compute the steady state temperature distribution across the spreader.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6908</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6908</id><created>2014-02-27</created><updated>2015-05-11</updated><authors><author><keyname>Tat</keyname><forenames>Lee Yin</forenames></author><author><keyname>Chun</keyname><forenames>Lam Ka</forenames></author><author><keyname>Ming</keyname><forenames>Lui Lok</forenames></author></authors><title>Landmark-matching Transformation with Large Deformation via
  n-dimensional Quasi-conformal Maps</title><categories>cs.CG math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1210.8025 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a new method to obtain landmark-matching transformations between
n-dimensional Euclidean spaces with large deformations. Given a set of feature
correspondences, our algorithm searches for an optimal folding-free mapping
that satisfies the prescribed landmark constraints. The standard conformality
distortion defined for mappings between 2-dimensional spaces is first
generalized to the $n$-dimensional conformality distortion $K(f)$ for a mapping
$f$ between $n$-dimensional Euclidean spaces $(n \geq 3)$. We then propose a
variational model involving $K(f)$ to tackle the landmark-matching problem in
higher dimensional spaces. The generalized conformality term $K(f)$ enforces
the bijectivity of the optimized mapping and minimizes its local geometric
distortions even with large deformations. Another challenge is the high
computational cost of the proposed model. To tackle this, we have also proposed
a numerical method to solve the optimization problem more efficiently.
Alternating direction method with multiplier (ADMM) is applied to split the
optimization problem into two subproblems. Preconditioned conjugate gradient
method with multi-grid preconditioner is applied to solve one of the
sub-problems, while a fixed-point iteration is proposed to solve another
subproblem. Experiments have been carried out on both synthetic examples and
lung CT images to compute the diffeomorphic landmark-matching transformation
with different landmark constraints. Results show the efficacy of our proposed
model to obtain a folding-free landmark-matching transformation between
$n$-dimensional spaces with large deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6926</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6926</id><created>2014-02-27</created><updated>2014-09-28</updated><authors><author><keyname>Foster</keyname><forenames>Peter</forenames></author><author><keyname>Mauch</keyname><forenames>Matthias</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author></authors><title>Sequential Complexity as a Descriptor for Musical Similarity</title><categories>cs.IR cs.LG cs.SD</categories><comments>13 pages, 9 figures, 8 tables. Accepted version</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol. 22 no. 12, pp. 1965-1977, 2014</journal-ref><doi>10.1109/TASLP.2014.2357676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose string compressibility as a descriptor of temporal structure in
audio, for the purpose of determining musical similarity. Our descriptors are
based on computing track-wise compression rates of quantised audio features,
using multiple temporal resolutions and quantisation granularities. To verify
that our descriptors capture musically relevant information, we incorporate our
descriptors into similarity rating prediction and song year prediction tasks.
We base our evaluation on a dataset of 15500 track excerpts of Western popular
music, for which we obtain 7800 web-sourced pairwise similarity ratings. To
assess the agreement among similarity ratings, we perform an evaluation under
controlled conditions, obtaining a rank correlation of 0.33 between intersected
sets of ratings. Combined with bag-of-features descriptors, we obtain
performance gains of 31.1% and 10.9% for similarity rating prediction and song
year prediction. For both tasks, analysis of selected descriptors reveals that
representing features at multiple time scales benefits prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6932</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6932</id><created>2014-02-27</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Llull</keyname><forenames>Patrick</forenames></author><author><keyname>Liao</keyname><forenames>Xuejun</forenames></author><author><keyname>Yang</keyname><forenames>Jianbo</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author><author><keyname>Brady</keyname><forenames>David J.</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Low-Cost Compressive Sensing for Color Video and Depth</title><categories>cs.CV</categories><comments>8 pages, CVPR 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple and inexpensive (low-power and low-bandwidth) modification is made
to a conventional off-the-shelf color video camera, from which we recover
{multiple} color frames for each of the original measured frames, and each of
the recovered frames can be focused at a different depth. The recovery of
multiple frames for each measured frame is made possible via high-speed coding,
manifested via translation of a single coded aperture; the inexpensive
translation is constituted by mounting the binary code on a piezoelectric
device. To simultaneously recover depth information, a {liquid} lens is
modulated at high speed, via a variable voltage. Consequently, during the
aforementioned coding process, the liquid lens allows the camera to sweep the
focus through multiple depths. In addition to designing and implementing the
camera, fast recovery is achieved by an anytime algorithm exploiting the
group-sparsity of wavelet/DCT coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6942</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6942</id><created>2014-02-27</created><authors><author><keyname>Nalepa</keyname><forenames>Jakub</forenames></author><author><keyname>Czech</keyname><forenames>Zbigniew J.</forenames></author></authors><title>A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with
  Time Windows</title><categories>cs.DC cs.NE</categories><comments>15 pages</comments><journal-ref>Studia Informatica 33 (1), pp 91-106 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a parallel memetic algorithm for solving the vehicle
routing problem with time windows (VRPTW). The VRPTW is a well-known NP-hard
discrete optimization problem with two objectives. The main objective is to
minimize the number of vehicles serving customers scattered on the map, and the
second one is to minimize the total distance traveled by the vehicles. Here,
the fleet size is minimized in the first phase of the proposed method using the
parallel heuristic algorithm (PHA), and the traveled distance is minimized in
the second phase by the parallel memetic algorithm (PMA). In both parallel
algorithms, the parallel components co-operate periodically in order to
exchange the best solutions found so far. An extensive experimental study
performed on the Gehring and Homberger's benchmark proves the high convergence
capabilities and robustness of both PHA and PMA. Also, we present the speedup
analysis of the PMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6952</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6952</id><created>2014-02-27</created><authors><author><keyname>Bri&#xeb;t</keyname><forenames>Jop</forenames></author><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Hu</keyname><forenames>Guangda</forenames></author><author><keyname>Saraf</keyname><forenames>Shubhangi</forenames></author></authors><title>Lower Bounds for Approximate LDC</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an approximate version of $q$-query LDCs (Locally Decodable Codes)
over the real numbers and prove lower bounds on the encoding length of such
codes. A $q$-query $(\alpha,\delta)$-approximate LDC is a set $V$ of $n$ points
in $\mathbb{R}^d$ so that, for each $i \in [d]$ there are $\Omega(\delta n)$
disjoint $q$-tuples $(\vec{u}_1,\ldots,\vec{u}_q) $ in $V$ so that
$\text{span}(\vec{u}_1,\ldots,\vec{u}_q)$ contains a unit vector whose $i$'th
coordinate is at least $\alpha$. We prove exponential lower bounds of the form
$n \geq 2^{\Omega(\alpha \delta \sqrt{d})}$ for the case $q=2$ and, in some
cases, stronger bounds (exponential in $d$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6964</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6964</id><created>2014-02-27</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Rajwa</keyname><forenames>Bartek</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Scalable methods for nonnegative matrix factorizations of near-separable
  tall-and-skinny matrices</title><categories>cs.LG cs.DC cs.NA stat.ML</categories><acm-class>G.1.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous algorithms are used for nonnegative matrix factorization under the
assumption that the matrix is nearly separable. In this paper, we show how to
make these algorithms efficient for data matrices that have many more rows than
columns, so-called &quot;tall-and-skinny matrices&quot;. One key component to these
improved methods is an orthogonal matrix transformation that preserves the
separability of the NMF problem. Our final methods need a single pass over the
data matrix and are suitable for streaming, multi-core, and MapReduce
architectures. We demonstrate the efficacy of these algorithms on
terabyte-sized synthetic matrices and real-world matrices from scientific
computing and bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6970</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6970</id><created>2014-02-11</created><authors><author><keyname>Song</keyname><forenames>D.</forenames></author></authors><title>The P versus NP Problem in Quantum Physics</title><categories>physics.gen-ph cs.CC</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that information is encoded and processed by physical
systems, the P versus NP problem is examined in terms of physical processes. In
particular, we consider P as a class of deterministic, and NP as
nondeterministic, polynomial-time physical processes. Based on these
identifications, we review a self-reference physical process in quantum theory,
which belongs to NP but cannot be contained in P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6973</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6973</id><created>2014-02-27</created><authors><author><keyname>Markovitch</keyname><forenames>Michael</forenames></author><author><keyname>Scalosub</keyname><forenames>Gabriel</forenames></author></authors><title>Bounded Delay Scheduling with Packet Dependencies</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common situation occurring when dealing with multimedia traffic is having
large data frames fragmented into smaller IP packets, and having these packets
sent independently through the network. For real-time multimedia traffic,
dropping even few packets of a frame may render the entire frame useless. Such
traffic is usually modeled as having {\em inter-packet dependencies}. We study
the problem of scheduling traffic with such dependencies, where each packet has
a deadline by which it should arrive at its destination. Such deadlines are
common for real-time multimedia applications, and are derived from stringent
delay constraints posed by the application. The figure of merit in such
environments is maximizing the system's {\em goodput}, namely, the number of
frames successfully delivered.
  We study online algorithms for the problem of maximizing goodput of
delay-bounded traffic with inter-packet dependencies, and use competitive
analysis to evaluate their performance. We present competitive algorithms for
the problem, as well as matching lower bounds that are tight up to a constant
factor. We further present the results of a simulation study which further
validates our algorithmic approach and shows that insights arising from our
analysis are indeed manifested in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6978</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6978</id><created>2014-02-27</created><authors><author><keyname>Namuduri</keyname><forenames>Kamesh</forenames></author><author><keyname>Mehta</keyname><forenames>Gayatri</forenames></author></authors><title>Fundamental Limits of Video Coding: A Closed-form Characterization of
  Rate Distortion Region from First Principles</title><categories>cs.IT cs.MM math.IT</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical motion-compensated video coding methods have been standardized by
MPEG over the years and video codecs have become integral parts of media
entertainment applications. Despite the ubiquitous use of video coding
techniques, it is interesting to note that a closed form rate-distortion
characterization for video coding is not available in the literature. In this
paper, we develop a simple, yet, fundamental characterization of
rate-distortion region in video coding based on information-theoretic first
principles. The concept of conditional motion estimation is used to derive the
closedform expression for rate-distortion region without losing its generality.
Conditional motion estimation offers an elegant means to analyze the
rate-distortion trade-offs and demonstrates the viability of achieving the
bounds derived. The concept involves classifying image regions into active and
inactive based on the amount of motion activity. By appropriately modeling the
residuals corresponding to active and inactive regions, a closed form
expression for rate-distortion function is derived in terms of motion activity
and spatio-temporal correlation that commonly exist in video content.
Experiments on real video clips using H.264 codec are presented to demonstrate
the practicality and validity of the proposed rate-distortion analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.6993</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.6993</id><created>2014-02-27</created><updated>2014-08-22</updated><authors><author><keyname>Caracciolo</keyname><forenames>Sergio</forenames></author><author><keyname>Lucibello</keyname><forenames>Carlo</forenames></author><author><keyname>Parisi</keyname><forenames>Giorgio</forenames></author><author><keyname>Sicuro</keyname><forenames>Gabriele</forenames></author></authors><title>Scaling hypothesis for the Euclidean bipartite matching problem</title><categories>cond-mat.dis-nn cs.DM cs.GR</categories><comments>11 pages</comments><journal-ref>Phys. Rev. E 90, 012118 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple yet very predictive form, based on a Poisson's equation,
for the functional dependence of the cost from the density of points in the
Euclidean bipartite matching problem. This leads, for quadratic costs, to the
analytic prediction of the large $N$ limit of the average cost in dimension
$d=1,2$ and of the subleading correction in higher dimension. A non-trivial
scaling exponent, $\gamma_d=\frac{d-2}{d}$, which differs from the
monopartite's one, is found for the subleading correction. We argue that the
same scaling holds true for a generic cost exponent in dimension $d&gt;2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7001</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7001</id><created>2014-02-27</created><authors><author><keyname>van der Maaten</keyname><forenames>Laurens</forenames></author><author><keyname>Chen</keyname><forenames>Minmin</forenames></author><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian</forenames></author></authors><title>Marginalizing Corrupted Features</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of machine learning is to develop predictors that generalize well to
test data. Ideally, this is achieved by training on an almost infinitely large
training data set that captures all variations in the data distribution. In
practical learning settings, however, we do not have infinite data and our
predictors may overfit. Overfitting may be combatted, for example, by adding a
regularizer to the training objective or by defining a prior over the model
parameters and performing Bayesian inference. In this paper, we propose a
third, alternative approach to combat overfitting: we extend the training set
with infinitely many artificial training examples that are obtained by
corrupting the original training data. We show that this approach is practical
and efficient for a range of predictors and corruption models. Our approach,
called marginalized corrupted features (MCF), trains robust predictors by
minimizing the expected value of the loss function under the corruption model.
We show empirically on a variety of data sets that MCF classifiers can be
trained efficiently, may generalize substantially better to test data, and are
also more robust to feature deletion at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7005</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7005</id><created>2014-02-27</created><authors><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Shakibi</keyname><forenames>Babak</forenames></author><author><keyname>Jin</keyname><forenames>Lin</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Bayesian Multi-Scale Optimistic Optimization</title><categories>stat.ML cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization is a powerful global optimization technique for
expensive black-box functions. One of its shortcomings is that it requires
auxiliary optimization of an acquisition function at each iteration. This
auxiliary optimization can be costly and very hard to carry out in practice.
Moreover, it creates serious theoretical concerns, as most of the convergence
results assume that the exact optimum of the acquisition function can be found.
In this paper, we introduce a new technique for efficient global optimization
that combines Gaussian process confidence bounds and treed simultaneous
optimistic optimization to eliminate the need for auxiliary optimization of
acquisition functions. The experiments with global optimization benchmarks and
a novel application to automatic information extraction demonstrate that the
resulting technique is more efficient than the two approaches from which it
draws inspiration. Unlike most theoretical analyses of Bayesian optimization
with Gaussian processes, our finite-time convergence rate proofs do not require
exact optimization of an acquisition function. That is, our approach eliminates
the unsatisfactory assumption that a difficult, potentially NP-hard, problem
has to be solved in order to obtain vanishing regret rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7011</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7011</id><created>2014-02-26</created><updated>2014-05-22</updated><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author><author><keyname>Brockmann</keyname><forenames>Dirk</forenames></author><author><keyname>Chadefaux</keyname><forenames>Thomas</forenames></author><author><keyname>Donnay</keyname><forenames>Karsten</forenames></author><author><keyname>Blanke</keyname><forenames>Ulf</forenames></author><author><keyname>Woolley-Meza</keyname><forenames>Olivia</forenames></author><author><keyname>Moussaid</keyname><forenames>Mehdi</forenames></author><author><keyname>Johansson</keyname><forenames>Anders</forenames></author><author><keyname>Krause</keyname><forenames>Jens</forenames></author><author><keyname>Schutte</keyname><forenames>Sebastian</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Saving Human Lives: What Complexity Science and Information Systems can
  Contribute</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>67 pages, 25 figures; accepted for publication in Journal of
  Statistical Physics [for related work see http://www.futurict.eu/]</comments><journal-ref>J. Stat. Phys. 158 (2015) 735-781</journal-ref><doi>10.1007/s10955-014-1024-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss models and data of crowd disasters, crime, terrorism, war and
disease spreading to show that conventional recipes, such as deterrence
strategies, are often not effective and sufficient to contain them. Many common
approaches do not provide a good picture of the actual system behavior, because
they neglect feedback loops, instabilities and cascade effects. The complex and
often counter-intuitive behavior of social systems and their macro-level
collective dynamics can be better understood by means of complexity science. We
highlight that a suitable system design and management can help to stop
undesirable cascade effects and to enable favorable kinds of self-organization
in the system. In such a way, complexity science can help to save human lives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7015</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7015</id><created>2014-02-27</created><updated>2014-11-07</updated><authors><author><keyname>Pedregosa</keyname><forenames>Fabian</forenames><affiliation>INRIA Saclay - Ile de France, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Eickenberg</keyname><forenames>Michael</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Ciuciu</keyname><forenames>Philippe</forenames><affiliation>INRIA Saclay - Ile de France, NEUROSPIN</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>INRIA Saclay - Ile de France, NEUROSPIN</affiliation></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames><affiliation>LTCI</affiliation></author></authors><title>Data-driven HRF estimation for encoding and decoding models</title><categories>cs.CE cs.LG</categories><comments>appears in NeuroImage (2015)</comments><proxy>ccsd</proxy><doi>10.1016/j.neuroimage.2014.09.060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the common usage of a canonical, data-independent, hemodynamic
response function (HRF), it is known that the shape of the HRF varies across
brain regions and subjects. This suggests that a data-driven estimation of this
function could lead to more statistical power when modeling BOLD fMRI data.
However, unconstrained estimation of the HRF can yield highly unstable results
when the number of free parameters is large. We develop a method for the joint
estimation of activation and HRF using a rank constraint causing the estimated
HRF to be equal across events/conditions, yet permitting it to be different
across voxels. Model estimation leads to an optimization problem that we
propose to solve with an efficient quasi-Newton method exploiting fast gradient
computations. This model, called GLM with Rank-1 constraint (R1-GLM), can be
extended to the setting of GLM with separate designs which has been shown to
improve decoding accuracy in brain activity decoding experiments. We compare 10
different HRF modeling methods in terms of encoding and decoding score in two
different datasets. Our results show that the R1-GLM model significantly
outperforms competing methods in both encoding and decoding settings,
positioning it as an attractive method both from the points of view of accuracy
and computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7017</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7017</id><created>2014-02-27</created><updated>2014-09-01</updated><authors><author><keyname>Amdouni</keyname><forenames>Ichrak</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Adjih</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Minet</keyname><forenames>Pascale</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Joint Routing and STDMA-based Scheduling to Minimize Delays in Grid
  Wireless Sensor Networks</title><categories>cs.NI</categories><proxy>ccsd</proxy><report-no>RR-8488</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we study the issue of delay optimization and energy
efficiency in grid wireless sensor networks (WSNs). We focus on STDMA (Spatial
Reuse TDMA)) scheduling, where a predefined cycle is repeated, and where each
node has fixed transmission opportunities during specific slots (defined by
colors). We assume a STDMA algorithm that takes advantage of the regularity of
grid topology to also provide a spatially periodic coloring (&quot;tiling&quot; of the
same color pattern). In this setting, the key challenges are: 1) minimizing the
average routing delay by ordering the slots in the cycle 2) being energy
efficient. Our work follows two directions: first, the baseline performance is
evaluated when nothing specific is done and the colors are randomly ordered in
the STDMA cycle. Then, we propose a solution, ORCHID that deliberately
constructs an efficient STDMA schedule. It proceeds in two steps. In the first
step, ORCHID starts form a colored grid and builds a hierarchical routing based
on these colors. In the second step, ORCHID builds a color ordering, by
considering jointly both routing and scheduling so as to ensure that any node
will reach a sink in a single STDMA cycle. We study the performance of these
solutions by means of simulations and modeling. Results show the excellent
performance of ORCHID in terms of delays and energy compared to a shortest path
routing that uses the delay as a heuristic. We also present the adaptation of
ORCHID to general networks under the SINR interference model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7019</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7019</id><created>2014-02-24</created><authors><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author><author><keyname>Yi&#x11f;itler</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author></authors><title>A Three-State Received Signal Strength Model for Device-free
  Localization</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The indoor radio propagation channel is typically modeled as a two-state
time-variant process where one of the states represents the channel when the
environment is static, whereas the other state characterizes the medium when it
is altered by people. In this paper, the aforementioned process is augmented
with an additional state. It is shown that the changes in received signal
strength are dictated by: i) electronic noise, when a person is not present in
the monitored area; ii) reflection, when a person is moving in the close
vicinity of the line-of-sight; iii) shadowing, when a person is obstructing the
line-of-sight component of the transmitter-receiver pair. Statistical and
spatial models for the three states are derived and the models are empirically
validated. Based on the models, a simplistic device-free localization
application is designed which aims to: first, estimate the temporal state of
the channel using a hidden Markov model; second, track a person using a
particle filter. The results suggest that the tracking accuracy is enhanced by
at least 65% while the link's sensitivity region is increased by 100% or more
with respect to empirical models presented in earlier works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7025</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7025</id><created>2014-02-26</created><updated>2014-03-04</updated><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Exploiting the Statistics of Learning and Inference</title><categories>cs.LG</categories><comments>Proceedings of the NIPS workshop on &quot;Probabilistic Models for Big
  Data&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with datasets containing a billion instances or with simulations
that require a supercomputer to execute, computational resources become part of
the equation. We can improve the efficiency of learning and inference by
exploiting their inherent statistical nature. We propose algorithms that
exploit the redundancy of data relative to a model by subsampling data-cases
for every update and reasoning about the uncertainty created in this process.
In the context of learning we propose to test for the probability that a
stochastically estimated gradient points more than 180 degrees in the wrong
direction. In the context of MCMC sampling we use stochastic gradients to
improve the efficiency of MCMC updates, and hypothesis tests based on adaptive
mini-batches to decide whether to accept or reject a proposed parameter update.
Finally, we argue that in the context of likelihood free MCMC one needs to
store all the information revealed by all simulations, for instance in a
Gaussian process. We conclude that Bayesian methods will remain to play a
crucial role in the era of big data and big simulations, but only if we
overcome a number of computational challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7032</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7032</id><created>2014-02-24</created><authors><author><keyname>Fu</keyname><forenames>Xiangqun</forenames></author><author><keyname>Bao</keyname><forenames>Wansu</forenames></author><author><keyname>Shi</keyname><forenames>Jianhong</forenames></author><author><keyname>Li</keyname><forenames>Fada</forenames></author><author><keyname>Zhang</keyname><forenames>Yuchao</forenames></author></authors><title>Parameter security characterization of knapsack public-key crypto under
  quantum computing</title><categories>cs.CR cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to research the security of the knapsack problem under quantum
algorithm attack, we study the quantum algorithm for knapsack problem over Z_r
based on the relation between the dimension of the knapsack vector and r.
First, the oracle function is designed based on the knapsack vector B and S,
and the quantum algorithm for the knapsack problem over Z_r is presented. The
observation probability of target state is not improved by designing unitary
transform, but oracle function. Its complexity is polynomial. And its success
probability depends on the relation between n and r. From the above discussion,
we give the essential condition for the knapsack problem over Z_r against the
existing quantum algorithm attacks, i.e. r&lt;O(2^n). Then we analyze the security
of the Chor-Rivest public-key crypto.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7035</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7035</id><created>2014-02-27</created><updated>2014-02-27</updated><authors><author><keyname>Ramakrishnan</keyname><forenames>Naren</forenames></author><author><keyname>Butler</keyname><forenames>Patrick</forenames></author><author><keyname>Muthiah</keyname><forenames>Sathappan</forenames></author><author><keyname>Self</keyname><forenames>Nathan</forenames></author><author><keyname>Khandpur</keyname><forenames>Rupinder</forenames></author><author><keyname>Saraf</keyname><forenames>Parang</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Cadena</keyname><forenames>Jose</forenames></author><author><keyname>Vullikanti</keyname><forenames>Anil</forenames></author><author><keyname>Korkmaz</keyname><forenames>Gizem</forenames></author><author><keyname>Kuhlman</keyname><forenames>Chris</forenames></author><author><keyname>Marathe</keyname><forenames>Achla</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Hua</keyname><forenames>Ting</forenames></author><author><keyname>Chen</keyname><forenames>Feng</forenames></author><author><keyname>Lu</keyname><forenames>Chang-Tien</forenames></author><author><keyname>Huang</keyname><forenames>Bert</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author><author><keyname>Trinh</keyname><forenames>Khoa</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author><author><keyname>Katz</keyname><forenames>Graham</forenames></author><author><keyname>Doyle</keyname><forenames>Andy</forenames></author><author><keyname>Ackermann</keyname><forenames>Chris</forenames></author><author><keyname>Zavorin</keyname><forenames>Ilya</forenames></author><author><keyname>Ford</keyname><forenames>Jim</forenames></author><author><keyname>Summers</keyname><forenames>Kristen</forenames></author><author><keyname>Fayed</keyname><forenames>Youssef</forenames></author><author><keyname>Arredondo</keyname><forenames>Jaime</forenames></author><author><keyname>Gupta</keyname><forenames>Dipak</forenames></author><author><keyname>Mares</keyname><forenames>David</forenames></author></authors><title>'Beating the news' with EMBERS: Forecasting Civil Unrest using Open
  Source Indicators</title><categories>cs.SI cs.CY physics.soc-ph</categories><acm-class>K.4.1; J.4; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the design, implementation, and evaluation of EMBERS, an
automated, 24x7 continuous system for forecasting civil unrest across 10
countries of Latin America using open source indicators such as tweets, news
sources, blogs, economic indicators, and other data sources. Unlike
retrospective studies, EMBERS has been making forecasts into the future since
Nov 2012 which have been (and continue to be) evaluated by an independent T&amp;E
team (MITRE). Of note, EMBERS has successfully forecast the uptick and downtick
of incidents during the June 2013 protests in Brazil. We outline the system
architecture of EMBERS, individual models that leverage specific data sources,
and a fusion and suppression engine that supports trading off specific
evaluation criteria. EMBERS also provides an audit trail interface that enables
the investigation of why specific predictions were made along with the data
utilized for forecasting. Through numerous evaluations, we demonstrate the
superiority of EMBERS over baserate methods and its capability to forecast
significant societal happenings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7050</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7050</id><created>2014-02-27</created><authors><author><keyname>Ivaldi</keyname><forenames>Serena</forenames></author><author><keyname>Padois</keyname><forenames>Vincent</forenames></author><author><keyname>Nori</keyname><forenames>Francesco</forenames></author></authors><title>Tools for dynamics simulation of robots: a survey based on user feedback</title><categories>cs.RO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of tools for dynamics simulation has grown in the last years. It
is necessary for the robotics community to have elements to ponder which of the
available tools is the best for their research. As a complement to an objective
and quantitative comparison, difficult to obtain since not all the tools are
open-source, an element of evaluation is user feedback. With this goal in mind,
we created an online survey about the use of dynamical simulation in robotics.
This paper reports the analysis of the participants' answers and a descriptive
information fiche for the most relevant tools. We believe this report will be
helpful for roboticists to choose the best simulation tool for their
researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7060</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7060</id><created>2014-02-27</created><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Classifying the Clique-Width of $H$-Free Bipartite Graphs</title><categories>cs.DM math.CO</categories><comments>13 pages, 4 figures</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a bipartite graph, and let $H$ be a bipartite graph with a fixed
bipartition $(B_H,W_H)$. We consider three different, natural ways of
forbidding $H$ as an induced subgraph in $G$. First, $G$ is $H$-free if it does
not contain $H$ as an induced subgraph. Second, $G$ is strongly $H$-free if $G$
is $H$-free or else has no bipartition $(B_G,W_G)$ with $B_H\subseteq B_G$ and
$W_H\subseteq W_G$. Third, $G$ is weakly $H$-free if $G$ is $H$-free or else
has at least one bipartition $(B_G,W_G)$ with $B_H\not\subseteq B_G$ or
$W_H\not\subseteq W_G$. Lozin and Volz characterized all bipartite graphs $H$
for which the class of strongly $H$-free bipartite graphs has bounded
clique-width. We extend their result by giving complete classifications for the
other two variants of $H$-freeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7063</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7063</id><created>2014-02-27</created><authors><author><keyname>Nodarakis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Sioutas</keyname><forenames>Spyros</forenames></author><author><keyname>Tsoumakos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Tzimas</keyname><forenames>Giannis</forenames></author><author><keyname>Pitoura</keyname><forenames>Evaggelia</forenames></author></authors><title>Rapid AkNN Query Processing for Fast Classification of Multidimensional
  Data in the Cloud</title><categories>cs.DB</categories><comments>12 pages, 14 figures, 4 tables (it will be submitted to DEXA 2014)</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-nearest neighbor ($k$NN) query determines the $k$ nearest points, using
distance metrics, from a specific location. An all $k$-nearest neighbor
(A$k$NN) query constitutes a variation of a $k$NN query and retrieves the $k$
nearest points for each point inside a database. Their main usage resonates in
spatial databases and they consist the backbone of many location-based
applications and not only (i.e. $k$NN joins in databases, classification in
data mining). So, it is very crucial to develop methods that answer them
efficiently. In this work, we propose a novel method for classifying
multidimensional data using an A$k$NN algorithm in the MapReduce framework. Our
approach exploits space decomposition techniques for processing the
classification procedure in a parallel and distributed manner. To our
knowledge, we are the first to study the classification of multidimensional
objects under this perspective. Through an extensive experimental evaluation we
prove that our solution is efficient and scalable in processing the given
queries. We investigate many different perspectives that can affect the total
computational cost, such as different dataset distributions, number of
dimensions, growth of $k$ value and granularity of space decomposition and
prove that our system is efficient, robust and scalable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7105</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7105</id><created>2014-02-27</created><authors><author><keyname>Wise</keyname><forenames>Jennifer</forenames></author><author><keyname>Loeb</keyname><forenames>Sarah</forenames></author></authors><title>Fool's Solitaire on Joins and Cartesian Products of Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peg solitaire is a game generalized to connected graphs by Beeler and
Hoilman. In the game pegs are placed on all but one vertex. If $xyz$ form a
3-vertex path and $x$ and $y$ each have a peg but $z$ does not, then we can
remove the pegs at $x$ and $y$ and place a peg at $z$. By analogy with the
moves in the original game, this is called a jump. The goal of the peg
solitaire game on graphs is to find jumps that reduce the number of pegs on the
graph to 1.
  Beeler and Rodriguez proposed a variant where we instead want to maximize the
number of pegs remaining when no more jumps can be made. Maximizing over all
initial locations of a single hole, the maximum number of pegs left on a graph
$G$ when no jumps remain is the fool's solitaire number $F(G)$. We determine
the fool's solitaire number for the join of any graphs $G$ and $H$. For the
cartesian product, we determine $F(G \Box K_k)$ when $k \ge 3$ and $G$ is
connected and show why our argument fails when $k=2$. Finally, we give
conditions on graphs $G$ and $H$ that imply $F(G \Box H) \ge F(G) F(H)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7118</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7118</id><created>2014-02-27</created><authors><author><keyname>Clear</keyname><forenames>Michael</forenames></author><author><keyname>Patsakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Laird</keyname><forenames>Paul</forenames></author></authors><title>Lightweight Self-Bootstrapping Multiparty Computations of Time-Series
  Data with Custom Collusion Tolerance</title><categories>cs.CR</categories><comments>preprint, 14 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this work we compare two recent multiparty computation (MPC) protocols for
private summation in terms of performance. Both protocols allow multiple rounds
of aggregation from the same set of public keys generated by parties in an
initial stage. We instantiate the protocols with a fast elliptic curve and
provide an experimental comparison of their performance for different phases of
the protocol. Furthermore, we introduce a technique that allows the
computational load of both protocols to be reduced at the expense of protection
against collusion tolerance. We prove that both protocols remain secure with
this technique, and evaluate its impact on collusion tolerance and the number
of rounds supported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7122</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7122</id><created>2014-02-27</created><updated>2014-03-04</updated><authors><author><keyname>Bienvenu</keyname><forenames>Meghyn</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Ortiz</keyname><forenames>Magdalena</forenames></author><author><keyname>Simkus</keyname><forenames>Mantas</forenames></author></authors><title>Nested Regular Path Queries in Description Logics</title><categories>cs.LO cs.AI cs.DB</categories><comments>added Figure 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way regular path queries (2RPQs) have received increased attention
recently due to their ability to relate pairs of objects by flexibly navigating
graph-structured data. They are present in property paths in SPARQL 1.1, the
new standard RDF query language, and in the XML query language XPath. In line
with XPath, we consider the extension of 2RPQs with nesting, which allows one
to require that objects along a path satisfy complex conditions, in turn
expressed through (nested) 2RPQs. We study the computational complexity of
answering nested 2RPQs and conjunctions thereof (CN2RPQs) in the presence of
domain knowledge expressed in description logics (DLs). We establish tight
complexity bounds in data and combined complexity for a variety of DLs, ranging
from lightweight DLs (DL-Lite, EL) up to highly expressive ones. Interestingly,
we are able to show that adding nesting to (C)2RPQs does not affect worst-case
data complexity of query answering for any of the considered DLs. However, in
the case of lightweight DLs, adding nesting to 2RPQs leads to a surprising jump
in combined complexity, from P-complete to Exp-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7131</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7131</id><created>2014-02-27</created><authors><author><keyname>Umami</keyname><forenames>Pesos</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Yadi</keyname><forenames>Ilman Zuhri</forenames></author></authors><title>Sistem pendukung keputusan pemberian beasiswa bidik misi</title><categories>cs.CY</categories><comments>Konferensi Nasional Sistem Informasi (KNSI), STMIK Dipanegara
  Makassar, Sulawesi Selatan, 2014</comments><journal-ref>P. Umami, et al., &quot;Sistem penunjang keputusan pemberian beasiswa
  bidik misi,&quot; in Konferensi Nasional Sistem Informasi (KNSI), STMIK Dipanegara
  Makassar, Sulawesi Selatan, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision support systems (DSS) have been used in many applications to speed
up decision making. This article will discuss the implementation of DSS for
decision-making for scholarships &quot;bidik misi&quot;. In terms of processing data
related to the bidik misi scholarship are accordance with the regulations set
by UBD. To facilitate the selection process of bidik misi scholarship then the
authors used SAW method. The criteria for this scholarship involves academic
achievement index, parents income, parents dependents number, semester, etc.
Author also used waterfall model approach to developed the systems. The result
is a decision support system that able to help decisions making quickly and in
accordance with the rules of bidik misi scholarship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7136</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7136</id><created>2014-02-28</created><authors><author><keyname>Benes</keyname><forenames>Peter Mark</forenames></author><author><keyname>Bukovsky</keyname><forenames>Ivo</forenames></author><author><keyname>Cejnek</keyname><forenames>Matous</forenames></author><author><keyname>Kalivoda</keyname><forenames>Jan</forenames></author></authors><title>Neural Network Approach to Railway Stand Lateral Skew Control</title><categories>cs.SY cs.NE</categories><comments>P. M. Benes et al., &quot;Neural Network Approach to Railway Stand Lateral
  Skew Control&quot; in Computer Science &amp; Information Technology (CS&amp; IT), Sydney,
  NSW, Australia, AIRCC, 2014, pp. 327-339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a study of an adaptive approach to lateral skew control
for an experimental railway stand. The preliminary experiments with the real
experimental railway stand and simulations with its 3-D mechanical model,
indicates difficulties of model-based control of the device. Thus, use of
neural networks for identification and control of lateral skew shall be
investigated. This paper focuses on real-data based modeling of the railway
stand by various neural network models, i.e; linear neural unit and quadratic
neural unit architectures. Furthermore, training methods of these neural
architectures as such, real-time-recurrent-learning and a variation of
back-propagation-through-time are examined, accompanied by a discussion of the
produced experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7142</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7142</id><created>2014-02-28</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Roe</keyname><forenames>David</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Vaccon</keyname><forenames>Tristan</forenames><affiliation>IRMAR</affiliation></author></authors><title>Tracking p-adic precision</title><categories>math.NT cs.SC</categories><proxy>ccsd</proxy><doi>10.1112/S1461157014000357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method to propagate $p$-adic precision in computations,
which also applies to other ultrametric fields. We illustrate it with many
examples and give a toy application to the stable computation of the SOMOS 4
sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7143</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7143</id><created>2014-02-28</created><updated>2014-04-10</updated><authors><author><keyname>Rajadesingan</keyname><forenames>Ashwin</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Identifying Users with Opposing Opinions in Twitter Debates</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Corrected typos in Section 4, under &quot;Visibly Opinionated Users&quot;. The
  numbers did not add up. Results remain unchanged</comments><journal-ref>Lecture Notes in Computer Science, Vol. 8393, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, social media sites such as Twitter have been extensively
used for debating politics and public policies. These debates span millions of
tweets and numerous topics of public importance. Thus, it is imperative that
this vast trove of data is tapped in order to gain insights into public opinion
especially on hotly contested issues such as abortion, gun reforms etc. Thus,
in our work, we aim to gauge users' stance on such topics in Twitter. We
propose ReLP, a semi-supervised framework using a retweet-based label
propagation algorithm coupled with a supervised classifier to identify users
with differing opinions. In particular, our framework is designed such that it
can be easily adopted to different domains with little human supervision while
still producing excellent accuracy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7150</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7150</id><created>2014-02-28</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames></author><author><keyname>Martin</keyname><forenames>Milo</forenames></author><author><keyname>Raghothaman</keyname><forenames>Mukund</forenames></author><author><keyname>Stergiou</keyname><forenames>Christos</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author><author><keyname>Udupa</keyname><forenames>Abhishek</forenames></author></authors><title>Synthesizing Finite-state Protocols from Scenarios and Requirements</title><categories>cs.FL cs.LO</categories><comments>This is the working draft of a paper currently in submission.
  (February 10, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scenarios, or Message Sequence Charts, offer an intuitive way of describing
the desired behaviors of a distributed protocol. In this paper we propose a new
way of specifying finite-state protocols using scenarios: we show that it is
possible to automatically derive a distributed implementation from a set of
scenarios augmented with a set of safety and liveness requirements, provided
the given scenarios adequately \emph{cover} all the states of the desired
implementation. We first derive incomplete state machines from the given
scenarios, and then synthesis corresponds to completing the transition relation
of individual processes so that the global product meets the specified
requirements. This completion problem, in general, has the same complexity,
PSPACE, as the verification problem, but unlike the verification problem, is
NP-complete for a constant number of processes. We present two algorithms for
solving the completion problem, one based on a heuristic search in the space of
possible completions and one based on OBDD-based symbolic fixpoint computation.
We evaluate the proposed methodology for protocol specification and the
effectiveness of the synthesis algorithms using the classical alternating-bit
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7162</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7162</id><created>2014-02-28</created><authors><author><keyname>Yalic</keyname><forenames>Hamdi Yalin</forenames></author></authors><title>Visual Saliency Model using SIFT and Comparison of Learning Approaches</title><categories>cs.CV</categories><comments>8 pages, 6 figures, 2 tables</comments><acm-class>I.2.10; I.5.4</acm-class><journal-ref>Computer Science &amp; Information Technology, Volume 4, Number 2,
  2014, page 275-282</journal-ref><doi>10.5121/csit.2014.4223</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Humans' ability to detect and locate salient objects on images is remarkably
fast and successful. Performing this process by using eye tracking equipment is
expensive and cannot be easily applied, and computer modeling of this human
behavior is still a problem to be solved. In our study, one of the largest
public eye-tracking databases which has fixation points of 15 observers on 1003
images is used. In addition to low, medium and high-level features which have
been used in previous studies, SIFT features extracted from the images are used
to improve the classification accuracy of the models. A second contribution of
this paper is the comparison and statistical analysis of different machine
learning methods that can be used to train our model. As a result, a best
feature set and learning model to predict where humans look at images, is
determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7170</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7170</id><created>2014-02-28</created><authors><author><keyname>Olmos</keyname><forenames>Pablo M.</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Improving the Finite-Length Performance of Spatially Coupled LDPC Codes
  by Connecting Multiple Code Chains</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, February 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the finite-length performance of codes on graphs
constructed by connecting spatially coupled low-density parity-check (SC-LDPC)
code chains. Successive (peeling) decoding is considered for the binary erasure
channel (BEC). The evolution of the undecoded portion of the bipartite graph
remaining after each iteration is analyzed as a dynamical system. When
connecting short SC-LDPC chains, we show that, in addition to superior
iterative decoding thresholds, connected chain ensembles have better
finite-length performance than single chain ensembles of the same rate and
length. In addition, we present a novel encoding/transmission scheme to improve
the performance of a system using long SC-LDPC chains, where, instead of
transmitting codewords corresponding to a single SC-LDPC chain independently,
we connect consecutive chains in a multi-layer format to form a connected chain
ensemble. We refer to such a transmission scheme to as continuous chain (CC)
transmission of SC-LDPC codes. We show that CC transmission can be implemented
with no significant increase in encoding/decoding complexity or decoding delay
with respect a system using a single SC-LDPC code chain for encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7184</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7184</id><created>2014-02-28</created><authors><author><keyname>Wedin</keyname><forenames>Edvin</forenames></author><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author></authors><title>The Hegselmann-Krause dynamics for continuous agents and a regular
  opinion function do not always lead to consensus</title><categories>math.DS cs.SI cs.SY</categories><comments>11 pages, 3 figures</comments><msc-class>93C55, 91F99, 26A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an example of a regular opinion function which, as it evolves in
accordance with the discrete-time Hegselmann-Krause bounded confidence
dynamics, always retains opinions which are separated by more than two. This
confirms a conjecture of Blondel, Hendrickx and Tsitsiklis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7190</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7190</id><created>2014-02-28</created><authors><author><keyname>kumarasawamy</keyname><forenames>S</forenames></author><author><keyname>L</keyname><forenames>Srikanth P</forenames></author><author><keyname>H</keyname><forenames>Manjula S</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Two Stage Prediction Process with Gradient Descent Methods Aligning with
  the Data Privacy Preservation</title><categories>cs.DB</categories><comments>14 pages</comments><journal-ref>International Journal of Information Processing, 7(3), 68-82, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy preservation emphasize on authorization of data, which signifies that
data should be accessed only by authorized users. Ensuring the privacy of data
is considered as one of the challenging task in data management. The
generalization of data with varying concept hierarchies seems to be interesting
solution. This paper proposes two stage prediction processes on privacy
preserved data. The privacy is preserved using generalization and betraying
other communicating parties by disguising generalized data which adds another
level of privacy. The generalization with betraying is performed in first stage
to define the knowledge or hypothesis and which is further optimized using
gradient descent method in second stage prediction for accurate prediction of
data. The experiment carried with both batch and stochastic gradient methods
and it is shown that bulk operation performed by batch takes long time and more
iterations than stochastic to give more accurate solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7198</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7198</id><created>2014-02-28</created><authors><author><keyname>Prakash</keyname><forenames>T Shiva</forenames></author><author><keyname>Raja</keyname><forenames>K B</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Iyengar</keyname><forenames>S S</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Two-Hop Routing with Traffic-Differentiation for QoS Guarantee in
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>13 pages</comments><journal-ref>International Journal of Information Processing, 7(3), 100-112,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Traffic-Differentiated Two-Hop Routing protocol for
Quality of Service (QoS) in Wireless Sensor Networks (WSNs). It targets WSN
applications having different types of data traffic with several priorities.
The protocol achieves to increase Packet Reception Ratio (PRR) and reduce
end-to-end delay while considering multi-queue priority policy, two-hop
neighborhood information, link reliability and power efficiency. The protocol
is modular and utilizes effective methods for estimating the link metrics.
Numerical results show that the proposed protocol is a feasible solution to
addresses QoS service differenti- ation for traffic with different priorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7200</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7200</id><created>2014-02-28</created><authors><author><keyname>G</keyname><forenames>Leena Giri</forenames></author><author><keyname>L</keyname><forenames>Srikanth P</forenames></author><author><keyname>Manjula</keyname><forenames>S H</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Mathematical Model of Semantic Look - An Efficient Context Driven Search
  Engine</title><categories>cs.IR</categories><comments>12 pages</comments><journal-ref>International Journal of Information Processing, 7(2), 20-31, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The WorldWideWeb (WWW) is a huge conservatory of web pages. Search Engines
are key applications that fetch web pages for the user query. In the current
generation web architecture, search engines treat keywords provided by the user
as isolated keywords without considering the context of the user query. This
results in a lot of unrelated pages or links being displayed to the user.
Semantic Web is based on the current web with a revised framework to display a
more precise result set as response to a user query. The current web pages need
to be annotated by finding relevant meta data to be added to each of them, so
that they become useful to Semantic Web search engines. Semantic Look explores
the context of user query by processing the Semantic information recorded in
the web pages. It is compared with an existing algorithm called OntoLook and it
is shown that Semantic Look is a better optimized search engine by being more
than twice as fast as OntoLook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7205</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7205</id><created>2014-02-28</created><updated>2014-06-25</updated><authors><author><keyname>Faugere</keyname><forenames>Jean-Charles</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Spaenlehauer</keyname><forenames>Pierre-Jean</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Svartz</keyname><forenames>Jules</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Sparse Gr\&quot;obner Bases: the Unmixed Case</title><categories>cs.SC</categories><comments>20 pages, Corollary 6.1 has been corrected, ISSAC 2014, Kobe : Japan
  (2014)</comments><proxy>ccsd</proxy><doi>10.1145/2608628.2608663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Toric (or sparse) elimination theory is a framework developped during the
last decades to exploit monomial structures in systems of Laurent polynomials.
Roughly speaking, this amounts to computing in a \emph{semigroup algebra},
\emph{i.e.} an algebra generated by a subset of Laurent monomials. In order to
solve symbolically sparse systems, we introduce \emph{sparse Gr\&quot;obner bases},
an analog of classical Gr\&quot;obner bases for semigroup algebras, and we propose
sparse variants of the $F_5$ and FGLM algorithms to compute them. Our prototype
&quot;proof-of-concept&quot; implementation shows large speed-ups (more than 100 for some
examples) compared to optimized (classical) Gr\&quot;obner bases software. Moreover,
in the case where the generating subset of monomials corresponds to the points
with integer coordinates in a normal lattice polytope $\mathcal P\subset\mathbb
R^n$ and under regularity assumptions, we prove complexity bounds which depend
on the combinatorial properties of $\mathcal P$. These bounds yield new
estimates on the complexity of solving $0$-dim systems where all polynomials
share the same Newton polytope (\emph{unmixed case}). For instance, we
generalize the bound $\min(n_1,n_2)+1$ on the maximal degree in a Gr\&quot;obner
basis of a $0$-dim. bilinear system with blocks of variables of sizes
$(n_1,n_2)$ to the multilinear case: $\sum n_i - \max(n_i)+1$. We also propose
a variant of Fr\&quot;oberg's conjecture which allows us to estimate the complexity
of solving overdetermined sparse systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7213</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7213</id><created>2014-02-28</created><authors><author><keyname>Camby</keyname><forenames>Eglantine</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>A new characterization of $P_k$-free graphs</title><categories>cs.DM math.CO</categories><comments>13 pages, 4 figures</comments><msc-class>05C69, 05C75, 05C38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of graphs that do not contain an induced path on $k$ vertices,
$P_k$-free graphs, plays a prominent role in algorithmic graph theory. This
motivates the search for special structural properties of $P_k$-free graphs,
including alternative characterizations.
  Let $G$ be a connected $P_k$-free graph, $k \ge 4$. We show that $G$ admits a
connected dominating set whose induced subgraph is either $P_{k-2}$-free, or
isomorphic to $P_{k-2}$. Surprisingly, it turns out that every minimum
connected dominating set of $G$ has this property.
  This yields a new characterization for $P_k$-free graphs: a graph $G$ is
$P_k$-free if and only if each connected induced subgraph of $G$ has a
connected dominating set whose induced subgraph is either $P_{k-2}$-free, or
isomorphic to $C_k$. This improves and generalizes several previous results;
the particular case of $k=7$ solves a problem posed by van 't Hof and Paulusma
[A new characterization of $P_6$-free graphs, COCOON 2008].
  In the second part of the paper, we present an efficient algorithm that,
given a connected graph $G$ on $n$ vertices and $m$ edges, computes a connected
dominating set $X$ of $G$ with the following property: for the minimum $k$ such
that $G$ is $P_k$-free, the subgraph induced by $X$ is $P_{k-2}$-free or
isomorphic to $P_{k-2}$.
  As an application our results, we prove that Hypergraph 2-Colorability, an
NP-complete problem in general, can be solved in polynomial time for
hypergraphs whose vertex-hyperedge incidence graph is $P_7$-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7216</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7216</id><created>2014-02-28</created><authors><author><keyname>Paz&#xfa;rikov&#xe1;</keyname><forenames>Jana</forenames></author></authors><title>Large-Scale Molecular Dynamics Simulations for Highly Parallel
  Infrastructures</title><categories>cs.DC cs.CE physics.comp-ph</categories><comments>thesis proposal</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computational chemistry allows researchers to experiment in sillico: by
running a computer simulations of a biological or chemical processes of
interest. Molecular dynamics with molecular mechanics model of interactions
simulates N-body problem of atoms$-$it computes movements of atoms according to
Newtonian physics and empirical descriptions of atomic electrostatic
interactions. These simulations require high performance computing resources,
as evaluations within each step are computationally demanding and billions of
steps are needed to reach interesting timescales. Current methods decompose the
spatial domain of the problem and calculate on parallel/distributed
infrastructures. Even the methods with the highest strong scaling hit the limit
at half a million cores: they are not able to cut the time to result if
provided with more processors. At the dawn of exascale computing with massively
parallel computational resources, we want to increase the level of parallelism
by incorporating parallel-in-time computation to molecular dynamics
simulations. Calculation of results in several successive time points
simultaneously without a priori knowledge has been examined with no major
success. We will study and implement a novel combinations of methods that
according to our theoretical analyses should achieve promising speed-up
compared to sequential-in-time calculation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7223</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7223</id><created>2014-02-28</created><authors><author><keyname>Boldt</keyname><forenames>Dennis</forenames></author><author><keyname>Hasemann</keyname><forenames>Henning</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author><author><keyname>Karnstedt</keyname><forenames>Marcel</forenames></author><author><keyname>von der Weth</keyname><forenames>Christian</forenames></author></authors><title>SPARQL for Networks of Embedded Systems</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Web (or Web of Data) represents the successful efforts towards
linking and sharing data over the Web. The cornerstones of the Web of Data are
RDF as data format and SPARQL as de-facto standard query language. Recent
trends show the evolution of the Web of Data towards the Web of Things,
integrating embedded devices and smart objects. Data stemming from such devices
do not share a common format, making the integration and querying impossible.
To overcome this problem, we present our approach to make embedded systems
first-class citizens of the Web of Things. Our framework abstracts from
individual deployments to represent them as common data sources in line with
the ideas behind the Semantic Web. This includes the execution of arbitrary
SPARQL queries over the data from a pool of embedded devices and/or external
data sources. Handling verbose RDF data and executing SPARQL queries in an
embedded network poses major challenges to minimize the involved processing and
communication cost. We therefore present an in-network query processor aiming
to push processing steps onto devices. We demonstrate the practical application
and the potential benefits of our framework in a comprehensive evaluation using
a real-world deployment and a range of SPARQL queries stemming from a common
use case of the Web of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7224</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7224</id><created>2014-02-28</created><authors><author><keyname>Grigoriev</keyname><forenames>Alexander</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author><author><keyname>Lekic</keyname><forenames>Nela</forenames></author></authors><title>On low treewidth graphs and supertrees</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compatibility of unrooted phylogenetic trees is a well studied problem in
phylogenetics. It asks to determine whether for a set of k input trees there
exists a larger tree (called a supertree) that contains the topologies of all k
input trees. When any such supertree exists we call the instance compatible and
otherwise incompatible. It is known that the problem is NP-hard and FPT,
although a constructive FPT algorithm is not known. It has been shown that
whenever the treewidth of an auxiliary structure known as the display graph is
strictly larger than the number of input trees, the instance is incompatible.
Here we show that whenever the treewidth of the display graph is at most 2, the
instance is compatible. Furthermore, we give a polynomial-time algorithm to
construct a supertree in this case. Finally, we demonstrate both compatible and
incompatible instances that have display graphs with treewidth 3, highlighting
that the treewidth of the display graph is (on its own) not sufficient to
determine compatibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7228</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7228</id><created>2014-02-28</created><authors><author><keyname>Hasemann</keyname><forenames>Henning</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author><author><keyname>Pagel</keyname><forenames>Max</forenames></author></authors><title>The Wiselib TupleStore: A Modular RDF Database for the Internet of
  Things</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things movement provides self-configuring and universally
interoperable devices. While such devices are often built with a specific
application in mind, they often turn out to be useful in other contexts as
well. We claim that by describing the devices' knowledge in a universal way,
IoT devices can become first-class citizens in the Internet. They can then
exchange data between heterogeneous hardware, different applications and large
data sources on the Web. Our key idea --- in contrast to most existing
approaches --- is to not restrict the domain of knowledge that can be expressed
on the device in any way and, at the same time, allow this knowledge to be
machine-understandable and linkable across different locations.
  We propose an architecture that allows to connect embedded devices to the
Semantic Web by expressing their knowledge in the Resource Description
Framework (RDF). We present the Wiselib TupleStore, a modular embedded database
tailored specifically for the storage of RDF. The Wiselib TupleStore is
portable to many platforms including Contiki and TinyOS and allows a variety of
trade-offs, making it able to scale to a large variety of hardware scenarios.
We discuss the applicability of RDF to heterogeneous resource-constrained
devices and compare our system to the existing embedded tuple stores Antelope
and TeenyLIME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7242</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7242</id><created>2014-02-28</created><updated>2015-12-09</updated><authors><author><keyname>Rahman</keyname><forenames>Mustazee</forenames></author></authors><title>Percolation with small clusters on random graphs</title><categories>math.PR cs.DM math.CO</categories><comments>The main result (Theorem 1) has been improved significantly and
  references have been updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of determining the maximal induced subgraph in a random
$d$-regular graph such that its components remain bounded as the size of the
graph becomes arbitrarily large. We show, for asymptotically large $d$, that
any such induced subgraph has size density at most $2(\log d)/d$ with high
probability. A matching lower bound is known for independent sets. We also
prove the analogous result for sparse Erd\H{o}s-R\'{e}nyi graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7247</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7247</id><created>2014-02-28</created><updated>2014-05-11</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Rong</keyname><forenames>Beiyu</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Optimal Discrete Power Control in Poisson-Clustered Ad Hoc Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power control in a digital handset is practically implemented in a discrete
fashion and usually such a discrete power control (DPC) scheme is suboptimal.
In this paper, we first show that in a Poison-distributed ad hoc network, if
DPC is properly designed with a certain condition satisfied, it can strictly
work better than constant power control (i.e. no power control) in terms of
average signal-to-interference ratio, outage probability and spatial reuse.
This motivates us to propose an $N$-layer DPC scheme in a wireless clustered ad
hoc network, where transmitters and their intended receivers in circular
clusters are characterized by a Poisson cluster process (PCP) on the plane
$\mathbb{R}^2$. The cluster of each transmitter is tessellated into $N$-layer
annuli with transmit power $P_i$ adopted if the intended receiver is located at
the $i$-th layer. Two performance metrics of transmission capacity (TC) and
outage-free spatial reuse factor are redefined based on the $N$-layer DPC. The
outage probability of each layer in a cluster is characterized and used to
derive the optimal power scaling law
$P_i=\Theta\left(\eta_i^{-\frac{\alpha}{2}}\right)$, with $\eta_i$ the
probability of selecting power $P_i$ and $\alpha$ the path loss exponent.
Moreover, the specific design approaches to optimize $P_i$ and $N$ based on
$\eta_i$ are also discussed. Simulation results indicate that the proposed
optimal $N$-layer DPC significantly outperforms other existing power control
schemes in terms of TC and spatial reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7248</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7248</id><created>2014-02-28</created><updated>2015-12-01</updated><authors><author><keyname>Connor</keyname><forenames>Stephen B.</forenames></author><author><keyname>Kendall</keyname><forenames>Wilfrid S.</forenames></author></authors><title>Perfect Simulation of $M/G/c$ Queues</title><categories>math.PR cs.PF</categories><comments>28 pages, 5 figures</comments><msc-class>65CO5 (Primary), 60K25, 60JO5, 68U20 (Secondary)</msc-class><journal-ref>Adv. Appl. Prob. 47, 1-25 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a perfect simulation algorithm for the stable
$M/G/c$ queue. Sigman (2011: Exact Simulation of the Stationary Distribution of
the FIFO M/G/c Queue. Journal of Applied Probability, 48A, 209--213) showed how
to build a dominated CFTP algorithm for perfect simulation of the super-stable
$M/G/c$ queue operating under First Come First Served discipline, with
dominating process provided by the corresponding $M/G/1$ queue (using Wolff's
sample path monotonicity, which applies when service durations are coupled in
order of initiation of service), and exploiting the fact that the workload
process for the $M/G/1$ queue remains the same under different queueing
disciplines, in particular under the Processor Sharing discipline, for which a
dynamic reversibility property holds. We generalize Sigman's construction to
the stable case by comparing the $M/G/c$ queue to a copy run under Random
Assignment. This allows us to produce a naive perfect simulation algorithm
based on running the dominating process back to the time it first empties. We
also construct a more efficient algorithm that uses sandwiching by lower and
upper processes constructed as coupled $M/G/c$ queues started respectively from
the empty state and the state of the $M/G/c$ queue under Random Assignment. A
careful analysis shows that appropriate ordering relationships can still be
maintained, so long as service durations continue to be coupled in order of
initiation of service. We summarize statistical checks of simulation output,
and demonstrate that the mean run-time is finite so long as the second moment
of the service duration distribution is finite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7253</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7253</id><created>2014-02-28</created><updated>2014-05-21</updated><authors><author><keyname>Valmari</keyname><forenames>Antti</forenames><affiliation>Tampere University of Technology</affiliation></author></authors><title>A Simple Character String Proof of the &quot;True but Unprovable&quot; Version of
  G\&quot;odel's First Incompleteness Theorem</title><categories>cs.LO</categories><comments>In Proceedings AFL 2014, arXiv:1405.5272</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 151, 2014, pp. 355-369</journal-ref><doi>10.4204/EPTCS.151.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rather easy yet rigorous proof of a version of G\&quot;odel's first
incompleteness theorem is presented. The version is &quot;each recursively
enumerable theory of natural numbers with 0, 1, +, *, =, logical and, logical
not, and the universal quantifier either proves a false sentence or fails to
prove a true sentence&quot;. The proof proceeds by first showing a similar result on
theories of finite character strings, and then transporting it to natural
numbers, by using them to model strings and their concatenation. Proof systems
are expressed via Turing machines that halt if and only if their input string
is a theorem. This approach makes it possible to present all but one parts of
the proof rather briefly with simple and straightforward constructions. The
details require some care, but do not require significant background knowledge.
The missing part is the widely known fact that Turing machines can perform
complicated computational tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7254</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7254</id><created>2014-02-28</created><updated>2015-03-18</updated><authors><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author></authors><title>Generalizations of the distributed Deutsch-Jozsa promise problem</title><categories>quant-ph cs.CC cs.DC cs.FL</categories><comments>we correct some errors of and improve the presentation the previous
  version. arXiv admin note: substantial text overlap with arXiv:1309.7739</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the {\em distributed Deutsch-Jozsa promise problem}, two parties are to
determine whether their respective strings $x,y\in\{0,1\}^n$ are at the {\em
Hamming distance} $H(x,y)=0$ or $H(x,y)=\frac{n}{2}$. Buhrman et al. (STOC' 98)
proved that the exact {\em quantum communication complexity} of this problem is
${\bf O}(\log {n})$ while the {\em deterministic communication complexity} is
${\bf \Omega}(n)$. This was the first impressive (exponential) gap between
quantum and classical communication complexity.
  In this paper, we generalize the above distributed Deutsch-Jozsa promise
problem to determine, for any fixed $\frac{n}{2}\leq k\leq n$, whether
$H(x,y)=0$ or $H(x,y)= k$, and show that an exponential gap between exact
quantum and deterministic communication complexity still holds if $k$ is an
even such that $\frac{1}{2}n\leq k&lt;(1-\lambda) n$, where $0&lt;
\lambda&lt;\frac{1}{2}$ is given. We also deal with a promise version of the
well-known {\em disjointness} problem and show also that for this promise
problem there exists an exponential gap between quantum (and also
probabilistic) communication complexity and deterministic communication
complexity of the promise version of such a disjointness problem. Finally, some
applications to quantum, probabilistic and deterministic finite automata of the
results obtained are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7258</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7258</id><created>2014-02-28</created><authors><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author><author><keyname>Edfors</keyname><forenames>Ove</forenames></author></authors><title>An Information Theoretic Charachterization of Channel Shortening
  Receivers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal data detection of data transmitted over a linear channel can always
be implemented through the Viterbi algorithm (VA). However, in many cases of
interest the memory of the channel prohibits application of the VA. A popular
and conceptually simple method in this case, studied since the early 70s, is to
first filter the received signal in order to shorten the memory of the channel,
and then to apply a VA that operates with the shorter memory. We shall refer to
this as a channel shortening (CS) receiver. Although studied for almost four
decades, an information theoretic understanding of what such a simple receiver
solution is actually doing is not available.
  In this paper we will show that an optimized CS receiver is implementing the
chain rule of mutual information, but only up to the shortened memory that the
receiver is operating with. Further, we will show that the tools for analyzing
the ensuing achievable rates from an optimized CS receiver are precisely the
same as those used for analyzing the achievable rates of a minimum mean square
error (MMSE) receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7265</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7265</id><created>2014-02-28</created><authors><author><keyname>Gal</keyname><forenames>Yarin</forenames></author></authors><title>Semantics, Modelling, and the Problem of Representation of Meaning -- a
  Brief Survey of Recent Literature</title><categories>cs.CL</categories><comments>15 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past 50 years many have debated what representation should be used
to capture the meaning of natural language utterances. Recently new needs of
such representations have been raised in research. Here I survey some of the
interesting representations suggested to answer for these new needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7268</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7268</id><created>2014-02-28</created><authors><author><keyname>Sarig&#xf6;l</keyname><forenames>Emre</forenames></author><author><keyname>Pfitzner</keyname><forenames>Rene</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Predicting Scientific Success Based on Coauthorship Networks</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>21 pages, 2 figures, incl. Supplementary Material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question to what extent the success of scientific articles is
due to social influence. Analyzing a data set of over 100000 publications from
the field of Computer Science, we study how centrality in the coauthorship
network differs between authors who have highly cited papers and those who do
not. We further show that a machine learning classifier, based only on
coauthorship network centrality measures at time of publication, is able to
predict with high precision whether an article will be highly cited five years
after publication. By this we provide quantitative insight into the social
dimension of scientific publishing - challenging the perception of citations as
an objective, socially unbiased measure of scientific success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7276</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7276</id><created>2014-02-28</created><authors><author><keyname>Belle</keyname><forenames>Vaishak</forenames></author><author><keyname>Levesque</keyname><forenames>Hector</forenames></author></authors><title>Robot Location Estimation in the Situation Calculus</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Eleventh International Symposium on
  Logical Formalizations on Commonsense Reasoning, Cyprus, May 27-29, 2013</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location estimation is a fundamental sensing task in robotic applications,
where the world is uncertain, and sensors and effectors are noisy. Most systems
make various assumptions about the dependencies between state variables, and
especially about how these dependencies change as a result of actions. Building
on a general framework by Bacchus, Halpern and Levesque for reasoning about
degrees of belief in the situation calculus, and a recent extension to it for
continuous domains, in this paper we illustrate location estimation in the
presence of a rich theory of actions using an example. We also show that while
actions might affect prior distributions in nonstandard ways, suitable
posterior beliefs are nonetheless entailed as a side-effect of the overall
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7289</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7289</id><created>2014-02-28</created><authors><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author><author><keyname>Nagy-Gyorgy</keyname><forenames>Judit</forenames></author></authors><title>On nonpermutational transformation semigroups with an application to
  syntactic complexity</title><categories>cs.FL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1304.5714</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an upper bound of $n((n-1)!-(n-3)!)$ for the possible largest size of
a subsemigroup of the full transformational semigroup over $n$ elements
consisting only of nonpermutational transformations. As an application we gain
the same upper bound for the syntactic complexity of (generalized) definite
languages as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7292</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7292</id><created>2014-02-28</created><updated>2014-09-11</updated><authors><author><keyname>ElBamby</keyname><forenames>Mohammed S.</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Dynamic Uplink-Downlink Optimization in TDD-based Small Cell Networks</title><categories>cs.NI cs.GT</categories><comments>In the IEEE 11th International Symposium on Wireless Communication
  Systems (ISWCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Time-division duplex (TDD) can provide efficient and flexible
splitting of the common wireless cellular resources between uplink (UL) and
downlink (DL) users. In this paper, the UL/DL optimization problem is
formulated as a noncooperative game among the small cell base stations (SCBSs)
in which each base station aims at minimizing its total UL and DL flow delays.
To solve this game, a self-organizing UL/DL resource configuration scheme for
TDD-based small cell networks is proposed. Using the proposed scheme, an SCBS
is able to estimate and learn the UL and DL loads autonomously while optimizing
its UL/DL configuration accordingly. Simulations results show that the proposed
algorithm achieves significant gains in terms of packet throughput in case of
asymmetric UL and DL traffic loads. This gain increases as the traffic
asymmetry increases, reaching up to 97% and 200% gains relative to random and
fixed duplexing schemes respectively. Our results also show that the proposed
algorithm is well- adapted to dynamic traffic conditions and different network
sizes, and operates efficiently in case of severe cross-link interference in
which neighboring cells transmit in opposite directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7293</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7293</id><created>2014-02-28</created><authors><author><keyname>Matsubayashi</keyname><forenames>Akira</forenames></author></authors><title>Separator-Based Graph Embedding into Multidimensional Grids with Small
  Edge-Congestion</title><categories>cs.DM math.CO</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of embedding a guest graph with minimum edge-congestion
into a multidimensional grid with the same size as that of the guest graph.
Based on a well-known notion of graph separators, we show that an embedding
with a smaller edge-congestion can be obtained if the guest graph has a smaller
separator, and if the host grid has a higher but constant dimension.
Specifically, we prove that any graph with $N$ nodes, maximum node degree
$\Delta$, and with a node-separator of size $O(n^\alpha)$ ($0\leq\alpha&lt;1$) can
be embedded into a grid of a fixed dimension $d\geq 2$ with at least $N$ nodes,
with an edge-congestion of $O(\Delta)$ if $d&gt;1/(1-\alpha)$, $O(\Delta\log N)$
if $d=1/(1-\alpha)$, and $O(\Delta N^{\alpha-1+\frac{1}{d}})$ if $d&lt;
1/(1-\alpha)$. This edge-congestion achieves constant ratio approximation if
$d&gt;1/(1-\alpha)$, and matches an existential lower bound within a constant
factor if $d\leq 1/(1-\alpha)$. Our result implies that if the guest graph has
an excluded minor of a fixed size, such as a planar graph, then we can obtain
an edge-congestion of $O(\Delta\log N)$ for $d=2$ and $O(\Delta)$ for any fixed
$d\geq 3$. Moreover, if the guest graph has a fixed treewidth, such as a tree,
an outerplanar graph, and a series-parallel graph, then we can obtain an
edge-congestion of $O(\Delta)$ for any fixed $d\geq 2$. To design our embedding
algorithm, we introduce edge-separators bounding expansion, such that in
partitioning a graph into isolated nodes using edge-separators recursively, the
number of outgoing edges from a subgraph to be partitioned in a recursive step
is bounded. We present an algorithm to construct an edge-separator with
expansion of $O(\Delta n^\alpha)$ from a node-separator of size $O(n^\alpha)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7301</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7301</id><created>2014-02-28</created><authors><author><keyname>Hougardy</keyname><forenames>Stefan</forenames></author><author><keyname>Schroeder</keyname><forenames>Rasmus T.</forenames></author></authors><title>Edge Elimination in TSP Instances</title><categories>cs.DM cs.DS math.CO</categories><msc-class>90C06 90C27 90C59 68Q25 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling Salesman Problem is one of the best studied NP-hard problems in
combinatorial optimization. Powerful methods have been developed over the last
60 years to find optimum solutions to large TSP instances. The largest TSP
instance so far that has been solved optimally has 85,900 vertices. Its
solution required more than 136 years of total CPU time using the
branch-and-cut based Concorde TSP code [1]. In this paper we present graph
theoretic results that allow to prove that some edges of a TSP instance cannot
occur in any optimum TSP tour. Based on these results we propose a
combinatorial algorithm to identify such edges. The runtime of the main part of
our algorithm is $O(n^2 \log n)$ for an n-vertex TSP instance. By combining our
approach with the Concorde TSP solver we are able to solve a large TSPLIB
instance more than 11 times faster than Concorde alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7305</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7305</id><created>2014-02-28</created><updated>2015-03-17</updated><authors><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author></authors><title>Similarity Decomposition Approach to Oscillatory Synchronization for
  Multiple Mechanical Systems With a Virtual Leader</title><categories>cs.SY math.OC</categories><comments>15 pages, 3 figures, published in 2014 Chinese Control Conference</comments><doi>10.1109/ChiCC.2014.6896794</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the oscillatory synchronization problem for multiple
uncertain mechanical systems with a virtual leader, and the interaction
topology among them is assumed to contain a directed spanning tree. We propose
an adaptive control scheme to achieve the goal of oscillatory synchronization.
Using the similarity decomposition approach, we show that the position and
velocity synchronization errors between each mechanical system (or follower)
and the virtual leader converge to zero. The performance of the proposed
adaptive scheme is shown by numerical simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7314</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7314</id><created>2014-02-28</created><authors><author><keyname>Poularakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Iosifidis</keyname><forenames>George</forenames></author><author><keyname>Sourlas</keyname><forenames>Vasilis</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Multicast-aware Caching for Small Cell Networks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The deployment of small cells is expected to gain huge momentum in the near
future, as a solution for managing the skyrocketing mobile data demand growth.
Local caching of popular files at the small cell base stations has been
recently proposed, aiming at reducing the traffic incurred when transferring
the requested content from the core network to the users. In this paper, we
propose and analyze a novel caching approach that can achieve significantly
lower traffic compared to the traditional caching schemes. Our cache design
policy carefully takes into account the fact that an operator can serve the
requests for the same file that happen at nearby times via a single multicast
transmission. The latter incurs less traffic as the requested file is
transmitted to the users only once, rather than with many unicast
transmissions. Systematic experiments demonstrate the effectiveness of our
approach, as compared to the existing caching schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7324</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7324</id><created>2014-02-28</created><authors><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Geometrical approach to modeling of nonlinear systems from experimental
  data</title><categories>cs.CE</categories><report-no>ISBN 978--5--8122--0926--1</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This monograph presents a geometric modeling method nonlinear dynamical
systems from experimental data . basis method is a qualitative approach to the
analysis of linear models and construction of the symmetry groups of attractors
of dynamical systems with controls . A theoretical study including the central
theorem manifold defining conditions of existence of the class in question
models in the local area , taking into account the group properties ,
estimation algorithms invariant characteristics , methods of constructing
models and identifiable description of the results obtained using the method
for simulation -driven engineering processes . included two application is the
development of the proposed approach : identification of groups symmetries on
the phase portraits of dynamical systems and the method of constructing neural
network predictive models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7340</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7340</id><created>2014-02-28</created><updated>2014-03-10</updated><authors><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Bagnoli</keyname><forenames>Franco</forenames></author></authors><title>Hierarchical community structure in complex (social) networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.5506/APhysPolBSupp.7.379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The investigation of community structure in networks is a task of great
importance in many disciplines, namely physics, sociology, biology and computer
science where systems are often represented as graphs. One of the challenges is
to find local communities from a local viewpoint in a graph without global
information in order to reproduce the subjective hierarchical vision for each
vertex. In this paper we present the improvement of an information dynamics
algorithm in which the label propagation of nodes is based on the Markovian
flow of information in the network under cognitive-inspired constraints
\cite{Massaro2012}. In this framework we have introduced two more complex
heuristics that allow the algorithm to detect the multi-resolution hierarchical
community structure of networks from a source vertex or communities adopting
fixed values of model's parameters. Experimental results show that the proposed
methods are efficient and well-behaved in both real-world and synthetic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7341</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7341</id><created>2014-02-28</created><authors><author><keyname>Mehta</keyname><forenames>Brijesh B.</forenames></author><author><keyname>Rao</keyname><forenames>Udai Pratap</forenames></author></authors><title>A Novel approach as Multi-place Watermarking for Security in Database</title><categories>cs.DB cs.CR cs.MM</categories><comments>5 pages, 3 figures, Int'l Conf. Security and Management, SAM'11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital multimedia watermarking technology had suggested in the last decade
to embed copyright information in digital objects such as images, audio and
video. However, the increasing use of relational database systems in many
real-life applications created an ever-increasing need for watermarking
database systems. As a result, watermarking relational database system is now
emerging as a research area that deals with the legal issue of copyright
protection of database systems. The main goal of database watermarking is to
generate robust and impersistent watermark for database. In this paper we
propose a method, based on image as watermark and this watermark is embedded
over the database at two different attribute of tuple, one in the numeric
attribute of tuple and another in the date attribute's time (seconds) field.
Our approach can be applied for numerical and categorical database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7344</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7344</id><created>2014-02-28</created><updated>2015-03-05</updated><authors><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Tarifi</keyname><forenames>Mohamad</forenames></author><author><keyname>Wang</keyname><forenames>Menghan</forenames></author></authors><title>An Incidence Geometry approach to Dictionary Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Dictionary Learning (aka Sparse Coding) problem of obtaining a
sparse representation of data points, by learning \emph{dictionary vectors}
upon which the data points can be written as sparse linear combinations. We
view this problem from a geometry perspective as the spanning set of a subspace
arrangement, and focus on understanding the case when the underlying hypergraph
of the subspace arrangement is specified. For this Fitted Dictionary Learning
problem, we completely characterize the combinatorics of the associated
subspace arrangements (i.e.\ their underlying hypergraphs). Specifically, a
combinatorial rigidity-type theorem is proven for a type of geometric incidence
system. The theorem characterizes the hypergraphs of subspace arrangements that
generically yield (a) at least one dictionary (b) a locally unique dictionary
(i.e.\ at most a finite number of isolated dictionaries) of the specified size.
We are unaware of prior application of combinatorial rigidity techniques in the
setting of Dictionary Learning, or even in machine learning. We also provide a
systematic classification of problems related to Dictionary Learning together
with various algorithms, their assumptions and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7347</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7347</id><created>2014-02-28</created><authors><author><keyname>Wang</keyname><forenames>Menghan</forenames></author><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author></authors><title>Cayley Analysis of Mechanism Configuration Spaces using CayMos: Software
  Functionalities and Architecture</title><categories>cs.CG</categories><comments>arXiv admin note: text overlap with arXiv:1112.6008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a common class of 2D mechanisms called 1-dof tree decomposable linkages,
we present a software CayMos which uses new theoretical results to implement
efficient algorithmic solutions for: (a) meaningfully representing and
visualizing the connected components in the Euclidean realization space; (b)
finding a path of continuous motion between two realizations in the same
connected component, with or without restricting the realization type
(sometimes called orientation type); (c) finding two ``closest'' realizations
in different connected components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7350</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7350</id><created>2014-02-28</created><authors><author><keyname>Shechtman</keyname><forenames>Yoav</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Cohen</keyname><forenames>Oren</forenames></author><author><keyname>Chapman</keyname><forenames>Henry N.</forenames></author><author><keyname>Miao</keyname><forenames>Jianwei</forenames></author><author><keyname>Segev</keyname><forenames>Mordechai</forenames></author></authors><title>Phase Retrieval with Application to Optical Imaging</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This review article provides a contemporary overview of phase retrieval in
optical imaging, linking the relevant optical physics to the information
processing methods and algorithms. Its purpose is to describe the current state
of the art in this area, identify challenges, and suggest vision and areas
where signal processing methods can have a large impact on optical imaging and
on the world of imaging at large, with applications in a variety of fields
ranging from biology and chemistry to physics and engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7351</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7351</id><created>2014-02-28</created><authors><author><keyname>Hegazy</keyname><forenames>Osman</forenames></author><author><keyname>Soliman</keyname><forenames>Omar S.</forenames></author><author><keyname>Salam</keyname><forenames>Mustafa Abdul</forenames></author></authors><title>A Machine Learning Model for Stock Market Prediction</title><categories>cs.CE cs.NE</categories><comments>7 Pages. arXiv admin note: substantial text overlap with
  arXiv:1402.6366</comments><journal-ref>International Journal of Computer Science and Telecommunications
  [Volume 4, Issue 12, December 2013]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stock market prediction is the act of trying to determine the future value of
a company stock or other financial instrument traded on a financial exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7352</identifier>
 <datestamp>2014-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7352</id><created>2014-02-28</created><authors><author><keyname>Wang</keyname><forenames>Hanlei</forenames></author><author><keyname>Cheng</keyname><forenames>Long</forenames></author></authors><title>Second-Order Consensus of Networked Mechanical Systems With
  Communication Delays</title><categories>cs.SY math.OC</categories><comments>16 pages, 5 figures, submitted to IEEE Transactions on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the second-order consensus problem for networked
mechanical systems subjected to nonuniform communication delays, and the
mechanical systems are assumed to interact on a general directed topology. We
propose an adaptive controller plus a distributed velocity observer to realize
the objective of second-order consensus. It is shown that both the positions
and velocities of the mechanical agents synchronize, and furthermore, the
velocities of the mechanical agents converge to the scaled weighted average
value of their initial ones. We further demonstrate that the proposed
second-order consensus scheme can be used to solve the leader-follower
synchronization problem with a constant-velocity leader and under constant
communication delays. Simulation results are provided to illustrate the
performance of the proposed adaptive controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1402.7359</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1402.7359</id><created>2014-02-28</created><authors><author><keyname>Low</keyname><forenames>Guang Hao</forenames></author><author><keyname>Yoder</keyname><forenames>Theodore J.</forenames></author><author><keyname>Chuang</keyname><forenames>Isaac L.</forenames></author></authors><title>Quantum Inference on Bayesian Networks</title><categories>quant-ph cs.DS</categories><comments>8 pages, 3 figures. Submitted to PRX</comments><journal-ref>Physical Review A 2014</journal-ref><doi>10.1103/PhysRevA.89.062315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing exact inference on Bayesian networks is known to be #P-hard.
Typically approximate inference techniques are used instead to sample from the
distribution on query variables given the values $e$ of evidence variables.
Classically, a single unbiased sample is obtained from a Bayesian network on
$n$ variables with at most $m$ parents per node in time
$\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the
evidence might occur in the first place. By implementing a quantum version of
rejection sampling, we obtain a square-root speedup, taking
$\mathcal{O}(n2^mP(e)^{-\frac12})$ time per sample. We exploit the Bayesian
network's graph structure to efficiently construct a quantum state, a q-sample,
representing the intended classical distribution, and also to efficiently apply
amplitude amplification, the source of our speedup. Thus, our speedup is
notable as it is unrelativized -- we count primitive operations and require no
blackbox oracle queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0001</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0001</id><created>2014-02-28</created><authors><author><keyname>Prakash</keyname><forenames>T Shiva</forenames></author><author><keyname>Raja</keyname><forenames>K B</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Iyengar</keyname><forenames>S S</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Link-Reliability Based Two-Hop Routing for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>15 pages. arXiv admin note: substantial text overlap with
  arXiv:1402.7198</comments><journal-ref>International Journal of Information Processing, 7(1), 15-29, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) emerge as underlying infrastructures for new
classes of large scale net- worked embedded systems. However, WSNs system
designers must fulfill the Quality-of-Service (QoS) requirements imposed by the
applications (and users). Very harsh and dynamic physical environments and
extremely limited energy/computing/memory/communication node resources are
major obstacles for satisfying QoS metrics such as reliability, timeliness and
system lifetime. The limited communication range of WSN nodes, link asymmetry
and the characteristics of the physical environment lead to a major source of
QoS degradation in WSNs. This paper proposes a Link Reliability based Two-Hop
Routing protocol for wireless Sensor Networks (WSNs). The protocol achieves to
reduce packet deadline miss ratio while consid- ering link reliability, two-hop
velocity and power efficiency and utilizes memory and computational effective
methods for estimating the link metrics. Numerical results provide insights
that the protocol has a lower packet deadline miss ratio and longer sensor
network lifetime. The results show that the proposed protocol is a feasible
solution to the QoS routing problem in wireless sensor networks that support
real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0012</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0012</id><created>2014-02-28</created><updated>2015-03-15</updated><authors><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>A Stochastic Geometry Analysis of Inter-cell Interference Coordination
  and Intra-cell Diversity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-cell interference coordination (ICIC) and intra-cell diversity (ICD)
play important roles in improving cellular downlink coverage. Modeling cellular
base stations (BSs) as a homogeneous Poisson point process (PPP), this paper
provides explicit finite-integral expressions for the coverage probability with
ICIC and ICD, taking into account the temporal/spectral correlation of the
signal and interference. In addition, we show that in the high-reliability
regime, where the user outage probability goes to zero, ICIC and ICD affect the
network coverage in drastically different ways: ICD can provide order gain
while ICIC only offers linear gain. In the high-spectral efficiency regime
where the SIR threshold goes to infinity, the order difference in the coverage
probability does not exist, however the linear difference makes ICIC a better
scheme than ICD for realistic path loss exponents. Consequently, depending on
the SIR requirements, different combinations of ICIC and ICD optimize the
coverage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0017</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0017</id><created>2014-02-28</created><updated>2014-04-10</updated><authors><author><keyname>Majkic</keyname><forenames>Zoran</forenames></author></authors><title>Intensional RDB Manifesto: a Unifying NewSQL Model for Flexible Big Data</title><categories>cs.DB</categories><comments>29 pages. arXiv admin note: text overlap with arXiv:1103.0967,
  arXiv:1103.0680</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new family of Intensional RDBs (IRDBs) which
extends the traditional RDBs with the Big Data and flexible and 'Open schema'
features, able to preserve the user-defined relational database schemas and all
preexisting user's applications containing the SQL statements for a deployment
of such a relational data. The standard RDB data is parsed into an internal
vector key/value relation, so that we obtain a column representation of data
used in Big Data applications, covering the key/value and column-based Big Data
applications as well, into a unifying RDB framework. We define a query
rewriting algorithm, based on the GAV Data Integration methods, so that each
user-defined SQL query is rewritten into a SQL query over this vector relation,
and hence the user-defined standard RDB schema is maintained as an empty global
schema for the RDB schema modeling of data and as the SQL interface to stored
vector relation. Such an IRDB architecture is adequate for the massive
migrations from the existing slow RDBMSs into this new family of fast IRDBMSs
by offering a Big Data and new flexible schema features as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0034</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0034</id><created>2014-02-28</created><updated>2014-03-22</updated><authors><author><keyname>Eppe</keyname><forenames>Manfred</forenames></author></authors><title>Tractable Epistemic Reasoning with Functional Fluents, Static Causal
  Laws and Postdiction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an epistemic action theory for tractable epistemic reasoning as an
extension to the h-approximation (HPX) theory. In contrast to existing
tractable approaches, the theory supports functional fluents and postdictive
reasoning with static causal laws. We argue that this combination is
particularly synergistic because it allows one not only to perform direct
postdiction about the conditions of actions, but also indirect postdiction
about the conditions of static causal laws. We show that despite the richer
expressiveness, the temporal projection problem remains tractable (polynomial),
and therefore the planning problem remains in NP. We present the operational
semantics of our theory as well as its formulation as Answer Set Programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0036</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0036</id><created>2014-02-28</created><authors><author><keyname>Wang</keyname><forenames>Menghan</forenames></author></authors><title>Dynamic Decision Process Modeling and Relation-line Handling in
  Distributed Cooperative Modeling System</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Distributed Cooperative Modeling System (DCMS) solves complex decision
problems involving a lot of participants with different viewpoints by network
based distributed modeling and multi-template aggregation.
  This thesis aims at extending the system with support for dynamic decision
making process. First, the thesis presents a discussion of characteristics and
optimal policy finding Markov Decision Process as well as a brief introduction
to dynamic Bayesian decision network, which is inherently equal to MDP. After
that, discussion and implementation of prediction in Markov process for both
discrete and continuous random variable are given, as well as several different
kinds of correlation analysis among multiple indices which could help
decision-makers to realize the interaction of indices and design appropriate
policy.
  Appending history data of Macau industry, as the foundation of extending
DCMS, is introduced. Additional works include rearrangement of graphical class
hierarchy in DCMS, which in turn allows convenient implementation of curve
relation-line, which makes template modeling clearer and friendlier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0041</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0041</id><created>2014-02-28</created><updated>2014-08-01</updated><authors><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>Individual dynamics induces symmetry in network controllability</title><categories>math.OC cs.SI physics.soc-ph</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling complex networked systems to a desired state is a key research
goal in contemporary science. Despite recent advances in studying the impact of
network topology on controllability, a comprehensive understanding of the
synergistic effect of network topology and individual dynamics on
controllability is still lacking. Here we offer a theoretical study with
particular interest in the diversity of dynamic units characterized by
different types of individual dynamics. Interestingly, we find a global
symmetry accounting for the invariance of controllability with respect to
exchanging the densities of any two different types of dynamic units,
irrespective of the network topology. The highest controllability arises at the
global symmetry point, at which different types of dynamic units are of the
same density. The lowest controllability occurs when all self-loops are either
completely absent or present with identical weights. These findings further
improve our understanding of network controllability and have implications for
devising the optimal control of complex networked systems in a wide range of
fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0052</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0052</id><created>2014-03-01</created><authors><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>IDSL, INRIA Saclay - Ile de France, CMB</affiliation></author></authors><title>TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding
  Initiative guidelines</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an attempt to customise the TEI (Text Encoding
Initiative) guidelines in order to offer the possibility to incorporate TBX
(TermBase eXchange) based terminological entries within any kind of TEI
documents. After presenting the general historical, conceptual and technical
contexts, we describe the various design choices we had to take while creating
this customisation, which in turn have led to make various changes in the
actual TBX serialisation. Keeping in mind the objective to provide the TEI
guidelines with, again, an onomasiological model, we try to identify the best
comprise in maintaining both the isomorphism with the existing TBX Basic
standard and the characteristics of the TEI framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0054</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0054</id><created>2014-03-01</created><updated>2015-04-23</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Resource Allocation for Secure Communication in
  Cognitive Radio Networks with Wireless Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted with minor revisions for publication as a regular paper in
  the IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study resource allocation for multiuser multiple-input
single-output secondary communication systems with multiple system design
objectives. We consider cognitive radio networks where the secondary receivers
are able to harvest energy from the radio frequency when they are idle. The
secondary system provides simultaneous wireless power and secure information
transfer to the secondary receivers. We propose a multi-objective optimization
framework for the design of a Pareto optimal resource allocation algorithm
based on the weighted Tchebycheff approach. In particular, the algorithm design
incorporates three important system objectives: total transmit power
minimization, energy harvesting efficiency maximization, and interference power
leakage-to-transmit power ratio minimization. The proposed framework takes into
account a quality of service requirement regarding communication secrecy in the
secondary system and the imperfection of the channel state information of
potential eavesdroppers (idle secondary receivers and primary receivers) at the
secondary transmitter. The adopted multi-objective optimization problem is
non-convex and is recast as a convex optimization problem via semidefinite
programming (SDP) relaxation. It is shown that the global optimal solution of
the original problem can be constructed by exploiting both the primal and the
dual optimal solutions of the SDP relaxed problem. Besides, two suboptimal
resource allocation schemes for the case when the solution of the dual problem
is unavailable for constructing the optimal solution are proposed. Numerical
results not only demonstrate the close-to-optimal performance of the proposed
suboptimal schemes, but also unveil an interesting trade-off between the
considered conflicting system design objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0057</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0057</id><created>2014-03-01</created><updated>2014-11-21</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Tian</forenames></author><author><keyname>Yang</keyname><forenames>Cheng</forenames></author></authors><title>Real-time Topic-aware Influence Maximization Using Preprocessing</title><categories>cs.SI cs.LG</categories><comments>1 figure and 10 tables. Extended abstract</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is the task of finding a set of seed nodes in a social
network such that the influence spread of these seed nodes based on certain
influence diffusion model is maximized. Topic-aware influence diffusion models
have been recently proposed to address the issue that influence between a pair
of users are often topic-dependent and information, ideas, innovations etc.
being propagated in networks (referred collectively as items in this paper) are
typically mixtures of topics. In this paper, we focus on the topic-aware
influence maximization task. In particular, we study preprocessing methods for
these topics to avoid redoing influence maximization for each item from
scratch. We explore two preprocessing algorithms with theoretical
justifications. Our empirical results on data obtained in a couple of existing
studies demonstrate that one of our algorithms stands out as a strong candidate
providing microsecond online response time and competitive influence spread,
with reasonable preprocessing effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0062</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0062</id><created>2014-03-01</created><authors><author><keyname>De Castro</keyname><forenames>Pedro Machado Manh&#xe3;es</forenames><affiliation>CIn</affiliation></author><author><keyname>M&#xe9;rigot</keyname><forenames>Quentin</forenames><affiliation>LJK</affiliation></author><author><keyname>Thibert</keyname><forenames>Boris</forenames><affiliation>LJK</affiliation></author></authors><title>Intersection of paraboloids and application to Minkowski-type problems</title><categories>cs.CG math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study the intersection (or union) of the convex hull of N
confocal paraboloids (or ellipsoids) of revolution. This study is motivated by
a Minkowski-type problem arising in geometric optics. We show that in each of
the four cases, the combinatorics is given by the intersection of a power
diagram with the unit sphere. We prove the complexity is O(N) for the
intersection of paraboloids and Omega(N^2) for the intersection and the union
of ellipsoids. We provide an algorithm to compute these intersections using the
exact geometric computation paradigm. This algorithm is optimal in the case of
the intersection of ellipsoids and is used to solve numerically the far-field
reflector problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0068</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0068</id><created>2014-03-01</created><authors><author><keyname>Nithya</keyname><forenames>C.</forenames></author><author><keyname>Saravanan</keyname><forenames>K.</forenames></author></authors><title>Semantic Annotation and Search for Educational Resources Supporting
  Distance Learning</title><categories>cs.IR cs.CY cs.DL</categories><comments>Linked Data, Semantic search, Cloud Applications, Web services,
  Semantic annotation, Ontology</comments><journal-ref>IJETT V8(6),277-285 February 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V8P252</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Multimedia educational resources play an important role in education,
particularly for distance learning environments. With the rapid growth of the
multimedia web, large numbers of education articles video resources are
increasingly being created by several different organizations. It is crucial to
explore, share, reuse, and link these educational resources for better
e-learning experiences. Most of the video resources are currently annotated in
an isolated way, which means that they lack semantic connections. Thus,
providing the facilities for annotating these video resources is highly
demanded. These facilities create the semantic connections among video
resources and allow their metadata to be understood globally. Adopting Linked
Data technology, this paper introduces a video annotation and browser platform
with two online tools: Notitia and Sansu-Wolke. Notitia enables users to
semantically annotate video resources using vocabularies defined in the Linked
Data cloud. Sansu-Wolke allows users to browse semantically linked educational
video resources with enhanced web information from different online resources.
In the prototype development, the platform uses existing video resources for
education articles. The result of the initial development demonstrates the
benefits of applying Linked Data technology in the aspects of reusability,
scalability, and extensibility
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0087</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0087</id><created>2014-03-01</created><authors><author><keyname>Estrada</keyname><forenames>Francisco J.</forenames></author></authors><title>Temporal Image Fusion</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces temporal image fusion. The proposed technique builds
upon previous research in exposure fusion and expands it to deal with the
limited Temporal Dynamic Range of existing sensors and camera technologies. In
particular, temporal image fusion enables the rendering of long-exposure
effects on full frame-rate video, as well as the generation of arbitrarily long
exposures from a sequence of images of the same scene taken over time. We
explore the problem of temporal under-exposure, and show how it can be
addressed by selectively enhancing dynamic structure. Finally, we show that the
use of temporal image fusion together with content-selective image filters can
produce a range of striking visual effects on a given input sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0093</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0093</id><created>2014-03-01</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author><author><keyname>Marquez</keyname><forenames>Horacio J.</forenames></author></authors><title>Robust Nonlinear L2 Filtering of Uncertain Lipschitz Systems via Pareto
  Optimization</title><categories>cs.SY math.OC</categories><comments>21 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1010.0696</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for robust Hinfty filtering for a class of Lipschitz nonlinear
systems with time-varying uncertainties both in the linear and nonlinear parts
of the system is proposed in an LMI framework. The admissible Lipschitz
constant of the system and the disturbance attenuation level are maximized
simultaneously through convex multiobjective optimization. The resulting Hinfty
filter guarantees asymptotic stability of the estimation error dynamics with
exponential convergence and is robust against nonlinear additive uncertainty
and time-varying parametric uncertainties. Explicit bounds on the nonlinear
uncertainty are derived based on norm-wise and element-wise robustness
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0099</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0099</id><created>2014-03-01</created><authors><author><keyname>Shachnai</keyname><forenames>Hadas</forenames></author><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Parameterized Algorithms for Graph Partitioning Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a broad class of graph partitioning problems, where each problem is
specified by a graph $G=(V,E)$, and parameters $k$ and $p$. We seek a subset
$U\subseteq V$ of size $k$, such that $\alpha_1m_1 + \alpha_2m_2$ is at most
(or at least) $p$, where $\alpha_1,\alpha_2\in\mathbb{R}$ are constants
defining the problem, and $m_1, m_2$ are the cardinalities of the edge sets
having both endpoints, and exactly one endpoint, in $U$, respectively. This
class of fixed cardinality graph partitioning problems (FGPP) encompasses Max
$(k,n-k)$-Cut, Min $k$-Vertex Cover, $k$-Densest Subgraph, and $k$-Sparsest
Subgraph.
  Our main result is an $O^*(4^{k+o(k)}\Delta^k)$ algorithm for any problem in
this class, where $\Delta \geq 1$ is the maximum degree in the input graph.
This resolves an open question posed by Bonnet et al. [IPEC 2013]. We obtain
faster algorithms for certain subclasses of FGPPs, parameterized by $p$, or by
$(k+p)$. In particular, we give an $O^*(4^{p+o(p)})$ time algorithm for Max
$(k,n-k)$-Cut, thus improving significantly the best known $O^*(p^p)$ time
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0100</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0100</id><created>2014-03-01</created><authors><author><keyname>Ray</keyname><forenames>Abhishek</forenames></author><author><keyname>Mishra</keyname><forenames>Siba</forenames></author><author><keyname>Mohapatra</keyname><forenames>Durga Prasad</forenames></author></authors><title>A Novel Approach for Computing Dynamic Slices of Aspect-Oriented
  Programs</title><categories>cs.SE</categories><comments>7 Pages</comments><journal-ref>International Journal of Computer Information Systems, Vol.5, No.
  3, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamic slicing algorithm to compute the slices of
aspect-oriented programs. We use a dependence based intermediate program
representation called Aspect System Dependence Graph (AOSG) to represent
aspect-oriented programs. Then, we propose the dynamic slicing algorithm for
AOPs, which is an extended version of EMDS algorithm for object-oriented
programs. Our algorithm is based on marking and unmarking of the edges of AOSG
appropriately during runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0126</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0126</id><created>2014-03-01</created><authors><author><keyname>Gorla</keyname><forenames>Elisa</forenames></author><author><keyname>Massierer</keyname><forenames>Maike</forenames></author></authors><title>Point compression for the trace zero subgroup over a small degree
  extension field</title><categories>math.AG cs.CR</categories><comments>23 pages, to appear in Designs, Codes and Cryptography</comments><msc-class>14G50, 11G25, 14H52, 11T71, 14K15</msc-class><doi>10.1007/s10623-014-9921-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Semaev's summation polynomials, we derive a new equation for the
$\mathbb{F}_q$-rational points of the trace zero variety of an elliptic curve
defined over $\mathbb{F}_q$. Using this equation, we produce an optimal-size
representation for such points. Our representation is compatible with scalar
multiplication. We give a point compression algorithm to compute the
representation and a decompression algorithm to recover the original point (up
to some small ambiguity). The algorithms are efficient for trace zero varieties
coming from small degree extension fields. We give explicit equations and
discuss in detail the practically relevant cases of cubic and quintic field
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0135</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0135</id><created>2014-03-01</created><authors><author><keyname>Consoli</keyname><forenames>Sergio</forenames></author><author><keyname>Recupero</keyname><forenames>Diego Reforgiato</forenames></author><author><keyname>Zavarella</keyname><forenames>Vanni</forenames></author></authors><title>A survey on tidal analysis and forecasting methods for Tsunami detection</title><categories>cs.CE math.OC physics.ao-ph</categories><comments>Review paper</comments><journal-ref>Science of Tsunami Hazards, 33(1):1-56; Feb. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate analysis and forecasting of tidal level are very important tasks for
human activities in oceanic and coastal areas. They can be crucial in
catastrophic situations like occurrences of Tsunamis in order to provide a
rapid alerting to the human population involved and to save lives. Conventional
tidal forecasting methods are based on harmonic analysis using the least
squares method to determine harmonic parameters. However, a large number of
parameters and long-term measured data are required for precise tidal level
predictions with harmonic analysis. Furthermore, traditional harmonic methods
rely on models based on the analysis of astronomical components and they can be
inadequate when the contribution of non-astronomical components, such as the
weather, is significant. Other alternative approaches have been developed in
the literature in order to deal with these situations and provide predictions
with the desired accuracy, with respect also to the length of the available
tidal record. These methods include standard high or band pass filtering
techniques, although the relatively deterministic character and large amplitude
of tidal signals make special techniques, like artificial neural networks and
wavelets transform analysis methods, more effective. This paper is intended to
provide the communities of both researchers and practitioners with a broadly
applicable, up to date coverage of tidal analysis and forecasting methodologies
that have proven to be successful in a variety of circumstances, and that hold
particular promise for success in the future. Classical and novel methods are
reviewed in a systematic and consistent way, outlining their main concepts and
components, similarities and differences, advantages and disadvantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0153</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0153</id><created>2014-03-01</created><authors><author><keyname>Nandi</keyname><forenames>Utpal</forenames></author><author><keyname>Mandal</keyname><forenames>Jyotsna Kumar</forenames></author></authors><title>Size Adaptive Region Based Huffman Compression Technique</title><categories>cs.IT math.IT</categories><comments>3 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A loss-less compression technique is proposed which uses a variable length
Region formation technique to divide the input file into a number of variable
length regions. Huffman codes are obtained for entire file after formation of
regions. Symbols of each region are compressed one by one. Comparisons are made
among proposed technique, Region Based Huffman compression technique and
classical Huffman technique. The proposed technique offers better compression
ratio for some files than other two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0156</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0156</id><created>2014-03-01</created><authors><author><keyname>Babaie</keyname><forenames>Tahereh</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Abeysuriya</keyname><forenames>Romesh</forenames></author></authors><title>Sleep Analytics and Online Selective Anomaly Detection</title><categories>cs.LG</categories><comments>Submitted to 20th ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to
model a specific scenario emerging from research in sleep science. Scientists
have segmented sleep into several stages and stage two is characterized by two
patterns (or anomalies) in the EEG time series recorded on sleep subjects.
These two patterns are sleep spindle (SS) and K-complex. The OSAD problem was
introduced to design a residual system, where all anomalies (known and unknown)
are detected but the system only triggers an alarm when non-SS anomalies
appear. The solution of the OSAD problem required us to combine techniques from
both machine learning and control theory. Experiments on data from real
subjects attest to the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0157</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0157</id><created>2014-03-01</created><authors><author><keyname>Babaie</keyname><forenames>Tahereh</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Ardon</keyname><forenames>Sebastien</forenames></author></authors><title>Network Traffic Decomposition for Anomaly Detection</title><categories>cs.LG cs.NI</categories><comments>Submitted to The Journal of Data Mining and Knowledge Discovery
  (DAMI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the detection of network anomalies like Denial of
Service (DoS) attacks and port scans in a unified manner. While there has been
an extensive amount of research in network anomaly detection, current state of
the art methods are only able to detect one class of anomalies at the cost of
others. The key tool we will use is based on the spectral decomposition of a
trajectory/hankel matrix which is able to detect deviations from both between
and within correlation present in the observed network traffic data. Detailed
experiments on synthetic and real network traces shows a significant
improvement in detection capability over competing approaches. In the process
we also address the issue of robustness of anomaly detection systems in a
principled fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0162</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0162</id><created>2014-03-02</created><authors><author><keyname>Deng</keyname><forenames>Xuegong</forenames></author></authors><title>A Solution to Bargaining Problem on Divisible Goods</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-person bargaining problem is considered as to allocate a number of goods
between two players. This paper suggests that any non-trivial division of goods
cause a non-zero change on the solution of bargaining. So, a axiom of sharing
division is presented, as an alternative axiom to Nash axiom of independence of
irrelevant alternatives and Kalai-Smorodinsky axiom of monotonicity. This
solution is targeted at the partialities of Nash and Kalai-Smorodinsky solution
on some specific issues, but not to say it is better than others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0173</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0173</id><created>2014-03-02</created><authors><author><keyname>Thai</keyname><forenames>Chan Dai Truyen</forenames></author><author><keyname>Berbineau</keyname><forenames>Marion</forenames></author></authors><title>Coordinated Direct and Relay Schemes for Two-Hop Communication in VANETS</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to accommodate increasing need and offer communication with high
performance, both vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V)
communications are exploited. The advantages of static nodes and vehicular
nodes are combined to achieve an optimal routing scheme. In this paper, we
consider the communications between a static node and the vehicular nodes
moving in an adjacent area of it. The adjacent area is defined as the zone
where a vehicular can communicate with the static node within maximum two hops.
We only consider single-hop and two-hop transmissions because these
transmissions can be considered as building blocks to construct transmissions
with a higher number of hops. Different cases in which an uplink or a downlink
for the two-hop user combined with an uplink or a downlink for the single-hop
user correspond to different CDR schemes. Using side information to
intentionally cancel the interference, Network Coding (NC), CDR, overhearing
and multi-way schemes aggregate communications flows in order to increase the
performance of the network. We apply the mentioned schemes to a V2I network and
propose novel schemes to optimally arrange and combine the transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0178</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0178</id><created>2014-03-02</created><updated>2014-11-23</updated><authors><author><keyname>Knudsen</keyname><forenames>Mathias B&#xe6;k Tejs</forenames></author></authors><title>Additive Spanners: A Simple Construction</title><categories>cs.DS</categories><comments>To appear at proceedings of the 14th Scandinavian Symposium and
  Workshop on Algorithm Theory (SWAT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider additive spanners of unweighted undirected graphs. Let $G$ be a
graph and $H$ a subgraph of $G$. The most na\&quot;ive way to construct an additive
$k$-spanner of $G$ is the following: As long as $H$ is not an additive
$k$-spanner repeat: Find a pair $(u,v) \in H$ that violates the
spanner-condition and a shortest path from $u$ to $v$ in $G$. Add the edges of
this path to $H$.
  We show that, with a very simple initial graph $H$, this na\&quot;ive method gives
additive $6$- and $2$-spanners of sizes matching the best known upper bounds.
For additive $2$-spanners we start with $H=\emptyset$ and end with $O(n^{3/2})$
edges in the spanner. For additive $6$-spanners we start with $H$ containing
$\lfloor n^{1/3} \rfloor$ arbitrary edges incident to each node and end with a
spanner of size $O(n^{4/3})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0184</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0184</id><created>2014-03-02</created><updated>2014-03-12</updated><authors><author><keyname>Barbulescu</keyname><forenames>Razvan</forenames><affiliation>LORIA</affiliation></author><author><keyname>Lachand</keyname><forenames>Armand</forenames><affiliation>IECL</affiliation></author></authors><title>Some mathematical remarks on the polynomial selection in NFS</title><categories>cs.CR math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the proportion of smooth (free of large prime
factors) values of a binary form $F(X_1,X_2)\in\Z[X_1,X_2]$. In a particular
case, we give an asymptotic equivalent for this proportion which depends on
$F$. This is related to Murphy's $\alpha$ function, which is known in the
cryptographic community, but which has not been studied before from a
mathematical point of view. Our result proves that, when $\alpha(F)$ is small,
$F$ has a high proportion of smooth values. This has consequences on the first
step, called polynomial selection, of the Number Field Sieve, the fastest
algorithm of integer factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0185</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0185</id><created>2014-03-02</created><updated>2016-01-20</updated><authors><author><keyname>Klimek</keyname><forenames>Radoslaw</forenames></author></authors><title>Behavior recognition and analysis in smart environments for
  context-aware applications</title><categories>cs.LO</categories><comments>Accepted conference paper: Proceedings of the IEEE International
  Conference on Systems, Man, and Cybernetics (SMC 2015), October 9--12, 2015,
  Hong Kong, pp. 1949-1955. IEEE Computer Society 2015. Available at
  DOI:10.1109/SMC.2015.340 or
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7379472</comments><doi>10.1109/SMC.2015.340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing accurate/suitable information on behaviors in sma\-rt environments
is a challenging and crucial task in pervasive computing where
context-awareness and pro-activity are of fundamental importance. Behavioral
identifications enable to abstract higher-level concepts that are interesting
to applications. This work proposes the unified logical-based framework to
recognize and analyze behavioral specifications understood as a formal logic
language that avoids ambiguity typical for natural languages. Automatically
discovering behaviors from sensory data streams as formal specifications is of
fundamental importance to build seamless human-computer interactions. Thus, the
knowledge about environment behaviors expressed in terms of temporal logic
formulas constitutes a base for the reactive and precise reasoning processes to
support trustworthy, unambiguous and pro-active decisions for applications that
are smart and context-aware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0190</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0190</id><created>2014-03-02</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>RZA-NLMF algorithm based adaptive sparse sensing for realizing
  compressive sensing problems</title><categories>cs.IT math.IT</categories><comments>15 pages, 9 figures, submitted for journal</comments><doi>10.1186/1687-6180-2014-125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear sparse sensing (NSS) techniques have been adopted for realizing
compressive sensing in many applications such as Radar imaging. Unlike the NSS,
in this paper, we propose an adaptive sparse sensing (ASS) approach using
reweighted zero-attracting normalized least mean fourth (RZA-NLMF) algorithm
which depends on several given parameters, i.e., reweighted factor,
regularization parameter and initial step-size. First, based on the independent
assumption, Cramer Rao lower bound (CRLB) is derived as for the trademark of
performance comparisons. In addition, reweighted factor selection method is
proposed for achieving robust estimation performance. Finally, to verify the
algorithm, Monte Carlo based computer simulations are given to show that the
ASS achieves much better mean square error (MSE) performance than the NSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0192</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0192</id><created>2014-03-02</created><updated>2015-04-20</updated><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Shan</keyname><forenames>Lin</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Compressive sensing based Bayesian sparse channel estimation for OFDM
  communication systems: high performance and low complexity</title><categories>cs.IT math.IT</categories><comments>24 pages,16 figures, submitted for a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In orthogonal frequency division modulation (OFDM) communication systems,
channel state information (CSI) is required at receiver due to the fact that
frequency-selective fading channel leads to disgusting inter-symbol
interference (ISI) over data transmission. Broadband channel model is often
described by very few dominant channel taps and they can be probed by
compressive sensing based sparse channel estimation (SCE) methods, e.g.,
orthogonal matching pursuit algorithm, which can take the advantage of sparse
structure effectively in the channel as for prior information. However, these
developed methods are vulnerable to both noise interference and column
coherence of training signal matrix. In other words, the primary objective of
these conventional methods is to catch the dominant channel taps without a
report of posterior channel uncertainty. To improve the estimation performance,
we proposed a compressive sensing based Bayesian sparse channel estimation
(BSCE) method which can not only exploit the channel sparsity but also mitigate
the unexpected channel uncertainty without scarifying any computational
complexity. The propose method can reveal potential ambiguity among multiple
channel estimators that are ambiguous due to observation noise or correlation
interference among columns in the training matrix. Computer simulations show
that propose method can improve the estimation performance when comparing with
conventional SCE methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0214</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0214</id><created>2014-03-02</created><authors><author><keyname>Guang</keyname><forenames>Xuan</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author></authors><title>Variable-Rate Linear Network Error Correction MDS Codes</title><categories>cs.IT math.IT</categories><comments>Single column, 34 pages, submitted for publication. arXiv admin note:
  text overlap with arXiv:1311.7466, arXiv:1011.1377</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network communication, the source often transmits messages at several
different information rates within a session. How to deal with information
transmission and network error correction simultaneously under different rates
is introduced in this paper as a variable-rate network error correction
problem. Apparently, linear network error correction MDS codes are expected to
be used for these different rates. For this purpose, designing a linear network
error correction MDS code based on the existing results for each information
rate is an efficient solution. In order to solve the problem more efficiently,
we present the concept of variable-rate linear network error correction MDS
codes, that is, these linear network error correction MDS codes of different
rates have the same local encoding kernel at each internal node. Further, we
propose an approach to construct such a family of variable-rate network MDS
codes and give an algorithm for efficient implementation. This approach saves
the storage space for each internal node, and resources and time for the
transmission on networks. Moreover, the performance of our proposed algorithm
is analyzed, including the field size, the time complexity, the encoding
complexity at the source node, and the decoding methods. Finally, a random
method is introduced for constructing variable-rate network MDS codes and we
obtain a lower bound on the success probability of this random method, which
shows that this probability will approach to one as the base field size goes to
infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0222</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0222</id><created>2014-03-02</created><updated>2014-12-20</updated><authors><author><keyname>Chen</keyname><forenames>Hubie</forenames><affiliation>Universidad del Pa&#xed;s Vasco and IKERBASQUE</affiliation></author></authors><title>Beyond Q-Resolution and Prenex Form: A Proof System for Quantified
  Constraint Satisfaction</title><categories>cs.LO cs.AI cs.CC</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  23, 2014) lmcs:1012</journal-ref><doi>10.2168/LMCS-10(4:14)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantified constraint satisfaction problem (QCSP) which is to
decide, given a structure and a first-order sentence (not assumed here to be in
prenex form) built from conjunction and quantification, whether or not the
sentence is true on the structure. We present a proof system for certifying the
falsity of QCSP instances and develop its basic theory; for instance, we
provide an algorithmic interpretation of its behavior. Our proof system places
the established Q-resolution proof system in a broader context, and also allows
us to derive QCSP tractability results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0224</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0224</id><created>2014-03-02</created><authors><author><keyname>Sinha</keyname><forenames>Mitali</forenames></author><author><keyname>Pattanaik</keyname><forenames>Suchismita</forenames></author><author><keyname>Mohanty</keyname><forenames>Rakesh</forenames></author><author><keyname>Tripathy</keyname><forenames>Prachi</forenames></author></authors><title>Experimental Study of A Novel Variant of Fiduccia Mattheyses(FM)
  Partitioning Algorithm</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioning is a well studied research problem in the area of VLSI physical
design automation. In this problem, input is an integrated circuit and output
is a set of almost equal disjoint blocks. The main objective of partitioning is
to assign the components of circuit to blocks in order to minimize the numbers
of inter-block connections. A partitioning algorithm using hypergraph was
proposed by Fiduccia and Mattheyses with linear time complexity which has been
popularly known as FM algorithm. Most of the hypergraph based partitioning
algorithms proposed in the literature are variants of FM algorithm. In this
paper, we have proposed a novel variant of FM algorithm by using pair wise
swapping technique. We have performed a comparative experimental study of FM
algorithm and our proposed algorithm using two dataset such as ISPD98 and
ISPD99. Experimental results show that performance of our proposed algorithm is
better than the FM algorithm using the above dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0230</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0230</id><created>2014-03-02</created><authors><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Bloodsworth</keyname><forenames>Peter</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Habib</keyname><forenames>Irfan</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author><author><keyname>Consortium</keyname><forenames>the neuGRID</forenames></author></authors><title>Research Traceability using Provenance Services for Biomedical Analysis</title><categories>cs.DB</categories><comments>9 pages, 5 figures; Proceedings of the 8th HealthGrid Int. Conference
  (HG'10). Paris, France. June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline the approach being developed in the neuGRID project to use
provenance management techniques for the purposes of capturing and preserving
the provenance data that emerges in the specification and execution of
workflows in biomedical analyses. In the neuGRID project a provenance service
has been designed and implemented that is intended to capture, store, retrieve
and reconstruct the workflow information needed to facilitate users in
conducting user analyses. We describe the architecture of the neuGRID
provenance service and discuss how the CRISTAL system from CERN is being
adapted to address the requirements of the project and then consider how a
generalised approach for provenance management could emerge for more generic
application to the (Health)Grid community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0240</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0240</id><created>2014-03-02</created><authors><author><keyname>Sbalzarini</keyname><forenames>Ivo F.</forenames></author><author><keyname>Schneider</keyname><forenames>Sophie</forenames></author><author><keyname>Cardinale</keyname><forenames>Janick</forenames></author></authors><title>Particle methods enable fast and simple approximation of Sobolev
  gradients in image segmentation</title><categories>cs.CV cs.CE cs.NA q-bio.QM</categories><comments>21 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-image analysis is challenging due to inhomogeneous intensity
distributions and high levels of noise in the images. Bayesian inference
provides a principled way for regularizing the problem using prior knowledge. A
fundamental choice is how one measures &quot;distances&quot; between shapes in an image.
It has been shown that the straightforward geometric L2 distance is degenerate
and leads to pathological situations. This is avoided when using Sobolev
gradients, rendering the segmentation problem less ill-posed. The high
computational cost and implementation overhead of Sobolev gradients, however,
have hampered practical applications. We show how particle methods as applied
to image segmentation allow for a simple and computationally efficient
implementation of Sobolev gradients. We show that the evaluation of Sobolev
gradients amounts to particle-particle interactions along the contour in an
image. We extend an existing particle-based segmentation algorithm to using
Sobolev gradients. Using synthetic and real-world images, we benchmark the
results for both 2D and 3D images using piecewise smooth and piecewise constant
region models. The present particle approximation of Sobolev gradients is 2.8
to 10 times faster than the previous reference implementation, but retains the
known favorable properties of Sobolev gradients. This speedup is achieved by
using local particle-particle interactions instead of solving a global Poisson
equation at each iteration. The computational time per iteration is higher for
Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition
the optimization problem, however, a smaller number of overall iterations may
be necessary for the algorithm to converge, which can in some cases amortize
the higher per-iteration cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0252</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0252</id><created>2014-03-02</created><authors><author><keyname>Larkin</keyname><forenames>Daniel H.</forenames></author><author><keyname>Sen</keyname><forenames>Siddhartha</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author></authors><title>A Back-to-Basics Empirical Study of Priority Queues</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory community has proposed several new heap variants in the recent
past which have remained largely untested experimentally. We take the field
back to the drawing board, with straightforward implementations of both classic
and novel structures using only standard, well-known optimizations. We study
the behavior of each structure on a variety of inputs, including artificial
workloads, workloads generated by running algorithms on real map data, and
workloads from a discrete event simulator used in recent systems networking
research. We provide observations about which characteristics are most
correlated to performance. For example, we find that the L1 cache miss rate
appears to be strongly correlated with wallclock time. We also provide
observations about how the input sequence affects the relative performance of
the different heap variants. For example, we show (both theoretically and in
practice) that certain random insertion-deletion sequences are degenerate and
can lead to misleading results. Overall, our findings suggest that while the
conventional wisdom holds in some cases, it is sorely mistaken in others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0258</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0258</id><created>2014-03-02</created><authors><author><keyname>Karimoddini1</keyname><forenames>Ali</forenames></author><author><keyname>Karimadini</keyname><forenames>Mohammad</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author></authors><title>Decentralized Hybrid Formation Control of Unmanned Aerial Vehicles</title><categories>cs.SY</categories><report-no>NCAT-ACCESS-14-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a decentralized hybrid supervisory control approach for a
team of unmanned helicopters that are involved in a leader-follower formation
mission. Using a polar partitioning technique, the motion dynamics of the
follower helicopters are abstracted to finite state machines. Then, a discrete
supervisor is designed in a modular way for different components of the
formation mission including reaching the formation, keeping the formation, and
collision avoidance. Furthermore, a formal technique is developed to design the
local supervisors decentralizedly, so that the team of helicopters as whole,
can cooperatively accomplish a collision-free formation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0259</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0259</id><created>2014-03-02</created><authors><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>The Effect of Block-wise Feedback on the Throughput-Delay Trade-off in
  Streaming</title><categories>cs.IT cs.MM math.IT</categories><comments>Accepted to INFOCOM 2014 Workshop on Communication and Networking
  Techniques for Contemporary Video</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike traditional file transfer where only total delay matters, streaming
applications impose delay constraints on each packet and require them to be in
order. To achieve fast in-order packet decoding, we have to compromise on the
throughput. We study this trade-off between throughput and in-order decoding
delay, and in particular how it is affected by the frequency of block-wise
feedback to the source. When there is immediate feedback, we can achieve the
optimal throughput and delay simultaneously. But as the feedback delay
increases, we have to compromise on at least one of these metrics. We present a
spectrum of coding schemes that span different points on the throughput-delay
trade-off. Depending upon the delay-sensitivity and bandwidth limitations of
the application, one can choose an appropriate operating point on this
trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0268</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0268</id><created>2014-03-02</created><updated>2014-12-15</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Tropical optimization problems with application to project scheduling
  with minimum makespan</title><categories>math.OC cs.SY</categories><comments>21 pages</comments><msc-class>65K10 (Primary), 15A80, 65K05, 90C48, 90B35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multidimensional optimization problems in the framework of
tropical mathematics. The problems are formulated to minimize a nonlinear
objective function that is defined on vectors in a finite-dimensional
semimodule over an idempotent semifield and calculated by means of
multiplicative conjugate transposition. We start with an unconstrained problem
and offer two complete direct solutions, which follow different argumentation
schemes. The first solution consists of the derivation of a sharp lower bound
for the objective function and the solving of an equation to find all vectors
that yield the bound. The second is based on extremal properties of the
spectral radius of matrices and involves the evaluation of this radius for a
certain matrix. The second solution is then extended to problems with boundary
constraints that specify the feasible solution set by a double inequality, and
with a linear inequality constraint given by a matrix. We apply the results
obtained to solve problems in project scheduling under the minimum makespan
criterion subject to various precedence constraints imposed on the time of
initiation and completion of activities in the project. To illustrate the
solutions, simple numerical examples are also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0270</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0270</id><created>2014-03-02</created><updated>2014-09-07</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Kais</keyname><forenames>Sabre</forenames></author></authors><title>Quantum Random State Generation with Predefined Entanglement Constraint</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>International Journal of Quantum Information Vol. 12, No. 5 (2014)</journal-ref><doi>10.1142/S0219749914500300</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entanglement plays an important role in quantum communication, algorithms,
and error correction. Schmidt coefficients are correlated to the eigenvalues of
the reduced density matrix. These eigenvalues are used in Von Neumann entropy
to quantify the amount of the bipartite entanglement. In this paper, we map the
Schmidt basis and the associated coefficients to quantum circuits to generate
random quantum states. We also show that it is possible to adjust the
entanglement between subsystems by changing the quantum gates corresponding to
the Schmidt coefficients. In this manner, random quantum states with predefined
bipartite entanglement amounts can be generated using random Schmidt basis.
This provides a technique for generating equivalent quantum states for given
weighted graph states, which are very useful in the study of entanglement,
quantum computing, and quantum error correction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0282</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0282</id><created>2014-03-02</created><authors><author><keyname>Creado</keyname><forenames>Orhio Mark</forenames></author><author><keyname>Srinivasan</keyname><forenames>Bala</forenames></author><author><keyname>Le</keyname><forenames>Phu Dung</forenames></author><author><keyname>Tan</keyname><forenames>Jefferson</forenames></author></authors><title>An Explicit Trust Model Towards Better System Security</title><categories>cs.CR</categories><comments>13 pages, 7 figures, The Fourth International Conference on Computer
  Science and Information Technology, CCSIT 2014</comments><doi>10.5121/csit.2014.4212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trust is an absolute necessity for digital communications; but is often
viewed as an implicit singular entity. The use of the internet as the primary
vehicle for information exchange has made accountability and verifiability of
system code almost obsolete. This paper proposes a novel approach towards
enforcing system security by requiring the explicit definition of trust for all
operating code. By identifying the various classes and levels of trust required
within a computing system; trust is defined as a combination of individual
characteristics. Trust is then represented as a calculable metric obtained
through the collective enforcement of each of these characteristics to varying
degrees. System Security is achieved by facilitating trust to be a constantly
evolving aspect for each operating code segment capable of getting stronger or
weaker over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0284</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0284</id><created>2014-03-02</created><updated>2014-04-13</updated><authors><author><keyname>Zheng</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Shengjin</forenames></author><author><keyname>Zhou</keyname><forenames>Wengang</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval</title><categories>cs.CV</categories><comments>8 pages, 7 figures, 6 tables, accepted to CVPR 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Bag-of-Words (BoW) representation is well applied to recent
state-of-the-art image retrieval works. Typically, multiple vocabularies are
generated to correct quantization artifacts and improve recall. However, this
routine is corrupted by vocabulary correlation, i.e., overlapping among
different vocabularies. Vocabulary correlation leads to an over-counting of the
indexed features in the overlapped area, or the intersection set, thus
compromising the retrieval accuracy. In order to address the correlation
problem while preserve the benefit of high recall, this paper proposes a Bayes
merging approach to down-weight the indexed features in the intersection set.
Through explicitly modeling the correlation problem in a probabilistic view, a
joint similarity on both image- and feature-level is estimated for the indexed
features in the intersection set.
  We evaluate our method through extensive experiments on three benchmark
datasets. Albeit simple, Bayes merging can be well applied in various merging
tasks, and consistently improves the baselines on multi-vocabulary merging.
Moreover, Bayes merging is efficient in terms of both time and memory cost, and
yields competitive performance compared with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0297</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0297</id><created>2014-03-02</created><authors><author><keyname>Miller</keyname><forenames>Brad</forenames></author><author><keyname>Huang</keyname><forenames>Ling</forenames></author><author><keyname>Joseph</keyname><forenames>A. D.</forenames></author><author><keyname>Tygar</keyname><forenames>J. D.</forenames></author></authors><title>I Know Why You Went to the Clinic: Risks and Realization of HTTPS
  Traffic Analysis</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Revelations of large scale electronic surveillance and data mining by
governments and corporations have fueled increased adoption of HTTPS. We
present a traffic analysis attack against over 6000 webpages spanning the HTTPS
deployments of 10 widely used, industry-leading websites in areas such as
healthcare, finance, legal services and streaming video. Our attack identifies
individual pages in the same website with 89% accuracy, exposing personal
details including medical conditions, financial and legal affairs and sexual
orientation. We examine evaluation methodology and reveal accuracy variations
as large as 18% caused by assumptions affecting caching and cookies. We present
a novel defense reducing attack accuracy to 27% with a 9% traffic increase, and
demonstrate significantly increased effectiveness of prior defenses in our
evaluation context, inclusive of enabled caching, user-specific cookies and
pages within the same website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0298</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0298</id><created>2014-03-02</created><authors><author><keyname>Mestre</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Verschae</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>A 4-approximation for scheduling on a single machine with general cost
  function</title><categories>cs.DS</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single machine scheduling problem that seeks to minimize a
generalized cost function: given a subset of jobs we must order them so as to
minimize $\sum f_j(C_j)$, where $C_j$ is the completion time of job $j$ and
$f_j$ is a job-dependent cost function. This problem has received a
considerably amount of attention lately, partly because it generalizes a large
number of sequencing problems while still allowing constant approximation
guarantees.
  In a recent paper, Cheung and Shmoys provided a primal-dual algorithm for the
problem and claimed that is a 2-approximation. In this paper we show that their
analysis cannot yield an approximation guarantee better than $4$. We then cast
their algorithm as a local ratio algorithm and show that in fact it has an
approximation ratio of $4$. Additionally, we consider a more general problem
where jobs has release dates and can be preempted. For this version we give a
$4\kappa$-approximation algorithm where $\kappa$ is the number of distinct
release dates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0300</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0300</id><created>2014-03-02</created><authors><author><keyname>Kadhim</keyname><forenames>Laith Awda</forenames></author><author><keyname>Mohammed</keyname><forenames>Salih</forenames></author><author><keyname>Saad</keyname><forenames>Osamah</forenames></author></authors><title>A Proposed Improvement Equalizer for Telephone and Mobile Circuit
  Channels</title><categories>cs.IT math.IT</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the transmission of digital data at a relatively high rate over a
particular band limited channel, it is normally necessary to employ an
equalizer at the receiver in order to correct the signal distortion introduced
by the channel .ISI (inter symbol interference) leads to large error
probability if it is not suppressed .The possible solutions for coping with ISI
such as equalization technique. Maximum Likelihood Sequence Estimation (MLSE)
implemented with Viterbi algorithm is the optimal equalizer for this ISI
problem sense it minimizes the sequence of error rate. This estimator involves
a very considerable amount of equipment complexity especially when detecting a
multilevel digital signal having large alphabet, and/or operating under a
channel with long impulse response, this arises a need to develop detection
algorithms with reduced complexity without losing the performance. The aim of
this work is to study the various ways to remove the ISI, concentrating on the
decision-based algorithms (DFE, MLSE, and near MLSE), analyzing the difference
between them from both performance and complexity point of view. An Improved
non linear equalizer with Perturbation algorithm has been suggested which
trying to enhance the performance and reduce the computational complexity by
comparing it with the other existing detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0306</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0306</id><created>2014-03-02</created><authors><author><keyname>Tran</keyname><forenames>Loc V.</forenames></author><author><keyname>Nguyen</keyname><forenames>Vinh Phu</forenames></author><author><keyname>Wahab</keyname><forenames>M. Abdel</forenames></author><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author></authors><title>An extended isogeometric analysis for vibration of cracked FGM plates
  using higher-order shear deformation theory</title><categories>cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel and effective formulation that combines the eXtended IsoGeometric
Approach (XIGA) and Higher-order Shear Deformation Theory (HSDT) is proposed to
study the free vibration of cracked Functionally Graded Material (FGM) plates.
Herein, the general HSDT model with five unknown variables per node is applied
for calculating the stiffness matrix without needing Shear Correction Factor
(SCF). In order to model the discontinuous and singular phenomena in the
cracked plates, IsoGeometric Analysis (IGA) utilizing the Non-Uniform Rational
B-Spline (NURBS) functions is incorporated with enrichment functions through
the partition of unity method. NURBS basis functions with their inherent
arbitrary high order smoothness permit the C1 requirement of the HSDT model.
The material properties of the FGM plates vary continuously through the plate
thickness according to an exponent function. The effects of gradient index,
crack length, crack location, length to thickness on the natural frequencies
and mode shapes of simply supported and clamped FGM plate are studied.
Numerical examples are provided to show excellent performance of the proposed
method compared with other published solutions in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0307</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0307</id><created>2014-03-02</created><authors><author><keyname>Tran</keyname><forenames>Loc V.</forenames></author><author><keyname>Thai</keyname><forenames>Chien H.</forenames></author><author><keyname>Gan</keyname><forenames>Buntara S.</forenames></author><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author></authors><title>Isogeometric finite element analysis of laminated composite plates based
  on a four variable refined plate theory</title><categories>cs.CE math.NA</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.1847</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel and effective formulation based on isogeometric
approach (IGA) and Refined Plate Theory (RPT) is proposed to study the behavior
of laminated composite plates. Using many kinds of higher-order distributed
functions, RPT model naturally satisfies the traction-free boundary conditions
at plate surfaces and describes the non-linear distribution of shear stresses
without requiring shear correction factor (SCF). IGA utilizes the basis
functions, namely B-splines or non-uniform rational B-splines (NURBS), which
achieve easily the smoothness of any arbitrary order. It hence satisfies the C1
requirement of the RPT model. The static, dynamic and buckling analysis of
rectangular plates is investigated for different boundary conditions. Numerical
results show high effectiveness of the present formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0309</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0309</id><created>2014-03-02</created><authors><author><keyname>Shirazi</keyname><forenames>Sareh</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Object Tracking via Non-Euclidean Geometry: A Grassmann Approach</title><categories>cs.CV math.MG stat.ML</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2014</comments><acm-class>I.2.10; I.4.6; I.4.7; I.4.8; I.5.1; I.5.4; G.3</acm-class><doi>10.1109/WACV.2014.6836008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust visual tracking system requires an object appearance model that is
able to handle occlusion, pose, and illumination variations in the video
stream. This can be difficult to accomplish when the model is trained using
only a single image. In this paper, we first propose a tracking approach based
on affine subspaces (constructed from several images) which are able to
accommodate the abovementioned variations. We use affine subspaces not only to
represent the object, but also the candidate areas that the object may occupy.
We furthermore propose a novel approach to measure affine subspace-to-subspace
distance via the use of non-Euclidean geometry of Grassmann manifolds. The
tracking problem is then considered as an inference task in a Markov Chain
Monte Carlo framework via particle filtering. Quantitative evaluation on
challenging video sequences indicates that the proposed approach obtains
considerably better performance than several recent state-of-the-art methods
such as Tracking-Learning-Detection and MILtrack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0315</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0315</id><created>2014-03-03</created><authors><author><keyname>Carvajal</keyname><forenames>Johanna</forenames></author><author><keyname>McCool</keyname><forenames>Chris</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Summarisation of Short-Term and Long-Term Videos using Texture and
  Colour</title><categories>cs.CV stat.AP</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2014</comments><acm-class>I.4.6; I.4.7; I.4.9; I.5.4; I.2.10</acm-class><doi>10.1109/WACV.2014.6836025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to video summarisation that makes use of a
Bag-of-visual-Textures (BoT) approach. Two systems are proposed, one based
solely on the BoT approach and another which exploits both colour information
and BoT features. On 50 short-term videos from the Open Video Project we show
that our BoT and fusion systems both achieve state-of-the-art performance,
obtaining an average F-measure of 0.83 and 0.86 respectively, a relative
improvement of 9% and 13% when compared to the previous state-of-the-art. When
applied to a new underwater surveillance dataset containing 33 long-term
videos, the proposed system reduces the amount of footage by a factor of 27,
with only minor degradation in the information content. This order of magnitude
reduction in video data represents significant savings in terms of time and
potential labour cost when manually reviewing such footage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0316</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0316</id><created>2014-03-03</created><authors><author><keyname>Zhang</keyname><forenames>Kang</forenames></author><author><keyname>Fang</keyname><forenames>Yuqiang</forenames></author><author><keyname>Min</keyname><forenames>Dongbo</forenames></author><author><keyname>Sun</keyname><forenames>Lifeng</forenames></author><author><keyname>Yan</keyname><forenames>Shiqiang Yang. Shuicheng</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Cross-Scale Cost Aggregation for Stereo Matching</title><categories>cs.CV</categories><comments>To Appear in 2013 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR). 2014 (poster, 29.88%)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human beings process stereoscopic correspondence across multiple scales.
However, this bio-inspiration is ignored by state-of-the-art cost aggregation
methods for dense stereo correspondence. In this paper, a generic cross-scale
cost aggregation framework is proposed to allow multi-scale interaction in cost
aggregation. We firstly reformulate cost aggregation from a unified
optimization perspective and show that different cost aggregation methods
essentially differ in the choices of similarity kernels. Then, an inter-scale
regularizer is introduced into optimization and solving this new optimization
problem leads to the proposed framework. Since the regularization term is
independent of the similarity kernel, various cost aggregation methods can be
integrated into the proposed general framework. We show that the cross-scale
framework is important as it effectively and efficiently expands
state-of-the-art cost aggregation methods and leads to significant
improvements, when evaluated on Middlebury, KITTI and New Tsukuba datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0320</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0320</id><created>2014-03-03</created><authors><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Matching Image Sets via Adaptive Multi Convex Hull</title><categories>cs.CV stat.ML</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2014</comments><acm-class>I.5; I.5.1; I.5.4; G.3</acm-class><doi>10.1109/WACV.2014.6835985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional nearest points methods use all the samples in an image set to
construct a single convex or affine hull model for classification. However,
strong artificial features and noisy data may be generated from combinations of
training samples when significant intra-class variations and/or noise occur in
the image set. Existing multi-model approaches extract local models by
clustering each image set individually only once, with fixed clusters used for
matching with various image sets. This may not be optimal for discrimination,
as undesirable environmental conditions (eg. illumination and pose variations)
may result in the two closest clusters representing different characteristics
of an object (eg. frontal face being compared to non-frontal face). To address
the above problem, we propose a novel approach to enhance nearest points based
methods by integrating affine/convex hull classification with an adapted
multi-model approach. We first extract multiple local convex hulls from a query
image set via maximum margin clustering to diminish the artificial variations
and constrain the noise in local convex hulls. We then propose adaptive
reference clustering (ARC) to constrain the clustering of each gallery image
set by forcing the clusters to have resemblance to the clusters in the query
image set. By applying ARC, noisy clusters in the query set can be discarded.
Experiments on Honda, MoBo and ETH-80 datasets show that the proposed method
outperforms single model approaches and other recent techniques, such as Sparse
Approximated Nearest Points, Mutual Subspace Method and Manifold Discriminant
Analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0334</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0334</id><created>2014-03-03</created><authors><author><keyname>Bhoi</keyname><forenames>Sourav Kumar</forenames></author><author><keyname>Panda</keyname><forenames>Sanjaya Kumar</forenames></author><author><keyname>Faruk</keyname><forenames>Imran Hossain</forenames></author></authors><title>Design and Performance Evaluation of an Optimized Disk Scheduling
  Algorithm (ODSA)</title><categories>cs.OS</categories><comments>8 pages, 26 figures</comments><report-no>pxc3877329</report-no><journal-ref>International Journal of Computer Applications 2012</journal-ref><doi>10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329 10.5120/5010-7329
  10.5120/5010-7329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Management of disk scheduling is a very important aspect of operating system.
Performance of the disk scheduling completely depends on how efficient is the
scheduling algorithm to allocate services to the request in a better manner.
Many algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.) are developed in the
recent years in order to optimize the system disk I/O performance. By reducing
the average seek time and transfer time, we can improve the performance of disk
I/O operation. In our proposed algorithm, Optimize Disk Scheduling Algorithm
(ODSA) is taking less average seek time and transfer time as compare to other
disk scheduling algorithms (FIFO, SSTF, SCAN, C-SCAN, LOOK, etc.), which
enhances the efficiency of the disk performance in a better manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0335</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0335</id><created>2014-03-03</created><authors><author><keyname>Panda</keyname><forenames>Sanjaya Kumar</forenames></author><author><keyname>Dash</keyname><forenames>Debasis</forenames></author><author><keyname>Rout</keyname><forenames>Jitendra Kumar</forenames></author></authors><title>A Group based Time Quantum Round Robin Algorithm using Min-Max Spread
  Measure</title><categories>cs.OS</categories><comments>7 pages, 16 figures</comments><report-no>pxc3885445</report-no><journal-ref>International Journal of Computer Applications 2013</journal-ref><doi>10.5120/10667-5445</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Round Robin (RR) Scheduling is the basis of time sharing environment. It is
the combination of First Come First Served (FCFS) scheduling algorithm and
preemption among processes. It is basically used in a time sharing operating
system. It switches from one process to another process in a time interval. The
time interval or Time Quantum (TQ) is fixed for all available processes. So,
the larger process suffers from Context Switches (CS). To increase efficiency,
we have to select different TQ for processes. The main objective of RR is to
reduce the CS, maximize the utilization of CPU and minimize the turn around and
the waiting time. In this paper, we have considered different TQ for a group of
processes. It reduces CS as well as enhancing the performance of RR algorithm.
TQ can be calculated using min-max dispersion measure. Our experimental
analysis shows that Group Based Time Quantum (GBTQ) RR algorithm performs
better than existing RR algorithm with respect to Average Turn Around Time
(ATAT), Average Waiting Time (AWT) and CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0338</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0338</id><created>2014-03-03</created><authors><author><keyname>Rout</keyname><forenames>Jitendra Kumar</forenames></author><author><keyname>Bhoi</keyname><forenames>Sourav Kumar</forenames></author><author><keyname>Panda</keyname><forenames>Sanjaya Kumar</forenames></author></authors><title>SFTP : A Secure and Fault-Tolerant Paradigm against Blackhole Attack in
  MANET</title><categories>cs.NI</categories><comments>6 pages, 9 figures</comments><report-no>pxc3885343</report-no><journal-ref>International Journal of Computer Applications 2013</journal-ref><doi>10.5120/10623-5343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security issues in MANET are a challenging task nowadays. MANETs are
vulnerable to passive attacks and active attacks because of a limited number of
resources and lack of centralized authority. Blackhole attack is an attack in
network layer which degrade the network performance by dropping the packets. In
this paper, we have proposed a Secure Fault-Tolerant Paradigm (SFTP) which
checks the Blackhole attack in the network. The three phases used in SFTP
algorithm are designing of coverage area to find the area of coverage, Network
Connection algorithm to design a fault-tolerant model and Route Discovery
algorithm to discover the route and data delivery from source to destination.
SFTP gives better network performance by making the network fault free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0339</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0339</id><created>2014-03-03</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>On the Behavioral Interpretation of System-Environment Fit and
  Auto-Resilience</title><categories>cs.OH</categories><comments>Draft submitted for publication in the Proceedings of the IEEE 2014
  Conference on Norbert Wiener in the 21st Century</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Already 71 years ago Rosenblueth, Wiener, and Bigelow introduced the concept
of the &quot;behavioristic study of natural events&quot; and proposed a classification of
systems according to the quality of the behaviors they are able to exercise. In
this paper we consider the problem of the resilience of a system when deployed
in a changing environment, which we tackle by considering the behaviors both
the system organs and the environment mutually exercise. We then introduce a
partial order and a metric space for those behaviors, and we use them to define
a behavioral interpretation of the concept of system-environment fit. Moreover
we suggest that behaviors based on the extrapolation of future environmental
requirements would allow systems to proactively improve their own
system-environment fit and optimally evolve their resilience. Finally we
describe how we plan to express a complex optimization strategy in terms of the
concepts introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0353</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0353</id><created>2014-03-03</created><updated>2014-05-13</updated><authors><author><keyname>Zhu</keyname><forenames>Xuzhen</forenames></author><author><keyname>Tian</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Haifeng</forenames></author><author><keyname>Cai</keyname><forenames>Shimin</forenames></author></authors><title>Personalized recommendation against crowd's popular selection</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>This paper has been withdrawn by the author due to a crucial idea
  repeatation with &quot;Information filtering via preferential diffusion&quot; published
  in Physical Review E</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of personalized recommendation in an ocean of data attracts more
and more attention recently. Most traditional researches ignore the popularity
of the recommended object, which resulting in low personality and accuracy. In
this Letter, we proposed a personalized recommendation method based on weighted
object network, punishing the recommended object that is the crowd's popular
selection, namely, Anti-popularity index(AP), which can give enhanced
personality, accuracy and diversity in contrast to mainstream baselines with a
low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0354</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0354</id><created>2014-03-03</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy Harvesting Cooperative Networks: Is the Max-Min Criterion Still
  Diversity-Optimal?</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a general energy harvesting cooperative network with M
source-destination (SD) pairs and one relay, where the relay schedules only m
user pairs for transmissions. For the special case of m = 1, the addressed
scheduling problem is equivalent to relay selection for the scenario with one
SD pair and M relays. In conventional cooperative networks, the max-min
selection criterion has been recognized as a diversity-optimal strategy for
relay selection and user scheduling. The main contribution of this paper is to
show that the use of the max-min criterion will result in loss of diversity
gains in energy harvesting cooperative networks. Particularly when only a
single user is scheduled, analytical results are developed to demonstrate that
the diversity gain achieved by the max-min criterion is only (M+1)/2, much less
than the maximal diversity gain M. The max-min criterion suffers this diversity
loss because it does not reflect the fact that the source-relay channels are
more important than the relay-destination channels in energy harvesting
networks. Motivated by this fact, a few user scheduling approaches tailored to
energy harvesting networks are developed and their performance is analyzed.
Simulation results are provided to demonstrate the accuracy of the developed
analytical results and facilitate the performance comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0355</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0355</id><created>2014-03-03</created><authors><author><keyname>Kang</keyname><forenames>Xin</forenames></author><author><keyname>Chong</keyname><forenames>Hon Fah</forenames></author><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Ergodic Sum-Rate Maximization for Fading Cognitive Multiple Access
  Channels without Successive Interference Cancellation</title><categories>cs.IT math.IT</categories><comments>Under Review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the ergodic sum-rate of a fading cognitive multiple access
channel (C-MAC) is studied, where a secondary network (SN) with multiple
secondary users (SUs) transmitting to a secondary base station (SBS) shares the
spectrum band with a primary user (PU). An interference power constraint (IPC)
is imposed on the SN to protect the PU. Under such a constraint and the
individual transmit power constraint (TPC) imposed on each SU, we investigate
the power allocation strategies to maximize the ergodic sum-rate of a fading
C-MAC without successive interference cancellation (SIC). In particular, this
paper considers two types of constraints: (1) average TPC and average IPC, (2)
peak TPC and peak IPC. For the first case, it is proved that the optimal power
allocation is dynamic time-division multiple-access (D-TDMA), which is exactly
the same as the optimal power allocation to maximize the ergodic sum-rate of
the fading C-MAC with SIC under the same constraints. For the second case, it
is proved that the optimal solution must be at the extreme points of the
feasible region. It is shown that D-TDMA is optimal with high probability when
the number of SUs is large. Besides, we show that, when the SUs can be sorted
in a certain order, an algorithm with linear complexity can be used to find the
optimal power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0359</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0359</id><created>2014-03-03</created><authors><author><keyname>Bonsma</keyname><forenames>Paul</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Reconfiguring Independent Sets in Claw-Free Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial-time algorithm that, given two independent sets in a
claw-free graph $G$, decides whether one can be transformed into the other by a
sequence of elementary steps. Each elementary step is to remove a vertex $v$
from the current independent set $S$ and to add a new vertex $w$ (not in $S$)
such that the result is again an independent set. We also consider the more
restricted model where $v$ and $w$ have to be adjacent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0388</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0388</id><created>2014-03-03</created><updated>2015-02-02</updated><authors><author><keyname>Zamani</keyname><forenames>Mohammadzaman</forenames></author><author><keyname>Beigy</keyname><forenames>Hamid</forenames></author><author><keyname>Shaban</keyname><forenames>Amirreza</forenames></author></authors><title>Cascading Randomized Weighted Majority: A New Online Ensemble Learning
  Algorithm</title><categories>stat.ML cs.LG</categories><comments>15 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing volume of data in the world, the best approach for
learning from this data is to exploit an online learning algorithm. Online
ensemble methods are online algorithms which take advantage of an ensemble of
classifiers to predict labels of data. Prediction with expert advice is a
well-studied problem in the online ensemble learning literature. The Weighted
Majority algorithm and the randomized weighted majority (RWM) are the most
well-known solutions to this problem, aiming to converge to the best expert.
Since among some expert, the best one does not necessarily have the minimum
error in all regions of data space, defining specific regions and converging to
the best expert in each of these regions will lead to a better result. In this
paper, we aim to resolve this defect of RWM algorithms by proposing a novel
online ensemble algorithm to the problem of prediction with expert advice. We
propose a cascading version of RWM to achieve not only better experimental
results but also a better error bound for sufficiently large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0406</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0406</id><created>2014-03-03</created><updated>2015-03-21</updated><authors><author><keyname>Yamada</keyname><forenames>Akihisa</forenames></author><author><keyname>Winkler</keyname><forenames>Sarah</forenames></author><author><keyname>Hirokawa</keyname><forenames>Nao</forenames></author><author><keyname>Middeldorp</keyname><forenames>Aart</forenames></author></authors><title>AC-KBO Revisited</title><categories>cs.LO</categories><comments>31 pages, To appear in Theory and Practice of Logic Programming
  (TPLP) special issue for the 12th International Symposium on Functional and
  Logic Programming (FLOPS 2014)</comments><doi>10.1017/S1471068415000083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Equational theories that contain axioms expressing associativity and
commutativity (AC) of certain operators are ubiquitous. Theorem proving methods
in such theories rely on well-founded orders that are compatible with the AC
axioms. In this paper we consider various definitions of AC-compatible
Knuth-Bendix orders. The orders of Steinbach and of Korovin and Voronkov are
revisited. The former is enhanced to a more powerful version, and we modify the
latter to amend its lack of monotonicity on non-ground terms. We further
present new complexity results. An extension reflecting the recent proposal of
subterm coefficients in standard Knuth-Bendix orders is also given. The various
orders are compared on problems in termination and completion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0417</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0417</id><created>2014-03-03</created><authors><author><keyname>Find</keyname><forenames>Magnus Gausdal</forenames></author></authors><title>On the Complexity of Computing Two Nonlinearity Measures</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of two Boolean nonlinearity measures:
the nonlinearity and the multiplicative complexity. We show that if one-way
functions exist, no algorithm can compute the multiplicative complexity in time
$2^{O(n)}$ given the truth table of length $2^n$, in fact under the same
assumption it is impossible to approximate the multiplicative complexity within
a factor of $(2-\epsilon)^{n/2}$. When given a circuit, the problem of
determining the multiplicative complexity is in the second level of the
polynomial hierarchy. For nonlinearity, we show that it is #P hard to compute
given a function represented by a circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0429</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0429</id><created>2014-03-03</created><authors><author><keyname>de Silva</keyname><forenames>Lavindra</forenames></author><author><keyname>Winikoff</keyname><forenames>Michael</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author></authors><title>Extending Agents by Transmitting Protocols in Open Systems</title><categories>cs.MA</categories><journal-ref>In Proceedings of the 2003 AAMAS-Workshop on Challenges in Open
  Agent Systems, Melbourne, Australia, 2003</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agents in an open system communicate using interaction protocols. Suppose
that we have a system of agents and that we want to add a new protocol that all
(or some) agents should be able to understand. Clearly, modifying the source
code for each agent implementation is not practical. A solution to this problem
of upgrading an open system is to have a mechanism that allows agents to
receive a description of an interaction protocol and use it. In this paper we
propose a representation for protocols based on extending Petri nets. However,
this is not enough: in an open system the source of a protocol may not be
trusted and a protocol that is received may contain steps that are erroneous or
that make confidential information public. We therefore also describe an
analysis method that infers whether a protocol is safe. Finally, we give an
execution model for extended Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0439</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0439</id><created>2014-03-03</created><authors><author><keyname>Hanspach</keyname><forenames>Michael</forenames></author></authors><title>Verbesserung von OS- und Service-Fingerprinting mittels Fuzzing</title><categories>cs.CR cs.NI</categories><comments>75 pages, Diplom thesis / Diplomarbeit. September 2008, German
  language content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprinting of services and operating systems is an essential part of
penetration tests. In order to successfully penetrate the computing system's
security measurements, preexisting fingerprinting methods are described and the
paradigm of fingerprinting with mutation-based fuzzing is established. A case
study about operating system and FTP server fingerprinting is presented whereby
the feasibility of the approach is demonstrated. The research results show that
the developed tools can be used for even more precise fingerprinting than the
preexisting tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0445</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0445</id><created>2014-03-03</created><updated>2015-03-30</updated><authors><author><keyname>Boutier</keyname><forenames>Matthieu</forenames><affiliation>PPS</affiliation></author><author><keyname>Chroboczek</keyname><forenames>Juliusz</forenames><affiliation>PPS</affiliation></author></authors><title>Source-specific routing</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Source-specific routing (not to be confused with source routing) is a routing
technique where routing decisions depend on both the source and the destination
address of a packet. Source-specific routing solves some difficult problems
related to multihoming, notably in edge networks, and is therefore a useful
addition to the multihoming toolbox. In this paper, we describe the semantics
of source-specific packet forwarding, and describe the design and
implementation of a source-specific extension to the Babel routing protocol as
well as its implementation - to our knowledge, the first complete
implementation of a source-specific dynamic routing protocol, including a
disambiguation algorithm that makes our implementation work over widely
available networking APIs. We further discuss interoperability between ordinary
next-hop and source-specific dynamic routing protocols. Our implementation has
seen a moderate amount of deployment, notably as a testbed for the IETF Homenet
working group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0448</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0448</id><created>2014-03-03</created><authors><author><keyname>Ding</keyname><forenames>Yimin</forenames></author><author><keyname>Zhou</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Xiaosong</forenames></author></authors><title>Hybrid evolving clique-networks and their communicability</title><categories>cs.SI physics.soc-ph</categories><doi>10.1016/j.physa.2014.03.089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming to understand real-world hierarchical networks whose degree
distributions are neither power law nor exponential, we construct a hybrid
clique network that includes both homogeneous and inhomogeneous parts, and
introduce an inhomogeneity parameter to tune the ratio between the homogeneous
part and the inhomogeneous one. We perform Monte-Carlo simulations to study
various properties of such a network, including the degree distribution, the
average shortest-path-length, the clustering coefficient, the clustering
spectrum, and the communicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0457</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0457</id><created>2014-03-03</created><updated>2014-07-14</updated><authors><author><keyname>Larsson</keyname><forenames>N. Jesper</forenames></author><author><keyname>Fuglsang</keyname><forenames>Kasper</forenames></author><author><keyname>Karlsson</keyname><forenames>Kenneth</forenames></author></authors><title>Efficient Representation for Online Suffix Tree Construction</title><categories>cs.DS</categories><comments>Presented at SEA 2014, Copenhagen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suffix tree construction algorithms based on suffix links are popular because
they are simple to implement, can operate online in linear time, and because
the suffix links are often convenient for pattern matching. We present an
approach using edge-oriented suffix links, which reduces the number of branch
lookup operations (known to be a bottleneck in construction time) with some
additional techniques to reduce construction cost. We discuss various effects
of our approach and compare it to previous techniques. An experimental
evaluation shows that we are able to reduce construction time to around half
that of the original algorithm, and about two thirds that of previously known
branch-reduced construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0461</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0461</id><created>2014-02-24</created><updated>2014-04-22</updated><authors><author><keyname>Bistarelli</keyname><forenames>Stefano</forenames></author><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Meo</keyname><forenames>Maria Chiara</forenames></author><author><keyname>Santini</keyname><forenames>Francesco</forenames></author></authors><title>Timed Soft Concurrent Constraint Programs: An Interleaved and a Parallel
  Approach</title><categories>cs.PL cs.AI</categories><journal-ref>Theory and Practice of Logic Programming 15 (2014) 743-782</journal-ref><doi>10.1017/S1471068414000106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a timed and soft extension of Concurrent Constraint Programming.
The time extension is based on the hypothesis of bounded asynchrony: the
computation takes a bounded period of time and is measured by a discrete global
clock. Action prefixing is then considered as the syntactic marker which
distinguishes a time instant from the next one. Supported by soft constraints
instead of crisp ones, tell and ask agents are now equipped with a preference
(or consistency) threshold which is used to determine their success or
suspension. In the paper we provide a language to describe the agents behavior,
together with its operational and denotational semantics, for which we also
prove the compositionality and correctness properties. After presenting a
semantics using maximal parallelism of actions, we also describe a version for
their interleaving on a single processor (with maximal parallelism for time
elapsing). Coordinating agents that need to take decisions both on preference
values and time events may benefit from this language. To appear in Theory and
Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0466</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0466</id><created>2014-02-28</created><updated>2015-04-17</updated><authors><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Xiao-long</forenames></author><author><keyname>Xiang</keyname><forenames>Xin</forenames></author><author><keyname>Tang</keyname><forenames>Bu-zhou</forenames></author><author><keyname>Chen</keyname><forenames>Qing-cai</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Bu</keyname><forenames>Jun-zhao</forenames></author></authors><title>Automatic exploration of structural regularities in networks</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages, 3 figures</comments><acm-class>I.5.3; H.2.8; G.3</acm-class><journal-ref>J. Stat. Mech. (2015) P10004</journal-ref><doi>10.1088/1742-5468/2015/10/P10004</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Complex networks provide a powerful mathematical representation of complex
systems in nature and society. To understand complex networks, it is crucial to
explore their internal structures, also called structural regularities. The
task of network structure exploration is to determine how many groups in a
complex network and how to group the nodes of the network. Most existing
structure exploration methods need to specify either a group number or a
certain type of structure when they are applied to a network. In the real
world, however, not only the group number but also the certain type of
structure that a network has are usually unknown in advance. To automatically
explore structural regularities in complex networks, without any prior
knowledge about the group number or the certain type of structure, we extend a
probabilistic mixture model that can handle networks with any type of structure
but needs to specify a group number using Bayesian nonparametric theory and
propose a novel Bayesian nonparametric model, called the Bayesian nonparametric
mixture (BNPM) model. Experiments conducted on a large number of networks with
different structures show that the BNPM model is able to automatically explore
structural regularities in networks with a stable and state-of-the-art
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0468</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0468</id><created>2014-02-28</created><authors><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author><author><keyname>Kozlov</keyname><forenames>Oleg</forenames></author></authors><title>Identification of Structural Model for Chaotic Systems</title><categories>math.DS cs.CE cs.SY nlin.CD</categories><journal-ref>Journal of Modern Physics, 2013, 4, 1381-1392</journal-ref><doi>10.4236/jmp.2013.410166</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This article is talking about the study constructive method of structural
identification systems with chaotic dynamics. It is shown that the
reconstructed attractors are a source of information not only about the
dynamics but also on the basis of the attractors which can be identified and
the mere sight of models. It is known that the knowledge of the symmetry group
allows you to specify the form of a minimal system. Forming a group
transformation can be found in the recon-structed attractor. The affine system
as the basic model is selected. Type of a nonlinear system is the subject of
calcula-tions. A theoretical analysis is performed and proof of the possibility
of constructing models in the central invariant manifold reduced. This
developed algorithm for determining the observed symmetry in the attractor. The
results of identification used in real systems are an application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0476</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0476</id><created>2014-03-03</created><updated>2015-04-01</updated><authors><author><keyname>Kozik</keyname><forenames>Marcin</forenames></author><author><keyname>Ochremiak</keyname><forenames>Joanna</forenames></author></authors><title>Algebraic Properties of Valued Constraint Satisfaction Problem</title><categories>cs.CC</categories><comments>arXiv admin note: text overlap with arXiv:1207.6692 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an algebraic framework for optimization problems
expressible as Valued Constraint Satisfaction Problems. Our results generalize
the algebraic framework for the decision version (CSPs) provided by Bulatov et
al. [SICOMP 2005]. We introduce the notions of weighted algebras and varieties
and use the Galois connection due to Cohen et al. [SICOMP 2013] to link VCSP
languages to weighted algebras. We show that the difficulty of VCSP depends
only on the weighted variety generated by the associated weighted algebra.
Paralleling the results for CSPs we exhibit a reduction to cores and rigid
cores which allows us to focus on idempotent weighted varieties. Further, we
propose an analogue of the Algebraic CSP Dichotomy Conjecture; prove the
hardness direction and verify that it agrees with known results for VCSPs on
two-element sets [Cohen et al. 2006], finite-valued VCSPs [Thapper and Zivny
2013] and conservative VCSPs [Kolmogorov and Zivny 2013].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0481</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0481</id><created>2014-03-03</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author></authors><title>Support Vector Machine Model for Currency Crisis Discrimination</title><categories>cs.LG stat.ML</categories><comments>Book Chapter Selected Works in Infrastructural Finance, Rudra P.
  Pradhan, Indian Institute of Technology Kharagpur, Editor, Macmillan
  Publishers, India, pp 249 - 256, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machine (SVM) is powerful classification technique based on
the idea of structural risk minimization. Use of kernel function enables curse
of dimensionality to be addressed. However, proper kernel function for certain
problem is dependent on specific dataset and as such there is no good method on
choice of kernel function. In this paper, SVM is used to build empirical models
of currency crisis in Argentina. An estimation technique is developed by
training model on real life data set which provides reasonably accurate model
outputs and helps policy makers to identify situations in which currency crisis
may happen. The third and fourth order polynomial kernel is generally best
choice to achieve high generalization of classifier performance. SVM has high
level of maturity with algorithms that are simple, easy to implement, tolerates
curse of dimensionality and good empirical performance. The satisfactory
results show that currency crisis situation is properly emulated using only
small fraction of database and could be used as an evaluation tool as well as
an early warning system. To the best of knowledge this is the first work on SVM
approach for currency crisis evaluation of Argentina.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0485</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0485</id><created>2014-03-03</created><authors><author><keyname>Parmar</keyname><forenames>Divyarajsinh N.</forenames></author><author><keyname>Mehta</keyname><forenames>Brijesh B.</forenames></author></authors><title>Face Recognition Methods &amp; Applications</title><categories>cs.CV</categories><comments>3 pages, 1 figure</comments><journal-ref>International Journal of Computer Technology &amp; Applications, Vol 4
  (1), pp. 84-86, Jan-Feb 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0486</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0486</id><created>2014-03-03</created><updated>2014-03-04</updated><authors><author><keyname>Devanur</keyname><forenames>Nikhil</forenames></author><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author><author><keyname>Yaroslavtsev</keyname><forenames>Grigory</forenames></author></authors><title>Online Algorithms for Machine Minimization</title><categories>cs.DM cs.DC cs.DS</categories><comments>After the first version of the manuscript was posted online, it was
  brought to our attention that Theorem 1.1 also follows from the results of
  Bansal, Kimbrel and Pruhs, &quot;Speed Scaling to Minimize Energy and Temperature&quot;
  (JACM'07), who consider energy-minimizing scheduling problems. This result
  follows from Lemma 4.7 and Lemma 4.8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the online version of the machine minimization
problem (introduced by Chuzhoy et al., FOCS 2004), where the goal is to
schedule a set of jobs with release times, deadlines, and processing lengths on
a minimum number of identical machines. Since the online problem has strong
lower bounds if all the job parameters are arbitrary, we focus on jobs with
uniform length. Our main result is a complete resolution of the deterministic
complexity of this problem by showing that a competitive ratio of $e$ is
achievable and optimal, thereby improving upon existing lower and upper bounds
of 2.09 and 5.2 respectively. We also give a constant-competitive online
algorithm for the case of uniform deadlines (but arbitrary job lengths); to the
best of our knowledge, no such algorithm was known previously. Finally, we
consider the complimentary problem of throughput maximization where the goal is
to maximize the sum of weights of scheduled jobs on a fixed set of identical
machines (introduced by Bar-Noy et al. STOC 1999). We give a randomized online
algorithm for this problem with a competitive ratio of e/e-1; previous results
achieved this bound only for the case of a single machine or in the limit of an
infinite number of machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0493</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0493</id><created>2014-03-03</created><authors><author><keyname>Carli</keyname><forenames>Thomas</forenames></author><author><keyname>Henriot</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Cohen</keyname><forenames>Johanne</forenames></author><author><keyname>Tomasik</keyname><forenames>Joanna</forenames></author></authors><title>A packing problem approach to energy-aware load distribution in Clouds</title><categories>cs.DS</categories><comments>SUPELEC Technical Report, CEI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cloud Computing paradigm consists in providing customers with virtual
services of the quality which meets customers' requirements. A cloud service
operator is interested in using his infrastructure in the most efficient way
while serving customers. The efficiency of infrastructure exploitation may be
expressed, amongst others, by the electrical energy consumption of computing
centers.
  We propose to model the energy consumption of private Clouds, which provides
virtual computation services, by a variant of the Bin Packing problem. This
novel generalization is obtained by introducing such constraints as: variable
bin size, cost of packing and the possibility of splitting items.
  We analyze the packing problem generalization from a theoretical point of
view. We advance on-line and off-line approximation algorithms to solve our
problem to balance the load either on-the-fly or on the planning stage. In
addition to the computation of the approximation factors of these two
algorithms, we evaluate experimentally their performance.
  The quality of the results is encouraging. This conclusion makes a packing
approach a serious candidate to model energy-aware load balancing in Cloud
Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0500</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0500</id><created>2014-03-03</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>McKee</keyname><forenames>Gerard</forenames></author><author><keyname>Alexandrov</keyname><forenames>Vassil</forenames></author></authors><title>Automating Fault Tolerance in High-Performance Computational Biological
  Jobs Using Multi-Agent Approaches</title><categories>cs.DC cs.CE cs.MA</categories><comments>Computers in Biology and Medicine</comments><doi>10.1016/j.compbiomed.2014.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Large-scale biological jobs on high-performance computing systems
require manual intervention if one or more computing cores on which they
execute fail. This places not only a cost on the maintenance of the job, but
also a cost on the time taken for reinstating the job and the risk of losing
data and execution accomplished by the job before it failed. Approaches which
can proactively detect computing core failures and take action to relocate the
computing core's job onto reliable cores can make a significant step towards
automating fault tolerance.
  Method: This paper describes an experimental investigation into the use of
multi-agent approaches for fault tolerance. Two approaches are studied, the
first at the job level and the second at the core level. The approaches are
investigated for single core failure scenarios that can occur in the execution
of parallel reduction algorithms on computer clusters. A third approach is
proposed that incorporates multi-agent technology both at the job and core
level. Experiments are pursued in the context of genome searching, a popular
computational biology application.
  Result: The key conclusion is that the approaches proposed are feasible for
automating fault tolerance in high-performance computing systems with minimal
human intervention. In a typical experiment in which the fault tolerance is
studied, centralised and decentralised checkpointing approaches on an average
add 90% to the actual time for executing the job. On the other hand, in the
same experiment the multi-agent approaches add only 10% to the overall
execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0503</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0503</id><created>2014-03-03</created><authors><author><keyname>Yousefi</keyname><forenames>Siamak</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author></authors><title>Distributed Cooperative Localization in Wireless Sensor Networks without
  NLOS Identification</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted in WPNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a 2-stage robust distributed algorithm is proposed for
cooperative sensor network localization using time of arrival (TOA) data
without identification of non-line of sight (NLOS) links. In the first stage,
to overcome the effect of outliers, a convex relaxation of the Huber loss
function is applied so that by using iterative optimization techniques, good
estimates of the true sensor locations can be obtained. In the second stage,
the original (non-relaxed) Huber cost function is further optimized to obtain
refined location estimates based on those obtained in the first stage. In both
stages, a simple gradient descent technique is used to carry out the
optimization. Through simulations and real data analysis, it is shown that the
proposed convex relaxation generally achieves a lower root mean squared error
(RMSE) compared to other convex relaxation techniques in the literature. Also
by doing the second stage, the position estimates are improved and we can
achieve an RMSE close to that of the other distributed algorithms which know
\textit{a priori} which links are in NLOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0504</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0504</id><created>2014-03-03</created><updated>2014-07-10</updated><authors><author><keyname>Paige</keyname><forenames>Brooks</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>A Compilation Target for Probabilistic Programming Languages</title><categories>cs.AI cs.PL stat.ML</categories><comments>In Proceedings of the 31st International Conference on Machine
  Learning (ICML), 2014</comments><journal-ref>JMLR W&amp;CP 32 (1) : 1935-1943, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forward inference techniques such as sequential Monte Carlo and particle
Markov chain Monte Carlo for probabilistic programming can be implemented in
any programming language by creative use of standardized operating system
functionality including processes, forking, mutexes, and shared memory.
Exploiting this we have defined, developed, and tested a probabilistic
programming language intermediate representation language we call probabilistic
C, which itself can be compiled to machine code by standard compilers and
linked to operating system libraries yielding an efficient, scalable, portable
probabilistic programming compilation target. This opens up a new hardware and
systems research path for optimizing probabilistic programming systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0509</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0509</id><created>2014-03-03</created><authors><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Unary Pushdown Automata and Straight-Line Programs</title><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider decision problems for deterministic pushdown automata over a
unary alphabet (udpda, for short). Udpda are a simple computation model that
accept exactly the unary regular languages, but can be exponentially more
succinct than finite-state automata. We complete the complexity landscape for
udpda by showing that emptiness (and thus universality) is P-hard, equivalence
and compressed membership problems are P-complete, and inclusion is
coNP-complete. Our upper bounds are based on a translation theorem between
udpda and straight-line programs over the binary alphabet (SLPs). We show that
the characteristic sequence of any udpda can be represented as a pair of
SLPs---one for the prefix, one for the lasso---that have size linear in the
size of the udpda and can be computed in polynomial time. Hence, decision
problems on udpda are reduced to decision problems on SLPs. Conversely, any SLP
can be converted in logarithmic space into a udpda, and this forms the basis
for our lower bound proofs. We show coNP-hardness of the ordered matching
problem for SLPs, from which we derive coNP-hardness for inclusion. In
addition, we complete the complexity landscape for unary nondeterministic
pushdown automata by showing that the universality problem is $\Pi_2 \mathrm
P$-hard, using a new class of integer expressions. Our techniques have
applications beyond udpda. We show that our results imply $\Pi_2 \mathrm
P$-completeness for a natural fragment of Presburger arithmetic and coNP lower
bounds for compressed matching problems with one-character wildcards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0515</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0515</id><created>2014-03-03</created><authors><author><keyname>Jiao</keyname><forenames>Yuling</forenames></author><author><keyname>Jin</keyname><forenames>Bangti</forenames></author><author><keyname>Lu</keyname><forenames>Xiliang</forenames></author></authors><title>A Primal Dual Active Set with Continuation Algorithm for the
  \ell^0-Regularized Optimization Problem</title><categories>math.OC cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a primal dual active set with continuation algorithm for solving
the \ell^0-regularized least-squares problem that frequently arises in
compressed sensing. The algorithm couples the the primal dual active set method
with a continuation strategy on the regularization parameter. At each inner
iteration, it first identifies the active set from both primal and dual
variables, and then updates the primal variable by solving a (typically small)
least-squares problem defined on the active set, from which the dual variable
can be updated explicitly. Under certain conditions on the sensing matrix,
i.e., mutual incoherence property or restricted isometry property, and the
noise level, the finite step global convergence of the algorithm is
established. Extensive numerical examples are presented to illustrate the
efficiency and accuracy of the algorithm and the convergence analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0522</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0522</id><created>2014-03-03</created><authors><author><keyname>Azar</keyname><forenames>Ahmad Taher</forenames></author><author><keyname>Hassanien</keyname><forenames>Aboul Ella</forenames></author></authors><title>Expert System Based On Neural-Fuzzy Rules for Thyroid Diseases Diagnosis</title><categories>cs.AI</categories><comments>Conference Paper</comments><doi>10.1007/978-3-642-35521-9_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The thyroid, an endocrine gland that secretes hormones in the blood,
circulates its products to all tissues of the body, where they control vital
functions in every cell. Normal levels of thyroid hormone help the brain,
heart, intestines, muscles and reproductive system function normally. Thyroid
hormones control the metabolism of the body. Abnormalities of thyroid function
are usually related to production of too little thyroid hormone
(hypothyroidism) or production of too much thyroid hormone (hyperthyroidism).
Therefore, the correct diagnosis of these diseases is very important topic. In
this study, Linguistic Hedges Neural-Fuzzy Classifier with Selected Features
(LHNFCSF) is presented for diagnosis of thyroid diseases. The performance
evaluation of this system is estimated by using classification accuracy and
k-fold cross-validation. The results indicated that the classification accuracy
without feature selection was 98.6047% and 97.6744% during training and testing
phases, respectively with RMSE of 0.02335. After applying feature selection
algorithm, LHNFCSF achieved 100% for all cluster sizes during training phase.
However, in the testing phase LHNFCSF achieved 88.3721% using one cluster for
each class, 90.6977% using two clusters, 91.8605% using three clusters and
97.6744% using four clusters for each class and 12 fuzzy rules. The obtained
classification accuracy was very promising with regard to the other
classification applications in literature for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0529</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0529</id><created>2014-03-03</created><updated>2014-06-28</updated><authors><author><keyname>Diaby</keyname><forenames>Moustapha</forenames></author><author><keyname>Karwan</keyname><forenames>M. H.</forenames></author></authors><title>Limits to the scope of applicability of extended formulations for LP
  models of combinatorial optimization problems: A summary</title><categories>cs.CC cs.DM math.CO math.OC</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1309.1823; This version (.v2) is simply a more formalized presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that new definitions of the notion of &quot;projection&quot; on which some of
the recent &quot;extended formulations&quot; works (such as Kaibel (2011); Fiorini et al.
(2011; 2012); Kaibel and Walter (2013); Kaibel and Weltge (2013) for example)
have been based can cause those works to over-reach in their conclusions in
relating polytopes to one another when the sets of the descriptive variables
for those polytopes are disjoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0530</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0530</id><created>2014-03-03</created><authors><author><keyname>Elyengui</keyname><forenames>Saida</forenames></author><author><keyname>Bouhouchi</keyname><forenames>Riadh</forenames></author><author><keyname>Ezzedine</keyname><forenames>Tahar</forenames></author></authors><title>The Enhancement of Communication Technologies and Networks for Smart
  Grid Applications</title><categories>cs.NI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current electrical grid is perhaps the greatest engineering achievement
of the 20th century. However, it is increasingly outdated and overburdened,
leading to costly blackouts and burnouts. For this and various other
reasons,transformation efforts are underway to make the current electrical grid
smarter. A reliable, universal and secure communication infrastructure is
mandatory for the implementation and deployment of the future smart grid. A
special interest is given to the design of efficient and robust network
architecture capable of managing operation and control of the next generation
power grid. For this purpose new wired and wireless technologies are emerging
in addition to the formerly applied to help upgrade the current power grid. In
this paper we will give an overview of smart grid reference model, and a
comprehensive survey of the available networks for the smart grid and a
critical review of the progress of wired and wireless communication
technologies for smart grid communication infrastructure. And we propose end to
end communication architecture for Home Area Networks (HANs), Neighborhood Area
Networks (NANs) and Wide Area Networks (WANs) for smart grid applications. We
believe that this work will provide appreciated insights for the novices who
would like to follow related research in the SG domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0531</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0531</id><created>2014-03-03</created><authors><author><keyname>Zayner</keyname><forenames>Josiah P.</forenames></author></authors><title>We Tweet Like We Talk and Other Interesting Observations: An Analysis of
  English Communication Modalities</title><categories>cs.CL</categories><comments>9 pages, 8 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modalities of communication for human beings are gradually increasing in
number with the advent of new forms of technology. Many human beings can
readily transition between these different forms of communication with little
or no effort, which brings about the question: How similar are these different
communication modalities? To understand technology$\text{'}$s influence on
English communication, four different corpora were analyzed and compared:
Writing from Books using the 1-grams database from the Google Books project,
Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices
revealed that Talking has the most similarity when compared to the other modes
of communication, while 1-grams were the least similar form of communication
analyzed. Based on the analysis of word usage, word usage frequency
distributions, and word class usage, among other things, Talking is also the
most similar to Twitter and IRC Chat. This suggests that communicating using
Twitter and IRC Chat evolved from Talking rather than Writing. When we
communicate online, even though we are writing, we do not Tweet or Chat how we
write books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing
were clearly differentiable from our analysis with Twitter and Chat being much
more similar to Fiction than Nonfiction writing. These hypotheses were then
tested using author and journalists Cory Doctorow. Mr. Doctorow$\text{'}$s
Writing, Twitter usage, and Talking were all found to have very similar
vocabulary usage patterns as the amalgamized populations, as long as the
writing was Fiction. However, Mr. Doctorow$\text{'}$s Nonfiction writing is
different from 1-grams and other collected Nonfiction writings. This data could
perhaps be used to create more entertaining works of Nonfiction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0533</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0533</id><created>2014-03-03</created><updated>2014-08-20</updated><authors><author><keyname>Botnan</keyname><forenames>Magnus Bakke</forenames></author><author><keyname>Spreemann</keyname><forenames>Gard</forenames></author></authors><title>Approximating Persistent Homology in Euclidean Space Through Collapses</title><categories>math.AT cs.CG</categories><doi>10.1007/s00200-014-0247-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \v{C}ech complex is one of the most widely used tools in applied
algebraic topology. Unfortunately, due to the inclusive nature of the \v{C}ech
filtration, the number of simplices grows exponentially in the number of input
points. A practical consequence is that computations may have to terminate at
smaller scales than what the application calls for.
  In this paper we propose two methods to approximate the \v{C}ech persistence
module. Both are constructed on the level of spaces, i.e. as sequences of
simplicial complexes induced by nerves. We also show how the bottleneck
distance between such persistence modules can be understood by how tightly they
are sandwiched on the level of spaces. In turn, this implies the correctness of
our approximation methods.
  Finally, we implement our methods and apply them to some example point clouds
in Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0537</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0537</id><created>2014-03-03</created><updated>2015-05-05</updated><authors><author><keyname>Romero-Jerez</keyname><forenames>Juan M.</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author></authors><title>A New Framework for the Performance Analysis of Wireless Communications
  under Hoyt (Nakagami-q) Fading</title><categories>cs.IT math.IT</categories><comments>The manuscript has been submitted to IEEE Transactions on Information
  Theory. It contains 28 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel relationship between the distribution of circular and
non-circular complex Gaussian random variables. Specifically, we show that the
distribution of the squared norm of a non-circular complex Gaussian random
variable, usually referred to as squared Hoyt distribution, can be constructed
from a conditional exponential distribution. From this fundamental connection
we introduce a new approach, the Hoyt transform method, that allows to analyze
the performance of a wireless link under Hoyt (Nakagami-q) fading in a very
simple way. We illustrate that many performance metrics for Hoyt fading can be
calculated by leveraging well-known results for Rayleigh fading and only
performing a finite-range integral. We use this technique to obtain novel
results for some information and communication-theoretic metrics in Hoyt fading
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0541</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0541</id><created>2014-03-03</created><authors><author><keyname>Anwar</keyname><forenames>Saadat</forenames></author></authors><title>Representing, reasoning and answering questions about biological
  pathways - various applications</title><categories>cs.AI cs.CE cs.CL</categories><comments>thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological organisms are composed of numerous interconnected biochemical
processes. Diseases occur when normal functionality of these processes is
disrupted. Thus, understanding these biochemical processes and their
interrelationships is a primary task in biomedical research and a prerequisite
for diagnosing diseases, and drug development. Scientists studying these
processes have identified various pathways responsible for drug metabolism, and
signal transduction, etc.
  Newer techniques and speed improvements have resulted in deeper knowledge
about these pathways, resulting in refined models that tend to be large and
complex, making it difficult for a person to remember all aspects of it. Thus,
computer models are needed to analyze them. We want to build such a system that
allows modeling of biological systems and pathways in such a way that we can
answer questions about them.
  Many existing models focus on structural and/or factoid questions, using
surface-level knowledge that does not require understanding the underlying
model. We believe these are not the kind of questions that a biologist may ask
someone to test their understanding of the biological processes. We want our
system to answer the kind of questions a biologist may ask. Such questions
appear in early college level text books.
  Thus the main goal of our thesis is to develop a system that allows us to
encode knowledge about biological pathways and answer such questions about them
demonstrating understanding of the pathway. To that end, we develop a language
that will allow posing such questions and illustrate the utility of our
framework with various applications in the biological domain. We use some
existing tools with modifications to accomplish our goal.
  Finally, we apply our system to real world applications by extracting pathway
knowledge from text and answering questions related to drug development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0543</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0543</id><created>2014-03-03</created><updated>2014-05-19</updated><authors><author><keyname>Hasegawa</keyname><forenames>Hideo</forenames><affiliation>Tokyo Gakugei Univ.</affiliation></author></authors><title>Quantum tunneling and evolution speed in an exactly solvable coupled
  double-well system</title><categories>quant-ph cs.IT math.IT</categories><comments>23 pages, 16 figures, revised Fig. 16 with changed title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact analytical calculations of eigenvalues and eigenstates are presented
for quantum coupled double-well (DW) systems with Razavy's hyperbolic
potential. With the use of four kinds of initial wavepackets, we have
calculated the tunneling period $T$ and the orthogonality time $\tau$ which
signifies a time interval for an initial state to evolve to its orthogonal
state. We discuss the coupling dependence of $T$ and $\tau$, and the relation
between $\tau$ and the concurrence $C$ which is a typical measure of the
entanglement in two qubits. Our calculations have shown that it is not clear
whether the speed of quantum evolution may be measured by $T$ or $\tau$ and
that the evolution speed measured by $\tau$ (or $T$) is not necessarily
increased with increasing $C$. This is in contrast with the earlier study [V.
Giovannetti, S. Lloyd and L. Maccone, Europhys. Lett. {\bf 62} (2003) 615]
which pointed out that the evolution speed measured by $\tau$ is enhanced by
the entanglement in the two-level model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0598</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0598</id><created>2014-03-03</created><authors><author><keyname>Yanardag</keyname><forenames>Pinar</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>The Structurally Smoothed Graphlet Kernel</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A commonly used paradigm for representing graphs is to use a vector that
contains normalized frequencies of occurrence of certain motifs or sub-graphs.
This vector representation can be used in a variety of applications, such as,
for computing similarity between graphs. The graphlet kernel of Shervashidze et
al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj
[28]) as motifs in the vector representation, and computes the kernel via a dot
product between these vectors. One can easily show that this is a valid kernel
between graphs. However, such a vector representation suffers from a few
drawbacks. As k becomes larger we encounter the sparsity problem; most higher
order graphlets will not occur in a given graph. This leads to diagonal
dominance, that is, a given graph is similar to itself but not to any other
graph in the dataset. On the other hand, since lower order graphlets tend to be
more numerous, using lower values of k does not provide enough discrimination
ability. We propose a smoothing technique to tackle the above problems. Our
method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing
techniques from natural language processing to graphs. We use the relationships
between lower order and higher order graphlets in order to derive our method.
Consequently, our smoothing algorithm not only respects the dependency between
sub-graphs but also tackles the diagonal dominance problem by distributing the
probability mass across graphlets. In our experiments, the smoothed graphlet
kernel outperforms graph kernels based on raw frequency counts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0600</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0600</id><created>2014-03-03</created><updated>2014-07-31</updated><authors><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Modeling Website Popularity Competition in the Attention-Activity
  Marketplace</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does a new startup drive the popularity of competing websites into
oblivion like Facebook famously did to MySpace? This question is of great
interest to academics, technologists, and financial investors alike. In this
work we exploit the singular way in which Facebook wiped out the popularity of
MySpace, Hi5, Friendster, and Multiply to guide the design of a new popularity
competition model. Our model provides new insights into what Nobel Laureate
Herbert A. Simon called the &quot;marketplace of attention,&quot; which we recast as the
attention-activity marketplace. Our model design is further substantiated by
user-level activity of 250,000 MySpace users obtained between 2004 and 2009.
The resulting model not only accurately fits the observed Daily Active Users
(DAU) of Facebook and its competitors but also predicts their fate four years
into the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0603</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0603</id><created>2014-03-03</created><updated>2014-03-05</updated><authors><author><keyname>Tsianos</keyname><forenames>Konstantinos I.</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Efficient Distributed Online Prediction and Stochastic Optimization with
  Approximate Distributed Averaging</title><categories>cs.IT cs.DC cs.SY math.IT math.OC</categories><comments>30 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed methods for online prediction and stochastic
optimization. Our approach is iterative: in each round nodes first perform
local computations and then communicate in order to aggregate information and
synchronize their decision variables. Synchronization is accomplished through
the use of a distributed averaging protocol. When an exact distributed
averaging protocol is used, it is known that the optimal regret bound of
$\mathcal{O}(\sqrt{m})$ can be achieved using the distributed mini-batch
algorithm of Dekel et al. (2012), where $m$ is the total number of samples
processed across the network. We focus on methods using approximate distributed
averaging protocols and show that the optimal regret bound can also be achieved
in this setting. In particular, we propose a gossip-based optimization method
which achieves the optimal regret bound. The amount of communication required
depends on the network topology through the second largest eigenvalue of the
transition matrix of a random walk on the network. In the setting of stochastic
optimization, the proposed gossip-based approach achieves nearly-linear
scaling: the optimization error is guaranteed to be no more than $\epsilon$
after $\mathcal{O}(\frac{1}{n \epsilon^2})$ rounds, each of which involves
$\mathcal{O}(\log n)$ gossip iterations, when nodes communicate over a
well-connected graph. This scaling law is also observed in numerical
experiments on a cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0613</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0613</id><created>2014-03-03</created><updated>2015-02-13</updated><authors><author><keyname>Li</keyname><forenames>Sanjiang</forenames></author><author><keyname>Long</keyname><forenames>Zhiguo</forenames></author><author><keyname>Liu</keyname><forenames>Weiming</forenames></author><author><keyname>Duckham</keyname><forenames>Matt</forenames></author><author><keyname>Both</keyname><forenames>Alan</forenames></author></authors><title>On Redundant Topological Constraints</title><categories>cs.AI</categories><comments>An extended abstract appears in Proceedings of the 14th International
  Conference on the Principles of Knowledge Representation and Reasoning
  (KR-14), Vienna, Austria, July 20-24, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Region Connection Calculus (RCC) is a well-known calculus for
representing part-whole and topological relations. It plays an important role
in qualitative spatial reasoning, geographical information science, and
ontology. The computational complexity of reasoning with RCC5 and RCC8 (two
fragments of RCC) as well as other qualitative spatial/temporal calculi has
been investigated in depth in the literature. Most of these works focus on the
consistency of qualitative constraint networks. In this paper, we consider the
important problem of redundant qualitative constraints. For a set $\Gamma$ of
qualitative constraints, we say a constraint $(x R y)$ in $\Gamma$ is redundant
if it is entailed by the rest of $\Gamma$. A prime subnetwork of $\Gamma$ is a
subset of $\Gamma$ which contains no redundant constraints and has the same
solution set as $\Gamma$. It is natural to ask how to compute such a prime
subnetwork, and when it is unique.
  In this paper, we show that this problem is in general intractable, but
becomes tractable if $\Gamma$ is over a tractable subalgebra $\mathcal{S}$ of a
qualitative calculus. Furthermore, if $\mathcal{S}$ is a subalgebra of RCC5 or
RCC8 in which weak composition distributes over nonempty intersections, then
$\Gamma$ has a unique prime subnetwork, which can be obtained in cubic time by
removing all redundant constraints simultaneously from $\Gamma$. As a
byproduct, we show that any path-consistent network over such a distributive
subalgebra is weakly globally consistent and minimal. A thorough empirical
analysis of the prime subnetwork upon real geographical data sets demonstrates
the approach is able to identify significantly more redundant constraints than
previously proposed algorithms, especially in constraint networks with larger
proportions of partial overlap relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0622</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0622</id><created>2014-03-03</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Minimal counterexamples and discharging method</title><categories>math.CO cs.DM</categories><comments>8 pages. Preliminary version, comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the author found that there is a common mistake in some papers by
using minimal counterexample and discharging method. We first discuss how the
mistake is generated, and give a method to fix the mistake. As an illustration,
we consider total coloring of planar or toroidal graphs, and show that: if $G$
is a planar or toroidal graph with maximum degree at most $\kappa - 1$, where
$\kappa \geq 11$, then the total chromatic number is at most $\kappa$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0623</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0623</id><created>2014-03-03</created><authors><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Pandey</keyname><forenames>Daya Shankar</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author></authors><title>Global solar irradiation prediction using a multi-gene genetic
  programming approach</title><categories>cs.NE cs.CE stat.AP</categories><comments>31 pages, 16 figures, 5 tables</comments><journal-ref>Journal of Renewable and Sustainable Energy, vol. 5, no. 6, pp.
  063129, 2013</journal-ref><doi>10.1063/1.4850495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a nonlinear symbolic regression technique using an
evolutionary algorithm known as multi-gene genetic programming (MGGP) is
applied for a data-driven modelling between the dependent and the independent
variables. The technique is applied for modelling the measured global solar
irradiation and validated through numerical simulations. The proposed modelling
technique shows improved results over the fuzzy logic and artificial neural
network (ANN) based approaches as attempted by contemporary researchers. The
method proposed here results in nonlinear analytical expressions, unlike those
with neural networks which is essentially a black box modelling approach. This
additional flexibility is an advantage from the modelling perspective and helps
to discern the important variables which affect the prediction. Due to the
evolutionary nature of the algorithm, it is able to get out of local minima and
converge to a global optimum unlike the back-propagation (BP) algorithm used
for training neural networks. This results in a better percentage fit than the
ones obtained using neural networks by contemporary researchers. Also a
hold-out cross validation is done on the obtained genetic programming (GP)
results which show that the results generalize well to new data and do not
over-fit the training samples. The multi-gene GP results are compared with
those, obtained using its single-gene version and also the same with four
classical regression models in order to show the effectiveness of the adopted
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0628</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0628</id><created>2014-03-03</created><updated>2014-05-21</updated><authors><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>Unconstrained Online Linear Learning in Hilbert Spaces: Minimax
  Algorithms and Normal Approximations</title><categories>cs.LG</categories><comments>Proceedings of the 27th Annual Conference on Learning Theory (COLT
  2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithms for online linear optimization in Hilbert spaces,
focusing on the case where the player is unconstrained. We develop a novel
characterization of a large class of minimax algorithms, recovering, and even
improving, several previous results as immediate corollaries. Moreover, using
our tools, we develop an algorithm that provides a regret bound of
$\mathcal{O}\Big(U \sqrt{T \log(U \sqrt{T} \log^2 T +1)}\Big)$, where $U$ is
the $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to
the player. This bound is optimal up to $\sqrt{\log \log T}$ terms. When $T$ is
known, we derive an algorithm with an optimal regret bound (up to constant
factors). For both the known and unknown $T$ case, a Normal approximation to
the conditional value of the game proves to be the key analysis tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0636</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0636</id><created>2014-03-03</created><authors><author><keyname>Toole</keyname><forenames>Jameson L.</forenames></author><author><keyname>Colak</keyname><forenames>Serdar</forenames></author><author><keyname>Alhasoun</keyname><forenames>Fahad</forenames></author><author><keyname>Evsukoff</keyname><forenames>Alexandre</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author></authors><title>The path most travelled: Mining road usage patterns from massive call
  data</title><categories>physics.soc-ph cs.SI</categories><comments>submitted to KDD 2014</comments><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid urbanization places increasing stress on already burdened
transportation systems, resulting in delays and poor levels of service.
Billions of spatiotemporal call detail records (CDRs) collected from mobile
devices create new opportunities to quantify and solve these problems. However,
there is a need for tools to map new data onto existing transportation
infrastructure. In this work, we propose a system that leverages this data to
identify patterns in road usage. First, we develop an algorithm to mine
billions of calls and learn location transition probabilities of callers. These
transition probabilities are then upscaled with demographic data to estimate
origin-destination (OD) flows of residents between any two intersections of a
city. Next, we implement a distributed incremental traffic assignment algorithm
to route these flows on road networks and estimate congestion and level of
service for each roadway. From this assignment, we construct a bipartite usage
network by connecting census tracts to the roads used by their inhabitants.
Comparing the topologies of the physical road network and bipartite usage
network allows us to classify each road's role in a city's transportation
network and detect causes of local bottlenecks. Finally, we demonstrate an
interactive, web-based visualization platform that allows researchers,
policymakers, and drivers to explore road congestion and usage in a new
dimension. To demonstrate the flexibility of this system, we perform these
analyses in multiple cities across the globe with diverse geographical and
sociodemographic qualities. This platform provides a foundation to build
congestion mitigation solutions and generate new insights into urban mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0641</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0641</id><created>2014-03-03</created><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Proof identity for mere mortals</title><categories>cs.LO</categories><comments>10 pages. Submitted to the Mark Stickel Festschrift</comments><msc-class>68T15</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proof identity problem asks: When are two proofs the same? The question
naturally occurs when one reflects on mathematical practice. The problem
understandably can be seen as a challenge for mathematical logic, and indeed
various perspectives on the problem can be found in the proof theory
literature. From the proof theory perspective, the challenge is met by laying
down new calculi that eliminate ``bureaucracy''; techniques such as
normalization and cut-elimination, as well as proof compression, are employed.
In this note a new perspective on the proof identity problem is outlined. The
new approach employs the concepts and tools of automated theorem proving and
complements the rather more theoretical perspectives coming from pure proof
theory. The practical approach is illustrated with experiments coming from the
TPTP Problem Library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0648</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0648</id><created>2014-03-03</created><authors><author><keyname>Hu</keyname><forenames>Jinli</forenames></author><author><keyname>Storkey</keyname><forenames>Amos</forenames></author></authors><title>Multi-period Trading Prediction Markets with Connections to Machine
  Learning</title><categories>cs.GT cs.LG q-fin.TR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new model for prediction markets, in which we use risk measures
to model agents and introduce a market maker to describe the trading process.
This specific choice on modelling tools brings us mathematical convenience. The
analysis shows that the whole market effectively approaches a global objective,
despite that the market is designed such that each agent only cares about its
own goal. Additionally, the market dynamics provides a sensible algorithm for
optimising the global objective. An intimate connection between machine
learning and our markets is thus established, such that we could 1) analyse a
market by applying machine learning methods to the global objective, and 2)
solve machine learning problems by setting up and running certain markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0667</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0667</id><created>2014-03-03</created><updated>2015-10-01</updated><authors><author><keyname>Voss</keyname><forenames>James</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author></authors><title>The Hidden Convexity of Spectral Clustering</title><categories>cs.LG stat.ML</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, spectral clustering has become a standard method for data
analysis used in a broad range of applications. In this paper we propose a new
class of algorithms for multiway spectral clustering based on optimization of a
certain &quot;contrast function&quot; over the unit sphere. These algorithms, partly
inspired by certain Independent Component Analysis techniques, are simple, easy
to implement and efficient.
  Geometrically, the proposed algorithms can be interpreted as hidden basis
recovery by means of function optimization. We give a complete characterization
of the contrast functions admissible for provable basis recovery. We show how
these conditions can be interpreted as a &quot;hidden convexity&quot; of our optimization
problem on the sphere; interestingly, we use efficient convex maximization
rather than the more common convex minimization. We also show encouraging
experimental results on real and simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0686</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0686</id><created>2014-03-04</created><updated>2014-07-29</updated><authors><author><keyname>Soleimani-Nasab</keyname><forenames>E.</forenames></author><author><keyname>Kalantari</keyname><forenames>A.</forenames></author><author><keyname>Ardebilipour</keyname><forenames>M.</forenames></author></authors><title>Performance Analysis of Multi-Antenna Relay Networks over Nakagami-m
  Fading Channel</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter, the authors present the performance of multi-antenna
selective combining decode-and-forward (SC-DF) relay networks over independent
and identically distributed (i.i.d) Nakagami-m fading channels. The outage
probability, moment generation function, symbol error probability and average
channel capacity are derived in closed-form using the Signal-to-Noise-Ratio
(SNR) statistical characteristics. After that, the authors formulate the outage
probability problem, optimize it with an approximated problem, and then solve
it analytically. Finally, for comparison with analytical formulas, the authors
perform some Monte-Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0699</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0699</id><created>2014-03-04</created><authors><author><keyname>Alavi</keyname><forenames>Azadeh</forenames></author><author><keyname>Yang</keyname><forenames>Yan</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Multi-Shot Person Re-Identification via Relational Stein Divergence</title><categories>cs.CV stat.ML</categories><comments>IEEE International Conference on Image Processing (ICIP), 2013</comments><acm-class>I.5.1; I.5.4; I.2.10; I.4.7; I.4.8; I.4.10</acm-class><doi>10.1109/ICIP.2013.6738731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification is particularly challenging due to significant
appearance changes across separate camera views. In order to re-identify
people, a representative human signature should effectively handle differences
in illumination, pose and camera parameters. While general appearance-based
methods are modelled in Euclidean spaces, it has been argued that some
applications in image and video analysis are better modelled via non-Euclidean
manifold geometry. To this end, recent approaches represent images as
covariance matrices, and interpret such matrices as points on Riemannian
manifolds. As direct classification on such manifolds can be difficult, in this
paper we propose to represent each manifold point as a vector of similarities
to class representers, via a recently introduced form of Bregman matrix
divergence known as the Stein divergence. This is followed by using a
discriminative mapping of similarity vectors for final classification. The use
of similarity vectors is in contrast to the traditional approach of embedding
manifolds into tangent spaces, which can suffer from representing the manifold
structure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS
datasets for the person re-identification task show that the proposed approach
obtains better performance than recent techniques such as Histogram Plus
Epitome, Partial Least Squares, and Symmetry-Driven Accumulation of Local
Features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0700</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0700</id><created>2014-03-04</created><authors><author><keyname>Alavi</keyname><forenames>Azadeh</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Random Projections on Manifolds of Symmetric Positive Definite Matrices
  for Image Classification</title><categories>cs.CV stat.ML</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2014</comments><acm-class>I.4.7; I.4.10; I.5.1; I.5.4</acm-class><doi>10.1109/WACV.2014.6836085</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances suggest that encoding images through Symmetric Positive
Definite (SPD) matrices and then interpreting such matrices as points on
Riemannian manifolds can lead to increased classification performance. Taking
into account manifold geometry is typically done via (1) embedding the
manifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert
Spaces (RKHS). While embedding into tangent spaces allows the use of existing
Euclidean-based learning algorithms, manifold shape is only approximated which
can cause loss of discriminatory information. The RKHS approach retains more of
the manifold structure, but may require non-trivial effort to kernelise
Euclidean-based learning algorithms. In contrast to the above approaches, in
this paper we offer a novel solution that allows SPD matrices to be used with
unmodified Euclidean-based learning algorithms, with the true manifold shape
well-preserved. Specifically, we propose to project SPD matrices using a set of
random projection hyperplanes over RKHS into a random projection space, which
leads to representing each matrix as a vector of projection coefficients.
Experiments on face recognition, person re-identification and texture
classification show that the proposed approach outperforms several recent
methods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian
Locality Preserving Projection and Relational Divergence Classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0701</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0701</id><created>2014-03-04</created><authors><author><keyname>Kyrola</keyname><forenames>Aapo</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>GraphChi-DB: Simple Design for a Scalable Graph Database System -- on
  Just a PC</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new data structure, Parallel Adjacency Lists (PAL), for
efficiently managing graphs with billions of edges on disk. The PAL structure
is based on the graph storage model of GraphChi (Kyrola et. al., OSDI 2012),
but we extend it to enable online database features such as queries and fast
insertions. In addition, we extend the model with edge and vertex attributes.
Compared to previous data structures, PAL can store graphs more compactly while
allowing fast access to both the incoming and the outgoing edges of a vertex,
without duplicating data. Based on PAL, we design a graph database management
system, GraphChi-DB, which can also execute powerful analytical graph
computation.
  We evaluate our design experimentally and demonstrate that GraphChi-DB
achieves state-of-the-art performance on graphs that are much larger than the
available memory. GraphChi-DB enables anyone with just a laptop or a PC to work
with extremely large graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0728</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0728</id><created>2014-03-04</created><authors><author><keyname>Birdal</keyname><forenames>Tolga</forenames></author><author><keyname>Bala</keyname><forenames>Emrah</forenames></author></authors><title>A Novel Method for Vectorization</title><categories>cs.CV cs.CG cs.GR</categories><comments>Prepared in Siggraph format, not published in a conference, 7 pages,
  9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vectorization of images is a key concern uniting computer graphics and
computer vision communities. In this paper we are presenting a novel idea for
efficient, customizable vectorization of raster images, based on Catmull Rom
spline fitting. The algorithm maintains a good balance between photo-realism
and photo abstraction, and hence is applicable to applications with artistic
concerns or applications where less information loss is crucial. The resulting
algorithm is fast, parallelizable and can satisfy general soft realtime
requirements. Moreover, the smoothness of the vectorized images aesthetically
outperforms outputs of many polygon-based methods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0734</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0734</id><created>2014-03-04</created><updated>2014-07-08</updated><authors><author><keyname>Finocchi</keyname><forenames>Irene</forenames></author><author><keyname>Finocchi</keyname><forenames>Marco</forenames></author><author><keyname>Fusco</keyname><forenames>Emanuele G.</forenames></author></authors><title>Clique counting in MapReduce: theory and experiments</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of counting the number of $k$-cliques in large-scale
graphs, for any constant $k \ge 3$. Clique counting is essential in a variety
of applications, among which social network analysis. Due to its
computationally intensive nature, we settle for parallel solutions in the
MapReduce framework, which has become in the last few years a {\em de facto}
standard for batch processing of massive data sets. We give both theoretical
and experimental contributions.
  On the theory side, we design the first exact scalable algorithm for counting
(and listing) $k$-cliques. Our algorithm uses $O(m^{3/2})$ total space and
$O(m^{k/2})$ work, where $m$ is the number of graph edges. This matches the
best-known bounds for triangle listing when $k=3$ and is work-optimal in the
worst case for any $k$, while keeping the communication cost independent of
$k$. We also design a sampling-based estimator that can dramatically reduce the
running time and space requirements of the exact approach, while providing very
accurate solutions with high probability.
  We then assess the effectiveness of different clique counting approaches
through an extensive experimental analysis over the Amazon EC2 platform,
considering both our algorithms and their state-of-the-art competitors. The
experimental results clearly highlight the algorithm of choice in different
scenarios and prove our exact approach to be the most effective when the number
of $k$-cliques is large, gracefully scaling to non-trivial values of $k$ even
on clusters of small/medium size. Our approximation algorithm achieves
extremely accurate estimates and large speedups, especially on the toughest
instances for the exact algorithms. As a side effect, our study also sheds
light on the number of $k$-cliques of several real-world graphs, mainly social
networks, and on its growth rate as a function of $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0736</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0736</id><created>2014-03-04</created><updated>2014-10-03</updated><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>De Smet</keyname><forenames>Frank</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>Fast Prediction with SVM Models Containing RBF Kernels</title><categories>stat.ML cs.LG</categories><comments>9 pages, 1 figure, 3 tables</comments><acm-class>G.3; I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approximation scheme for support vector machine models that use
an RBF kernel. A second-order Maclaurin series approximation is used for
exponentials of inner products between support vectors and test instances. The
approximation is applicable to all kernel methods featuring sums of kernel
evaluations and makes no assumptions regarding data normalization. The
prediction speed of approximated models no longer relates to the amount of
support vectors but is quadratic in terms of the number of input dimensions. If
the number of input dimensions is small compared to the amount of support
vectors, the approximated model is significantly faster in prediction and has a
smaller memory footprint. An optimized C++ implementation was made to assess
the gain in prediction speed in a set of practical tests. We additionally
provide a method to verify the approximation accuracy, prior to training models
or during run-time, to ensure the loss in accuracy remains acceptable and
within known bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0745</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0745</id><created>2014-03-04</created><authors><author><keyname>Claesen</keyname><forenames>Marc</forenames></author><author><keyname>De Smet</keyname><forenames>Frank</forenames></author><author><keyname>Suykens</keyname><forenames>Johan</forenames></author><author><keyname>De Moor</keyname><forenames>Bart</forenames></author></authors><title>EnsembleSVM: A Library for Ensemble Learning Using Support Vector
  Machines</title><categories>stat.ML cs.LG</categories><comments>5 pages, 1 table</comments><acm-class>G.3; I.2.6; I.5.1</acm-class><journal-ref>Journal of Machine Learning Research. 15 (2014) 141-145</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EnsembleSVM is a free software package containing efficient routines to
perform ensemble learning with support vector machine (SVM) base models. It
currently offers ensemble methods based on binary SVM models. Our
implementation avoids duplicate storage and evaluation of support vectors which
are shared between constituent models. Experimental results show that using
ensemble approaches can drastically reduce training complexity while
maintaining high predictive accuracy. The EnsembleSVM software package is
freely available online at http://esat.kuleuven.be/stadius/ensemblesvm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0749</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0749</id><created>2014-03-04</created><updated>2014-06-08</updated><authors><author><keyname>Capriotti</keyname><forenames>Paolo</forenames><affiliation>University of Nottingham</affiliation></author><author><keyname>Kaposi</keyname><forenames>Ambrus</forenames><affiliation>University of Nottingham</affiliation></author></authors><title>Free Applicative Functors</title><categories>cs.PL math.CT</categories><comments>In Proceedings MSFP 2014, arXiv:1406.1534</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 153, 2014, pp. 2-30</journal-ref><doi>10.4204/EPTCS.153.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applicative functors are a generalisation of monads. Both allow the
expression of effectful computations into an otherwise pure language, like
Haskell. Applicative functors are to be preferred to monads when the structure
of a computation is fixed a priori. That makes it possible to perform certain
kinds of static analysis on applicative values. We define a notion of free
applicative functor, prove that it satisfies the appropriate laws, and that the
construction is left adjoint to a suitable forgetful functor. We show how free
applicative functors can be used to implement embedded DSLs which can be
statically analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0750</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0750</id><created>2014-03-04</created><updated>2014-05-05</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Recent Developments with the licas System 2</title><categories>cs.DC</categories><comments>White paper / pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The licas (lightweight Internet-based communication for autonomic services)
system is a Java-based open source framework for building service-based
networks, similar to what you would use a Cloud or SOA platform for. The
framework comes with a server for running the services on, mechanisms for
adding services to the server, mechanisms for linking services with each other,
and mechanisms for allowing the services to communicate with each other. The
general architecture of the system is now fairly well set, where this paper
describes recent developments that have focused on making the framework more
robust and additional features for easier programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0751</identifier>
 <datestamp>2015-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0751</id><created>2014-03-04</created><updated>2015-07-03</updated><authors><author><keyname>Kwanashie</keyname><forenames>Augustine</forenames></author><author><keyname>Irving</keyname><forenames>Robert W.</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author><author><keyname>Sng</keyname><forenames>Colin T. S.</forenames></author></authors><title>Profile-based optimal matchings in the Student/Project Allocation
  problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Student / Project Allocation problem (SPA) we seek to assign students
to individual or group projects offered by lecturers. Students provide a list
of projects they find acceptable in order of preference. Each student can be
assigned to at most one project and there are constraints on the maximum number
of students that can be assigned to each project and lecturer. We seek
matchings of students to projects that are optimal with respect to profile,
which is a vector whose rth component indicates how many students have their
rth-choice project. We present an efficient algorithm for finding a greedy
maximum matching in the SPA context - this is a maximum matching whose profile
is lexicographically maximum. We then show how to adapt this algorithm to find
a generous maximum matching - this is a matching whose reverse profile is
lexicographically minimum. Our algorithms involve finding optimal flows in
networks. We demonstrate how this approach can allow for additional
constraints, such as lecturer lower quotas, to be handled flexibly. Finally we
present results obtained from an empirical evaluation of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0753</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0753</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Recent Developments with the licas System</title><categories>cs.DC</categories><comments>White paper / pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes recent developments with the licas (lightweight
Internet-based communication for autonomic services) software package. In
particular, it describes how the architecture and functionality have changed
from the first version release. The autonomous nature of the system is focused
on, which requires independent behaviour and metadata descriptions of each
service. The system has now also been ported to the Java mobile environment.
Then some open questions or problems will be discussed in the areas of metadata
consistency, security and trust. Finally, some solutions to these problems will
also be suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0761</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0761</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>The Obvious Solution to Semantic Mapping -- Ask an Expert</title><categories>cs.IR</categories><comments>White paper / pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic mapping problem is probably the main obstacle to
computer-to-computer communication. If computer A knows that its concept X is
the same as computer B's concept Y, then the two machines can communicate. They
will in effect be talking the same language. This paper describes a relatively
straightforward way of enhancing the semantic descriptions of Web Service
interfaces by using online sources of keyword definitions. Method interface
descriptions can be enhanced using these standard dictionary definitions.
Because the generated metadata is now standardised, this means that any other
computer that has access to the same source, or understands standard language
concepts, can now understand the description. This helps to remove a lot of the
heterogeneity that would otherwise build up though humans creating their own
descriptions independently of each other. The description comes in the form of
an XML script that can be retrieved and read through the Web Service interface
itself. An additional use for these scripts would be for adding descriptions in
different languages, which would mean that human users that speak a different
language would also understand what the service was about.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0762</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0762</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Evaluating Dynamic Linking through the Query Process using the Licas
  Test Platform</title><categories>cs.DC</categories><comments>White paper / pre-print</comments><journal-ref>IOSR Journal of Engineering (IOSRJEN), Vol. 5, issue 2, February,
  pp. 45 - 52, 2015. ISSN (e): 2250-3021, ISSN (p): 2278-8719</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel linking mechanism has been described previously [4] that can be used
to autonomously link sources that provide related answers to queries executed
over an information network. The test query platform has now been re-written
resulting in essentially a new test platform using the same basic query
mechanism, but with a slightly different algorithm. This paper describes recent
test results on the same query test process that supports the original findings
and also shows the effectiveness of the linking mechanism in a new set of test
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0764</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0764</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Clustering Concept Chains from Ordered Data without Path Descriptions</title><categories>cs.AI</categories><comments>Pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a process for clustering concepts into chains from data
presented randomly to an evaluating system. There are a number of rules or
guidelines that help the system to determine more accurately what concepts
belong to a particular chain and what ones do not, but it should be possible to
write these in a generic way. This mechanism also uses a flat structure without
any hierarchical path information, where the link between two concepts is made
at the level of the concept itself. It does not require related metadata, but
instead, a simple counting mechanism is used. Key to this is a count for both
the concept itself and also the group or chain that it belongs to. To test the
possible success of the mechanism, concept chain parts taken randomly from a
larger ontology were presented to the system, but only at a depth of 2 concepts
each time. That is - root concept plus a concept that it is linked to. The
results show that this can still lead to very variable structures being formed
and can also accommodate some level of randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0766</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0766</id><created>2014-03-04</created><authors><author><keyname>Hanspach</keyname><forenames>Michael</forenames></author><author><keyname>Schumann</keyname><forenames>Ralf</forenames></author><author><keyname>Schemmer</keyname><forenames>Stefan</forenames></author><author><keyname>Vandersee</keyname><forenames>Sebastian</forenames></author></authors><title>Service-Fingerprinting mittels Fuzzing</title><categories>cs.CR cs.NI</categories><comments>10 pages, May 2009, German language content. This paper is based on
  the Diplom thesis / Diplomarbeit: &quot;Verbesserung von OS- und
  Service-Fingerprinting mittels Fuzzing&quot;, Michael Hanspach, September 2008.
  DACH Security 2009: Bestandsaufnahme - Konzepte - Anwendungen - Perspektiven,
  May 2009, ISBN 300027488X</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service fingerprinting (i.e. the identification of network services and other
applications on computing systems) is an essential part of penetration tests.
The main contribution of this paper is a study on the improvement of
fingerprinting tools. By applying mutation-based fuzzing as a fingerprint
generation method, subtle differences in response messages can be identified.
These differences in response messages provide means for the differentiation
and identification of network services. To prove the feasibility of the
approach, an implementation of a fingerprinting tool for ftp servers is
presented and compared to preexisting fingerprinting tools. As a result of this
study it is shown that mutation-based fuzzing is an appropriate method for
service fingerprinting that even offers advantages over preexisting methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0770</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0770</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>A Metric for Modelling and Measuring Complex Behavioural Systems</title><categories>cs.MA</categories><comments>Published</comments><journal-ref>IOSR Journal of Engineering (IOSRJEN), Vol. 3, Issue 11, November
  2013, pp. 11 - 28</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a metric for measuring the success of a complex system
composed of agents performing autonomous behaviours. Because of the difficulty
in evaluating such systems, this metric will help to give an initial indication
as to how suitable the agents would be for solving the problem. The system is
modelled as a script, or behavioural ontology, with a number of variables to
represent each of the behaviour attributes. The set of equations can be used
both for modeling and as part of the simulation evaluation. Behaviours can be
nested, allowing for compound behaviours of arbitrary complexity to be built.
There is also the capability for including rules or decision making into the
script. The paper also gives some test examples to show how the metric might be
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0778</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0778</id><created>2014-03-04</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Dynamic Move Chains -- a Forward Pruning Approach to Tree Search in
  Computer Chess</title><categories>cs.AI cs.NE</categories><comments>Published</comments><journal-ref>Advances in Artificial Intelligence, Vol. 2013, Article ID 357068,
  9 pages, 2013. Hindawi</journal-ref><doi>10.1155/2013/357068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new mechanism for pruning a search game-tree in
computer chess. The algorithm stores and then reuses chains or sequences of
moves, built up from previous searches. These move sequences have a built-in
forward-pruning mechanism that can radically reduce the search space. A typical
search process might retrieve a move from a Transposition Table, where the
decision of what move to retrieve would be based on the position itself. This
algorithm stores move sequences based on what previous sequences were better,
or caused cutoffs. This is therefore position independent and so it could also
be useful in games with imperfect information or uncertainty, where the whole
situation is not known at any one time. Over a small set of tests, the
algorithm was shown to clearly out-perform Transposition Tables, both in terms
of search reduction and game-play results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0779</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0779</id><created>2014-03-04</created><updated>2014-05-02</updated><authors><author><keyname>Jiang</keyname><forenames>Minhao</forenames></author><author><keyname>Fu</keyname><forenames>Ada Wai-Chee</forenames></author><author><keyname>Wong</keyname><forenames>Raymond Chi-Wing</forenames></author><author><keyname>Xu</keyname><forenames>Yanyan</forenames></author></authors><title>Hop Doubling Label Indexing for Point-to-Point Distance Querying on
  Scale-Free Networks</title><categories>cs.DB</categories><comments>13 pages. More experiments and discussions are added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of point-to-point distance querying for massive
scale-free graphs, which is important for numerous applications. Given a
directed or undirected graph, we propose to build an index for answering such
queries based on a hop-doubling labeling technique. We derive bounds on the
index size, the computation costs and I/O costs based on the properties of
unweighted scale-free graphs. We show that our method is much more efficient
compared to the state-of-the-art technique, in terms of both querying time and
indexing time. Our empirical study shows that our method can handle graphs that
are orders of magnitude larger than existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0783</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0783</id><created>2014-03-04</created><authors><author><keyname>Amarilli</keyname><forenames>Antoine</forenames></author><author><keyname>Amsterdamer</keyname><forenames>Yael</forenames></author><author><keyname>Milo</keyname><forenames>Tova</forenames></author></authors><title>Uncertainty in Crowd Data Sourcing under Structural Constraints</title><categories>cs.DB</categories><comments>8 pages, vision paper. To appear at UnCrowd 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications extracting data from crowdsourcing platforms must deal with the
uncertainty of crowd answers in two different ways: first, by deriving
estimates of the correct value from the answers; second, by choosing crowd
questions whose answers are expected to minimize this uncertainty relative to
the overall data collection goal. Such problems are already challenging when we
assume that questions are unrelated and answers are independent, but they are
even more complicated when we assume that the unknown values follow hard
structural constraints (such as monotonicity).
  In this vision paper, we examine how to formally address this issue with an
approach inspired by [Amsterdamer et al., 2013]. We describe a generalized
setting where we model constraints as linear inequalities, and use them to
guide the choice of crowd questions and the processing of answers. We present
the main challenges arising in this setting, and propose directions to solve
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0789</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0789</id><created>2014-03-04</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author><author><keyname>van Leeuwen</keyname><forenames>Erik Jan</forenames></author></authors><title>Induced Disjoint Paths in Circular-Arc Graphs in Linear Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Induced Disjoint Paths problem is to test whether a graph G with k
distinct pairs of vertices (s_i,t_i) contains paths P_1,...,P_k such that P_i
connects s_i and t_i for i=1,...,k, and P_i and P_j have neither common
vertices nor adjacent vertices (except perhaps their ends) for 1&lt;=i &lt; j&lt;=k. We
present a linear-time algorithm for Induced Disjoint Paths on circular-arc
graphs. For interval graphs, we exhibit a linear-time algorithm for the
generalization of Induced Disjoint Paths where the pairs (s_i,t_i) are not
necessarily distinct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0800</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0800</id><created>2014-03-04</created><updated>2014-07-14</updated><authors><author><keyname>Larsson</keyname><forenames>N. Jesper</forenames></author></authors><title>Most Recent Match Queries in On-Line Suffix Trees (with appendix)</title><categories>cs.DS</categories><comments>Result presented at CPM 2014, Moscow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A suffix tree is able to efficiently locate a pattern in an indexed string,
but not in general the most recent copy of the pattern in an online stream,
which is desirable in some applications. We study the most general version of
the problem of locating a most recent match: supporting queries for arbitrary
patterns, at each step of processing an online stream. We present augmentations
to Ukkonen's suffix tree construction algorithm for optimal-time queries,
maintaining indexing time within a logarithmic factor in the size of the
indexed string. We show that the algorithm is applicable to sliding-window
indexing, and sketch a possible optimization for use in the special case of
Lempel-Ziv compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0801</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0801</id><created>2014-03-04</created><updated>2014-03-05</updated><authors><author><keyname>Higgins</keyname><forenames>Derrick</forenames></author><author><keyname>Brew</keyname><forenames>Chris</forenames></author><author><keyname>Heilman</keyname><forenames>Michael</forenames></author><author><keyname>Ziai</keyname><forenames>Ramon</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Cahill</keyname><forenames>Aoife</forenames></author><author><keyname>Flor</keyname><forenames>Michael</forenames></author><author><keyname>Madnani</keyname><forenames>Nitin</forenames></author><author><keyname>Tetreault</keyname><forenames>Joel</forenames></author><author><keyname>Blanchard</keyname><forenames>Daniel</forenames></author><author><keyname>Napolitano</keyname><forenames>Diane</forenames></author><author><keyname>Lee</keyname><forenames>Chong Min</forenames></author><author><keyname>Blackmore</keyname><forenames>John</forenames></author></authors><title>Is getting the right answer just about choosing the right words? The
  role of syntactically-informed features in short answer scoring</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developments in the educational landscape have spurred greater interest in
the problem of automatically scoring short answer questions. A recent shared
task on this topic revealed a fundamental divide in the modeling approaches
that have been applied to this problem, with the best-performing systems split
between those that employ a knowledge engineering approach and those that
almost solely leverage lexical information (as opposed to higher-level
syntactic information) in assigning a score to a given response. This paper
aims to introduce the NLP community to the largest corpus currently available
for short-answer scoring, provide an overview of methods used in the shared
task using this data, and explore the extent to which more
syntactically-informed features can contribute to the short answer scoring task
in a way that avoids the question-specific manual effort of the knowledge
engineering approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0802</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0802</id><created>2014-03-04</created><authors><author><keyname>You</keyname><forenames>Jianting Zhang Simin</forenames></author></authors><title>Large-Scale Geospatial Processing on Multi-Core and Many-Core
  Processors: Evaluations on CPUs, GPUs and MICs</title><categories>cs.DB cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geospatial Processing, such as queries based on point-to-polyline shortest
distance and point-in-polygon test, are fundamental to many scientific and
engineering applications, including post-processing large-scale environmental
and climate model outputs and analyzing traffic and travel patterns from
massive GPS collections in transportation engineering and urban studies.
Commodity parallel hardware, such as multi-core CPUs, many-core GPUs and Intel
MIC accelerators, provide enormous computing power which can potentially
achieve significant speedups on existing geospatial processing and open the
opportunities for new applications. However, the realizable potential for
geospatial processing on these new hardware devices is largely unknown due to
the complexity in porting serial algorithms to diverse parallel hardware
platforms. In this study, we aim at experimenting our data-parallel designs and
implementations of point-to-polyline shortest distance computation (P2P) and
point-in-polygon topological test (PIP) on different commodity hardware using
real large-scale geospatial data, comparing their performance and discussing
important factors that may significantly affect the performance. Our
experiments have shown that, while GPUs can be several times faster than
multi-core CPUs without utilizing the increasingly available SIMD computing
power on Vector Processing Units (VPUs) that come with multi-core CPUs and
MICs, multi-core CPUs and MICs can be several times faster than GPUs when VPUs
are utilized. By adopting a Domain Specific Language (DSL) approach to
exploiting the VPU computing power in geospatial processing, we are free from
programming SIMD intrinsic functions directly which makes the new approach more
effective, portable and scalable. Our designs, implementations and experiments
can serve as case studies for parallel geospatial computing on modern commodity
parallel hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0804</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0804</id><created>2014-02-10</created><authors><author><keyname>Samadieh</keyname><forenames>Mehdi</forenames></author><author><keyname>Gholamiy</keyname><forenames>Mohammad</forenames></author></authors><title>Double Cylinder Cycle codes of Arbitrary Girth</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>e.g.: 10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A particular class of low-density parity-check codes referred to as
cylinder-type BC-LDPC codes is proposed by Gholami and Eesmaeili. In this paper
We represent a double cylinder-type parity-check matrix H by a graph called the
block-structure graph of H and denoted by BSG(H). Using the properties of
BSG(H) we propose some mother matrices with column-weight two such that the
rate of corresponding cycle codes are greater tan cycle codes constructed by
Gholami with same girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0811</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0811</id><created>2014-03-04</created><authors><author><keyname>Choi</keyname><forenames>Han-Lim</forenames></author><author><keyname>Lee</keyname><forenames>Su-Jin</forenames></author></authors><title>A Potential Game Approach for Information-Maximizing Cooperative
  Planning of Sensor Networks</title><categories>cs.SY cs.GT</categories><comments>9 pages, 4 figures, submitted to IEEE Transactions on Control Systems
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a potential game approach for distributed cooperative
selection of informative sensors, when the goal is to maximize the mutual
information between the measurement variables and the quantities of interest.
It is proved that a local utility function defined by the conditional mutual
information of an agent conditioned on the other agents' sensing decisions
leads to a potential game with the global potential being the original mutual
information of the cooperative planning problem. The joint strategy fictitious
play method is then applied to obtain a distributed solution that provably
converges to a pure strategy Nash equilibrium. Two numerical examples on
simplified weather forecasting and range-only target tracking verify
convergence and performance characteristics of the proposed game-theoretic
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0820</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0820</id><created>2014-03-04</created><updated>2015-02-13</updated><authors><author><keyname>Anirudh</keyname><forenames>Rushil</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Geometry-based Adaptive Symbolic Approximation for Fast Sequence
  Matching on Manifolds</title><categories>cs.CV math.DG</categories><comments>Under major revision at IJCV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of fast and efficient indexing
techniques for sequences evolving in non-Euclidean spaces. This problem has
several applications in the areas of human activity analysis, where there is a
need to perform fast search, and recognition in very high dimensional spaces.
The problem is made more challenging when representations such as landmarks,
contours, and human skeletons etc. are naturally studied in a non-Euclidean
setting where even simple operations are much more computationally intensive
than their Euclidean counterparts. We propose a geometry and data adaptive
symbolic framework that is shown to enable the deployment of fast and accurate
algorithms for activity recognition, dynamic texture recognition, motif
discovery. Toward this end, we present generalizations of key concepts of
piece-wise aggregation and symbolic approximation for the case of non-Euclidean
manifolds. We show that one can replace expensive geodesic computations with
much faster symbolic computations with little loss of accuracy in activity
recognition and discovery applications. The framework is general enough to work
across both Euclidean and non-Euclidean spaces, depending on appropriate
feature representations without compromising on the ultra-low bandwidth, high
speed and high accuracy. The proposed methods are ideally suited for real-time
systems and low complexity scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0829</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0829</id><created>2014-03-02</created><authors><author><keyname>Liu</keyname><forenames>W.</forenames></author><author><keyname>Liu</keyname><forenames>H.</forenames></author><author><keyname>Tao</keyname><forenames>D.</forenames></author><author><keyname>Wang</keyname><forenames>Y.</forenames></author><author><keyname>Lu</keyname><forenames>Ke</forenames></author></authors><title>Multiview Hessian regularized logistic regression for action recognition</title><categories>cs.CV cs.LG stat.ML</categories><comments>13 pages,2 figures, submitted to signal processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of social media sharing, people often need to
manage the growing volume of multimedia data such as large scale video
classification and annotation, especially to organize those videos containing
human activities. Recently, manifold regularized semi-supervised learning
(SSL), which explores the intrinsic data probability distribution and then
improves the generalization ability with only a small number of labeled data,
has emerged as a promising paradigm for semiautomatic video classification. In
addition, human action videos often have multi-modal content and different
representations. To tackle the above problems, in this paper we propose
multiview Hessian regularized logistic regression (mHLR) for human action
recognition. Compared with existing work, the advantages of mHLR lie in three
folds: (1) mHLR combines multiple Hessian regularization, each of which
obtained from a particular representation of instance, to leverage the
exploring of local geometry; (2) mHLR naturally handle multi-view instances
with multiple representations; (3) mHLR employs a smooth loss function and then
can be effectively optimized. We carefully conduct extensive experiments on the
unstructured social activity attribute (USAA) dataset and the experimental
results demonstrate the effectiveness of the proposed multiview Hessian
regularized logistic regression for human action recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0833</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0833</id><created>2014-03-04</created><updated>2014-05-12</updated><authors><author><keyname>Pierre</keyname><forenames>Hyvernat</forenames><affiliation>Universit&#xe9; de Savoie</affiliation></author></authors><title>A Linear Category of Polynomial Functors (extensional part)</title><categories>cs.LO</categories><comments>21 pages +2 pages d'appendice</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (May 13,
  2014) lmcs:1185</journal-ref><doi>10.2168/LMCS-10(2:2)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a symmetric monoidal closed category of polynomial endofunctors
(as objects) and simulation cells (as morphisms). This structure is defined
using universal properties without reference to representing polynomial
diagrams and is reminiscent of Day's convolution on presheaves. We then make
this category into a model for intuitionistic linear logic by defining an
additive and exponential structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0835</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0835</id><created>2014-03-04</created><updated>2014-04-05</updated><authors><author><keyname>Mustafa</keyname><forenames>Nabil H.</forenames></author><author><keyname>Raman</keyname><forenames>Rajiv</forenames></author><author><keyname>Ray</keyname><forenames>Saurabh</forenames></author></authors><title>QPTAS for Geometric Set-Cover Problems via Optimal Separators</title><categories>cs.CG</categories><comments>26 pages. Revised to include an additional set-cover QPTAS for
  halfspaces</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted geometric set-cover problems arise naturally in several geometric
and non-geometric settings (e.g. the breakthrough of Bansal-Pruhs (FOCS 2010)
reduces a wide class of machine scheduling problems to weighted geometric
set-cover). More than two decades of research has succeeded in settling the
$(1+\epsilon)$-approximability status for most geometric set-cover problems,
except for four basic scenarios which are still lacking. One is that of
weighted disks in the plane for which, after a series of papers, Varadarajan
(STOC 2010) presented a clever \emph{quasi-sampling} technique, which together
with improvements by Chan \etal~(SODA 2012), yielded a $O(1)$-approximation
algorithm. Even for the unweighted case, a PTAS for a fundamental class of
objects called pseudodisks (which includes disks, unit-height rectangles,
translates of convex sets etc.) is currently unknown. Another fundamental case
is weighted halfspaces in $\Re^3$, for which a PTAS is currently lacking. In
this paper, we present a QPTAS for all of these remaining problems. Our results
are based on the separator framework of Adamaszek-Wiese (FOCS 2013, SODA 2014),
who recently obtained a QPTAS for weighted independent set of polygonal
regions. This rules out the possibility that these problems are APX-hard,
assuming $\textbf{NP} \nsubseteq \textbf{DTIME}(2^{polylog(n)})$. Together with
the recent work of Chan-Grant (CGTA 2014), this settles the APX-hardness status
for all natural geometric set-cover problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0836</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0836</id><created>2014-03-04</created><authors><author><keyname>Li</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Wymeersch</keyname><forenames>H.</forenames></author></authors><title>Locally-Optimized Reweighted Belief Propagation for Decoding LDPC Codes
  with Finite-Length</title><categories>cs.IT math.IT</categories><comments>5 figures, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practice, LDPC codes are decoded using message passing methods. These
methods offer good performance but tend to converge slowly and sometimes fail
to converge and to decode the desired codewords correctly. Recently,
tree-reweighted message passing methods have been modified to improve the
convergence speed at little or no additional complexity cost. This paper
extends this line of work and proposes a new class of locally optimized
reweighting strategies, which are suitable for both regular and irregular LDPC
codes. The proposed decoding algorithm first splits the factor graph into
subgraphs and subsequently performs a local optimization of reweighting
parameters. Simulations show that the proposed decoding algorithm significantly
outperforms the standard message passing and existing reweighting techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0847</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0847</id><created>2014-03-04</created><updated>2014-03-23</updated><authors><author><keyname>Li</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Knowledge-Aided Reweighted Belief Propagation LDPC Decoding using
  Regular and Irregular Designs</title><categories>cs.IT math.IT</categories><comments>5 figures, 6 pages</comments><journal-ref>IET Communications, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new message passing algorithm, which takes advantage of both
tree-based re-parameterization and the knowledge of short cycles, is introduced
for the purpose of decoding LDPC codes with short block lengths. The proposed
algorithm is called variable factor appearance probability belief propagation
(VFAP-BP) algorithm and is suitable for wireless communications applications,
where both good decoding performance and low-latency are expected. Our
simulation results show that the VFAP-BP algorithm outperforms the standard BP
algorithm and requires a significantly smaller number of iterations than
existing algorithms when decoding both regular and irregular LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0850</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0850</id><created>2014-03-04</created><authors><author><keyname>Neglia</keyname><forenames>Giovanni</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Ye</keyname><forenames>Xiuhui</forenames><affiliation>DET</affiliation></author><author><keyname>Gabielkov</keyname><forenames>Maksym</forenames><affiliation>Inria Sophia Antipolis</affiliation></author><author><keyname>Legout</keyname><forenames>Arnaud</forenames><affiliation>Inria Sophia Antipolis</affiliation></author></authors><title>How to Network in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>NetSciCom 2014 - The Sixth IEEE International Workshop on Network
  Science for Communication Networks (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider how to maximize users' influence in Online Social
Networks (OSNs) by exploiting social relationships only. Our first contribution
is to extend to OSNs the model of Kempe et al. [1] on the propagation of
information in a social network and to show that a greedy algorithm is a good
approximation of the optimal algorithm that is NP-hard. However, the greedy
algorithm requires global knowledge, which is hardly practical. Our second
contribution is to show on simulations on the full Twitter social graph that
simple and practical strategies perform close to the greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0873</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0873</id><created>2014-03-04</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J</forenames></author><author><keyname>Theran</keyname><forenames>Louis</forenames></author></authors><title>Matroid Regression</title><categories>math.ST cs.DM cs.LG stat.ME stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algebraic combinatorial method for solving large sparse linear
systems of equations locally - that is, a method which can compute single
evaluations of the signal without computing the whole signal. The method scales
only in the sparsity of the system and not in its size, and allows to provide
error estimates for any solution method. At the heart of our approach is the
so-called regression matroid, a combinatorial object associated to sparsity
patterns, which allows to replace inversion of the large matrix with the
inversion of a kernel matrix that is constant size. We show that our method
provides the best linear unbiased estimator (BLUE) for this setting and the
minimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,
and furthermore we show that the size of the kernel matrix which is to be
inverted can be traded off with accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0875</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0875</id><created>2014-03-04</created><updated>2015-04-11</updated><authors><author><keyname>Guillermo</keyname><forenames>Mauricio</forenames></author><author><keyname>Miquey</keyname><forenames>&#xc9;tienne</forenames></author></authors><title>Classical realizability and arithmetical formul{\ae}</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1101.4364 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we treat the specification problem in classical realizability
(as defined in [20]) in the case of arithmetical formul{\ae}. In the continuity
of [10] and [11], we characterize the universal realizers of a formula as being
the winning strategies for a game (defined according to the formula). In the
first section we recall the definition of classical realizability, as well as a
few technical results. In Section 5, we introduce in more details the
specification problem and the intuition of the game-theoretic point of view we
adopt later. We first present a game $G_1$, that we prove to be adequate and
complete if the language contains no instructions &quot;quote&quot; [18], using
interaction constants to do substitution over execution threads. Then we show
that as soon as the language contain &quot;quote&quot;, the game is no more complete, and
present a second game ${G}_2$ that is both adequate and complete in the general
case. In the last Section, we draw attention to a model-theoretic point of
view, and use our specification result to show that arithmetical formul{\ae}
are absolute for realizability models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0879</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0879</id><created>2014-03-04</created><authors><author><keyname>Salazar</keyname><forenames>Harold Roberto Martinez</forenames></author><author><keyname>Carbajal</keyname><forenames>Juan Pablo</forenames></author><author><keyname>Ivanenko</keyname><forenames>Yuri P.</forenames></author></authors><title>Robustness: a new SLIP model based criterion for gait transitions in
  bipedal locomotion</title><categories>cs.RO</categories><comments>unpublished, in preparation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bipedal locomotion is a phenomenon that still eludes a fundamental and
concise mathematical understanding. Conceptual models that capture some
relevant aspects of the process exist but their full explanatory power is not
yet exhausted. In the current study, we introduce the robustness criterion
which defines the conditions for stable locomotion when steps are taken with
imprecise angle of attack. Intuitively, the necessity of a higher precision
indicates the difficulty to continue moving with a given gait. We show that the
spring-loaded inverted pendulum model, under the robustness criterion, is
consistent with previously reported findings on attentional demand during human
locomotion. This criterion allows transitions between running and walking, many
of which conserve forward speed. Simulations of transitions predict Froude
numbers below the ones observed in humans, nevertheless the model
satisfactorily reproduces several biomechanical indicators such as hip
excursion, gait duty factor and vertical ground reaction force profiles.
Furthermore, we identify reversible robust walk-run transitions, which allow
the system to execute a robust version of the hopping gait. These findings
foster the spring-loaded inverted pendulum model as the unifying framework for
the understanding of bipedal locomotion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0885</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0885</id><created>2014-03-04</created><updated>2014-07-09</updated><authors><author><keyname>Heilman</keyname><forenames>Steven</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Neeman</keyname><forenames>Joe</forenames></author></authors><title>Standard Simplices and Pluralities are Not the Most Noise Stable</title><categories>math.PR cs.CC math.FA</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Standard Simplex Conjecture and the Plurality is Stablest Conjecture are
two conjectures stating that certain partitions are optimal with respect to
Gaussian and discrete noise stability respectively. These two conjectures are
natural generalizations of the Gaussian noise stability result by Borell (1985)
and the Majority is Stablest Theorem (2004). Here we show that the standard
simplex is not the most stable partition in Gaussian space and that Plurality
is not the most stable low influence partition in discrete space for every
number of parts $k \geq 3$, for every value $\rho \neq 0$ of the noise and for
every prescribed measures for the different parts as long as they are not all
equal to $1/k$. Our results do not contradict the original statements of the
Plurality is Stablest and Standard Simplex Conjectures in their original
statements concerning partitions to sets of equal measure. However, they
indicate that if these conjectures are true, their veracity and their proofs
will crucially rely on assuming that the sets are of equal measures, in stark
contrast to Borell's result, the Majority is Stablest Theorem and many other
results in isoperimetric theory. Given our results it is natural to ask for
(conjectured) partitions achieving the optimum noise stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0917</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0917</id><created>2014-03-04</created><authors><author><keyname>Chakraborty</keyname><forenames>Anurag</forenames></author></authors><title>An Extension Of Weiler-Atherton Algorithm To Cope With The
  Self-intersecting Polygon</title><categories>cs.GR cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new algorithm has been proposed which can fix the problem of
Weiler Atherton algorithm. The problem of Weiler Atherton algorithm lies in
clipping self intersecting polygon. Clipping self intersecting polygon is not
considered in Weiler Atherton algorithm and hence it is also a main
disadvantage of this algorithm. In our new algorithm a self intersecting
polygon has been divided into non self intersecting contours and then perform
the Weiler Atherton clipping algorithm on those sub polygons. For holes we have
to store the edges that is not own boundary of hole contour from recently
clipped polygon. Thus if both contour is hole then we have to store all the
edges of the recently clipped polygon. Finally the resultant polygon has been
produced by eliminating all the stored edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0921</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0921</id><created>2014-03-04</created><authors><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Dynamic stochastic blockmodels for time-evolving social networks</title><categories>cs.SI cs.LG physics.soc-ph stat.ME</categories><comments>To appear in Journal of Selected Topics in Signal Processing special
  issue: Signal Processing for Social Networks</comments><acm-class>G.3; G.2.2</acm-class><journal-ref>IEEE Journal of Selected Topics in Signal Processing 8 (2014)
  552-562</journal-ref><doi>10.1109/JSTSP.2014.2310294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant efforts have gone into the development of statistical models for
analyzing data in the form of networks, such as social networks. Most existing
work has focused on modeling static networks, which represent either a single
time snapshot or an aggregate view over time. There has been recent interest in
statistical modeling of dynamic networks, which are observed at multiple points
in time and offer a richer representation of many complex phenomena. In this
paper, we present a state-space model for dynamic networks that extends the
well-known stochastic blockmodel for static networks to the dynamic setting. We
fit the model in a near-optimal manner using an extended Kalman filter (EKF)
augmented with a local search. We demonstrate that the EKF-based algorithm
performs competitively with a state-of-the-art algorithm based on Markov chain
Monte Carlo sampling but is significantly less computationally demanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0930</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0930</id><created>2014-03-04</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author><author><keyname>Tawfik</keyname><forenames>Hazim</forenames></author></authors><title>Spectrum Sensing Via Reconfigurable Antennas: Fundamental Limits and
  Potential Gains</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel paradigm for spectrum sensing in cognitive radio networks
that provides diversity and capacity benefits using a single antenna at the
Secondary User (SU) receiver. The proposed scheme is based on a reconfigurable
antenna: an antenna that is capable of altering its radiation characteristics
by changing its geometric configuration. Each configuration is designated as an
antenna mode or state and corresponds to a distinct channel realization. Based
on an abstract model for the reconfigurable antenna, we tackle two different
settings for the cognitive radio problem and present fundamental limits on the
achievable diversity and throughput gains. First, we explore the (to cooperate
or not to cooperate) tradeoff between the diversity and coding gains in
conventional cooperative and noncooperative spectrum sensing schemes, showing
that cooperation is not always beneficial. Based on this analysis, we propose
two sensing schemes based on reconfigurable antennas that we term as state
switching and state selection. It is shown that each of these schemes
outperform both cooperative and non-cooperative spectrum sensing under a global
energy constraint. Next, we study the (sensing-throughput) trade-off, and
demonstrate that using reconfigurable antennas, the optimal sensing time is
reduced allowing for a longer transmission time, and thus better throughput.
Moreover, state selection can be applied to boost the capacity of SU
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0949</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0949</id><created>2014-03-04</created><authors><author><keyname>Xin</keyname><forenames>Yufeng</forenames></author><author><keyname>Baldin</keyname><forenames>Ilya</forenames></author><author><keyname>Chase</keyname><forenames>Jeff</forenames></author><author><keyname>Ogan</keyname><forenames>Kemafor</forenames></author></authors><title>Leveraging Semantic Web Technologies for Managing Resources in a
  Multi-Domain Infrastructure-as-a-Service Environment</title><categories>cs.NI cs.DC</categories><report-no>RENCI TR-14-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on experience with using semantically-enabled network
resource models to construct an operational multi-domain networked
infrastructure-as-a-service (NIaaS) testbed called ExoGENI, recently funded
through NSF's GENI project. A defining property of NIaaS is the deep
integration of network provisioning functions alongside the more common storage
and computation provisioning functions. Resource provider topologies and user
requests can be described using network resource models with common base
classes for fundamental cyber-resources (links, nodes, interfaces) specialized
via virtualization and adaptations between networking layers to specific
technologies.
  This problem space gives rise to a number of application areas where semantic
web technologies become highly useful - common information models and resource
class hierarchies simplify resource descriptions from multiple providers,
pathfinding and topology embedding algorithms rely on query abstractions as
building blocks.
  The paper describes how the semantic resource description models enable
ExoGENI to autonomously instantiate on-demand virtual topologies of virtual
machines provisioned from cloud providers and are linked by on-demand virtual
connections acquired from multiple autonomous network providers to serve a
variety of applications ranging from distributed system experiments to
high-performance computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0950</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0950</id><created>2014-03-04</created><updated>2014-03-06</updated><authors><author><keyname>Margellos</keyname><forenames>Kostas</forenames></author><author><keyname>Prandini</keyname><forenames>Maria</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>On the connection between compression learning and scenario based
  optimization</title><categories>cs.SY</categories><comments>29 pages, shorter version submitted to the IEEE Transactions on
  Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connections between compression learning and scenario
based optimization. We first show how to strengthen, or relax the consistency
assumption at the basis of compression learning and study the learning and
generalization properties of the algorithm involved. We then consider different
constrained optimization problems affected by uncertainty represented by means
of scenarios. We show that the issue of providing guarantees on the probability
of constraint violation reduces to a learning problem for an appropriately
chosen algorithm that enjoys compression learning properties. The compression
learning perspective provides a unifying framework for scenario based
optimization and allows us to revisit the scenario approach and the
probabilistically robust design, a recently developed technique based on a
mixture of randomized and robust optimization, and to extend the guarantees on
the probability of constraint violation to cascading optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0952</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0952</id><created>2014-02-26</created><authors><author><keyname>Maler</keyname><forenames>Oded</forenames><affiliation>CNRS-VERIMAG</affiliation></author></authors><title>Algorithmic Verification of Continuous and Hybrid Systems</title><categories>cs.SY cs.FL cs.LO cs.NA</categories><comments>In Proceedings INFINITY 2013, arXiv:1402.6610</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 140, 2014, pp. 48-69</journal-ref><doi>10.4204/EPTCS.140.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a tutorial introduction to reachability computation, a class of
computational techniques that exports verification technology toward continuous
and hybrid systems. For open under-determined systems, this technique can
sometimes replace an infinite number of simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0957</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0957</id><created>2014-03-04</created><authors><author><keyname>Ashraphijuo</keyname><forenames>Mehdi</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>On the Symmetric K-user Interference Channels with Limited Feedback</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Th., Mar 2014. arXiv admin note:
  substantial text overlap with arXiv:1110.6487 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop achievability schemes for symmetric K-user
interference channels with a rate-limited feedback from each receiver to the
corresponding transmitter. We study this problem under two different channel
models: the linear deterministic model, and the Gaussian model. For the
deterministic model, the proposed scheme achieves a symmetric rate that is the
minimum of the symmetric capacity with infinite feedback, and the sum of the
symmetric capacity without feedback and the symmetric amount of feedback. For
the Gaussian interference channel, we use lattice codes to propose a
transmission strategy that incorporates the techniques of Han-Kobayashi message
splitting, interference decoding, and decode and forward. This strategy
achieves a symmetric rate which is within a constant number of bits to the
minimum of the upper bound on the symmetric capacity with infinite feedback,
and the sum of the upper bound on the symmetric capacity without feedback and
the amount of symmetric feedback. This constant is obtained as a function of
the number of users, K. The symmetric achievable rate is used to characterize
the achievable generalized degrees of freedom which exhibits a gradual increase
from no feedback to perfect feedback in the presence of feedback links with
limited capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0965</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0965</id><created>2014-03-04</created><updated>2014-03-12</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Fodor</keyname><forenames>Gabor</forenames></author><author><keyname>Athanasiou</keyname><forenames>Georgios</forenames></author></authors><title>Design Challenges of Millimeter Wave Communications: A MAC Layer
  Perspective</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper has been withdrawn by the authors due to a crucial error
  in the proposed protocol and also Fig. 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the spectrum is becoming more scarce due to exponential demand of
formidable data quantities, the new millimiterwave (mmW) band is considered as
an enabling player of 5G communications to provide multi-gigabits wireless
acccess. MmW communications exhibit high attenuation and blockage,
directionality due to massive beamforming, deafness, low-interference, and may
need micro waves networks for coordination and fallback support. The current
mmW standardizations are challenged by the overwhelming complexity given by
such heterogeneous communication systems and mmW band characteristics. This
demands new substantial protocol developments at all layers. In this paper, the
medium access control issues for mmW communications are reviewed. It is
discussed that while existing standards address some of these issues for
personal and local area networks, little has been done for cellular networks.
It is argued that the medium access control layer should be equipped with
adaptation mechanisms that are aware of the special mmW characteristics.
Recommendations for mmW medium access control design in 5G are provided. It is
concluded that the design of efficient access control techniques for mmW is in
its infancy and much work still has to be done.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0968</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0968</id><created>2014-03-04</created><authors><author><keyname>Medina</keyname><forenames>David S</forenames></author><author><keyname>St-Cyr</keyname><forenames>Amik</forenames></author><author><keyname>Warburton</keyname><forenames>T.</forenames></author></authors><title>OCCA: A unified approach to multi-threading languages</title><categories>cs.DC</categories><comments>25 pages, 6 figures, 9 code listings, 8 tables, Submitted to the SIAM
  Journal on Scientific Computing (SISC), presented at the Oil &amp; Gas Workshop
  2014 at Rice University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inability to predict lasting languages and architectures led us to
develop OCCA, a C++ library focused on host-device interaction. Using run-time
compilation and macro expansions, the result is a novel single kernel language
that expands to multiple threading languages. Currently, OCCA supports device
kernel expansions for the OpenMP, OpenCL, and CUDA platforms. Computational
results using finite difference, spectral element and discontinuous Galerkin
methods show OCCA delivers portable high performance in different architectures
and platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0974</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0974</id><created>2014-03-04</created><updated>2014-08-03</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Mestre</keyname><forenames>Juli&#xe1;n</forenames></author></authors><title>Parametrized Algorithms for Random Serial Dictatorship</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting and assignment are two of the most fundamental settings in social
choice theory. For both settings, random serial dictatorship (RSD) is a
well-known rule that satisfies anonymity, ex post efficiency, and
strategyproofness. Recently, it was shown that computing the resulting
probabilities is #P-complete both in the voting and assignment setting. In this
paper, we study RSD from a parametrized complexity perspective. More
specifically, we present efficient algorithms to compute the RSD probabilities
under the condition that the number of agent types, alternatives, or objects is
bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0982</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0982</id><created>2014-03-04</created><authors><author><keyname>Shirazipourazad</keyname><forenames>Shahrzad</forenames></author><author><keyname>Ghosh</keyname><forenames>Pavel</forenames></author><author><keyname>Sen</keyname><forenames>Arunabha</forenames></author></authors><title>On Connectivity of Airborne Networks</title><categories>cs.NI</categories><comments>The section on Computation of Critical Transmission Range In Faculty
  Scenario has been published in proceedings of Milcom 2011: S.
  Shirazipourazad, P. Ghosh and A. Sen, On Connectivity of Airborne Networks in
  Presence of Region-based Faults, IEEE Milcom, Military Communications
  Conference, Baltimore, MD, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility pattern of nodes in a mobile network has significant impact on the
connectivity properties of the network. One such mobile network that has drawn
attention of researchers in the past few years is the Airborne Networks (AN)
due to its importance in civil and military purpose and due to the several
complex issues in these domains. Since the nodes in an airborne network (AN)
are heterogeneous and mobile, the design of a reliable and robust AN is highly
complex and challenging. In this paper a persistent backbone based architecture
for an AN has been considered where a set of airborne networking platforms
(ANPs - aircrafts, UAVs and satellites) form the backbone of the AN. End to end
connectivity of the backbone nodes is crucial in providing the communication
among the hosts. Since ANPs are prone to failure because of attacks like EMP
attack or jamming, another important issue is to improve the robustness of the
backbone network against these attacks. Such attacks will impact specific
geographic regions at specific times and if an ANP is within the fault region
during the time of attack, it will fail. This paper focuses on connectivity and
fault-tolerance issues in ANs and studies algorithms to compute the minimum
transmission range of ANPs in fault free and faulty scenarios to ensure network
connectivity all the times. It also considers the scenario where the network
may have to operate in a disconnected mode for some part of time and data
transmissions may be tolerant to some amount of delay. Hence, ANPs may not need
to have end-to-end paths all the time but they should be able to transmit data
to each other within bounded time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.0989</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.0989</id><created>2014-03-04</created><updated>2014-11-14</updated><authors><author><keyname>Peel</keyname><forenames>Leto</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Detecting change points in the large-scale structure of evolving
  networks</title><categories>cs.SI physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions among people or objects are often dynamic in nature and can be
represented as a sequence of networks, each providing a snapshot of the
interactions over a brief period of time. An important task in analyzing such
evolving networks is change-point detection, in which we both identify the
times at which the large-scale pattern of interactions changes fundamentally
and quantify how large and what kind of change occurred. Here, we formalize for
the first time the network change-point detection problem within an online
probabilistic learning framework and introduce a method that can reliably solve
it. This method combines a generalized hierarchical random graph model with a
Bayesian hypothesis test to quantitatively determine if, when, and precisely
how a change point has occurred. We analyze the detectability of our method
using synthetic data with known change points of different types and
magnitudes, and show that this method is more accurate than several previously
used alternatives. Applied to two high-resolution evolving social networks,
this method identifies a sequence of change points that align with known
external &quot;shocks&quot; to these networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1005</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1005</id><created>2014-03-04</created><updated>2014-04-30</updated><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinrich</forenames></author><author><keyname>Peake</keyname><forenames>Ian</forenames></author></authors><title>From abstract modelling to remote cyber-physical
  integration/interoperability testing</title><categories>cs.SE</categories><comments>In Improving Systems and Software Engineering Conference (iSSEC) 2013
  incorporating SEPG(SM) Asia-Pacific Conference, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An appropriate system model gives developers a better overview, and the
ability to fix more inconsistencies more effectively and earlier in system
development, reducing overall effort and cost. However, modelling assumes
abstraction of several aspects of the system and its environment, and this
abstraction should be not overlooked, but properly taken into account during
later development phases. This is especially especially important for the cases
of remote integration, testing/verification, and manufacturing of
cyber-physical systems. For this reason we introduce a development methodology
for cyber-physical systems (CPS) with a focus on the abstraction levels of the
system representation, based on the idea of refinement-based development of
complex, interactive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1006</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1006</id><created>2014-03-05</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author></authors><title>Towards Focus on Time</title><categories>cs.FL cs.LO</categories><comments>12th International Workshop on Automated Verification of Critical
  Systems (AVoCS'12), 2012</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper introduces a model for the specification and verification of
real-time system design: timed state transition diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1013</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1013</id><created>2014-03-05</created><updated>2015-10-11</updated><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Adversary's Ignorance of Transmission Time Increases Covert Throughput</title><categories>cs.IT math.IT</categories><comments>v2: updated references/discussion of steganography, no change in
  results; v3: significant update, includes new theorem 1.2</comments><report-no>UM-CS-2014-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent square root law (SRL) for covert communication demonstrates that
Alice can reliably transmit $\mathcal{O}(\sqrt{n})$ bits to Bob in $n$ uses of
an additive white Gaussian noise (AWGN) channel while keeping ineffective any
detector employed by the adversary; conversely, exceeding this limit either
results in detection by the adversary with high probability or non-zero
decoding error probability at Bob. This SRL is under the assumption that the
adversary knows when Alice transmits (if she transmits); however, in many
operational scenarios he does not know this. Hence, here we study the impact of
the adversary's ignorance of the time of the communication attempt. We employ a
slotted AWGN channel model with $T(n)$ slots each containing $n$ symbol
periods, where Alice may use a single slot out of $T(n)$. Provided that Alice's
slot selection is secret, the adversary needs to monitor all $T(n)$ slots for
possible transmission. We show that this allows Alice to reliably transmit
$\mathcal{O}(\min\{\sqrt{n\log T(n)},n\})$ bits to Bob (but no more) while
keeping the adversary's detector ineffective. To achieve this gain over SRL,
Bob does not have to know the time of transmission provided $T(n)&lt;2^{c_{\rm
T}n}$, $c_{\rm T}=\mathcal{O}(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1019</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1019</id><created>2014-03-05</created><updated>2014-03-10</updated><authors><author><keyname>Shi</keyname><forenames>Zheng</forenames></author><author><keyname>Koh</keyname><forenames>Khee Meng</forenames></author></authors><title>Counting the Number of Minimum Roman Dominating Functions of a Graph</title><categories>cs.DS cs.DM</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide two algorithms counting the number of minimum Roman dominating
functions of a graph on n vertices in O(1.5673^n) time and polynomial space. We
also show that the time complexity can be reduced to O(1.5014^n) if exponential
space is used. Our result is obtained by transforming the Roman domination
problem into other combinatorial problems on graphs for which exact algorithms
already exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1023</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1023</id><created>2014-03-05</created><updated>2014-10-08</updated><authors><author><keyname>Cohen</keyname><forenames>Kobi</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author></authors><title>Active Hypothesis Testing for Quickest Anomaly Detection</title><categories>cs.IT math.IT</categories><comments>46 pages, 3 figures, part of this work was presented at the
  Information Theory and Applications (ITA) Workshop, Feb. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest detection of an anomalous process among M processes
is considered. At each time, a subset of the processes can be observed, and the
observations from each chosen process follow two different distributions,
depending on whether the process is normal or abnormal. The objective is a
sequential search strategy that minimizes the expected detection time subject
to an error probability constraint. This problem can be considered as a special
case of active hypothesis testing first considered by Chernoff in 1959 where a
randomized strategy, referred to as the Chernoff test, was proposed and shown
to be asymptotically (as the error probability approaches zero) optimal. For
the special case considered in this paper, we show that a simple deterministic
test achieves asymptotic optimality and offers better performance in the finite
regime. We further extend the problem to the case where multiple anomalous
processes are present. In particular, we examine the case where only an upper
bound on the number of anomalous processes is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1024</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1024</id><created>2014-03-05</created><updated>2014-05-15</updated><authors><author><keyname>Song</keyname><forenames>Hyun Oh</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Mairal</keyname><forenames>Julien</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>On learning to localize objects with minimal supervision</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to localize objects with minimal supervision is an important problem
in computer vision, since large fully annotated datasets are extremely costly
to obtain. In this paper, we propose a new method that achieves this goal with
only image-level labels of whether the objects are present or not. Our approach
combines a discriminative submodular cover problem for automatically
discovering a set of positive object windows with a smoothed latent SVM
formulation. The latter allows us to leverage efficient quasi-Newton
optimization techniques. Our experiments demonstrate that the proposed approach
provides a 50% relative improvement in mean average precision over the current
state-of-the-art on PASCAL VOC 2007 detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1048</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1048</id><created>2014-03-05</created><authors><author><keyname>Kim</keyname><forenames>Young Jin</forenames></author><author><keyname>Roh</keyname><forenames>Myungkyoon</forenames></author><author><keyname>Son</keyname><forenames>Seung-Woo</forenames></author></authors><title>Network Structures between Strategies in Iterated Prisoners' Dilemma
  Games</title><categories>physics.soc-ph cs.GT</categories><journal-ref>The Korean Physical Society February 2014, Volume 64, Issue 3, pp
  341-345</journal-ref><doi>10.3938/jkps.64.341</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use replicator dynamics to study an iterated prisoners' dilemma game with
memory. In this study, we investigate the characteristics of all 32 possible
strategies with a single-step memory by observing the results when each
strategy encounters another one. Based on these results, we define similarity
measures between the 32 strategies and perform a network analysis of the
relationship between the strategies by constructing a strategies network.
Interestingly, we find that a win-lose circulation, like rock-paper-scissors,
exists between strategies and that the circulation results from one unusual
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1056</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1056</id><created>2014-03-05</created><authors><author><keyname>Sanin</keyname><forenames>Andres</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian
  Detection</title><categories>cs.CV</categories><comments>IEEE International Conference on Image Processing (ICIP), 2012</comments><acm-class>I.4.7; I.4.10; I.5.1; I.5.4</acm-class><doi>10.1109/ICIP.2012.6466899</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For covariance-based image descriptors, taking into account the curvature of
the corresponding feature space has been shown to improve discrimination
performance. This is often done through representing the descriptors as points
on Riemannian manifolds, with the discrimination accomplished on a tangent
space. However, such treatment is restrictive as distances between arbitrary
points on the tangent space do not represent true geodesic distances, and hence
do not represent the manifold structure accurately. In this paper we propose a
general discriminative model based on the combination of several tangent
spaces, in order to preserve more details of the structure. The model can be
used as a weak learner in a boosting-based pedestrian detection framework.
Experiments on the challenging INRIA and DaimlerChrysler datasets show that the
proposed model leads to considerably higher performance than methods based on
histograms of oriented gradients as well as previous Riemannian-based
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1061</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1061</id><created>2014-03-05</created><authors><author><keyname>Shlezinger</keyname><forenames>Nir</forenames></author><author><keyname>Dabora</keyname><forenames>Ron</forenames></author></authors><title>Frequency-Shift Filtering for OFDM Signal Recovery in Narrowband Power
  Line Communications</title><categories>cs.OH</categories><comments>Accepted to the ieee transactions on communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power line communications (PLC) has been drawing considerable interest in
recent years due to the growing interest in smart grid implementation.
Specifically, network control and grid applications are allocated the frequency
band of 0-500 kHz, commonly referred to as the narrowband PLC channel. This
frequency band is characterized by strong periodic noise which results in low
signal to noise ratio (SNR). In this work we propose a receiver which uses
frequency shift filtering to exploit the cyclostationary properties of both the
narrowband power line noise, as well as the information signal, digitally
modulated using orthogonal frequency division multiplexing. An adaptive
implementation for the proposed receiver is presented as well. The proposed
receiver is compared to existing receivers via analysis and simulation. The
results show that the receiver proposed in this work obtains a substantial
performance gain over previously proposed receivers, without requiring any
coordination with the transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1065</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1065</id><created>2014-03-05</created><updated>2014-06-05</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>Cording</keyname><forenames>Patrick Hagge</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author></authors><title>Compressed Subsequence Matching and Packed Tree Coloring</title><categories>cs.DS</categories><comments>To appear at CPM '14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for subsequence matching in grammar compressed
strings. Given a grammar of size $n$ compressing a string of size $N$ and a
pattern string of size $m$ over an alphabet of size $\sigma$, our algorithm
uses $O(n+\frac{n\sigma}{w})$ space and $O(n+\frac{n\sigma}{w}+m\log N\log
w\cdot occ)$ or $O(n+\frac{n\sigma}{w}\log w+m\log N\cdot occ)$ time. Here $w$
is the word size and $occ$ is the number of occurrences of the pattern. Our
algorithm uses less space than previous algorithms and is also faster for
$occ=o(\frac{n}{\log N})$ occurrences. The algorithm uses a new data structure
that allows us to efficiently find the next occurrence of a given character
after a given position in a compressed string. This data structure in turn is
based on a new data structure for the tree color problem, where the node colors
are packed in bit strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1070</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1070</id><created>2014-03-05</created><updated>2016-02-16</updated><authors><author><keyname>Walk</keyname><forenames>Simon</forenames></author><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author><author><keyname>Noy</keyname><forenames>Natalya F.</forenames></author><author><keyname>Musen</keyname><forenames>Mark</forenames></author></authors><title>How to Apply Markov Chains for Modeling Sequential Edit Patterns in
  Collaborative Ontology-Engineering Projects</title><categories>cs.HC cs.SI</categories><doi>10.1016/j.ijhcs.2015.07.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing popularity of large-scale collaborative ontology-engineering
projects, such as the creation of the 11th revision of the International
Classification of Diseases, we need new methods and insights to help project-
and community-managers to cope with the constantly growing complexity of such
projects. In this paper, we present a novel application of Markov chains to
model sequential usage patterns that can be found in the change-logs of
collaborative ontology-engineering projects. We provide a detailed presentation
of the analysis process, describing all the required steps that are necessary
to apply and determine the best fitting Markov chain model. Amongst others, the
model and results allow us to identify structural properties and regularities
as well as predict future actions based on usage sequences. We are specifically
interested in determining the appropriate Markov chain orders which postulate
on how many previous actions future ones depend on. To demonstrate the
practical usefulness of the extracted Markov chains we conduct sequential
pattern analyses on a large-scale collaborative ontology-engineering dataset,
the International Classification of Diseases in its 11th revision. To further
expand on the usefulness of the presented analysis, we show that the collected
sequential patterns provide potentially actionable information for
user-interface designers, ontology-engineering tool developers and
project-managers to monitor, coordinate and dynamically adapt to the natural
development processes that occur when collaboratively engineering an ontology.
We hope that presented work will spur a new line of ontology-development tools,
evaluation-techniques and new insights, further taking the interactive nature
of the collaborative ontology-engineering process into consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1073</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1073</id><created>2014-03-05</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Artificial Neuron Modelling Based on Wave Shape</title><categories>cs.NE</categories><comments>Published</comments><journal-ref>BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
  Volume 4, Issues 1-4, October 2013, pp. 20 - 25, ISSN 2067-3957 (online),
  ISSN 2068 - 0473 (print)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new model for an artificial neural network processing
unit or neuron. It is slightly different to a traditional feedforward network
by the fact that it favours a mechanism of trying to match the wave-like
'shape' of the input with the shape of the output against specific value error
corrections. The expectation is then that a best fit shape can be transposed
into the desired output values more easily. This allows for notions of
reinforcement through resonance and also the construction of synapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1076</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1076</id><created>2014-03-05</created><updated>2015-06-29</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Is Intelligence Artificial?</title><categories>cs.AI</categories><comments>This new version adds some clarity to the discussion, which has
  realised a new idea. Also the opportunity to extend or update some sections.
  Some new references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our understanding of intelligence is directed primarily at the level of human
beings. This paper attempts to give a more unifying definition that can be
applied to the natural world in general. The definition would be used more to
verify a degree of intelligence, not to quantify it and might help when making
judgements on the matter. A version of an accepted test for AI is then put
forward as the 'acid test' for Artificial Intelligence itself. It might be what
a free-thinking program or robot would try to achieve. Recent work by the
author on AI has been more from a direction of mechanical processes, or ones
that might operate automatically. This paper will not try to question the idea
of intelligence, in the sense of a pro-active or conscious event, but try to
put it into a more passive, automatic and mechanical context. The paper also
suggests looking at intelligence and consciousness as being slightly different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1078</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1078</id><created>2014-03-05</created><authors><author><keyname>Li</keyname><forenames>Yongli</forenames></author><author><keyname>Pin</keyname><forenames>Paolo</forenames></author><author><keyname>Wu</keyname><forenames>Chong</forenames></author></authors><title>A network centrality method for the rating problem</title><categories>physics.soc-ph cs.SI</categories><comments>25 pages, 8 figures</comments><doi>10.1371/journal.pone.0120247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for aggregating the information of multiple reviewers
rating multiple products. Our approach is based on the network relations
induced between products by the rating activity of the reviewers. We show that
our method is algorithmically implementable even for large numbers of both
products and consumers, as is the case for many online sites. Moreover,
comparing it with the simple average, which is mostly used in practice, and
with other methods previously proposed in the literature, it performs very well
under various dimension, proving itself to be an optimal trade--off between
computational efficiency, accordance with the reviewers original orderings, and
robustness with respect to the inclusion of systematically biased reports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1080</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1080</id><created>2014-03-05</created><updated>2014-07-05</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>New Ideas for Brain Modelling</title><categories>cs.AI q-bio.NC</categories><comments>Pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes some biologically-inspired processes that could be used
to build the sort of networks that we associate with the human brain. New to
this paper, a 'refined' neuron will be proposed. This is a group of neurons
that by joining together can produce a more analogue system, but with the same
level of control and reliability that a binary neuron would have. With this new
structure, it will be possible to think of an essentially binary system in
terms of a more variable set of values. The paper also shows how recent
research can be combined with established theories, to produce a more complete
picture. The propositions are largely in line with conventional thinking, but
possibly with one or two more radical suggestions. An earlier cognitive model
can be filled in with more specific details, based on the new research results,
where the components appear to fit together almost seamlessly. The intention of
the research has been to describe plausible 'mechanical' processes that can
produce the appropriate brain structures and mechanisms, but that could be used
without the magical 'intelligence' part that is still not fully understood.
There are also some important updates from an earlier version of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1081</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1081</id><created>2014-03-05</created><updated>2015-08-21</updated><authors><author><keyname>Adler</keyname><forenames>Isolde</forenames></author><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author></authors><title>Linear rank-width of distance-hereditary graphs I. A polynomial-time
  algorithm</title><categories>math.CO cs.DM cs.DS</categories><comments>28 pages, 3 figures, 2 table. A preliminary version appeared in the
  proceedings of WG'14</comments><msc-class>05C85</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear rank-width is a linearized variation of rank-width, and it is deeply
related to matroid path-width. In this paper, we show that the linear
rank-width of every $n$-vertex distance-hereditary graph, equivalently a graph
of rank-width at most $1$, can be computed in time $\mathcal{O}(n^2\cdot \log_2
n)$, and a linear layout witnessing the linear rank-width can be computed with
the same time complexity. As a corollary, we show that the path-width of every
$n$-element matroid of branch-width at most $2$ can be computed in time
$\mathcal{O}(n^2\cdot \log_2 n)$, provided that the matroid is given by an
independent set oracle.
  To establish this result, we present a characterization of the linear
rank-width of distance-hereditary graphs in terms of their canonical split
decompositions. This characterization is similar to the known characterization
of the path-width of forests given by Ellis, Sudborough, and Turner [The vertex
separation and search number of a graph. Inf. Comput., 113(1):50--79, 1994].
However, different from forests, it is non-trivial to relate substructures of
the canonical split decomposition of a graph with some substructures of the
given graph. We introduce a notion of `limbs' of canonical split
decompositions, which correspond to certain vertex-minors of the original
graph, for the right characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1091</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1091</id><created>2014-03-05</created><updated>2014-05-15</updated><authors><author><keyname>Selva</keyname><forenames>J.</forenames></author></authors><title>Signal Estimation from Nonuniform Samples with RMS Error Bound --
  Application to OFDM Channel Estimation</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a channel spectral estimator for OFDM signals containing pilot
carriers, assuming a known delay spread or a bound on this parameter. The
estimator is based on modeling the channel's spectrum as a band-limited
function, instead of as the discrete Fourier transform of a tapped delay line
(TDL). Its main advantage is its immunity to the truncation mismatch in usual
TDL models (Gibbs phenomenon). In order to assess the estimator, we compare it
with the well-known TDL maximum likelihood (ML) estimator in terms of
root-mean-square (RMS) error. The main result is that the proposed estimator
improves on the ML estimator significantly, whenever the average spectral
sampling rate is above the channel's delay spread. The improvement increases
with the spectral oversampling ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1097</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1097</id><created>2014-03-05</created><authors><author><keyname>Rahaman</keyname><forenames>Ramij</forenames></author><author><keyname>Parker</keyname><forenames>Matthew G.</forenames></author></authors><title>Quantum secret sharing based on local distinguishability</title><categories>quant-ph cs.CR math-ph math.MP</categories><comments>7 pages, Comments are welcome</comments><journal-ref>Phys. Rev. A 91, 022330 (2015)</journal-ref><doi>10.1103/PhysRevA.91.022330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the (im)possibility of the exact distinguishability
of orthogonal multipartite entangled states under {\em restricted local
operation and classical communication}. Based on this local distinguishability
analysis we propose a new scheme for quantum secret sharing (QSS). Our QSS
scheme is quite general and cost efficient compared to other schemes. In our
scheme no joint quantum operation is needed to reconstruct the secret. We also
present an interesting $(2,n)$-threshold QSS scheme, where any two cooperating
players, one from each of two disjoint groups of players, can always
reconstruct the secret. This QSS scheme is quite uncommon, as most
$(k,n)$-threshold schemes have the restriction $k\geq\lceil\frac{n}{2}\rceil$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1104</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1104</id><created>2014-03-05</created><authors><author><keyname>Pigott</keyname><forenames>Fiona</forenames></author><author><keyname>Marin</keyname><forenames>Mauricio Rene Herrera</forenames></author></authors><title>Proposal for a Correction to the Temporal Correlation Coefficient
  Calculation for Temporal Networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures, work done at Universidad del Desarrollo as a part
  of the project: Understanding the role of social contagion and its actors in
  the diffusion of innovations through weighted and time-varying networks</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the topological overlap of two graphs becomes important when
assessing the changes between temporally adjacent graphs in a time-evolving
network. Current methods depend on the fraction of nodes that have persisting
edges. This breaks down when there are nodes with no edges, persisting or
otherwise. The following outlines a proposed correction to ensure that
correlation metrics have the expected behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1112</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1112</id><created>2014-03-05</created><authors><author><keyname>Tax</keyname><forenames>Niek</forenames></author></authors><title>Analyzing a practitioner perspective on relevance of published empirical
  research in Requirements Engineering</title><categories>cs.SE</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Relevance to industry and scientific rigor have long been an area
of friction in IS research. However little work has been done on how to
evaluate IS research relevance. Kitchenham et al [13] proposed one of the few
relevance evaluating instruments in literature, later revised by Daneva et al
[7]. Aim: To analyze the practitioner/consultant perspective checklist1 for
relevance in order to evaluate its comprehensibility and applicability from the
point of view of the practitioner/consultant in the context of an advanced
university classroom. Method: Five master level students in the field of IS
assessed a set of 24 papers using the relevance checklist1. For each question
in the checklist, inter-rater agreement has been calculated and the reasoning
that the practitioners applied has been reconstructed from comments. Results:
Inter-rater agreement only showed to be slight for three questions and poor for
all other questions. Analysis of comments provided by the practitioners showed
only two questions that were interpreted in the same way by all practitioners.
These two questions showed significantly higher inter-rater agreement than
other questions. Conclusions: The generally low inter-rater agreement could be
explained as an indication that the checklist1 is in its current form not
appropriate for measuring industry relevance of IS research. The different
interpretations found for the checklist questions provide useful insight for
reformulation of questions. Reformulations are proposed for some questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1117</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1117</id><created>2014-03-05</created><updated>2014-04-25</updated><authors><author><keyname>Pei</keyname><forenames>Yu</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author><author><keyname>Nordio</keyname><forenames>Martin</forenames></author><author><keyname>Wei</keyname><forenames>Yi</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author><author><keyname>Zeller</keyname><forenames>Andreas</forenames></author></authors><title>Automated Fixing of Programs with Contracts</title><categories>cs.SE</categories><comments>Minor changes after proofreading</comments><journal-ref>IEEE Transactions on Software Engineering, 40(5):427-449. IEEE
  Computer Society, May 2014</journal-ref><doi>10.1109/TSE.2014.2312918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes AutoFix, an automatic debugging technique that can fix
faults in general-purpose software. To provide high-quality fix suggestions and
to enable automation of the whole debugging process, AutoFix relies on the
presence of simple specification elements in the form of contracts (such as
pre- and postconditions). Using contracts enhances the precision of dynamic
analysis techniques for fault detection and localization, and for validating
fixes. The only required user input to the AutoFix supporting tool is then a
faulty program annotated with contracts; the tool produces a collection of
validated fixes for the fault ranked according to an estimate of their
suitability.
  In an extensive experimental evaluation, we applied AutoFix to over 200
faults in four code bases of different maturity and quality (of implementation
and of contracts). AutoFix successfully fixed 42% of the faults, producing, in
the majority of cases, corrections of quality comparable to those competent
programmers would write; the used computational resources were modest, with an
average time per fix below 20 minutes on commodity hardware. These figures
compare favorably to the state of the art in automated program fixing, and
demonstrate that the AutoFix approach is successfully applicable to reduce the
debugging burden in real-world scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1120</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1120</id><created>2014-03-05</created><authors><author><keyname>Gonz&#xe1;lez-Valiente</keyname><forenames>C. L.</forenames></author><author><keyname>S&#xe1;nchez-Rodr&#xed;guez</keyname><forenames>Y.</forenames></author><author><keyname>Lezcano-P&#xe9;rez</keyname><forenames>Y.</forenames></author></authors><title>Estudio exploratorio sobre las competencias informacionales de los
  estudiantes de la Universidad de La Habana</title><categories>cs.CY</categories><comments>http://www.redalyc.org/pdf/1814/181423798009.pdf</comments><journal-ref>Ciencias de la Informaci\'on; 2012, 43 (2): 61-68</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The present article shows the results of a survey aimed at identifying the
informational abilities of Havana University students. Several methods such as
the survey, expert's interviews and content and document analysis are used. The
questionnaire has been structured base on three basic variables: information
search, information analysis and release and self evaluation elements. The
identification of these abilities was a key element for guiding libraries in
the development of actions focused on their communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1124</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1124</id><created>2014-03-05</created><updated>2014-07-02</updated><authors><author><keyname>Karvanen</keyname><forenames>Juha</forenames></author></authors><title>Estimating complex causal effects from incomplete observational data</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the major advances taken in causal modeling, causality is still an
unfamiliar topic for many statisticians. In this paper, it is demonstrated from
the beginning to the end how causal effects can be estimated from observational
data assuming that the causal structure is known. To make the problem more
challenging, the causal effects are highly nonlinear and the data are missing
at random. The tools used in the estimation include causal models with design,
causal calculus, multiple imputation and generalized additive models. The main
message is that a trained statistician can estimate causal effects by
judiciously combining existing tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1131</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1131</id><created>2014-03-05</created><updated>2014-04-22</updated><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Angel</forenames></author></authors><title>Evolutionary network games: equilibria from imitation and best-response
  dynamics</title><categories>physics.soc-ph cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider games of strategic substitutes and strategic complements on
networks. We introduce two different evolutionary dynamics in order to refine
their multiplicity of equilibria, and we analyse the system through a mean
field approach. We find that for the best-shot game, taken as a model for
substitutes, a replicator-like dynamics does not lead to Nash equilibria,
whereas it leads to unique equilibria (full cooperation or full defection,
depending on the initial condition and the game parameter) for complements,
represented by a coordination game. On the other hand, when the dynamics
becomes more cognitively demanding in the form of a best response evolution,
predictions are always Nash equilibria (at least when individuals are fully
rational): For the best-shot game we find equilibria with a definite value of
the fraction of contributors, whereas for the coordination game symmetric
equilibria arise only for low or high initial fractions of cooperators. We also
extend our study by considering complex heterogeneous topologies, and show that
the nature of the selected equilibria does not change for the best-shot game.
However for coordination games we reveal an important difference, namely that
on infinitely large scale-free networks cooperation arises for any value of the
incentive to cooperate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1140</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1140</id><created>2014-03-05</created><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author></authors><title>Matrix Methods for Solving Algebraic Systems</title><categories>cs.MS cs.SC</categories><comments>13 pages. arXiv admin note: text overlap with arXiv:1201.5810</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our public-domain software for the following tasks in sparse (or
toric) elimination theory, given a well-constrained polynomial system. First, C
code for computing the mixed volume of the system. Second, Maple code for
defining an overconstrained system and constructing a Sylvester-type matrix of
its sparse resultant. Third, C code for a Sylvester-type matrix of the sparse
resultant and a superset of all common roots of the initial well-constrained
system by computing the eigen-decomposition of a square matrix obtained from
the resultant matrix. We conclude with experiments in computing molecular
conformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1142</identifier>
 <datestamp>2014-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1142</id><created>2014-03-05</created><updated>2014-05-12</updated><authors><author><keyname>Kremer</keyname><forenames>Steve</forenames><affiliation>INRIA Nancy - Grand Est / LORIA / LIFC</affiliation></author><author><keyname>Robert</keyname><forenames>K&#xfc;nnemann</forenames></author></authors><title>Automated analysis of security protocols with global state</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security APIs, key servers and protocols that need to keep the status of
transactions, require to maintain a global, non-monotonic state, e.g., in the
form of a database or register. However, most existing automated verification
tools do not support the analysis of such stateful security protocols -
sometimes because of fundamental reasons, such as the encoding of the protocol
as Horn clauses, which are inherently monotonic. A notable exception is the
recent tamarin prover which allows specifying protocols as multiset rewrite
(msr) rules, a formalism expressive enough to encode state. As multiset
rewriting is a &quot;low-level&quot; specification language with no direct support for
concurrent message passing, encoding protocols correctly is a difficult and
error-prone process. We propose a process calculus which is a variant of the
applied pi calculus with constructs for manipulation of a global state by
processes running in parallel. We show that this language can be translated to
msr rules whilst preserving all security properties expressible in a dedicated
first-order logic for security properties. The translation has been implemented
in a prototype tool which uses the tamarin prover as a backend. We apply the
tool to several case studies among which a simplified fragment of PKCS\#11, the
Yubikey security token, and an optimistic contract signing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1165</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1165</id><created>2014-03-05</created><authors><author><keyname>Hanspach</keyname><forenames>Michael</forenames></author><author><keyname>Keller</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>A Taxonomy for Attack Patterns on Information Flows in Component-Based
  Operating Systems</title><categories>cs.CR cs.OS cs.SD</categories><comments>9 pages</comments><journal-ref>In Proceedings of the 7th Layered Assurance Workshop, New Orleans,
  LA, USA, December 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a taxonomy and an algebra for attack patterns on component-based
operating systems. In a multilevel security scenario, where isolation of
partitions containing data at different security classifications is the primary
security goal and security breaches are mainly defined as undesired disclosure
or modification of classified data, strict control of information flows is the
ultimate goal. In order to prevent undesired information flows, we provide a
classification of information flow types in a component-based operating system
and, by this, possible patterns to attack the system. The systematic
consideration of informations flows reveals a specific type of operating system
covert channel, the covert physical channel, which connects two former isolated
partitions by emitting physical signals into the computer's environment and
receiving them at another interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1168</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1168</id><created>2014-03-05</created><updated>2015-12-18</updated><authors><author><keyname>Santani</keyname><forenames>Darshan</forenames></author><author><keyname>Gatica-Perez</keyname><forenames>Daniel</forenames></author></authors><title>Loud and Trendy: Crowdsourcing Impressions of Social Ambiance in Popular
  Indoor Urban Places</title><categories>cs.SI physics.soc-ph</categories><doi>10.1145/2733373.2806277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New research cutting across architecture, urban studies, and psychology is
contextualizing the understanding of urban spaces according to the perceptions
of their inhabitants. One fundamental construct that relates place and
experience is ambiance, which is defined as &quot;the mood or feeling associated
with a particular place&quot;. We posit that the systematic study of ambiance
dimensions in cities is a new domain for which multimedia research can make
pivotal contributions. We present a study to examine how images collected from
social media can be used for the crowdsourced characterization of indoor
ambiance impressions in popular urban places. We design a crowdsourcing
framework to understand suitability of social images as data source to convey
place ambiance, to examine what type of images are most suitable to describe
ambiance, and to assess how people perceive places socially from the
perspective of ambiance along 13 dimensions. Our study is based on 50,000
Foursquare images collected from 300 popular places across six cities
worldwide. The results show that reliable estimates of ambiance can be obtained
for several of the dimensions. Furthermore, we found that most aggregate
impressions of ambiance are similar across popular places in all studied
cities. We conclude by presenting a multidisciplinary research agenda for
future research in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1169</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1169</id><created>2014-03-04</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>A proof challenge: multiple alignment and information compression</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These notes pose a &quot;proof challenge&quot;: a proof, or disproof, of the
proposition that &quot;For any given body of information, I, expressed as a
one-dimensional sequence of atomic symbols, a multiple alignment concept,
described in the document, provides a means of encoding all the redundancy that
may exist in I. Aspects of the challenge are described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1170</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1170</id><created>2014-03-04</created><authors><author><keyname>Wang</keyname><forenames>Zhenghuan</forenames></author><author><keyname>Liu</keyname><forenames>Heng</forenames></author><author><keyname>Xu</keyname><forenames>Shengxin</forenames></author><author><keyname>Bu</keyname><forenames>Xiangyuan</forenames></author><author><keyname>An</keyname><forenames>Jianping</forenames></author></authors><title>Multichannel RSS-based Device-Free Localization with Wireless Sensor
  Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RSS-based device-free localization (DFL) is a very promising technique which
allows localizing the target without attaching any electronic tags in wireless
environments. In cluttered indoor environments, the performance of DFL degrades
due to multipath interference. In this paper, we propose a multichannel
obstructed link detection method based on the RSS variation on difference
channels. Multichannel detection is proved to be very effective in multipath
environments compared to the single channel detection. We also propose a new
localization method termed as robust weighted least square (RWLS) method. RWLS
first use spatial property to eliminate the interference links and then employ
WLS method to localize the target. Since the spatial detection relies on the
unknown position of the target. A coarse position estimation of target is also
presented. RWLS is robust to interference links and has low computation
complexity. Results from real experiments verify the effectiveness of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1177</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1177</id><created>2014-03-05</created><updated>2014-06-27</updated><authors><author><keyname>Backlund</keyname><forenames>Ville-Pekka</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author></authors><title>Effects of temporal correlations on cascades: Threshold models on
  temporal networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>9 pages, 7 figures, Published version</comments><journal-ref>Phys. Rev. E 89, 062815 (2014)</journal-ref><doi>10.1103/PhysRevE.89.062815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A person's decision to adopt an idea or product is often driven by the
decisions of peers, mediated through a network of social ties. A common way of
modeling adoption dynamics is to use threshold models, where a node may become
an adopter given a high enough rate of contacts with adopted neighbors. We
study the dynamics of threshold models that take both the network topology and
the timings of contacts into account, using empirical contact sequences as
substrates. The models are designed such that adoption is driven by the number
of contacts with different adopted neighbors within a chosen time. We find that
while some networks support cascades leading to network-level adoption, some do
not: the propagation of adoption depends on several factors from the frequency
of contacts to burstiness and timing correlations of contact sequences. More
specifically, burstiness is seen to suppress cascades sizes when compared to
randomised contact timings, while timing correlations between contacts on
adjacent links facilitate cascades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1178</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1178</id><created>2014-02-21</created><authors><author><keyname>Tamta</keyname><forenames>Pawan</forenames></author><author><keyname>Pande</keyname><forenames>B. P.</forenames></author><author><keyname>Dhami</keyname><forenames>H. S.</forenames></author></authors><title>A Polynomial Time Solution to the Clique Problem</title><categories>cs.DS math.OC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Clique Problem has a reduction to the Maximum Flow Network Interdiction
Problem. We review the reduction to evolve a polynomial time algorithm for the
Clique Problem. A computer program in C language has been written to validate
the easiness of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1180</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1180</id><created>2014-02-04</created><updated>2014-09-25</updated><authors><author><keyname>Chondros</keyname><forenames>Nikos</forenames></author><author><keyname>Roussopoulos</keyname><forenames>Mema</forenames></author></authors><title>A distributed Integrity Catalog for digital repositories</title><categories>cs.DB cs.DC cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital repositories, either digital preservation systems or archival
systems, periodically check the integrity of stored objects to assure users of
their correctness. To do so, prior solutions calculate integrity metadata and
require the repository to store it alongside the actual data objects. This
integrity metadata is essential for regularly verifying the correctness of the
stored data objects. To safeguard and detect damage to this metadata, prior
solutions rely on widely visible media, that is unaffiliated third parties, to
store and provide back digests of the metadata to verify it is intact. However,
they do not address recovery of the integrity metadata in case of damage or
attack by an adversary. In essence, they do not preserve this metadata. We
introduce IntegrityCatalog, a system that collects all integrity related
metadata in a single component, and treats them as first class objects,
managing both their integrity and their preservation. We introduce a
treap-based persistent authenticated dictionary managing arbitrary length
key/value pairs, which we use to store all integrity metadata, accessible
simply by object name. Additionally, IntegrityCatalog is a distributed system
that includes a network protocol that manages both corruption detection and
preservation of this metadata, using administrator-selected network peers with
two possible roles. Verifiers store and offer attestations on digests and have
minimal storage requirements, while preservers efficiently synchronize a
complete copy of the catalog to assist in recovery in case of a detected
catalog compromise on the local system. We describe our prototype
implementation of IntegrityCatalog, measure its performance empirically, and
demonstrate its effectiveness in real-world situations, with worst measured
throughput of approximately 1K insertions per second, and 2K verified search
operations per second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1182</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1182</id><created>2014-03-05</created><authors><author><keyname>Dehghan</keyname><forenames>Ali</forenames></author><author><keyname>Sadeghi</keyname><forenames>Mohammad-Reza</forenames></author><author><keyname>Ahadi</keyname><forenames>Arash</forenames></author></authors><title>On the complexity of deciding whether the regular number is at most two</title><categories>math.CO cs.CC</categories><comments>9 pages, 2 figures, accepted for publication in &quot;Graphs and
  Combinatorics&quot;</comments><doi>10.1007/s00373-014-1446-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The regular number of a graph G denoted by reg(G) is the minimum number of
subsets into which the edge set of G can be partitioned so that the subgraph
induced by each subset is regular. In this work we answer to the problem posed
as an open problem in A. Ganesan et al. (2012) [3] about the complexity of
determining the regular number of graphs. We show that computation of the
regular number for connected bipartite graphs is NP-hard. Furthermore, we show
that, determining whether reg(G) = 2 for a given connected 3-colorable graph G
is NP-complete. Also, we prove that a new variant of the Monotone Not-All-Equal
3-Sat problem is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1185</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1185</id><created>2014-03-05</created><authors><author><keyname>Castillo</keyname><forenames>Isaac P&#xe9;rez</forenames></author><author><keyname>Katzav</keyname><forenames>Eytan</forenames></author><author><keyname>Vivo</keyname><forenames>Pierpaolo</forenames></author></authors><title>Phase transitions in the condition number distribution of Gaussian
  random matrices</title><categories>cond-mat.stat-mech cs.CC cs.IT math-ph math.IT math.MP stat.OT</categories><comments>5 pag. + 7 pag. Suppl. Material. 3 Figures</comments><doi>10.1103/PhysRevE.90.050103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the statistics of the condition number
$\kappa=\lambda_{\mathrm{max}}/\lambda_{\mathrm{min}}$ (the ratio between
largest and smallest squared singular values) of $N\times M$ Gaussian random
matrices. Using a Coulomb fluid technique, we derive analytically and for large
$N$ the cumulative $\mathcal{P}[\kappa&lt;x]$ and tail-cumulative
$\mathcal{P}[\kappa&gt;x]$ distributions of $\kappa$. We find that these
distributions decay as $\mathcal{P}[\kappa&lt;x]\approx\exp\left(-\beta N^2
\Phi_{-}(x)\right)$ and $\mathcal{P}[\kappa&gt;x]\approx\exp\left(-\beta N
\Phi_{+}(x)\right)$, where $\beta$ is the Dyson index of the ensemble. The left
and right rate functions $\Phi_{\pm}(x)$ are independent of $\beta$ and
calculated exactly for any choice of the rectangularity parameter
$\alpha=M/N-1&gt;0$. Interestingly, they show a weak non-analytic behavior at
their minimum $\langle\kappa\rangle$ (corresponding to the average condition
number), a direct consequence of a phase transition in the associated Coulomb
fluid problem. Matching the behavior of the rate functions around
$\langle\kappa\rangle$, we determine exactly the scale of typical fluctuations
$\sim\mathcal{O}(N^{-2/3})$ and the tails of the limiting distribution of
$\kappa$. The analytical results are in excellent agreement with numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1194</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1194</id><created>2014-03-05</created><authors><author><keyname>Sasaki</keyname><forenames>Minoru</forenames></author></authors><title>Latent Semantic Word Sense Disambiguation Using Global Co-occurrence
  Information</title><categories>cs.CL cs.IR</categories><comments>6 pages, 2figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, I propose a novel word sense disambiguation method based on
the global co-occurrence information using NMF. When I calculate the dependency
relation matrix, the existing method tends to produce very sparse co-occurrence
matrix from a small training set. Therefore, the NMF algorithm sometimes does
not converge to desired solutions. To obtain a large number of co-occurrence
relations, I propose to use co-occurrence frequencies of dependency relations
between word features in the whole training set. This enables us to solve data
sparseness problem and induce more effective latent features. To evaluate the
efficiency of the method of word sense disambiguation, I make some experiments
to compare with the result of the two baseline methods. The results of the
experiments show this method is effective for word sense disambiguation in
comparison with the all baseline methods. Moreover, the proposed method is
effective for obtaining a stable effect by analyzing the global co-occurrence
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1200</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1200</id><created>2014-03-05</created><authors><author><keyname>Gil</keyname><forenames>A.</forenames></author><author><keyname>Segura</keyname><forenames>J.</forenames></author><author><keyname>Temme</keyname><forenames>N. M.</forenames></author></authors><title>Recent software developments for special functions in the
  Santander-Amsterdam project</title><categories>math.CA cs.MS cs.NA math.NA</categories><comments>To appear in Science of Computer Programming</comments><doi>10.1016/j.scico.2013.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an overview of published algorithms by our group and of current
activities and future plans. In particular, we give details on methods for
computing special functions and discuss in detail two current lines of
research. Firstly, we describe the recent developments for the computation of
central and non-central chi-square cumulative distributions (also called Marcum
Q-functions), and we present a new quadrature method for computing them.
Secondly, we describe the fourth-order methods for computing zeros of special
functions recently developed, and we provide an explicit example for the
computation of complex zeros of Bessel functions. We end with an overview of
published software by our group for computing special functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1202</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1202</id><created>2014-03-05</created><updated>2015-01-21</updated><authors><author><keyname>Cavagna</keyname><forenames>Andrea</forenames></author><author><keyname>Del Castello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Giardina</keyname><forenames>Irene</forenames></author><author><keyname>Grigera</keyname><forenames>Tomas</forenames></author><author><keyname>Jelic</keyname><forenames>Asja</forenames></author><author><keyname>Melillo</keyname><forenames>Stefania</forenames></author><author><keyname>Mora</keyname><forenames>Thierry</forenames></author><author><keyname>Parisi</keyname><forenames>Leonardo</forenames></author><author><keyname>Silvestri</keyname><forenames>Edmondo</forenames></author><author><keyname>Viale</keyname><forenames>Massimiliano</forenames></author><author><keyname>Walczak</keyname><forenames>Aleksandra M.</forenames></author></authors><title>Flocking and turning: a new model for self-organized collective motion</title><categories>cond-mat.stat-mech cs.RO cs.SY physics.bio-ph q-bio.PE</categories><comments>Accepted for the Special Issue of the Journal of Statistical Physics:
  Collective Behavior in Biological Systems, 17 pages, 4 figures, 3 videos</comments><journal-ref>J.Stat.Phys. 158 (2015) 601-627</journal-ref><doi>10.1007/s10955-014-1119-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Birds in a flock move in a correlated way, resulting in large polarization of
velocities. A good understanding of this collective behavior exists for linear
motion of the flock. Yet observing actual birds, the center of mass of the
group often turns giving rise to more complicated dynamics, still keeping
strong polarization of the flock. Here we propose novel dynamical equations for
the collective motion of polarized animal groups that account for correlated
turning including solely social forces. We exploit rotational symmetries and
conservation laws of the problem to formulate a theory in terms of generalized
coordinates of motion for the velocity directions akin to a Hamiltonian
formulation for rotations. We explicitly derive the correspondence between this
formulation and the dynamics of the individual velocities, thus obtaining a new
model of collective motion. In the appropriate overdamped limit we recover the
well-known Vicsek model, which dissipates rotational information and does not
allow for polarized turns. Although the new model has its most vivid success in
describing turning groups, its dynamics is intrinsically different from
previous ones in a wide dynamical regime, while reducing to the hydrodynamic
description of Toner and Tu at very large length-scales. The derived framework
is therefore general and it may describe the collective motion of any strongly
polarized active matter system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1214</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1214</id><created>2014-03-05</created><updated>2014-08-28</updated><authors><author><keyname>Ajorlou</keyname><forenames>Saeede</forenames></author><author><keyname>Shams</keyname><forenames>Issac</forenames></author><author><keyname>Yang</keyname><forenames>Kai</forenames></author></authors><title>A fast clustering algorithm for mining social network data</title><categories>cs.SI physics.soc-ph</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in figures</comments><msc-class>62H30, 91C20</msc-class><acm-class>H.2.8; I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many groups with diverse convictions are interacting online. Interactions in
online communities help people to engage each other and enhance understanding
across groups. Online communities include multiple sub-communities whose
members are similar due to social ties, characteristics, or ideas on a topic.
In this research, we are interested in understanding the changes in the
relative size and activity of these sub-communities, their merging or splitting
patterns, and the changes in the perspectives of the members of these
sub-communities due to endogenous dynamics inside the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1218</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1218</id><created>2014-03-05</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author><author><keyname>Morrison</keyname><forenames>Katherine</forenames></author><author><keyname>Troha</keyname><forenames>Carolyn</forenames></author></authors><title>Cyclic Orbit Codes and Stabilizer Subfields</title><categories>cs.IT math.IT</categories><msc-class>11T71, 94B60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic orbit codes are constant dimension subspace codes that arise as the
orbit of a cyclic subgroup of the general linear group acting on subspaces in
the given ambient space. With the aid of the largest subfield over which the
given subspace is a vector space, the cardinality of the orbit code can be
determined, and estimates for its distance can be found. This subfield is
closely related to the stabilizer of the generating subspace. Finally, with a
linkage construction larger, and longer, constant dimension codes can be
derived from cyclic orbit codes without compromising the distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1228</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1228</id><created>2014-03-05</created><authors><author><keyname>Albert</keyname><forenames>Reka</forenames></author><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Mobasheri</keyname><forenames>Nasim</forenames></author></authors><title>Topological implications of negative curvature for biological and social
  networks</title><categories>q-bio.MN cs.DM cs.SI physics.soc-ph</categories><comments>Physical Review E, 2014</comments><msc-class>92C42, 68R10, 05C40, 05C38, 05C82, 91D30</msc-class><acm-class>E.1; J.3; J.4</acm-class><journal-ref>Physical Review E, 89 (3), 032811, 2014</journal-ref><doi>10.1103/PhysRevE.89.032811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network measures that reflect the most salient properties of complex
large-scale networks are in high demand in the network research community. In
this paper we adapt a combinatorial measure of negative curvature (also called
hyperbolicity) to parameterized finite networks, and show that a variety of
biological and social networks are hyperbolic. This hyperbolicity property has
strong implications on the higher-order connectivity and other topological
properties of these networks. Specifically, we derive and prove bounds on the
distance among shortest or approximately shortest paths in hyperbolic networks.
We describe two implications of these bounds to cross-talk in biological
networks, and to the existence of central, influential neighborhoods in both
biological and social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1241</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1241</id><created>2014-03-05</created><authors><author><keyname>Ogburn</keyname><forenames>Elizabeth L.</forenames></author><author><keyname>VanderWeele</keyname><forenames>Tyler J.</forenames></author></authors><title>Vaccines, Contagion, and Social Networks</title><categories>stat.ME cs.SI physics.soc-ph</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the causal effect that one individual's treatment may have on
another individual's outcome when the outcome is contagious, with specific
application to the effect of vaccination on an infectious disease outcome. The
effect of one individual's vaccination on another's outcome can be decomposed
into two different causal effects, called the &quot;infectiousness&quot; and &quot;contagion&quot;
effects. We present identifying assumptions and estimation or testing
procedures for infectiousness and contagion effects in two different settings:
(1) using data sampled from independent groups of observations, and (2) using
data collected from a single interdependent social network. The methods that we
propose for social network data require fitting generalized linear models
(GLMs). GLMs and other statistical models that require independence across
subjects have been used widely to estimate causal effects in social network
data, but, because the subjects in networks are presumably not independent, the
use of such models is generally invalid, resulting in inference that is
expected to be anticonservative. We introduce a way to ensure that GLM
residuals are uncorrelated across subjects despite the fact that outcomes are
non-independent. This simultaneously demonstrates the possibility of using GLMs
and related statistical models for network data and highlights their
limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1243</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1243</id><created>2014-03-05</created><authors><author><keyname>Vinogradova</keyname><forenames>Julia</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Estimation of Toeplitz Covariance Matrices in Large Dimensional Regime
  with Application to Source Detection</title><categories>cs.IT math.IT</categories><comments>20 pages, 3 figures, submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2015.2447493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we derive concentration inequalities for the spectral norm
of two classical sample estimators of large dimensional Toeplitz covariance
matrices, demonstrating in particular their asymptotic almost sure consistence.
The consistency is then extended to the case where the aggregated matrix of
time samples is corrupted by a rank one (or more generally, low rank) matrix.
As an application of the latter, the problem of source detection in the context
of large dimensional sensor networks within a temporally correlated noise
environment is studied. As opposed to standard procedures, this application is
performed online, i.e. without the need to possess a learning set of pure noise
samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1248</identifier>
 <datestamp>2014-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1248</id><created>2014-03-05</created><authors><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Integrating Energy Storage into the Smart Grid: A Prospect Theoretic
  Approach</title><categories>cs.GT cs.IT math.IT</categories><comments>5 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the interactions and energy exchange decisions of a number of
geographically distributed storage units are studied under decision-making
involving end-users. In particular, a noncooperative game is formulated between
customer-owned storage units where each storage unit's owner can decide on
whether to charge or discharge energy with a given probability so as to
maximize a utility that reflects the tradeoff between the monetary transactions
from charging/discharging and the penalty from power regulation. Unlike
existing game-theoretic works which assume that players make their decisions
rationally and objectively, we use the new framework of prospect theory (PT) to
explicitly incorporate the users' subjective perceptions of their expected
utilities. For the two-player game, we show the existence of a proper mixed
Nash equilibrium for both the standard game-theoretic case and the case with PT
considerations. Simulation results show that incorporating user behavior via PT
reveals several important insights into load management as well as economics of
energy storage usage. For instance, the results show that deviations from
conventional game theory, as predicted by PT, can lead to undesirable grid
loads and revenues thus requiring the power company to revisit its pricing
schemes and the customers to reassess their energy storage usage choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1252</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1252</id><created>2014-03-05</created><updated>2014-06-27</updated><authors><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>Inducing Language Networks from Continuous Space Word Representations</title><categories>cs.LG cs.CL cs.SI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in unsupervised feature learning have developed powerful
latent representations of words. However, it is still not clear what makes one
representation better than another and how we can learn the ideal
representation. Understanding the structure of latent spaces attained is key to
any future advancement in unsupervised learning. In this work, we introduce a
new view of continuous space word representations as language networks. We
explore two techniques to create language networks from learned features by
inducing them for two popular word representation methods and examining the
properties of their resulting networks. We find that the induced networks
differ from other methods of creating language networks, and that they contain
meaningful community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1274</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1274</id><created>2014-03-05</created><updated>2014-03-07</updated><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Lugosi</keyname><forenames>Gabor</forenames></author></authors><title>Almost optimal sparsification of random geometric graphs</title><categories>math.PR cs.DM cs.NI math.CO</categories><msc-class>05C80, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random geometric irrigation graph $\Gamma_n(r_n,\xi)$ has $n$ vertices
identified by $n$ independent uniformly distributed points $X_1,\ldots,X_n$ in
the unit square $[0,1]^2$. Each point $X_i$ selects $\xi_i$ neighbors at
random, without replacement, among those points $X_j$ ($j\neq i$) for which
$\|X_i-X_j\| &lt; r_n$, and the selected vertices are connected to $X_i$ by an
edge. The number $\xi_i$ of the neighbors is an integer-valued random variable,
chosen independently with identical distribution for each $X_i$ such that
$\xi_i$ satisfies $1\le \xi_i \le \kappa$ for a constant $\kappa&gt;1$. We prove
that when $r_n = \gamma_n \sqrt{\log n/n}$ for $\gamma_n \to \infty$ with
$\gamma_n =o(n^{1/6}/\log^{5/6}n)$, then the random geometric irrigation graph
experiences explosive percolation in the sense that when $\mathbf E \xi_i=1$,
then the largest connected component has size $o(n)$ but if $\mathbf E \xi_i
&gt;1$, then the size of the largest connected component is with high probability
$n-o(n)$. This offers a natural non-centralized sparsification of a random
geometric graph that is mostly connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1276</identifier>
 <datestamp>2014-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1276</id><created>2014-03-05</created><updated>2014-05-15</updated><authors><author><keyname>Gong</keyname><forenames>Xun</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Quantifying the Information Leakage in Timing Side Channels in
  Deterministic Work-Conserving Schedulers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple job processes are served by a single scheduler, the queueing
delays of one process are often affected by the others, resulting in a timing
side channel that leaks the arrival pattern of one process to the others. In
this work, we study such a timing side channel between a regular user and a
malicious attacker. Utilizing Shannon's mutual information as a measure of
information leakage between the user and attacker, we analyze
privacy-preserving behaviors of common work-conserving schedulers. We find that
the attacker can always learn perfectly the user's arrival process in a
longest-queue-first (LQF) scheduler. When the user's job arrival rate is very
low (near zero), first-come-first-serve (FCFS) and round robin schedulers both
completely reveal the user's arrival pattern. The near-complete information
leakage in the low-rate traffic region is proven to be reduced by half in a
work-conserving version of TDMA (WC-TDMA) scheduler, which turns out to be
privacy-optimal in the class of deterministic-working-conserving (det-WC)
schedulers, according to a universal lower bound on information leakage we
derive for all det-WC schedulers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1279</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1279</id><created>2014-03-05</created><authors><author><keyname>Toussi</keyname><forenames>Hamid A.</forenames></author><author><keyname>Bigham</keyname><forenames>Bahram Sadeghi</forenames></author></authors><title>Design, Implementation and Evaluation of MTBDD based Fuzzy Sets and
  Binary Fuzzy Relations</title><categories>cs.DS cs.SC</categories><comments>A shorter version was published in Proceeding of International
  Conference on Computer, Information Technology and Digital Media, Tehran,
  Iran, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fast and efficient analysis of large sets of fuzzy data, elimination of
redundancies in the memory representation is needed. We used MTBDDs as the
underlying data-structure to represent fuzzy sets and binary fuzzy relations.
This leads to elimination of redundancies in the representation, less
computations, and faster analyses. We have also extended a BDD package (BuDDy)
to support MTBDDs in general and fuzzy sets and relations in particular.
Different fuzzy operations such as max, min and max-min composition were
implemented based on our representation. Effectiveness of our representation is
shown by applying it on fuzzy connectedness and image segmentation problem.
Compared to a base implementation, the running time of our MTBDD based
implementation was faster (in our test cases) by a factor ranging from 2 to 27.
Also, when the MTBDD based data-structure was employed, the memory needed to
represent the final results was improved by a factor ranging from 37.9 to
265.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1288</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1288</id><created>2014-03-05</created><authors><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Jorge</forenames></author></authors><title>Computational search of small point sets with small rectilinear crossing
  number</title><categories>math.CO cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\crs(K_n)$ be the minimum number of crossings over all rectilinear
drawings of the complete graph on $n$ vertices on the plane. In this paper we
prove that $\crs(K_n) &lt; 0.380473\binom{n}{4}+\Theta(n^3)$; improving thus on
the previous best known upper bound. This is done by obtaining new rectilinear
drawings of $K_n$ for small values of $n$, and then using known constructions
to obtain arbitrarily large good drawings from smaller ones. The &quot;small&quot; sets
where found using a simple heuristic detailed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1289</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1289</id><created>2014-03-05</created><updated>2014-03-11</updated><authors><author><keyname>Fu</keyname><forenames>Yong</forenames></author><author><keyname>Holler</keyname><forenames>Anne</forenames></author><author><keyname>Lu</keyname><forenames>Chenyang</forenames></author></authors><title>CloudPowerCap: Integrating Power Budget and Resource Management across a
  Virtualized Server Cluster</title><categories>cs.DC</categories><comments>Tech report for USENIX ICAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many datacenters, server racks are highly underutilized. Rack slots are
left empty to keep the sum of the server nameplate maximum power below the
power provisioned to the rack. And the servers that are placed in the rack
cannot make full use of available rack power. The root cause of this rack
underutilization is that the server nameplate power is often much higher than
can be reached in practice. To address rack underutilization, server vendors
are shipping support for per-host power caps, which provide a server-enforced
limit on the amount of power that the server can draw. Using this feature,
datacenter operators can set power caps on the hosts in the rack to ensure that
the sum of those caps does not exceed the rack's provisioned power. While this
approach improves rack utilization, it burdens the operator with managing the
rack power budget across the hosts and does not lend itself to flexible
allocation of power to handle workload usage spikes or to respond to changes in
the amount of powered-on server capacity in the rack. In this paper we present
CloudPowerCap, a practical and scalable solution for power budget management in
a virtualized environment. CloudPowerCap manages the power budget for a cluster
of virtualized servers, dynamically adjusting the per-host power caps for hosts
in the cluster. We show how CloudPowerCap can provide better use of power than
per-host static settings, while respecting virtual machine resource
entitlements and constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1305</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1305</id><created>2014-03-05</created><authors><author><keyname>Arudchutha</keyname><forenames>S.</forenames></author><author><keyname>Nishanthy</keyname><forenames>T.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>String Matching with Multicore CPUs: Performing Better with the
  Aho-Corasick Algorithm</title><categories>cs.DC</categories><journal-ref>8th IEEE International Conference on Industrial and Information
  Systems (ICIIS), 2013, pp. 231-236, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6731987</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple string matching is known as locating all the occurrences of a given
number of patterns in an arbitrary string. It is used in bio-computing
applications where the algorithms are commonly used for retrieval of
information such as sequence analysis and gene/protein identification.
Extremely large amount of data in the form of strings has to be processed in
such bio-computing applications. Therefore, improving the performance of
multiple string matching algorithms is always desirable. Multicore
architectures are capable of providing better performance by parallelizing the
multiple string matching algorithms. The Aho-Corasick algorithm is the one that
is commonly used in exact multiple string matching algorithms. The focus of
this paper is the acceleration of Aho-Corasick algorithm through a multicore
CPU based software implementation. Through our implementation and evaluation of
results, we prove that our method performs better compared to the state of the
art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1307</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1307</id><created>2014-03-05</created><updated>2014-07-24</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author></authors><title>An n\log n Lower Bound for Fourier Transform Computation in the Well
  Conditioned Model</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obtaining a non-trivial (super-linear) lower bound for computation of the
Fourier transform in the linear circuit model has been a long standing open
problem for over 40 years.
  An early result by Morgenstern from 1973, provides an $\Omega(n \log n)$
lower bound for the unnormalized Fourier transform when the constants used in
the computation are bounded. The proof uses a potential function related to a
determinant. The result does not explain why the normalized Fourier transform
(of unit determinant) should be difficult to compute in the same model. Hence,
the result is not scale insensitive.
  More recently, Ailon (2013) showed that if only unitary 2-by-2 gates are
used, and additionally no extra memory is allowed, then the normalized Fourier
transform requires $\Omega(n\log n)$ steps. This rather limited result is also
sensitive to scaling, but highlights the complexity inherent in the Fourier
transform arising from introducing entropy, unlike, say, the identity matrix
(which is as complex as the Fourier transform using Morgenstern's arguments,
under proper scaling).
  In this work we extend the arguments of Ailon (2013). In the first extension,
which is also the main contribution, we provide a lower bound for computing any
scaling of the Fourier transform. Our restriction is that, the composition of
all gates up to any point must be a well conditioned linear transformation. The
lower bound is $\Omega(R^{-1}n\log n)$, where $R$ is the uniform condition
number. Second, we assume extra space is allowed, as long as it contains
information of bounded norm at the end of the computation.
  The main technical contribution is an extension of matrix entropy used in
Ailon (2013) for unitary matrices to a potential function computable for any
matrix, using Shannon entropy on &quot;quasi-probabilities&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1310</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1310</id><created>2014-03-05</created><authors><author><keyname>Jiffriya</keyname><forenames>M. A. C.</forenames></author><author><keyname>Jahan</keyname><forenames>M. A. C. Akmal</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Deegalla</keyname><forenames>S.</forenames></author></authors><title>AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based
  Assignments</title><categories>cs.IR cs.CL cs.DL</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 376 - 380, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6732013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plagiarism is one of the growing issues in academia and is always a concern
in Universities and other academic institutions. The situation is becoming even
worse with the availability of ample resources on the web. This paper focuses
on creating an effective and fast tool for plagiarism detection for text based
electronic assignments. Our plagiarism detection tool named AntiPlag is
developed using the tri-gram sequence matching technique. Three sets of text
based assignments were tested by AntiPlag and the results were compared against
an existing commercial plagiarism detection tool. AntiPlag showed better
results in terms of false positives compared to the commercial tool due to the
pre-processing steps performed in AntiPlag. In addition, to improve the
detection latency, AntiPlag applies a data clustering technique making it four
times faster than the commercial tool considered. AntiPlag could be used to
isolate plagiarized text based assignments from non-plagiarised assignments
easily. Therefore, we present AntiPlag, a fast and effective tool for
plagiarism detection on text based electronic assignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1313</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1313</id><created>2014-03-05</created><authors><author><keyname>Perera</keyname><forenames>P.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Accelerating motif finding in DNA sequences with multicore CPUs</title><categories>cs.CE cs.DC</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 242-247, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6731989</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motif discovery in DNA sequences is a challenging task in molecular biology.
In computational motif discovery, Planted (l, d) motif finding is a widely
studied problem and numerous algorithms are available to solve it. Both
hardware and software accelerators have been introduced to accelerate the motif
finding algorithms. However, the use of hardware accelerators such as FPGAs
needs hardware specialists to design such systems. Software based acceleration
methods on the other hand are easier to implement than hardware acceleration
techniques. Grid computing is one such software based acceleration technique
which has been used in acceleration of motif finding. However, drawbacks such
as network communication delays and the need of fast interconnection between
nodes in the grid can limit its usage and scalability. As using multicore CPUs
to accelerate CPU intensive tasks are becoming increasingly popular and common
nowadays, we can employ it to accelerate motif finding and it can be a faster
method than grid based acceleration. In this paper, we have explored the use of
multicore CPUs to accelerate motif finding. We have accelerated the Skip-Brute
Force algorithm on multicore CPUs parallelizing it using the POSIX thread
library. Our method yielded an average speed up of 34x on a 32-core processor
compared to a speed up of 21x on a grid based implementation of 32 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1314</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1314</id><created>2014-03-05</created><authors><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author><author><keyname>Herath</keyname><forenames>P.</forenames></author><author><keyname>Senanayake</keyname><forenames>U.</forenames></author></authors><title>Authorship detection of SMS messages using unigrams</title><categories>cs.CL cs.IR</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 387-392 , 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6732015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SMS messaging is a popular media of communication. Because of its popularity
and privacy, it could be used for many illegal purposes. Additionally, since
they are part of the day to day life, SMSes can be used as evidence for many
legal disputes. Since a cellular phone might be accessible to people close to
the owner, it is important to establish the fact that the sender of the message
is indeed the owner of the phone. For this purpose, the straight forward
solutions seem to be the use of popular stylometric methods. However, in
comparison with the data used for stylometry in the literature, SMSes have
unusual characteristics making it hard or impossible to apply these methods in
a conventional way. Our target is to come up with a method of authorship
detection of SMS messages that could still give a usable accuracy. We argue
that, considering the methods of author attribution, the best method that could
be applied to SMS messages is an n-gram method. To prove our point, we checked
two different methods of distribution comparison with varying number of
training and testing data. We specifically try to compare how well our
algorithms work under less amount of testing data and large number of candidate
authors (which we believe to be the real world scenario) against controlled
tests with less number of authors and selected SMSes with large number of
words. To counter the lack of information in an SMS message, we propose the
method of stacking together few SMSes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1317</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1317</id><created>2014-03-05</created><authors><author><keyname>Vidanagamachchi</keyname><forenames>S. M.</forenames></author><author><keyname>Dewasurendra</keyname><forenames>S. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Hardware software co-design of the Aho-Corasick algorithm: Scalable for
  protein identification?</title><categories>cs.CE</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 321-325, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6732003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern matching is commonly required in many application areas and
bioinformatics is a major area of interest that requires both exact and
approximate pattern matching. Much work has been done in this area, yet there
is still a significant space for improvement in efficiency, flexibility, and
throughput. This paper presents a hardware software co-design of Aho-Corasick
algorithm in Nios II soft-processor and a study on its scalability for a
pattern matching application. A software only approach is used to compare the
throughput and the scalability of the hardware software co-design approach.
According to the results we obtained, we conclude that the hardware software
co-design implementation shows a maximum of 10 times speed up for pattern size
of 1200 peptides compared to the software only implementation. The results also
show that the hardware software co-design approach scales well for increasing
data size compared to the software only approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1319</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1319</id><created>2014-03-05</created><authors><author><keyname>Vidanagamachchi</keyname><forenames>S. M.</forenames></author><author><keyname>Dewasurendra</keyname><forenames>S. D.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Hardware accelerated protein inference framework</title><categories>cs.CE</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 649-653, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6732061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein inference plays a vital role in the proteomics study. Two major
approaches could be used to handle the problem of protein inference; top-down
and bottom-up. This paper presents a framework for protein inference, which
uses hardware accelerated protein inference framework for handling the most
important step in a bottom-up approach, viz. peptide identification during the
assembling process. In our framework, identified peptides and their
probabilities are used to predict the most suitable reference protein cluster
for a given input amino acid sequence with the probability of identified
peptides. The framework is developed on an FPGA where hardware software
co-design techniques are used to accelerate the computationally intensive parts
of the protein inference process. In the paper we have measured, compared and
reported the time taken for the protein inference process in our framework
against a pure software implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1322</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1322</id><created>2014-03-05</created><authors><author><keyname>Herath</keyname><forenames>U.</forenames></author><author><keyname>Alawatugoda</keyname><forenames>J.</forenames></author><author><keyname>Ragel</keyname><forenames>R. G.</forenames></author></authors><title>Software implementation level countermeasures against the cache timing
  attack on advanced encryption standard</title><categories>cs.CR</categories><journal-ref>Industrial and Information Systems (ICIIS), 2013 8th IEEE
  International Conference on, pp. 75-80, 17-20 Dec. 2013</journal-ref><doi>10.1109/ICIInfS.2013.6731958</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced Encryption Standard (AES) is a symmetric key encryption algorithm
which is extensively used in secure electronic data transmission. When
introduced, although it was tested and declared as secure, in 2005, a
researcher named Bernstein claimed that it is vulnerable to side channel
attacks. The cache-based timing attack is the type of side channel attack
demonstrated by Bernstein, which uses the timing variation in cache hits and
misses. This kind of attacks can be prevented by masking the actual timing
information from the attacker. Such masking can be performed by altering the
original AES software implementation while preserving its semantics. This paper
presents possible software implementation level countermeasures against
Bernstein's cache timing attack. Two simple software based countermeasures
based on the concept of &quot;constant-encryption-time&quot; were demonstrated against
the remote cache timing attack with positive outcomes, in which we establish a
secured environment for the AES encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1323</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1323</id><created>2014-03-05</created><authors><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Qi</keyname><forenames>Wangdong</forenames></author><author><keyname>Li</keyname><forenames>Guangxia</forenames></author><author><keyname>Zhang</keyname><forenames>Su</forenames></author></authors><title>Performance of ML Range Estimator in Radio Interferometric Positioning
  Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 1 figure</comments><doi>10.1109/LSP.2014.2352270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radio interferometric positioning system (RIPS) is a novel positioning
solution used in wireless sensor networks. This letter explores the ranging
accuracy of RIPS in two configurations. In the linear step-frequency (LSF)
configuration, we derive the mean square error (MSE) of the maximum likelihood
(ML) estimator. In the random step-frequency (RSF) configuration, we introduce
average MSE to characterize the performance of the ML estimator. The simulation
results fit well with theoretical analysis. It is revealed that RSF is superior
to LSF in that the former is more robust in a jamming environment with similar
ranging accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1327</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1327</id><created>2014-03-05</created><authors><author><keyname>Liu</keyname><forenames>Hongli</forenames></author><author><keyname>Liu</keyname><forenames>Weifeng</forenames></author><author><keyname>Wang</keyname><forenames>Yanjiang</forenames></author></authors><title>Multi-view Face Analysis Based on Gabor Features</title><categories>cs.CV</categories><comments>8 pages, 3 figures, Journal of Information and Computational Science</comments><journal-ref>Journal of Information and Computational Science,
  2014,11(13):4637-4644</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial analysis has attracted much attention in the technology for
human-machine interface. Different methods of classification based on sparse
representation and Gabor kernels have been widely applied in the fields of
facial analysis. However, most of these methods treat face from a whole view
standpoint. In terms of the importance of different facial views, in this
paper, we present multi-view face analysis based on sparse representation and
Gabor wavelet coefficients. To evaluate the performance, we conduct face
analysis experiments including face recognition (FR) and face expression
recognition (FER) on JAFFE database. Experiments are conducted from two parts:
(1) Face images are divided into three facial parts which are forehead, eye and
mouth. (2) Face images are divided into 8 parts by the orientation of Gabor
kernels. Experimental results demonstrate that the proposed methods can
significantly boost the performance and perform better than the other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1329</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1329</id><created>2014-03-05</created><authors><author><keyname>Ott</keyname><forenames>Lionel</forenames></author><author><keyname>Pang</keyname><forenames>Linsey</forenames></author><author><keyname>Ramos</keyname><forenames>Fabio</forenames></author><author><keyname>Howe</keyname><forenames>David</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author></authors><title>Integer Programming Relaxations for Integrated Clustering and Outlier
  Detection</title><categories>cs.LG</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present methods for exemplar based clustering with outlier
selection based on the facility location formulation. Given a distance function
and the number of outliers to be found, the methods automatically determine the
number of clusters and outliers. We formulate the problem as an integer program
to which we present relaxations that allow for solutions that scale to large
data sets. The advantages of combining clustering and outlier selection
include: (i) the resulting clusters tend to be compact and semantically
coherent (ii) the clusters are more robust against data perturbations and (iii)
the outliers are contextualised by the clusters and more interpretable, i.e. it
is easier to distinguish between outliers which are the result of data errors
from those that may be indicative of a new pattern emergent in the data. We
present and contrast three relaxations to the integer program formulation: (i)
a linear programming formulation (LP) (ii) an extension of affinity propagation
to outlier detection (APOC) and (iii) a Lagrangian duality based formulation
(LD). Evaluation on synthetic as well as real data shows the quality and
scalability of these different methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1336</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1336</id><created>2014-03-05</created><authors><author><keyname>Sree</keyname><forenames>Pokkuluri Kiran</forenames></author><author><keyname>Babu</keyname><forenames>Inampudi Ramesh</forenames></author></authors><title>An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated
  MACA based Clonal Classifier for Protein Coding and Promoter Region
  Prediction)</title><categories>cs.CE cs.LG</categories><comments>5 Pages, Review of Bioinformatics and Biometrics (RBB) Volume 3 Issue
  1, March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has
created good impact on solving major problems in bioinformatics like protein
region identification and promoter region prediction with less time (Pokkuluri
Kiran Sree, 2014). This AIS-INMACA is now came with several variations
(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics
for solving many problems in bioinformatics. So this paper will be very much
useful for so many researchers who are working in the domain of bioinformatics
with cellular automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1343</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1343</id><created>2014-03-05</created><updated>2014-07-24</updated><authors><author><keyname>Simkin</keyname><forenames>Mark</forenames></author><author><keyname>Schroeder</keyname><forenames>Dominique</forenames></author><author><keyname>Bulling</keyname><forenames>Andreas</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Ubic: Bridging the gap between digital cryptography and the physical
  world</title><categories>cs.CR cs.CV</categories><comments>In ESORICS 2014, volume 8712 of Lecture Notes in Computer Science,
  pp. 56-75, Wroclaw, Poland, September 7-11, 2014. Springer, Berlin, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in computing technology increasingly blur the boundary between the
digital domain and the physical world. Although the research community has
developed a large number of cryptographic primitives and has demonstrated their
usability in all-digital communication, many of them have not yet made their
way into the real world due to usability aspects. We aim to make another step
towards a tighter integration of digital cryptography into real world
interactions. We describe Ubic, a framework that allows users to bridge the gap
between digital cryptography and the physical world. Ubic relies on
head-mounted displays, like Google Glass, resource-friendly computer vision
techniques as well as mathematically sound cryptographic primitives to provide
users with better security and privacy guarantees. The framework covers key
cryptographic primitives, such as secure identification, document verification
using a novel secure physical document format, as well as content hiding. To
make a contribution of practical value, we focused on making Ubic as simple,
easily deployable, and user friendly as possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1347</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1347</id><created>2014-03-06</created><authors><author><keyname>Zhou</keyname><forenames>Jian</forenames></author><author><keyname>Troyanskaya</keyname><forenames>Olga G.</forenames></author></authors><title>Deep Supervised and Convolutional Generative Stochastic Network for
  Protein Secondary Structure Prediction</title><categories>q-bio.QM cs.CE cs.LG</categories><comments>Accepted by ICML 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting protein secondary structure is a fundamental problem in protein
structure prediction. Here we present a new supervised generative stochastic
network (GSN) based method to predict local secondary structure with deep
hierarchical representations. GSN is a recently proposed deep learning
technique (Bengio &amp; Thibodeau-Laufer, 2013) to globally train deep generative
model. We present the supervised extension of GSN, which learns a Markov chain
to sample from a conditional distribution, and applied it to protein structure
prediction. To scale the model to full-sized, high-dimensional data, like
protein sequences with hundreds of amino acids, we introduce a convolutional
architecture, which allows efficient learning across multiple layers of
hierarchical representations. Our architecture uniquely focuses on predicting
structured low-level labels informed with both low and high-level
representations learned by the model. In our application this corresponds to
labeling the secondary structure state of each amino-acid residue. We trained
and tested the model on separate sets of non-homologous proteins sharing less
than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513
dataset, better than the previously reported best performance 64.9% (Wang et
al., 2011) for this challenging secondary structure prediction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1349</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1349</id><created>2014-03-06</created><updated>2014-10-17</updated><authors><author><keyname>Anzaroot</keyname><forenames>Sam</forenames></author><author><keyname>Passos</keyname><forenames>Alexandre</forenames></author><author><keyname>Belanger</keyname><forenames>David</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Learning Soft Linear Constraints with Application to Citation Field
  Extraction</title><categories>cs.CL cs.DL cs.IR</categories><comments>appears in Proc. the 52nd Annual Meeting of the Association for
  Computational Linguistics (ACL2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurately segmenting a citation string into fields for authors, titles, etc.
is a challenging task because the output typically obeys various global
constraints. Previous work has shown that modeling soft constraints, where the
model is encouraged, but not require to obey the constraints, can substantially
improve segmentation performance. On the other hand, for imposing hard
constraints, dual decomposition is a popular technique for efficient prediction
given existing algorithms for unconstrained inference. We extend the technique
to perform prediction subject to soft constraints. Moreover, with a technique
for performing inference given soft constraints, it is easy to automatically
generate large families of constraints and learn their costs with a simple
convex optimization problem during training. This allows us to obtain
substantial gains in accuracy on a new, challenging citation extraction
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1353</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1353</id><created>2014-03-06</created><authors><author><keyname>Wu</keyname><forenames>Yang</forenames></author><author><keyname>Jarich</keyname><forenames>Vansteenberge</forenames></author><author><keyname>Mukunoki</keyname><forenames>Masayuki</forenames></author><author><keyname>Minoh</keyname><forenames>Michihiko</forenames></author></authors><title>Collaborative Representation for Classification, Sparse or Non-sparse?</title><categories>cs.CV cs.AI cs.LG</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representation based classification (SRC) has been proved to be a
simple, effective and robust solution to face recognition. As it gets popular,
doubts on the necessity of enforcing sparsity starts coming up, and primary
experimental results showed that simply changing the $l_1$-norm based
regularization to the computationally much more efficient $l_2$-norm based
non-sparse version would lead to a similar or even better performance. However,
that's not always the case. Given a new classification task, it's still unclear
which regularization strategy (i.e., making the coefficients sparse or
non-sparse) is a better choice without trying both for comparison. In this
paper, we present as far as we know the first study on solving this issue,
based on plenty of diverse classification experiments. We propose a scoring
function for pre-selecting the regularization strategy using only the dataset
size, the feature dimensionality and a discrimination score derived from a
given feature representation. Moreover, we show that when dictionary learning
is taking into account, non-sparse representation has a more significant
superiority to sparse representation. This work is expected to enrich our
understanding of sparse/non-sparse collaborative representation for
classification and motivate further research activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1362</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1362</id><created>2014-03-06</created><authors><author><keyname>Chintalapati</keyname><forenames>Shireesha</forenames></author><author><keyname>Raghunadh</keyname><forenames>M. V.</forenames></author></authors><title>Illumination,Expression and Occlusion Invariant Pose-Adaptive Face
  Recognition System for Real-Time Applications</title><categories>cs.CV</categories><comments>7 pages,8 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V8(6),292-298 February 2014. Published by seventh sense research group</journal-ref><doi>10.14445/22315381/IJETT-V8P254</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition in real-time scenarios is mainly affected by illumination,
expression and pose variations and also by occlusion. This paper presents the
framework for pose adaptive component-based face recognition system. The
framework proposed deals with all the above mentioned issues. The steps
involved in the presented framework are (i) facial landmark localisation, (ii)
facial component extraction, (iii) pre-processing of facial image (iv) facial
pose estimation (v) feature extraction using Local Binary Pattern Histograms of
each component followed by (vi) fusion of pose adaptive classification of
components. By employing pose adaptive classification, the recognition process
is carried out on some part of database, based on estimated pose, instead of
applying the recognition process on the whole database. Pre-processing
techniques employed to overcome the problems due to illumination variation are
also discussed in this paper. Component-based techniques provide better
recognition rates when face images are occluded compared to the holistic
methods. Our method is simple, feasible and provides better results when
compared to other holistic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1364</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1364</id><created>2014-03-06</created><updated>2014-09-01</updated><authors><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author><author><keyname>Vildh&#xf8;j</keyname><forenames>Hjalte Wedel</forenames></author></authors><title>A Suffix Tree Or Not A Suffix Tree?</title><categories>cs.DS</categories><comments>Full version. An extended abstract has been accepted to IWOCA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the structure of suffix trees. Given an unlabeled tree
$\tau$ on $n$ nodes and suffix links of its internal nodes, we ask the question
&quot;Is $\tau$ a suffix tree?&quot;, i.e., is there a string $S$ whose suffix tree has
the same topological structure as $\tau$? We place no restrictions on $S$, in
particular we do not require that $S$ ends with a unique symbol. This
corresponds to considering the more general definition of implicit or extended
suffix trees. Such general suffix trees have many applications and are for
example needed to allow efficient updates when suffix trees are built online.
We prove that $\tau$ is a suffix tree if and only if it is realized by a string
$S$ of length $n-1$, and we give a linear-time algorithm for inferring $S$ when
the first letter on each edge is known. This generalizes the work of I et al.
[Discrete Appl. Math. 163, 2014].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1366</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1366</id><created>2014-03-06</created><updated>2014-03-14</updated><authors><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>An Accurate and Efficient Analysis of a MBSFN Network</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), 2014, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new accurate analysis is presented for an OFDM-based multicast-broadcast
single-frequency network (MBSFN). The topology of the network is modeled by a
constrained random spatial model involving a fixed number of base stations
placed over a finite area with a minimum separation. The analysis is driven by
a new closed-form expression for the conditional outage probability at each
location of the network, where the conditioning is with respect to the network
realization. The analysis accounts for the diversity combining of signals
transmitted by different base stations of a given MBSFN area, and also accounts
for the interference caused by the base stations of other MBSFN areas. The
analysis features a flexible channel model, accounting for path loss, Nakagami
fading, and correlated shadowing. The analysis is used to investigate the
influence of the minimum base-station separation and provides insight regarding
the optimal size of the MBSFN areas. In order to highlight the percentage of
the network that will fail to successfully receive the broadcast, the area
below an outage threshold (ABOT) is here used and defined as the fraction of
the network that provides an outage probability (averaged over the fading) that
meets a threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1376</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1376</id><created>2014-03-06</created><authors><author><keyname>H&#xf6;hn</keyname><forenames>Wiebke</forenames></author><author><keyname>Mestre</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>How Unsplittable-Flow-Covering helps Scheduling with Job-Dependent Cost
  Functions</title><categories>cs.DS</categories><comments>2 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalizing many well-known and natural scheduling problems, scheduling with
job-specific cost functions has gained a lot of attention recently. In this
setting, each job incurs a cost depending on its completion time, given by a
private cost function, and one seeks to schedule the jobs to minimize the total
sum of these costs. The framework captures many important scheduling objectives
such as weighted flow time or weighted tardiness. Still, the general case as
well as the mentioned special cases are far from being very well understood
yet, even for only one machine. Aiming for better general understanding of this
problem, in this paper we focus on the case of uniform job release dates on one
machine for which the state of the art is a 4-approximation algorithm. This is
true even for a special case that is equivalent to the covering version of the
well-studied and prominent unsplittable flow on a path problem, which is
interesting in its own right. For that covering problem, we present a
quasi-polynomial time $(1+\epsilon)$-approximation algorithm that yields an
$(e+\epsilon)$-approximation for the above scheduling problem. Moreover, for
the latter we devise the best possible resource augmentation result regarding
speed: a polynomial time algorithm which computes a solution with \emph{optimal
}cost at $1+\epsilon$ speedup. Finally, we present an elegant QPTAS for the
special case where the cost functions of the jobs fall into at most $\log n$
many classes. This algorithm allows the jobs even to have up to $\log n$ many
distinct release dates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1381</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1381</id><created>2014-03-06</created><authors><author><keyname>Zaragoza</keyname><forenames>Daniel</forenames></author></authors><title>The TCP-modified Engset Model Revisited</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.8173</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the TCP-modified Engset model proposed by Heyman et al. in [1].
The model deals with the superposition of a limited number of TCP connections
alternating between file transmission and silence in a web-like fashion. We
consider homogeneous sources only. (a) We take into account the effects of slow
start and limited receiver window as well as small average file sizes. (b) We
propose an alternative way for calculating the average connection rate in the
superposition. (c) From the model we propose a way for calculating the queuing
behavior; i.e., the overflow probability. (d) From this last point, we propose
a new link buffer sizing rule. Comparison with extensive simulations shows that
the average rate and duration, as well as, link utilization are accurately
predicted for exponentially distributed file sizes. For longer tail
distributions, the model remains accurate provided the receiver window is
adjusted appropriately. The accuracy increases with increasing load. As
concerns the queuing behavior, the same observation applies. Finally, the
revisited model cannot be used to predict losses larger than about 1%. The
model overestimates loss rates above that threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1383</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1383</id><created>2014-03-06</created><authors><author><keyname>Grieco</keyname><forenames>Luigi Alfredo</forenames><affiliation>DEE</affiliation></author><author><keyname>Alaya</keyname><forenames>Mahdi Ben</forenames><affiliation>LAAS</affiliation></author><author><keyname>Monteil</keyname><forenames>Thierry</forenames><affiliation>LAAS</affiliation></author><author><keyname>Drira</keyname><forenames>Khalil</forenames><affiliation>LAAS</affiliation></author></authors><title>Architecting Information Centric ETSI-M2M systems</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The European Telecommunications Standards Institute (ETSI) released a set of
specifications to define a restful architecture for enabling seamless service
provisioning across heterogeneous Machine-to-Machine (M2M) systems. The current
version of this architecture is strongly centralized, thus requiring new
enhancements to its scalability, fault tolerance, and flexibility. To bridge
this gap, herein it is presented an Overlay Service Capability Layer, based on
Information Centric Networking design. Key features, example use cases and
preliminary performance assessments are also discussed to highlight the
potential of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1403</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1403</id><created>2014-03-06</created><authors><author><keyname>Panisson</keyname><forenames>A.</forenames></author><author><keyname>Gauvin</keyname><forenames>L.</forenames></author><author><keyname>Quaggiotto</keyname><forenames>M.</forenames></author><author><keyname>Cattuto</keyname><forenames>C.</forenames></author></authors><title>Mining Concurrent Topical Activity in Microblog Streams</title><categories>physics.soc-ph cs.SI</categories><comments>Proceedings of the 4th workshop on 'Making Sense of Microposts',
  World Wide Web Conference 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streams of user-generated content in social media exhibit patterns of
collective attention across diverse topics, with temporal structures determined
both by exogenous factors and endogenous factors. Teasing apart different
topics and resolving their individual, concurrent, activity timelines is a key
challenge in extracting knowledge from microblog streams. Facing this challenge
requires the use of methods that expose latent signals by using term
correlations across posts and over time. Here we focus on content posted to
Twitter during the London 2012 Olympics, for which a detailed schedule of
events is independently available and can be used for reference. We mine the
temporal structure of topical activity by using two methods based on
non-negative matrix factorization. We show that for events in the Olympics
schedule that can be semantically matched to Twitter topics, the extracted
Twitter activity timeline closely matches the known timeline from the schedule.
Our results show that, given appropriate techniques to detect latent signals,
Twitter can be used as a social sensor to extract topical-temporal information
on real-world events at high temporal resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1412</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1412</id><created>2014-03-06</created><updated>2014-08-08</updated><authors><author><keyname>Saishankar</keyname><forenames>K. P.</forenames></author><author><keyname>Kalyani</keyname><forenames>Sheetal</forenames></author><author><keyname>Narendran</keyname><forenames>K.</forenames></author></authors><title>Rate Prediction and Selection in LTE systems using Modified Source
  Encoding Techniques</title><categories>stat.AP cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In current wireless systems, the base-Station (eNodeB) tries to serve its
user-equipment (UE) at the highest possible rate that the UE can reliably
decode. The eNodeB obtains this rate information as a quantized feedback from
the UE at time n and uses this, for rate selection till the next feedback is
received at time n + {\delta}. The feedback received at n can become outdated
before n + {\delta}, because of a) Doppler fading, and b) Change in the set of
active interferers for a UE. Therefore rate prediction becomes essential.
Since, the rates belong to a discrete set, we propose a discrete sequence
prediction approach, wherein, frequency trees for the discrete sequences are
built using source encoding algorithms like Prediction by Partial Match (PPM).
Finding the optimal depth of the frequency tree used for prediction is cast as
a model order selection problem. The rate sequence complexity is analysed to
provide an upper bound on model order. Information-theoretic criteria are then
used to solve the model order problem. Finally, two prediction algorithms are
proposed, using the PPM with optimal model order and system level simulations
demonstrate the improvement in packet loss and throughput due to these
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1430</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1430</id><created>2014-03-06</created><updated>2014-05-01</updated><authors><author><keyname>Hu</keyname><forenames>Zhenfang</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Yueming</forenames></author><author><keyname>Wu</keyname><forenames>Zhaohui</forenames></author></authors><title>Sparse Principal Component Analysis via Rotation and Truncation</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse principal component analysis (sparse PCA) aims at finding a sparse
basis to improve the interpretability over the dense basis of PCA, meanwhile
the sparse basis should cover the data subspace as much as possible. In
contrast to most of existing work which deal with the problem by adding some
sparsity penalties on various objectives of PCA, in this paper, we propose a
new method SPCArt, whose motivation is to find a rotation matrix and a sparse
basis such that the sparse basis approximates the basis of PCA after the
rotation. The algorithm of SPCArt consists of three alternating steps: rotate
PCA basis, truncate small entries, and update the rotation matrix. Its
performance bounds are also given. SPCArt is efficient, with each iteration
scaling linearly with the data dimension. It is easy to choose parameters in
SPCArt, due to its explicit physical explanations. Besides, we give a unified
view to several existing sparse PCA methods and discuss the connection with
SPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA
algorithm, to overcome its drawback. Experimental results demonstrate that
SPCArt achieves the state-of-the-art performance. It also achieves a good
tradeoff among various criteria, including sparsity, explained variance,
orthogonality, balance of sparsity among loadings, and computational speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1437</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1437</id><created>2014-03-06</created><updated>2014-05-12</updated><authors><author><keyname>Kleineberg</keyname><forenames>Kaj-Kolja</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author></authors><title>Evolution of the digital society reveals balance between viral and mass
  media influence</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI physics.comp-ph</categories><journal-ref>Phys. Rev. X 4, 031046, 2014</journal-ref><doi>10.1103/PhysRevX.4.031046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks (OSNs) enable researchers to study the social universe
at a previously unattainable scale. The worldwide impact and the necessity to
sustain their rapid growth emphasize the importance to unravel the laws
governing their evolution. We present a quantitative two-parameter model which
reproduces the entire topological evolution of a quasi-isolated OSN with
unprecedented precision from the birth of the network. This allows us to
precisely gauge the fundamental macroscopic and microscopic mechanisms
involved. Our findings suggest that the coupling between the real pre-existing
underlying social structure, a viral spreading mechanism, and mass media
influence govern the evolution of OSNs. The empirical validation of our model,
on a macroscopic scale, reveals that virality is four to five times stronger
than mass media influence and, on a microscopic scale, individuals have a
higher subscription probability if invited by weaker social contacts, in
agreement with the &quot;strength of weak ties&quot; paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1451</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1451</id><created>2014-03-06</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Spina</keyname><forenames>Damiano</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Raquel</forenames></author><author><keyname>Fresno</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>Real-Time Classification of Twitter Trends</title><categories>cs.IR cs.CL cs.SI</categories><comments>Pre-print of article accepted for publication in Journal of the
  American Society for Information Science and Technology copyright @ 2013
  (American Society for Information Science and Technology)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media users give rise to social trends as they share about common
interests, which can be triggered by different reasons. In this work, we
explore the types of triggers that spark trends on Twitter, introducing a
typology with following four types: 'news', 'ongoing events', 'memes', and
'commemoratives'. While previous research has analyzed trending topics in a
long term, we look at the earliest tweets that produce a trend, with the aim of
categorizing trends early on. This would allow to provide a filtered subset of
trends to end users. We analyze and experiment with a set of straightforward
language-independent features based on the social spread of trends to
categorize them into the introduced typology. Our method provides an efficient
way to accurately categorize trending topics without need of external data,
enabling news organizations to discover breaking news in real-time, or to
quickly identify viral memes that might enrich marketing decisions, among
others. The analysis of social features also reveals patterns associated with
each type of trend, such as tweets about ongoing events being shorter as many
were likely sent from mobile devices, or memes having more retweets originating
from a few trend-setters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1455</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1455</id><created>2014-03-06</created><authors><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Jha</keyname><forenames>Ranjan</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>IMJ, INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Non-singular assembly mode changing trajectories in the workspace for
  the 3-RPS parallel robot</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>14th International Symposium on Advances in Robot Kinematics,
  Ljubljana : Slovenia (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having non-singular assembly modes changing trajectories for the 3-RPS
parallel robot is a well-known feature. The only known solution for defining
such trajectory is to encircle a cusp point in the joint space. In this paper,
the aspects and the characteristic surfaces are computed for each operation
mode to define the uniqueness of the domains. Thus, we can easily see in the
workspace that at least three assembly modes can be reached for each operation
mode. To validate this property, the mathematical analysis of the determinant
of the Jacobian is done. The image of these trajectories in the joint space is
depicted with the curves associated with the cusp points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1458</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1458</id><created>2014-03-06</created><updated>2014-03-07</updated><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Phase Transitions in Phase Retrieval</title><categories>cs.IT math.AG math.FA math.IT</categories><comments>Book chapter, survey of recent literature, submitted to Excursions in
  Harmonic Analysis: The February Fourier Talks at the Norbert Wiener Center</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Consider a scenario in which an unknown signal is transformed by a known
linear operator, and then the pointwise absolute value of the unknown output
function is reported. This scenario appears in several applications, and the
goal is to recover the unknown signal -- this is called phase retrieval. Phase
retrieval has been a popular subject of research in the last few years, both in
determining whether complete information is available with a given linear
operator, and in finding efficient and stable phase retrieval algorithms in the
cases where complete information is available. Interestingly, there are a few
ways to measure information completeness, and each way appears to be governed
by a phase transition of sorts. This chapter will survey the state of the art
with some of these phase transitions, and identify a few open problems for
further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1460</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1460</id><created>2014-03-06</created><authors><author><keyname>Li</keyname><forenames>Gang</forenames></author><author><keyname>Wimalajeewa</keyname><forenames>Thakshila</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Decentralized Subspace Pursuit for Joint Sparsity Pattern Recovery</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, accepted by ICASSP 2014</comments><msc-class>68W15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To solve the problem of joint sparsity pattern recovery in a decen-tralized
network, we propose an algorithm named decentralized and collaborative subspace
pursuit (DCSP). The basic idea of DCSP is to embed collaboration among nodes
and fusion strategy into each iteration of the standard subspace pursuit (SP)
algorithm. In DCSP, each node collaborates with several of its neighbors by
sharing high-dimensional coefficient estimates and communicates with other
remote nodes by exchanging low-dimensional support set estimates. Experimental
evaluations show that, compared with several existing algorithms for sparsity
pattern recovery, DCSP produces satisfactory results in terms of accuracy of
sparsity pattern recovery with much less communication cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1465</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1465</id><created>2014-03-06</created><authors><author><keyname>Molins-Ruano</keyname><forenames>P.</forenames></author><author><keyname>Gonz&#xe1;lez-Sacrist&#xe1;n</keyname><forenames>C.</forenames></author><author><keyname>D&#xed;ez</keyname><forenames>F.</forenames></author><author><keyname>Rodriguez</keyname><forenames>P.</forenames></author><author><keyname>Sacha</keyname><forenames>G. M.</forenames></author></authors><title>Adaptive Model for Computer-Assisted Assessment in Programming Skills</title><categories>cs.CY cs.HC</categories><comments>7 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we show a methodology aimed to improve the quality of the
assessment process for subjects related to basic programming. The method takes
into account the relevance of the items and the students answers to follow
different paths to improve the accuracy of the assessment process. We have
developed numerical simulations and experiments with real students that
demonstrate the advantages of this model when compared with traditional
evaluation tools. This method improves the objectiveness and takes into account
the relevance of the subject contents. We also demonstrate that the
architecture of the algorithm is fully compatible with traditional multiple
choice test formalisms. Our results can be directly used in computer-assisted
tests for different subjects and disciplines, as well as used by the students
as a self-evaluation tool with the objective of correcting their deficiencies
in the learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1468</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1468</id><created>2014-03-06</created><authors><author><keyname>Paterson</keyname><forenames>Maura B.</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author></authors><title>Optimal constructions for ID-based one-way-function key predistribution
  schemes realizing specified communication graphs</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a method for key predistribution in a network of $n$ users where
pairwise keys are computed by hashing users' IDs along with secret information
that has been (pre)distributed to the network users by a trusted entity. A
communication graph $G$ can be specified to indicate which pairs of users
should be able to compute keys. We determine necessary and sufficient
conditions for schemes of this type to be secure. We also consider the problem
of minimizing the storage requirements of such a scheme; we are interested in
the total storage as well as the maximum storage required by any user.
Minimizing the total storage is NP-hard, whereas minimizing the maximum storage
required by a user can be computed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1476</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1476</id><created>2014-03-06</created><authors><author><keyname>Bliss</keyname><forenames>Daniel W.</forenames></author></authors><title>Cooperative Radar and Communications Signaling: The Estimation and
  Information Theory Odd Couple</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, to be presented at 2014 IEEE Radar Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate cooperative radar and communications signaling. While each
system typically considers the other system a source of interference, by
considering the radar and communications operations to be a single joint
system, the performance of both systems can, under certain conditions, be
improved by the existence of the other. As an initial demonstration, we focus
on the radar as relay scenario and present an approach denoted multiuser
detection radar (MUDR). A novel joint estimation and information theoretic
bound formulation is constructed for a receiver that observes communications
and radar return in the same frequency allocation. The joint performance bound
is presented in terms of the communication rate and the estimation rate of the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1477</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1477</id><created>2014-03-06</created><updated>2014-03-30</updated><authors><author><keyname>M&#xf8;gelberg</keyname><forenames>Rasmus Ejlers</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Staton</keyname><forenames>Sam</forenames><affiliation>Computer Laboratory, University of Cambridge</affiliation></author></authors><title>Linear usage of state</title><categories>cs.PL</categories><comments>This article expands on a paper presented at the Fourth International
  Conference on Algebra and Coalgebra in Computer Science (CALCO 2011)</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 1 (March 25,
  2014) lmcs:743</journal-ref><doi>10.2168/LMCS-10(1:17)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the phenomenon that &quot;every monad is a linear state monad&quot;. We
do this by studying a fully-complete state-passing translation from an impure
call-by-value language to a new linear type theory: the enriched call-by-value
calculus. The results are not specific to store, but can be applied to any
computational effect expressible using algebraic operations, even to effects
that are not usually thought of as stateful. There is a bijective
correspondence between generic effects in the source language and state access
operations in the enriched call-by-value calculus. From the perspective of
categorical models, the enriched call-by-value calculus suggests a refinement
of the traditional Kleisli models of effectful call-by-value languages. The new
models can be understood as enriched adjunctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1486</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1486</id><created>2014-03-06</created><authors><author><keyname>Haralabopoulos</keyname><forenames>Giannis</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Lifespan and propagation of information in On-line Social Networks a
  Case Study</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>32 Pages, 13 Figures, 6 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since 1950, information flows have been in the centre of scientific research.
Up until internet penetration in the late 90s, these studies were based over
traditional offline social networks. Several observations in offline
information flows studies, such as two-step flow of communication and the
importance of weak ties, were verified in several online studies, showing that
the diffused information flows from one Online Social Network (OSN) to several
others. Within that flow, information is shared to and reproduced by the users
of each network. Furthermore, the original content is enhanced or weakened
according to its topic, the dynamic and exposure of each OSNs. In such a
concept, each OSN is considered a layer of information flows that interacts
with each other. In this paper, we examine such flows in several social
networks, as well as their diffusion and lifespan across multiple OSNs, in
terms of user-generated content. Our results verify the perception of content
and information connection in various OSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1497</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1497</id><created>2014-03-06</created><authors><author><keyname>Lopes</keyname><forenames>Manuel</forenames></author><author><keyname>Montesano</keyname><forenames>Luis</forenames></author></authors><title>Active Learning for Autonomous Intelligent Agents: Exploration,
  Curiosity, and Interaction</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this survey we present different approaches that allow an intelligent
agent to explore autonomous its environment to gather information and learn
multiple tasks. Different communities proposed different solutions, that are in
many cases, similar and/or complementary. These solutions include active
learning, exploration/exploitation, online-learning and social learning. The
common aspect of all these approaches is that it is the agent to selects and
decides what information to gather next. Applications for these approaches
already include tutoring systems, autonomous grasping learning, navigation and
mapping and human-robot interaction. We discuss how these approaches are
related, explaining their similarities and their differences in terms of
problem assumptions and metrics of success. We consider that such an integrated
discussion will improve inter-disciplinary research and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1501</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1501</id><created>2014-03-06</created><authors><author><keyname>Hage</keyname><forenames>Clemens</forenames></author><author><keyname>Habigt</keyname><forenames>Tim</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Sparse DOA Estimation of Wideband Sound Sources Using Circular Harmonics</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse signal models are in the focus of recent developments in narrowband
DOA estimation. Applying these methods to localizing audio sources, however, is
challenging due to the wideband nature of the signals. The common approach of
processing all frequency bands separately and fusing the results is costly and
can introduce errors in the solution. We show how these problems can be
overcome by decomposing the wavefield of a circular microphone array and using
circular harmonic coefficients instead of time-frequency data for sparse DOA
estimation. As a result, we present the super-resolution localization method
WASCHL (Wideband Audio Sparse Circular Harmonics Localizer) that is inherently
frequency-coherent and highly efficient from a computational point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1508</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1508</id><created>2014-03-06</created><updated>2014-05-05</updated><authors><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Frederiksen</keyname><forenames>S&#xf8;ren Kristoffer Stiil</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Social welfare in one-sided matchings: Random priority and beyond</title><categories>cs.GT</categories><comments>13 pages</comments><acm-class>J.4; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of approximate social welfare maximization (without
money) in one-sided matching problems when agents have unrestricted cardinal
preferences over a finite set of items. Random priority is a very well-known
truthful-in-expectation mechanism for the problem. We prove that the
approximation ratio of random priority is Theta(n^{-1/2}) while no
truthful-in-expectation mechanism can achieve an approximation ratio better
than O(n^{-1/2}), where n is the number of agents and items. Furthermore, we
prove that the approximation ratio of all ordinal (not necessarily
truthful-in-expectation) mechanisms is upper bounded by O(n^{-1/2}), indicating
that random priority is asymptotically the best truthful-in-expectation
mechanism and the best ordinal mechanism for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1512</identifier>
 <datestamp>2014-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1512</id><created>2014-03-06</created><updated>2014-08-04</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author></authors><title>Parameterized Complexity of the $k$-Arc Chinese Postman Problem</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Mixed Chinese Postman Problem (MCPP), given an edge-weighted mixed
graph $G$ ($G$ may have both edges and arcs), our aim is to find a minimum
weight closed walk traversing each edge and arc at least once. The MCPP
parameterized by the number of edges was known to be fixed-parameter tractable
using a simple argument. Solving an open question of van Bevern et al., we
prove that the MCPP parameterized by the number of arcs is also fixed-parameter
tractable. Our proof is more involved and, in particular, uses a well-known
result of Marx, O'Sullivan and Razgon (2013) on the treewidth of torso graphs
with respect to small separators. We obtain a small cut analog of this result,
and use it to construct a tree decomposition which, despite not having bounded
width, has other properties allowing us to design a fixed-parameter algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1515</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1515</id><created>2014-03-06</created><updated>2014-10-09</updated><authors><author><keyname>Cao</keyname><forenames>Yixin</forenames></author></authors><title>Linear Recognition of Almost Interval Graphs</title><categories>cs.DM cs.DS q-bio.GN</categories><comments>Completely restructured, and results on unit interval graphs have
  been dropped to make this version more focused</comments><msc-class>05C75, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mbox{interval} + k v$, $\mbox{interval} + k e$, and $\mbox{interval} -
k e$ denote the classes of graphs that can be obtained from some interval graph
by adding $k$ vertices, adding $k$ edges, and deleting $k$ edges, respectively.
When $k$ is small, these graph classes are called almost interval graphs. They
are well motivated from computational biology, where the data ought to be
represented by an interval graph while we can only expect an almost interval
graph for the best. For any fixed $k$, we give linear-time algorithms for
recognizing all these classes, and in the case of membership, our algorithms
provide also a specific interval graph as evidence. When $k$ is part of the
input, these problems are also known as graph modification problems, all
NP-complete. Our results imply that they are fixed-parameter tractable
parameterized by $k$, thereby resolving the long-standing open problem on the
parameterized complexity of recognizing $\mbox{interval}+ k e$, first asked by
Bodlaender et al. [Bioinformatics, 11:49--57, 1995]. Moreover, our algorithms
for recognizing $\mbox{interval}+ k v$ and $\mbox{interval}- k e$ run in times
$O(6^k \cdot (n + m))$ and $O(8^k \cdot (n + m))$, (where $n$ and $m$ stand for
the numbers of vertices and edges respectively in the input graph,)
significantly improving the $O(k^{2k}\cdot n^3m)$-time algorithm of Heggernes
et al. [STOC 2007] and the $O(10^k \cdot n^9)$-time algorithm of Cao and Marx
[SODA 2014] respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1521</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1521</id><created>2014-03-06</created><authors><author><keyname>Helmke</keyname><forenames>Ian</forenames></author><author><keyname>Kreymer</keyname><forenames>Daniel</forenames></author><author><keyname>Wiegand</keyname><forenames>Karl</forenames></author></authors><title>Approximation Models of Combat in StarCraft 2</title><categories>cs.AI</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time strategy (RTS) games make heavy use of artificial intelligence
(AI), especially in the design of computerized opponents. Because of the
computational complexity involved in managing all aspects of these games, many
AI opponents are designed to optimize only a few areas of playing style. In
games like StarCraft 2, a very popular and recently released RTS, most AI
strategies revolve around economic and building efficiency: AI opponents try to
gather and spend all resources as quickly and effectively as possible while
ensuring that no units are idle. The aim of this work was to help address the
need for AI combat strategies that are not computationally intensive. Our goal
was to produce a computationally efficient model that is accurate at predicting
the results of complex battles between diverse armies, including which army
will win and how many units will remain. Our results suggest it may be possible
to develop a relatively simple approximation model of combat that can
accurately predict many battles that do not involve micromanagement. Future
designs of AI opponents may be able to incorporate such an approximation model
into their decision and planning systems to provide a challenge that is
strategically balanced across all aspects of play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1523</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1523</id><created>2014-03-06</created><updated>2014-06-27</updated><authors><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author><author><keyname>Yin</keyname><forenames>Xuemeng E.</forenames></author><author><keyname>Wang</keyname><forenames>Jiasong</forenames></author></authors><title>A Novel Method for Comparative Analysis of DNA Sequences by
  Ramanujan-Fourier Transform</title><categories>cs.CE cs.AI</categories><comments>Ramanujan-Fourier transform and DNA sequences</comments><msc-class>42A16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alignment-free sequence analysis approaches provide important alternatives
over multiple sequence alignment (MSA) in biological sequence analysis because
alignment-free approaches have low computation complexity and are not dependent
on high level of sequence identity, however, most of the existing
alignment-free methods do not employ true full information content of sequences
and thus can not accurately reveal similarities and differences among DNA
sequences. We present a novel alignment-free computational method for sequence
analysis based on Ramanujan-Fourier transform (RFT), in which complete
information of DNA sequences is retained. We represent DNA sequences as four
binary indicator sequences and apply RFT on the indicator sequences to convert
them into frequency domain. The Euclidean distance of the complete RFT
coefficients of DNA sequences are used as similarity measure. To address the
different lengths in Euclidean space of RFT coefficients, we pad zeros to short
DNA binary sequences so that the binary sequences equal the longest length in
the comparison sequence data. Thus, the DNA sequences are compared in the same
dimensional frequency space without information loss. We demonstrate the
usefulness of the proposed method by presenting experimental results on
hierarchical clustering of genes and genomes. The proposed method opens a new
channel to biological sequence analysis, classification, and structural module
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1528</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1528</id><created>2014-03-06</created><updated>2014-06-22</updated><authors><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author><author><keyname>Qiu</keyname><forenames>Judy</forenames></author><author><keyname>Luckow</keyname><forenames>Andre</forenames></author><author><keyname>Mantha</keyname><forenames>Pradeep</forenames></author><author><keyname>Fox</keyname><forenames>Geoffrey C.</forenames></author></authors><title>A Tale of Two Data-Intensive Paradigms: Applications, Abstractions, and
  Architectures</title><categories>cs.DC</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific problems that depend on processing large amounts of data require
overcoming challenges in multiple areas: managing large-scale data
distribution, co-placement and scheduling of data with compute resources, and
storing and transferring large volumes of data. We analyze the ecosystems of
the two prominent paradigms for data-intensive applications, hereafter referred
to as the high-performance computing and the Apache-Hadoop paradigm. We propose
a basis, common terminology and functional factors upon which to analyze the
two approaches of both paradigms. We discuss the concept of &quot;Big Data Ogres&quot;
and their facets as means of understanding and characterizing the most common
application workloads found across the two paradigms. We then discuss the
salient features of the two paradigms, and compare and contrast the two
approaches. Specifically, we examine common implementation/approaches of these
paradigms, shed light upon the reasons for their current &quot;architecture&quot; and
discuss some typical workloads that utilize them. In spite of the significant
software distinctions, we believe there is architectural similarity. We discuss
the potential integration of different implementations, across the different
levels and components. Our comparison progresses from a fully qualitative
examination of the two paradigms, to a semi-quantitative methodology. We use a
simple and broadly used Ogre (K-means clustering), characterize its performance
on a range of representative platforms, covering several implementations from
both paradigms. Our experiments provide an insight into the relative strengths
of the two paradigms. We propose that the set of Ogres will serve as a
benchmark to evaluate the two paradigms along different dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1541</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1541</id><created>2014-03-06</created><authors><author><keyname>Davoodi</keyname><forenames>Arash Gholami</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Aligned Image Sets under Channel Uncertainty: Settling a Conjecture by
  Lapidoth, Shamai and Wigger on the Collapse of Degrees of Freedom under
  Finite Precision CSIT</title><categories>cs.IT math.IT</categories><comments>27 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conjecture made by Lapidoth, Shamai and Wigger at Allerton 2005 (also an
open problem presented at ITA 2006) states that the DoF of a 2 user broadcast
channel, where the transmitter is equipped with 2 antennas and each user is
equipped with 1 antenna, must collapse under finite precision CSIT. In this
work we prove that the conjecture is true in all non-degenerate settings (e.g.,
where the probability density function of unknown channel coefficients exists
and is bounded). The DoF collapse even when perfect channel knowledge for one
user is available to the transmitter. This also settles a related recent
conjecture by Tandon et al. The key to our proof is a bound on the number of
codewords that can cast the same image (within noise distortion) at the
undesired receiver whose channel is subject to finite precision CSIT, while
remaining resolvable at the desired receiver whose channel is precisely known
by the transmitter. We are also able to generalize the result along two
directions. First, if the peak of the probability density function is allowed
to scale as O(P^(\alpha/2)), representing the concentration of probability
density (improving CSIT) due to, e.g., quantized feedback at rate
(\alpha/2)\log(P), then the DoF are bounded above by 1+\alpha, which is also
achievable under quantized feedback. Second, we generalize the result to the K
user broadcast channel with K antennas at the transmitter and a single antenna
at each receiver. Here also the DoF collapse under non-degenerate channel
uncertainty. The result directly implies a collapse of DoF to unity under
non-degenerate channel uncertainty for the general K-user interference and MxN
user X networks as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1546</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1546</id><created>2014-03-06</created><updated>2015-09-14</updated><authors><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Measuring and modelling correlations in multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 16 figures, 6 tables</comments><journal-ref>Phys. Rev. E 92, 032805 (2015)</journal-ref><doi>10.1103/PhysRevE.92.032805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interactions among the elementary components of many complex systems can
be qualitatively different. Such systems are therefore naturally described in
terms of multiplex or multi-layer networks, i.e. networks where each layer
stands for a different type of interaction between the same set of nodes. There
is today a growing interest in understanding when and why a description in
terms of a multiplex network is necessary and more informative than a
single-layer projection. Here, we contribute to this debate by presenting a
comprehensive study of correlations in multiplex networks. Correlations in node
properties, especially degree-degree correlations, have been thoroughly studied
in single-layer networks. Here we extend this idea to investigate and
characterize correlations between the different layers of a multiplex network.
Such correlations are intrinsically multiplex, and we first study them
empirically by constructing and analyzing several multiplex networks from the
real-world. In particular, we introduce various measures to characterize
correlations in the activity of the nodes and in their degree at the different
layers, and between activities and degrees. We show that real-world networks
exhibit indeed non-trivial multiplex correlations. For instance, we find cases
where two layers of the same multiplex network are positively correlated in
terms of node degrees, while other two layers are negatively correlated. We
then focus on constructing synthetic multiplex networks, proposing a series of
models to reproduce the correlations observed empirically and/or to assess
their relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1556</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1556</id><created>2014-03-05</created><authors><author><keyname>Eger</keyname><forenames>Steffen</forenames></author></authors><title>Corrections to the results derived in &quot;A Unified Approach to Algorithms
  Generating Unrestricted and Restricted Integer Compositions and Integer
  Partitions&quot;'; and a comparison of four restricted integer composition
  generation algorithms</title><categories>cs.DS cs.DM</categories><comments>This will remain an unpublished note. The journal wherein the
  original article appeared (the one which is being criticized) no longer
  exists (in its original form). Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, I discuss results on integer compositions/partitions given in
the paper &quot;A Unified Approach to Algorithms Generating Unrestricted and
Restricted Integer Compositions and Integer Partitions&quot;. I also experiment with
four different generation algorithms for restricted integer compositions and
find the algorithm designed in the named paper to be pretty slow,
comparatively.
  Some of my comments may be subjective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1569</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1569</id><created>2014-03-06</created><updated>2014-06-11</updated><authors><author><keyname>Sodagari</keyname><forenames>Shabnam</forenames></author></authors><title>On Effects of Imperfect Channel State Information on Null Space Based
  Cognitive MIMO Communication</title><categories>cs.IT math.IT</categories><journal-ref>Computing, Networking and Communications (ICNC), 2015
  International Conference on, vol., no., pp.438,444, 16-19 Feb. 2015</journal-ref><doi>10.1109/ICCNC.2015.7069384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks, when secondary users transmit in the null space
of their interference channel with primary user, they can avoid interference.
However, performance of this scheme depends on knowledge of channel state
information for secondary user to perform inverse waterfilling. We evaluate the
effects of imperfect channel estimation on error rates and performance
degradation of primary user and elucidate the tradeoffs, such as amount of
interference and guard distance. Results show that, based on the amount of
perturbation in channel matrices, performance of null space based technique can
degrade to that of open loop MIMO. Outcomes presented in this paper also apply
to null space based MIMO radar waveform design to avoid interference with
commercial communication systems, operating in same or adjacent bands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1572</identifier>
 <datestamp>2014-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1572</id><created>2014-03-06</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Antonioni</keyname><forenames>Alberto</forenames></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Binary birth-death dynamics and the expansion of cooperation by means of
  self-organized growth</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>6 two-column pages, 6 figures; accepted for publication in
  Europhysics Letters</comments><journal-ref>EPL 105 (2014) 48001</journal-ref><doi>10.1209/0295-5075/105/48001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural selection favors the more successful individuals. This is the
elementary premise that pervades common models of evolution. Under extreme
conditions, however, the process may no longer be probabilistic. Those that
meet certain conditions survive and may reproduce while others perish. By
introducing the corresponding binary birth-death dynamics to spatial
evolutionary games, we observe solutions that are fundamentally different from
those reported previously based on imitation dynamics. Social dilemmas
transform to collective enterprises, where the availability of free expansion
ranges and limited exploitation possibilities dictates self-organized growth.
Strategies that dominate are those that are collectively most apt in meeting
the survival threshold, rather than those who succeed in exploiting others for
unfair benefits. Revisiting Darwinian principles with the focus on survival
rather than imitation thus reveals the most counterintuitive ways of
reconciling cooperation with competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1591</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1591</id><created>2014-03-06</created><updated>2014-12-26</updated><authors><author><keyname>Zhan</keyname><forenames>Jinchun</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>Robust PCA with Partial Subspace Knowledge</title><categories>cs.IT math.IT</categories><comments>19 pages, 9 figures, submitted to IEEE Transaction on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, robust Principal Components Analysis (PCA) has been posed as
a problem of recovering a low-rank matrix $\mathbf{L}$ and a sparse matrix
$\mathbf{S}$ from their sum, $\mathbf{M}:= \mathbf{L} + \mathbf{S}$ and a
provably exact convex optimization solution called PCP has been proposed. This
work studies the following problem. Suppose that we have partial knowledge
about the column space of the low rank matrix $\mathbf{L}$. Can we use this
information to improve the PCP solution, i.e. allow recovery under weaker
assumptions? We propose here a simple but useful modification of the PCP idea,
called modified-PCP, that allows us to use this knowledge. We derive its
correctness result which shows that, when the available subspace knowledge is
accurate, modified-PCP indeed requires significantly weaker incoherence
assumptions than PCP. Extensive simulations are also used to illustrate this.
Comparisons with PCP and other existing work are shown for a stylized real
application as well. Finally, we explain how this problem naturally occurs in
many applications involving time series data, i.e. in what is called the online
or recursive robust PCA problem. A corollary for this case is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1596</identifier>
 <datestamp>2014-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1596</id><created>2014-03-06</created><updated>2014-03-19</updated><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris</forenames></author><author><keyname>Bjornson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Energy Consumption in multi-user MIMO systems: Impact of user mobility</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, 1 table, conference. IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the downlink of a single-cell multi-user
multiple-input multiple-output system in which zero-forcing precoding is used
at the base station (BS) to serve a certain number of user equipments (UEs). A
fixed data rate is guaranteed at each UE. The UEs move around in the cell
according to a Brownian motion, thus the path losses change over time and the
energy consumption fluctuates accordingly. We aim at determining the
distribution of the energy consumption. To this end, we analyze the asymptotic
regime where the number of antennas at the BS and the number of UEs grow large
with a given ratio. It turns out that the energy consumption is asymptotically
a Gaussian random variable whose mean and variance are derived analytically.
These results can, for example, be used to approximate the probability that a
battery-powered BS runs out of energy within a certain time period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1600</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1600</id><created>2014-03-06</created><authors><author><keyname>Zhu</keyname><forenames>Kai</forenames></author><author><keyname>Wu</keyname><forenames>Rui</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Collaborative Filtering with Information-Rich and Information-Sparse
  Entities</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a popular model for collaborative filtering in
recommender systems where some users of a website rate some items, such as
movies, and the goal is to recover the ratings of some or all of the unrated
items of each user. In particular, we consider both the clustering model, where
only users (or items) are clustered, and the co-clustering model, where both
users and items are clustered, and further, we assume that some users rate many
items (information-rich users) and some users rate only a few items
(information-sparse users). When users (or items) are clustered, our algorithm
can recover the rating matrix with $\omega(MK \log M)$ noisy entries while $MK$
entries are necessary, where $K$ is the number of clusters and $M$ is the
number of items. In the case of co-clustering, we prove that $K^2$ entries are
necessary for recovering the rating matrix, and our algorithm achieves this
lower bound within a logarithmic factor when $K$ is sufficiently large. We
compare our algorithms with a well-known algorithms called alternating
minimization (AM), and a similarity score-based algorithm known as the
popularity-among-friends (PAF) algorithm by applying all three to the MovieLens
and Netflix data sets. Our co-clustering algorithm and AM have similar overall
error rates when recovering the rating matrix, both of which are lower than the
error rate under PAF. But more importantly, the error rate of our co-clustering
algorithm is significantly lower than AM and PAF in the scenarios of interest
in recommender systems: when recommending a few items to each user or when
recommending items to users who only rated a few items (these users are the
majority of the total user population). The performance difference increases
even more when noise is added to the datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1615</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1615</id><created>2014-03-06</created><authors><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Majid</keyname><forenames>Arslan Javaid</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author><author><keyname>Doyle</keyname><forenames>Linda E.</forenames></author><author><keyname>Farhang-Boroujeny</keyname><forenames>Behrouz</forenames></author></authors><title>Interference Localization for Uplink OFDMA Systems in Presence of CFOs</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE WCNC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple carrier frequency offsets (CFOs) present in the uplink of orthogonal
frequency division multiple access (OFDMA) systems adversely affect subcarrier
orthogonality and impose a serious performance loss. In this paper, we propose
the application of time domain receiver windowing to concentrate the leakage
caused by CFOs to a few adjacent subcarriers with almost no additional
computational complexity. This allows us to approximate the interference matrix
with a quasi-banded matrix by neglecting small elements outside a certain band
which enables robust and computationally efficient signal detection. The
proposed CFO compensation technique is applicable to all types of subcarrier
assignment techniques. Simulation results show that the quasi-banded
approximation of the interference matrix is accurate enough to provide almost
the same bit error rate performance as that of the optimal solution. The
excellent performance of our proposed method is also proven through running an
experiment using our FPGA-based system setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1618</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1618</id><created>2014-03-06</created><authors><author><keyname>Mahmoodi</keyname><forenames>Maryam</forenames></author><author><keyname>Varnamkhasti</keyname><forenames>Mohammad Mahmoodi</forenames></author></authors><title>Design a Persian Automated Plagiarism Detector (AMZPPD)</title><categories>cs.AI cs.CL</categories><comments>3 pages, Published with International Journal of Engineering Trends
  and Technology (IJETT)Published with International Journal of Engineering
  Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V8(8),465-467 February 2014</journal-ref><doi>10.14445/22315381/IJETT-V8P280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently there are lots of plagiarism detection approaches. But few of them
implemented and adapted for Persian languages. In this paper, our work on
designing and implementation of a plagiarism detection system based on
pre-processing and NLP technics will be described. And the results of testing
on a corpus will be presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1626</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1626</id><created>2014-03-06</created><updated>2014-11-13</updated><authors><author><keyname>Lu</keyname><forenames>Zhiwu</forenames></author><author><keyname>Fu</keyname><forenames>Zhenyong</forenames></author><author><keyname>Xiang</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Wen</keyname><forenames>Ji-Rong</forenames></author></authors><title>Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a weakly supervised sparse learning approach to the
problem of noisily tagged image parsing, or segmenting all the objects within a
noisily tagged image and identifying their categories (i.e. tags). Different
from the traditional image parsing that takes pixel-level labels as strong
supervisory information, our noisily tagged image parsing is provided with
noisy tags of all the images (i.e. image-level labels), which is a natural
setting for social image collections (e.g. Flickr). By oversegmenting all the
images into regions, we formulate noisily tagged image parsing as a weakly
supervised sparse learning problem over all the regions, where the initial
labels of each region are inferred from image-level labels. Furthermore, we
develop an efficient algorithm to solve such weakly supervised sparse learning
problem. The experimental results on two benchmark datasets show the
effectiveness of our approach. More notably, the reported surprising results
shed some light on answering the question: can image-level labels replace
pixel-level labels (hard to access) as supervisory information for image
parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1628</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1628</id><created>2014-03-06</created><authors><author><keyname>Egu&#xed;a</keyname><forenames>Martiniano</forenames></author><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author></authors><title>Disimplicial arcs, transitive vertices, and disimplicial eliminations</title><categories>cs.DM</categories><comments>17 pags., 3 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we deal with the problems of finding the disimplicial arcs of
a digraph and recognizing some interesting graph classes defined by their
existence. A diclique of a digraph is a pair $V \to W$ of sets of vertices such
that $v \to w$ is an arc for every $v \in V$ and $w \in W$. An arc $v \to w$ is
disimplicial when $N^-(w) \to N^+(v)$ is a diclique. We show that the problem
of finding the disimplicial arcs is equivalent, in terms of time and space
complexity, to that of locating the transitive vertices. As a result, an
efficient algorithm to find the bisimplicial edges of bipartite graphs is
obtained. Then, we develop simple algorithms to build disimplicial elimination
schemes, which can be used to generate bisimplicial elimination schemes for
bipartite graphs. Finally, we study two classes related to perfect disimplicial
elimination digraphs, namely weakly diclique irreducible digraphs and diclique
irreducible digraphs. The former class is associated to finite posets, while
the latter corresponds to dedekind complete finite posets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1631</identifier>
 <datestamp>2014-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1631</id><created>2014-03-06</created><updated>2014-03-28</updated><authors><author><keyname>Tang</keyname><forenames>Adrian</forenames></author><author><keyname>Sethumadhavan</keyname><forenames>Simha</forenames></author><author><keyname>Stolfo</keyname><forenames>Salvatore</forenames></author></authors><title>Unsupervised Anomaly-based Malware Detection using Hardware Features</title><categories>cs.CR</categories><comments>1 page, Latex; added description for feature selection in Section 4,
  results unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have shown promise in using microarchitectural execution
patterns to detect malware programs. These detectors belong to a class of
detectors known as signature-based detectors as they catch malware by comparing
a program's execution pattern (signature) to execution patterns of known
malware programs. In this work, we propose a new class of detectors -
anomaly-based hardware malware detectors - that do not require signatures for
malware detection, and thus can catch a wider range of malware including
potentially novel ones. We use unsupervised machine learning to build profiles
of normal program execution based on data from performance counters, and use
these profiles to detect significant deviations in program behavior that occur
as a result of malware exploitation. We show that real-world exploitation of
popular programs such as IE and Adobe PDF Reader on a Windows/x86 platform can
be detected with nearly perfect certainty. We also examine the limits and
challenges in implementing this approach in face of a sophisticated adversary
attempting to evade anomaly-based detection. The proposed detector is
complementary to previously proposed signature-based detectors and can be used
together to improve security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1639</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1639</id><created>2014-03-06</created><authors><author><keyname>Eshghi</keyname><forenames>Soheil</forenames></author><author><keyname>Khouzani</keyname><forenames>MHR.</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author><author><keyname>Venkatesh</keyname><forenames>Santosh S.</forenames></author></authors><title>Optimal Patching in Clustered Malware Epidemics</title><categories>cs.CR cs.NI cs.SI cs.SY math.OC</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies on the propagation of malware in mobile networks have revealed that
the spread of malware can be highly inhomogeneous. Platform diversity, contact
list utilization by the malware, clustering in the network structure, etc. can
also lead to differing spreading rates. In this paper, a general formal
framework is proposed for leveraging such heterogeneity to derive optimal
patching policies that attain the minimum aggregate cost due to the spread of
malware and the surcharge of patching. Using Pontryagin's Maximum Principle for
a stratified epidemic model, it is analytically proven that in the mean-field
deterministic regime, optimal patch disseminations are simple single-threshold
policies. Through numerical simulations, the behavior of optimal patching
policies is investigated in sample topologies and their advantages are
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1642</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1642</id><created>2014-03-06</created><updated>2015-06-02</updated><authors><author><keyname>Eshghi</keyname><forenames>Soheil</forenames></author><author><keyname>Khouzani</keyname><forenames>MHR.</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author><author><keyname>Venkatesh</keyname><forenames>Santosh S.</forenames></author></authors><title>Optimal Energy-Aware Epidemic Routing in DTNs</title><categories>cs.SY cs.DC cs.NI math.OC</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the use of epidemic routing in energy
constrained Delay Tolerant Networks (DTNs). In epidemic routing, messages are
relayed by intermediate nodes at contact opportunities, i.e., when pairs of
nodes come within the transmission range of each other. Each node needs to
decide whether to forward its message upon contact with a new node based on its
own residual energy level and the age of that message. We mathematically
characterize the fundamental trade-off between energy conservation and a
measure of Quality of Service as a dynamic energy-dependent optimal control
problem. We prove that in the mean-field regime, the optimal dynamic forwarding
decisions follow simple threshold-based structures in which the forwarding
threshold for each node depends on its current remaining energy. We then
characterize the nature of this dependence. Our simulations reveal that the
optimal dynamic policy significantly outperforms heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1649</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1649</id><created>2014-03-06</created><authors><author><keyname>Gandham</keyname><forenames>Rajesh</forenames></author><author><keyname>Esler</keyname><forenames>Ken</forenames></author><author><keyname>Zhang</keyname><forenames>Yongpeng</forenames></author></authors><title>A GPU Accelerated Aggregation Algebraic Multigrid Method</title><categories>math.NA cs.NA</categories><comments>18 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient, robust and fully GPU-accelerated aggregation-based
algebraic multigrid preconditioning technique for the solution of large sparse
linear systems. These linear systems arise from the discretization of elliptic
PDEs. The method involves two stages, setup and solve. In the setup stage,
hierarchical coarse grids are constructed through aggregation of the fine grid
nodes. These aggregations are obtained using a set of maximal independent nodes
from the fine grid nodes. We use a ``fine-grain'' parallel algorithm for
finding a maximal independent set from a graph of strong negative connections.
The aggregations are combined with a piece-wise constant (unsmooth)
interpolation from the coarse grid solution to the fine grid solution, ensuring
low setup and interpolation cost. The grid independent convergence is achieved
by using recursive Krylov iterations (K-cycles) in the solve stage. An
efficient combination of K-cycles and standard multigrid V-cycles is used as
the preconditioner for Krylov iterative solvers such as generalized minimal
residual and conjugate gradient. We compare the solver performance with other
solvers based on smooth aggregation and classical algebraic multigrid methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1653</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1653</id><created>2014-03-06</created><authors><author><keyname>Killpack</keyname><forenames>Marc D.</forenames></author></authors><title>Automated Tracking and Estimation for Control of Non-rigid Cloth</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report is a summary of research conducted on cloth tracking for
automated textile manufacturing during a two semester long research course at
Georgia Tech. This work was completed in 2009. Advances in current sensing
technology such as the Microsoft Kinect would now allow me to relax certain
assumptions and generally improve the tracking performance. This is because a
major part of my approach described in this paper was to track features in a 2D
image and use these to estimate the cloth deformation. Innovations such as the
Kinect would improve estimation due to the automatic depth information obtained
when tracking 2D pixel locations. Additionally, higher resolution camera images
would probably give better quality feature tracking. However, although I would
use different technology now to implement this tracker, the algorithm described
and implemented in this paper is still a viable approach which is why I am
publishing this as a tech report for reference. In addition, although the
related work is a bit exhaustive, it will be useful to a reader who is new to
methods for tracking and estimation as well as modeling of cloth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1655</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1655</id><created>2014-03-07</created><authors><author><keyname>Pavithra</keyname><forenames>G.</forenames></author><author><keyname>Devaki</keyname><forenames>P.</forenames></author></authors><title>Link and Location Based Routing Mechanism for Energy Efficiency in
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 pages, 3 figures</comments><journal-ref>IJETT Journal, Volume-8 Number-4, Year of Publication : 2014</journal-ref><doi>10.14445/22315381/IJETT-V8P239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless Sensor Networks, sensed data are reported to the sink by the
available nodes in the communication range. The sensed data should be reported
to the sink with the frequency expected by the sink. In order to have a
communication between source and sink, Link based routing is used. Link based
routing aims to achieve an energy efficient and reliable routing path. This
mechanism considers the status (current energy level in terms of Joules) of
each node, link condition (number of transmissions that the Cluster Head
(CH)and the Gateway (GW) candidates conducts) and the transmit power (power
required for transmission in terms of Joules). A metric called Predicted
Transmission Count (PTX) for each node is calculated using its status, link
condition and transmit power. The node which has highest PTX will have the
highest priority and it will be the potential candidate to act as CH or GW.
Thus the selection of proper CH or GW reduces the energy consumption, and the
network lifetime is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1660</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1660</id><created>2014-03-07</created><authors><author><keyname>Soorma</keyname><forenames>Neha</forenames><affiliation>M.TECH</affiliation></author><author><keyname>Singh</keyname><forenames>Jaikaran</forenames><affiliation>Department of Electronics and Communication, SSSIST, Sehore, M.P. India</affiliation></author><author><keyname>Tiwari</keyname><forenames>Mukesh</forenames><affiliation>Department of Electronics and Communication, SSSIST, Sehore, M.P. India</affiliation></author></authors><title>Feature Extraction of ECG Signal Using HHT Algorithm</title><categories>cs.CV</categories><comments>7 pages,&quot;Published with International Journal of Engineering Trends
  and Technology (IJETT)&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describe the features extraction algorithm for electrocardiogram
(ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signal
for an individual human being is different due to unique heart structure. The
purpose of feature extraction of ECG signal would allow successful abnormality
detection and efficient prognosis due to heart disorder. Some major important
features will be extracted from ECG signals such as amplitude, duration,
pre-gradient, post-gradient and so on. Therefore, we need a strong mathematical
model to extract such useful parameter. Here an adaptive mathematical analysis
model is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huang
transform, is implemented to analyze the non-linear and nonstationary data. It
is unique and different from the existing methods of data analysis and does not
require an a priori functional basis. The effectiveness of the proposed scheme
is verified through the simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1666</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1666</id><created>2014-03-07</created><authors><author><keyname>Li</keyname><forenames>Jianwen</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Pu</keyname><forenames>Geguang</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author><author><keyname>He</keyname><forenames>Jifeng</forenames></author></authors><title>LTLf satisfiability checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider here Linear Temporal Logic (LTL) formulas interpreted over
\emph{finite} traces. We denote this logic by LTLf. The existing approach for
LTLf satisfiability checking is based on a reduction to standard LTL
satisfiability checking. We describe here a novel direct approach to LTLf
satisfiability checking, where we take advantage of the difference in the
semantics between LTL and LTLf. While LTL satisfiability checking requires
finding a \emph{fair cycle} in an appropriate transition system, here we need
to search only for a finite trace. This enables us to introduce specialized
heuristics, where we also exploit recent progress in Boolean SAT solving. We
have implemented our approach in a prototype tool and experiments show that our
approach outperforms existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1687</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1687</id><created>2014-03-07</created><authors><author><keyname>SIfre</keyname><forenames>Laurent</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Rigid-Motion Scattering for Texture Classification</title><categories>cs.CV</categories><comments>19 pages, submitted to International Journal of Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rigid-motion scattering computes adaptive invariants along translations and
rotations, with a deep convolutional network. Convolutions are calculated on
the rigid-motion group, with wavelets defined on the translation and rotation
variables. It preserves joint rotation and translation information, while
providing global invariants at any desired scale. Texture classification is
studied, through the characterization of stationary processes from a single
realization. State-of-the-art results are obtained on multiple texture data
bases, with important rotation and scaling variabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1694</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1694</id><created>2014-03-07</created><authors><author><keyname>Petrov</keyname><forenames>Anton</forenames></author></authors><title>Methods of executable code protection</title><categories>cs.PL cs.CR</categories><comments>11 pages, 5 figures</comments><msc-class>00-02</msc-class><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article deals with the problems in constructing a protection system of
executable code. The techniques of breaking the integrity of executable code
and ways to eliminate them are described. The adoption of virtual machine
technology in the context of executable code protection from analysis is
considered. The substantiation of the application of virtual machines as the
best way to oppose the analysis of executable code is made. The protection of
executable code by transferring the protected code in a virtual execution
environment is considered. An efficient implementation of the method is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1696</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1696</id><created>2014-03-07</created><authors><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Roumy</keyname><forenames>Aline</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Exact Performance Analysis of the Oracle Receiver for Compressed Sensing
  Reconstruction</title><categories>cs.IT math.IT</categories><comments>To be published in ICASSP 2014 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sparse or compressible signal can be recovered from a certain number of
noisy random projections, smaller than what dictated by classic Shannon/Nyquist
theory. In this paper, we derive the closed-form expression of the mean square
error performance of the oracle receiver, knowing the sparsity pattern of the
signal. With respect to existing bounds, our result is exact and does not
depend on a particular realization of the sensing matrix. Moreover, our result
holds irrespective of whether the noise affecting the measurements is white or
correlated. Numerical results show a perfect match between equations and
simulations, confirming the validity of the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1697</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1697</id><created>2014-03-07</created><authors><author><keyname>Kuiteing</keyname><forenames>Simeon Kamdem</forenames></author><author><keyname>Coluccia</keyname><forenames>Giulio</forenames></author><author><keyname>Barducci</keyname><forenames>Alessandro</forenames></author><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Compressive Hyperspectral Imaging Using Progressive Total Variation</title><categories>cs.IT cs.CV math.IT</categories><comments>To be published on ICASSP 2014 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) is suitable for remote acquisition of hyperspectral
images for earth observation, since it could exploit the strong spatial and
spectral correlations, llowing to simplify the architecture of the onboard
sensors. Solutions proposed so far tend to decouple spatial and spectral
dimensions to reduce the complexity of the reconstruction, not taking into
account that onboard sensors progressively acquire spectral rows rather than
acquiring spectral channels. For this reason, we propose a novel progressive CS
architecture based on separate sensing of spectral rows and joint
reconstruction employing Total Variation. Experimental results run on raw
AVIRIS and AIRS images confirm the validity of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1706</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1706</id><created>2014-03-07</created><authors><author><keyname>K&#xf6;ster</keyname><forenames>Johannes</forenames></author><author><keyname>Rahmann</keyname><forenames>Sven</forenames></author></authors><title>Massively parallel read mapping on GPUs with PEANUT</title><categories>cs.DS cs.DC q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present PEANUT (ParallEl AligNment UTility), a highly parallel GPU-based
read mapper with several distinguishing features, including a novel q-gram
index (called the q-group index) with small memory footprint built on-the-fly
over the reads and the possibility to output both the best hits or all hits of
a read. Designing the algorithm particularly for the GPU architecture, we were
able to reach maximum core occupancy for several key steps. Our benchmarks show
that PEANUT outperforms other state-of- the-art mappers in terms of speed and
sensitivity. The software is available at http://peanut.readthedocs.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1722</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1722</id><created>2014-03-07</created><authors><author><keyname>D'Angeli</keyname><forenames>Daniele</forenames></author><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author></authors><title>A geometric approach to (semi)-groups defined by automata via dual
  transducers</title><categories>math.GR cs.FL</categories><msc-class>20E08, 20M35, 68Q45, 05C63</msc-class><journal-ref>Geometriae Dedicata February 2015, Volume 174, Issue 1, pp 375-400</journal-ref><doi>10.1007/s10711-014-0024-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a geometric approach to groups defined by automata via the notion of
enriched dual of an inverse transducer. Using this geometric correspondence we
first provide some finiteness results, then we consider groups generated by the
dual of Cayley type of machines. Lastly, we address the problem of the study of
the action of these groups in the boundary. We show that examples of groups
having essentially free actions without critical points lie in the class of
groups defined by the transducers whose enriched dual generate a torsion-free
semigroup. Finally, we provide necessary and sufficient conditions to have
finite Schreier graphs on the boundary yielding to the decidability of the
algorithmic problem of checking the existence of Schreier graphs on the
boundary whose cardinalities are upper bounded by some fixed integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1727</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1727</id><created>2014-03-07</created><authors><author><keyname>Kamada</keyname><forenames>Yukihiro</forenames></author><author><keyname>Miyasaki</keyname><forenames>Kiyonori</forenames></author></authors><title>On the Sequence of State Configurations in the Garden of Eden</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous threshold element circuit networks are used to investigate the
structure of neural networks. With these circuits, as the transition functions
are threshold functions, it is necessary to consider the existence of sequences
of state configurations that cannot be transitioned. In this study, we focus on
all logical functions of four or fewer variables, and we discuss the periodic
sequences and transient series that transition from all sequences of state
configurations. Furthermore, by using the sequences of state configurations in
the Garden of Eden, we show that it is easy to obtain functions that determine
the operation of circuit networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1729</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1729</id><created>2014-03-07</created><authors><author><keyname>Aziz</keyname><forenames>Amira Sayed A.</forenames></author><author><keyname>Azar</keyname><forenames>Ahmad Taher</forenames></author><author><keyname>Hassanien</keyname><forenames>Aboul Ella</forenames></author><author><keyname>Hanafy</keyname><forenames>Sanaa Al-Ola</forenames></author></authors><title>Continuous Features Discretization for Anomaly Intrusion Detectors
  Generation</title><categories>cs.NI cs.CR cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network security is a growing issue, with the evolution of computer systems
and expansion of attacks. Biological systems have been inspiring scientists and
designs for new adaptive solutions, such as genetic algorithms. In this paper,
we present an approach that uses the genetic algorithm to generate anomaly net-
work intrusion detectors. In this paper, an algorithm propose use a
discretization method for the continuous features selected for the intrusion
detection, to create some homogeneity between values, which have different data
types. Then,the intrusion detection system is tested against the NSL-KDD data
set using different distance methods. A comparison is held amongst the results,
and it is shown by the end that this proposed approach has good results, and
recommendations is given for future experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1732</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1732</id><created>2014-03-07</created><authors><author><keyname>Munir</keyname><forenames>Jawad</forenames></author><author><keyname>Mezghani</keyname><forenames>Amine</forenames></author><author><keyname>Khawar</keyname><forenames>Haris</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>Chromatic Dispersion Compensation Using Filter Bank Based Complex-Valued
  All-Pass Filter</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long-haul transmission of 100 Gb/s without optical chromatic-dispersion
(CD) compensation provides a range of benefits regarding cost effectiveness,
power budget, and nonlinearity tolerance. The channel memory is largely
dominated by CD in this case with an intersymbol-interference spread of more
than 100 symbol durations. In this paper, we propose CD equalization technique
based on nonmaximally decimated discrete Fourier transform (NMDFT) filter bank
(FB) with non-trivial prototype filter and complex-valued infinite impulse
response (IIR) all-pass filter per sub-band. The design of the sub-band IIR
all-pass filter is based on minimizing the mean square error (MSE) in group
delay and phase cost functions in an optimization framework. Necessary
conditions are derived and incorporated in a multi-step and multi-band
optimization framework to ensure the stability of the resulting IIR filter. It
is shown that the complexity of the proposed method grows logarithmically with
the channel memory, therefore, larger CD values can be tolerated with our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1734</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1734</id><created>2014-03-07</created><updated>2015-01-29</updated><authors><author><keyname>Bastug</keyname><forenames>Mert</forenames></author><author><keyname>Petreczky</keyname><forenames>Mihaly</forenames></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames></author><author><keyname>Leth</keyname><forenames>John</forenames></author></authors><title>Model Reduction by Moment Matching for Linear Switched Systems</title><categories>cs.SY</categories><comments>Sent for publication in IEEE TAC, on October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two moment-matching methods for model reduction of linear switched systems
(LSSs) are presented. The methods are similar to the Krylov subspace methods
used for moment matching for linear systems. The more general one of the two
methods, is based on the so called &quot;nice selection&quot; of some vectors in the
reachability or observability space of the LSS. The underlying theory is
closely related to the (partial) realization theory of LSSs. In this paper, the
connection of the methods to the realization theory of LSSs is provided, and
algorithms are developed for the purpose of model reduction. Conditions for
applicability of the methods for model reduction are stated and finally the
results are illustrated on numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1735</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1735</id><created>2014-03-07</created><authors><author><keyname>Asad</keyname><forenames>Ahmed. H.</forenames></author><author><keyname>Azar</keyname><forenames>Ahmad Taher</forenames></author><author><keyname>El-Bendary</keyname><forenames>Nashwa</forenames></author><author><keyname>Hassaanien</keyname><forenames>Aboul Ella</forenames></author></authors><title>Ant Colony based Feature Selection Heuristics for Retinal Vessel
  Segmentation</title><categories>cs.NE cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Features selection is an essential step for successful data classification,
since it reduces the data dimensionality by removing redundant features.
Consequently, that minimizes the classification complexity and time in addition
to maximizing its accuracy. In this article, a comparative study considering
six features selection heuristics is conducted in order to select the best
relevant features subset. The tested features vector consists of fourteen
features that are computed for each pixel in the field of view of retinal
images in the DRIVE database. The comparison is assessed in terms of
sensitivity, specificity, and accuracy measurements of the recommended features
subset resulted by each heuristic when applied with the ant colony system.
Experimental results indicated that the features subset recommended by the
relief heuristic outperformed the subsets recommended by the other experienced
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1738</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1738</id><created>2014-03-07</created><updated>2015-09-09</updated><authors><author><keyname>De Santis</keyname><forenames>Marianna</forenames></author><author><keyname>Lucidi</keyname><forenames>Stefano</forenames></author><author><keyname>Rinaldi</keyname><forenames>Francesco</forenames></author></authors><title>A Fast Active Set Block Coordinate Descent Algorithm for
  $\ell_1$-regularized least squares</title><categories>math.OC cs.IT math.IT</categories><comments>28 pages, 5 figures</comments><msc-class>65K05, 90C25, 90C06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding sparse solutions to underdetermined systems of linear
equations arises in several applications (e.g. signal and image processing,
compressive sensing, statistical inference). A standard tool for dealing with
sparse recovery is the $\ell_1$-regularized least-squares approach that has
been recently attracting the attention of many researchers. In this paper, we
describe an active set estimate (i.e. an estimate of the indices of the zero
variables in the optimal solution) for the considered problem that tries to
quickly identify as many active variables as possible at a given point, while
guaranteeing that some approximate optimality conditions are satisfied. A
relevant feature of the estimate is that it gives a significant reduction of
the objective function when setting to zero all those variables estimated
active. This enables to easily embed it into a given globally converging
algorithmic framework. In particular, we include our estimate into a block
coordinate descent algorithm for $\ell_1$-regularized least squares, analyze
the convergence properties of this new active set method, and prove that its
basic version converges with linear rate. Finally, we report some numerical
results showing the effectiveness of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1745</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1745</id><created>2014-03-07</created><authors><author><keyname>Khaleque</keyname><forenames>Abdul</forenames></author><author><keyname>Chatterjee</keyname><forenames>Arnab</forenames></author><author><keyname>Sen</keyname><forenames>Parongama</forenames></author></authors><title>On the evolution and utility of annual citation indices</title><categories>cs.DL physics.soc-ph</categories><comments>7 pages, 8 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the statistics of citations made to the top ranked indexed journals
for Science and Social Science databases in the Journal Citation Reports using
different measures. Total annual citation and impact factor, as well as a third
measure called the annual citation rate are used to make the detailed analysis.
We observe that the distribution of the annual citation rate has an universal
feature - it shows a maximum at the rate scaled by half the average,
irrespective of how the journals are ranked, and even across Science and Social
Science journals, and fits well to log-Gumbel distribution. Correlations
between different quantities are studied and a comparative analysis of the
three measures is presented. The newly introduced annual citation rate factor
helps in understanding the effect of scaling the number of citation by the
total number of publications. The effect of the impact factor on authors
contributing to the journals as well as on editorial policies is also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1749</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1749</id><created>2014-03-07</created><authors><author><keyname>Joshi</keyname><forenames>Saurabh</forenames></author><author><keyname>Lal</keyname><forenames>Akash</forenames></author></authors><title>Automatically finding atomic regions for fixing bugs in Concurrent
  programs</title><categories>cs.SE</categories><comments>16 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technique for automatically constructing a fix for
buggy concurrent programs: given a concurrent program that does not satisfy
user-provided assertions, we infer atomic blocks that fix the program. An
atomic block protects a piece of code and ensures that it runs without
interruption from other threads. Our technique uses a verification tool as a
subroutine to find the smallest atomic regions that remove all bugs in a given
program. Keeping the atomic regions small allows for maximum concurrency. We
have implemented our approach in a tool called AtomicInf. A user of AtomicInf
can choose between strong and weak atomicity semantics for the inferred fix.
While the former is simpler to find, the latter provides more information about
the bugs that got fixed.
  We ran AtomicInf on several benchmarks and came up with the smallest and the
most precise atomic regions in all of them. We implemented an earlier technique
to our setting and observed that AtomicInf is 1.7 times faster on an average as
compared to an earlier approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1757</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1757</id><created>2014-03-07</created><updated>2015-07-21</updated><authors><author><keyname>D&#x229;bowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Hilberg Exponents: New Measures of Long Memory in the Process</title><categories>cs.IT math.IT</categories><comments>22 pages</comments><journal-ref>IEEE Transactions on Information Theory, 61(10):5716-5726, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concerns the rates of power-law growth of mutual information
computed for a stationary measure or for a universal code. The rates are called
Hilberg exponents and four such quantities are defined for each measure and
each code: two random exponents and two expected exponents. A particularly
interesting case arises for conditional algorithmic mutual information. In this
case, the random Hilberg exponents are almost surely constant on ergodic
sources and are bounded by the expected Hilberg exponents. This property is a
&quot;second-order&quot; analogue of the Shannon-McMillan-Breiman theorem, proved without
invoking the ergodic theorem. It carries over to Hilberg exponents for the
underlying probability measure via Shannon-Fano coding and Barron inequality.
Moreover, the expected Hilberg exponents can be linked for different universal
codes. Namely, if one code dominates another, the expected Hilberg exponents
are greater for the former than for the latter. The paper is concluded by an
evaluation of Hilberg exponents for certain sources such as the mixture
Bernoulli process and the Santa Fe processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1768</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1768</id><created>2014-03-07</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Lov&#xe1;sz</keyname><forenames>L&#xe1;szl&#xf3; Mikl&#xf3;s</forenames></author></authors><title>A tight lower bound for Szemer\'edi's regularity lemma</title><categories>math.CO cs.DM</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Addressing a question of Gowers, we determine the order of the tower height
for the partition size in a version of Szemer\'edi's regularity lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1773</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1773</id><created>2014-03-07</created><authors><author><keyname>Morstatter</keyname><forenames>Fred</forenames></author><author><keyname>Lubold</keyname><forenames>Nichola</forenames></author><author><keyname>Pon-Barry</keyname><forenames>Heather</forenames></author><author><keyname>Pfeffer</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Finding Eyewitness Tweets During Crises</title><categories>cs.CL cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disaster response agencies have started to incorporate social media as a
source of fast-breaking information to understand the needs of people affected
by the many crises that occur around the world. These agencies look for tweets
from within the region affected by the crisis to get the latest updates of the
status of the affected region. However only 1% of all tweets are geotagged with
explicit location information. First responders lose valuable information
because they cannot assess the origin of many of the tweets they collect. In
this work we seek to identify non-geotagged tweets that originate from within
the crisis region. Towards this, we address three questions: (1) is there a
difference between the language of tweets originating within a crisis region
and tweets originating outside the region, (2) what are the linguistic patterns
that can be used to differentiate within-region and outside-region tweets, and
(3) for non-geotagged tweets, can we automatically identify those originating
within the crisis region in real-time?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1818</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1818</id><created>2014-03-07</created><authors><author><keyname>Horan</keyname><forenames>Victoria</forenames></author><author><keyname>Hurlbert</keyname><forenames>Glenn</forenames></author></authors><title>Gray Codes and Overlap Cycles for Restricted Weight Words</title><categories>math.CO cs.DM cs.IT math.IT</categories><comments>13 pages, 2 figures</comments><msc-class>05A05 (primary) 68R15, 68W32 (secondary)</msc-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A Gray code is a listing structure for a set of combinatorial objects such
that some consistent (usually minimal) change property is maintained throughout
adjacent elements in the list. While Gray codes for m-ary strings have been
considered in the past, we provide a new, simple Gray code for fixed-weight
m-ary strings. In addition, we consider a relatively new type of Gray code
known as overlap cycles and prove basic existence results concerning overlap
cycles for fixed-weight and weight-range m-ary words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1824</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1824</id><created>2014-03-07</created><updated>2015-12-31</updated><authors><author><keyname>Meyer</keyname><forenames>Florian</forenames></author><author><keyname>Hlinka</keyname><forenames>Ondrej</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author></authors><title>Distributed Localization and Tracking of Mobile Networks Including
  Noncooperative Objects - Extended Version</title><categories>cs.IT cs.DC math.IT</categories><comments>16 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian method for distributed sequential localization of
mobile networks composed of both cooperative agents and noncooperative objects.
Our method provides a consistent combination of cooperative self-localization
(CS) and distributed tracking (DT). Multiple mobile agents and objects are
localized and tracked using measurements between agents and objects and between
agents. For a distributed operation and low complexity, we combine
particle-based belief propagation with a consensus or gossip scheme. High
localization accuracy is achieved through a probabilistic information transfer
between the CS and DT parts of the underlying factor graph. Simulation results
demonstrate significant improvements in both agent self-localization and object
localization performance compared to separate CS and DT, and very good scaling
properties with respect to the numbers of agents and objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1835</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1835</id><created>2014-03-04</created><authors><author><keyname>Colbourn</keyname><forenames>Charles J.</forenames></author><author><keyname>Horsley</keyname><forenames>Daniel</forenames></author><author><keyname>Syrotiuk</keyname><forenames>Violet R.</forenames></author></authors><title>Hierarchical Recovery in Compressive Sensing</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, 6 figures</comments><msc-class>68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A combinatorial approach to compressive sensing based on a deterministic
column replacement technique is proposed. Informally, it takes as input a
pattern matrix and ingredient measurement matrices, and results in a larger
measurement matrix by replacing elements of the pattern matrix with columns
from the ingredient matrices. This hierarchical technique yields great
flexibility in sparse signal recovery. Specifically, recovery for the resulting
measurement matrix does not depend on any fixed algorithm but rather on the
recovery scheme of each ingredient matrix. In this paper, we investigate
certain trade-offs for signal recovery, considering the computational
investment required. Coping with noise in signal recovery requires additional
conditions, both on the pattern matrix and on the ingredient measurement
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1840</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1840</id><created>2014-03-07</created><updated>2014-09-08</updated><authors><author><keyname>Gong</keyname><forenames>Yunchao</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Guo</keyname><forenames>Ruiqi</forenames></author><author><keyname>Lazebnik</keyname><forenames>Svetlana</forenames></author></authors><title>Multi-scale Orderless Pooling of Deep Convolutional Activation Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (CNN) have shown their promise as a
universal representation for recognition. However, global CNN activations lack
geometric invariance, which limits their robustness for classification and
matching of highly variable scenes. To improve the invariance of CNN
activations without degrading their discriminative power, this paper presents a
simple but effective scheme called multi-scale orderless pooling (MOP-CNN).
This scheme extracts CNN activations for local patches at multiple scale
levels, performs orderless VLAD pooling of these activations at each level
separately, and concatenates the result. The resulting MOP-CNN representation
can be used as a generic feature for either supervised or unsupervised
recognition tasks, from image classification to instance-level retrieval; it
consistently outperforms global CNN activations without requiring any joint
training of prediction layers for a particular target dataset. In absolute
terms, it achieves state-of-the-art results on the challenging SUN397 and MIT
Indoor Scenes classification datasets, and competitive results on
ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1846</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1846</id><created>2014-03-07</created><authors><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author><author><keyname>Mahbub</keyname><forenames>Sumaya</forenames></author><author><keyname>Intiaz-ud-Din</keyname><forenames>K. M.</forenames></author></authors><title>Verification of A Security Adaptive Protocol Suite Using SPIN</title><categories>cs.NI</categories><comments>3 pages, 1 Figure</comments><journal-ref>International Journal of Engineering and Technology, vol. 5, no.
  2, pp. 254-256, 2013</journal-ref><doi>10.7763/IJET.2013.V5.553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of mobile and wireless communication technologies in recent
years introduced various adaptive protocols to adapt the need for secured
communications. Security is a crucial success factor for any communication
protocols, especially in mobile environment due to its ad hoc behavior. Formal
verification plays an important role in development and application of safety
critical systems. Formalized exhausted verification techniques to analyze the
security and the safety properties of communications protocols increase and
confirm the protocol confidence. SPIN is a powerful model checker that verifies
the correctness of distributed communication models in a rigorous and automated
fashion. This short paper proposes a SPIN based formal verification approach of
a security adaptive protocol suite. The protocol suite includes a neighbor
discovery mechanism and routing protocol. Both parts of the protocol suite are
modeled into SPIN and exhaustively checked various temporal properties which
ensure the applicability of the protocol suite in real-life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1861</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1861</id><created>2014-03-07</created><updated>2014-09-02</updated><authors><author><keyname>Wang</keyname><forenames>Timothy</forenames></author><author><keyname>Jobredeaux</keyname><forenames>Romain</forenames></author><author><keyname>Pantel</keyname><forenames>Marc</forenames></author><author><keyname>Garoche</keyname><forenames>Pierre-Loic</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>Henrion</keyname><forenames>Didier</forenames></author></authors><title>Credible Autocoding of Convex Optimization Algorithms</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of modern optimization methods, coupled with increasing
computational resources, has led to the possibility of real-time optimization
algorithms acting in safety critical roles. There is a considerable body of
mathematical proofs on on-line optimization programs which can be leveraged to
assist in the development and verification of their implementation. In this
paper, we demonstrate how theoretical proofs of real-time optimization
algorithms can be used to describe functional properties at the level of the
code, thereby making it accessible for the formal methods community. The
running example used in this paper is a generic semi-definite programming (SDP)
solver. Semi-definite programs can encode a wide variety of optimization
problems and can be solved in polynomial time at a given accuracy. We describe
a top-to-down approach that transforms a high-level analysis of the algorithm
into useful code annotations. We formulate some general remarks about how such
a task can be incorporated into a convex programming autocoder. We then take a
first step towards the automatic verification of the optimization program by
identifying key issues to be adressed in future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1863</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1863</id><created>2014-03-07</created><authors><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Jonckheere</keyname><forenames>Edmond</forenames></author></authors><title>Statistical Structure Learning, Towards a Robust Smart Grid</title><categories>cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust control and maintenance of the grid relies on accurate data. Both PMUs
and state estimators are prone to false data injection attacks. Thus, it is
crucial to have a mechanism for fast and accurate detection of an agent
maliciously tampering with the data---for both preventing attacks that may lead
to blackouts, and for routine monitoring and control tasks of current and
future grids. We propose a decentralized false data injection detection scheme
based on Markov graph of the bus phase angles. We utilize the Conditional
Covariance Test (CCT) to learn the structure of the grid. Using the DC power
flow model, we show that under normal circumstances, and because of
walk-summability of the grid graph, the Markov graph of the voltage angles can
be determined by the power grid graph. Therefore, a discrepancy between
calculated Markov graph and learned structure should trigger the alarm. Local
grid topology is available online from the protection system and we exploit it
to check for mismatch. Should a mismatch be detected, we use correlation
anomaly score to detect the set of attacked nodes. Our method can detect the
most recent stealthy deception attack on the power grid that assumes knowledge
of bus-branch model of the system and is capable of deceiving the state
estimator, damaging power network observatory, control, monitoring, demand
response and pricing schemes. Specifically, under the stealthy deception
attack, the Markov graph of phase angles changes. In addition to detect a state
of attack, our method can detect the set of attacked nodes. To the best of our
knowledge, our remedy is the first to comprehensively detect this sophisticated
attack and it does not need additional hardware. Moreover, our detection scheme
is successful no matter the size of the attacked subset. Simulation of various
power networks confirms our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1866</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1866</id><created>2014-03-07</created><updated>2014-04-07</updated><authors><author><keyname>Fett</keyname><forenames>Daniel</forenames></author><author><keyname>K&#xfc;sters</keyname><forenames>Ralf</forenames></author><author><keyname>Schmitz</keyname><forenames>Guido</forenames></author></authors><title>An Expressive Model for the Web Infrastructure: Definition and
  Application to the BrowserID SSO System</title><categories>cs.CR</categories><comments>An abridged version appears in S&amp;P 2014</comments><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web constitutes a complex infrastructure and as demonstrated by numerous
attacks, rigorous analysis of standards and web applications is indispensable.
  Inspired by successful prior work, in particular the work by Akhawe et al. as
well as Bansal et al., in this work we propose a formal model for the web
infrastructure. While unlike prior works, which aim at automatic analysis, our
model so far is not directly amenable to automation, it is much more
comprehensive and accurate with respect to the standards and specifications. As
such, it can serve as a solid basis for the analysis of a broad range of
standards and applications.
  As a case study and another important contribution of our work, we use our
model to carry out the first rigorous analysis of the BrowserID system (a.k.a.
Mozilla Persona), a recently developed complex real-world single sign-on system
that employs technologies such as AJAX, cross-document messaging, and HTML5 web
storage. Our analysis revealed a number of very critical flaws that could not
have been captured in prior models. We propose fixes for the flaws, formally
state relevant security properties, and prove that the fixed system in a
setting with a so-called secondary identity provider satisfies these security
properties in our model. The fixes for the most critical flaws have already
been adopted by Mozilla and our findings have been rewarded by the Mozilla
Security Bug Bounty Program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1890</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1890</id><created>2014-03-07</created><authors><author><keyname>Hamid</keyname><forenames>Sherin Abdel</forenames></author><author><keyname>Hassanein</keyname><forenames>Hossam S.</forenames></author><author><keyname>Takahara</keyname><forenames>Glen</forenames></author></authors><title>Vehicle as a Resource (VaaR)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent vehicles are considered key enablers for intelligent
transportation systems. They are equipped with resources/components to enable
services for vehicle occupants, other vehicles on the road, and third party
recipients. In-vehicle sensors, communication modules, and on-board units with
computing and storage capabilities allow the intelligent vehicle to work as a
mobile service provider of sensing, data storage, computing, cloud, data
relaying, infotainment, and localization services. In this paper, we introduce
the concept of Vehicle as a Resource (VaaR) and shed light on the services a
vehicle can potentially provide on the road or parked. We anticipate that an
intelligent vehicle can be a significant service provider in a variety of
situations, including emergency scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1891</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1891</id><created>2014-03-07</created><updated>2014-03-12</updated><authors><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Chen</keyname><forenames>Shunbao</forenames></author><author><keyname>Kleban</keyname><forenames>Jim</forenames></author><author><keyname>Gupta</keyname><forenames>Ankur</forenames></author></authors><title>Counterfactual Estimation and Optimization of Click Metrics for Search
  Engines</title><categories>cs.LG cs.AI stat.AP stat.ML</categories><acm-class>G.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing an interactive system against a predefined online metric is
particularly challenging, when the metric is computed from user feedback such
as clicks and payments. The key challenge is the counterfactual nature: in the
case of Web search, any change to a component of the search engine may result
in a different search result page for the same query, but we normally cannot
infer reliably from search log how users would react to the new result page.
Consequently, it appears impossible to accurately estimate online metrics that
depend on user feedback, unless the new engine is run to serve users and
compared with a baseline in an A/B test. This approach, while valid and
successful, is unfortunately expensive and time-consuming. In this paper, we
propose to address this problem using causal inference techniques, under the
contextual-bandit framework. This approach effectively allows one to run
(potentially infinitely) many A/B tests offline from search log, making it
possible to estimate and optimize online metrics quickly and inexpensively.
Focusing on an important component in a commercial search engine, we show how
these ideas can be instantiated and applied, and obtain very promising results
that suggest the wide applicability of these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1893</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1893</id><created>2014-03-07</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author></authors><title>Becoming More Robust to Label Noise with Classifier Diversity</title><categories>stat.ML cs.AI cs.LG</categories><comments>37 pages, 10 tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely known in the machine learning community that class noise can be
(and often is) detrimental to inducing a model of the data. Many current
approaches use a single, often biased, measurement to determine if an instance
is noisy. A biased measure may work well on certain data sets, but it can also
be less effective on a broader set of data sets. In this paper, we present
noise identification using classifier diversity (NICD) -- a method for deriving
a less biased noise measurement and integrating it into the learning process.
To lessen the bias of the noise measure, NICD selects a diverse set of
classifiers (based on their predictions of novel instances) to determine which
instances are noisy. We examine NICD as a technique for filtering, instance
weighting, and selecting the base classifiers of a voting ensemble. We compare
NICD with several other noise handling techniques that do not consider
classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD
significantly increases the classification accuracy over the other considered
approaches and is effective across a broad set of data sets and learning
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1896</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1896</id><created>2014-03-07</created><authors><author><keyname>Ma</keyname><forenames>Weidong</forenames></author><author><keyname>Zheng</keyname><forenames>Bo</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Online Mechanism Design for Cloud Computing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the problem of online mechanism design for resources
allocation and pricing in cloud computing (RAPCC). We show that in general the
allocation problems in RAPCC are NP-hard, and therefore we focus on designing
dominant-strategy incentive compatible (DSIC) mechanisms with good competitive
ratios compared to the offline optimal allocation (with the prior knowledge
about the future jobs). We propose two kinds of DSIC online mechanisms. The
first mechanism, which is based on a greedy allocation rule and leverages a
priority function for allocation, is very fast and has a tight competitive
bound. We discuss several priority functions including exponential and linear
priority functions, and show that the former one has a better competitive
ratio. The second mechanism, which is based on a dynamic program for
allocation, also has a tight competitive ratio and performs better than the
first one when the maximum demand of cloud customers is close to the capacity
of the cloud provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1897</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1897</id><created>2014-03-07</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Kumar</keyname><forenames>B V K Vijaya</forenames></author></authors><title>On the Duality of Erasures and Defects</title><categories>cs.IT math.IT</categories><comments>40 pages, 8 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the duality of erasures and defects will be investigated by
comparing the binary erasure channel (BEC) and the binary defect channel (BDC).
The duality holds for channel capacities, capacity achieving schemes, minimum
distances, and upper bounds on the probability of failure to retrieve the
original message. Also, the binary defect and erasure channel (BDEC) will be
introduced by combining the properties of the BEC and the BDC. It will be shown
that the capacity of the BDEC can be achieved by the coding scheme that
combines the encoding for the defects and the decoding for the erasures. This
coding scheme for the BDEC has two separate redundancy parts for correcting
erasures and masking defects. Thus, we will investigate the problem of
redundancy allocation between these two parts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1902</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1902</id><created>2014-03-07</created><authors><author><keyname>Bahrampour</keyname><forenames>Soheil</forenames></author><author><keyname>Ray</keyname><forenames>Asok</forenames></author><author><keyname>Nasrabadi</keyname><forenames>Nasser M.</forenames></author><author><keyname>Jenkins</keyname><forenames>Kenneth W.</forenames></author></authors><title>Quality-based Multimodal Classification Using Tree-Structured Sparsity</title><categories>cs.CV</categories><comments>To Appear in 2014 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2014)</comments><journal-ref>CVPR 2014, pp. 4114 - 4121</journal-ref><doi>10.1109/CVPR.2014.524</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have demonstrated advantages of information fusion based on
sparsity models for multimodal classification. Among several sparsity models,
tree-structured sparsity provides a flexible framework for extraction of
cross-correlated information from different sources and for enforcing group
sparsity at multiple granularities. However, the existing algorithm only solves
an approximated version of the cost functional and the resulting solution is
not necessarily sparse at group levels. This paper reformulates the
tree-structured sparse model for multimodal classification task. An accelerated
proximal algorithm is proposed to solve the optimization problem, which is an
efficient tool for feature-level fusion among either homogeneous or
heterogeneous sources of information. In addition, a (fuzzy-set-theoretic)
possibilistic scheme is proposed to weight the available modalities, based on
their respective reliability, in a joint optimization problem for finding the
sparsity codes. This approach provides a general framework for quality-based
fusion that offers added robustness to several sparsity-based multimodal
classification algorithms. To demonstrate their efficacy, the proposed methods
are evaluated on three different applications - multiview face recognition,
multimodal face recognition, and target classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1906</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1906</id><created>2014-03-07</created><authors><author><keyname>Coull</keyname><forenames>Scott</forenames></author><author><keyname>Dyer</keyname><forenames>Kevin</forenames></author></authors><title>Privacy Failures in Encrypted Messaging Services: Apple iMessage and
  Beyond</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instant messaging services are quickly becoming the most dominant form of
communication among consumers around the world. Apple iMessage, for example,
handles over 2 billion message each day, while WhatsApp claims 16 billion
messages from 400 million international users. To protect user privacy, these
services typically implement end-to-end and transport layer encryption, which
are meant to make eavesdropping infeasible even for the service providers
themselves. In this paper, however, we show that it is possible for an
eavesdropper to learn information about user actions, the language of messages,
and even the length of those messages with greater than 96% accuracy despite
the use of state-of-the-art encryption technologies simply by observing the
sizes of encrypted packet. While our evaluation focuses on Apple iMessage, the
attacks are completely generic and we show how they can be applied to many
popular messaging services, including WhatsApp, Viber, and Telegram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1910</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1910</id><created>2014-03-07</created><authors><author><keyname>Yang</keyname><forenames>Yulong</forenames></author><author><keyname>Lindqvist</keyname><forenames>Janne</forenames></author><author><keyname>Oulasvirta</keyname><forenames>Antti</forenames></author></authors><title>Text Entry Method Affects Password Security</title><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-based passwords continue to be the prime form of authentication to
computer systems. Today, they are increasingly created and used with mobile
text entry methods, such as touchscreens and mobile keyboards, in addition to
traditional physical keyboards. This raises a foundational question for usable
security: whether text entry methods affect password generation and password
security. This paper presents results from a between-group study with 63
participants, in which each group generated passwords for multiple virtual
accounts using a different text entry method. Participants were also asked to
recall their passwords afterwards. We applied analysis of structures and
probabilities, with standard and recent security metrics and also performed
cracking attacks on the collected data. The results show a significant effect
of text entry methods on passwords. In particular, one of the experimental
groups created passwords with significantly more lowercase letters per password
than the control group ($t(60) = 2.99, p = 0.004$). The choices for character
types in each group were also significantly different ($p=0.048, FET$). Our
cracking attacks consequently expose significantly different resistance across
groups ($p=0.031, FET$) and text entry method vulnerabilities. Our findings
contribute to the understanding of password security in the context of usable
interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1911</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1911</id><created>2014-03-07</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Candy Crush is NP-hard</title><categories>cs.CC</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that playing Candy Crush to achieve a given score in a fixed number
of swaps is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1923</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1923</id><created>2014-03-07</created><authors><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Qi</keyname><forenames>Wangdong</forenames></author><author><keyname>Zhang</keyname><forenames>Su</forenames></author></authors><title>The Unambiguous Distance in a Phase-based Ranging System with Hopping
  Frequencies</title><categories>cs.OH</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a challenge to specify unambiguous distance (UD) in a phase-based
ranging system with hopping frequencies (PRSHF). In this letter, we propose to
characterize the UD in a PRSHF by the probability that it takes on its maximum
value. We obtain a very simple and elegant expression of the probability with
growth estimation techniques from analytic number theory. It is revealed that
the UD in a PRSHF usually takes on the maximum value with as few as 10
frequencies in measurement, almost independent of the specific distribution of
available bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1928</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1928</id><created>2014-03-07</created><authors><author><keyname>Haryono</keyname></author><author><keyname>Istiyanto</keyname><forenames>Jazi Eko</forenames></author><author><keyname>Harjoko</keyname><forenames>Agus</forenames></author><author><keyname>Putra</keyname><forenames>Agfianto Eko</forenames></author></authors><title>Five Modular Redundancy with Mitigation Technique to Recover the Error
  Module</title><categories>cs.AR</categories><comments>6 Pages</comments><journal-ref>Haryono, Istiyanto, J.E., Harjoko, A., Putra, A.E., 2014,
  International Journal of advanced studies in Computer Science and Engineering
  IJASCSE, Volume 3, Issue 2, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hazard radiation can lead the system fault therefore Fault Tolerance is
required. Fault Tolerant is a system, which is designed to keep operations
running, despite the degradation in the specific module is happening. Many
fault tolerances have been developed to handle the problem, to find the most
robust and efficient in the possible technology. This paper will present the
Five Modular Redundancy (FMR) with Mitigation Technique to Recover the Error
Module. With Dynamic Partial Reconfiguration technology that have already
available today, such fault tolerance technique can be implemented
successfully. The project showed the robustness of the system is increased and
module which is error can be recovered immediately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1936</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1936</id><created>2014-03-08</created><authors><author><keyname>Rahman</keyname><forenames>Md. Mijanur</forenames></author><author><keyname>Ripon</keyname><forenames>Shamim</forenames></author></authors><title>Elicitation and Modeling Non-Functional Requirements - A POS Case Study</title><categories>cs.SE</categories><comments>5 pages, 5 figures</comments><journal-ref>International Journal of Future Computer and Communication vol. 2,
  no. 5, pp. 485-489, 2013</journal-ref><doi>10.7763/IJFCC.2013.V2.211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proper management of requirements is crucial to successful development
software within limited time and cost. Nonfunctional requirements (NFR) are one
of the key criteria to derive a comparison among various software systems. In
most of software development NFR have be specified as an additional requirement
of software. NFRs such as performance, reliability, maintainability, security,
accuracy etc. have to be considered at the early stage of software development
as functional requirement (FR). However, identifying NFR is not an easy task.
Although there are well developed techniques for eliciting functional
requirement, there is a lack of elicitation mechanism for NFR and there is no
proper consensus regarding NFR elicitation techniques. Eliciting NFRs are
considered to be one of the challenging jobs in requirement analysis. This
paper proposes a UML use case based questionary approach to identifying and
classifying NFR of a system. The proposed approach is illustrated by using a
Point of Sale (POS) case study
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1937</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1937</id><created>2014-03-08</created><updated>2015-02-08</updated><authors><author><keyname>Gurumoorthy</keyname><forenames>Karthik S.</forenames></author><author><keyname>Peter</keyname><forenames>Adrian M.</forenames></author><author><keyname>Guan</keyname><forenames>Birmingham Hang</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author></authors><title>A fast eikonal equation solver using the Schrodinger wave equation</title><categories>math.NA cs.CV cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a Schr\&quot;odinger wave equation formalism to solve the eikonal equation.
In our framework, a solution to the eikonal equation is obtained in the limit
as Planck's constant $\hbar$ (treated as a free parameter) tends to zero of the
solution to the corresponding linear Schr\&quot;odinger equation. The Schr\&quot;odinger
equation corresponding to the eikonal turns out to be a \emph{generalized,
screened Poisson equation}. Despite being linear, it does not have a
closed-form solution for arbitrary forcing functions. We present two different
techniques to solve the screened Poisson equation. In the first approach we use
a standard perturbation analysis approach to derive a new algorithm which is
guaranteed to converge provided the forcing function is bounded and positive.
The perturbation technique requires a sequence of discrete convolutions which
can be performed in $O(N\log N)$ using the Fast Fourier Transform (FFT) where
$N$ is the number of grid points. In the second method we discretize the linear
Laplacian operator by the finite difference method leading to a sparse linear
system of equations which can be solved using the plethora of sparse solvers.
The eikonal solution is recovered from the exponent of the resultant scalar
field. Our approach eliminates the need to explicitly construct viscosity
solutions as customary with direct solutions to the eikonal. Since the linear
equation is computed for a small but non-zero $\hbar$, the obtained solution is
an approximation. Though our solution framework is applicable to the general
class of eikonal problems, we detail specifics for the popular vision
applications of shape-from-shading, vessel segmentation, and path planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1939</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1939</id><created>2014-03-08</created><authors><author><keyname>Sirsat</keyname><forenames>Sandeep</forenames></author></authors><title>Extraction of Core Contents from Web Pages</title><categories>cs.IR</categories><comments>6 Pages, 3 Figures, 11 references. arXiv admin note: text overlap
  with arXiv:1207.0246 by other authors without attribution</comments><journal-ref>Sandeep Sirsat. &quot;Extraction of Core Contents from Web Pages&quot;,
  International Journal of Engineering Trends and Technology(IJETT),
  V8(9),484-489 February 2014. ISSN:2231-5381. www.ijettjournal.org. published
  by seventh sense research group</journal-ref><doi>10.14445/22315381/IJETT-V8P285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information available on web pages mostly contains semi-structured text
documents which are represented either in XML, or HTML, or XHTML format that
lacks formatted document structure. The document does not discriminate between
the text and the schema that represent the text. Also the amount of structure
used to represent the text depends on the purpose and size of text document. No
semantic is applied to semi-structured documents. This requires extracting core
contents of text document to analyse words or sentences to generate useful
knowledge. This paper discusses several techniques and approaches useful for
extracting core content from semi-structured text documents and their merits
and demerits
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1942</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1942</id><created>2014-03-08</created><updated>2014-12-01</updated><authors><author><keyname>Sarkar</keyname><forenames>Chandrima</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>Predictive Overlapping Co-Clustering</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the authors due to a crucial sign
  error in objective function</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years co-clustering has emerged as an important data mining
tool for two way data analysis. Co-clustering is more advantageous over
traditional one dimensional clustering in many ways such as, ability to find
highly correlated sub-groups of rows and columns. However, one of the
overlooked benefits of co-clustering is that, it can be used to extract
meaningful knowledge for various other knowledge extraction purposes. For
example, building predictive models with high dimensional data and
heterogeneous population is a non-trivial task. Co-clusters extracted from such
data, which shows similar pattern in both the dimension, can be used for a more
accurate predictive model building. Several applications such as finding
patient-disease cohorts in health care analysis, finding user-genre groups in
recommendation systems and community detection problems can benefit from
co-clustering technique that utilizes the predictive power of the data to
generate co-clusters for improved data analysis.
  In this paper, we present the novel idea of Predictive Overlapping
Co-Clustering (POCC) as an optimization problem for a more effective and
improved predictive analysis. Our algorithm generates optimal co-clusters by
maximizing predictive power of the co-clusters subject to the constraints on
the number of row and column clusters. In this paper precision, recall and
f-measure have been used as evaluation measures of the resulting co-clusters.
Results of our algorithm has been compared with two other well-known techniques
- K-means and Spectral co-clustering, over four real data set namely, Leukemia,
Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate
the effectiveness and utility of our algorithm POCC in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1944</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1944</id><created>2014-03-08</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Li</keyname><forenames>Hong</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author></authors><title>Multi-label ensemble based on variable pairwise constraint projection</title><categories>cs.LG cs.CV stat.ML</categories><comments>19 pages,5 tables, 2 figures; Published with Information Sciences
  (INS)</comments><acm-class>I.2.6</acm-class><journal-ref>Information Sciences, 222, 2013, pp.269-281.(Available online 7
  August 2012)</journal-ref><doi>10.1016/j.ins.2012.07.066</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-label classification has attracted an increasing amount of attention in
recent years. To this end, many algorithms have been developed to classify
multi-label data in an effective manner. However, they usually do not consider
the pairwise relations indicated by sample labels, which actually play
important roles in multi-label classification. Inspired by this, we naturally
extend the traditional pairwise constraints to the multi-label scenario via a
flexible thresholding scheme. Moreover, to improve the generalization ability
of the classifier, we adopt a boosting-like strategy to construct a multi-label
ensemble from a group of base classifiers. To achieve these goals, this paper
presents a novel multi-label classification framework named Variable Pairwise
Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take
advantage of the variable pairwise constraint projection to learn a
lower-dimensional data representation, which preserves the correlations between
samples and labels. Thereafter, the base classifiers are trained in the new
data space. For the boosting-like strategy, we employ both the variable
pairwise constraints and the bootstrap steps to diversify the base classifiers.
Empirical studies have shown the superiority of the proposed method in
comparison with other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1946</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1946</id><created>2014-03-08</created><authors><author><keyname>Naseriparsa</keyname><forenames>Mehdi</forenames></author><author><keyname>Bidgoli</keyname><forenames>Amir-masoud</forenames></author><author><keyname>Varaee</keyname><forenames>Touraj</forenames></author></authors><title>Improving Performance of a Group of Classification Algorithms Using
  Resampling and Feature Selection</title><categories>cs.LG</categories><comments>7 pages</comments><journal-ref>World of Computer Science and Information Technology Journal,Vol
  3, No 4,pp 70-76,2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the importance of finding a meaningful pattern from huge
datasets has become more challenging. Data miners try to adopt innovative
methods to face this problem by applying feature selection methods. In this
paper we propose a new hybrid method in which we use a combination of
resampling, filtering the sample domain and wrapper subset evaluation method
with genetic search to reduce dimensions of Lung-Cancer dataset that we
received from UCI Repository of Machine Learning databases. Finally, we apply
some well- known classification algorithms (Na\&quot;ive Bayes, Logistic, Multilayer
Perceptron, Best First Decision Tree and JRIP) to the resulting dataset and
compare the results and prediction rates before and after the application of
our feature selection method on that dataset. The results show a substantial
progress in the average performance of five classification algorithms
simultaneously and the classification error for these classifiers decreases
considerably. The experiments also show that this method outperforms other
feature selection methods with a lower cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1949</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1949</id><created>2014-03-08</created><authors><author><keyname>Naseriparsa</keyname><forenames>Mehdi</forenames></author><author><keyname>Kashani</keyname><forenames>Mohammad Mansour Riahi</forenames></author></authors><title>Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in
  Lung Cancer Dataset</title><categories>cs.LG cs.CE</categories><comments>6 pages. arXiv admin note: text overlap with arXiv:1106.1813,
  arXiv:1001.1446 by other authors</comments><journal-ref>International Journal of Computer Applications,Vol 77,No 3,pp
  33-38,2013</journal-ref><doi>10.5120/13376-0987</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification algorithms are unable to make reliable models on the datasets
with huge sizes. These datasets contain many irrelevant and redundant features
that mislead the classifiers. Furthermore, many huge datasets have imbalanced
class distribution which leads to bias over majority class in the
classification process. In this paper combination of unsupervised
dimensionality reduction methods with resampling is proposed and the results
are tested on Lung-Cancer dataset. In the first step PCA is applied on
Lung-Cancer dataset to compact the dataset and eliminate irrelevant features
and in the second step SMOTE resampling is carried out to balance the class
distribution and increase the variety of sample domain. Finally, Naive Bayes
classifier is applied on the resulting dataset and the results are compared and
evaluation metrics are calculated. The experiments show the effectiveness of
the proposed method across four evaluation metrics: Overall accuracy, False
Positive Rate, Precision, Recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1951</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1951</id><created>2014-03-08</created><updated>2014-10-03</updated><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>&#x17d;emli&#x10d;ka</keyname><forenames>Jan</forenames></author></authors><title>Algebraic properties of word equations</title><categories>cs.FL math.AC</categories><comments>revised version, an improved and extended exposition</comments><msc-class>68R15, 13P05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question about maximal size of independent system of word equations is
one of the most striking problems in combinatorics on words. Recently, Aleksi
Saarela has introduced a new approach to the problem that is based on
linear-algebraic properties of polynomials encoding the equations and their
solutions. In this paper we develop further this approach and take into account
other algebraic properties of polynomials, namely their factorization. This, in
particular, allows to improve the bound for the number of independent equations
with maximal rank from quadratic to linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1956</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1956</id><created>2014-03-08</created><authors><author><keyname>Permatasari</keyname><forenames>Hanum Putri</forenames></author><author><keyname>Harlena</keyname><forenames>Silvia</forenames></author><author><keyname>Erlangga</keyname><forenames>Donny</forenames></author><author><keyname>Chandra</keyname><forenames>Reza</forenames></author></authors><title>Effect of Social Media on Website Popularity: Differences between Public
  and Private Universities in Indonesia</title><categories>cs.CY cs.SI</categories><comments>6 pages, 3 figures, 5 tables</comments><journal-ref>World of Computer Science and Information Technology Journal
  (WCSIT), ISSN: 2221-0741, Vol. 3, No. 2, pp. 32-37, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media has become something that is important to enhance social
networking and sharing of information through the website. Social media have
not only changed social networking, they provide a valuable tool for social
organization, activism, political, healthcare and even academic relations in
the university. The researchers conducted present study with objectives to a).
examine the academic use of social media by universities, b). measure the
popularity and visibility of social media owned by universities. This study was
delimited to the universities in Indonesia. The population of the study
consisted both on public and private universities. The sample size comprised
totally of 264 universities that their ranks included both in Webometrics and
4ICU in July 2012 edition. The social media which was examined included
Facebook, Twitter, Flicker, LinkedIn, Youtube, Wikipeda, Blogs, social network
community owned by the university and Open Course Ware. This study used an
approach for data collection and measurement: by using Alexa and Majestic SEO.
Data analysis using the Pearson Chi-square for social media ownership that
using data ordinal and independent t test for examining effects of social media
on website popularity. The study revealed that majority of the social media
users used Facebook, then followed by Twitter. There are also most significant
differences for result of popularity by Alexa Rank and visibility by Majestic
SEO in universities whether used social media or no.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1974</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1974</id><created>2014-03-08</created><authors><author><keyname>Singh</keyname><forenames>Jaspinder Pal</forenames></author></authors><title>Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the
  Greening of Potatoes</title><categories>cs.CV</categories><comments>5 pages, 8 figures, 2 tables, &quot;Published with International Journal
  of Engineering Trends and Technology (IJETT)&quot; ISSN:2231-5381.
  http://www.ijettjournal.org. published by seventh sense research group</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V8(8),438-442 February 2014</journal-ref><doi>10.14445/22315381/IJETT-V8P275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potato quality control has improved in the last years thanks to automation
techniques like machine vision, mainly making the classification task between
different quality degrees faster, safer and less subjective. In our study we
are going to design a computer vision algorithm for grading of potatoes
according to the greening of the surface color of potato. The ratio of green
pixels to the total number of pixels of the potato surface is found. The higher
the ratio the worse is the potato. First the image is converted into serial
data and then processing is done in RGB colour space. Green part of the potato
is also shown by de-serializing the output. The same algorithm is then
synthesized on FPGA and the result shows thousand times speed improvement in
case of hardware synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.1988</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.1988</id><created>2014-03-08</created><authors><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Kranakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Krizanc</keyname><forenames>Danny</forenames></author></authors><title>Excuse Me! or The Courteous Theatregoers' Problem</title><categories>cs.DM</categories><comments>21 pages, 5 figures. An extended abstract of this paper appears in
  the Proceedings of Seventh International Conference on Fun with Algorithms,
  July 1--3, 2014, Lipari Island, Sicily, Italy, Springer LNCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a theatre consisting of $m$ rows each containing $n$ seats.
Theatregoers enter the theatre along aisles and pick a row which they enter
along one of its two entrances so as to occupy a seat. Assume they select their
seats uniformly and independently at random among the empty ones. A row of
seats is narrow and an occupant who is already occupying a seat is blocking
passage to new incoming theatregoers. As a consequence, occupying a specific
seat depends on the courtesy of theatregoers and their willingness to get up so
as to create free space that will allow passage to others. Thus, courtesy
facilitates and may well increase the overall seat occupancy of the theatre. We
say a theatregoer is courteous if (s)he will get up to let others pass.
Otherwise, the theatregoer is selfish. A set of theatregoers is $p$-courteous
if each theatregoer in the set is courteous with probability $p$, randomly and
independently. It is assumed that the behaviour of a theatregoer does not
change during the occupancy of the row.
  In this paper, we are interested in the following question: what is the
expected number of occupied seats as a function of the total number of seats in
a theatre, $n$, and the probability that a theatregoer is courteous, $p$? We
study and analyze interesting variants of this problem reflecting behaviour of
the theatregoers as entirely selfish, and $p$-courteous for a row of seats with
one or two entrances and as a consequence for a theatre with $m$ rows of seats
with multiple aisles. We also consider the case where seats in a row are chosen
according to the geometric distribution and the Zipf distibrution (as opposed
to the uniform distribution) and provide bounds on the occupancy of a row (and
thus the theatre) in each case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2000</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2000</id><created>2014-03-08</created><authors><author><keyname>Kramer</keyname><forenames>Simon</forenames></author></authors><title>A Galois-Connection between Myers-Briggs' Type Indicators and Szondi's
  Personality Profiles</title><categories>cs.CE cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a computable Galois-connection between Myers-Briggs' Type
Indicators (MBTIs), the most widely-used personality measure for
non-psychiatric populations (based on C.G. Jung's personality types), and
Szondi's personality profiles (SPPs), a less well-known but, as we show, finer
personality measure for psychiatric as well as non-psychiatric populations
(conceived as a unification of the depth psychology of S. Freud, C.G. Jung, and
A. Adler). The practical significance of our result is that our
Galois-connection provides a pair of computable, interpreting translations
between the two personality spaces of MBTIs and SPPs: one concrete from
MBTI-space to SPP-space (because SPPs are finer) and one abstract from
SPP-space to MBTI-space (because MBTIs are coarser). Thus Myers-Briggs' and
Szondi's personality-test results are mutually interpretable and
inter-translatable, even automatically by computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2001</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2001</id><created>2014-03-08</created><authors><author><keyname>Daou</keyname><forenames>Hoda</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>EEG Compression of Scalp Recordings based on Dipole Fitting</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel technique for Electroencephalogram (EEG) compression is proposed in
this article. This technique models the intrinsic dependency inherent between
the different EEG channels. It is based on dipole fitting that is usually used
in order to find a solution to the classic problems in EEG analysis: inverse
and forward problems. The suggested compression system uses dipole fitting as a
first building block to provide an approximation of the recorded signals. Then,
(based on a smoothness factor,) appropriate coding techniques are suggested to
compress the residuals of the fitting process. Results show that this technique
works well for different types of recordings and is even able to provide near-
lossless compression for event-related potentials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2002</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2002</id><created>2014-03-08</created><authors><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author><author><keyname>Mert</keyname><forenames>Cihan</forenames></author><author><keyname>Al-Naami</keyname><forenames>Khaled</forenames></author><author><keyname>Ozalp</keyname><forenames>Nuri</forenames></author><author><keyname>Ayan</keyname><forenames>Ugur</forenames></author></authors><title>Time Series Analysis on Stock Market for Text Mining Correlation of
  Economy News</title><categories>cs.CE cs.IR</categories><comments>23 pages</comments><journal-ref>International Journal of Social Sciences and Humanity Studies Vol
  6, No 1, 2014 ISSN: 1309-8063 (Online)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an information retrieval method for the economy news. The
effect of economy news, are researched in the word level and stock market
values are considered as the ground proof. The correlation between stock market
prices and economy news is an already addressed problem for most of the
countries. The most well-known approach is applying the text mining approaches
to the news and some time series analysis techniques over stock market closing
values in order to apply classification or clustering algorithms over the
features extracted. This study goes further and tries to ask the question what
are the available time series analysis techniques for the stock market closing
values and which one is the most suitable? In this study, the news and their
dates are collected into a database and text mining is applied over the news,
the text mining part has been kept simple with only term frequency-inverse
document frequency method. For the time series analysis part, we have studied
10 different methods such as random walk, moving average, acceleration,
Bollinger band, price rate of change, periodic average, difference, momentum or
relative strength index and their variation. In this study we have also
explained these techniques in a comparative way and we have applied the methods
over Turkish Stock Market closing values for more than a 2 year period. On the
other hand, we have applied the term frequency-inverse document frequency
method on the economy news of one of the high-circulating newspapers in Turkey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2003</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2003</id><created>2014-03-08</created><authors><author><keyname>Arslan</keyname><forenames>M. Lutfi</forenames></author><author><keyname>Seker</keyname><forenames>Sadi Evren</forenames></author></authors><title>The Impact of Employment Web Sites' Traffic on Unemployment: A Cross
  Country Comparison</title><categories>stat.AP cs.CY cs.IR</categories><comments>9 pages</comments><journal-ref>International Journal of Social Sciences and Humanity Studies Vol
  5, No 2, 2013 ISSN: 1309-8063 (Online)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although employment web sites have recently become the main source for re-
cruitment and selection process, the relation between those sites and unemploy-
ment rates is seldom addressed. Deriving data from 32 countries and 427 web
sites, this study explores the correlation between unemployment rates of
European countries and the attractiveness of country specific employment web
sites. It also compares the changes in unemployment rates and traffic on all
the aforementioned web sites. The results showed that there is a strong
correlation between web sites traffic and unemployment rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2004</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2004</id><created>2014-03-08</created><authors><author><keyname>Stewart</keyname><forenames>Michael</forenames></author></authors><title>Natural Language Feature Selection via Cooccurrence</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specificity is important for extracting collocations, keyphrases, multi-word
and index terms [Newman et al. 2012]. It is also useful for tagging, ontology
construction [Ryu and Choi 2006], and automatic summarization of documents
[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and
inverse-document frequency (TF-IDF) are typically used to do this, but fail to
take advantage of the semantic relationships between terms [Church and Gale
1995]. The result is that general idiomatic terms are mistaken for specific
terms. We demonstrate use of relational data for estimation of term
specificity. The specificity of a term can be learned from its distribution of
relations with other terms. This technique is useful for identifying relevant
words or terms for other natural language processing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2006</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2006</id><created>2014-03-08</created><authors><author><keyname>Kharaji</keyname><forenames>Morteza Yousefi</forenames></author><author><keyname>Rizi</keyname><forenames>Fatemeh Salehi</forenames></author></authors><title>An IAC Approach for Detecting Profile Cloning in Online Social Networks</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Online Social Networks are popular websites on the internet, which
millions of users register on and share their own personal information with
others. Privacy threats and disclosing personal information are the most
important concerns of OSNs users. Recently, a new attack which is named
Identity Cloned Attack is detected on OSNs. In this attack the attacker tries
to make a fake identity of a real user in order to access to private
information of the users friends which they do not publish on the public
profiles. In today OSNs, there are some verification services, but they are not
active services and they are useful for users who are familiar with online
identity issues. In this paper, Identity cloned attacks are explained in more
details and a new and precise method to detect profile cloning in online social
networks is proposed. In this method, first, the social network is shown in a
form of graph, then, according to similarities among users, this graph is
divided into smaller communities. Afterwards, all of the similar profiles to
the real profile are gathered (from the same community), then strength of
relationship (among all selected profiles and the real profile) is calculated,
and those which have the less strength of relationship will be verified by
mutual friend system. In this study, in order to evaluate the effectiveness of
proposed method, all steps are applied on a dataset of Facebook, and finally
this work is compared with two previous works by applying them on the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2009</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2009</id><created>2014-03-08</created><authors><author><keyname>Hassan</keyname><forenames>Olawale</forenames><affiliation>School of Computing, DePaul University</affiliation></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames><affiliation>School of Computing, DePaul University</affiliation></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames><affiliation>Department of Informatics, University of Bergen, Bergen, Norway</affiliation></author><author><keyname>Perkovi&#x107;</keyname><forenames>Ljubomir</forenames><affiliation>School of Computing, DePaul University</affiliation></author></authors><title>On the Ordered List Subgraph Embedding Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the (parameterized) Ordered List Subgraph Embedding problem (p-OLSE) we
are given two graphs $G$ and $H$, each with a linear order defined on its
vertices, a function $L$ that associates with every vertex in $G$ a list of
vertices in $H$, and a parameter $k$. The question is to decide if we can embed
(one-to-one) a subgraph $S$ of $G$ of $k$ vertices into $H$ such that: (1)
every vertex of $S$ is mapped to a vertex from its associated list, (2) the
linear orders inherited by $S$ and its image under the embedding are respected,
and (3) if there is an edge between two vertices in $S$ then there is an edge
between their images. If we require the subgraph $S$ to be embedded as an
induced subgraph, we obtain the Ordered List Induced Subgraph Embedding problem
(p-OLISE). The p-OLSE and p-OLISE problems model various problems in
Bioinformatics related to structural comparison/alignment of proteins.
  We investigate the complexity of p-OLSE and p-OLISE with respect to the
following structural parameters: the width $\Delta_L$ of the function $L$ (size
of the largest list), and the maximum degree $\Delta_H$ of $H$ and $\Delta_G$
of $G$. In terms of the structural parameters under consideration, we draw a
complete complexity landscape of p-OLSE and p-OLISE (and their optimization
versions) with respect to the computational frameworks of classical complexity,
parameterized complexity, and approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2010</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2010</id><created>2014-03-08</created><authors><author><keyname>Fanaei</keyname><forenames>Mohammad</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author><author><keyname>Schmid</keyname><forenames>Natalia A.</forenames></author></authors><title>Optimal Power Allocation for Distributed BLUE Estimation with Linear
  Spatial Collaboration</title><categories>cs.IT math.IT</categories><comments>5 Pages, 2 Figures, To appear at the IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of linear spatial collaboration for
distributed estimation in wireless sensor networks. In this context, the
sensors share their local noisy (and potentially spatially correlated)
observations with each other through error-free, low cost links based on a
pattern defined by an adjacency matrix. Each sensor connected to a central
entity, known as the fusion center (FC), forms a linear combination of the
observations to which it has access and sends the resulting signal to the FC
through an orthogonal fading channel. The FC combines these received signals to
find the best linear unbiased estimator of the vector of unknown signals
observed by individual sensors. The main novelty of this paper is the
derivation of an optimal power-allocation scheme in which the coefficients used
to form linear combinations of noisy observations at the sensors connected to
the FC are optimized. Through this optimization, the total estimation
distortion at the FC is minimized, given a constraint on the maximum cumulative
transmit power in the entire network. Numerical results show that even with a
moderate connectivity across the network, spatial collaboration among sensors
significantly reduces the estimation distortion at the FC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2013</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2013</id><created>2014-03-08</created><updated>2014-05-25</updated><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Performance and Robustness Analysis of Stochastic Jump Linear Systems
  using Wasserstein metric</title><categories>cs.SY math.PR</categories><doi>10.1016/j.automatica.2014.10.080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the performance and the robustness analysis of
stochastic jump linear systems. The state trajectory under stochastic jump
process becomes random variables, which brings forth the probability
distributions in the system state. Therefore, we need to adopt a proper metric
to measure the system performance with respect to stochastic switching. In this
perspective, Wasserstein metric that assesses the distance between probability
density functions is applied to provide the performance and the robustness
analysis. Both the transient and steady-state performance of the systems with
given initial state uncertainties can be measured in this framework. Also, we
prove that the convergence of this metric implies the mean square stability.
Overall, this study provides a unifying framework for the performance and the
robustness analysis of general stochastic jump linear systems, but not
necessarily Markovian jump process that is commonly used for stochastic
switching. The practical usefulness and efficiency of the proposed method are
verified through numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2024</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2024</id><created>2014-03-08</created><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Node Removal Vulnerability of the Largest Component of a Network</title><categories>cs.SI cs.NI</categories><comments>Published in IEEE GlobalSIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connectivity structure of a network can be very sensitive to removal of
certain nodes in the network. In this paper, we study the sensitivity of the
largest component size to node removals. We prove that minimizing the largest
component size is equivalent to solving a matrix one-norm minimization problem
whose column vectors are orthogonal and sparse and they form a basis of the
null space of the associated graph Laplacian matrix. A greedy node removal
algorithm is then proposed based on the matrix one-norm minimization. In
comparison with other node centralities such as node degree and betweenness,
experimental results on US power grid dataset validate the effectiveness of the
proposed approach in terms of reduction of the largest component size with
relatively few node removals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2031</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2031</id><created>2014-03-09</created><authors><author><keyname>Asha</keyname><forenames>V.</forenames></author><author><keyname>Bhajantri</keyname><forenames>N. U.</forenames></author><author><keyname>Nagabhushan</keyname><forenames>P.</forenames></author></authors><title>Texture Defect Detection in Gradient Space</title><categories>cs.CV</categories><comments>4 pages, ICFoCS-2011</comments><report-no>ICFoCS-2011</report-no><msc-class>11K70, 39A14, 39A70, 47A30, 62H30, 68M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a machine vision algorithm for automatically
detecting defects in patterned textures with the help of gradient space and its
energy. Experiments on real fabric images with defects show that the proposed
method can be used for automatic detection of fabric defects in textile
industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2036</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2036</id><created>2014-03-09</created><authors><author><keyname>McLean</keyname><forenames>Mathew W.</forenames></author></authors><title>Straightforward Bibliography Management in R with the RefManageR Package</title><categories>cs.DL cs.MS stat.CO</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces the R package RefManageR, which provides tools for
importing and working with bibliographic references. It extends the bibentry
class in R in a number of useful ways, including providing R with previously
unavailable support for BibLaTeX. BibLaTeX provides a superset of the
functionality of BibTeX, including full Unicode support, no memory limitations,
additional fields and entry types, and more sophisticated sorting of
references. RefManageR provides functions for citing and generating a
bibliography with hyperlinks for documents prepared with RMarkdown or RHTML.
Existing .bib files can be read into R and converted from BibTeX to BibLaTeX
and vice versa. References can also be imported via queries to NCBI's Entrez,
Zotero libraries, Google Scholar, and CrossRef. Additionally, references can be
created by reading PDFs stored on the user's machine with the help of Poppler.
Entries stored in the reference manager can be easily searched by any field, by
date ranges, and by various formats for name lists (author by last names,
translator by full names, etc.). Entries can also be updated, combined, sorted,
printed in a number of styles, and exported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2041</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2041</id><created>2014-03-09</created><authors><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author><author><keyname>Mitsou</keyname><forenames>Valia</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>Parameterized Edge Hamiltonicity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the parameterized complexity of the classical Edge Hamiltonian Path
problem and give several fixed-parameter tractability results. First, we settle
an open question of Demaine et al. by showing that Edge Hamiltonian Path is FPT
parameterized by vertex cover, and that it also admits a cubic kernel. We then
show fixed-parameter tractability even for a generalization of the problem to
arbitrary hypergraphs, parameterized by the size of a (supplied) hitting set.
We also consider the problem parameterized by treewidth or clique-width.
Surprisingly, we show that the problem is FPT for both of these standard
parameters, in contrast to its vertex version, which is W-hard for
clique-width. Our technique, which may be of independent interest, relies on a
structural characterization of clique-width in terms of treewidth and complete
bipartite subgraphs due to Gurski and Wanke.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2042</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2042</id><created>2014-03-09</created><authors><author><keyname>Sachdeva</keyname><forenames>Niharika</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Online Social Media and Police in India: Behavior, Perceptions,
  Challenges</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Police agencies across the globe are increasingly using Online Social Media
(OSM) to acquire intelligence and connect with citizens. Developed nations have
well thought of strategies to use OSM for policing. However, developing nations
like India are exploring and evolving OSM as a policing solution. India, in
recent years, experienced many events where rumors and fake content on OSM
instigated communal violence. In contrast to traditional media (e.g. television
and print media) used by Indian police departments, OSM offers velocity,
variety, veracity and large volume of information. These introduce new
challenges for police like platforms selection, secure usage strategy,
developing trust, handling offensive comments, and security / privacy
implication of information shared through OSM. Success of police initiatives on
OSM to maintain law and order depends both on their understanding of OSM and
citizen's acceptance / participation on these platforms.
  This study provides multidimensional understanding of behavior, perceptions,
interactions, and expectation regarding policing through OSM. First, we
examined recent updates from four different police pages- Delhi, Bangalore,
Uttar Pradesh and Chennai to comprehend various dimensions of police
interaction with citizens on OSM. Second, we conducted 20 interviews with IPS
officers (Indian Police Service) and 17 interviews with citizens to understand
decision rationales and expectation gaps between two stakeholders (police and
citizens); this was followed up with 445 policemen surveys and 204 citizen
surveys. We also present differences between police expectations of Indian and
police departments in developed countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2043</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2043</id><created>2014-03-09</created><authors><author><keyname>Gupta</keyname><forenames>Ruhi</forenames></author></authors><title>Implementation of an efficient RBAC in Cloud Computing using .NET
  environment</title><categories>cs.DC</categories><comments>6 pages, 5 figures, 1 flowchart, published By International Journal
  of Computer Trends and Technology(IJCTT)</comments><journal-ref>Ruhi Gupta. &quot;Implementation of an Efficient RBAC Technique of
  Cloud Computing In .NET Environment in
  (IJCTT)V8(3):120125,February2014.ISSN:22312803.www.ijcttjournal.org.Published
  by Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V8P122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing is flourishing day by day and it will continue in developing
phase until computers and internet era is in existence. While dealing with
cloud computing, a number of security and traffic related issues are
confronted. Load Balancing is one of the answers to these issues. RBAC deals
with such an answer. The proposed technique involves the hybrid of FCFS with
RBAC technique. RBAC will assign roles to the clients and clients with a
particular role can only access the particular document. Hence identity
management and access management are fully implemented using this technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2048</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2048</id><created>2014-03-09</created><updated>2014-08-24</updated><authors><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Era of Big Data Processing: A New Approach via Tensor Networks and
  Tensor Decompositions</title><categories>cs.ET</categories><comments>Part of this work was presented on the International Workshop on
  Smart Info-Media Systems in Asia,(invited talk - SISA-2013) Sept.30--Oct.2,
  2013, Nagoya, JAPAN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in computational neuroscience, neuroinformatics, pattern/image
recognition, signal processing and machine learning generate massive amounts of
multidimensional data with multiple aspects and high dimensionality. Tensors
(i.e., multi-way arrays) provide often a natural and compact representation for
such massive multidimensional data via suitable low-rank approximations. Big
data analytics require novel technologies to efficiently process huge datasets
within tolerable elapsed times. Such a new emerging technology for
multidimensional big data is a multiway analysis via tensor networks (TNs) and
tensor decompositions (TDs) which represent tensors by sets of factor
(component) matrices and lower-order (core) tensors. Dynamic tensor analysis
allows us to discover meaningful hidden structures of complex data and to
perform generalizations by capturing multi-linear and multi-aspect
relationships. We will discuss some fundamental TN models, their mathematical
and graphical descriptions and associated learning algorithms for large-scale
TDs and TNs, with many potential applications including: Anomaly detection,
feature extraction, classification, cluster analysis, data fusion and
integration, pattern recognition, predictive modeling, regression, time series
analysis and multiway component analysis.
  Keywords: Large-scale HOSVD, Tensor decompositions, CPD, Tucker models,
Hierarchical Tucker (HT) decomposition, low-rank tensor approximations (LRA),
Tensorization/Quantization, tensor train (TT/QTT) - Matrix Product States
(MPS), Matrix Product Operator (MPO), DMRG, Strong Kronecker Product (SKP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2056</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2056</id><created>2014-03-09</created><authors><author><keyname>Bingmann</keyname><forenames>Timo</forenames></author><author><keyname>Eberle</keyname><forenames>Andreas</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>Engineering Parallel String Sorting</title><categories>cs.DS cs.DC</categories><comments>46 pages, extension of &quot;Parallel String Sample Sort&quot; arXiv:1305.1157</comments><acm-class>F.2.2; E.5; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how string sorting algorithms can be parallelized on modern
multi-core shared memory machines. As a synthesis of the best sequential string
sorting algorithms and successful parallel sorting algorithms for atomic
objects, we first propose string sample sort. The algorithm makes effective use
of the memory hierarchy, uses additional word level parallelism, and largely
avoids branch mispredictions. Then we focus on NUMA architectures, and develop
parallel multiway LCP-merge and -mergesort to reduce the number of random
memory accesses to remote nodes. Additionally, we parallelize variants of
multikey quicksort and radix sort that are also useful in certain situations.
Comprehensive experiments on five current multi-core platforms are then
reported and discussed. The experiments show that our implementations scale
very well on real-world inputs and modern machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2065</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2065</id><created>2014-03-09</created><updated>2016-01-14</updated><authors><author><keyname>Yu</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Categorization Axioms for Clustering Results</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster analysis has attracted more and more attention in the field of
machine learning and data mining. Numerous clustering algorithms have been
proposed and are being developed due to diverse theories and various
requirements of emerging applications. Therefore, it is very worth establishing
an unified axiomatic framework for data clustering. In the literature, it is an
open problem and has been proved very challenging. In this paper, clustering
results are axiomatized by assuming that an proper clustering result should
satisfy categorization axioms. The proposed axioms not only introduce
classification of clustering results and inequalities of clustering results,
but also are consistent with prototype theory and exemplar theory of
categorization models in cognitive science. Moreover, the proposed axioms lead
to three principles of designing clustering algorithm and cluster validity
index, which follow many popular clustering algorithms and cluster validity
indices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2077</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2077</id><created>2014-03-09</created><updated>2014-03-14</updated><authors><author><keyname>Sodagari</keyname><forenames>Shabnam</forenames></author></authors><title>Application of Asynchronous Weak Commitment Search in Autonomous Quality
  of Service Provision in Cognitive Radio Networks</title><categories>cs.NI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a distributed solution to autonomous quality of service
provision in cognitive radio networks. Specifically, cognitive STDMA and CDMA
communication networks are studied. Based on asynchronous weak commitment
search the task of QoS provision is distributed among different network nodes.
Simulation results verify this scheme converges very fast to optimal solution,
which makes it suitable for practical real time systems. This application of
artificial intelligence in wireless and mobile communications can be used in
home automation and networking, and vehicular technology. The generalizations
and extensions of this approach can be used in Long Term Evolution Self
Organizing Networks (LTE-SONs). In addition, it can pave the way for
decentralized and autonomous QoS provision in capillary networks that reach end
nodes at Internet of Things, where central management is either unavailable or
not efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2079</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2079</id><created>2014-03-09</created><updated>2015-03-21</updated><authors><author><keyname>Kalantari</keyname><forenames>Ashkan</forenames></author><author><keyname>Maleki</keyname><forenames>Sina</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Joint Power Control in Wiretap Interference Channels</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Wireless Communications (TWC), 15 double-column
  pages and 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference in wireless networks degrades the signal quality at the
terminals. However, it can potentially enhance the secrecy rate. This paper
investigates the secrecy rate in a two-user interference network where one of
the users, namely user 1, requires to establish a confidential connection. User
1 wants to prevent an unintended user of the network to decode its
transmission. User 1 has to transmit such that its secrecy rate is maximized
while the quality of service at the destination of the other user, user 2, is
satisfied, and both user's power limits are taken into account. We consider two
scenarios: 1) user 2 changes its power in favor of user 1, an altruistic
scenario, 2) user 2 is selfish and only aims to maintain the minimum quality of
service at its destination, an egoistic scenario. It is shown that there is a
threshold for user 2's transmission power that only below or above which,
depending on the channel qualities, user 1 can achieve a positive secrecy rate.
Closed-form solutions are obtained in order to perform joint optimal power
control. Further, a new metric called secrecy energy efficiency is introduced.
We show that in general, the secrecy energy efficiency of user 1 in an
interference channel scenario is higher than that of an interference-free
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2081</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2081</id><created>2014-03-09</created><authors><author><keyname>Song</keyname><forenames>Changick</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Diversity of Linear Transceivers in MIMO AF Half-duplex Relaying
  Channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear transceiving schemes between the relay and the destination have
recently attracted much interest in MIMO amplify-and-forward (AF) relaying
systems due to low implementation complexity. In this paper, we provide
comprehensive analysis on the diversity order of the linear zero-forcing (ZF)
and minimum mean squared error (MMSE) transceivers. Firstly, we obtain a
compact closed-form expression for the diversity-multiplexing tradeoff (DMT)
through tight upper and lower bounds. While our DMT analysis accurately
predicts the performance of the ZF transceivers, it is observed that the MMSE
transceivers exhibit a complicated rate dependent behavior, and thus are very
unpredictable via DMT for finite rate cases. Secondly, we highlight this
interesting behavior of the MMSE transceivers and characterize the diversity
order at all finite rates. This leads to a closed-form expression for the
diversity-rate tradeoff (DRT) which reveals the relationship between the
diversity, the rate, and the number of antennas at each node. Our DRT analysis
compliments our previous work on DMT, thereby providing a complete
understanding on the diversity order of linear transceiving schemes in MIMO AF
relaying channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2090</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2090</id><created>2014-03-09</created><updated>2014-07-03</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>Upper Bounds on Syntactic Complexity of Left and Two-Sided Ideals</title><categories>cs.FL</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve two open problems concerning syntactic complexity: We prove that the
cardinality of the syntactic semigroup of a left ideal or a suffix-closed
language with $n$ left quotients (that is, with state complexity $n$) is at
most $n^{n-1}+n-1$, and that of a two-sided ideal or a factor-closed language
is at most $n^{n-2}+(n-2)2^{n-2}+1$. Since these bounds are known to be
reachable, this settles the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2092</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2092</id><created>2014-03-09</created><authors><author><keyname>de S&#xe1;</keyname><forenames>Vin&#xed;cius Gusm&#xe3;o Pereira</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina Miraglia Herrera</forenames></author></authors><title>Blind-friendly von Neumann's Heads or Tails</title><categories>cs.DS</categories><comments>12 pages, 1 figure</comments><msc-class>60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The toss of a coin is usually regarded as the epitome of randomness, and has
been used for ages as a means to resolve disputes in a simple, fair way.
Perhaps as ancient as consulting objects such as coins and dice is the art of
maliciously biasing them in order to unbalance their outcomes. However, it is
possible to employ a biased device to produce equiprobable results in a number
of ways, the most famous of which is the method suggested by von Neumann back
in 1951. This paper addresses how to extract uniformly distributed bits of
information from a nonuniform source. We study some probabilities related to
biased dice and coins, culminating in an interesting variation of von Neumann's
mechanism that can be employed in a more restricted setting where the actual
results of the coin tosses are not known to the contestants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2096</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2096</id><created>2014-03-09</created><authors><author><keyname>Petruzzelli</keyname><forenames>Antonio Messeni</forenames></author><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Albino</keyname><forenames>Vito</forenames></author></authors><title>Determinants of Patent Citations in Biotechnology: An Analysis of Patent
  Influence Across the Industrial and Organizational Boundaries</title><categories>cs.DL</categories><comments>Technological Forecasting and Social Change (2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper extends the literature investigating key drivers leading
certain patents to exert a stronger influence on the subsequent technological
developments (inventions) than other ones. We investigated six key
determinants, as (i) the use of scientific knowledge, (ii) the breadth of the
technological base, (iii) the existence of collaboration in patent development,
(iv) the number of claims, (v) the scope, and (vi) the novelty, and how the
effect of these determinants varies when patent influence - as measured by the
number of forward citations the patent received - is distinguished as within
and across the industrial and organizational boundaries. We conducted an
empirical analysis on a sample of 5671 patents granted to 293 US biotechnology
firms from 1976 to 2003. Results reveal that the contribution of the
determinants to patent influence differs across the domains that are identified
by the industrial and organizational boundaries. Findings, for example, show
that the use of scientific knowledge negatively affects patent influence
outside the biotechnology industry, while it positively contributes to make a
patent more relevant for the assignee's subsequent technological developments.
In addition, the broader the scope of a patent the higher the number of
citations the patent receives from subsequent non-biotechnology patents. This
relationship is inverted-U shaped when considering the influence of a patent on
inventions granted to other organizations than the patent's assignee. Finally,
the novelty of a patent is inverted-U related with the influence the patent
exerts on the subsequent inventions granted across the industrial and
organizational boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2098</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2098</id><created>2014-03-09</created><authors><author><keyname>Cao</keyname><forenames>Zizhong</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author></authors><title>Technical Report: Efficient Buffering and Scheduling for a Single-Chip
  Crosspoint-Queued Switch</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The single-chip crosspoint-queued (CQ) switch is a compact switching
architecture that has all its buffers placed at the crosspoints of input and
output lines. Scheduling is also performed inside the switching core, and does
not rely on latency-limited communications with input or output line-cards.
Compared with other legacy switching architectures, the CQ switch has the
advantages of high throughput, minimal delay, low scheduling complexity, and no
speedup requirement. However, the crosspoint buffers are small and segregated,
thus how to efficiently use the buffers and avoid packet drops remains a major
problem that needs to be addressed. In this paper, we consider load balancing,
deflection routing, and buffer pooling for efficient buffer sharing in the CQ
switch. We also design scheduling algorithms to maintain the correct packet
order even while employing multi-path switching and resolve contentions caused
by multiplexing. All these techniques require modest hardware modifications and
memory speedup in the switching core, but can greatly boost the buffer
utilizations by up to 10 times and reduce the packet drop rates by one to three
orders of magnitude. Extensive simulations and analyses have been done to
demonstrate the advantages of the proposed buffering and scheduling techniques
in various aspects. By pushing the on-chip memory to the limit of current ASIC
technology, we show that a cell drop rate of 10e-8, which is low enough for
practical uses, can be achieved under real Internet traffic traces
corresponding to a load of 0.9.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2111</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2111</id><created>2014-03-09</created><authors><author><keyname>Chen</keyname><forenames>Tsung-Yi</forenames></author><author><keyname>Vakilinia</keyname><forenames>Kasra</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Protograph-Based Raptor-Like LDPC Codes</title><categories>cs.IT math.IT</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a class of rate-compatible LDPC codes, called
protograph-based Raptor-like (PBRL) codes. The construction is focused on
binary codes for BI-AWGN channels. As with the Raptor codes, additional parity
bits are produced by exclusive-OR operations on the precoded bits, providing
extensive rate compatibility. Unlike Raptor codes, the structure of each
additional parity bit in the protograph is explicitly designed through density
evolution. The construction method provides low iterative decoding thresholds
and the lifted codes result in excellent error rate performance for
long-blocklength PBRL codes. For short-blocklength PBRL codes the protograph
design and lifting must avoid undesired graphical structures such as trapping
sets and absorbing sets while also seeking to minimize the density evolution
threshold. Simulation results are shown in information block sizes of $k=192$,
$16368$ and $16384$. Comparing at the same information block size of $k=16368$
bits, the PBRL codes outperform the best known standardized code, the AR4JA
codes in the waterfall region. The PBRL codes also perform comparably to DVB-S2
codes even though the DVB-S2 codes use LDPC codes with longer blocklengths and
are concatenated with outer BCH codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2116</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2116</id><created>2014-03-09</created><authors><author><keyname>N&#xfa;&#xf1;ez</keyname><forenames>Felipe</forenames></author><author><keyname>Wang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Doyle</keyname><forenames>Francis J.</forenames><suffix>III</suffix></author></authors><title>Global Synchronization of Pulse-Coupled Oscillators Interacting on Cycle
  Graphs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of pulse-coupled oscillators (PCOs) in biology and engineering
has motivated research to understand basic properties of PCO networks. Despite
the large body of work addressing PCOs, a global synchronization result for
networks that are more general than all-to-all connected is still unavailable.
In this paper we address global synchronization of PCO networks described by
cycle graphs. It is shown for the bidirectional cycle case that as the number
of oscillators in the cycle grows, the coupling strength must be increased in
order to guarantee synchronization for arbitrary initial conditions. For the
unidirectional cycle case, the strongest coupling cannot ensure global
synchronization yet a refractory period in the phase response curve is
sufficient to enable global synchronization. Analytical findings are confirmed
by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2123</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2123</id><created>2014-03-09</created><updated>2014-11-23</updated><authors><author><keyname>Freudiger</keyname><forenames>Julien</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Brito</keyname><forenames>Alex</forenames></author></authors><title>Privacy-Friendly Collaboration for Cyber Threat Mitigation</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing of security data across organizational boundaries has often been
advocated as a promising way to enhance cyber threat mitigation. However,
collaborative security faces a number of important challenges, including
privacy, trust, and liability concerns with the potential disclosure of
sensitive data. In this paper, we focus on data sharing for predictive
blacklisting, i.e., forecasting attack sources based on past attack
information. We propose a novel privacy-enhanced data sharing approach in which
organizations estimate collaboration benefits without disclosing their
datasets, organize into coalitions of allied organizations, and securely share
data within these coalitions. We study how different partner selection
strategies affect prediction accuracy by experimenting on a real-world dataset
of 2 billion IP addresses and observe up to a 105% prediction improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2124</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2124</id><created>2014-03-09</created><authors><author><keyname>Davis</keyname><forenames>Hannah</forenames></author><author><keyname>Mohammad</keyname><forenames>Saif M.</forenames></author></authors><title>Generating Music from Literature</title><categories>cs.CL</categories><journal-ref>In Proceedings of the EACL Workshop on Computational Linguistics
  for Literature, April 2014, Gothenburg, Sweden</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system, TransProse, that automatically generates musical pieces
from text. TransProse uses known relations between elements of music such as
tempo and scale, and the emotions they evoke. Further, it uses a novel
mechanism to determine sequences of notes that capture the emotional activity
in the text. The work has applications in information visualization, in
creating audio-visual e-books, and in developing music apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2140</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2140</id><created>2014-03-10</created><updated>2014-04-18</updated><authors><author><keyname>Szanto-Varnagy</keyname><forenames>Adam</forenames></author><author><keyname>Pollner</keyname><forenames>Peter</forenames></author><author><keyname>Vicsek</keyname><forenames>Tamas</forenames></author><author><keyname>Farkas</keyname><forenames>Illes J.</forenames></author></authors><title>Scientometrics: Untangling the topics</title><categories>cs.DL cs.SI physics.data-an physics.soc-ph</categories><comments>2 pages, 2 figures</comments><journal-ref>National Science Review (September 2014) 1 (3): 343-345</journal-ref><doi>10.1093/nsr/nwu027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring science is based on comparing articles to similar others. However,
keyword-based groups of thematically similar articles are dominantly small.
These small sizes keep the statistical errors of comparisons high. With the
growing availability of bibliographic data such statistical errors can be
reduced by merging methods of thematic grouping, citation networks and keyword
co-usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2150</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2150</id><created>2014-03-10</created><authors><author><keyname>Triantafillou</keyname><forenames>Sofia</forenames></author><author><keyname>Tsamardinos</keyname><forenames>Ioannis</forenames></author></authors><title>Constraint-based Causal Discovery from Multiple Interventions over
  Overlapping Variable Sets</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific practice typically involves repeatedly studying a system, each
time trying to unravel a different perspective. In each study, the scientist
may take measurements under different experimental conditions (interventions,
manipulations, perturbations) and measure different sets of quantities
(variables). The result is a collection of heterogeneous data sets coming from
different data distributions. In this work, we present algorithm COmbINE, which
accepts a collection of data sets over overlapping variable sets under
different experimental conditions; COmbINE then outputs a summary of all causal
models indicating the invariant and variant structural characteristics of all
models that simultaneously fit all of the input data sets. COmbINE converts
estimated dependencies and independencies in the data into path constraints on
the data-generating causal model and encodes them as a SAT instance. The
algorithm is sound and complete in the sample limit. To account for conflicting
constraints arising from statistical errors, we introduce a general method for
sorting constraints in order of confidence, computed as a function of their
corresponding p-values. In our empirical evaluation, COmbINE outperforms in
terms of efficiency the only pre-existing similar algorithm; the latter
additionally admits feedback cycles, but does not admit conflicting constraints
which hinders the applicability on real data. As a proof-of-concept, COmbINE is
employed to co-analyze 4 real, mass-cytometry data sets measuring
phosphorylated protein concentrations of overlapping protein sets under 3
different interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2152</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2152</id><created>2014-03-10</created><authors><author><keyname>Freeman</keyname><forenames>Robert John</forenames></author></authors><title>Parsing using a grammar of word association vectors</title><categories>cs.CL cs.NE</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper was was first drafted in 2001 as a formalization of the system
described in U.S. patent U.S. 7,392,174. It describes a system for implementing
a parser based on a kind of cross-product over vectors of contextually similar
words. It is being published now in response to nascent interest in vector
combination models of syntax and semantics. The method used aggressive
substitution of contextually similar words and word groups to enable product
vectors to stay in the same space as their operands and make entire sentences
comparable syntactically, and potentially semantically. The vectors generated
had sufficient representational strength to generate parse trees at least
comparable with contemporary symbolic parsers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2170</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2170</id><created>2014-03-10</created><updated>2014-03-14</updated><authors><author><keyname>Alagoz</keyname><forenames>B. Baykant</forenames></author></authors><title>On the Harmonic Oscillation of High-order Linear Time Invariant Systems</title><categories>cs.DM cs.SY</categories><comments>8 Figures 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear time invariant (LTI) systems are widely used for modeling system
dynamics in science and engineering problems. Harmonic oscillation of LTI
systems are widely used for modeling and analyses of periodic physical
phenomenon. This study investigates sufficient conditions to obtain harmonic
oscillation for high-order LTI systems. The paper presents a design procedure
for controlling harmonic oscillation of singleinput single-output high-order
LTI systems. LTI system coefficients are calculated by the solution of linear
equation set, which imposes a stable sinusoidal oscillation solution for the
characteristic polynomials of LTI systems. An example design is demonstrated
for fourth-order LTI systems and the control of harmonic oscillations are
discussed by illustrating Hilbert transform and spectrogram of oscillation
signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2174</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2174</id><created>2014-03-10</created><authors><author><keyname>Wu</keyname><forenames>Yuanxin</forenames></author><author><keyname>Wang</keyname><forenames>Jinling</forenames></author><author><keyname>Hu</keyname><forenames>Dewen</forenames></author></authors><title>A New Technique for INS/GNSS Attitude and Parameter Estimation Using
  Online Optimization</title><categories>cs.RO cs.SY</categories><comments>IEEE Trans. on Signal Processing, to appear</comments><journal-ref>IEEE Trans. on Signal Processing, 62 (10), 2642 - 2655, 2014</journal-ref><doi>10.1109/TSP.2014.2312317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration of inertial navigation system (INS) and global navigation
satellite system (GNSS) is usually implemented in engineering applications by
way of Kalman-like filtering. This form of INS/GNSS integration is prone to
attitude initialization failure, especially when the host vehicle is moving
freely. This paper proposes an online constrained-optimization method to
simultaneously estimate the attitude and other related parameters including
GNSS antenna's lever arm and inertial sensor biases. This new technique
benefits from self-initialization in which no prior attitude or sensor
measurement noise information is required. Numerical results are reported to
validate its effectiveness and prospect in high accurate INS/GNSS applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2180</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2180</id><created>2014-03-10</created><updated>2014-12-18</updated><authors><author><keyname>Lachambre</keyname><forenames>Helene</forenames></author><author><keyname>Ricaud</keyname><forenames>Benjamin</forenames></author><author><keyname>Stempfel</keyname><forenames>Guillaume</forenames></author><author><keyname>Torresani</keyname><forenames>Bruno</forenames></author><author><keyname>Wiesmeyr</keyname><forenames>Christoph</forenames></author><author><keyname>Onchis</keyname><forenames>Darian M.</forenames></author></authors><title>Optimal Window and Lattice in Gabor Transform Application to Audio
  Analysis</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with the use of optimal lattice and optimal window in
Discrete Gabor Transform computation. In the case of a generalized Gaussian
window, extending earlier contributions, we introduce an additional local
window adaptation technique for non-stationary signals. We illustrate our
approach and the earlier one by addressing three time-frequency analysis
problems to show the improvements achieved by the use of optimal lattice and
window: close frequencies distinction, frequency estimation and SNR estimation.
The results are presented, when possible, with real world audio signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2187</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2187</id><created>2014-03-10</created><authors><author><keyname>Mansouri</keyname><forenames>Dou El Kefel</forenames></author><author><keyname>Benyettou</keyname><forenames>Mohamed</forenames></author></authors><title>A study of risk management in cloud computing bank</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing apparently helps in reducing costs and providing the
scheduling optimal level. In practice however it may confront the problem of
unavailability of resources. Taking into consideration the cloud computing bank
with its somehow commercial nature, the resources unavailability, such as
liquidity risk, remains. In this paper, an attempt to show through a solution
so far applied in economy, how would it be possible to predict such a liquidity
risk in cloud computing bank. The proposed solution can especially be adapted
to stock management. To reduce the risk we will also make use of a method
inspired from physics based on the fluids mechanics; it is an application of
Bernoulli's theorem called Torricelli. The resource bank will be considered as
a reservoir of liquid, and the availability of resources then will depend on
the liquid flow velocity and the replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2189</identifier>
 <datestamp>2015-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2189</id><created>2014-03-10</created><updated>2015-01-11</updated><authors><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Wireless Information and Energy Transfer with Reduced Feedback in
  MIMO Interference Channels</title><categories>cs.IT math.IT</categories><comments>accepted to IEEE Journal of Selected Areas in Communications (IEEE
  JSAC), Special Issue on Wireless Communications Powered by Energy Harvesting
  and Wireless Energy Transfer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To determine the transmission strategy for joint wireless information and
energy transfer (JWIET) in the MIMO interference channel (IFC), the information
access point (IAP) and energy access point (EAP) require the channel state
information (CSI) of their associated links to both the information-decoding
(ID) mobile stations (MSs) and energy-harvesting (EH) MSs (so-called local
CSI). In this paper, to reduce th e feedback overhead of MSs for the JWIET in
two-user MIMO IFC, we propose a Geodesic energy beamforming scheme that
requires partial CSI at the EAP. Furthermore, in the two-user MIMO IFC, it is
proved that the Geodesic energy beamforming is the optimal strategy. By adding
a rank-one constraint on the transmit signal covariance of IAP, we can further
reduce the feedback overhead to IAP by exploiting Geodesic information
beamforming. Under the rank-one constraint of IAP's transmit signal, we prove
that Geodesic information/energy beamforming approach is the optimal strategy
for JWIET in the two-user MIMO IFC. We also discuss the extension of the
proposed rank-one Geodesic information/energy beamforming strategies to general
K-user MIMO IFC. Finally, by analyzing the achievable rate-energy performance
statistically under imperfect partial CSIT, we propose an adaptive bit
allocation strategy for both EH MS and ID MS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2194</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2194</id><created>2014-03-10</created><updated>2014-05-13</updated><authors><author><keyname>Haralambous</keyname><forenames>Yannis</forenames></author><author><keyname>Quaresma</keyname><forenames>Pedro</forenames></author></authors><title>Querying Geometric Figures Using a Controlled Language, Ontological
  Graphs and Dependency Lattices</title><categories>cs.CG cs.AI cs.DB cs.IR</categories><comments>14 pages, 5 figures, accepted at CICM 2014</comments><msc-class>68T30, 68T50, 97G40</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic geometry systems (DGS) have become basic tools in many areas of
geometry as, for example, in education. Geometry Automated Theorem Provers
(GATP) are an active area of research and are considered as being basic tools
in future enhanced educational software as well as in a next generation of
mechanized mathematics assistants. Recently emerged Web repositories of
geometric knowledge, like TGTP and Intergeo, are an attempt to make the already
vast data set of geometric knowledge widely available. Considering the large
amount of geometric information already available, we face the need of a query
mechanism for descriptions of geometric constructions.
  In this paper we discuss two approaches for describing geometric figures
(declarative and procedural), and present algorithms for querying geometric
figures in declaratively and procedurally described corpora, by using a DGS or
a dedicated controlled natural language for queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2201</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2201</id><created>2014-03-10</created><updated>2014-03-18</updated><authors><author><keyname>Dowty</keyname><forenames>James G.</forenames></author></authors><title>SMML estimators for linear regression and tessellations of hyperbolic
  space</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The strict minimum message length (SMML) principle links data compression
with inductive inference. The corresponding estimators have many useful
properties but they can be hard to calculate. We investigate SMML estimators
for linear regression models and we show that they have close connections to
hyperbolic geometry. When equipped with the Fisher information metric, the
linear regression model with $p$ covariates and a sample size of $n$ becomes a
Riemannian manifold, and we show that this is isometric to $(p+1)$-dimensional
hyperbolic space $\mathbb{H}^{p+1}$ equipped with a metric tensor which is $2n$
times the usual metric tensor on $\mathbb{H}^{p+1}$. A natural identification
then allows us to also view the set of sufficient statistics for the linear
regression model as a hyperbolic space. We show that the partition of an SMML
estimator corresponds to a tessellation of this hyperbolic space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2225</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2225</id><created>2014-03-10</created><updated>2015-02-12</updated><authors><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames></author><author><keyname>Tan</keyname><forenames>Tony</forenames></author></authors><title>On the variable hierarchy of first-order spectra</title><categories>cs.LO cs.CC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectrum of a first-order logic sentence is the set of natural numbers
that are cardinalities of its finite models. In this paper we study the
hierarchy of first-order spectra based on the number of variables. It has been
conjectured that it collapses to three variable. We show the opposite: it forms
an infinite hierarchy. However, despite the fact that more variables can
express more spectra, we show that to establish whether the class of
first-order spectra is closed under complement, it is sufficient to consider
sentences using only three variables and binary relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2226</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2226</id><created>2014-03-10</created><updated>2014-10-13</updated><authors><author><keyname>Peng</keyname><forenames>Chengbin</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>Accelerating Community Detection by Using K-core Subgraphs</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is expensive, and the cost generally depends at least
linearly on the number of vertices in the graph. We propose working with a
reduced graph that has many fewer nodes but nonetheless captures key community
structure. The K-core of a graph is the largest subgraph within which each node
has at least K connections. We propose a framework that accelerates community
detection by applying an expensive algorithm (modularity optimization, the
Louvain method, spectral clustering, etc.) to the K-core and then using an
inexpensive heuristic (such as local modularity maximization) to infer
community labels for the remaining nodes. Our experiments demonstrate that the
proposed framework can reduce the running time by more than 80% while
preserving the quality of the solutions. Recent theoretical investigations
provide support for using the K-core as a reduced representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2235</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2235</id><created>2014-03-10</created><updated>2015-06-02</updated><authors><author><keyname>Delecroix</keyname><forenames>Vincent</forenames></author><author><keyname>Bertazzon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>\'Etude d'une \'equation int\'egrale avec des m\'ethodes combinatoires</title><categories>math.DS cs.DM math.CA</categories><comments>in French, 34 figures, les commentaires sont bienvenus !</comments><msc-class>37B10, 45A05, 68R15</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We build bounded solutions to a linear integral equation. Our functions are
built as limit of iterated Birkhoff sums over auto-similar dynamical systems.
  Nous construisons des solutions born\'ees \`a une \'equation int\'egrale.
Notre m\'ethode repose sur une limite de sommes de Birkhoff it\'er\'ees
au-dessus de certains syst\`emes dynamiques auto-similaires.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2237</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2237</id><created>2014-03-10</created><authors><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Pang</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Sun</keyname><forenames>Jun</forenames></author><author><keyname>Dong</keyname><forenames>Jin Song</forenames></author></authors><title>Stateful Security Protocol Verification</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long-standing research problem in security protocol design is how to
efficiently verify security protocols with tamper-resistant global states. In
this paper, we address this problem by first proposing a protocol specification
framework, which explicitly represents protocol execution states and state
transformations. Secondly, we develop an algorithm for verifying security
properties by utilizing the key ingredients of the first-order reasoning for
reachability analysis, while tracking state transformation and checking the
validity of newly generated states. Our verification algorithm is proven to be
(partially) correct, if it terminates. We have implemented the proposed
framework and verification algorithms in a tool named SSPA, and evaluate it
using a number of stateful security protocols. The experimental results show
that our approach is not only feasible but also practically efficient. In
particular, we have found a security flaw on the digital envelope protocol,
which could not be detected by existing security protocol verifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2239</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2239</id><created>2014-03-10</created><authors><author><keyname>Aubel</keyname><forenames>C&#xe9;line</forenames></author><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Super-Resolution from Short-Time Fourier Transform Measurements</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP), May 2014, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While spike trains are obviously not band-limited, the theory of
super-resolution tells us that perfect recovery of unknown spike locations and
weights from low-pass Fourier transform measurements is possible provided that
the minimum spacing, $\Delta$, between spikes is not too small. Specifically,
for a cutoff frequency of $f_c$, Donoho [2] shows that exact recovery is
possible if $\Delta &gt; 1/f_c$, but does not specify a corresponding recovery
method. On the other hand, Cand\`es and Fernandez-Granda [3] provide a recovery
method based on convex optimization, which provably succeeds as long as $\Delta
&gt; 2/f_c$. In practical applications one often has access to windowed Fourier
transform measurements, i.e., short-time Fourier transform (STFT) measurements,
only. In this paper, we develop a theory of super-resolution from STFT
measurements, and we propose a method that provably succeeds in recovering
spike trains from STFT measurements provided that $\Delta &gt; 1/f_c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2273</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2273</id><created>2014-03-07</created><authors><author><keyname>Kalinovsky</keyname><forenames>Yakiv O.</forenames></author><author><keyname>Lande</keyname><forenames>Dmitry V.</forenames></author><author><keyname>Boyarinova</keyname><forenames>Yuliya E.</forenames></author><author><keyname>Khitsko</keyname><forenames>Iana V.</forenames></author></authors><title>Some isomorphic classes for noncanonical hypercomplex number systems of
  dimension 2</title><categories>cs.NA</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Building of some isomorphic classes for noncanonical hypercomplex number
systems o dimension 2 is described. In general case, such systems with specific
constraints to structural constants can be isomorphic to complex, dual or
double number system. Isomorphic transition between noncanonical hypercomplex
number systems of the general form and diagonal form is built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2294</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2294</id><created>2014-03-10</created><authors><author><keyname>Nikolaev</keyname><forenames>Sergei</forenames></author></authors><title>Non-linear mass-spring system for large soft tissue deformations
  modeling</title><categories>cs.NA physics.comp-ph</categories><comments>9 pages, 2 figures, 4 charts</comments><journal-ref>Scientific and Technical Journal of Information Technologies,
  Mechanics and Optics 5(87) (2013) 88-94</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Implant placement under soft tissues operation is described. In this
operation tissues can reach such deformations that nonlinear properties are
appeared. A mass-spring model modification for modeling nonlinear tissue
operation is developed. A method for creating elasticity module using splines
is described. For Poisson ratio different stiffness for different types of
springs in cubic grid is used. For stiffness finding an equation system that
described material tension is solved. The model is verified with quadratic
sample tension experiment. These tests show that sample tension under external
forces is equal to defined nonlinear elasticity module. The accuracy of Poisson
ratio modeling is thirty five percent that is better the results of available
ratio modeling method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2295</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2295</id><created>2014-03-10</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh J.</forenames></author></authors><title>Sublinear Models for Graphs</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution extends linear models for feature vectors to sublinear
models for graphs and analyzes their properties. The results are (i) a
geometric interpretation of sublinear classifiers, (ii) a generic learning rule
based on the principle of empirical risk minimization, (iii) a convergence
theorem for the margin perceptron in the sublinearly separable case, and (iv)
the VC-dimension of sublinear functions. Empirical results on graph data show
that sublinear models on graphs have similar properties as linear models for
feature vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2301</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2301</id><created>2014-03-10</created><authors><author><keyname>Balan</keyname><forenames>Radu</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author></authors><title>Phase Retrieval using Lipschitz Continuous Maps</title><categories>math.FA cs.IT math.IT stat.ML</categories><comments>12 pages, 1 figure</comments><msc-class>15A29, 65H10, 90C26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove that reconstruction from magnitudes of frame
coefficients (the so called &quot;phase retrieval problem&quot;) can be performed using
Lipschitz continuous maps. Specifically we show that when the nonlinear
analysis map $\alpha:{\mathcal H}\rightarrow\mathbb{R}^m$ is injective, with
$(\alpha(x))_k=|&lt;x,f_k&gt;|^2$, where $\{f_1,\ldots,f_m\}$ is a frame for the
Hilbert space ${\mathcal H}$, then there exists a left inverse map
$\omega:\mathbb{R}^m\rightarrow {\mathcal H}$ that is Lipschitz continuous.
Additionally we obtain the Lipschitz constant of this inverse map in terms of
the lower Lipschitz constant of $\alpha$. Surprisingly the increase in
Lipschitz constant is independent of the space dimension or frame redundancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2307</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2307</id><created>2014-03-10</created><updated>2015-01-19</updated><authors><author><keyname>Roy</keyname><forenames>Sudip</forenames></author><author><keyname>Kot</keyname><forenames>Lucja</forenames></author><author><keyname>Bender</keyname><forenames>Gabriel</forenames></author><author><keyname>Ding</keyname><forenames>Bailu</forenames></author><author><keyname>Hojjat</keyname><forenames>Hossein</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>The Homeostasis Protocol: Avoiding Transaction Coordination Through
  Program Analysis</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Datastores today rely on distribution and replication to achieve improved
performance and fault-tolerance. But correctness of many applications depends
on strong consistency properties - something that can impose substantial
overheads, since it requires coordinating the behavior of multiple nodes. This
paper describes a new approach to achieving strong consistency in distributed
systems while minimizing communication between nodes. The key insight is to
allow the state of the system to be inconsistent during execution, as long as
this inconsistency is bounded and does not affect transaction correctness. In
contrast to previous work, our approach uses program analysis to extract
semantic information about permissible levels of inconsistency and is fully
automated. We then employ a novel homeostasis protocol to allow sites to
operate independently, without communicating, as long as any inconsistency is
governed by appropriate treaties between the nodes. We discuss mechanisms for
optimizing treaties based on workload characteristics to minimize
communication, as well as a prototype implementation and experiments that
demonstrate the benefits of our approach on common transactional benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2319</identifier>
 <datestamp>2014-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2319</id><created>2014-03-10</created><updated>2014-10-02</updated><authors><author><keyname>Monniaux</keyname><forenames>David</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author></authors><title>Speeding Up Logico-Numerical Strategy Iteration (extended version)</title><categories>cs.LO</categories><comments>Extended version of a paper submitted to SAS 2014</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an efficient combination of polyhedral analysis and predicate
partitioning. Template polyhedral analysis abstracts numerical variables inside
a program by one polyhedron per control location, with a priori fixed
directions for the faces. The strongest inductive invariant in such an abstract
domain may be computed by upward strategy iteration. If the transition relation
includes disjunctions and existential quantifiers (a succinct representation
for an exponential set of paths), this invariant can be computed by a
combination of strategy iteration and satisfiability modulo theory (SMT)
solving. Unfortunately, the above approaches lead to unacceptable space and
time costs if applied to a program whose control states have been partitioned
according to predicates. We therefore propose a modification of the strategy
iteration algorithm where the strategies are stored succinctly, and the linear
programs to be solved at each iteration step are simplified according to an
equivalence relation. We have implemented the technique in a prototype tool and
we demonstrate on a series of examples that the approach performs significantly
better than previous strategy iteration techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2330</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2330</id><created>2014-03-07</created><updated>2014-10-30</updated><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Subspace Clustering by Exploiting a Low-Rank Representation with a
  Symmetric Constraint</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a low-rank representation with symmetric constraint
(LRRSC) method for robust subspace clustering. Given a collection of data
points approximately drawn from multiple subspaces, the proposed technique can
simultaneously recover the dimension and members of each subspace. LRRSC
extends the original low-rank representation algorithm by integrating a
symmetric constraint into the low-rankness property of high-dimensional data
representation. The symmetric low-rank representation, which preserves the
subspace structures of high-dimensional data, guarantees weight consistency for
each pair of data points so that highly correlated data points of subspaces are
represented together. Moreover, it can be efficiently calculated by solving a
convex optimization problem. We provide a rigorous proof for minimizing the
nuclear-norm regularized least square problem with a symmetric constraint. The
affinity matrix for spectral clustering can be obtained by further exploiting
the angular information of the principal directions of the symmetric low-rank
representation. This is a critical step towards evaluating the memberships
between data points. Experimental results on benchmark databases demonstrate
the effectiveness and robustness of LRRSC compared with several
state-of-the-art subspace clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2331</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2331</id><created>2014-03-07</created><authors><author><keyname>Xie</keyname><forenames>Bo</forenames></author><author><keyname>Tan</keyname><forenames>Guang</forenames></author><author><keyname>Liu</keyname><forenames>Yunhuai</forenames></author><author><keyname>Lu</keyname><forenames>Mingming</forenames></author><author><keyname>Chen</keyname><forenames>Kongyang</forenames></author><author><keyname>He</keyname><forenames>Tian</forenames></author></authors><title>LIPS: A Light Intensity Based Positioning System For Indoor Environments</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents LIPS, a Light Intensity based Positioning System for
indoor environments. The system uses off-the-shelf LED lamps as signal sources,
and uses light sensors as signal receivers. The design is inspired by the
observation that a light sensor has deterministic sensitivity to both distance
and incident angle of light signal, an under-utilized feature of photodiodes
now widely found on mobile devices. We develop a stable and accurate light
intensity model to capture the phenomenon, based on which a new positioning
principle, Multi-Face Light Positioning (MFLP), is established that uses three
collocated sensors to uniquely determine the receiver's position, assuming
merely a single source of light. We have implemented a prototype on both
dedicated embedded systems and smartphones. Experimental results show average
positioning accuracy within 0.4 meters across different environments, with high
stability against interferences from obstacles, ambient lights, temperature
variation, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2345</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2345</id><created>2014-03-07</created><authors><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author><author><keyname>Nichols</keyname><forenames>Jeffrey</forenames></author><author><keyname>Drews</keyname><forenames>Clemens</forenames></author></authors><title>Home Location Identification of Twitter Users</title><categories>cs.SI cs.CL cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for inferring the home location of Twitter users
at different granularities, including city, state, time zone or geographic
region, using the content of users tweets and their tweeting behavior. Unlike
existing approaches, our algorithm uses an ensemble of statistical and
heuristic classifiers to predict locations and makes use of a geographic
gazetteer dictionary to identify place-name entities. We find that a
hierarchical classification approach, where time zone, state or geographic
region is predicted first and city is predicted next, can improve prediction
accuracy. We have also analyzed movement variations of Twitter users, built a
classifier to predict whether a user was travelling in a certain period of time
and use that to further improve the location detection accuracy. Experimental
evidence suggests that our algorithm works well in practice and outperforms the
best existing algorithms for predicting the home location of Twitter users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2360</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2360</id><created>2014-03-10</created><authors><author><keyname>Semiari</keyname><forenames>Omid</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author></authors><title>Matching theory for priority-based cell association in the downlink of
  wireless small cell networks</title><categories>cs.IT cs.GT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of small cells, overlaid on existing cellular infrastructure,
is seen as a key feature in next-generation cellular systems. In this paper,
the problem of user association in the downlink of small cell networks (SCNs)
is considered. The problem is formulated as a many-to-one matching game in
which the users and SCBSs rank one another based on utility functions that
account for both the achievable performance, in terms of rate and fairness to
cell edge users, as captured by newly proposed priorities. To solve this game,
a novel distributed algorithm that can reach a stable matching is proposed.
Simulation results show that the proposed approach yields an average utility
gain of up to 65% compared to a common association algorithm that is based on
received signal strength. Compared to the classical deferred acceptance
algorithm, the results also show a 40% utility gain and a more fair utility
distribution among the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2372</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2372</id><created>2014-03-08</created><authors><author><keyname>Naseriparsa</keyname><forenames>Mehdi</forenames></author><author><keyname>Bidgoli</keyname><forenames>Amir-Masoud</forenames></author><author><keyname>Varaee</keyname><forenames>Touraj</forenames></author></authors><title>A Hybrid Feature Selection Method to Improve Performance of a Group of
  Classification Algorithms</title><categories>cs.LG</categories><comments>8 pages. arXiv admin note: substantial text overlap with
  arXiv:1403.1946; and text overlap with arXiv:1106.1813 by other authors</comments><journal-ref>International Journal of Computer Applications,Vol 69,No 17,pp
  28-35,2013</journal-ref><doi>10.5120/12065-8172</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a hybrid feature selection method is proposed which takes
advantages of wrapper subset evaluation with a lower cost and improves the
performance of a group of classifiers. The method uses combination of sample
domain filtering and resampling to refine the sample domain and two feature
subset evaluation methods to select reliable features. This method utilizes
both feature space and sample domain in two phases. The first phase filters and
resamples the sample domain and the second phase adopts a hybrid procedure by
information gain, wrapper subset evaluation and genetic search to find the
optimal feature space. Experiments carried out on different types of datasets
from UCI Repository of Machine Learning databases and the results show a rise
in the average performance of five classifiers (Naive Bayes, Logistic,
Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and
the classification error for these classifiers decreases considerably. The
experiments also show that this method outperforms other feature selection
methods with a lower cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2373</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2373</id><created>2014-03-08</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A weakly universal cellular automaton in the pentagrid with five states</title><categories>nlin.CG cs.DM</categories><comments>23 pages, 21 figures</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct a cellular automaton on the pentagrid which is
planar, weakly universal and which have five states only. This result much
improves the best result which was with nine states
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2395</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2395</id><created>2014-03-10</created><updated>2014-03-17</updated><authors><author><keyname>Guillam&#xf3;n</keyname><forenames>Francisco Belch&#xed;</forenames></author><author><keyname>Mas</keyname><forenames>Aniceto Murillo</forenames></author></authors><title>A-infinity Persistence</title><categories>math.AT cs.CG cs.CV</categories><comments>22 pages, no figures. In versions 2 and 3, we added our e-mail
  addresses and made some minor corrections, thanks to Jim Stasheff</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study A-infinity persistence of a given homology filtration
of topological spaces. This is a family, one for each n &gt; 0, of homological
invariants which provide information not readily available by the (persistent)
Betti numbers of the given filtration. This may help to detect noise, not just
in the simplicial structure of the filtration but in further geometrical
properties in which the higher codiagonals of the A-infinity structure are
translated. Based in the classification of zigzag modules, a characterization
of the A-infinity persistence in terms of its associated barcode is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2400</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2400</id><created>2014-03-10</created><authors><author><keyname>Baik</keyname><forenames>Jinho</forenames></author><author><keyname>Nadakuditi</keyname><forenames>Raj Rao</forenames></author></authors><title>Batch latency analysis and phase transitions for a tandem of queues with
  exponentially distributed service times</title><categories>math.PR cs.IT math.IT</categories><comments>To be publised in Queuing Systems: Theory and Applications</comments><doi>10.1007/s11134-014-9401-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the latency or sojourn time L(m,n) for the last customer in a
batch of n customers to exit from the m-th queue in a tandem of m queues in the
setting where the queues are in equilibrium before the batch of customers
arrives at the first queue. We first characterize the distribution of L(m,n)
exactly for every m and n, under the assumption that the queues have unlimited
buffers and that each server has customer independent, exponentially
distributed service times with an arbitrary, known rate. We then evaluate the
first two leading order terms of the distributions in the large m and n limit
and bring into sharp focus the existence of phase transitions in the system
behavior. The phase transition occurs due to the presence of either slow
bottleneck servers or a high external arrival rate. We determine the critical
thresholds for the service rate and the arrival rate, respectively, about which
this phase transition occurs; it turns out that they are the same. This
critical threshold depends, in a manner we make explicit, on the individual
service rates, the number of customers and the number of queues but not on the
external arrival rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2404</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2404</id><created>2014-03-10</created><authors><author><keyname>Cheng</keyname><forenames>Long</forenames></author><author><keyname>Malik</keyname><forenames>Avinash</forenames></author><author><keyname>Kotoulas</keyname><forenames>Spyros</forenames></author><author><keyname>Ward</keyname><forenames>Tomas E</forenames></author><author><keyname>Theodoropoulos</keyname><forenames>Georgios</forenames></author></authors><title>Scalable RDF Data Compression using X10</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Web comprises enormous volumes of semi-structured data elements.
For interoperability, these elements are represented by long strings. Such
representations are not efficient for the purposes of Semantic Web applications
that perform computations over large volumes of information. A typical method
for alleviating the impact of this problem is through the use of compression
methods that produce more compact representations of the data. The use of
dictionary encoding for this purpose is particularly prevalent in Semantic Web
database systems. However, centralized implementations present performance
bottlenecks, giving rise to the need for scalable, efficient distributed
encoding schemes. In this paper, we describe an encoding implementation based
on the asynchronous partitioned global address space (APGAS) parallel
programming model. We evaluate performance on a cluster of up to 384 cores and
datasets of up to 11 billion triples (1.9 TB). Compared to the state-of-art
MapReduce algorithm, we demonstrate a speedup of 2.6-7.4x and excellent
scalability. These results illustrate the strong potential of the APGAS model
for efficient implementation of dictionary encoding and contributes to the
engineering of larger scale Semantic Web applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2407</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2407</id><created>2014-03-07</created><updated>2015-02-03</updated><authors><author><keyname>Bernstein</keyname><forenames>Andrey</forenames><affiliation>Laboratory for Communications and Applications 2. &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne</affiliation></author><author><keyname>Reyes-Chamorro</keyname><forenames>Lorenzo</forenames><affiliation>Distributed Electrical Systems Laboratory. &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne</affiliation></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames><affiliation>Laboratory for Communications and Applications 2. &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne</affiliation></author><author><keyname>Paolone</keyname><forenames>Mario</forenames><affiliation>Distributed Electrical Systems Laboratory. &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne</affiliation></author></authors><title>A Composable Method for Real-Time Control of Active Distribution
  Networks with Explicit Power Setpoints</title><categories>cs.SY</categories><comments>71 pages, 21 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional approach for the control of distribution networks, in the
presence of active generation and/or controllable loads and storage, involves a
combination of both frequency and voltage regulation at different time scales.
With the increased penetration of stochastic resources, distributed generation
and demand response, this approach shows severe limitations in both the optimal
and feasible operation of these networks, as well as in the aggregation of the
network resources for upper-layer power systems. An alternative approach is to
directly control the targeted grid by defining explicit and real-time setpoints
for active/reactive power absorptions/injections defined by a solution of a
specific optimization problem; but this quickly becomes intractable when
systems get large or diverse. In this paper, we address this problem and
propose a method for the explicit control of the grid status, based on a common
abstract model characterized by the main property of being composable. That is
to say, subsystems can be aggregated into virtual devices that hide their
internal complexity. Thus the proposed method can easily cope with systems of
any size or complexity. The framework is presented in this Part I, whilst in
Part II we illustrate its application to a CIGR\'E low voltage benchmark
microgrid. In particular, we provide implementation examples with respect to
typical devices connected to distribution networks and evaluate of the
performance and benefits of the proposed control framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2411</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2411</id><created>2014-03-10</created><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Probabilistic Robustness Analysis of Stochastic Jump Linear Systems</title><categories>cs.SY math.DS</categories><comments>2014 ACC(American Control Conference) paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new method to measure the probabilistic
robustness of stochastic jump linear system with respect to both the initial
state uncertainties and the randomness in switching. Wasserstein distance which
defines a metric on the manifold of probability density functions is used as
tool for the performance and the stability measures. Starting with Gaussian
distribution to represent the initial state uncertainties, the probability
density function of the system state evolves into mixture of Gaussian, where
the number of Gaussian components grows exponentially. To cope with
computational complexity caused by mixture of Gaussian, we prove that there
exists an alternative probability density function that preserves exact
information in the Wasserstein level. The usefulness and the efficiency of the
proposed methods are demonstrated by example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2431</identifier>
 <datestamp>2014-08-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2431</id><created>2014-03-10</created><updated>2014-08-07</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author></authors><title>A Subquadratic Algorithm for Minimum Palindromic Factorization</title><categories>cs.DS cs.DM</categories><comments>Accepted for publication in Journal of Discrete Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $\mathcal{O}(n \log n)$-time, $\mathcal{O}(n)$-space algorithm for
factoring a string into the minimum number of palindromic substrings. That is,
given a string $S [1..n]$, in $\mathcal{O}(n \log n)$ time our algorithm
returns the minimum number of palindromes $S_1,\ldots, S_\ell$ such that $S =
S_1 \cdots S_\ell$. We also show that the time complexity is $\mathcal{O}(n)$
on average and $\Omega(n\log n)$ in the worst case. The last result is based on
a characterization of the palindromic structure of Zimin words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2433</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2433</id><created>2014-03-10</created><authors><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author><author><keyname>Frongillo</keyname><forenames>Rafael M.</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>Generalised Mixability, Constant Regret, and Bayesian Updating</title><categories>cs.LG stat.ML</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixability of a loss is known to characterise when constant regret bounds are
achievable in games of prediction with expert advice through the use of Vovk's
aggregating algorithm. We provide a new interpretation of mixability via convex
analysis that highlights the role of the Kullback-Leibler divergence in its
definition. This naturally generalises to what we call $\Phi$-mixability where
the Bregman divergence $D_\Phi$ replaces the KL divergence. We prove that
losses that are $\Phi$-mixable also enjoy constant regret bounds via a
generalised aggregating algorithm that is similar to mirror descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2439</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2439</id><created>2014-03-10</created><authors><author><keyname>Acharya</keyname><forenames>Jayadev</forenames></author><author><keyname>Das</keyname><forenames>Hirakendu</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author><author><keyname>Orlitsky</keyname><forenames>Alon</forenames></author><author><keyname>Pan</keyname><forenames>Shengjun</forenames></author></authors><title>String Reconstruction from Substring Compositions</title><categories>cs.DM cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by mass-spectrometry protein sequencing, we consider a
simply-stated problem of reconstructing a string from the multiset of its
substring compositions. We show that all strings of length 7, one less than a
prime, or one less than twice a prime, can be reconstructed uniquely up to
reversal. For all other lengths we show that reconstruction is not always
possible and provide sometimes-tight bounds on the largest number of strings
with given substring compositions. The lower bounds are derived by
combinatorial arguments and the upper bounds by algebraic considerations that
precisely characterize the set of strings with the same substring compositions
in terms of the factorization of bivariate polynomials. The problem can be
viewed as a combinatorial simplification of the turnpike problem, and its
solution may shed light on this long-standing problem as well. Using well known
results on transience of multi-dimensional random walks, we also provide a
reconstruction algorithm that reconstructs random strings over alphabets of
size $\ge4$ in optimal near-quadratic time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2471</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2471</id><created>2014-03-10</created><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Halder</keyname><forenames>Abhishek</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Mean Square Stability for Stochastic Jump Linear Systems via Optimal
  Transport</title><categories>cs.SY math.DS</categories><comments>6 pages, no figures, IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we provide a unified framework for the mean square stability of
stochastic jump linear systems via optimal transport. The Wasserstein metric
known as an optimal transport, that assesses the distance between probability
density functions enables the stability analysis. Without any assumption on the
underlying jump process, this Wasserstein distance guarantees the mean square
stability for general stochastic jump linear systems, not necessarily for
Markovian jump. The validity of the proposed methods are proved by recovering
already-known stability conditions under this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2482</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2482</id><created>2014-03-11</created><authors><author><keyname>Hu</keyname><forenames>Haijuan</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Liu</keyname><forenames>Quansheng</forenames></author></authors><title>Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted
  Means</title><categories>cs.CV</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first establish a law of large numbers and a convergence theorem in
distribution to show the rate of convergence of the non-local means filter for
removing Gaussian noise. We then introduce the notion of degree of similarity
to measure the role of similarity for the non-local means filter. Based on the
convergence theorems, we propose a patch-based weighted means filter for
removing impulse noise and its mixture with Gaussian noise by combining the
essential idea of the trilateral filter and that of the non-local means filter.
Our experiments show that our filter is competitive compared to recently
proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2483</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2483</id><created>2014-03-11</created><updated>2015-03-02</updated><authors><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Optimal Sampling-Based Motion Planning under Differential Constraints:
  the Driftless Case</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning under differential constraints is a classic problem in
robotics. To date, the state of the art is represented by sampling-based
techniques, with the Rapidly-exploring Random Tree algorithm as a leading
example. Yet, the problem is still open in many aspects, including guarantees
on the quality of the obtained solution. In this paper we provide a thorough
theoretical framework to assess optimality guarantees of sampling-based
algorithms for planning under differential constraints. We exploit this
framework to design and analyze two novel sampling-based algorithms that are
guaranteed to converge, as the number of samples increases, to an optimal
solution (namely, the Differential Probabilistic RoadMap algorithm and the
Differential Fast Marching Tree algorithm). Our focus is on driftless
control-affine dynamical models, which accurately model a large class of
robotic systems. In this paper we use the notion of convergence in probability
(as opposed to convergence almost surely): the extra mathematical flexibility
of this approach yields convergence rate bounds - a first in the field of
optimal sampling-based motion planning under differential constraints.
Numerical experiments corroborating our theoretical results are presented and
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2484</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2484</id><created>2014-03-11</created><authors><author><keyname>Fang</keyname><forenames>Meng</forenames></author><author><keyname>Yin</keyname><forenames>Jie</forenames></author><author><keyname>Zhu</keyname><forenames>Xingquan</forenames></author></authors><title>Transfer Learning across Networks for Collective Classification</title><categories>cs.LG cs.SI</categories><comments>Published in the proceedings of IEEE ICDM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of transferring useful knowledge from a
source network to predict node labels in a newly formed target network. While
existing transfer learning research has primarily focused on vector-based data,
in which the instances are assumed to be independent and identically
distributed, how to effectively transfer knowledge across different information
networks has not been well studied, mainly because networks may have their
distinct node features and link relationships between nodes. In this paper, we
propose a new transfer learning algorithm that attempts to transfer common
latent structure features across the source and target networks. The proposed
algorithm discovers these latent features by constructing label propagation
matrices in the source and target networks, and mapping them into a shared
latent feature space. The latent features capture common structure patterns
shared by two networks, and serve as domain-independent features to be
transferred between networks. Together with domain-dependent node features, we
thereafter propose an iterative classification algorithm that leverages label
correlations to predict node labels in the target network. Experiments on
real-world networks demonstrate that our proposed algorithm can successfully
achieve knowledge transfer between networks to help improve the accuracy of
classifying nodes in the target network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2485</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2485</id><created>2014-03-11</created><updated>2014-05-25</updated><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>Optimal interval clustering: Application to Bregman clustering and
  statistical mixture learning</title><categories>cs.IT cs.LG math.IT</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generic dynamic programming method to compute the optimal
clustering of $n$ scalar elements into $k$ pairwise disjoint intervals. This
case includes 1D Euclidean $k$-means, $k$-medoids, $k$-medians, $k$-centers,
etc. We extend the method to incorporate cluster size constraints and show how
to choose the appropriate $k$ by model selection. Finally, we illustrate and
refine the method on two case studies: Bregman clustering and statistical
mixture learning maximizing the complete likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2486</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2486</id><created>2014-03-11</created><authors><author><keyname>Saito</keyname><forenames>Hiroshi</forenames></author><author><keyname>Kawahara</keyname><forenames>Ryoichi</forenames></author></authors><title>Theoretical Evaluation of Offloading through Wireless LANs</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offloading of cellular traffic through a wireless local area network (WLAN)
is theoretically evaluated. First, empirical data sets of the locations of WLAN
internet access points are analyzed and an inhomogeneous Poisson process
consisting of high, normal, and low density regions is proposed as a spatial
point process model for these configurations. Second, performance metrics, such
as mean available bandwidth for a user and the number of vertical handovers,
are evaluated for the proposed model through geometric analysis. Explicit
formulas are derived for the metrics, although they depend on many parameters
such as the number of WLAN access points, the shape of each WLAN coverage
region, the location of each WLAN access point, the available bandwidth (bps)
of the WLAN, and the shape and available bandwidth (bps) of each subregion
identified by the channel quality indicator in a cell of the cellular network.
Explicit formulas strongly suggest that the bandwidth a user experiences does
not depend on the user mobility. This is because the bandwidth available by a
user who does not move and that available by a user who moves are the same or
approximately the same as a probabilistic distribution. Numerical examples show
that parameters, such as the size of regions where placement of WLAN access
points is not allowed and the mean density of WLANs in high density regions,
have a large impact on performance metrics. In particular, a homogeneous
Poisson process model as the WLAN access point location model largely
overestimates the mean available bandwidth for a user and the number of
vertical handovers. The overestimated mean available bandwidth is, for example,
about 50% in a certain condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2498</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2498</id><created>2014-03-11</created><authors><author><keyname>Wu</keyname><forenames>Qihui</forenames><affiliation>Corresponding author</affiliation></author><author><keyname>Ding</keyname><forenames>Guoru</forenames><affiliation>Corresponding author</affiliation></author><author><keyname>Xu</keyname><forenames>Yuhua</forenames></author><author><keyname>Feng</keyname><forenames>Shuo</forenames></author><author><keyname>Du</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Jinlong</forenames></author><author><keyname>Long</keyname><forenames>Keping</forenames></author></authors><title>Cognitive Internet of Things: A New Paradigm beyond Connection</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research on Internet of Things (IoT) mainly focuses on how to enable
general objects to see, hear, and smell the physical world for themselves, and
make them connected to share the observations. In this paper, we argue that
only connected is not enough, beyond that, general objects should have the
capability to learn, think, and understand both physical and social worlds by
themselves. This practical need impels us to develop a new paradigm, named
Cognitive Internet of Things (CIoT), to empower the current IoT with a `brain'
for high-level intelligence. Specifically, we first present a comprehensive
definition for CIoT, primarily inspired by the effectiveness of human
cognition. Then, we propose an operational framework of CIoT, which mainly
characterizes the interactions among five fundamental cognitive tasks:
perception-action cycle, massive data analytics, semantic derivation and
knowledge discovery, intelligent decision-making, and on-demand service
provisioning. Furthermore, we provide a systematic tutorial on key enabling
techniques involved in the cognitive tasks. In addition, we also discuss the
design of proper performance metrics on evaluating the enabling techniques.
Last but not least, we present the research challenges and open issues ahead.
Building on the present work and potentially fruitful future studies, CIoT has
the capability to bridge the physical world (with objects, resources, etc.) and
the social world (with human demand, social behavior, etc.), and enhance smart
resource allocation, automatic network operation, and intelligent service
provisioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2499</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2499</id><created>2014-03-11</created><authors><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author></authors><title>Application of Constacyclic codes to Quantum MDS Codes</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum maximal-distance-separable (MDS) codes form an important class of
quantum codes. To get $q$-ary quantum MDS codes, it suffices to find linear MDS
codes $C$ over $\mathbb{F}_{q^2}$ satisfying $C^{\perp_H}\subseteq C$ by the
Hermitian construction and the quantum Singleton bound. If
$C^{\perp_{H}}\subseteq C$, we say that $C$ is a dual-containing code. Many new
quantum MDS codes with relatively large minimum distance have been produced by
constructing dual-containing constacyclic MDS codes (see \cite{Guardia11},
\cite{Kai13}, \cite{Kai14}). These works motivate us to make a careful study on
the existence condition for nontrivial dual-containing constacyclic codes. This
would help us to avoid unnecessary attempts and provide effective ideas in
order to construct dual-containing codes. Several classes of dual-containing
MDS constacyclic codes are constructed and their parameters are computed.
Consequently, new quantum MDS codes are derived from these parameters. The
quantum MDS codes exhibited here have parameters better than the ones available
in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2508</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2508</id><created>2014-03-11</created><authors><author><keyname>Khatua</keyname><forenames>Sunirmal</forenames></author><author><keyname>Sur</keyname><forenames>Preetam K.</forenames></author><author><keyname>Das</keyname><forenames>Rajib K.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Nandini</forenames></author></authors><title>Heuristic-based Optimal Resource Provisioning in Application-centric
  Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Service Providers (CSPs) adapt different pricing models for their
offered services. Some of the models are suitable for short term requirement
while others may be suitable for the Cloud Service User's (CSU) long term
requirement. In this paper, we look at the problem of finding the amount of
resources to be reserved to satisfy the CSU's long term demands with the aim of
minimizing the total cost. Finding the optimal resource requirement to satisfy
the the CSU's demand for resources needs sufficient research effort. Various
algorithms were discussed in the last couple of years for finding the optimal
resource requirement but most of them are based on IPP which is NP in nature.
In this paper, we derive some heuristic-based polynomial time algorithms to
find some near optimal solution to the problem. We show that the cost for CSU
using our approach is comparable to the solution obtained using optimal Integer
Programming Problem(IPP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2514</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2514</id><created>2014-03-11</created><authors><author><keyname>Trombetta</keyname><forenames>Alberto</forenames></author><author><keyname>Persiano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Braghin</keyname><forenames>Stefano</forenames></author></authors><title>Answering queries using pairings</title><categories>cs.CR</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Outsourcing data in the cloud has become nowadays very common. Since --
generally speaking -- cloud data storage and management providers cannot be
fully trusted, mechanisms providing the confidentiality of the stored data are
necessary. A possible solution is to encrypt all the data, but -- of course --
this poses serious problems about the effective usefulness of the stored data.
In this work, we propose to apply a well-known attribute-based cryptographic
scheme to cope with the problem of querying encrypted data. We have implemented
the proposed scheme with a real-world, off-the-shelf RDBMS and we provide
several experimental results showing the feasibility of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2519</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2519</id><created>2014-03-11</created><authors><author><keyname>Bella</keyname><forenames>Fabio</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author></authors><title>Baselining Wireless Internet Service Development: An Experience Report</title><categories>cs.SE</categories><comments>9 pages</comments><journal-ref>Proceedings of the 5th International Conference on Quality in
  Information and Communication Technologies (QUATIC 2004), pages 161-169,
  Porto, Portugal, October 18-20 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New, emerging domains such as the engineering of wireless Internet services
are characterized by a lack of experience based on quantitative data.
Systematic tracking and observation of representative pilot projects can be
seen as one means to capture experience, get valuable insight into a new
domain, and build initial baselines. This helps to improve the planning of real
development projects in business units. This article describes an approach to
capture software development experience for the wireless Internet services
domain by conducting and observing a series of case studies in the field.
Initial baselines concerning effort distribution from the development of two
wireless Internet pilot services are presented. Furthermore, major
domain-specific risk factors are discussed based on the results of project
retrospectives conducted with the developers of the services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2527</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2527</id><created>2014-03-11</created><updated>2014-09-15</updated><authors><author><keyname>Zhang</keyname><forenames>Runwei</forenames></author><author><keyname>Ingelrest</keyname><forenames>Francois</forenames></author><author><keyname>Barrenetxea</keyname><forenames>Guillermo</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>The Beauty of the Commons: Optimal Load Sharing by Base Station Hopping
  in Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless sensor networks (WSNs), the base station (BS) is a critical
sensor node whose failure causes severe data losses. Deploying multiple fixed
BSs improves the robustness, yet requires all BSs to be installed with large
batteries and large energy-harvesting devices due to the high energy
consumption of BSs. In this paper, we propose a scheme to coordinate the
multiple deployed BSs such that the energy supplies required by individual BSs
can be substantially reduced. In this scheme, only one BS is selected to be
active at a time and the other BSs act as regular sensor nodes. We first
present the basic architecture of our system, including how we keep the network
running with only one active BS and how we manage the handover of the role of
the active BS. Then, we propose an algorithm for adaptively selecting the
active BS under the spatial and temporal variations of energy resources. This
algorithm is simple to implement but is also asymptotically optimal under mild
conditions. Finally, by running simulations and real experiments on an outdoor
testbed, we verify that the proposed scheme is energy-efficient, has low
communication overhead and reacts rapidly to network changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2531</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2531</id><created>2014-03-11</created><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>HoTT formalisation in Coq: Dependency Graphs \&amp; ML4PG</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note is a response to Bas Spitter's email of 28 February 2014 about
ML4PG:
  &quot;We (Jason actually) are adding dependency graphs to our HoTT library:
https://github.com/HoTT/HoTT/wiki
  I seem to recall that finding the dependency graph was a main obstacle
preventing machine learning to be used in Coq. However, you seem to have made
progress on this. What tool are you using?
https://anne.pacalet.fr/dev/dpdgraph/ ? Or another tool? Would it be easy to
use your ML4PG on the HoTT library?&quot;
  This note gives explanation of how ML4PG can be used in the HoTT library and
how ML4PG relates to the two kinds of Dependency graphs available in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2535</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2535</id><created>2014-03-11</created><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>A Delay-Constrained Protocol with Adaptive Mode Selection for
  Bidirectional Relay Networks</title><categories>cs.IT math.IT</categories><comments>This technical report is an extended version of a paper submitted to
  IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a bidirectional relay network with half-duplex
nodes and block fading where the nodes transmit with a fixed transmission rate.
Thereby, user 1 and user 2 exchange information only via a relay node, i.e., a
direct link between both users is not present. Recently in [1], it was shown
that a considerable gain in terms of sum throughput can be obtained in
bidirectional relaying by optimally selecting the transmission modes or,
equivalently, the states of the nodes, i.e., the transmit, the receive, and the
silent states, in each time slot based on the qualities of the involved links.
To enable adaptive transmission mode selection, the relay has to be equipped
with two buffers for storage of the data received from the two users. However,
the protocol proposed in [1] was delay-unconstrained and provides an upper
bound for the performance of practical delay-constrained protocols. In this
paper, we propose a heuristic but efficient delay-constrained protocol which
can approach the performance upper bound reported in [1], even in cases where
only a small average delay is permitted. In particular, the proposed protocol
does not only take into account the instantaneous qualities of the involved
links for adaptive mode selection but also the states of the queues at the
buffers. The average throughput and the average delay of the proposed
delay-constrained protocol are evaluated by analyzing the Markov chain of the
states of the queues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2541</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2541</id><created>2014-03-11</created><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Turing: Then, Now and Still Key</title><categories>cs.AI</categories><comments>Published</comments><journal-ref>'Artificial Intelligence, Evolutionary Computation and
  Metaheuristics (AIECM) - Turing 2012', Eds. X-S. Yang, Studies in
  Computational Intelligence, 2013, Vol. 427/2013, pp. 43-62, Springer-Verlag
  Berlin Heidelberg</journal-ref><doi>10.1007/978-3-642-29694-9_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks at Turing's postulations about Artificial Intelligence in
his paper 'Computing Machinery and Intelligence', published in 1950. It notes
how accurate they were and how relevant they still are today. This paper notes
the arguments and mechanisms that he suggested and tries to expand on them
further. The paper however is mostly about describing the essential ingredients
for building an intelligent model and the problems related with that. The
discussion includes recent work by the author himself, who adds his own
thoughts on the matter that come from a purely technical investigation into the
problem. These are personal and quite speculative, but provide an interesting
insight into the mechanisms that might be used for building an intelligent
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2548</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2548</id><created>2014-03-11</created><authors><author><keyname>George</keyname><forenames>Neenu</forenames></author><author><keyname>Parani</keyname><forenames>T. K.</forenames></author></authors><title>Detection of Node Clones in Wireless Sensor Network Using Detection
  Protocols</title><categories>cs.NI</categories><comments>6 pages,5 figures,&quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;Volume8 Number 6-Feb2014</comments><journal-ref>nternational Journal of Engineering Trends and Technology (IJETT)
  Volume8 Number 6-Feb2014</journal-ref><doi>10.14445/22315381/IJETT-V8P253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks consist of hundreds to thousands of sensor nodes and
are widely used in civilian and security applications. One of the serious
physical attacks faced by the wireless sensor network is node clone attack.
Thus two node clone detection protocols are introduced via distributed hash
table and randomly directed exploration to detect node clones. The former is
based on a hash table value which is already distributed and provides key based
facilities like checking and caching to detect node clones. The later one is
using probabilistic directed forwarding technique and border determination. The
simulation results for storage consumption, communication cost and detection
probability is done using NS2 and obtained randomly directed exploration is the
best one having low communication cost and storage consumption and has good
detection probability.
  Keywords: wireless sensor networks (wsn), distributed hash table, randomly
directed exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2580</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2580</id><created>2014-03-11</created><updated>2014-09-15</updated><authors><author><keyname>Ju</keyname><forenames>Hyungsik</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Optimal Resource Allocation in Full-Duplex Wireless-Powered
  Communication Network</title><categories>cs.IT math.IT</categories><comments>31 pages, 10 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies optimal resource allocation in the wireless-powered
communication network (WPCN), where one hybrid access-point (H-AP) operating in
full-duplex (FD) broadcasts wireless energy to a set of distributed users in
the downlink (DL) and at the same time receives independent information from
the users via time-division-multiple-access (TDMA) in the uplink (UL). We
design an efficient protocol to support simultaneous wireless energy transfer
(WET) in the DL and wireless information transmission (WIT) in the UL for the
proposed FD-WPCN. We jointly optimize the time allocations to the H-AP for DL
WET and different users for UL WIT as well as the transmit power allocations
over time at the H-AP to maximize the users' weighted sum-rate of UL
information transmission with harvested energy. We consider both the cases with
perfect and imperfect self-interference cancellation (SIC) at the H-AP, for
which we obtain optimal and suboptimal time and power allocation solutions,
respectively. Furthermore, we consider the half-duplex (HD) WPCN as a baseline
scheme and derive its optimal resource allocation solution. Simulation results
show that the FD-WPCN outperforms HD-WPCN when effective SIC can be implemented
and more stringent peak power constraint is applied at the H-AP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2625</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2625</id><created>2014-03-11</created><authors><author><keyname>Chaudhuri</keyname><forenames>Sruti Gan</forenames></author><author><keyname>Ghike</keyname><forenames>Swapnil</forenames></author><author><keyname>Jain</keyname><forenames>Shrainik</forenames></author><author><keyname>Mukhopadhyaya</keyname><forenames>Krishnendu</forenames></author></authors><title>Pattern Formation for Asynchronous Robots without Agreement in Chirality</title><categories>cs.DC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2628</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2628</id><created>2014-03-11</created><updated>2014-05-28</updated><authors><author><keyname>Kahramanogullari</keyname><forenames>Ozan</forenames><affiliation>The Microsoft Research - University of Trento Centre for Computational and Syste</affiliation></author></authors><title>Interaction and Depth against Nondeterminism in Proof Search</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 2 (May 30,
  2014) lmcs:1089</journal-ref><doi>10.2168/LMCS-10(2:5)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep inference is a proof theoretic methodology that generalizes the standard
notion of inference of the sequent calculus, whereby inference rules become
applicable at any depth inside logical expressions. Deep inference provides
more freedom in the design of deductive systems for different logics and a rich
combinatoric analysis of proofs. In particular, construction of exponentially
shorter analytic proofs becomes possible, however with the cost of a greater
nondeterminism than in the sequent calculus. In this paper, we show that the
nondeterminism in proof search can be reduced without losing the shorter proofs
and without sacrificing proof theoretic cleanliness. For this, we exploit an
interaction and depth scheme in the logical expressions. We demonstrate our
method on deep inference systems for multiplicative linear logic and classical
logic, discuss its proof complexity and its relation to focusing, and present
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2630</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2630</id><created>2014-03-11</created><authors><author><keyname>Gnang</keyname><forenames>Edinah K.</forenames></author><author><keyname>Parzanchevski</keyname><forenames>Ori</forenames></author><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author></authors><title>A SageTeX Hypermatrix Algebra Package</title><categories>cs.MS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe here a rudimentary sage implementation of the Bhattacharya-Mesner
hypermatrix algebra package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2639</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2639</id><created>2014-03-11</created><authors><author><keyname>Mischler</keyname><forenames>Antoine</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>An Approach for Discovering Traceability Links between Regulatory
  Documents and Source Code Through User-Interface Labels</title><categories>cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In application domains that are regulated, software vendors must maintain
traceability links between the regulatory items and the code base implementing
them. In this paper, we present a traceability approach based on the intuition
that the regulatory documents and the user-interface of the corresponding
software applications are very close. First, they use the same terminology.
Second, most important regulatory pieces of information appear in the graphical
user-interface because the end-users in those application domains care about
the regulation (by construction). We evaluate our approach in the domain of
green building. The evaluation involves a domain expert, lead architect of a
commercial product within this area. The evaluation shows that the recovered
traceability links are accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2642</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2642</id><created>2014-03-11</created><updated>2014-05-02</updated><authors><author><keyname>Taylor</keyname><forenames>Richard H.</forenames></author><author><keyname>Rose</keyname><forenames>Frisco</forenames></author><author><keyname>Toher</keyname><forenames>Cormac</forenames></author><author><keyname>Levy</keyname><forenames>Ohad</forenames></author><author><keyname>Nardelli</keyname><forenames>Marco Buongiorno</forenames></author><author><keyname>Curtarolo</keyname><forenames>Stefano</forenames></author></authors><title>A RESTful API for exchanging Materials Data in the AFLOWLIB.org
  consortium</title><categories>cond-mat.mtrl-sci cs.DL</categories><comments>22 pages, 7 figures</comments><journal-ref>Comp. Mat. Sci., 93, 2014, 178-192</journal-ref><doi>10.1016/j.commatsci.2014.05.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continued advancement of science depends on shared and reproducible data.
In the field of computational materials science and rational materials design
this entails the construction of large open databases of materials properties.
To this end, an Application Program Interface (API) following REST principles
is introduced for the AFLOWLIB.org materials data repositories consortium.
AUIDs (Aflowlib Unique IDentifier) and AURLs (Aflowlib Uniform Resource
locator) are assigned to the database resources according to a well-defined
protocol described herein, which enables the client to access, through
appropriate queries, the desired data for post-processing. This introduces a
new level of openness into the AFLOWLIB repository, allowing the community to
construct high-level work-flows and tools exploiting its rich data set of
calculated structural, thermodynamic, and electronic properties. Furthermore,
federating these tools would open the door to collaborative investigation of
the data by an unprecedented extended community of users to accelerate the
advancement of computational materials design and development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2649</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2649</id><created>2014-03-11</created><authors><author><keyname>Basu</keyname><forenames>Kinjal</forenames></author><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>Low discrepancy constructions in the triangle</title><categories>math.NA cs.NA stat.CO</categories><msc-class>11K38, 65D32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most quasi-Monte Carlo research focuses on sampling from the unit cube. Many
problems, especially in computer graphics, are defined via quadrature over the
unit triangle. Quasi-Monte Carlo methods for the triangle have been developed
by Pillands and Cools (2005) and by Brandolini et al. (2013). This paper
presents two QMC constructions in the triangle with a vanishing discrepancy.
The first is a version of the van der Corput sequence customized to the unit
triangle. It is an extensible digital construction that attains a discrepancy
below 12/sqrt(N). The second construction rotates an integer lattice through an
angle whose tangent is a quadratic irrational number. It attains a discrepancy
of O(log(N)/N) which is the best possible rate. Previous work strongly
indicated that such a discrepancy was possible, but no constructions were
available. Scrambling the digits of the first construction improves its
accuracy for integration of smooth functions. Both constructions also yield
convergent estimates for integrands that are Riemann integrable on the triangle
without requiring bounded variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2653</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2653</id><created>2014-03-09</created><authors><author><keyname>Kov&#xe1;cs</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>G&#xe9;za</forenames></author></authors><title>Multiple coverings with closed polygons</title><categories>math.MG cs.CG math.CO</categories><comments>arXiv admin note: text overlap with arXiv:1009.4641 by other authors</comments><msc-class>52C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A planar set $P$ is said to be cover-decomposable if there is a constant
$k=k(P)$ such that every $k$-fold covering of the plane with translates of $P$
can be decomposed into two coverings. It is known that open convex polygons are
cover-decomposable. Here we show that closed, centrally symmetric convex
polygons are also cover-decomposable. We also show that an infinite-fold
covering of the plane with translates of $P$ can be decomposed into two
infinite-fold coverings. Both results hold for coverings of any subset of the
plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2654</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2654</id><created>2014-03-11</created><authors><author><keyname>Chen</keyname><forenames>Yanping</forenames></author><author><keyname>Why</keyname><forenames>Adena</forenames></author><author><keyname>Batista</keyname><forenames>Gustavo</forenames></author><author><keyname>Mafra-Neto</keyname><forenames>Agenor</forenames></author><author><keyname>Keogh</keyname><forenames>Eamonn</forenames></author></authors><title>Flying Insect Classification with Inexpensive Sensors</title><categories>cs.LG cs.CE</categories><msc-class>68T00</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The ability to use inexpensive, noninvasive sensors to accurately classify
flying insects would have significant implications for entomological research,
and allow for the development of many useful applications in vector control for
both medical and agricultural entomology. Given this, the last sixty years have
seen many research efforts on this task. To date, however, none of this
research has had a lasting impact. In this work, we explain this lack of
progress. We attribute the stagnation on this problem to several factors,
including the use of acoustic sensing devices, the over-reliance on the single
feature of wingbeat frequency, and the attempts to learn complex models with
relatively little data. In contrast, we show that pseudo-acoustic optical
sensors can produce vastly superior data, that we can exploit additional
features, both intrinsic and extrinsic to the insect's flight behavior, and
that a Bayesian classification approach allows us to efficiently learn
classification models that are very robust to over-fitting. We demonstrate our
findings with large scale experiments that dwarf all previous works combined,
as measured by the number of insects and the number of species considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2656</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2656</id><created>2014-03-11</created><authors><author><keyname>White</keyname><forenames>Robert R.</forenames></author><author><keyname>Munch</keyname><forenames>Kristin</forenames></author></authors><title>Handling Large and Complex Data in a Photovoltaic Research Institution
  Using a Custom Laboratory Information Management System</title><categories>cs.DL cs.CY</categories><comments>12 pages, 8 figures Prepared for the MRS Fall 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twenty-five years ago the desktop computer started becoming ubiquitous in the
scientific lab. Researchers were delighted with its ability to both control
instrumentation and acquire data on a single system, but they were not
completely satisfied. There were often gaps in knowledge that they thought
might be gained if they just had more data and they could get the data faster.
Computer technology has evolved in keeping with Moore's Law meeting those
desires; however those improvement have of late become both a boon and bane for
researchers. Computers are now capable of producing high speed data streams
containing terabytes of information; capabilities that evolved faster than
envisioned last century. Software to handle large scientific data sets has not
kept up. How much information might be lost through accidental mismanagement or
how many discoveries are missed through data overload are now vital questions.
An important new task in most scientific disciplines involves developing
methods to address those issues and to create software that can handle large
data sets with an eye towards scalability. This software must create archived,
indexed, and searchable data from heterogeneous instrumentation for the
implementation of a strong data-driven materials development strategy. At the
National Center for Photovoltaics in the National Renewable Energy Lab, we
began development a few years ago on a Laboratory Information Management System
(LIMS) designed to handle lab-wide scientific data acquisition, management,
processing, and mining needs for physics and materials science data. and with a
specific focus on future scalability for new equipment or research focuses. We
will present the decisions, process, and problems we went through while
building our LIMS for materials research, its current operational state, and
our steps for future development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2660</identifier>
 <datestamp>2014-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2660</id><created>2014-03-11</created><updated>2014-05-20</updated><authors><author><keyname>Minsker</keyname><forenames>Stanislav</forenames></author><author><keyname>Srivastava</keyname><forenames>Sanvesh</forenames></author><author><keyname>Lin</keyname><forenames>Lizhen</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Robust and Scalable Bayes via a Median of Subset Posterior Measures</title><categories>math.ST cs.DC cs.LG stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach to Bayesian analysis that is provably robust to
outliers in the data and often has computational advantages over standard
methods. Our technique is based on splitting the data into non-overlapping
subgroups, evaluating the posterior distribution given each independent
subgroup, and then combining the resulting measures. The main novelty of our
approach is the proposed aggregation step, which is based on the evaluation of
a median in the space of probability measures equipped with a suitable
collection of distances that can be quickly and efficiently evaluated in
practice. We present both theoretical and numerical evidence illustrating the
improvements achieved by our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2668</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2668</id><created>2014-03-11</created><updated>2014-11-26</updated><authors><author><keyname>Gallos</keyname><forenames>Lazaros K.</forenames></author><author><keyname>Fefferman</keyname><forenames>Nina H.</forenames></author></authors><title>Revealing effective classifiers through network comparison</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>11 pages, 4 figures, includes Supplementary Information. For the
  source code, see
  http://www-levich.engr.ccny.cuny.edu/~lazaros/research_comparison.html</comments><journal-ref>EPL 108, 38001 (2014)</journal-ref><doi>10.1209/0295-5075/108/38001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to compare complex systems can provide new insight into the
fundamental nature of the processes captured in ways that are otherwise
inaccessible to observation. Here, we introduce the $n$-tangle method to
directly compare two networks for structural similarity, based on the
distribution of edge density in network subgraphs. We demonstrate that this
method can efficiently introduce comparative analysis into network science and
opens the road for many new applications. For example, we show how the
construction of a phylogenetic tree across animal taxa according to their
social structure can reveal commonalities in the behavioral ecology of the
populations, or how students create similar networks according to the
University size. Our method can be expanded to study a multitude of additional
properties, such as network classification, changes during time evolution,
convergence of growth models, and detection of structural changes during
damage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2686</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2686</id><created>2014-03-11</created><authors><author><keyname>Maheshwari</keyname><forenames>Vandana</forenames></author></authors><title>Development of SyReC based expandable reversible logic circuits</title><categories>cs.AR</categories><comments>arXiv admin note: text overlap with arXiv:1110.2574 by other authors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Reversible computing is gaining high interest from researchers due to its
various promises. One of the prominent advantages perceived from reversible
logic is that of reduced power dissipation with many reversible gates at hand,
designing a reversible circuit (combinational) has received due attention and
achievement. A proposed language for description of reversible circuit, namely
SyReC, is also in place. What remain are the software tools which would help in
reversible circuit synthesis through simulation. Beginning with the smallest
reversible circuit realizations the SyReC statements and expressions, we employ
a hierarchal approach to develop a complete reversible circuit, entirely from
its SyReC code. We implement this as a software tool. The tool allows a user to
expand a reversible circuit of choice in terms of bit width of its inputs. The
background approach of expansion of a reversible circuit has also been proposed
as a part of this dissertation. Also, a user can use the tool to observe the
effect of expansion on incurred costs, in terms of increase in number of lines,
number of gates and quantum cost. The importance of observing the change in
costs with respect to scale of expansion is important not only from analysis
point of view, but also because the cost depends on the approach used for
expansion. This dissertation also proposes a reversible circuit design for
elevator controller (combinational) and the related costs. The aim is to
emphasize use of the proposed approach is designing customized circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2702</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2702</id><created>2014-03-11</created><updated>2014-03-21</updated><authors><author><keyname>Mheich</keyname><forenames>Zeina</forenames></author><author><keyname>Alberge</keyname><forenames>Florence</forenames></author><author><keyname>Duhamel</keyname><forenames>Pierre</forenames></author></authors><title>On the efficiency of transmission strategies for broadcast channels
  using finite size constellations</title><categories>cs.IT math.IT</categories><comments>EUSIPCO 2013. Proceedings of the 21st European Signal Processing
  Conference 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, achievable rates regions are derived for power constrained
Gaussian broadcast channel of two users using finite dimension constellations.
Various transmission strategies are studied, namely superposition coding (SC)
and superposition modulation (SM) and compared to standard schemes such as time
sharing (TS). The maximal achievable rates regions for SM and SC strategies are
obtained by optimizing over both the joint probability distribution and over
the positions of constellation symbols. The improvement in achievable rates for
each scheme of increasing complexity is evaluated in terms of SNR savings for a
given target achievable rate or/and percentage of gain in achievable rates for
one user with reference to a classical scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2708</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2708</id><created>2014-03-10</created><authors><author><keyname>Smilkov</keyname><forenames>Daniel</forenames></author><author><keyname>Hidalgo</keyname><forenames>Cesar A.</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Beyond network structure: How heterogenous susceptibility modulates the
  spread of epidemics</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compartmental models used to study epidemic spreading often assume the
same susceptibility for all individuals, and are therefore, agnostic about the
effects that differences in susceptibility can have on epidemic spreading. Here
we show that--for the SIS model--differential susceptibility can make networks
more vulnerable to the spread of diseases when the correlation between a node's
degree and susceptibility are positive, and less vulnerable when this
correlation is negative. Moreover, we show that networks become more likely to
contain a pocket of infection when individuals are more likely to connect with
others that have similar susceptibility (the network is segregated). These
results show that the failure to include differential susceptibility to
epidemic models can lead to a systematic over/under estimation of fundamental
epidemic parameters when the structure of the networks is not independent from
the susceptibility of the nodes or when there are correlations between the
susceptibility of connected individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2716</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2716</id><created>2014-03-10</created><authors><author><keyname>Aziz</keyname><forenames>Amira Sayed A.</forenames></author><author><keyname>Azar</keyname><forenames>Ahmad Taher</forenames></author><author><keyname>Hassanien</keyname><forenames>Aboul Ella</forenames></author><author><keyname>Hanafy</keyname><forenames>Sanaa El-Ola</forenames></author></authors><title>Negative Selection Approach Application in Network Intrusion Detection
  Systems</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nature has always been an inspiration to researchers with its diversity and
robustness of its systems, and Artificial Immune Systems are one of them. Many
algorithms were inspired by ongoing discoveries of biological immune systems
techniques and approaches. One of the basic and most common approach is the
Negative Selection Approach, which is simple and easy to implement. It was
applied in many fields, but mostly in anomaly detection for the similarity of
its basic idea. In this paper, a review is given on the application of negative
selection approach in network security, specifically the intrusion detection
system. As the work in this field is limited, we need to understand what the
challenges of this approach are. Recommendations are given by the end of the
paper for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2718</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2718</id><created>2014-03-11</created><authors><author><keyname>Bella</keyname><forenames>Fabio</forenames></author><author><keyname>M&#xfc;nch</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Ocampo</keyname><forenames>Alexis</forenames></author></authors><title>Observation-based Development of Software Process Baselines: An
  Experience Report</title><categories>cs.SE</categories><comments>7 pages. arXiv admin note: substantial text overlap with
  arXiv:1403.2519</comments><journal-ref>Proceedings of the 8th Conference on Quality Engineering in
  Software Technology (CONQUEST), pages 31-43, Nuremberg, Germany, September
  22-24 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The creation and deployment of software development processes for new domains
(such as wireless Internet services) is a challenging task due to the lack of
knowledge about adequate engineering techniques and their effects. In addition,
time-to-market pressure prevents applying long-lasting maturation of processes.
Nevertheless, developing software of a predetermined quality in a predictable
fashion can only be achieved with systematic development processes and the use
of engineering principles. A descriptive approach promises to quickly create
initial valuable process models and quantitative baselines that can be seen as
starting points for continuous improvement activities.
  This paper describes the creation of software development processes for the
development of wireless Internet services based on the observation of pilot
projects that were performed at distributed development sites. Different
techniques and tools such as descriptive process modeling, process
documentation generation, goal-oriented measurement, and capturing qualitative
experience were combined to gain process baselines. Results show that the
observation-based approach helped to quickly come up with stable
context-oriented development processes and get a better understanding of their
effects with respect to quantitative and qualitative means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2732</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2732</id><created>2014-03-11</created><authors><author><keyname>Myers</keyname><forenames>Seth A.</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>The Bursty Dynamics of the Twitter Information Network</title><categories>cs.SI physics.soc-ph stat.ML</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In online social media systems users are not only posting, consuming, and
resharing content, but also creating new and destroying existing connections in
the underlying social network. While each of these two types of dynamics has
individually been studied in the past, much less is known about the connection
between the two. How does user information posting and seeking behavior
interact with the evolution of the underlying social network structure?
  Here, we study ways in which network structure reacts to users posting and
sharing content. We examine the complete dynamics of the Twitter information
network, where users post and reshare information while they also create and
destroy connections. We find that the dynamics of network structure can be
characterized by steady rates of change, interrupted by sudden bursts.
Information diffusion in the form of cascades of post re-sharing often creates
such sudden bursts of new connections, which significantly change users' local
network structure. These bursts transform users' networks of followers to
become structurally more cohesive as well as more homogenous in terms of
follower interests. We also explore the effect of the information content on
the dynamics of the network and find evidence that the appearance of new topics
and real-world events can lead to significant changes in edge creations and
deletions. Lastly, we develop a model that quantifies the dynamics of the
network and the occurrence of these bursts as a function of the information
spreading through the network. The model can successfully predict which
information diffusion events will lead to bursts in network dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2739</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2739</id><created>2014-03-11</created><authors><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author></authors><title>Sufficient statistics for linear control strategies in decentralized
  systems with partial history sharing</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In decentralized control systems with linear dynamics, quadratic cost, and
Gaussian disturbance (also called decentralized LQG systems) linear control
strategies are not always optimal. Nonetheless, linear control strategies are
appealing due to analytic and implementation simplicity. In this paper, we
investigate decentralized LQG systems with partial history sharing information
structure and identify finite dimensional sufficient statistics for such
systems. Unlike prior work on decentralized LQG systems, we do not assume
partially nestedness or quadratic invariance. Our approach is based on the
common information approach of Nayyar \emph{et al}, 2013 and exploits the
linearity of the system dynamics and control strategies. To illustrate our
methodology, we identify sufficient statistics for linear strategies in
decentralized systems where controllers communicate over a strongly connected
graph with finite delays, and for decentralized systems consisting of coupled
subsystems with control sharing or one-sided one step delay sharing information
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2740</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2740</id><created>2014-02-22</created><updated>2014-12-06</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Moghaddam</keyname><forenames>Hamid Abrishami</forenames></author></authors><title>A consistent model for cardiac deformation estimation under abnormal
  ventricular muscle conditions</title><categories>cs.CE cs.NA</categories><comments>Published in the IFMBE Proceedings of World Congress on Medical
  Physics and Biomedical Engineering, September 7 - 12, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformation modeling of cardiac muscle is an important issue in the field of
cardiac analysis. For this reason, many approaches have been developed to best
estimate the cardiac muscle deformation, and to obtain a practical model to use
in diagnostic procedures. But there are some conditions, like in case of
myocardial infarction, in which the regular modeling approaches are not useful.
In this section, using a point-wise approach in deformation estimation, we try
to estimate the deformation under some abnormal conditions of cardiac muscle.
First, the endocardial and epicardial contour points are ordered with respect
to the center of gravity of endocardial contour and boundary point displacement
vectors are extracted. Then to solve the governing equation of deformation,
which is an elliptic equation, we apply boundary conditions in accordance with
the computed displacement vectors and then the Finite Element method (FEM) will
be used to solve the governing equation. Using obtained displacement field
through the cardiac muscle, strain map is extracted to show the mechanical
behavior of cardiac muscle. To validate the proposed algorithm in case of
infracted muscle, a non-homogeneous ring is modeled using ANSYS under a uniform
time varying internal pressure, which is the case in real cardiac muscle
deformation and then the proposed algorithm implemented in MATLAB and the
results for such problem are extracted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2745</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2745</id><created>2014-03-11</created><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Greenwood</keyname><forenames>Dazza</forenames></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Privacy for Personal Neuroinformatics</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human brain activity collected in the form of Electroencephalography (EEG),
even with low number of sensors, is an extremely rich signal. Traces collected
from multiple channels and with high sampling rates capture many important
aspects of participants' brain activity and can be used as a unique personal
identifier. The motivation for sharing EEG signals is significant, as a mean to
understand the relation between brain activity and well-being, or for
communication with medical services. As the equipment for such data collection
becomes more available and widely used, the opportunities for using the data
are growing; at the same time however inherent privacy risks are mounting. The
same raw EEG signal can be used for example to diagnose mental diseases, find
traces of epilepsy, and decode personality traits. The current practice of the
informed consent of the participants for the use of the data either prevents
reuse of the raw signal or does not truly respect participants' right to
privacy by reusing the same raw data for purposes much different than
originally consented to. Here we propose an integration of a personal
neuroinformatics system, Smartphone Brain Scanner, with a general privacy
framework openPDS. We show how raw high-dimensionality data can be collected on
a mobile device, uploaded to a server, and subsequently operated on and
accessed by applications or researchers, without disclosing the raw signal.
Those extracted features of the raw signal, called answers, are of
significantly lower-dimensionality, and provide the full utility of the data in
given context, without the risk of disclosing sensitive raw signal. Such
architecture significantly mitigates a very serious privacy risk related to raw
EEG recordings floating around and being used and reused for various purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2752</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2752</id><created>2014-03-11</created><authors><author><keyname>Basold</keyname><forenames>Henning</forenames></author></authors><title>Transformation von Scade-Modellen zur SMT-basierten Verifikation</title><categories>cs.LO</categories><comments>The implementation can be found at https://github.com/hbasold/LAMA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we develop a fully automatic verification procedure of safety
properties of Scade programs. We transform each such program into an SMT
instance (Satisfiability Modulo Theories) and feed this to a solver. The goal
is to have a publicly accessible experimentation platform for the verification
of Scade programs.
  The choice of SMT is determined by the fact that it offers more expressive
logics than propositional logic, yet their solvers have been shown to perform
very well. The expressiveness of SMT logics allows us to implement symbolic
model checking thus avoiding the expansion of the complete state space of the
models during the verification.
  In order to reduce the complexity we transform the Scade programs into SMT
instances in two steps. First they are reduced to programs of a synchronous
data flow language Lama. This language has simpler semantics than Scade while
still preserving some of the programmer's abstractions. Next we interpret such
a Lama program as a system of quantifier free first-order formulas.
  The remaining abstractions in Lama can be used to simplify these systems.
This in turn could lead to speeding up the verification process and allowing
more properties to be verifiable.
  We implemented these transformations successfully in a software using
Haskell. This work is concluded by a comparison of this software to the
existing verification software &quot;Scade Design Verifier&quot; which comes with the
Scade Suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2763</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2763</id><created>2014-03-11</created><updated>2014-05-01</updated><authors><author><keyname>Liu</keyname><forenames>Weimo</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Aggregate Estimation Over Dynamic Hidden Web Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many databases on the web are &quot;hidden&quot; behind (i.e., accessible only through)
their restrictive, form-like, search interfaces. Recent studies have shown that
it is possible to estimate aggregate query answers over such hidden web
databases by issuing a small number of carefully designed search queries
through the restrictive web interface. A problem with these existing work,
however, is that they all assume the underlying database to be static, while
most real-world web databases (e.g., Amazon, eBay) are frequently updated. In
this paper, we study the novel problem of estimating/tracking aggregates over
dynamic hidden web databases while adhering to the stringent query-cost
limitation they enforce (e.g., at most 1,000 search queries per day).
Theoretical analysis and extensive real-world experiments demonstrate the
effectiveness of our proposed algorithms and their superiority over baseline
solutions (e.g., the repeated execution of algorithms designed for static web
databases).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2765</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2765</id><created>2014-03-11</created><updated>2014-04-22</updated><authors><author><keyname>Rhodes</keyname><forenames>Christophe</forenames></author><author><keyname>Moringen</keyname><forenames>Jan</forenames></author><author><keyname>Lichteblau</keyname><forenames>David</forenames></author></authors><title>Generalizers: New Metaobjects for Generalized Dispatch</title><categories>cs.PL</categories><comments>8 pages; version accepted for presentation at 2014 European Lisp
  Symposium. http://eprints.gold.ac.uk/9924/</comments><acm-class>D.1; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new metaobject, the generalizer, which complements
the existing specializer metaobject. With the help of examples, we show that
this metaobject allows for the efficient implementation of complex
non-class-based dispatch within the framework of existing metaobject protocols.
We present our modifications to the generic function invocation protocol from
the Art of the Metaobject Protocol; in combination with previous work, this
produces a fully-functional extension of the existing mechanism for method
selection and combination, including support for method combination completely
independent from method selection. We discuss our implementation, within the
SBCL implementation of Common Lisp, and in that context compare the performance
of the new protocol with the standard one, demonstrating that the new protocol
can be tolerably efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2777</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2777</id><created>2014-03-11</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author></authors><title>Zig-zag Sort: A Simple Deterministic Data-Oblivious Sorting Algorithm
  Running in O(n log n) Time</title><categories>cs.DS</categories><comments>Appearing in ACM Symp. on Theory of Computing (STOC) 2014</comments><doi>10.1145/2591796.2591830</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze Zig-zag Sort--a deterministic data-oblivious sorting
algorithm running in O(n log n) time that is arguably simpler than previously
known algorithms with similar properties, which are based on the AKS sorting
network. Because it is data-oblivious and deterministic, Zig-zag Sort can be
implemented as a simple O(n log n)-size sorting network, thereby providing a
solution to an open problem posed by Incerpi and Sedgewick in 1985. In
addition, Zig-zag Sort is a variant of Shellsort, and is, in fact, the first
deterministic Shellsort variant running in O(n log n) time. The existence of
such an algorithm was posed as an open problem by Plaxton et al. in 1992 and
also by Sedgewick in 1996. More relevant for today, however, is the fact that
the existence of a simple data-oblivious deterministic sorting algorithm
running in O(n log n) time simplifies the inner-loop computation in several
proposed oblivious-RAM simulation methods (which utilize AKS sorting networks),
and this, in turn, implies simplified mechanisms for privacy-preserving data
outsourcing in several cloud computing applications. We provide both
constructive and non-constructive implementations of Zig-zag Sort, based on the
existence of a circuit known as an epsilon-halver, such that the constant
factors in our constructive implementations are orders of magnitude smaller
than those for constructive variants of the AKS sorting network, which are also
based on the use of epsilon-halvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2779</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2779</id><created>2014-03-11</created><authors><author><keyname>Kuijper</keyname><forenames>Margreta</forenames></author><author><keyname>Napp</keyname><forenames>Diego</forenames></author></authors><title>Erasure codes with simplex locality</title><categories>cs.IT math.IT</categories><comments>submitted in December 2013 to Mathematical Theory of Networks and
  Systems (MTNS) International Symposium 2014</comments><msc-class>11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on erasure codes for distributed storage. The distributed storage
setting imposes locality requirements because of easy repair demands on the
decoder. We first establish the characterization of various locality properties
in terms of the generator matrix of the code. These lead to bounds on locality
and notions of optimality. We then examine the locality properties of a family
of non-binary codes with simplex structure. We investigate their optimality and
design several easy repair decoding methods. In particular, we show that any
correctable erasure pattern can be solved by easy repair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2785</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2785</id><created>2014-03-11</created><authors><author><keyname>Pirbadian</keyname><forenames>Aras</forenames></author><author><keyname>Khairy</keyname><forenames>Muhammad S.</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Kurdahi</keyname><forenames>Fadi J.</forenames></author></authors><title>State Dependent Statistical Timing Model for Voltage Scaled Circuits</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel statistical state-dependent timing model for
voltage over scaled (VoS) logic circuits that accurately and rapidly finds the
timing distribution of output bits. Using this model erroneous VoS circuits can
be represented as error-free circuits combined with an error-injector. A case
study of a two point DFT unit employing the proposed model is presented and
compared to HSPICE circuit simulation. Results show an accurate match, with
significant speedup gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2787</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2787</id><created>2014-03-11</created><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Principles of scientific research team formation and evolution</title><categories>physics.soc-ph astro-ph.IM cs.DL cs.SI</categories><comments>Published in PNAS. Model tested on astronomy. Published version and
  Supplemental Information available at
  http://www.pnas.org/cgi/doi/10.1073/pnas.1309723111</comments><doi>10.1073/pnas.1309723111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research teams are the fundamental social unit of science, and yet there is
currently no model that describes their basic property: size. In most fields
teams have grown significantly in recent decades. We show that this is partly
due to the change in the character of team-size distribution. We explain these
changes with a comprehensive yet straightforward model of how teams of
different sizes emerge and grow. This model accurately reproduces the evolution
of empirical team-size distribution over the period of 50 years. The modeling
reveals that there are two modes of knowledge production. The first and more
fundamental mode employs relatively small, core teams. Core teams form by a
Poisson process and produce a Poisson distribution of team sizes in which
larger teams are exceedingly rare. The second mode employs extended teams,
which started as core teams, but subsequently accumulated new members
proportional to the past productivity of their members. Given time, this mode
gives rise to a power-law tail of large teams (10-1000 members), which features
in many fields today. Based on this model we construct an analytical functional
form that allows the contribution of different modes of authorship to be
determined directly from the data and is applicable to any field. The model
also offers a solid foundation for studying other social aspects of science,
such as productivity and collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2798</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2798</id><created>2014-03-11</created><updated>2014-05-30</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Dandres</keyname><forenames>Thomas</forenames></author><author><keyname>Lemieux</keyname><forenames>Yves</forenames></author><author><keyname>Samson</keyname><forenames>R&#xe9;jean</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>Challenges and complexities in application of LCA approaches in the case
  of ICT for a sustainable future</title><categories>cs.CY cs.HC</categories><comments>10 pages. Preprint/Accepted of a paper submitted to the ICT4S
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, three of many ICT-specific challenges of LCA are discussed.
First, the inconsistency versus uncertainty is reviewed with regard to the
meta-technological nature of ICT. As an example, the semiconductor technologies
are used to highlight the complexities especially with respect to energy and
water consumption. The need for specific representations and metric to
separately assess products and technologies is discussed. It is highlighted
that applying product-oriented approaches would result in abandoning or
disfavoring of new technologies that could otherwise help toward a better
world. Second, several believed-untouchable hot spots are highlighted to
emphasize on their importance and footprint. The list includes, but not limited
to, i) User Computer-Interfaces (UCIs), especially screens and displays, ii)
Network-Computer Interlaces (NCIs), such as electronic and optical ports, and
iii) electricity power interfaces. In addition, considering cross-regional
social and economic impacts, and also taking into account the marketing nature
of the need for many ICT's product and services in both forms of hardware and
software, the complexity of End of Life (EoL) stage of ICT products,
technologies, and services is explored. Finally, the impact of smart management
and intelligence, and in general software, in ICT solutions and products is
highlighted. In particular, it is observed that, even using the same
technology, the significance of software could be highly variable depending on
the level of intelligence and awareness deployed. With examples from an
interconnected network of data centers managed using Dynamic Voltage and
Frequency Scaling (DVFS) technology and smart cooling systems, it is shown that
the unadjusted assessments could be highly uncertain, and even inconsistent, in
calculating the management component's significance on the ICT impacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2800</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2800</id><created>2014-03-11</created><authors><author><keyname>Qin</keyname><forenames>Peng</forenames></author><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Huang</keyname><forenames>Benxiong</forenames></author><author><keyname>Xu</keyname><forenames>Guan</forenames></author></authors><title>Bandwidth-Aware Scheduling with SDN in Hadoop: A New Trend for Big Data</title><categories>cs.DC cs.NI cs.PF</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Software Defined Networking (SDN) is a revolutionary network architecture
that separates out network control functions from the underlying equipment and
is an increasingly trend to help enterprises build more manageable data centers
where big data processing emerges as an important part of applications. To
concurrently process large-scale data, MapReduce with an open source
implementation named Hadoop is proposed. In practical Hadoop systems one kind
of issue that vitally impacts the overall performance is know as the
NP-complete minimum make span problem. One main solution is to assign tasks on
data local nodes to avoid link occupation since network bandwidth is a scarce
resource. Many methodologies for enhancing data locality are proposed such as
the HDS and state-of-the-art scheduler BAR. However, all of them either ignore
allocating tasks in a global view or disregard available bandwidth as the basis
for scheduling. In this paper we propose a heuristic bandwidth-aware task
scheduler BASS to combine Hadoop with SDN. It is not only able to guarantee
data locality in a global view but also can efficiently assign tasks in an
optimized way. Both examples and experiments demonstrate that BASS has the best
performance in terms of job completion time. To our knowledge, BASS is the
first to exploit talent of SDN for big data processing and we believe it points
out a new trend for large-scale data processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2802</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2802</id><created>2014-03-11</created><authors><author><keyname>Fan</keyname><forenames>Haoqiang</forenames></author><author><keyname>Cao</keyname><forenames>Zhimin</forenames></author><author><keyname>Jiang</keyname><forenames>Yuning</forenames></author><author><keyname>Yin</keyname><forenames>Qi</forenames></author><author><keyname>Doudou</keyname><forenames>Chinchilla</forenames></author></authors><title>Learning Deep Face Representation</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face representation is a crucial step of face recognition systems. An optimal
face representation should be discriminative, robust, compact, and very
easy-to-implement. While numerous hand-crafted and learning-based
representations have been proposed, considerable room for improvement is still
present. In this paper, we present a very easy-to-implement deep learning
framework for face representation. Our method bases on a new structure of deep
network (called Pyramid CNN). The proposed Pyramid CNN adopts a
greedy-filter-and-down-sample operation, which enables the training procedure
to be very fast and computation-efficient. In addition, the structure of
Pyramid CNN can naturally incorporate feature sharing across multi-scale face
representations, increasing the discriminative ability of resulting
representation. Our basic network is capable of achieving high recognition
accuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. When
extended to feature-sharing Pyramid CNN, our system achieves the
state-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce a
new benchmark of realistic face images on social network and validate our
proposed representation has a good ability of generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2805</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2805</id><created>2014-03-12</created><authors><author><keyname>Ooms</keyname><forenames>Jeroen</forenames></author></authors><title>The jsonlite Package: A Practical and Consistent Mapping Between JSON
  Data and R Objects</title><categories>stat.CO cs.MS cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A naive realization of JSON data in R maps JSON arrays to an unnamed list,
and JSON objects to a named list. However, in practice a list is an awkward,
inefficient type to store and manipulate data. Most statistical applications
work with (homogeneous) vectors, matrices or data frames. Therefore JSON
packages in R typically define certain special cases of JSON structures which
map to simpler R types. Currently there exist no formal guidelines, or even
consensus between implementations on how R data should be represented in JSON.
Furthermore, upon closer inspection, even the most basic data structures in R
actually do not perfectly map to their JSON counterparts and leave some
ambiguity for edge cases. These problems have resulted in different behavior
between implementations and can lead to unexpected output. This paper
explicitly describes a mapping between R classes and JSON data, highlights
potential problems, and proposes conventions that generalize the mapping to
cover all common structures. We emphasize the importance of type consistency
when using JSON to exchange dynamic data, and illustrate using examples and
anecdotes. The jsonlite R package is used throughout the paper as a reference
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2808</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2808</id><created>2014-03-12</created><authors><author><keyname>Fouad</keyname><forenames>Hafez</forenames></author></authors><title>Web-based Database Management to support Telemedicine System</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transfer of the medical care services to the patient, rather than the
transport of the patient to the medical services providers is aim of the
project. This is achieved by using web-based applications including Modern
Medical Informatics Services which is easier, faster and less expensive. The
required system implements the suitable informatics and electronics solutions
efficiently for the Tele-medicine care. We proposed an approach to manage
different multimedia medical databases in the telemedicine system. In order to
be efficiently and effectively manage, search, and display database
information, we define an information package for both of doctor and patient as
a concise data set of their medical information from each visit. The
methodology for accessing various types of medical records will be provided,
also we will design two web-based interfaces, high-quality data and display for
many medical service purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2809</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2809</id><created>2014-03-12</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Equitable list point arboricity of graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is list point $k$-arborable if, whenever we are given a $k$-list
assignment $L(v)$ of colors for each vertex $v\in V(G)$, we can choose a color
$c(v)\in L(v)$ for each vertex $v$ so that each color class induces an acyclic
subgraph of $G$, and is equitable list point $k$-arborable if $G$ is list point
$k$-arborable and each color appears on at most $\lceil |V(G)|/k\rceil$
vertices of $G$. In this paper, we conjecture that every graph $G$ is equitable
list point $k$-arborable for every $k\geq \lceil(\Delta(G)+1)/2\rceil$ and
settle this for complete graphs, 2-degenerate graphs, 3-degenerate claw-free
graphs with maximum degree at least 4, and planar graphs with maximum degree at
least 8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2810</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2810</id><created>2014-03-12</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Equitable vertex arboricity of planar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G_1$ be a planar graph such that all cycles of length at most 4 are
independent and let $G_2$ be a planar graph without 3-cycles and adjacent
4-cycles. It is proved that the set of vertices of $G_1$ and $G_2$ can be
equitably partitioned into $t$ subsets for every $t\geq 3$ so that each subset
induces a forest. These results partially confirm a conjecture of Wu, Zhang and
Li.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2814</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2814</id><created>2014-03-12</created><authors><author><keyname>Naah</keyname><forenames>Gideon</forenames></author><author><keyname>Okoampa</keyname><forenames>Edwin Boadu</forenames></author></authors><title>Clustering Effects on Wireless Mobile Ad-Hoc Networks Performances</title><categories>cs.NI</categories><comments>journal, 19 pages, 14 figures and 1 table,
  http://airccse.org/journal/ijcsit2014_curr.html</comments><doi>10.5121/ijcsit.2014.6105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new era is dawning for wireless mobile ad hoc networks where communication
will be done using a group of mobile devices called cluster, hence clustered
network. In a clustered network, protocols used by these mobile devices are
different from those used in a wired network; which helps to save computation
time and resources efficiently. This paper focuses on Cluster-Based Routing
Protocol and Dynamic Source Routing. The results presented in this paper
illustrates the implementation of Ad-hoc On-Demand Distance Vector routing
protocol for enhancing mobile nodes performance and lifetime in a clustered
network and to demonstrate how this routing protocol results in time efficient
and resource saving in wireless mobile ad hoc networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2819</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2819</id><created>2014-03-12</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Campetelli</keyname><forenames>Alarico</forenames></author></authors><title>Towards system development methodologies: From software to
  cyber-physical domain</title><categories>cs.SE</categories><comments>First International Workshop on Formal Techniques for Safety-Critical
  Systems 2012 (FTSCS 2012), ICFEM 2012 Satellite Event</comments><msc-class>68M15</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In many cases, it is more profitable to apply existing methodologies than to
develop new ones. This holds, especially, for system development within the
cyber-physical domain: until a certain abstraction level we can (re)use the
methodologies for the software system development to benefit from the
advantages of these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2821</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2821</id><created>2014-03-12</created><authors><author><keyname>Garoui</keyname><forenames>Mohamed</forenames></author><author><keyname>Mazigh</keyname><forenames>Belhassen</forenames></author><author><keyname>Ayeb</keyname><forenames>B&#xe9;chir El</forenames></author><author><keyname>Koukam</keyname><forenames>Abderrafiaa</forenames></author></authors><title>Towards an Agent-Oriented Modeling and Evaluation Approach For Vehicular
  Systems Security</title><categories>cs.SE cs.MA</categories><comments>International Journal of Information Technology, Modeling and
  Computing (IJITMC) Vol. 2, No. 1, 2014. arXiv admin note: text overlap with
  arXiv:1204.1581 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent technology is a software paradigm that permits to implement large and
complex distributed applications. In order to assist the development of
multi-agent systems, agent-oriented methodologies (AOM) have been created in
the last years to support modeling more and more complex applications in many
different domains. By defining in a non-ambiguous way concepts used in a
specific domain, Meta modeling may represent a step towards such
interoperability. In the Transport domain, this paper propose an agent-oriented
meta-model that provides rigorous concepts for conducting transportation system
problem modeling. The aim is to allow analysts to produce a transportation
system model that precisely captures the knowledge of an organization so that
an agent-oriented requirements specification of the system-to-be and its
operational corporate environment can be derived from it. To this end, we
extend and adapt an existing meta-model, Extended Gaia, to build a meta-model
and an adequate model for transportation problems. Our new agent-oriented
meta-model aims to allow the analyst to model and specify any transportation
system as a multi-agent system. Based on the proposed meta-model, we proposes
an approach for modeling and evaluating the Transportation System based on
Stochastic Activity Network (SAN) components. The proposed process is based on
seven steps from Recognition phase to Quantitative Analysis phase. These
analyzes are based on the Dependability models which are built using the
formalism Stochastic Activity Network. A real case study of Urban Public
Transportation System has been conducted to show the benefits of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2828</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2828</id><created>2014-03-12</created><updated>2014-03-25</updated><authors><author><keyname>AlSoufi</keyname><forenames>Ali</forenames></author><author><keyname>Ali</keyname><forenames>Hayat</forenames></author></authors><title>Customers perception of mbanking adoption in Kingdom of Bahrain: an
  empirical assessment of an extended tam model</title><categories>cs.CY</categories><comments>13 pages, 2 figures, 2 tables</comments><journal-ref>International Journal of Managing Information Technology (IJMIT)
  Vol.6, No.1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile applications have been rapidly changing the way business organizations
deliver their services to their customers and how customers can interact with
their service providers in order to satisfy their needs. The use of mobile
applications increases rapidly, and has been used in many segments including
banking segment. This research aims at extending the Technology Adoption Model
(TAM) to incorporate the role of factors in influencing customers perception
towards mobile banking adoption. Furthermore, the extended TAM model was
evaluated empirically to measure its impact on m-banking adoption in of
Bahrain. The model was evaluated using a sample survey of 372 customers. The
results reveal that the intention to adopt mobile banking is mainly affected by
specific factors which are: Perceived Usefulness and Ease of Use. On the other
hand, some factors such as perceived cost and perceived risk did not show any
affect on the users' intention to use mobile banking. The result of this
research is beneficial for banking service managers to consider the factors
that can enforce the Mobile Banking services adoption and increase the take-up
of their mobile services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2835</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2835</id><created>2014-03-12</created><authors><author><keyname>Valsesia</keyname><forenames>Diego</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Compressive Signal Processing with Circulant Sensing Matrices</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing achieves effective dimensionality reduction of signals,
under a sparsity constraint, by means of a small number of random measurements
acquired through a sensing matrix. In a signal processing system, the problem
arises of processing the random projections directly, without first
reconstructing the signal. In this paper, we show that circulant sensing
matrices allow to perform a variety of classical signal processing tasks such
as filtering, interpolation, registration, transforms, and so forth, directly
in the compressed domain and in an exact fashion, \emph{i.e.}, without relying
on estimators as proposed in the existing literature. The advantage of the
techniques presented in this paper is to enable direct
measurement-to-measurement transformations, without the need of costly recovery
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2837</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2837</id><created>2014-03-12</created><authors><author><keyname>Rashidi</keyname><forenames>Ayshe</forenames></author><author><keyname>Lighvan</keyname><forenames>Mina Zolfy</forenames></author></authors><title>HPS: a hierarchical Persian stemming method</title><categories>cs.CL</categories><comments>10 pages, 6 tables, 2 figures, International Journal on Natural
  Language Computing (IJNLC), International Journal on Natural Language
  Computing (IJNLC) Vol. 3, No.1, February 2014</comments><doi>10.5121/ijnlc.2014.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel hierarchical Persian stemming approach based on the
Part-Of-Speech of the word in a sentence is presented. The implemented stemmer
includes hash tables and several deterministic finite automata in its different
levels of hierarchy for removing the prefixes and suffixes of the words. We had
two intentions in using hash tables in our method. The first one is that the
DFA don't support some special words, so hash table can partly solve the
addressed problem. the second goal is to speed up the implemented stemmer with
omitting the time that deterministic finite automata need. Because of the
hierarchical organization, this method is fast and flexible enough. Our
experiments on test sets from Hamshahri collection and security news (istna.ir)
show that our method has the average accuracy of 95.37% which is even improved
in using the method on a test set with common topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2842</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2842</id><created>2014-03-12</created><authors><author><keyname>Ulker</keyname><forenames>Ezgi Deniz</forenames></author><author><keyname>Ulker</keyname><forenames>Sadik</forenames></author></authors><title>Application of Particle Swarm Optimization to Microwave Tapered
  Microstrip Lines</title><categories>cs.NE</categories><comments>6 pages, 5 figures, 1 table</comments><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 4, No. 1, February 2014</journal-ref><doi>10.5121/cseij.2014.4106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Application of metaheuristic algorithms has been of continued interest in the
field of electrical engineering because of their powerful features. In this
work special design is done for a tapered transmission line used for matching
an arbitrary real load to a 50{\Omega} line. The problem at hand is to match
this arbitrary load to 50 {\Omega} line using three section tapered
transmission line with impedances in decreasing order from the load. So the
problem becomes optimizing an equation with three unknowns with various
conditions. The optimized values are obtained using Particle Swarm
Optimization. It can easily be shown that PSO is very strong in solving this
kind of multiobjective optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2844</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2844</id><created>2014-03-12</created><authors><author><keyname>Farouk</keyname><forenames>Amr</forenames></author><author><keyname>Fouad</keyname><forenames>Mohamed M.</forenames></author><author><keyname>Abdelhafez</keyname><forenames>Ahmed A.</forenames></author></authors><title>Analysis And Improvement of Pairing-Free Certificate-Less Two-Party
  Authenticated Key Agreement Protocol For Grid Computing</title><categories>cs.CR</categories><comments>14 pages</comments><journal-ref>International Journal of Security, Privacy and Trust Management
  (IJSPTM) Vol 3, No 1, February 2014</journal-ref><doi>10.5121/ijsptm.2014.3103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The predominant grid authentication mechanisms use public key infrastructure
(PKI). Nonetheless, certificate-less public key cryptography (CL-PKC) has
several advantages that seem to well align with the demands of grid computing.
Security and efficiency are the main objectives of grid authentication
protocols. Unfortunately, certificate-less authenticated key agreement
protocols rely on the bilinear pairing, that is extremely computational
expensive. In this paper, we analyze the recently secure certificateless key
agreement protocols without pairing. We then propose a novel grid pairing-free
certificate-less two-party authenticated key agreement (GPC-AKA) protocol,
providing a more lightweight key management approach for grid users. We also
show, a GPC-AKA security protocol proof using formal automated security
analysis Sycther tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2848</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2848</id><created>2014-03-12</created><authors><author><keyname>Bose</keyname><forenames>Ananya</forenames></author><author><keyname>Saha</keyname><forenames>Suprativ</forenames></author></authors><title>Delineation of Techniques to implement on the enhanced proposed model
  using data mining for protein sequence classification</title><categories>cs.DB cs.CE</categories><comments>8 pages, 1 figures</comments><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.6, No.1, February 2014</journal-ref><doi>10.5121/ijdms.2014.6105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In post genomic era with the advent of new technologies a huge amount of
complex molecular data are generated with high throughput. The management of
this biological data is definitely a challenging task due to complexity and
heterogeneity of data for discovering new knowledge. Issues like managing noisy
and incomplete data are needed to be dealt with. Use of data mining in
biological domain has made its inventory success. Discovering new knowledge
from the biological data is a major challenge in data mining technique. The
novelty of the proposed model is its combined use of intelligent techniques to
classify the protein sequence faster and efficiently. Use of FFT, fuzzy
classifier, String weighted algorithm, gram encoding method, neural network
model and rough set classifier in a single model and in an appropriate place
can enhance the quality of the classification system.Thus the primary challenge
is to identify and classify the large protein sequences in a very fast and easy
but intellectual way to decrease the time complexity and space complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2850</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2850</id><created>2014-03-12</created><authors><author><keyname>Mondani</keyname><forenames>Hernan</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author></authors><title>Fat-tailed fluctuations in the size of organizations: the role of social
  influence</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 4 figures</comments><journal-ref>PLoS ONE 9(7): e100527, 2014</journal-ref><doi>10.1371/journal.pone.0100527</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizational growth processes have consistently been shown to exhibit a
fatter-than-Gaussian growth-rate distribution in a variety of settings. Long
periods of relatively small changes are interrupted by sudden changes in all
size scales. This kind of extreme events can have important consequences for
the development of biological and socio-economic systems. Existing models do
not derive this aggregated pattern from agent actions at the micro level. We
develop an agent-based simulation model on a social network. We take our
departure in a model by a Schwarzkopf et al. on a scale-free network. We
reproduce the fat-tailed pattern out of internal dynamics alone, and also find
that it is robust with respect to network topology. Thus, the social network
and the local interactions are a prerequisite for generating the pattern, but
not the network topology itself. We further extend the model with a parameter
$\delta$ that weights the relative fraction of an individual's neighbours
belonging to a given organization, representing a contextual aspect of social
influence. In the lower limit of this parameter, the fraction is irrelevant and
choice of organization is random. In the upper limit of the parameter, the
largest fraction quickly dominates, leading to a winner-takes-all situation. We
recover the real pattern as an intermediate case between these two extremes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2863</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2863</id><created>2014-03-12</created><authors><author><keyname>Hadzhikoleva</keyname><forenames>Stanka</forenames></author><author><keyname>Hadzhikolev</keyname><forenames>Emil</forenames></author></authors><title>Consolidated Model of Procedures for Workflow Management</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents an approach to automation of business processes by means
of a consolidated model describing a class of processes. Rules and examples for
building a consolidated model are given. The model is validated through
development of a software application called COMPASS-P for monitoring of
procedures for evaluation and accreditation of education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2864</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2864</id><created>2014-03-12</created><updated>2014-04-10</updated><authors><author><keyname>Hashemi</keyname><forenames>Vahid</forenames></author><author><keyname>Hatefi</keyname><forenames>Hassan</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author></authors><title>Probabilistic Bisimulations for PCTL Model Checking of Interval MDPs</title><categories>cs.FL cs.LO</categories><comments>In Proceedings SynCoP 2014, arXiv:1403.7841</comments><proxy>Selena Clancy</proxy><msc-class>68Q10, 68Q60, 68Q85, 68Q25</msc-class><journal-ref>EPTCS 145, 2014, pp. 19-33</journal-ref><doi>10.4204/EPTCS.145.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification of PCTL properties of MDPs with convex uncertainties has been
investigated recently by Puggelli et al. However, model checking algorithms
typically suffer from state space explosion. In this paper, we address
probabilistic bisimulation to reduce the size of such an MDPs while preserving
PCTL properties it satisfies. We discuss different interpretations of
uncertainty in the models which are studied in the literature and that result
in two different definitions of bisimulations. We give algorithms to compute
the quotients of these bisimulations in time polynomial in the size of the
model and exponential in the uncertain branching. Finally, we show by a case
study that large models in practice can have small branching and that a
substantial state space reduction can be achieved by our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2871</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2871</id><created>2014-03-12</created><authors><author><keyname>Arrish</keyname><forenames>Senosy</forenames></author><author><keyname>Afif</keyname><forenames>Fadhil Noer</forenames></author><author><keyname>Maidorawa</keyname><forenames>Ahmadu</forenames></author><author><keyname>Salim</keyname><forenames>Naomie</forenames></author></authors><title>Shape-Based Plagiarism Detection for Flowchart Figures in Texts</title><categories>cs.CV cs.IR</categories><comments>12 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 1, February 2014</journal-ref><doi>10.5121/ijcsit.2014.6108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plagiarism detection is well known phenomenon in the academic arena. Copying
other people is considered as serious offence that needs to be checked. There
are many plagiarism detection systems such as turn-it-in that has been
developed to provide this checks. Most, if not all, discard the figures and
charts before checking for plagiarism. Discarding the figures and charts
results in look holes that people can take advantage. That means people can
plagiarized figures and charts easily without the current plagiarism systems
detecting it. There are very few papers which talks about flowcharts plagiarism
detection. Therefore, there is a need to develop a system that will detect
plagiarism in figures and charts. This paper presents a method for detecting
flow chart figure plagiarism based on shape-based image processing and
multimedia retrieval. The method managed to retrieve flowcharts with ranked
similarity according to different matching sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2877</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2877</id><created>2014-03-12</created><authors><author><keyname>Sorzano</keyname><forenames>C. O. S.</forenames></author><author><keyname>Vargas</keyname><forenames>J.</forenames></author><author><keyname>Montano</keyname><forenames>A. Pascual</forenames></author></authors><title>A survey of dimensionality reduction techniques</title><categories>stat.ML cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experimental life sciences like biology or chemistry have seen in the recent
decades an explosion of the data available from experiments. Laboratory
instruments become more and more complex and report hundreds or thousands
measurements for a single experiment and therefore the statistical methods face
challenging tasks when dealing with such high dimensional data. However, much
of the data is highly redundant and can be efficiently brought down to a much
smaller number of variables without a significant loss of information. The
mathematical procedures making possible this reduction are called
dimensionality reduction techniques; they have widely been developed by fields
like Statistics or Machine Learning, and are currently a hot research topic. In
this review we categorize the plethora of dimension reduction techniques
available and give the mathematical insight behind them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2895</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2895</id><created>2014-03-12</created><authors><author><keyname>Mart&#xed;nez-Zarzuela</keyname><forenames>M.</forenames></author><author><keyname>Pedraza-Hueso</keyname><forenames>M.</forenames></author><author><keyname>D&#xed;az-Pernas</keyname><forenames>F. J.</forenames></author><author><keyname>Gonz&#xe1;lez-Ortega</keyname><forenames>D.</forenames></author><author><keyname>Ant&#xf3;n-Rodr&#xed;guez</keyname><forenames>M.</forenames></author></authors><title>Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras</title><categories>cs.CV</categories><journal-ref>International Journal of Multimedia &amp; Its Applications 6(2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes the design and development of a system for remote
indoor 3D monitoring using an undetermined number of Microsoft(R) Kinect
sensors. In the proposed client-server system, the Kinect cameras can be
connected to different computers, addressing this way the hardware limitation
of one sensor per USB controller. The reason behind this limitation is the high
bandwidth needed by the sensor, which becomes also an issue for the distributed
system TCP/IP communications. Since traffic volume is too high, 3D data has to
be compressed before it can be sent over the network. The solution consists in
selfcoding the Kinect data into RGB images and then using a standard multimedia
codec to compress color maps. Information from different sources is collected
into a central client computer, where point clouds are transformed to
reconstruct the scene in 3D. An algorithm is proposed to merge the skeletons
detected locally by each Kinect conveniently, so that monitoring of people is
robust to self and inter-user occlusions. Final skeletons are labeled and
trajectories of every joint can be saved for event reconstruction or further
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2902</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2902</id><created>2014-03-12</created><updated>2015-07-08</updated><authors><author><keyname>Mi</keyname><forenames>De</forenames></author><author><keyname>Dianati</keyname><forenames>Mehrdad</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author></authors><title>A Novel Antenna Selection Scheme for Spatially Correlated Massive MIMO
  Uplinks with Imperfect Channel Estimation</title><categories>cs.IT math.IT</categories><comments>in Proc. IEEE 81st Vehicular Technology Conference (VTC), May 2015, 6
  pages, 5 figures</comments><doi>10.1109/VTCSpring.2015.7145629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new antenna selection scheme for a massive MIMO system with a
single user terminal and a base station with a large number of antennas. We
consider a practical scenario where there is a realistic correlation among the
antennas and imperfect channel estimation at the receiver side. The proposed
scheme exploits the sparsity of the channel matrix for the effective selection
of a limited number of antennas. To this end, we compute a sparse channel
matrix by minimising the mean squared error. This optimisation problem is then
solved by the well-known orthogonal matching pursuit algorithm. Widely used
models for spatial correlation among the antennas and channel estimation errors
are considered in this work. Simulation results demonstrate that when the
impacts of spatial correlation and imperfect channel estimation introduced, the
proposed scheme in the paper can significantly reduce complexity of the
receiver, without degrading the system performance compared to the maximum
ratio combining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2906</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2906</id><created>2014-03-12</created><authors><author><keyname>Karakaya</keyname><forenames>Murat</forenames></author></authors><title>Uav Route Planning For Maximum Target Coverage</title><categories>cs.RO cs.NE</categories><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 4, No. 1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilization of Unmanned Aerial Vehicles (UAVs) in military and civil
operations is getting popular. One of the challenges in effectively tasking
these expensive vehicles is planning the flight routes to monitor the targets.
In this work, we aim to develop an algorithm which produces routing plans for a
limited number of UAVs to cover maximum number of targets considering their
flight range. The proposed solution for this practical optimization problem is
designed by modifying the Max-Min Ant System (MMAS) algorithm. To evaluate the
success of the proposed method, an alternative approach, based on the Nearest
Neighbour (NN) heuristic, has been developed as well. The results showed the
success of the proposed MMAS method by increasing the number of covered targets
compared to the solution based on the NN heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2912</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2912</id><created>2014-03-12</created><updated>2014-03-14</updated><authors><author><keyname>Blanco-Chac&#xf3;n</keyname><forenames>Iv&#xe1;n</forenames></author><author><keyname>Rem&#xf3;n</keyname><forenames>Dion&#xed;s</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Alsina</keyname><forenames>Montserrat</forenames></author></authors><title>Nonuniform Fuchsian codes for noisy channels</title><categories>cs.IT math.IT</categories><journal-ref>Journal of the Franklin Institute 351 (2014), pp. 5076-5098</journal-ref><doi>10.1016/j.jfranklin.2014.08.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new transmission scheme for additive white Gaussian noisy (AWGN)
channels based on Fuchsian groups from rational quaternion algebras. The
structure of the proposed Fuchsian codes is nonlinear and nonuniform, hence
conventional decoding methods based on linearity and symmetry do not apply.
Previously, only brute force decoding methods with complexity that is linear in
the code size exist for general nonuniform codes. However, the properly
discontinuous character of the action of the Fuchsian groups on the complex
upper half-plane translates into decoding complexity that is logarithmic in the
code size via a recently introduced point reduction algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2914</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2914</id><created>2014-03-12</created><authors><author><keyname>Katyal</keyname><forenames>Mayanka</forenames></author><author><keyname>Mishra</keyname><forenames>Atul</forenames></author></authors><title>Application of Selective Algorithm for Effective Resource Provisioning
  in Cloud Computing Environment</title><categories>cs.DC cs.NI</categories><comments>10 Pages, International Journal on Cloud Computing: Services and
  Architecture (IJCCSA) ,Vol. 4, No. 1, February 2014</comments><journal-ref>International Journal on Cloud Computing: Services and
  Architecture (IJCCSA) ,Vol. 4, No. 1, February 2014</journal-ref><doi>10.5121/ijccsa.2014.4101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern day continued demand for resource hungry services and applications in
IT sector has led to development of Cloud computing. Cloud computing
environment involves high cost infrastructure on one hand and need high scale
computational resources on the other hand. These resources need to be
provisioned (allocation and scheduling) to the end users in most efficient
manner so that the tremendous capabilities of cloud are utilized effectively
and efficiently. In this paper we discuss a selective algorithm for allocation
of cloud resources to end-users on-demand basis. This algorithm is based on
min-min and max-min algorithms. These are two conventional task scheduling
algorithm. The selective algorithm uses certain heuristics to select between
the two algorithms so that overall makespan of tasks on the machines is
minimized. The tasks are scheduled on machines in either space shared or time
shared manner. We evaluate our provisioning heuristics using a cloud simulator,
called CloudSim. We also compared our approach to the statistics obtained when
provisioning of resources was done in First-Cum-First- Serve(FCFS) manner. The
experimental results show that overall makespan of tasks on given set of VMs
minimizes significantly in different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2919</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2919</id><created>2014-03-12</created><authors><author><keyname>Kindt</keyname><forenames>Philipp</forenames></author><author><keyname>Yunge</keyname><forenames>Daniel</forenames></author><author><keyname>Diemer</keyname><forenames>Robert</forenames></author><author><keyname>Chakraborty</keyname><forenames>Samarjit</forenames></author></authors><title>Precise Energy Modeling for the Bluetooth Low Energy Protocol</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bluetooth Low Energy (BLE) is a wireless protocol well suited for
ultra-low-power sensors running on small batteries. BLE is described as a new
protocol in the official Bluetooth 4.0 specification. To design
energy-efficient devices, the protocol provides a number of parameters that
need to be optimized within an energy, latency and throughput design space. To
minimize power consumption, the protocol parameters have to be optimized for a
given application. Therefore, an energy-model that can predict the energy
consumption of a BLE-based wireless device for different parameter value
settings, is needed. As BLE differs from the original Bluetooth significantly,
models for Bluetooth cannot be easily applied to the BLE protocol. Since the
last one year, there have been a couple of proposals on energy models for BLE.
However, none of them can model all the operating modes of the protocol. This
paper presents a precise energy model of the BLE protocol, that allows the
computation of a device's power consumption in all possible operating modes. To
the best of our knowledge, our proposed model is not only one of the most
accurate ones known so far (because it accounts for all protocol parameters),
but it is also the only one that models all the operating modes of BLE.
Furthermore, we present a sensitivity analysis of the different parameters on
the energy consumption and evaluate the accuracy of the model using both
discrete event simulation and actual measurements. Based on this model,
guidelines for system designers are presented, that help choosing the right
parameters for optimizing the energy consumption for a given application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2923</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2923</id><created>2014-03-12</created><updated>2014-11-28</updated><authors><author><keyname>Brigadir</keyname><forenames>Igor</forenames></author><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Adaptive Representations for Tracking Breaking News on Twitter</title><categories>cs.IR cs.NE</categories><comments>8 Page</comments><acm-class>I.5.4; I.5.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is often the most up-to-date source for finding and tracking breaking
news stories. Therefore, there is considerable interest in developing filters
for tweet streams in order to track and summarize stories. This is a
non-trivial text analytics task as tweets are short, and standard retrieval
methods often fail as stories evolve over time. In this paper we examine the
effectiveness of adaptive mechanisms for tracking and summarizing breaking news
stories. We evaluate the effectiveness of these mechanisms on a number of
recent news events for which manually curated timelines are available.
Assessments based on ROUGE metrics indicate that an adaptive approaches are
best suited for tracking evolving stories on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2926</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2926</id><created>2014-03-12</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Downey</keyname><forenames>Rodney G.</forenames></author></authors><title>Courcelle's theorem for triangulations</title><categories>math.GT cs.CC cs.CG math.CO</categories><comments>24 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In graph theory, Courcelle's theorem essentially states that, if an
algorithmic problem can be formulated in monadic second-order logic, then it
can be solved in linear time for graphs of bounded treewidth. We prove such a
metatheorem for a general class of triangulations of arbitrary fixed dimension
d, including all triangulated d-manifolds: if an algorithmic problem can be
expressed in monadic second-order logic, then it can be solved in linear time
for triangulations whose dual graphs have bounded treewidth.
  We apply our results to 3-manifold topology, a setting with many difficult
computational problems but very few parameterised complexity results, and where
treewidth has practical relevance as a parameter. Using our metatheorem, we
recover and generalise earlier fixed-parameter tractability results on taut
angle structures and discrete Morse theory respectively, and prove a new
fixed-parameter tractability result for computing the powerful but complex
Turaev-Viro invariants on 3-manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2933</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2933</id><created>2014-03-12</created><updated>2014-07-10</updated><authors><author><keyname>Larremore</keyname><forenames>Daniel B.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author></authors><title>Efficiently inferring community structure in bipartite networks</title><categories>cs.SI physics.data-an physics.soc-ph q-bio.QM stat.ML</categories><comments>12 pages, 9 figures</comments><journal-ref>Physical Review E 90(1): 012805 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bipartite networks are a common type of network data in which there are two
types of vertices, and only vertices of different types can be connected. While
bipartite networks exhibit community structure like their unipartite
counterparts, existing approaches to bipartite community detection have
drawbacks, including implicit parameter choices, loss of information through
one-mode projections, and lack of interpretability. Here we solve the community
detection problem for bipartite networks by formulating a bipartite stochastic
block model, which explicitly includes vertex type information and may be
trivially extended to $k$-partite networks. This bipartite stochastic block
model yields a projection-free and statistically principled method for
community detection that makes clear assumptions and parameter choices and
yields interpretable results. We demonstrate this model's ability to
efficiently and accurately find community structure in synthetic bipartite
networks with known structure and in real-world bipartite networks with unknown
structure, and we characterize its performance in practical contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2941</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2941</id><created>2014-02-27</created><updated>2014-07-06</updated><authors><author><keyname>Cormode</keyname><forenames>Graham</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Yan</keyname><forenames>Jinyun</forenames></author></authors><title>People Like Us: Mining Scholarly Data for Comparable Researchers</title><categories>cs.DL cs.SI</categories><comments>BigScholar at WWW 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the problem of finding comparable researchers for any given
researcher. This problem has many motivations. Firstly, know thyself. The
answers of where we stand among research community and who we are most alike
may not be easily found by existing evaluations of ones' research mainly based
on citation counts. Secondly, there are many situations where one needs to find
comparable researchers e.g., for reviewing peers, constructing programming
committees or compiling teams for grants. It is often done through an ad hoc
and informal basis. Utilizing the large scale scholarly data accessible on the
web, we address the problem of automatically finding comparable researchers. We
propose a standard to quantify the quality of research output, via the quality
of publishing venues. We represent a researcher as a sequence of her
publication records, and develop a framework of comparison of researchers by
sequence matching. Several variations of comparisons are considered including
matching by quality of publication venue and research topics, and performing
prefix matching. We evaluate our methods on a large corpus and demonstrate the
effectiveness of our methods through examples. In the end, we identify several
promising directions for further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2950</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2950</id><created>2014-03-12</created><authors><author><keyname>Saleema</keyname><forenames>J S</forenames></author><author><keyname>Bhagawathi</keyname><forenames>N</forenames></author><author><keyname>Monica</keyname><forenames>S</forenames></author><author><keyname>Shenoy</keyname><forenames>P Deepa</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Cancer Prognosis Prediction Using Balanced Stratified Sampling</title><categories>cs.LG</categories><msc-class>62D05</msc-class><acm-class>I.2.6; H.2.8</acm-class><journal-ref>International Journal on Soft Computing, Artificial Intelligence
  and Applications (IJSCAI), Vol.3, No. 1, February 2014, pp 9-18</journal-ref><doi>10.5121/ijscai.2014.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High accuracy in cancer prediction is important to improve the quality of the
treatment and to improve the rate of survivability of patients. As the data
volume is increasing rapidly in the healthcare research, the analytical
challenge exists in double. The use of effective sampling technique in
classification algorithms always yields good prediction accuracy. The SEER
public use cancer database provides various prominent class labels for
prognosis prediction. The main objective of this paper is to find the effect of
sampling techniques in classifying the prognosis variable and propose an ideal
sampling method based on the outcome of the experimentation. In the first phase
of this work the traditional random sampling and stratified sampling techniques
have been used. At the next level the balanced stratified sampling with
variations as per the choice of the prognosis class labels have been tested.
Much of the initial time has been focused on performing the pre_processing of
the SEER data set. The classification model for experimentation has been built
using the breast cancer, respiratory cancer and mixed cancer data sets with
three traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest
Neighbor. The three prognosis factors survival, stage and metastasis have been
used as class labels for experimental comparisons. The results shows a steady
increase in the prediction accuracy of balanced stratified model as the sample
size increases, but the traditional approach fluctuates before the optimum
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2958</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2958</id><created>2014-03-11</created><authors><author><keyname>S</keyname><forenames>Deepa</forenames></author></authors><title>An Approach for Normalizing Fuzzy Relational Databases Based on Join
  Dependency</title><categories>cs.DB</categories><comments>3 pages</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V9(1):1-3, March 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by
  Seventh Sense Research Group</journal-ref><doi>10.14445/22312803/IJCTT-V9P101</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Fuzziness in databases is used to denote uncertain or incomplete data.
Relational Databases stress on the nature of the data to be certain. This
certainty based data is used as the basis of the normalization approach
designed for traditional relational databases. But real world data may not
always be certain, thereby making it necessary to design an approach for
normalization that deals with fuzzy data. This paper focuses on the approach
for designing the fifth normal form (5NF) based on join dependencies for fuzzy
data. The basis of join dependency for fuzzy relational databases is derived
from the basic relational database concepts. As join dependency implies an
multivalued dependency by symmetry the proof of join dependency based
normalization is stated from the perspective of multivalued dependency based
normalization on fuzzy relational databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2972</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2972</id><created>2014-03-12</created><authors><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author></authors><title>Complexity of the General Chromatic Art Gallery Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the original Art Gallery Problem (AGP), one seeks the minimum number of
guards required to cover a polygon $P$. We consider the Chromatic AGP (CAGP),
where the guards are colored. As long as $P$ is completely covered, the number
of guards does not matter, but guards with overlapping visibility regions must
have different colors. This problem has applications in landmark-based mobile
robot navigation: Guards are landmarks, which have to be distinguishable (hence
the colors), and are used to encode motion primitives, \eg, &quot;move towards the
red landmark&quot;. Let $\chi_G(P)$, the chromatic number of $P$, denote the minimum
number of colors required to color any guard cover of $P$. We show that
determining, whether $\chi_G(P) \leq k$ is \NP-hard for all $k \geq 2$. Keeping
the number of colors minimal is of great interest for robot navigation, because
less types of landmarks lead to cheaper and more reliable recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2975</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2975</id><created>2014-03-12</created><updated>2015-05-14</updated><authors><author><keyname>Ross</keyname><forenames>Neil J.</forenames></author><author><keyname>Selinger</keyname><forenames>Peter</forenames></author></authors><title>Optimal ancilla-free Clifford+T approximation of z-rotations</title><categories>quant-ph cs.ET</categories><comments>38 pages. New in v2: added a section on approximation up to a phase</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decomposing arbitrary single-qubit z-rotations
into ancilla-free Clifford+T circuits, up to given epsilon. We present a new
efficient algorithm for solving this problem optimally, i.e., for finding the
shortest possible circuit whatsoever for the given problem instance. The
algorithm requires a factoring oracle (such as a quantum computer). Even in the
absence of a factoring oracle, the algorithm is still near-optimal: In this
case, it finds a solution of T-count m + O(log(log(1/epsilon))), where m is the
T-count of the second-to-optimal solution. In the typical case, this yields
circuit decompositions of T-count 3log_2(1/epsilon) + O(log(log(1/epsilon))).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2980</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2980</id><created>2014-03-12</created><authors><author><keyname>Gonzalez-Diaz</keyname><forenames>Rocio</forenames></author><author><keyname>Jimenez</keyname><forenames>Maria-Jose</forenames></author><author><keyname>Medrano</keyname><forenames>Belen</forenames></author></authors><title>3D Well-composed Polyhedral Complexes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary three-dimensional (3D) image $I$ is well-composed if the boundary
surface of its continuous analog is a 2D manifold. Since 3D images are not
often well-composed, there are several voxel-based methods (&quot;repairing&quot;
algorithms) for turning them into well-composed ones but these methods either
do not guarantee the topological equivalence between the original image and its
corresponding well-composed one or involve sub-sampling the whole image.
  In this paper, we present a method to locally &quot;repair&quot; the cubical complex
$Q(I)$ (embedded in $\mathbb{R}^3$) associated to $I$ to obtain a polyhedral
complex $P(I)$ homotopy equivalent to $Q(I)$ such that the boundary of every
connected component of $P(I)$ is a 2D manifold. The reparation is performed via
a new codification system for $P(I)$ under the form of a 3D grayscale image
that allows an efficient access to cells and their faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.2990</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.2990</id><created>2014-03-12</created><authors><author><keyname>Goyal</keyname><forenames>Aditya</forenames></author><author><keyname>Tyagi</keyname><forenames>Akanksha</forenames></author><author><keyname>Bhende</keyname><forenames>Manisha</forenames></author><author><keyname>Kawade</keyname><forenames>Swapnil</forenames></author></authors><title>A Framework for Pricing Schemes for Networks under Complete Information</title><categories>cs.NI</categories><comments>5 pages, 6 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT). arXiv admin note: text overlap
  with arXiv:1011.1065 by other authors</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  V9(2),69-73 March 2014</journal-ref><doi>10.14445/22315381/IJETT-V9P214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The revenue maximization problem of service provider is considered and
different pricing schemes to solve the above problem are implemented. The
service provider can choose an apt pricing scheme subjected to limited
resources, if he knows the utility function and identity of the user. The
complete price differentiation can achieve a large revenue gain but has high
implementation complexity. The partial price differentiation scheme to overcome
the high implementational complexity of complete price differentiation scheme
is also studied. A polynomial- time algorithm is designed for partial price
differentiation scheme that can compute the optimal partial differentiation
prices. The willingness of the users to pay is also considered while designing
price differentiation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3005</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3005</id><created>2014-03-12</created><updated>2015-11-13</updated><authors><author><keyname>Staudt</keyname><forenames>Christian L.</forenames></author><author><keyname>Sazonovs</keyname><forenames>Aleksejs</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>NetworKit: A Tool Suite for Large-scale Complex Network Analysis</title><categories>cs.SI cs.DC physics.soc-ph</categories><comments>21 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce NetworKit, an open-source software package for analyzing the
structure of large complex networks. Appropriate algorithmic solutions are
required to handle increasingly common large graph data sets containing up to
billions of connections. We describe the methodology applied to develop
scalable solutions to network analysis problems, including techniques like
parallelization, heuristics for computationally expensive problems, efficient
data structures, and modular software architecture. Our goal for the software
is to package results of our algorithm engineering efforts and put them into
the hands of domain experts. NetworKit is implemented as a hybrid combining the
kernels written in C++ with a Python front end, enabling integration into the
Python ecosystem of tested tools for data analysis and scientific computing.
The package provides a wide range of functionality (including common and novel
analytics algorithms and graph generators) and does so via a convenient
interface. In an experimental comparison with related software, NetworKit shows
the best performance on a range of typical analysis tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3007</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3007</id><created>2014-03-12</created><updated>2014-04-14</updated><authors><author><keyname>Jarry</keyname><forenames>Aubin</forenames></author></authors><title>The Four Principles of Geographic Routing</title><categories>cs.DC</categories><comments>This manuscript on geographic routing incoporates team feedback and
  expanded experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic routing consists in using the position information of nodes to
assist in the routing process, and has been a widely studied subject in sensor
networks. One of the outstanding challenges facing geographic routing has been
its applicability. Authors either make some broad assumptions on an idealized
version of wireless networks which are often unverifiable, or they use costly
methods to planarize the communication graph.
  The overarching questions that drive us are the following. When, and how
should we use geographic routing? Is there a criterion to tell whether a
communication network is fit for geographic routing? When exactly does
geographic routing make sense?
  In this paper we formulate the four principles that define geographic routing
and explore their topological consequences. Given a localized communication
network, we then define and compute its geographic eccentricity, which measures
its fitness for geographic routing. Finally we propose a distributed algorithm
that either enables geographic routing on the network or proves that its
geographic eccentricity is too high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3011</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3011</id><created>2014-03-12</created><updated>2014-05-29</updated><authors><author><keyname>La Rocca</keyname><forenames>C. E.</forenames></author><author><keyname>Braunstein</keyname><forenames>L. A.</forenames></author><author><keyname>Vazquez</keyname><forenames>F.</forenames></author></authors><title>The influence of persuasion in opinion formation and polarization</title><categories>physics.soc-ph cs.SI</categories><journal-ref>EPL 106 (2014) 40004</journal-ref><doi>10.1209/0295-5075/106/40004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model that explores the influence of persuasion in a population
of agents with positive and negative opinion orientations. The opinion of each
agent is represented by an integer number $k$ that expresses its level of
agreement on a given issue, from totally against $k=-M$ to totally in favor
$k=M$. Same-orientation agents persuade each other with probability $p$,
becoming more extreme, while opposite-orientation agents become more moderate
as they reach a compromise with probability $q$. The population initially
evolves to (a) a polarized state for $r=p/q&gt;1$, where opinions' distribution is
peaked at the extreme values $k=\pm M$, or (b) a centralized state for $r&lt;1$,
with most opinions around $k=\pm 1$. When $r \gg 1$, polarization lasts for a
time that diverges as $r^M \ln N$, where $N$ is the population's size. Finally,
an extremist consensus ($k=M$ or $-M$) is reached in a time that scales as
$r^{-1}$ for $r \ll 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3017</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3017</id><created>2014-03-12</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>Searching in Unstructured Overlays Using Local Knowledge and Gossip</title><categories>cs.DC cs.NI</categories><comments>A revised version of the paper appears in Proc. of the 5th
  International Workshop on Complex Networks (CompleNet 2014) - Studies in
  Computational Intelligence Series, Springer-Verlag, Bologna (Italy), March
  2014</comments><doi>10.1007/978-3-319-05401-8_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes a class of dissemination algorithms for the discovery of
distributed contents in Peer-to-Peer unstructured overlay networks. The
algorithms are a mix of protocols employing local knowledge of peers'
neighborhood and gossip. By tuning the gossip probability and the depth k of
the k-neighborhood of which nodes have information, we obtain different
dissemination protocols employed in literature over unstructured P2P overlays.
The provided analysis and simulation results confirm that, when properly
configured, these schemes represent a viable approach to build effective P2P
resource discovery in large-scale, dynamic distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3021</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3021</id><created>2014-03-12</created><authors><author><keyname>Shu</keyname><forenames>Huazhong</forenames><affiliation>CRIBS, LIST</affiliation></author><author><keyname>Zhou</keyname><forenames>Jian</forenames><affiliation>CRIBS, LTSI</affiliation></author><author><keyname>Han</keyname><forenames>Guo-Niu</forenames><affiliation>IRMA</affiliation></author><author><keyname>Luo</keyname><forenames>Limin M.</forenames><affiliation>CRIBS, LIST</affiliation></author><author><keyname>Coatrieux</keyname><forenames>Jean-Louis</forenames><affiliation>CRIBS, LTSI</affiliation></author></authors><title>Image reconstruction from limited range projections using orthogonal
  moments</title><categories>cs.CV math.NA</categories><proxy>ccsd</proxy><journal-ref>Pattern Recognition 40, 2 (2007) 670-680</journal-ref><doi>10.1016/j.patcog.2006.05.035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of orthonormal polynomials is proposed for image reconstruction from
projection data. The relationship between the projection moments and image
moments is discussed in detail, and some interesting properties are
demonstrated. Simulation results are provided to validate the method and to
compare its performance with previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3022</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3022</id><created>2014-03-12</created><authors><author><keyname>Yang</keyname><forenames>Guanyu</forenames><affiliation>LTSI, CRIBS, LIST</affiliation></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames><affiliation>CRIBS, LIST</affiliation></author><author><keyname>Toumoulin</keyname><forenames>Christine</forenames><affiliation>LTSI, CRIBS</affiliation></author><author><keyname>Han</keyname><forenames>Guo-Niu</forenames><affiliation>IRMA</affiliation></author><author><keyname>Luo</keyname><forenames>Limin M.</forenames><affiliation>CRIBS, LIST</affiliation></author></authors><title>Efficient Legendre moment computation for grey level images</title><categories>cs.CV math.NA</categories><proxy>ccsd</proxy><journal-ref>Pattern Recognition 39, 1 (2006) 74-80</journal-ref><doi>10.1016/j.patcog.2005.08.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Legendre orthogonal moments have been widely used in the field of image
analysis. Because their computation by a direct method is very time expensive,
recent efforts have been devoted to the reduction of computational complexity.
Nevertheless, the existing algorithms are mainly focused on binary images. We
propose here a new fast method for computing the Legendre moments, which is not
only suitable for binary images but also for grey levels. We first set up the
recurrence formula of one-dimensional (1D) Legendre moments by using the
recursive property of Legendre polynomials. As a result, the 1D Legendre
moments of order p, Lp = Lp(0), can be expressed as a linear combination of
Lp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0)
is thus obtained from the array of L1(a) and L0(a) where a is an integer number
less than p. To further decrease the computation complexity, an algorithm, in
which no multiplication is required, is used to compute these quantities. The
method is then extended to the calculation of the two-dimensional Legendre
moments Lpq. We show that the proposed method is more efficient than the direct
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3034</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3034</id><created>2014-03-09</created><updated>2014-03-23</updated><authors><author><keyname>James</keyname><forenames>Phillip</forenames></author><author><keyname>Roggenbach</keyname><forenames>Markus</forenames></author></authors><title>Encapsulating Formal Methods within Domain Specific Languages: A
  Solution for Verifying Railway Scheme Plans</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development and application of formal methods is a long standing research
topic within the field of computer science. One particular challenge that
remains is the uptake of formal methods into industrial practices. This paper
introduces a methodology for developing domain specific languages for modelling
and verification to aid in the uptake of formal methods within industry. It
illustrates the successful application of this methodology within the railway
domain. The presented methodology addresses issues surrounding faithful
modelling, scalability of verification and accessibility to modelling and
verification processes for practitioners within the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3036</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3036</id><created>2014-03-12</created><updated>2015-02-10</updated><authors><author><keyname>Bassi</keyname><forenames>Germ&#xe1;n</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author></authors><title>Capacity Bounds for a Class of Interference Relay Channels</title><categories>cs.IT math.IT</categories><comments>23 pages, 6 figures. Submitted to IEEE Transactions on Information
  Theory (revised version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of a class of Interference Relay Channels (IRC) -the Injective
Semideterministic IRC where the relay can only observe one of the sources- is
investigated. We first derive a novel outer bound and two inner bounds which
are based on a careful use of each of the available cooperative strategies
together with the adequate interference decoding technique. The outer bound
extends Telatar and Tse's work while the inner bounds contain several known
results in the literature as special cases. Our main result is the
characterization of the capacity region of the Gaussian class of IRCs studied
within a fixed number of bits per dimension -constant gap. The proof relies on
the use of the different cooperative strategies in specific SNR regimes due to
the complexity of the schemes. As a matter of fact, this issue reveals the
complex nature of the Gaussian IRC where the combination of a single coding
scheme for the Gaussian relay and interference channel may not lead to a good
coding scheme for this problem, even when the focus is only on capacity to
within a constant gap over all possible fading statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3039</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3039</id><created>2014-03-11</created><authors><author><keyname>Khan-Afshar</keyname><forenames>Sanaz</forenames></author><author><keyname>Siddique</keyname><forenames>Umair</forenames></author><author><keyname>Mahmoud</keyname><forenames>Mohamed Yousri</forenames></author><author><keyname>Aravantinos</keyname><forenames>Vincent</forenames></author><author><keyname>Seddiki</keyname><forenames>Ons</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author><author><keyname>Tahar</keyname><forenames>Sofiene</forenames></author></authors><title>Formal Analysis of Optical Systems</title><categories>cs.LO physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical systems are becoming increasingly important by resolving many
bottlenecks in today's communication, electronics, and biomedical systems.
However, given the continuous nature of optics, the inability to efficiently
analyze optical system models using traditional paper-and-pencil and computer
simulation approaches sets limits especially in safety-critical applications.
In order to overcome these limitations, we propose to employ higher-order-logic
theorem proving as a complement to computational and numerical approaches to
improve optical model analysis in a comprehensive framework. The proposed
framework allows formal analysis of optical systems at four abstraction levels,
i.e., ray, wave, electromagnetic, and quantum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3046</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3046</id><created>2014-03-12</created><authors><author><keyname>Troshchiev</keyname><forenames>Y. V.</forenames></author></authors><title>Improvement of the monotonicity properties of the difference schemes by
  building in them of the monotonizing operators</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of monotonization of difference schemes is being considered in the
paper. The method was earlier proposed by the author for stationary problems.
It is investigated in the paper more profoundly. The idea of the method is to
build the monotonizing operators into the schemes so that the balance relations
from point to point are not violated. Different monotonizing operators can be
used to be installed in the schemes. Propositions concerning approximation and
stability of the monotonized schemes are formulated and proved. Also a
proposition significant for practical use of the schemes is formulated and
proved. The idea is to use the monotonized schemes in the cases when the
proposition conditions are fulfilled. The proposition is based on closeness of
solutions of the initial and auxiliary schemes. Constructions for solving of
time dependent problems are also written in the paper. One dimensional example
and three-dimensional hydrodynamic example are considered. The method allows to
considerably decrease value of calculations in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3049</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3049</id><created>2014-03-12</created><authors><author><keyname>Christofides</keyname><forenames>Demetres</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author></authors><title>First order convergence and roots</title><categories>math.CO cs.DM</categories><journal-ref>Combinator. Probab. Comp. 25 (2015) 213-221</journal-ref><doi>10.1017/S0963548315000048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nesetril and Ossona de Mendez introduced the notion of first order
convergence, which unifies the notions of convergence for sparse and dense
graphs. They asked whether if G_i is a sequence of graphs with M being their
first order limit and v is a vertex of M, then there exists a sequence v_i of
vertices such that the graphs G_i rooted at v_i converge to M rooted at v. We
show that this holds for almost all vertices v of M and we give an example
showing that the statement need not hold for all vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3055</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3055</id><created>2014-03-12</created><authors><author><keyname>Gkoutzioupas</keyname><forenames>Stylianos</forenames></author></authors><title>Fulfillment Request Management (The approach)</title><categories>cs.SE</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 1, February 2014</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we introduce the term FRM (Fulfillment Request Management).
According to the FRM in a BSS / OSS environment we can use a unified approach
to implement a SOA in order to integrate BSS with OSS and handle 1. Orders 2.
Events 3. Processes. So in a way that systems like ESB, Order Management, and
Business Process Management can be implemented under a unified architecture and
a unified implementation. We assume that all the above mentioned are 'requests'
and according to the system we want to implement, the request can be an event,
an order, a process etc. So instead of having N systems we have 1 system that
covers all the above (ESB, Order Management, BPM etc) With the FRM we can have
certain advantages such as: 1. adaptation 2. Interoperability. 3. Re-usability
4. Fast implementation 5. Easy reporting. In this paper we present a set of the
main principles in order to build an FRM System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3057</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3057</id><created>2014-03-12</created><authors><author><keyname>Sartin</keyname><forenames>Maicon A.</forenames></author><author><keyname>da Silva</keyname><forenames>Alexandre C. R.</forenames></author></authors><title>Evaluation of Image Segmentation and Filtering With ANN in the Papaya
  Leaf</title><categories>cs.NE cs.CV</categories><comments>12 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6 No 1 (2014) 47-58</journal-ref><doi>10.5121/ijcsit.2014.6104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precision agriculture is area with lack of cheap technology. The refinement
of the production system brings large advantages to the producer and the use of
images makes the monitoring a more cheap methodology. Macronutrients monitoring
can to determine the health and vulnerability of the plant in specific stages.
In this paper is analyzed the method based on computational intelligence to
work with image segmentation in the identification of symptoms of plant
nutrient deficiency. Artificial neural networks are evaluated for image
segmentation and filtering, several variations of parameters and insertion
impulsive noise were evaluated too. Satisfactory results are achieved with
artificial neural for segmentation same with high noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3060</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3060</id><created>2014-02-21</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vastrad</keyname><forenames>Chanabasayya M.</forenames></author></authors><title>Non linear Prediction of Antitubercular Activity Of Oxazolines and
  Oxazoles derivatives Making Use of Compact TS-Fuzzy models Through Clustering
  with orthogonal least sqaure technique and Fuzzy identification system</title><categories>cs.CE</categories><comments>Published 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The prediction of uncertain and predictive nonlinear systems is an important
and challenging problem. Fuzzy logic models are often a good choice to describe
such systems however in many cases these become complex soon. commonlly, too
less effort is put into descriptor selection and in the creation of suitable
local rules. Moreover, in common no model reduction is applied, while this may
analyze the model by removing redundant data. This paper suggests a combined
method that deal with these issues in order to create compact Takagi Sugeno
(TS) models that can be effectively used to represent complex predictive
systems. A new fuzzy clustering method is come up with for the identification
of compact TS-fuzzy models. The best relevant consequent variables of the TS
model are choosen by an orthogonal least squares technique based on the
obtained clusters.For the selection of the relevant antecedent (scheduling)
variables a new method has been developed based on Fisher's interclass
separability basis. This complete approach is demonstrated by means of the
Oxazolines and Oxazoles derivatives as antituberculosis agent for nonlinear
regression benchmark. The results are compared with results obtained by
neuro-fuzzy i.e. ANFIS algorithm and advanced fuzzyy clustering techniques i.e
FMID toolbox .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3061</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3061</id><created>2014-03-12</created><authors><author><keyname>Kasem</keyname><forenames>Hossam M.</forenames></author><author><keyname>El-Sabrouty</keyname><forenames>Maha</forenames></author></authors><title>A Comparative Study of Audio Compression Based on Compressed Sensing and
  Sparse Fast Fourier Transform (SFFT): Performance and Challenges</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio compression has become one of the basic multimedia technologies.
Choosing an efficient compression scheme that is capable of preserving the
signal quality while providing a high compression ratio is desirable in the
different standards worldwide. In this paper we study the application of two
highly acclaimed sparse signal processing algorithms, namely, Compressed
Sensing (CS) and Sparse Fart Fourier transform, to audio compression. In
addition, we present a Sparse Fast Fourier transform (SFFT)-based framework to
compress audio signal. This scheme embeds the K-largest frequencies indices as
part of the transmitted signal and thus saves in the bandwidth required for
transmission
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3077</identifier>
 <datestamp>2014-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3077</id><created>2014-03-12</created><authors><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Zhao</keyname><forenames>Minjian</forenames></author></authors><title>Set-Membership Adaptive Constant Modulus Algorithm with a Generalized
  Sidelobe Canceler and Dynamic Bounds for Beamforming</title><categories>cs.IT math.IT</categories><comments>9 figures. Signal Processing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose an adaptive set-membership constant modulus (SM-CM)
algorithm with a generalized sidelobe canceler (GSC) structure for blind
beamforming. We develop a stochastic gradient (SG) type algorithm based on the
concept of SM filtering for adaptive implementation. The filter weights are
updated only if the constraint cannot be satisfied. In addition, we also
propose an extension of two schemes of time-varying bounds for beamforming with
a GSC structure and incorporate parameter and interference dependence to
characterize the environment which improves the tracking performance of the
proposed algorithm in dynamic scenarios. A convergence analysis of the proposed
adaptive SM filtering techniques is carried out. Simulation results show that
the proposed adaptive SM-CM-GSC algorithm with dynamic bounds achieves superior
performance to previously reported methods at a reduced update rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3080</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3080</id><created>2014-03-12</created><updated>2014-04-24</updated><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Zhou</keyname><forenames>Dengyong</forenames></author></authors><title>Statistical Decision Making for Optimal Budget Allocation in Crowd
  Labeling</title><categories>cs.LG math.OC stat.ML</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In crowd labeling, a large amount of unlabeled data instances are outsourced
to a crowd of workers. Workers will be paid for each label they provide, but
the labeling requester usually has only a limited amount of the budget. Since
data instances have different levels of labeling difficulty and workers have
different reliability, it is desirable to have an optimal policy to allocate
the budget among all instance-worker pairs such that the overall labeling
accuracy is maximized. We consider categorical labeling tasks and formulate the
budget allocation problem as a Bayesian Markov decision process (MDP), which
simultaneously conducts learning and decision making. Using the dynamic
programming (DP) recurrence, one can obtain the optimal allocation policy.
However, DP quickly becomes computationally intractable when the size of the
problem increases. To solve this challenge, we propose a computationally
efficient approximate policy, called optimistic knowledge gradient policy. Our
MDP is a quite general framework, which applies to both pull crowdsourcing
marketplaces with homogeneous workers and push marketplaces with heterogeneous
workers. It can also incorporate the contextual information of instances when
they are available. The experiments on both simulated and real data show that
the proposed policy achieves a higher labeling accuracy than other existing
policies at the same budget level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3083</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3083</id><created>2014-03-13</created><updated>2014-11-01</updated><authors><author><keyname>Wang</keyname><forenames>Shuliang</forenames></author><author><keyname>Chen</keyname><forenames>Yasen</forenames></author></authors><title>A Novel Method to Extract Rocks from Mars Images</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 2</comments><journal-ref>Chinese Journal of Electronics,Vol.24, No.3, July 2015, pp.455-461</journal-ref><doi>10.1049/cje.2015.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel method is proposed to extract rocks from Martian
surface images by using 8 data field. It models the interaction between two
pixels of an image in the context of imagery 9 characteristics. First,
foreground rocks are differed from background information by binarizing 10
image on roughly partitioned images. Second, foreground rocks are grouped into
clusters by 11 locating the centers and edges of clusters in data field via
hierarchical grids. Third, the target 12 rocks are discovered for the Mars
Exploration Rover (MER) to keep healthy paths. The 13 experiment with images
taken by MER shows the proposed method is practical and potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3084</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3084</id><created>2014-03-12</created><authors><author><keyname>Garc&#xed;a-Ortega</keyname><forenames>R. H.</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>P.</forenames></author><author><keyname>Merelo</keyname><forenames>J. J.</forenames></author></authors><title>Emerging archetypes in massive artificial societies for literary
  purposes using genetic algorithms</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The creation of fictional stories is a very complex task that usually implies
a creative process where the author has to combine characters, conflicts and
plots to create an engaging narrative. This work presents a simulated
environment with hundreds of characters that allows the study of coherent and
interesting literary archetypes (or behaviours), plots and sub-plots. We will
use this environment to perform a study about the number of profiles
(parameters that define the personality of a character) needed to create two
emergent scenes of archetypes: &quot;natality control&quot; and &quot;revenge&quot;. A Genetic
Algorithm (GA) will be used to find the fittest number of profiles and
parameter configuration that enables the existence of the desired archetypes
(played by the characters without their explicit knowledge). The results show
that parametrizing this complex system is possible and that these kind of
archetypes can emerge in the given environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3100</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3100</id><created>2014-03-12</created><updated>2014-03-13</updated><authors><author><keyname>Anderson</keyname><forenames>Ashton</forenames></author><author><keyname>Huttenlocher</keyname><forenames>Daniel</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Engaging with Massive Online Courses</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>WWW 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Web has enabled one of the most visible recent developments in
education---the deployment of massive open online courses. With their global
reach and often staggering enrollments, MOOCs have the potential to become a
major new mechanism for learning. Despite this early promise, however, MOOCs
are still relatively unexplored and poorly understood.
  In a MOOC, each student's complete interaction with the course materials
takes place on the Web, thus providing a record of learner activity of
unprecedented scale and resolution. In this work, we use such trace data to
develop a conceptual framework for understanding how users currently engage
with MOOCs. We develop a taxonomy of individual behavior, examine the different
behavioral patterns of high- and low-achieving students, and investigate how
forum participation relates to other parts of the course.
  We also report on a large-scale deployment of badges as incentives for
engagement in a MOOC, including randomized experiments in which the
presentation of badges was varied across sub-populations. We find that making
badges more salient produced increases in forum engagement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3109</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3109</id><created>2014-03-12</created><authors><author><keyname>Aksoylar</keyname><forenames>Cem</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Sparse Recovery with Linear and Nonlinear Observations: Dependent and
  Noisy Data</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Extended version of the paper that was accepted to AISTATS 2014 as
  &quot;Information-Theoretic Characterization of Sparse Recovery&quot;. arXiv admin
  note: text overlap with arXiv:1304.0682</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate sparse support recovery as a salient set identification problem
and use information-theoretic analyses to characterize the recovery performance
and sample complexity. We consider a very general model where we are not
restricted to linear models or specific distributions. We state non-asymptotic
bounds on recovery probability and a tight mutual information formula for
sample complexity. We evaluate our bounds for applications such as sparse
linear regression and explicitly characterize effects of correlation or noisy
features on recovery performance. We show improvements upon previous work and
identify gaps between the performance of recovery algorithms and fundamental
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3115</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3115</id><created>2014-03-12</created><authors><author><keyname>Kotagiri</keyname><forenames>Vamsi Sashank</forenames></author></authors><title>Memory Capacity of Neural Networks using a Circulant Weight Matrix</title><categories>cs.NE</categories><comments>19 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3117</identifier>
 <datestamp>2014-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3117</id><created>2014-03-12</created><updated>2014-05-02</updated><authors><author><keyname>Bandyopadhyay</keyname><forenames>Saptarshi</forenames></author><author><keyname>Chung</keyname><forenames>Soon-Jo</forenames></author></authors><title>Distributed Estimation using Bayesian Consensus Filtering</title><categories>math.OC cs.IT math.IT math.PR</categories><comments>Submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Bayesian consensus filter (BCF) for tracking a moving target
using a networked group of sensing agents and achieving consensus on the best
estimate of the probability distributions of the target's states. Our BCF
framework can incorporate nonlinear target dynamic models, heterogeneous
nonlinear measurement models, non-Gaussian uncertainties, and higher-order
moments of the locally estimated posterior probability distribution of the
target's states obtained using Bayesian filters. If the agents combine their
estimated posterior probability distributions using a logarithmic opinion pool,
then the sum of Kullback--Leibler divergences between the consensual
probability distribution and the local posterior probability distributions is
minimized. Rigorous stability and convergence results for the proposed BCF
algorithm with single or multiple consensus loops are presented. Communication
of probability distributions and computational methods for implementing the BCF
algorithm are discussed along with a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3118</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3118</id><created>2014-03-12</created><authors><author><keyname>Moreira</keyname><forenames>Rodrigo da Silva</forenames></author><author><keyname>Ebecken</keyname><forenames>Nelson Francisco Favilla</forenames></author></authors><title>Parallel WiSARD object tracker: a ram-based tracking system</title><categories>cs.CV</categories><comments>15 pages, 7 figures</comments><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol. 4, No. 1, February 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new object
tracker based on the WiSARD weightless neural network that is robust against
quantization errors. Object tracking in video is an important and challenging
task in many applications. Difficulties can arise due to weather conditions,
target trajectory and appearance, occlusions, lighting conditions and noise.
Tracking is a high-level application and requires the object location frame by
frame in real time. This paper proposes a fast hybrid image segmentation
(threshold and edge detection) in YcbCr color model and a parallel RAM based
discriminator that improves efficiency when quantization errors occur. The
original WiSARD training algorithm was changed to allow the tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3119</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3119</id><created>2014-03-12</created><updated>2014-12-03</updated><authors><author><keyname>Petrov</keyname><forenames>V. V.</forenames></author><author><keyname>Semynozhenko</keyname><forenames>V. P.</forenames></author><author><keyname>Puzikov</keyname><forenames>V. M.</forenames></author><author><keyname>Kryuchyn</keyname><forenames>A. A.</forenames></author><author><keyname>Lapchuk</keyname><forenames>A. S.</forenames></author><author><keyname>Shanoilo</keyname><forenames>S. M.</forenames></author><author><keyname>Kosyak</keyname><forenames>I. V.</forenames></author><author><keyname>Borodin</keyname><forenames>Yu. O.</forenames></author><author><keyname>Gorbov</keyname><forenames>I. V.</forenames></author><author><keyname>Morozov</keyname><forenames>Ye. M.</forenames></author></authors><title>Readout Optical System of Sapphire Disks intended for Long-Term Data
  Storage</title><categories>cs.ET physics.optics physics.pop-ph</categories><comments>10 pages, 3 figures, 2 tables, ArXiv.org:1310.2961v1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of long-term data storage technology is one of the urging
problems of our time. This paper presents the results of implementation of
technical solution for long-term data storage technology proposed a few years
ago on the basis of single crystal sapphire. It is shown that the problem of
reading data through a substrate of negative single crystal sapphire can be
solved by using for reading a special optical system with a plate of positive
single crystal quartz. The experimental results confirm the efficiency of the
proposed method of compensation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3126</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3126</id><created>2014-03-12</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Signaling in sensor networks for sequential detection</title><categories>cs.SY</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential detection problems in sensor networks are considered. The true
state of nature/true hypothesis is modeled as a binary random variable $H$ with
known prior distribution. There are $N$ sensors making noisy observations about
the hypothesis; $\mathcal{N} =\{1,2,\ldots,N\}$ denotes the set of sensors.
Sensor $i$ can receive messages from a subset $\mathcal{P}^i \subset
\mathcal{N}$ of sensors and send a message to a subset $\mathcal{C}^i \subset
\mathcal{N}$. Each sensor is faced with a stopping problem. At each time $t$,
based on the observations it has taken so far and the messages it may have
received, sensor $i$ can decide to stop and communicate a binary decision to
the sensors in $\mathcal{C}^i$, or it can continue taking observations and
receiving messages. After sensor $i$'s binary decision has been sent, it
becomes inactive. Sensors incur operational costs (cost of taking observations,
communication costs etc.) while they are active. In addition, the system incurs
a terminal cost that depends on the true hypothesis $H$, the sensors' binary
decisions and their stopping times. The objective is to determine decision
strategies for all sensors to minimize the total expected cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3131</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3131</id><created>2014-03-12</created><authors><author><keyname>Asili</keyname><forenames>Harika</forenames></author><author><keyname>Tanriover</keyname><forenames>Omer Ozgur</forenames></author></authors><title>Comparison Of Document Management Systems By Meta Modelling And
  Workforce Centric Tuning Measures</title><categories>cs.SE cs.HC</categories><comments>11 pages , 2 figures and 2 tables , journal paper</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 4,No. 1, February 2014</journal-ref><doi>10.5121/ijcseit.2014.4106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document management software are used widely for office paper management and
related workflows. However, they have some differences from a flexibility
perspective for the needs of workers. Moreover, they also differentiate in
their underlying conceptual design. In this paper, a set (eight) of widely used
document management systems are chosen from internet reviews [5] [6] and their
conceptual models are analysed with meta modeling and flexibility tuning
measures. The main objective is to compare these systems by analyzing their
human-orientation and flexibility. This analysis provides useful information
for organizations especially looking for document management systems which are
more workforce centric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3142</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3142</id><created>2014-03-12</created><updated>2014-07-14</updated><authors><author><keyname>Ghosh</keyname><forenames>Shalini</forenames></author><author><keyname>Elenius</keyname><forenames>Daniel</forenames></author><author><keyname>Li</keyname><forenames>Wenchao</forenames></author><author><keyname>Lincoln</keyname><forenames>Patrick</forenames></author><author><keyname>Shankar</keyname><forenames>Natarajan</forenames></author><author><keyname>Steiner</keyname><forenames>Wilfried</forenames></author></authors><title>ARSENAL: Automatic Requirements Specification Extraction from Natural
  Language</title><categories>cs.CL cs.SE</categories><comments>32 pages, 16 figures</comments><report-no>SRI-CSL-13-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language (supplemented with diagrams and some mathematical notations)
is convenient for succinct communication of technical descriptions between the
various stakeholders (e.g., customers, designers, implementers) involved in the
design of software systems. However, natural language descriptions can be
informal, incomplete, imprecise and ambiguous, and cannot be processed easily
by design and analysis tools. Formal languages, on the other hand, formulate
design requirements in a precise and unambiguous mathematical notation, but are
more difficult to master and use. We propose a methodology for connecting
semi-formal requirements with formal descriptions through an intermediate
representation. We have implemented this methodology in a research prototype
called Automatic Requirements Specification Extraction from Natural Language
(ARSENAL). The main novelty of ARSENAL lies in its ability to generate a
fully-specified complete formal model automatically from natural language
requirements. Currently, ARSENAL generates formal models in linear-time
temporal logic (LTL), but the approach can be adapted for other models, e.g.,
probabilistic relational models like Markov Logic Networks (MLN). The formal
models of the requirements can be used to check important design and system
properties, e.g., consistency, satisfiability, realizability. ARSENAL has a
modular and flexible architecture that facilitates porting it from one domain
to another. We evaluated ARSENAL on complex requirements from two real-world
case studies: the Time-Triggered Ethernet (TTEthernet) communication platform
used in space, and FAA-Isolette infant incubators used in NICU. We
systematically evaluated various aspects of ARSENAL - the accuracy of the
natural language processing stage, the degree of automation, and robustness to
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3148</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3148</id><created>2014-03-12</created><updated>2015-01-19</updated><authors><author><keyname>Kloster</keyname><forenames>Kyle</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Heat kernel based community detection</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>10 pages, published in KDD2014 proceedings</comments><msc-class>91D30 (Primary)</msc-class><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heat kernel is a particular type of graph diffusion that, like the
much-used personalized PageRank diffusion, is useful in identifying a community
nearby a starting seed node. We present the first deterministic, local
algorithm to compute this diffusion and use that algorithm to study the
communities that it produces. Our algorithm is formally a relaxation method for
solving a linear system to estimate the matrix exponential in a degree-weighted
norm. We prove that this algorithm stays localized in a large graph and has a
worst-case constant runtime that depends only on the parameters of the
diffusion, not the size of the graph. Our experiments on real-world networks
indicate that the communities produced by this method have better conductance
than those produced by PageRank, although they take slightly longer to compute
on large graphs. On a real-world community identification task, the heat kernel
communities perform better than those from the PageRank diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3155</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3155</id><created>2014-03-12</created><updated>2014-11-17</updated><authors><author><keyname>Zhu</keyname><forenames>Feiyun</forenames></author><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Meng</keyname><forenames>Gaofeng</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Spectral Unmixing via Data-guided Sparsity</title><categories>cs.CV</categories><doi>10.1109/TIP.2014.2363423</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral unmixing, the process of estimating a common set of spectral
bases and their corresponding composite percentages at each pixel, is an
important task for hyperspectral analysis, visualization and understanding.
From an unsupervised learning perspective, this problem is very
challenging---both the spectral bases and their composite percentages are
unknown, making the solution space too large. To reduce the solution space,
many approaches have been proposed by exploiting various priors. In practice,
these priors would easily lead to some unsuitable solution. This is because
they are achieved by applying an identical strength of constraints to all the
factors, which does not hold in practice. To overcome this limitation, we
propose a novel sparsity based method by learning a data-guided map to describe
the individual mixed level of each pixel. Through this data-guided map, the
$\ell_{p}(0&lt;p&lt;1)$ constraint is applied in an adaptive manner. Such
implementation not only meets the practical situation, but also guides the
spectral bases toward the pixels under highly sparse constraint. What's more,
an elegant optimization scheme as well as its convergence proof have been
provided in this paper. Extensive experiments on several datasets also
demonstrate that the data-guided map is feasible, and high quality unmixing
results could be obtained by our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3156</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3156</id><created>2014-03-12</created><authors><author><keyname>Joshi</keyname><forenames>Kashyap</forenames></author><author><keyname>Gohil</keyname><forenames>Vipul</forenames></author></authors><title>ARM 7 Based Controller Area Network for Accident Avoidance in
  Automobiles</title><categories>cs.OH</categories><comments>6 pages, 6 figure , &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;. http://www.ijettjournal.org.
  published by seventh sense. research group</comments><journal-ref>IJETT,V9(2),61-65 March 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V9P212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on requirements of modern vehicle, in- vehicle Controller Area Network
(CAN) architecture has been implemented. In order to reduce point to point
wiring harness in vehicle automation, CAN is suggested as a means for data
communication within the vehicle environment. The benefits of CAN bus based
network over traditional point to point schemes will offer increased
flexibility and expandability for future technology insertions.
  This paper describes system which uses sensors to measure various parameters
of the car like speed, distance from the other car, presence of alcohol in car
and accidental change of lane and sends a warning signal to the driver if any
of the parameter goes out of range to avoid accidents . In addition to this if
accident occurs in any remote area then using bump sensor accident is detected
and SMS is send immediately using GSM. A situation that provides a good example
of how the system works is when a driver is about to change lanes, and there is
a car in his blind spot. The sensors will detect that car and inform the driver
before he starts turning, preventing him from potentially getting into a
serious accident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3157</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3157</id><created>2014-03-12</created><authors><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Ma</keyname><forenames>Minghui</forenames></author></authors><title>The Computational Compexity of Decision Problem in Additive Extensions
  of Nonassociative Lambek Calculus</title><categories>cs.LO cs.CC</categories><comments>12 pages</comments><msc-class>03B47, 03F20</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the complexity of decision problems for Boolean Nonassociative
Lambek Calculus admitting empty antecedent of sequents ($\mathsf{BFNL^*}$), and
the consequence relation of Distributive Full Nonassociative Lambek Calculus
($\mathsf{DFNL}$). We construct a polynomial reduction from modal logic
$\mathsf{K}$ into $\mathsf{BFNL^*}$. As a consequence, we prove that the
decision problem for $\mathsf{BFNL^*}$ is PSPACE-hard. We also prove that the
same result holds for the consequence relation of DFNL, by reducing
$\mathsf{BFNL^*}$ in polynomial time to DFNL enriched with finite set of
assumptions. Finally, we prove analogous results for variants of
$\mathsf{BFNL^*}$, including $\mathsf{BFNL^*e}$ ($\mathsf{BFNL^*}$ with
exchange), modal extensions of $\mathsf{BFNL^*_i}$ and $\mathsf{BFNL^*_{ei}}$
for $i \in \{\mathsf{K}, \mathsf{T}, \mathsf{K4}, \mathsf{S4}, \mathsf{S5}\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3159</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3159</id><created>2014-03-13</created><authors><author><keyname>Movahed</keyname><forenames>Amin</forenames></author><author><keyname>Reed</keyname><forenames>Mark C.</forenames></author></authors><title>Iterative Detection for Compressive Sensing:Turbo CS</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, IEEE ICC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider compressive sensing as a source coding method for signal
transmission. We concatenate a convolutional coding system with 1-bit
compressive sensing to obtain a serial concatenated system model for sparse
signal transmission over an AWGN channel. The proposed source/channel decoder,
which we refer to as turbo CS, is robust against channel noise and its signal
reconstruction performance at the receiver increases considerably through
iterations. We show 12 dB improvement with six turbo CS iterations compared to
a non-iterative concatenated source/channel decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3162</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3162</id><created>2014-03-13</created><authors><author><keyname>Ali</keyname><forenames>Shahzad</forenames></author><author><keyname>Madani</keyname><forenames>Sajjad A.</forenames></author><author><keyname>Khan</keyname><forenames>Atta ur Rehman</forenames></author><author><keyname>Khan</keyname><forenames>Imran Ali</forenames></author></authors><title>Routing protocols for mobile sensor networks: a comparative study</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Systems Science &amp; Engineering,
  vol 29, no 1, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparison of cluster-based position and non
position-based routing protocols for mobile wireless sensor networks to outline
design considerations of protocols for mobile environments. The selected
protocols are compared on the basis of multiple parameters, which include
packet delivery ratio, packet loss, network lifetime, and control overhead
using variable number of nodes and speeds. The extensive simulation and
analysis of results show that position-based routing protocols incur less
packet loss as compared to the non position based protocols. However,
position-based protocols require localization mechanism or a GPS for the
location information, which consumes energy and affects the network lifetime.
Alternatively, non position-based protocols are more energy efficient and
provide extended network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3185</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3185</id><created>2014-03-13</created><authors><author><keyname>Haque</keyname><forenames>Md. Ansarul</forenames></author></authors><title>Sentiment Analysis by Using Fuzzy Logic</title><categories>cs.IR cs.CL</categories><comments>16 pages.
  http://airccse.org/journal/ijcseit/papers/4114ijcseit04.pdf, February 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How could a product or service is reasonably evaluated by anyone in the
shortest time? A million dollar question but it is having a simple answer:
Sentiment analysis. Sentiment analysis is consumers review on products and
services which helps both the producers and consumers (stakeholders) to take
effective and efficient decision within a shortest period of time. Producers
can have better knowledge of their products and services through the sentiment
analysis (ex. positive and negative comments or consumers likes and dislikes)
which will help them to know their products status (ex. product limitations or
market status). Consumers can have better knowledge of their interested
products and services through the sentiment analysis (ex. positive and negative
comments or consumers likes and dislikes) which will help them to know their
deserving products status (ex. product limitations or market status). For more
specification of the sentiment values, fuzzy logic could be introduced.
Therefore, sentiment analysis with the help of fuzzy logic (deals with
reasoning and gives closer views to the exact sentiment values) will help the
producers or consumers or any interested person for taking the effective
decision according to their product or service interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3196</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3196</id><created>2014-03-13</created><updated>2014-03-26</updated><authors><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Wu</keyname><forenames>Jinsong</forenames></author><author><keyname>Song</keyname><forenames>Enbin</forenames></author><author><keyname>Wang</keyname><forenames>Yaming</forenames></author></authors><title>Secure Beamforming For MIMO Broadcasting With Wireless Information And
  Power Transfer</title><categories>cs.IT math.IT</categories><comments>Submitted to journal for possible publication. First submission to
  arXiv Mar. 14 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a basic MIMO information-energy (I-E) broadcast system,
where a multi-antenna transmitter transmits information and energy
simultaneously to a multi-antenna information receiver and a dual-functional
multi-antenna energy receiver which is also capable of decoding information.
Due to the open nature of wireless medium and the dual purpose of information
and energy transmission, secure information transmission while ensuring
efficient energy harvesting is a critical issue for such a broadcast system.
Assuming that physical layer security techniques are applied to the system to
ensure secure transmission from the transmitter to the information receiver, we
study beamforming design to maximize the achievable secrecy rate subject to a
total power constraint and an energy harvesting constraint. First, based on
semidefinite relaxation, we propose global optimal solutions to the secrecy
rate maximization (SRM) problem in the single-stream case and a specific
full-stream case where the difference of Gram matrices of the channel matrices
is positive semidefinite. Then, we propose a simple iterative algorithm named
inexact block coordinate descent (IBCD) algorithm to tackle the SRM problem of
general case with arbitrary number of streams. We proves that the IBCD
algorithm can monotonically converge to a Karush-Kuhn-Tucker (KKT) solution to
the SRM problem. Furthermore, we extend the IBCD algorithm to the joint
beamforming and artificial noise design problem. Finally, simulations are
performed to validate the performance of the proposed beamforming algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3228</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3228</id><created>2014-03-13</created><authors><author><keyname>Fuchs</keyname><forenames>Benedikt</forenames></author><author><keyname>Sornette</keyname><forenames>Didier</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Fractal multi-level organisation of human groups in a virtual world</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are fundamentally social. They have progressively dominated their
environment by the strength and creativity provided by and within their
grouping. It is well recognised that human groups are highly structured, and
the anthropological literature has loosely classified them according to their
size and function, such as support cliques, sympathy groups, bands, cognitive
groups, tribes, linguistic groups and so on. Recently, combining data on human
grouping patterns in a comprehensive and systematic study, Zhou et al.
identified a quantitative discrete hierarchy of group sizes with a preferred
scaling ratio close to $3$, which was later confirmed for hunter-gatherer
groups and for other mammalian societies. Using high precision large scale
Internet-based social network data, we extend these early findings on a very
large data set. We analyse the organisational structure of a complete,
multi-relational, large social multiplex network of a human society consisting
of about 400,000 odd players of a massive multiplayer online game for which we
know all about the group memberships of every player. Remarkably, the online
players exhibit the same type of structured hierarchical layers as the
societies studied by anthropologists, where each of these layers is three to
four times the size of the lower layer. Our findings suggest that the
hierarchical organisation of human society is deeply nested in human
psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3235</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3235</id><created>2014-03-13</created><authors><author><keyname>Rana</keyname><forenames>Abhay</forenames></author><author><keyname>Nagda</keyname><forenames>Rushil</forenames></author></authors><title>A Security Analysis of Browser Extensions</title><categories>cs.CR</categories><comments>More information on the following links: http://nullcon.captnemo.in/
  http://captnemo.in/blog/2014/02/26/nullcon-experience/
  https://github.com/captn3m0/nullcon2014/</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Browser Extensions (often called plugins or addons) are small pieces of code
that let developers add additional functionality to the browser. However, with
extensions comes a security price: the user must trust the developer. We look
at ways in which this trust can be broken and malicious extensions installed.
We also look at silent installations of plugins in various browsers and work on
ways to make silent installations possible in browsers that work against it. We
compare the browser extension mechanism among various browsers, and try to
create a set of rules to maintain the principle of least privileges in the
browser. We track various plugins and determine whether the least privileges
required match with the privileges asked for. We also work on a survey of
extensions (for various browsers) and determine the nature of attacks possible.
For eg, if a developer account gets hacked, updating of a normal extension with
a malicious one is possible. We look at privilege abuse and survey extensions
that ask for more privileges than they use. We finally provide a solution and
allow a person to check the authenticity of the extension even before they
download it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3251</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3251</id><created>2014-03-13</created><updated>2015-03-30</updated><authors><author><keyname>Markl</keyname><forenames>Matthias</forenames></author><author><keyname>Ammer</keyname><forenames>Regina</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author><author><keyname>K&#xf6;rner</keyname><forenames>Carolin</forenames></author></authors><title>Numerical Investigations on Hatching Process Strategies for Powder Bed
  Based Additive Manufacturing using an Electron Beam</title><categories>cs.CE</categories><journal-ref>The International Journal of Advanced Manufacturing Technology:
  Volume 78, Issue 1 (2015), Page 239-247</journal-ref><doi>10.1007/s00170-014-6594-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates in hatching process strategies for additive
manufacturing using an electron beam by numerical simulations. The underlying
physical model and the corresponding three dimensional thermal free surface
lattice Boltzmann method of the simulation software are briefly presented. The
simulation software has already been validated on the basis of experiments up
to 1.2 kW beam power by hatching a cuboid with a basic process strategy,
whereby the results are classified into `porous', `good' and `uneven',
depending on their relative density and top surface smoothness. In this paper
we study the limitations of this basic process strategy in terms of higher beam
powers and scan velocities to exploit the future potential of high power
electron beam guns up to 10 kW. Subsequently, we introduce modified process
strategies, which circumvent these restrictions, to build the part as fast as
possible under the restriction of a fully dense part with a smooth top surface.
These process strategies are suitable to reduce the build time and costs,
maximize the beam power usage and therefore use the potential of high power
electron beam guns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3253</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3253</id><created>2014-02-21</created><authors><author><keyname>Kumar</keyname><forenames>Ranjan</forenames></author><author><keyname>Sahoo</keyname><forenames>G.</forenames></author></authors><title>Cloud Computing Simulation Using CloudSim</title><categories>cs.DC</categories><comments>5 pages, 2 figures,&quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;. http://www.ijettjournal.org.
  published by seventh sense research group</comments><journal-ref>International Journal of Engineering Trends and Technology(IJETT),
  8(2),82-86 February 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V8P216</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As we know that Cloud Computing is a new paradigm in IT. It has many
advantages and disadvantages. But in future it will spread in the whole world.
Many researches are going on for securing the cloud services. Simulation is the
act of imitating or pretending. It is a situation in which a particular set of
condition is created artificially in order to study that could exit in reality.
We need only a simple Operating System with some memory to startup our
Computer. All our resources will be available in the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3255</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3255</id><created>2014-02-28</created><authors><author><keyname>Soundarabai</keyname><forenames>P Beaulah</forenames></author><author><keyname>Sahai</keyname><forenames>Ritesh</forenames></author><author><keyname>J</keyname><forenames>Thriveni</forenames></author><author><keyname>Venugopal</keyname><forenames>K R</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Improved Bully Election Algorithm for Distributed Systems</title><categories>cs.DC</categories><comments>12 pages</comments><journal-ref>International Journal of Information Processing, 7(4), 43-54, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electing a leader is a classical problem in distributed computing system.
Synchronization between processes often requires one process acting as a
coordinator. If an elected leader node fails, the other nodes of the system
need to elect another leader without much wasting of time. The bully algorithm
is a classical approach for electing a leader in a synchronous distributed
computing system, which is used to determine the process with highest priority
number as the coordinator. In this paper, we have discussed the limitations of
Bully algorithm and proposed a simple and efficient method for the Bully
algorithm which reduces the number of messages during the election. Our
analytical simulation shows that, our proposed algorithm is more efficient than
the Bully algorithm with fewer messages passing and fewer stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3274</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3274</id><created>2014-03-13</created><authors><author><keyname>Ahmad</keyname><forenames>B. I.</forenames></author><author><keyname>Yakubu</keyname><forenames>F.</forenames></author><author><keyname>Bagiwa</keyname><forenames>M. A.</forenames></author><author><keyname>Abdullahi</keyname><forenames>U. I.</forenames></author></authors><title>Remote Home Management: An alternative for working at home while away</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote home management is one of the developing areas in current technology.
In this paper we described how to manage and control home appliances using
mobile phone, people can use this system to do things in their home from a far
place before they reach home. For instance, user may start his/her room cooler
or heater so that before they reach home the condition in the room will be
conducive, also appliances like washing machine and cooker can be started and
if the time taken for this appliances to perform a task is known that can also
be set, so that if the time elapsed the appliance will automatically switch off
itself. To control an appliance the user sends a command in form of SMS from
his/her mobile phone to a computer which is connected to the appliance, once
the message is received the computer will send the command to a microcontroller
for controlling the appliance appropriately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3286</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3286</id><created>2014-03-13</created><authors><author><keyname>Soudjani</keyname><forenames>S. Esmaeil Zadeh</forenames></author><author><keyname>Gevaerts</keyname><forenames>C.</forenames></author><author><keyname>Abate</keyname><forenames>A.</forenames></author></authors><title>FAUST$^2$: Formal Abstractions of Uncountable-STate STochastic processes</title><categories>cs.SY</categories><comments>This paper is submitted to the 26th International Conference on
  Computer Aided Verification (CAV 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FAUST$^2$ is a software tool that generates formal abstractions of (possibly
non-deterministic) discrete-time Markov processes (dtMP) defined over
uncountable (continuous) state spaces. A dtMP model is specified in MATLAB and
abstracted as a finite-state Markov chain or Markov decision processes. The
abstraction procedure runs in MATLAB and employs parallel computations and fast
manipulations based on vector calculus. The abstract model is formally put in
relationship with the concrete dtMP via a user-defined maximum threshold on the
approximation error introduced by the abstraction procedure. FAUST$^2$ allows
exporting the abstract model to well-known probabilistic model checkers, such
as PRISM or MRMC. Alternatively, it can handle internally the computation of
PCTL properties (e.g. safety or reach-avoid) over the abstract model, and
refine the outcomes over the concrete dtMP via a quantified error that depends
on the abstraction procedure and the given formula. The toolbox is available at
http://sourceforge.net/projects/faust2/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3297</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3297</id><created>2014-03-13</created><authors><author><keyname>Sur</keyname><forenames>Samarendra Nath</forenames></author><author><keyname>Bera</keyname><forenames>Dr. Rabindranath</forenames></author><author><keyname>Maji</keyname><forenames>Dr. Bansibadan</forenames></author></authors><title>Channel Capacity Analysis of MIMO System in Correlated Nakagami-m Fading
  Environment</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures,Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>IJETT, V9(3),101-105 March 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V9P221</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We consider Vertical Bell Laboratories Layered Space-Time (V-BLAST) systems
in correlated multiple-input multiple-output (MIMO) Nakagami-m fading channels
with equal power allocated to each transmit antenna and also we consider that
the channel state information (CSI) is available only at the receiver. Now for
practical application, study of the VBLAST MIMO system in correlated
environment is necessary. In this paper, we present a detailed study of the
channel capacity in correlated and uncorrelated channel condition and also
validated the result with appropriate mathematical relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3298</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3298</id><created>2014-03-13</created><updated>2014-03-14</updated><authors><author><keyname>Garas</keyname><forenames>Antonios</forenames></author><author><keyname>Tomasello</keyname><forenames>Mario V.</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Selection rules in alliance formation: strategic decisions or abundance
  of choice?</title><categories>physics.soc-ph cs.SI</categories><comments>15 pages, 4 figures plus 25 pages supporting material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how firms select partners using a large database of publicly
announced R&amp;D alliances over a period of 25 years. We identify, for the first
time, two distinct behavioral strategies of firms in forming these alliances.
By reconstructing and analysing the temporal R&amp;D network of 14,000
international firms and 21.000 publicly announced alliances, we find a
&quot;universal&quot; behavior in firms changing between these strategies. In the first
strategy, newcomers and nodes of low centrality initially establish links to
nodes of similar or higher centrality. After these firms have consolidated
their position and increased their centrality, they switch to the second
strategy, and preferably form links to less central nodes. In addition, we show
that $k$-core centrality can be established as a measure of firm's success that
correlates e.g. with the number of patents (obtained from a dataset of 3 Mio
patents). To synthesize our findings, we provide a network growth model based
on $k$-core centrality which reproduces the strategic behavior of firms, as
well as other properties of the empirical network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3300</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3300</id><created>2014-03-13</created><authors><author><keyname>Papavassilopoulos</keyname><forenames>G. P.</forenames></author></authors><title>Limiting Behavior of LQ Deterministic Infinite Horizon Nash Games with
  Symmetric Players as the Number of Players goes to Infinity</title><categories>cs.GT cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Linear Quadratic Deterministic Continuous Time Game with many symmetric
players is considered and the Linear Feedback Nash strategies are studied as
the number of players goes to infinity. We show that under some conditions the
limit of the solutions exists and can be used to approximate the case with a
finite but large number of players. It is shown that in the limit each player
acts as if he were faced with one player only, who represents the average
behavior of the others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3304</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3304</id><created>2014-03-13</created><authors><author><keyname>Hajari</keyname><forenames>Hadi</forenames></author><author><keyname>Hakimpour</keyname><forenames>Farshad</forenames></author></authors><title>A Spatial Data Model for Moving Object Databases</title><categories>cs.DB</categories><comments>This paper includes 20 pages and is published in IJDMS</comments><acm-class>H.2.1; H.2.3; H.2.8</acm-class><journal-ref>International Journal of Database Management Systems ( IJDMS )
  Vol.6, No.1, February 2014. ISSN:0975-5705 (Online); 0975-5985</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Moving Object Databases will have significant role in Geospatial Information
Systems as they allow users to model continuous movements of entities in the
databases and perform spatio-temporal analysis. For representing and querying
moving objects, and algebra with a comprehensive framework of User Defined
Types together with a set of functions on those types is needed. Moreover,
concerning real world applications, moving objects move along constrained
environments like transportation networks so that an extra algebra for modeling
networks is demanded, too. These algebras can be inserted in any data model if
their designs are based on available standards such as Open Geospatial
Consortium that provides a common model for existing DBMS's. In this paper, we
focus on extending a spatial data model for constrained moving objects. Static
and moving geometries in our model are based on Open Geospatial Consortium
standards. We also extend Structured Query Language for retrieving, querying,
and manipulating spatio-temporal data related to moving objects as a simple and
expressive query language. Finally as a proof of concept, we implement a
generator to generate data for moving objects constrained by a transportation
network. Such a generator primarily aims at traffic planning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3305</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3305</id><created>2014-03-13</created><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Salavati</keyname><forenames>Amir Hesam</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Noise Facilitation in Associative Memories of Exponential Capacity</title><categories>cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1301.6265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in associative memory design through structured pattern sets
and graph-based inference algorithms have allowed reliable learning and recall
of an exponential number of patterns. Although these designs correct external
errors in recall, they assume neurons that compute noiselessly, in contrast to
the highly variable neurons in brain regions thought to operate associatively
such as hippocampus and olfactory cortex.
  Here we consider associative memories with noisy internal computations and
analytically characterize performance. As long as the internal noise level is
below a specified threshold, the error probability in the recall phase can be
made exceedingly small. More surprisingly, we show that internal noise actually
improves the performance of the recall phase while the pattern retrieval
capacity remains intact, i.e., the number of stored patterns does not reduce
with noise (up to a threshold). Computational experiments lend additional
support to our theoretical analysis. This work suggests a functional benefit to
noisy neurons in biological neuronal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3312</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3312</id><created>2014-03-13</created><authors><author><keyname>Dave</keyname><forenames>Manish B</forenames></author></authors><title>Optimal number of users in Co-operative spectrum sensing in WRAN using
  Cyclo-Stationary Detector</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 6 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</comments><journal-ref>International Journal of Engineering Trends and Technology (IJETT)
  4(7):2806-2810, July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio allows unlicensed users to access licensed frequency bands
through dynamic spectrum access so as to reduce spectrum scarcity. This
requires intelligent spectrum sensing techniques. This paper investigates the
use of cyclo-stationary detector and performance evaluation for Digital Video
Broadcast-Terrestrial (DVB-T) signals. Generally, DVB-T is specified in IEEE
802.22 standard in VHF and UHF TV broadcasting spectrum. Simulations results
show that implementing co-operative spectrum sensing help in better utilization
of resources. The paper further proposes to find number of optimal users in a
scenario to optimize the detection probability and makes use of the particle
swarm optimization (PSO) technique to find an optimum value of threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3320</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3320</id><created>2014-03-13</created><updated>2016-03-01</updated><authors><author><keyname>Zhang</keyname><forenames>Jiong</forenames></author><author><keyname>Duits</keyname><forenames>Remco</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Gonzalo</forenames></author><author><keyname>Romeny</keyname><forenames>Bart M. ter Haar</forenames></author></authors><title>Numerical Approaches for Linear Left-invariant Diffusions on SE(2),
  their Comparison to Exact Solutions, and their Applications in Retinal
  Imaging</title><categories>math.NA cs.CV</categories><comments>A final and corrected version of the manuscript is Published in
  Numerical Mathematics: Theory, Methods and Applications (NM-TMA), vol. (9),
  p.1-50, 2016</comments><journal-ref>Numerical Mathematics: Theory, Methods and Applications (NM-TMA),
  vol. (9), p.1-50, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (and
their resolvent equations) have been widely studied in the fields of cortical
modeling and image analysis. They include hypo-elliptic diffusion (for contour
enhancement) proposed by Citti &amp; Sarti, and Petitot, and they include the
direction process (for contour completion) proposed by Mumford. This paper
presents a thorough study and comparison of the many numerical approaches,
which, remarkably, is missing in the literature. Existing numerical approaches
can be classified into 3 categories: Finite difference methods, Fourier based
methods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (Monte
Carlo simulations). There are also 3 types of exact solutions to the
PDE-evolutions that were derived explicitly (in the spatial Fourier domain) in
previous works by Duits and van Almsick in 2005. Here we provide an overview of
these 3 types of exact solutions and explain how they relate to each of the 3
numerical approaches. We compute relative errors of all numerical approaches to
the exact solutions, and the Fourier based methods show us the best performance
with smallest relative errors. We also provide an improvement of Mathematica
algorithms for evaluating Mathieu-functions, crucial in implementations of the
exact solutions. Furthermore, we include an asymptotical analysis of the
singularities within the kernels and we propose a probabilistic extension of
underlying stochastic processes that overcomes the singular behavior in the
origin of time-integrated kernels. Finally, we show retinal imaging
applications of combining left-invariant PDE-evolutions with invertible
orientation scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3325</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3325</id><created>2014-03-13</created><authors><author><keyname>Zocca</keyname><forenames>Alessandro</forenames></author><author><keyname>Borst</keyname><forenames>Sem C.</forenames></author><author><keyname>van Leeuwaarden</keyname><forenames>Johan S. H.</forenames></author></authors><title>Slow transitions, slow mixing and starvation in dense random-access
  networks</title><categories>math.PR cs.NI</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dense wireless random-access networks, modeled as systems of
particles with hard-core interaction. The particles represent the network users
that try to become active after an exponential back-off time, and stay active
for an exponential transmission time. Due to wireless interference, active
users prevent other nearby users from simultaneous activity, which we describe
as hard-core interaction on a conflict graph. We show that dense networks with
aggressive back-off schemes lead to extremely slow transitions between dominant
states, and inevitably cause long mixing times and starvation effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3328</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3328</id><created>2014-03-13</created><authors><author><keyname>Mool</keyname><forenames>Madhulika</forenames></author><author><keyname>Sabale</keyname><forenames>Prajyoti</forenames></author><author><keyname>Parpelli</keyname><forenames>Sneha</forenames></author><author><keyname>Chowriwar</keyname><forenames>Shalaka</forenames></author><author><keyname>Sambhe</keyname><forenames>Nilesh</forenames></author></authors><title>Mitigating Denial Of Services Using Secure Overlay Service Model</title><categories>cs.NI cs.CR</categories><comments>5 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denial of service (DoS) and Distributed Denial of Service (DDoS) attacks
continue to threaten the reliability of networking systems. Previous approaches
for protecting networks from DoS attacks are reactive in that they wait for an
attack to be launched before taking appropriate measures to protect the
network. This leaves the door open for other attacks that use more
sophisticated methods to mask their traffic. A secure overlay services (SOS)
architecture has been proposed to provide reliable communication between
clients and a target under DoS attacks. The SOS architecture employs a set of
overlay nodes arranged in three hierarchical layers that controls access to the
target. We propose an architecture called secure overlay services (SOS) that
proactively prevents denial of service (DoS) attacks, which works toward
supporting emergency services, or similar types of communication. The
architecture uses a combination of secure overlay tunneling, routing via
consistent hashing, and filtering. We reduce the probability of successful
attacks by: 1) performing intensive filtering near protected network edges,
pushing the attack point into the core of the network, where high-speed routers
can handle the volume of attack traffic and 2) introducing randomness and
anonymity into the forwarding architecture, making it difficult for an attacker
to target nodes along the path to a specific SOSprotected destination. Using
simple analytical models, we evaluate the likelihood that an attacker can
successfully launch a DoS attack against an SOS protected network. Our analysis
demonstrates that such an architecture reduces the likelihood of a successful
attack to minuscule levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3336</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3336</id><created>2014-03-13</created><authors><author><keyname>Knowles</keyname><forenames>Kenneth</forenames></author></authors><title>Executable Refinement Types</title><categories>cs.PL</categories><comments>Ph.D. dissertation. Accepted by the University of California, Santa
  Cruz, in March 2014. 278 pages (295 including frontmatter)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This dissertation introduces executable refinement types, which refine
structural types by semi-decidable predicates, and establishes their metatheory
and accompanying implementation techniques. These results are useful for
undecidable type systems in general.
  Particular contributions include: (1) Type soundness and a logical relation
for extensional equivalence for executable refinement types (though type
checking is undecidable); (2) hybrid type checking for executable refinement
types, which blends static and dynamic checks in a novel way, in some sense
performing better statically than any decidable approximation; (3) a type
reconstruction algorithm - reconstruction is decidable even though type
checking is not, when suitably redefined to apply to undecidable type systems;
(4) a novel use of existential types with dependent types to ensure that the
language of logical formulae is closed under type checking (5) a prototype
implementation, Sage, of executable refinement types such that all dynamic
errors are communicated back to the compiler and are thenceforth static errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3339</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3339</id><created>2014-03-13</created><authors><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author></authors><title>Capacity of a Nonlinear Optical Channel with Finite Memory</title><categories>cs.IT math.IT physics.optics</categories><journal-ref>IEEE/OSA Journal of Lightwave Technology, vol. 32, no. 16, pp.
  2862-2876, Aug. 2014</journal-ref><doi>10.1109/JLT.2014.2328518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The channel capacity of a nonlinear, dispersive fiber-optic link is
revisited. To this end, the popular Gaussian noise (GN) model is extended with
a parameter to account for the finite memory of realistic fiber channels. This
finite-memory model is harder to analyze mathematically but, in contrast to
previous models, it is valid also for nonstationary or heavy-tailed input
signals. For uncoded transmission and standard modulation formats, the new
model gives the same results as the regular GN model when the memory of the
channel is about 10 symbols or more. These results confirm previous results
that the GN model is accurate for uncoded transmission. However, when coding is
considered, the results obtained using the finite-memory model are very
different from those obtained by previous models, even when the channel memory
is large. In particular, the peaky behavior of the channel capacity, which has
been reported for numerous nonlinear channel models, appears to be an artifact
of applying models derived for independent input in a coded (i.e., dependent)
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3342</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3342</id><created>2014-03-13</created><authors><author><keyname>Smith</keyname><forenames>Michael R.</forenames></author><author><keyname>Martinez</keyname><forenames>Tony</forenames></author><author><keyname>Giraud-Carrier</keyname><forenames>Christophe</forenames></author></authors><title>The Potential Benefits of Filtering Versus Hyper-Parameter Optimization</title><categories>stat.ML cs.LG</categories><comments>11 pages, 4 tables, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of an induced model by a learning algorithm is dependent on the
quality of the training data and the hyper-parameters supplied to the learning
algorithm. Prior work has shown that improving the quality of the training data
(i.e., by removing low quality instances) or tuning the learning algorithm
hyper-parameters can significantly improve the quality of an induced model. A
comparison of the two methods is lacking though. In this paper, we estimate and
compare the potential benefits of filtering and hyper-parameter optimization.
Estimating the potential benefit gives an overly optimistic estimate but also
empirically demonstrates an approximation of the maximum potential benefit of
each method. We find that, while both significantly improve the induced model,
improving the quality of the training set has a greater potential effect than
hyper-parameter optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3344</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3344</id><created>2014-03-13</created><authors><author><keyname>Mocanu</keyname><forenames>Delia</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Karsai</keyname><forenames>M&#xe0;rton</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Collective attention in the age of (mis)information</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>misinformation, attention patterns, false information, social
  response</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study, on a sample of 2.3 million individuals, how Facebook
users consumed different information at the edge of political discussion and
news during the last Italian electoral competition. Pages are categorized,
according to their topics and the communities of interests they pertain to, in
a) alternative information sources (diffusing topics that are neglected by
science and main stream media); b) online political activism; and c) main
stream media. We show that attention patterns are similar despite the different
qualitative nature of the information, meaning that unsubstantiated claims
(mainly conspiracy theories) reverberate for as long as other information.
Finally, we categorize users according to their interaction patterns among the
different topics and measure how a sample of this social ecosystem (1279 users)
responded to the injection of 2788 false information posts. Our analysis
reveals that users which are prominently interacting with alternative
information sources (i.e. more exposed to unsubstantiated claims) are more
prone to interact with false claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3349</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3349</id><created>2014-03-12</created><authors><author><keyname>Mowla</keyname><forenames>Md. Munjure</forenames></author><author><keyname>Ali</keyname><forenames>Md. Yeakub</forenames></author><author><keyname>Aoni</keyname><forenames>Rifat Ahmmed</forenames></author></authors><title>Performance Comparison of Two Clipping Based Filtering Methods for PAPR
  Reduction in OFDM Signal</title><categories>cs.NI</categories><comments>11 pages, 8 Figures, 5 Tables, Published: International Journal of
  Mobile Network Communications &amp; Telematics (IJMNCT), Vol. 4, No. 1, pp.23-34,
  February 2014 (ISSN: 1839-5678)</comments><doi>10.5121/ijmnct.2014.4103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of wireless communication technologies has been producing the
intense demand for high-speed, efficient, reliable voice &amp; data communication.
As a result, third generation partnership project (3GPP) has implemented next
generation wireless communication technology long term evolution (LTE) which is
designed to increase the capacity and speed of existing mobile telephone &amp; data
networks. LTE has adopted a multicarrier transmission technique known as
orthogonal frequency division multiplexing (OFDM). OFDM meets the LTE
requirement for spectrum flexibility and enables cost-efficient solutions for
very wide carriers. One major generic problem of OFDM technique is high peak to
average power ratio (PAPR) which is defined as the ratio of the peak power to
the average power of the OFDM signal. A trade-off is necessary for reducing
PAPR with increasing bit error rate (BER), computational complexity or data
rate loss etc. In this paper, two clipping based filtering methods have been
implemented &amp; also analyzed their modulation effects on reducing PAPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3351</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3351</id><created>2014-03-13</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Semantic Unification A sheaf theoretic approach to natural language</title><categories>cs.CL</categories><comments>12 pages</comments><journal-ref>Categories and Types in Logic, Language, and Physics, A Festshrift
  for Jim Lambek. Casadio, Coecke, Moortgat, Scott (eds.), Lecture Notes in
  Computer Science, Volume 8222, pp. 1-13, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language is contextual and sheaf theory provides a high level mathematical
framework to model contextuality. We show how sheaf theory can model the
contextual nature of natural language and how gluing can be used to provide a
global semantics for a discourse by putting together the local logical
semantics of each sentence within the discourse. We introduce a presheaf
structure corresponding to a basic form of Discourse Representation Structures.
Within this setting, we formulate a notion of semantic unification --- gluing
meanings of parts of a discourse into a coherent whole --- as a form of
sheaf-theoretic gluing. We illustrate this idea with a number of examples where
it can used to represent resolutions of anaphoric references. We also discuss
multivalued gluing, described using a distributions functor, which can be used
to represent situations where multiple gluings are possible, and where we may
need to rank them using quantitative measures.
  Dedicated to Jim Lambek on the occasion of his 90th birthday.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3354</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3354</id><created>2014-02-23</created><authors><author><keyname>Ma</keyname><forenames>Minghui</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author></authors><title>Residuated Basic Logic I</title><categories>math.LO cs.LO</categories><comments>18 pages with 1 figure</comments><msc-class>03B47</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the residuated basic logic ($\mathsf{RBL}$) of residuated basic
algebra in which the basic implication of Visser's basic propositional logic
($\mathsf{BPL}$) is interpreted as the right residual of a non-associative
binary operator $\cdot$ (product). We develop an algebraic system
$\mathsf{S_{RBL}}$ of residuated basic algebra by which we show that
$\mathsf{RBL}$ is a conservative extension of $\mathsf{BPL}$. We present the
sequent formalization $\mathsf{L_{RBL}}$ of $\mathsf{S_{RBL}}$ which is an
extension of distributive full non-associative Lambek calculus
($\mathsf{DFNL}$), and show that the cut elimination and subformula property
hold for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3366</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3366</id><created>2014-03-13</created><authors><author><keyname>Das</keyname><forenames>Anupam</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author></authors><title>Fingerprinting Smart Devices Through Embedded Acoustic Components</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use of smart devices gives rise to both security and privacy
concerns. Fingerprinting smart devices can assist in authenticating physical
devices, but it can also jeopardize privacy by allowing remote identification
without user awareness. We propose a novel fingerprinting approach that uses
the microphones and speakers of smart phones to uniquely identify an individual
device. During fabrication, subtle imperfections arise in device microphones
and speakers which induce anomalies in produced and received sounds. We exploit
this observation to fingerprint smart devices through playback and recording of
audio samples. We use audio-metric tools to analyze and explore different
acoustic features and analyze their ability to successfully fingerprint smart
devices. Our experiments show that it is even possible to fingerprint devices
that have the same vendor and model; we were able to accurately distinguish
over 93% of all recorded audio clips from 15 different units of the same model.
Our study identifies the prominent acoustic features capable of fingerprinting
devices with high success rate and examines the effect of background noise and
other variables on fingerprinting accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3369</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3369</id><created>2014-03-13</created><authors><author><keyname>Jaeger</keyname><forenames>Herbert</forenames></author></authors><title>Controlling Recurrent Neural Networks by Conceptors</title><categories>cs.NE</categories><comments>195 pages, 35 figures</comments><report-no>Jacobs University Technical Report Nr 31</report-no><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human brain is a dynamical system whose extremely complex sensor-driven
neural processes give rise to conceptual, logical cognition. Understanding the
interplay between nonlinear neural dynamics and concept-level cognition remains
a major scientific challenge. Here I propose a mechanism of neurodynamical
organization, called conceptors, which unites nonlinear dynamics with basic
principles of conceptual abstraction and logic. It becomes possible to learn,
store, abstract, focus, morph, generalize, de-noise and recognize a large
number of dynamical patterns within a single neural system; novel patterns can
be added without interfering with previously acquired ones; neural noise is
automatically filtered. Conceptors help explaining how conceptual-level
information processing emerges naturally and robustly in neural systems, and
remove a number of roadblocks in the theory and applications of recurrent
neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3371</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3371</id><created>2014-03-13</created><updated>2014-04-09</updated><authors><author><keyname>Firouzi</keyname><forenames>Hamed</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Spectral Correlation Hub Screening of Multivariate Time Series</title><categories>stat.OT cs.LG stat.AP</categories><comments>32 pages, To appear in Excursions in Harmonic Analysis: The February
  Fourier Talks at the Norbert Wiener Center</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter discusses correlation analysis of stationary multivariate
Gaussian time series in the spectral or Fourier domain. The goal is to identify
the hub time series, i.e., those that are highly correlated with a specified
number of other time series. We show that Fourier components of the time series
at different frequencies are asymptotically statistically independent. This
property permits independent correlation analysis at each frequency,
alleviating the computational and statistical challenges of high-dimensional
time series. To detect correlation hubs at each frequency, an existing
correlation screening method is extended to the complex numbers to accommodate
complex-valued Fourier components. We characterize the number of hub
discoveries at specified correlation and degree thresholds in the regime of
increasing dimension and fixed sample size. The theory specifies appropriate
thresholds to apply to sample correlation matrices to detect hubs and also
allows statistical significance to be attributed to hub discoveries. Numerical
results illustrate the accuracy of the theory and the usefulness of the
proposed spectral framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3375</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3375</id><created>2014-03-13</created><updated>2014-08-01</updated><authors><author><keyname>Shahrivari</keyname><forenames>Saeed</forenames></author><author><keyname>Jalili</keyname><forenames>Saeed</forenames></author></authors><title>Beyond Batch Processing: Towards Real-Time and Streaming Big Data</title><categories>cs.DC</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, big data is generated from many sources and there is a huge demand for
storing, managing, processing, and querying on big data. The MapReduce model
and its counterpart open source implementation Hadoop, has proven itself as the
de facto solution to big data processing. Hadoop is inherently designed for
batch and high throughput processing jobs. Although Hadoop is very suitable for
batch jobs but there is an increasing demand for non-batch processes on big
data like: interactive jobs, real-time queries, and big data streams. Since
Hadoop is not proper for these non-batch workloads, new solutions are proposed
to these new challenges. In this article, we discuss two categories of these
solutions: real-time processing, and stream processing for big data. For each
category, we discuss paradigms, strengths and differences to Hadoop. We also
introduce some practical systems and frameworks for each category. Finally,
some simple experiments are done to show effectiveness of some solutions
compared to available Hadoop-based solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3376</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3376</id><created>2014-03-13</created><updated>2015-04-08</updated><authors><author><keyname>Gao</keyname><forenames>Xiang</forenames></author><author><keyname>Edfors</keyname><forenames>Ove</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author></authors><title>Massive MIMO performance evaluation based on measured propagation data</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Wireless Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO, also known as very-large MIMO or large-scale antenna systems,
is a new technique that potentially can offer large network capacities in
multi-user scenarios. With a massive MIMO system, we consider the case where a
base station equipped with a large number of antenna elements simultaneously
serves multiple single-antenna users in the same time-frequency resource. So
far, investigations are mostly based on theoretical channels with independent
and identically distributed (i.i.d.) complex Gaussian coefficients, i.e.,
i.i.d. Rayleigh channels. Here, we investigate how massive MIMO performs in
channels measured in real propagation environments. Channel measurements were
performed at 2.6 GHz using a virtual uniform linear array (ULA) which has a
physically large aperture, and a practical uniform cylindrical array (UCA)
which is more compact in size, both having 128 antenna ports. Based on
measurement data, we illustrate channel behavior of massive MIMO in three
representative propagation conditions, and evaluate the corresponding
performance. The investigation shows that the measured channels, for both array
types, allow us to achieve performance close to that in i.i.d. Rayleigh
channels. It is concluded that in real propagation environments we have
characteristics that can allow for efficient use of massive MIMO, i.e., the
theoretical advantages of this new technology can also be harvested in real
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3378</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3378</id><created>2014-03-13</created><updated>2014-06-07</updated><authors><author><keyname>Goh</keyname><forenames>Siong Thye</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Box Drawings for Learning with Imbalanced Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast majority of real world classification problems are imbalanced,
meaning there are far fewer data from the class of interest (the positive
class) than from other classes. We propose two machine learning algorithms to
handle highly imbalanced classification problems. The classifiers constructed
by both methods are created as unions of parallel axis rectangles around the
positive examples, and thus have the benefit of being interpretable. The first
algorithm uses mixed integer programming to optimize a weighted balance between
positive and negative class accuracies. Regularization is introduced to improve
generalization performance. The second method uses an approximation in order to
assist with scalability. Specifically, it follows a \textit{characterize then
discriminate} approach, where the positive class is characterized first by
boxes, and then each box boundary becomes a separate discriminative classifier.
This method has the computational advantages that it can be easily
parallelized, and considers only the relevant regions of feature space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3391</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3391</id><created>2014-03-13</created><authors><author><keyname>Chatterjee</keyname><forenames>Siddharth</forenames></author><author><keyname>Sen</keyname><forenames>Arunava</forenames></author></authors><title>Automated Reasoning in Social Choice Theory - Some Remarks</title><categories>cs.LO cs.GT</categories><msc-class>2000</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our objective in this note is to comment briefly on the newly emerging
literature on computer-aided proofs in Social Choice Theory. We shall
specifically comment on two papers, one by Tang and Lin (2009) and another by
Geist and Endriss (2011). We also provide statements and brief descriptions of
the results discussed in this note.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3427</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3427</id><created>2014-03-13</created><authors><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Explicit Matrices with the Restricted Isometry Property: Breaking the
  Square-Root Bottleneck</title><categories>math.FA cs.IT math.CO math.IT</categories><comments>Book chapter, submitted to Compressed Sensing and its Applications</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Matrices with the restricted isometry property (RIP) are of particular
interest in compressed sensing. To date, the best known RIP matrices are
constructed using random processes, while explicit constructions are notorious
for performing at the &quot;square-root bottleneck,&quot; i.e., they only accept sparsity
levels on the order of the square root of the number of measurements. The only
known explicit matrix which surpasses this bottleneck was constructed by
Bourgain, Dilworth, Ford, Konyagin and Kutzarova. This chapter provides three
contributions to further the groundbreaking work of Bourgain et al.: (i) we
develop an intuition for their matrix construction and underlying proof
techniques; (ii) we prove a generalized version of their main result; and (iii)
we apply this more general result to maximize the extent to which their matrix
construction surpasses the square-root bottleneck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3431</identifier>
 <datestamp>2014-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3431</id><created>2014-03-13</created><updated>2014-03-21</updated><authors><author><keyname>De Biasi</keyname><forenames>Marzio</forenames></author></authors><title>Minimal TSP Tour is coNP-Complete</title><categories>cs.CC</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of deciding if a Traveling Salesman Problem (TSP) tour is minimal
was proved to be coNP-complete by Papadimitriou and Steiglitz. We give an
alternative proof based on a polynomial time reduction from 3SAT. Like the
original proof, our reduction also shows that given a graph $G$ and an
Hamiltonian path of $G$, it is NP-complete to check if $G$ contains an
Hamiltonian cycle (Restricted Hamiltonian Cycle problem).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3434</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3434</id><created>2014-03-13</created><updated>2014-08-29</updated><authors><author><keyname>Khazaeni</keyname><forenames>Yasaman</forenames></author><author><keyname>Cassandras</keyname><forenames>Christos G.</forenames></author></authors><title>A New Event-Driven Cooperative Receding Horizon Controller for
  Multi-agent Systems in Uncertain Environments</title><categories>cs.SY math.OC</categories><comments>One results subsection added. Some typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, a Cooperative Receding Horizon (CRH) controller was
developed for solving cooperative multi-agent problems in uncertain
environments. In this paper, we overcome several limitations of this
controller, including potential instabilities in the agent trajectories and
poor performance due to inaccurate estimation of a reward-to-go function. We
propose an event-driven CRH controller to solve the maximum reward collection
problem (MRCP) where multiple agents cooperate to maximize the total reward
collected from a set of stationary targets in a given mission space. Rewards
are non-increasing functions of time and the environment is uncertain with new
targets detected by agents at random time instants. The controller sequentially
solves optimization problems over a planning horizon and executes the control
for a shorter action horizon, where both are defined by certain events
associated with new information becoming available. In contrast to the earlier
CRH controller, we reduce the originally infinite-dimensional feasible control
set to a finite set at each time step. We prove some properties of this new
controller and include simulation results showing its improved performance
compared to the original one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3438</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3438</id><created>2014-03-13</created><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>Agustsson</keyname><forenames>Eirikur</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Neighborhood Selection for Thresholding-based Subspace Clustering</title><categories>stat.ML cs.IT math.IT</categories><comments>ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering refers to the problem of clustering high-dimensional data
points into a union of low-dimensional linear subspaces, where the number of
subspaces, their dimensions and orientations are all unknown. In this paper, we
propose a variation of the recently introduced thresholding-based subspace
clustering (TSC) algorithm, which applies spectral clustering to an adjacency
matrix constructed from the nearest neighbors of each data point with respect
to the spherical distance measure. The new element resides in an individual and
data-driven choice of the number of nearest neighbors. Previous performance
results for TSC, as well as for other subspace clustering algorithms based on
spectral clustering, come in terms of an intermediate performance measure,
which does not address the clustering error directly. Our main analytical
contribution is a performance analysis of the modified TSC algorithm (as well
as the original TSC algorithm) in terms of the clustering error directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3448</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3448</id><created>2014-03-13</created><updated>2014-08-26</updated><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author></authors><title>Coloring Large Complex Networks</title><categories>cs.SI cs.DS math.CO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large social or information network, how can we partition the
vertices into sets (i.e., colors) such that no two vertices linked by an edge
are in the same set while minimizing the number of sets used. Despite the
obvious practical importance of graph coloring, existing works have not
systematically investigated or designed methods for large complex networks. In
this work, we develop a unified framework for coloring large complex networks
that consists of two main coloring variants that effectively balances the
tradeoff between accuracy and efficiency. Using this framework as a fundamental
basis, we propose coloring methods designed for the scale and structure of
complex networks. In particular, the methods leverage triangles,
triangle-cores, and other egonet properties and their combinations. We
systematically compare the proposed methods across a wide range of networks
(e.g., social, web, biological networks) and find a significant improvement
over previous approaches in nearly all cases. Additionally, the solutions
obtained are nearly optimal and sometimes provably optimal for certain classes
of graphs (e.g., collaboration networks). We also propose a parallel algorithm
for the problem of coloring neighborhood subgraphs and make several key
observations. Overall, the coloring methods are shown to be (i) accurate with
solutions close to optimal, (ii) fast and scalable for large networks, and
(iii) flexible for use in a variety of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3455</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3455</id><created>2014-03-13</created><updated>2015-08-31</updated><authors><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Asynchronous Convex Consensus in the Presence of Crash Faults</title><categories>cs.DC</categories><comments>A version of this work is published in PODC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a new consensus problem, convex consensus. Similar to
vector consensus [13, 20, 19], the input at each process is a d-dimensional
vector of reals (or, equivalently, a point in the d-dimensional Euclidean
space). However, for convex consensus, the output at each process is a convex
polytope contained within the convex hull of the inputs at the fault-free
processes. We explore the convex consensus problem under crash faults with
incorrect inputs, and present an asynchronous approximate convex consensus
algorithm with optimal fault tolerance that reaches consensus on an optimal
output polytope. Convex consensus can be used to solve other related problems.
For instance, a solution for convex consensus trivially yields a solution for
vector consensus. More importantly, convex consensus can potentially be used to
solve other more interesting problems, such as convex function optimization [5,
4].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3458</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3458</id><created>2014-03-13</created><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Inkulu</keyname><forenames>Rajasekhar</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Two-Point $L_1$ Shortest Path Queries in the Plane</title><categories>cs.CG cs.DS</categories><comments>38 pages, 13 figures. A preliminary version appeared in SoCG 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{P}$ be a set of $h$ pairwise-disjoint polygonal obstacles with
a total of $n$ vertices in the plane. We consider the problem of building a
data structure that can quickly compute an $L_1$ shortest obstacle-avoiding
path between any two query points $s$ and $t$. Previously, a data structure of
size $O(n^2\log n)$ was constructed in $O(n^2\log^2 n)$ time that answers each
two-point query in $O(\log^2 n+k)$ time, i.e., the shortest path length is
reported in $O(\log^2 n)$ time and an actual path is reported in additional
$O(k)$ time, where $k$ is the number of edges of the output path. In this
paper, we build a new data structure of size $O(n+h^2\cdot \log h \cdot
4^{\sqrt{\log h}})$ in $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time
that answers each query in $O(\log n+k)$ time. Note that $n+h^2\cdot \log^{2} h
\cdot 4^{\sqrt{\log h}}=O(n+h^{2+\epsilon})$ for any constant $\epsilon&gt;0$.
Further, we extend our techniques to the weighted rectilinear version in which
the &quot;obstacles&quot; of $\mathcal{P}$ are rectilinear regions with &quot;weights&quot; and
allow $L_1$ paths to travel through them with weighted costs. Our algorithm
answers each query in $O(\log n+k)$ time with a data structure of size
$O(n^2\cdot \log n\cdot 4^{\sqrt{\log n}})$ that is built in $O(n^2\cdot
\log^{2} n\cdot 4^{\sqrt{\log n}})$ time (note that $n^2\cdot \log^{2} n\cdot
4^{\sqrt{\log n}}= O(n^{2+\epsilon})$ for any constant $\epsilon&gt;0$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1403.3460</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1403.3460</id><created>2014-03-13</created><authors><author><keyname>Wang</keyname><forenames>Chi</forenames></author><author><keyname>Liu</keyname><forenames>Xueqing</forenames></author><author><keyname>Song</keyname><forenames>Yanglei</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>Scalable and Robust Construction of Topical Hierarchies</title><categories>cs.LG cs.CL cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated generation of high-quality topical hierarchies for a text
collection is a dream problem in knowledge engineering with many valuable
applications. In this paper a scalable and robust algorithm is proposed for
constructing a hierarchy of topics from a text collection. We divide and
conquer the problem using a top-down recursive framework, based on a tensor
orthogonal decomposition technique. We solve a critical challenge to perform
scalable inference for our newly designed hierarchical topic model. Experiments
with various real-world datasets illustrate its ability to generate robust,
high-quality hierarchies efficiently. Our method reduces the time of
construction by several orders of magnitude, and its robust feature renders it
possible for users to interactively revise the hierarchy.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="57000" completeListSize="102538">1122234|58001</resumptionToken>
</ListRecords>
</OAI-PMH>
